<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6687 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6687</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6687</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-3219527aa44d7789c2ed842c90bbc6da0eacd527</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3219527aa44d7789c2ed842c90bbc6da0eacd527" target="_blank">Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions.</p>
                <p><strong>Paper Abstract:</strong> Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6687.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6687.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AE-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-Elimination Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-network deep RL architecture that combines a standard DQN for control with an Action Elimination Network (AEN) trained to predict an elimination signal; a contextual linear-bandit built on the AEN's last-layer features is used to eliminate inadmissible actions and reduce the effective action set during learning and acting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AE-DQN (Action-Elimination DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two neural networks: (1) a DQN (CNN-based NLP architecture) that estimates Q-values over actions; (2) an Action Elimination Network (AEN, CNN-based) that is trained in a supervised manner to predict a binary elimination signal. Every L steps the AEN's last-layer activations are used to build per-action linear contextual bandit models (V_a, b_a) whose estimates and confidence bounds produce an admissible action set A' used by the DQN for action selection and target computation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / Zork (text-based games); subdomains: Egg Quest, Troll Quest; Open Zork</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay buffer + short-term concatenated state history + per-action contextual-bandit statistics (covariance / reward-sum matrices) used as a persistent elimination memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored transitions (s,a,r,s', elimination_signal), last-layer AEN activations phi(s) (features), concatenated last-4-state embeddings as the agent's state representation, and per-action matrices V_a (lambda I + sum phi phi^T) and vectors b_a (sum phi * e) for the contextual bandit</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append-only experience replay (FIFO up to capacity N) for transitions and elimination signals; periodic batch AENUpdate every L steps which computes V_a and b_a from stored phi(s) and e labels and sets the AEN target last-layer weights to V_a^{-1} b_a; periodic target-network updates for Q^- and E^-; the last-4-states concatenation is computed online per step</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>analytic linear-contextual-bandit retrieval: compute confidence bound sqrt(beta * phi(s)^T V_a^{-1} phi(s)) and use AEN(s)_a - confidence > ell criterion to rule out actions; admissible actions A' = {a : E(s)_a - confidence < ell}; no nearest-neighbor or embedding search is described</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning (DQN) combined with supervised learning for AEN (MSE against elimination signal) and periodic linear contextual-bandit fitting on AEN features (closed-form ridge regression per action)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>game score / cumulative reward (final/max reward), steps-to-solve for quests, learning curves averaged over seeds</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>AE-DQN (with action elimination memory mechanisms) consistently learned faster and achieved higher reward than the vanilla DQN baseline across experiments (Egg Quest, Troll Quest, Open Zork); e.g., in Troll Quest AE-DQN significantly outperformed DQN and reached performance comparable to an "optimal elimination" 35-action baseline (exact numeric scores are shown in the paper's figures and tables but are not reported as single summary numbers in the provided text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Vanilla DQN (no action-elimination memory) learned more slowly and achieved lower scores in the same text-game tasks; the paper reports qualitative and plotted differences (AE-DQN > DQN), but does not provide a single unified numeric comparison value in the excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Key empirical findings related to the elimination/memory mechanism: (1) gains from action elimination increase with the number of actions (AE-DQN more robust when action set grows); (2) AE-DQN is more robust to hyperparameter settings in large action sets; (3) theoretical analysis shows invalid actions are sampled a finite (noise-free) or logarithmic number of times (noisy) due to the contextual-bandit elimination, improving sample complexity; (4) AEN features must be fixed while fitting the linear bandit, so the authors use periodic (batch) updates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires an elimination signal (provided by environment or rule-based system); features used by the contextual bandit must be fixed between updates (necessitating batch AENUpdate); choice of threshold ell influences behavior (authors suggest ell≈0.5); coupling between elimination and control is non-trivial if done jointly (their approach decouples via contextual bandits); no claims about using pretrained large language models — architecture is CNN-based NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Design or provide an elimination signal (e.g., parser feedback) and train an AEN to generalize it; use last-layer activations of the AEN as the contextual features for per-action linear bandits; perform periodic (batch) AENUpdates to produce fixed features for bandit estimation; choose ell≈0.5 when the expectation threshold is unknown; use admissible-action sets both for action selection (ACT) and for target (Targets) computation to reduce Q-function approximation error.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6687.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AEN / Contextual Bandit Memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action Elimination Network with per-action contextual linear-bandit statistics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The AEN is a supervised CNN trained to predict a binary elimination signal; its last-layer activations are periodically used as contextual features phi(s) to fit per-action ridge-regression bandit models (V_a, b_a), which serve as a persistent memory for eliminating actions via confidence bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AEN + contextual linear bandit (component of AE-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>AEN: CNN trained with MSE on the binary elimination signal; features phi(s) are taken from the last hidden layer and used to construct per-action V_a and b_a which are stored and inverted to compute V_a^{-1} b_a (ridge solution). These serve both as a target for the AEN's last layer (LastLayer(E_a^-) set to V_a^{-1} b_a during AENUpdate) and as a memory for computing action admissibility at query time.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Zork (text-based games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>per-action sufficient-statistics memory (V_a, b_a) built from AEN features</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>V_a (lambda I + sum phi phi^T) and b_a (sum phi * e) per action; AEN last-layer activations phi(s) stored in replay to recompute these matrices</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>periodic batch recomputation from replay memory (AENUpdate) which recomputes V_a and b_a from stored phi(s) and e labels and writes V_a^{-1} b_a into the AEN target last-layer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>closed-form linear algebra: compute phi(s)^T V_a^{-1} phi(s) to form confidence bounds and compute estimated elimination expectation via theta_hat_a^T phi(s) (theta_hat_a = V_a^{-1} b_a)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>supervised learning for AEN (MSE on elimination), closed-form ridge regression for per-action bandits, combined with DQN RL</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>impact on DQN learning speed and final reward; sample complexity (theoretical bounds on number of times invalid actions are sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Enabling per-action bandit memory and elimination produced faster learning and higher rewards versus vanilla DQN across tasks (qualitative and plotted results provided).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Without the elimination/bandit memory (vanilla DQN), agents explore many invalid actions and learn more slowly; theoretical and empirical evidence in paper indicate substantial slowdown when elimination is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Theoretical results: invalid actions are sampled at most O(beta_t / (u - ell)^2) times; in the noise-free case invalid actions are sampled only a finite number of times. Empirical: elimination yields larger gains when action space is larger.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires storing phi(s) and elimination labels in replay; features must be approximately realizable for the linear model; periodic batch updates are needed because features change during learning.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use last-layer activations as contextual features, hold features fixed between bandit fits (batch updates), choose regularization lambda and ell sensibly (ell≈0.5 suggested), and use confidence-based thresholds to avoid eliminating valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6687.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay (transitions + elimination)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Experience Replay Buffer storing transitions and elimination labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard replay memory used to store transitions (s, a, r, s') plus the environment-provided binary elimination signal e(s,a); these stored tuples are sampled for both DQN updates and for periodic AEN/bandit fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Replay buffer (component of AE-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An experience replay of capacity N stores tuples {s_t, a_t, r_t, e_t, s_{t+1}}; minibatches sampled from the buffer are used to update both the DQN (Q network) and the AEN (E network). The same buffer is the source for computing per-action V_a and b_a during AENUpdate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Zork and synthetic grid domains</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic/trajectory buffer (replay memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw stored tuples: full concatenated state representation (last-4 states), action, reward, elimination signal, next-state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>append with capacity limit N (standard replay mechanism); sampled uniformly for minibatch SGD and for AENUpdate computations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>random minibatch sampling from replay for SGD; full-scan by action to compute V_a and b_a during periodic AENUpdate</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning with replay-based minibatch updates for DQN and supervised AEN updates</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>learning curves (reward vs training steps), sample complexity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Replay enabled stable learning and provided data to fit the bandit elimination memory; required for AENUpdate and for supervised AEN training (qualitative in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not explicitly ablated in the paper, but replay is a standard DQN component; removing replay would prohibit the described AENUpdate-from-replay procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Replay must store elimination labels as well as transitions; computing per-action V_a from large replay can be expensive for large action sets (paper implements this periodically).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Store elimination signal with transitions; use periodic batch recomputation from replay to update bandit statistics to keep features fixed during fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6687.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Narasimhan2015 (LSTM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning (Narasimhan, Kulkarni, Barzilay 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited in the paper that used a word-level LSTM to learn representations end-to-end for control in text-based games; the LSTM provides a recurrent representation (hidden state) that can act as working memory over sequences of words/observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language understanding for text-based games using deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-based text-game agent (Narasimhan et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A word-level LSTM was used to build state representations end-to-end for DQN-style control in text-based games; the recurrent hidden state captures sequence information from the textual observation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (recurrent network)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text-based games (prior experiments cited)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent hidden state (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>hidden-state vectors summarizing recent textual observations</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>sequential recurrent updates (LSTM cell updates each time-step)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>implicit via the recurrent hidden state passed to downstream layers (no explicit retrieval mechanism described in this paper's mention)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>deep reinforcement learning (end-to-end) as described in the cited work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6687.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>He2015 (unbounded actions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep reinforcement learning with an unbounded action space (He et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work that extended DQN to unbounded (natural-language) action spaces by learning separate representations for states and actions with two different DNNs and modeling Q(s,a) as an inner product between their embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with an unbounded action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>State–action embedding DQN (He et al., 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-network architecture: one network produces a state embedding, another produces action embeddings; Q-value computed via inner product between state and action embeddings, enabling scaling to large (potentially unbounded) natural-language action spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>two neural networks (state and action encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text-based games (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>reinforcement learning with neural-network based Q approximation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>The paper notes He et al. generalized to large action spaces but in practice considered only small action sets per state (4 actions) in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6687.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TextWorld (Côté2018)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TextWorld: A learning environment for text-based games (Côté et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited environment/framework for training and evaluating agents on text-based games; provides tooling and benchmarks but is only referenced, not used directly by AE-DQN experiments (Zork and subdomains are used).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TextWorld: A learning environment for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TextWorld environment (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A framework for generating and running text-based game environments intended for research on RL+NLP; mentioned in related work as part of the text-game research landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TextWorld (environment)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6687.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yuan2018 (Counting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counting to explore and generalize in text-based games (Yuan et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited piece of related work in the text-based game literature addressing exploration and generalization; mentioned in the paper's list of recent text-game agents but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Counting to explore and generalize in text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Counting-based exploration agents (Yuan et al., 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Cited work that uses counting / exploration strategies in text-based games; the present paper references it as an example of prior methods tackling exploration and long-term memory requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6687.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6687.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zelinka2018</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using reinforcement learning to learn how to play text-based games (Zelinka, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work that investigated RL approaches for text-based games; mentioned as part of the prior literature but not described in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using reinforcement learning to learn how to play text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RL-based text-game agents (Zelinka, 2018)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced work combining prior representation approaches for text-game control; no details on memory mechanisms are provided in the present paper's mention.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with an unbounded action space <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Counting to explore and generalize in text-based games <em>(Rating: 1)</em></li>
                <li>Using reinforcement learning to learn how to play text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6687",
    "paper_id": "paper-3219527aa44d7789c2ed842c90bbc6da0eacd527",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "AE-DQN",
            "name_full": "Action-Elimination Deep Q-Network",
            "brief_description": "A two-network deep RL architecture that combines a standard DQN for control with an Action Elimination Network (AEN) trained to predict an elimination signal; a contextual linear-bandit built on the AEN's last-layer features is used to eliminate inadmissible actions and reduce the effective action set during learning and acting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "AE-DQN (Action-Elimination DQN)",
            "agent_description": "Two neural networks: (1) a DQN (CNN-based NLP architecture) that estimates Q-values over actions; (2) an Action Elimination Network (AEN, CNN-based) that is trained in a supervised manner to predict a binary elimination signal. Every L steps the AEN's last-layer activations are used to build per-action linear contextual bandit models (V_a, b_a) whose estimates and confidence bounds produce an admissible action set A' used by the DQN for action selection and target computation.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho / Zork (text-based games); subdomains: Egg Quest, Troll Quest; Open Zork",
            "memory_used": true,
            "memory_type": "experience replay buffer + short-term concatenated state history + per-action contextual-bandit statistics (covariance / reward-sum matrices) used as a persistent elimination memory",
            "memory_representation": "stored transitions (s,a,r,s', elimination_signal), last-layer AEN activations phi(s) (features), concatenated last-4-state embeddings as the agent's state representation, and per-action matrices V_a (lambda I + sum phi phi^T) and vectors b_a (sum phi * e) for the contextual bandit",
            "memory_update_mechanism": "append-only experience replay (FIFO up to capacity N) for transitions and elimination signals; periodic batch AENUpdate every L steps which computes V_a and b_a from stored phi(s) and e labels and sets the AEN target last-layer weights to V_a^{-1} b_a; periodic target-network updates for Q^- and E^-; the last-4-states concatenation is computed online per step",
            "memory_retrieval_method": "analytic linear-contextual-bandit retrieval: compute confidence bound sqrt(beta * phi(s)^T V_a^{-1} phi(s)) and use AEN(s)_a - confidence &gt; ell criterion to rule out actions; admissible actions A' = {a : E(s)_a - confidence &lt; ell}; no nearest-neighbor or embedding search is described",
            "training_method": "reinforcement learning (DQN) combined with supervised learning for AEN (MSE against elimination signal) and periodic linear contextual-bandit fitting on AEN features (closed-form ridge regression per action)",
            "evaluation_metric": "game score / cumulative reward (final/max reward), steps-to-solve for quests, learning curves averaged over seeds",
            "performance_with_memory": "AE-DQN (with action elimination memory mechanisms) consistently learned faster and achieved higher reward than the vanilla DQN baseline across experiments (Egg Quest, Troll Quest, Open Zork); e.g., in Troll Quest AE-DQN significantly outperformed DQN and reached performance comparable to an \"optimal elimination\" 35-action baseline (exact numeric scores are shown in the paper's figures and tables but are not reported as single summary numbers in the provided text).",
            "performance_without_memory": "Vanilla DQN (no action-elimination memory) learned more slowly and achieved lower scores in the same text-game tasks; the paper reports qualitative and plotted differences (AE-DQN &gt; DQN), but does not provide a single unified numeric comparison value in the excerpt.",
            "has_comparative_results": true,
            "ablation_findings": "Key empirical findings related to the elimination/memory mechanism: (1) gains from action elimination increase with the number of actions (AE-DQN more robust when action set grows); (2) AE-DQN is more robust to hyperparameter settings in large action sets; (3) theoretical analysis shows invalid actions are sampled a finite (noise-free) or logarithmic number of times (noisy) due to the contextual-bandit elimination, improving sample complexity; (4) AEN features must be fixed while fitting the linear bandit, so the authors use periodic (batch) updates.",
            "reported_limitations": "Requires an elimination signal (provided by environment or rule-based system); features used by the contextual bandit must be fixed between updates (necessitating batch AENUpdate); choice of threshold ell influences behavior (authors suggest ell≈0.5); coupling between elimination and control is non-trivial if done jointly (their approach decouples via contextual bandits); no claims about using pretrained large language models — architecture is CNN-based NLP.",
            "best_practices_recommendations": "Design or provide an elimination signal (e.g., parser feedback) and train an AEN to generalize it; use last-layer activations of the AEN as the contextual features for per-action linear bandits; perform periodic (batch) AENUpdates to produce fixed features for bandit estimation; choose ell≈0.5 when the expectation threshold is unknown; use admissible-action sets both for action selection (ACT) and for target (Targets) computation to reduce Q-function approximation error.",
            "uuid": "e6687.0",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "AEN / Contextual Bandit Memory",
            "name_full": "Action Elimination Network with per-action contextual linear-bandit statistics",
            "brief_description": "The AEN is a supervised CNN trained to predict a binary elimination signal; its last-layer activations are periodically used as contextual features phi(s) to fit per-action ridge-regression bandit models (V_a, b_a), which serve as a persistent memory for eliminating actions via confidence bounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "AEN + contextual linear bandit (component of AE-DQN)",
            "agent_description": "AEN: CNN trained with MSE on the binary elimination signal; features phi(s) are taken from the last hidden layer and used to construct per-action V_a and b_a which are stored and inverted to compute V_a^{-1} b_a (ridge solution). These serve both as a target for the AEN's last layer (LastLayer(E_a^-) set to V_a^{-1} b_a during AENUpdate) and as a memory for computing action admissibility at query time.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Zork (text-based games)",
            "memory_used": true,
            "memory_type": "per-action sufficient-statistics memory (V_a, b_a) built from AEN features",
            "memory_representation": "V_a (lambda I + sum phi phi^T) and b_a (sum phi * e) per action; AEN last-layer activations phi(s) stored in replay to recompute these matrices",
            "memory_update_mechanism": "periodic batch recomputation from replay memory (AENUpdate) which recomputes V_a and b_a from stored phi(s) and e labels and writes V_a^{-1} b_a into the AEN target last-layer",
            "memory_retrieval_method": "closed-form linear algebra: compute phi(s)^T V_a^{-1} phi(s) to form confidence bounds and compute estimated elimination expectation via theta_hat_a^T phi(s) (theta_hat_a = V_a^{-1} b_a)",
            "training_method": "supervised learning for AEN (MSE on elimination), closed-form ridge regression for per-action bandits, combined with DQN RL",
            "evaluation_metric": "impact on DQN learning speed and final reward; sample complexity (theoretical bounds on number of times invalid actions are sampled)",
            "performance_with_memory": "Enabling per-action bandit memory and elimination produced faster learning and higher rewards versus vanilla DQN across tasks (qualitative and plotted results provided).",
            "performance_without_memory": "Without the elimination/bandit memory (vanilla DQN), agents explore many invalid actions and learn more slowly; theoretical and empirical evidence in paper indicate substantial slowdown when elimination is not used.",
            "has_comparative_results": true,
            "ablation_findings": "Theoretical results: invalid actions are sampled at most O(beta_t / (u - ell)^2) times; in the noise-free case invalid actions are sampled only a finite number of times. Empirical: elimination yields larger gains when action space is larger.",
            "reported_limitations": "Requires storing phi(s) and elimination labels in replay; features must be approximately realizable for the linear model; periodic batch updates are needed because features change during learning.",
            "best_practices_recommendations": "Use last-layer activations as contextual features, hold features fixed between bandit fits (batch updates), choose regularization lambda and ell sensibly (ell≈0.5 suggested), and use confidence-based thresholds to avoid eliminating valid actions.",
            "uuid": "e6687.1",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Experience Replay (transitions + elimination)",
            "name_full": "Experience Replay Buffer storing transitions and elimination labels",
            "brief_description": "Standard replay memory used to store transitions (s, a, r, s') plus the environment-provided binary elimination signal e(s,a); these stored tuples are sampled for both DQN updates and for periodic AEN/bandit fitting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "Replay buffer (component of AE-DQN)",
            "agent_description": "An experience replay of capacity N stores tuples {s_t, a_t, r_t, e_t, s_{t+1}}; minibatches sampled from the buffer are used to update both the DQN (Q network) and the AEN (E network). The same buffer is the source for computing per-action V_a and b_a during AENUpdate.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Zork and synthetic grid domains",
            "memory_used": true,
            "memory_type": "episodic/trajectory buffer (replay memory)",
            "memory_representation": "raw stored tuples: full concatenated state representation (last-4 states), action, reward, elimination signal, next-state",
            "memory_update_mechanism": "append with capacity limit N (standard replay mechanism); sampled uniformly for minibatch SGD and for AENUpdate computations",
            "memory_retrieval_method": "random minibatch sampling from replay for SGD; full-scan by action to compute V_a and b_a during periodic AENUpdate",
            "training_method": "reinforcement learning with replay-based minibatch updates for DQN and supervised AEN updates",
            "evaluation_metric": "learning curves (reward vs training steps), sample complexity",
            "performance_with_memory": "Replay enabled stable learning and provided data to fit the bandit elimination memory; required for AENUpdate and for supervised AEN training (qualitative in paper).",
            "performance_without_memory": "Not explicitly ablated in the paper, but replay is a standard DQN component; removing replay would prohibit the described AENUpdate-from-replay procedure.",
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Replay must store elimination labels as well as transitions; computing per-action V_a from large replay can be expensive for large action sets (paper implements this periodically).",
            "best_practices_recommendations": "Store elimination signal with transitions; use periodic batch recomputation from replay to update bandit statistics to keep features fixed during fitting.",
            "uuid": "e6687.2",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Narasimhan2015 (LSTM agent)",
            "name_full": "Language understanding for text-based games using deep reinforcement learning (Narasimhan, Kulkarni, Barzilay 2015)",
            "brief_description": "Prior work cited in the paper that used a word-level LSTM to learn representations end-to-end for control in text-based games; the LSTM provides a recurrent representation (hidden state) that can act as working memory over sequences of words/observations.",
            "citation_title": "Language understanding for text-based games using deep reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "LSTM-based text-game agent (Narasimhan et al., 2015)",
            "agent_description": "A word-level LSTM was used to build state representations end-to-end for DQN-style control in text-based games; the recurrent hidden state captures sequence information from the textual observation.",
            "model_name": "LSTM (recurrent network)",
            "model_size": null,
            "benchmark_name": "text-based games (prior experiments cited)",
            "memory_used": true,
            "memory_type": "recurrent hidden state (LSTM)",
            "memory_representation": "hidden-state vectors summarizing recent textual observations",
            "memory_update_mechanism": "sequential recurrent updates (LSTM cell updates each time-step)",
            "memory_retrieval_method": "implicit via the recurrent hidden state passed to downstream layers (no explicit retrieval mechanism described in this paper's mention)",
            "training_method": "deep reinforcement learning (end-to-end) as described in the cited work",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": null,
            "best_practices_recommendations": null,
            "uuid": "e6687.3",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "He2015 (unbounded actions)",
            "name_full": "Deep reinforcement learning with an unbounded action space (He et al., 2015)",
            "brief_description": "Cited prior work that extended DQN to unbounded (natural-language) action spaces by learning separate representations for states and actions with two different DNNs and modeling Q(s,a) as an inner product between their embeddings.",
            "citation_title": "Deep reinforcement learning with an unbounded action space",
            "mention_or_use": "mention",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "State–action embedding DQN (He et al., 2015)",
            "agent_description": "Two-network architecture: one network produces a state embedding, another produces action embeddings; Q-value computed via inner product between state and action embeddings, enabling scaling to large (potentially unbounded) natural-language action spaces.",
            "model_name": "two neural networks (state and action encoders)",
            "model_size": null,
            "benchmark_name": "text-based games (mentioned)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "reinforcement learning with neural-network based Q approximation",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "The paper notes He et al. generalized to large action spaces but in practice considered only small action sets per state (4 actions) in experiments.",
            "best_practices_recommendations": null,
            "uuid": "e6687.4",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "TextWorld (Côté2018)",
            "name_full": "TextWorld: A learning environment for text-based games (Côté et al., 2018)",
            "brief_description": "A cited environment/framework for training and evaluating agents on text-based games; provides tooling and benchmarks but is only referenced, not used directly by AE-DQN experiments (Zork and subdomains are used).",
            "citation_title": "TextWorld: A learning environment for text-based games",
            "mention_or_use": "mention",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "TextWorld environment (cited)",
            "agent_description": "A framework for generating and running text-based game environments intended for research on RL+NLP; mentioned in related work as part of the text-game research landscape.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "TextWorld (environment)",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": null,
            "best_practices_recommendations": null,
            "uuid": "e6687.5",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Yuan2018 (Counting)",
            "name_full": "Counting to explore and generalize in text-based games (Yuan et al., 2018)",
            "brief_description": "A cited piece of related work in the text-based game literature addressing exploration and generalization; mentioned in the paper's list of recent text-game agents but not detailed here.",
            "citation_title": "Counting to explore and generalize in text-based games",
            "mention_or_use": "mention",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "Counting-based exploration agents (Yuan et al., 2018)",
            "agent_description": "Cited work that uses counting / exploration strategies in text-based games; the present paper references it as an example of prior methods tackling exploration and long-term memory requirements.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "text-based games",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": null,
            "best_practices_recommendations": null,
            "uuid": "e6687.6",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Zelinka2018",
            "name_full": "Using reinforcement learning to learn how to play text-based games (Zelinka, 2018)",
            "brief_description": "Cited prior work that investigated RL approaches for text-based games; mentioned as part of the prior literature but not described in detail in this paper.",
            "citation_title": "Using reinforcement learning to learn how to play text-based games",
            "mention_or_use": "mention",
            "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
            "agent_name": "RL-based text-game agents (Zelinka, 2018)",
            "agent_description": "Referenced work combining prior representation approaches for text-game control; no details on memory mechanisms are provided in the present paper's mention.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "text-based games",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": null,
            "best_practices_recommendations": null,
            "uuid": "e6687.7",
            "source_info": {
                "paper_title": "Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning with an unbounded action space",
            "rating": 2
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Counting to explore and generalize in text-based games",
            "rating": 1
        },
        {
            "paper_title": "Using reinforcement learning to learn how to play text-based games",
            "rating": 1
        }
    ],
    "cost": 0.017951250000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning</h1>
<p>Tom Zahavy<em>1,2, Matan Haroush ${ }^{</em> 1}$, Nadav Merlis ${ }^{<em> 1}$, Daniel J. Mankowitz ${ }^{3}$, Shie Mannor ${ }^{1}$<br>${ }^{1}$ The Technion - Israel Institute of Technology, ${ }^{2}$ Google research, ${ }^{3}$ Deepmind<br></em> Equal contribution<br>Corresponding to {tomzahavy, matan.h, merlis}@campus.technion.ac.il</p>
<h4>Abstract</h4>
<p>Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.</p>
<h2>1 Introduction</h2>
<p>Learning control policies for sequential decision-making tasks where both the state space and the action space are vast is critical when applying Reinforcement Learning (RL) to real-world problems. This is because there is an exponential growth of computational requirements as the problem size increases, known as the curse of dimensionality (Bertsekas and Tsitsiklis, 1995). Deep RL (DRL) tackles the curse of dimensionality due to large state spaces by utilizing a Deep Neural Network (DNN) to approximate the value function and/or the policy. This enables the agent to generalize across states without domain-specific knowledge (Tesauro, 1995; Mnih et al., 2015).
Despite the great success of DRL methods, deploying them in real-world applications is still limited. One of the main challenges towards that goal is dealing with large action spaces, especially when many of the actions are redundant or irrelevant (for many states). While humans can usually detect the subset of feasible actions in a given situation from the context, RL agents may attempt irrelevant actions or actions that are inferior, thus wasting computation time. Control systems for large industrial processes like power grids (Wen, O'Neill, and Maei, 2015; Glavic, Fonteneau, and Ernst, 2017; Dalal, Gilboa, and Mannor, 2016) and traffic control (Mannion, Duggan, and Howley, 2016; Van der Pol and Oliehoek, 2016) may have millions of possible actions that can be applied at every time step. Other domains utilize natural language to represent the actions. These action spaces are typically composed of all possible sequences of words from a fixed size dictionary resulting in considerably large action spaces. Common examples of systems that use this action space representation include conversational agents such as personal assistants (Dhingra et al., 2016; Li et al., 2017; Su et al., 2016; Lipton et al., 2016b; Liu et al., 2017; Zhao and Eskenazi, 2016; Wu et al., 2016), travel planners (Peng et al., 2017), restaurant/hotel bookers (Budzianowski et al., 2017), chat-bots (Serban et al., 2017; Li et al., 2016) and text-based game agents (Narasimhan, Kulkarni, and Barzilay, 2015; He et al., 2015; Zelinka, 2018; Yuan et al., 2018; Côté et al., 2018). RL is currently being applied in all of these domains, facing new challenges in function approximation and exploration due to the larger action space.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) Zork interface. The state in the game (observation) and the player actions are describe in natural language. (b) The action elimination framework. Upon taking action $a_{t}$, the agent observes a reward $r_{t}$, the next state $s_{t+1}$ and an elimination signal $e_{t}$. The agent uses this information to learn two function approximation deep networks: a DQN and an AEN. The AEN provides an admissible actions set $A^{\prime}$ to the DQN, which uses this set to decide how to act and learn.</p>
<p>In this work, we propose a new approach for dealing with large action spaces that is based on action elimination; that is, restricting the available actions in each state to a subset of the most likely ones (Figure 1(b)). We propose a method that eliminates actions by utilizing an elimination signal; a specific form of an auxiliary reward (Jaderberg et al., 2016), which incorporates domain-specific knowledge in text games. Specifically, it provides the agent with immediate feedback regarding taken actions that are not optimal. In many domains, creating an elimination signal can be done using rule-based systems. For example, in parser-based text games, the parser gives feedback regarding irrelevant actions after the action is played (e.g., Player: "Climb the tree." Parser: "There are no trees to climb"). Given such signal, we can train a machine learning model to predict it and then use it to generalize to unseen states. Since the elimination signal provides immediate feedback, it is faster to learn which actions to eliminate (e.g., with a contextual bandit using the elimination signal) than to learn the optimal actions using only the reward (due to long term consequences). Therefore, we can design an algorithm that enjoys better performance by exploring invalid actions less frequently.</p>
<p>More specifically, we propose a system that learns an approximation of the Q-function and concurrently learns to eliminate actions. We focus on tasks where natural language characterizes both the states and the actions. In particular, the actions correspond to fixed length sentences defined over a finite dictionary (of words). In this case, the action space is of combinatorial size (in the length of the sentence and the size of the dictionary) and irrelevant actions must be eliminated to learn. We introduce a novel DRL approach with two DNNs, a DQN and an Action Elimination Network (AEN), both designed using a Convolutional Neural Network (CNN) that is suited to NLP tasks (Kim, 2014). Using the last layer activations of the AEN, we design a linear contextual bandit model that eliminates irrelevant actions with high probability, balancing exploration/exploitation, and allowing the DQN to explore and learn Q-values only for valid actions.</p>
<p>We tested our method in a text-based game called "Zork". This game takes place in a virtual world in which the player interacts with the world through a text-based interface (see Figure 1(a)). The player can type in any command, corresponding to the in-game action. Since the input is text-based, this yields more than a thousand possible actions in each state (e.g., "open door", "open mailbox" etc.). We demonstrate the agent's ability to advance in the game faster than the baseline agents by eliminating irrelevant actions.</p>
<h1>2 Related Work</h1>
<p>Text-Based Games (TBG): Video games, via interactive learning environments like the Arcade Learning Environment (ALE) (Bellemare et al., 2013), have been fundamental to the development of DRL algorithms. Before the ubiquitousness of graphical displays, TBG like Zork were popular in the adventure gaming and role-playing communities. TBG present complex, interactive simulations which use simple language to describe the state of the environment, as well as reporting the effects of player actions (See Figure 1(a)). Players interact with the environment through text commands that respect a predefined grammar, which must be discovered in each game.</p>
<p>TBG provide a testbed for research at the intersection of RL and NLP, presenting a broad spectrum of challenges for learning algorithms (Côté et al., 2018) ${ }^{1}$. In addition to language understanding, successful play generally requires long-term memory, planning, exploration (Yuan et al., 2018), affordance extraction (Fulda et al., 2017), and common sense. Text games also highlight major open challenges for RL: the action space (text) is combinatorial and compositional, while game states are partially observable since text is often ambiguous or under-specific. Also, TBG often introduce stochastic dynamics, which is currently missing in standard benchmarks (Machado et al., 2017). For example, in Zork, there is a random probability of a troll killing the player. A thief can appear (also randomly) in each room.
Representations for TBG: To learn control policies from high-dimensional complex data such as text, good word representations are necessary. Kim (2014) designed a shallow word-level CNN and demonstrated state-of-the-art results on a variety of classification tasks by using word embeddings. For classification tasks with millions of labeled data, random embeddings were shown to outperform state-of-the-art techniques (Zahavy et al., 2018). On smaller data sets, using word2vec (Mikolov et al., 2013) yields good performance (Kim, 2014).
Previous work on TBG used pre-trained embeddings directly for control (Kostka et al., 2017; Fulda et al., 2017). Other works combined pre-trained embeddings with neural networks. For example, He et al. (2015) proposed to use Bag Of Words features as an input to a neural network, learned separate embeddings for states and actions, and then computed the Q function from autocorrelations between these embeddings. Narasimhan et al. (2015) suggested to use a word level Long Short-Term Memory (Hochreiter and Schmidhuber, 1997) to learn a representation end-to-end, and Zelinka et al. (2018), combined these approaches.
DRL with linear function approximation: DRL methods such as the DQN have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy (Mnih et al., 2015; Zahavy, Ben-Zrihem, and Mannor, 2016; Zrihem, Zahavy, and Mannor, 2016). Batch reinforcement learning methods with linear representations, on the other hand, are more stable and enjoy accurate uncertainty estimates. Yet, substantial feature engineering is necessary to achieve good results. A natural attempt at getting the best of both worlds is to learn a (linear) control policy on top of the representation of the last layer of a DNN. This approach was shown to refine the performance of DQNs (Levine et al., 2017) and improve exploration (Azizzadenesheli, Brunskill, and Anandkumar, 2018). Similarly, for contextual linear bandits, Riquelme et al. showed that a neuro-linear Thompson sampling approach outperformed deep (and linear) bandit algorithms in practice (Riquelme, Tucker, and Snoek, 2018).
RL in Large Action Spaces: Being able to reason in an environment with a large number of discrete actions is essential to bringing reinforcement learning to a larger class of problems. Most of the prior work concentrated on factorizing the action space into binary subspaces (Pazis and Parr, 2011; Dulac-Arnold et al., 2012; Lagoudakis and Parr, 2003). Other works proposed to embed the discrete actions into a continuous space, use a continuous-action policy gradient to find optimal actions in the continuous space, and finally, choose the nearest discrete action (Dulac-Arnold et al., 2015; Van Hasselt and Wiering, 2009). He et. al. (2015) extended DQN to unbounded (natural language) action spaces. His algorithm learns representations for the states and actions with two different DNNs and then models the Q values as an inner product between these representation vectors. While this approach can generalize to large action spaces, in practice, they only considered a small number of available actions (4) in each state.
Learning to eliminate actions was first mentioned by (Even-Dar, Mannor, and Mansour, 2003) who studied elimination in multi-armed bandits and tabular MDPs. They proposed to learn confidence intervals around the value function in each state and then use it to eliminate actions that are not optimal with high probability. Lipton et al. (2016a) studied a related problem where an agent wants to avoid catastrophic forgetting of dangerous states. They proposed to learn a classifier that detects hazardous states and then use it to shape the reward of a DQN agent. Fulda et al. (2017) studied affordances, the set of behaviors enabled by a situation, and presented a method for affordance extraction via inner products of pre-trained word embeddings.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>3 Action Elimination</p>
<p>We now describe a learning algorithm for MDPs with an elimination signal. Our approach builds on the standard RL formulation <em>(Sutton and Barto, 1998)</em>. At each time step $t$, the agent observes a state $s_{t}$ and chooses a discrete action $a_{t}\in{1,..,|A|}$. After executing the action, the agent obtains a reward $r_{t}(s_{t},a_{t})$ and observes the next state $s_{t+1}$ according to a transition kernel $P(s_{t+1}|s_{t},a_{t})$. The goal of the algorithm is to learn a policy $\pi(a|s)$ that maximizes the discounted cumulative return $V^{\pi}(s)=\mathbb{E}^{\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})|s_{0}=s]$ where $0&lt;\gamma&lt;1$ is the discount factor and $V$ is the value function. The optimal value function is given by $V^{<em>}(s)=\max_{\pi} V^{\pi}(s)$ and the optimal policy by $\pi^{</em>}(s)=\arg\max_{\pi} V^{\pi}(s)$. The Q-function $Q^{\pi}(s,a)=\mathbb{E}^{\pi}[\sum_{t=0}^{\infty}\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a]$ corresponds to the value of taking action $a$ in state $s$ and continuing according to policy $\pi$. The optimal Q-function $Q^{<em>}(s,a)=Q^{\pi^{</em>}}(s,a)$ can be found using the Q-learning algorithm <em>(Watkins and Dayan, 1992)</em>, and the optimal policy is given by $\pi^{<em>}(s)=\arg\max_{a} Q^{</em>}(s,a)$.</p>
<p>After executing an action, the agent also observes a binary elimination signal $e(s,a)$, which equals 1 if action $a$ may be eliminated in state $s$; that is, any optimal policy in state $s$ will never choose action $a$ (and 0 otherwise). The elimination signal can help the agent determine which actions not to take, thus aiding in mitigating the problem of large discrete action spaces. We start with the following definitions:</p>
<p>Definition 1. Valid state-action pairs with respect to an elimination signal are state action pairs which the elimination process should not eliminate.</p>
<p>As stated before, we assume that the set of valid state-action pairs contains all of the state-action pairs that are a part of some optimal policy, i.e., only strictly suboptimal state-actions can be invalid.</p>
<p>Definition 2. Admissible state-action pairs with respect to an elimination algorithm are state action pairs which the elimination algorithm does not eliminate.</p>
<p>In the following section, we present the main advantages of action elimination in MDPs with large action spaces. Afterward, we show that under the framework of linear contextual bandits <em>(Chu et al., 2011)</em>, probability concentration results <em>(Abbasi-Yadkori, Pal, and Szepesvari, 2011)</em> can be adapted to guarantee that action elimination is correct in high probability. Finally, we prove that Q-learning coupled with action elimination converges.</p>
<h3>3.1 Advantages in action elimination</h3>
<p>Action elimination allows the agent to overcome some of the main difficulties in large action spaces, namely: Function Approximation and Sample Complexity.</p>
<p>Function Approximation: It is well known that errors in the Q-function estimates may cause the learning algorithm to converge to a suboptimal policy, a phenomenon that becomes more noticeable in environments with large action spaces <em>(Thrun and Schwartz, 1993)</em>. Action elimination may mitigate this effect by taking the max operator only on valid actions, thus, reducing potential overestimation errors. Another advantage of action elimination is that the Q-estimates need only be accurate for valid actions. The gain is two-fold: first, there is no need to sample invalid actions for the function approximation to converge. Second, the function approximation can learn a simpler mapping (i.e., only the Q-values of the valid state-action pairs), and therefore may converge faster and to a better solution by ignoring errors from states that are not explored by the Q-learning policy <em>(Hester et al., 2018)</em>.</p>
<p>Sample Complexity: The sample complexity of the MDP measures the number of steps, during learning, in which the policy is not $\epsilon$-optimal <em>(Kakade and others, 2003)</em>. Assume that there are $A^{\prime}$ actions that should be eliminated and are $\epsilon$-optimal, i.e., their value is at least $V^{<em>}(s)-\epsilon$. According to lower bounds by </em>(Lattimore and Hutter, 2012)<em>, We need at least $\epsilon^{-2}(1-\gamma)^{-3}\log 1/\delta$ samples per state-action pair to converge with probability $1-\delta$. If, for example, the eliminated action returns no reward and doesn’t change the state, the action gap is $\epsilon=(1-\gamma)V^{</em>}(s)$, which translates to $V^{*}(s)^{-2}(1-\gamma)^{-5}\log 1/\delta$ 'wasted' samples for learning each invalid state-action pair. For large $\gamma$, this can lead to a tremendous number of samples (e.g., for $\gamma=0.99$, $(1-\gamma)^{-5}=10^{10}$). Practically, elimination algorithms can eliminate these actions substantially faster, and can, therefore, speed up the learning process approximately by $A/A^{\prime}$ (such that learning is effectively performed on the valid state-action pairs).</p>
<p>Embedding the elimination signal into the MDP is not trivial. One option is to shape the original reward by adding an elimination penalty. That is, decreasing the rewards when selecting the wrong actions. Reward shaping, however, is tricky to tune, may slow the convergence of the function approximation, and is not sample efficient (irrelevant actions are explored). Another option is to design a policy that is optimized by interleaved policy gradient updates on the two signals, maximizing the reward and minimizing the elimination signal error. The main difficulty with this approach is that both models are strongly coupled, and each model affects the observations of the other model, such that the convergence of any of the models is not trivial.</p>
<p>Next, we present a method that decouples the elimination signal from the MDP by using contextual multi-armed bandits. The contextual bandit learns a mapping from states (represented by context vectors $x(s)$ ) to the elimination signal $e(s, a)$ that estimates which actions should be eliminated. We start by introducing theoretical results on linear contextual bandits, and most importantly, concentration bounds for contextual bandits that require almost no assumptions on the context distribution. We will later show that under this model we can decouple the action elimination from the learning process in the MDP, allowing us to learn using standard Q-learning while eliminating actions correctly.</p>
<h1>3.2 Action elimination with contextual bandits</h1>
<p>Let $x\left(s_{t}\right) \in \mathbb{R}^{d}$ be the feature representation of state $s_{t}$. We assume (realizability) that under this representation there exists a set of parameters $\theta_{a}^{<em>} \in \mathbb{R}^{d}$ such that the elimination signal in state $s_{t}$ is $e_{t}\left(s_{t}, a\right)=\theta_{a}^{</em> T} x\left(s_{t}\right)+\eta_{t}$, where $\left|\theta_{a}^{*}\right|<em t="t">{2} \leq S$. $\eta</em>}$ is an $R$-subgaussian random variable with zero mean that models additive noise to the elimination signal. When there is no noise in the elimination signal, then $R=0$. Otherwise, as the elimination signal is bounded in $[0,1]$, it holds that $R \leq 1$. We'll also relax our previous assumptions and allow the elimination signal to have values $0 \leq \mathbb{E}\left[e_{t}\left(s_{t}, a\right)\right] \leq \ell$ for any valid action and $u \leq \mathbb{E}\left[e_{t}\left(s_{t}, a\right)\right] \leq 1$ for any invalid action, with $\ell<u$. Next, we denote by $X_{t, a}\left(E_{t, a}\right)$ the matrix (vector) whose rows (elements) are the observed state representation vectors (elimination signals) in which action $a$ was chosen, up to time $t$. For example, the $i^{\text {th }}$ row in $X_{t, a}$ is the representation vector of the $i^{\text {th }}$ state on which the action $a$ was chosen. Denote the solution to the regularized linear regression $\left\|X_{t, a} \theta_{t, a}-E_{t, a}\right\|_{2}^{2}+\lambda\left\|\theta_{t, a}\right\|_{2}^{2}$ (for some $\lambda>0$ ) by $\hat{\theta<em a="a" t_="t,">{t, a}=\hat{V}</em>}^{-1} X_{t, a}^{T} E_{t, a}$ where $\hat{V<em a="a" t_="t,">{t, a}=\lambda I+X</em>$.}^{T} X_{t, a</p>
<p>Similarly to Theorem 2 in (Abbasi-Yadkori, Pal, and Szepesvari, 2011)2, for any state history and with probability of at least $1-\delta$, it holds for all $t&gt;0$ that $\left|\hat{\theta}<em t="t">{t-1, a}^{T} x\left(s</em>}\right)-\theta_{a}^{* T} x\left(s_{t}\right)\right| \leq$ $\sqrt{\beta_{t-1}(\delta) x\left(s_{t}\right)^{T} \hat{V<em t="t">{t-1, a}^{-1} x\left(s</em>}\right)}$, where $\sqrt{\beta_{t}(\delta)}=R \sqrt{2 \log \left(\operatorname{det}\left(\hat{V<em 2="2">{t, a}\right)^{1 / 2} \operatorname{det}(\lambda I)^{-1 / 2} / \delta\right)}+\lambda^{1 / 2} S$. If $\forall s,|x(s)|</em>=\delta / k$ and bound this probability for all the actions, i.e., $\forall a, t&gt;0$} \leq L$, then $\beta_{t}$ can be bounded by $\sqrt{\beta_{t}(\delta)} \leq R \sqrt{d \log \left(\frac{1+t L^{2} / \lambda}{\delta}\right)}+\lambda^{1 / 2} S$. Next, we define $\tilde{\delta</p>
<p>$$
\operatorname{Pr}\left{\left|\hat{\theta}<em t="t">{t-1, a}^{T} x\left(s</em>}\right)-\theta_{a}^{* T} x\left(s_{t}\right)\right| \leq \sqrt{\beta_{t-1}(\tilde{\delta}) x\left(s_{t}\right)^{T} \hat{V<em t="t">{t-1, a}^{-1} x\left(s</em>\right} \geq 1-\delta
$$}\right)</p>
<p>Recall that any valid action $a$ at state $s$ satisfies $\mathbb{E}\left[e_{t}(s, a)\right]=\theta_{a}^{* T} x\left(s_{t}\right) \leq \ell$. Thus, we can eliminate action $a$ at state $s_{t}$ if</p>
<p>$$
\tilde{\theta}<em t="t">{t-1, a}^{T} x\left(s</em>}\right)-\sqrt{\beta_{t-1}(\tilde{\delta}) x\left(s_{t}\right)^{T} \hat{V<em t="t">{t-1, a}^{-1} x\left(s</em>&gt;\ell
$$}\right)</p>
<p>This ensures that with probability $1-\delta$ we never eliminate any valid action. We emphasize that only the expectation of the elimination signal is linear in the context. The expectation does not have to be binary (while the signal itself is). For example, in conversational agents, if a sentence is not understood by $90 \%$ of the humans who hear it, it is still desirable to avoid saying it. We also note that we assume $\ell$ is known, but in most practical cases, choosing $\ell \approx 0.5$ should suffice. In the current formulation, knowing $u$ is not necessary, though its value will affect the overall performance.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 Concurrent Learning</h1>
<p>We now show how the Q-learning and contextual bandit algorithms can learn simultaneously, resulting in the convergence of both algorithms, i.e., finding an optimal policy and a minimal valid action space. The challenge here is that each learning process affects the state-action distribution of the other. We first define Action Elimination Q-learning.
Definition 3. Action Elimination Q-learning is a Q-learning algorithm which updates only admissible state-action pairs and chooses the best action in the next state from its admissible actions. We allow the base Q-learning algorithm to be any algorithm that converges to $Q^{*}$ with probability 1 after observing each state-action infinitely often.</p>
<p>If the elimination is done based on the concentration bounds of the linear contextual bandits, we can ensure that Action Elimination Q-learning converges, as can be seen in Proposition 1 (See Appendix A for a full proof).
Proposition 1. Assume that all state action pairs $(s, a)$ are visited infinitely often unless eliminated according to $\bar{\theta}<em t-1="t-1">{t-1, a}^{T} x(s)-\sqrt{\beta</em>}(\bar{\delta}) x(s)^{T} \bar{V<em a="a" s_="s,">{t-1, a}^{-1} x(s)}&gt;\ell$. Then, with a probability of at least $1-\delta$, action elimination $Q$-learning converges to the optimal $Q$-function for any valid state-action pairs. In addition, actions which should be eliminated are visited at most $T</em>+1$ times.}(t) \leq 4 \beta_{t} /(u-\ell)^{2</p>
<p>Notice that when there is no noise in the elimination signal $(R=0)$, we correctly eliminate actions with probability 1 , and invalid actions will be sampled a finite number of times. Otherwise, under very mild assumptions, invalid actions will be sampled a logarithmic number of times.</p>
<h2>4 Method</h2>
<p>Using raw features like word2vec, directly for control, results in exhaustive computations. Moreover, raw features are typically not realizable, i,.e., the assumption that $e_{t}\left(s_{t}, a\right)=\theta_{a}^{<em> T} x\left(s_{t}\right)+\eta_{t}$ does not hold. Thus, we propose learning a set of features $\phi\left(s_{t}\right)$ that are realizable, i.e., $e\left(s_{t}, a\right)=\theta_{a}^{</em> T} \phi\left(s_{t}\right)$, using neural networks (using the activations of the last layer as features). A practical challenge here is that the features must be fixed over time when used by the contextual bandit, while the activations change during optimization. We therefore follow a batch-updates framework (Levine et al., 2017; Riquelme, Tucker, and Snoek, 2018), where every few steps we learn a new contextual bandit model that uses the last layer activations of the AEN as features.
Our Algorithm presents a hybrid approach for DRL with Action Elimination (AE), by incorporating AE into the well-known DQN algorithm to yield our AE-DQN (Algorithm 1 and Figure 1(b)). AE-DQN trains two networks: a DQN denoted by $Q$ and an AEN denoted by $E$. The algorithm uses $E$, and creates a linear contextual bandit model from it every $L$ iterations with procedure AENUpdate(). This procedure uses the activations of the last hidden layer of $E$ as features, $\phi(s) \leftarrow$ LastLayerActivations $(E(s))$, which are then used to create a contextual linear bandit model ( $V_{a}=$ $\lambda I+\sum_{j: a_{j}=a} \phi\left(s_{j}\right) \phi\left(s_{j}\right)^{T}, b_{a}=\sum_{j: a_{j}=a} \phi\left(s_{j}\right)^{T} e_{j}$ ). AENUpdate() proceeds by solving this model, and plugging the solution into the target AEN (LastLayer $\left(E_{a}^{-}\right) \leftarrow V_{a}^{-1} b_{a}$ ). The contextual linear bandit model ( $E^{-}, V$ ) is then used to eliminate actions (with high probability) via the ACT() and Targets() functions. ACT() follows an $\epsilon$-greedy mechanism on the admissible actions set $A^{\prime}=\left{a: E(s)<em a="a">{a}-\sqrt{\beta \phi(s)^{T} V</em>$. Targets() estimates the value function by taking max over $Q$-values only among admissible actions, hence, reducing function approximation errors.
Architectures: The agent uses an Experience Replay (Lin, 1992) to store information about states, transitions, actions, and rewards. In addition, our agent also stores the elimination signal, provided by the environment (Figure 1(b)). The architecture for both the AEN and DQN is an NLP CNN, based on (Kim, 2014). We represent the state as a sequence of words, composed of the game descriptor (Figure 1(a), "Observation") and the player's inventory. These are truncated or zero-padded (for simplicity) to a length of 50 (descriptor) +15 (inventory) words and each word is embedded into continuous vectors using word2vec in $\mathbb{R}^{300}$. The features of the last four states are then concatenated together such that our final state representations $s$ are in $\mathbb{R}^{78,000}$. The AEN is trained to minimize the}^{-1} \phi(s)}&lt;\ell\right}$. If it decides to exploit, then it selects the action with highest $Q$-value by taking an $\arg \max$ on $Q$-values among $A^{\prime}$, and if it chooses to explore, then, it selects an action uniformly from $A^{\prime</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">deep</span><span class="w"> </span><span class="nx">Q</span><span class="o">-</span><span class="nx">learning</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="nx">elimination</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">epsilon</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">beta</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ell</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">C</span><span class="p">,</span><span class="w"> </span><span class="nx">L</span><span class="p">,</span><span class="w"> </span><span class="nx">N</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">AEN</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">DQN</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">random</span><span class="w"> </span><span class="nx">weights</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">omega</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">respectively</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">target</span><span class="w"> </span><span class="nx">networks</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">},</span><span class="w"> </span><span class="nx">E</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">copy</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">omega</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Define</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">phi</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">LastLayerActivations</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">E</span><span class="p">(</span><span class="nx">s</span><span class="p">))</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">Initialize</span><span class="w"> </span><span class="nx">Replay</span><span class="w"> </span><span class="nx">Memory</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">capacity</span><span class="w"> </span><span class="nx">N</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">ACT</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">Q</span><span class="p">,</span><span class="w"> </span><span class="nx">E</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">},</span><span class="w"> </span><span class="nx">V</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">epsilon</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ell</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">beta</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Execute</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">observe</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">e_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="nx">Store</span><span class="w"> </span><span class="nx">transition</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">e_</span><span class="p">{</span><span class="nx">t</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">t</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">D</span>
<span class="w">        </span><span class="nx">Sample</span><span class="w"> </span><span class="nx">transitions</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">e_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="nx">j</span><span class="p">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">m</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">D</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="nx">j</span><span class="p">}=</span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">Targets</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="o">+</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">r_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">gamma</span><span class="p">,</span><span class="w"> </span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">},</span><span class="w"> </span><span class="nx">E</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">},</span><span class="w"> </span><span class="nx">V</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">beta</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">ell</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta</span><span class="p">=</span><span class="err">\</span><span class="nx">theta</span><span class="o">-</span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">theta</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="o">-</span><span class="nx">Q</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">theta</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">omega</span><span class="p">=</span><span class="err">\</span><span class="nx">omega</span><span class="o">-</span><span class="err">\</span><span class="nx">nabla_</span><span class="p">{</span><span class="err">\</span><span class="nx">omega</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">sum_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">e_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="o">-</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">E</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">s_</span><span class="p">{</span><span class="nx">j</span><span class="p">},</span><span class="w"> </span><span class="nx">a_</span><span class="p">{</span><span class="nx">j</span><span class="p">}</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="err">\</span><span class="nx">omega</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">If</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">bmod</span><span class="w"> </span><span class="nx">C</span><span class="p">)=</span><span class="mi">0</span><span class="p">:</span><span class="w"> </span><span class="nx">Q</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">Q</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="nx">If</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">t</span><span class="w"> </span><span class="err">\</span><span class="nx">bmod</span><span class="w"> </span><span class="nx">L</span><span class="p">)=</span><span class="mi">0</span><span class="p">:</span><span class="err">\</span><span class="p">)</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">E</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="p">},</span><span class="w"> </span><span class="nx">V</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">AENUpdate</span><span class="p">}(</span><span class="nx">E</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">lambda</span><span class="p">,</span><span class="w"> </span><span class="nx">D</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>function \(\operatorname{Act}\left(s, Q, E, V^{-1}, \epsilon, \beta, \ell\right)\)
    \(A^{\prime} \leftarrow\left\{a: E(s)_{a}-\sqrt{\beta \phi(s)^{T} V_{a}^{-1} \phi(s)}&lt;\ell\right\}\)
    With probability \(\epsilon\), return Uniform \(\left(A^{\prime}\right)\)
    Otherwise, return \(\underset{a \in A^{\prime}}{\arg \max } Q(s, a)\)
end function
function \(\operatorname{TARGETS}\left(s, r, \gamma, Q, E, V^{-1}, \beta, \ell\right)\)
    if \(s\) is a terminal state then return \(r\) end if
    \(A^{\prime} \leftarrow\left\{a: E(s)_{a}-\sqrt{\beta \phi(s)^{T} V_{a}^{-1} \phi(s)}&lt;\ell\right\}\)
    \(\operatorname{return}\left(r+\gamma \max <span class="ge">_{a \in A} Q(s, a)\right)\)</span>
<span class="ge">end function</span>
<span class="ge">function \(\operatorname{AENUpdate}\left(E^{-}, \lambda, D\right)\)</span>
<span class="ge">    for \(a \in A\) do</span>
<span class="ge">        \(V_</span>{a}^{-1}=\left(\sum_{j: a_{j}=a} \phi\left(s_{j}\right) \phi\left(s_{j}\right)^{T}+\lambda I\right)^{-1}\)
        \(b_{a}=\sum_{j: a_{j}=a} \phi\left(s_{j}\right)^{T} e_{j}\)
        \(\operatorname{Set} \operatorname{LastLayer}\left(E_{a}^{-}\right) \leftarrow V_{a}^{-1} b_{a}\)
    end for
    return \(E^{-}, V^{-1}\)
end function
</code></pre></div>

<p>MSE loss, using the elimination signal as a label. We used 100 (500 for DQN) convolutional filters, with three different 1D kernels of length $(1,2,3)$ such that the last hidden layer size is $300 .{ }^{3}$</p>
<h1>5 Experimental Results</h1>
<p>Grid World Domain: We start with an evaluation of action elimination in a small grid world domain with 9 rooms, where we can carefully analyze the effect of action elimination. In this domain, the agent starts at the center of the grid and needs to navigate to its upper-left corner. On every step, the agent suffers a penalty of $(-1)$, with a terminal reward of 0 . Prior to the game, the states are randomly divided into $K$ categories. The environment has $4 K$ navigation actions, 4 for each category, each with a probability to move in a random direction. If the chosen action belongs to the same category as the state, the action is performed correctly with probability $p_{c}^{T}=0.75$. Otherwise, it will be performed correctly with probability $p_{c}^{F}=0.5$. If the action does not fit the state category, the elimination signal equals 1 , and if the action and state belong to the same category, then it equals 0 . An optimal policy only uses the navigation actions from the same type as the state, as the other actions are clearly suboptimal. We experimented with a vanilla Q-learning agent without action elimination and a tabular version of action elimination Q-learning. Our simulations show that action elimination dramatically improves the results in large action spaces. In addition, we observed that the gain from action elimination increases as the amount of categories grows, and as the grid size grows, since the elimination allows the agent to reach the goal earlier. We have also experimented with random elimination signal and other modifications in the domain. Due to space constraints, we refer the reader to the appendix for figures and visualization of the domain.</p>
<p>Zork domain: "This is an open field west of a white house, with a boarded front door. There is a small mailbox here. A rubber mat saying 'Welcome to Zork!' lies by the door". This is an excerpt from the opening scene provided to a player in "Zork I: The Great Underground Empire"; one of the first interactive fiction computer games, created by members of the MIT Dynamic Modeling Group in the late 70s. By exploring the world via interactive text-based dialogue, the players progress in the game. The world of Zork presents a rich environment with a large state and action space (Figure 2). Zork players describe their actions using natural language instructions. For example, in the opening excerpt, an action might be 'open the mailbox' (Figure 1(a)). Once the player describes his/her action, it is processed by a sophisticated natural language parser. Based on the parser's results, the game</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Left: the world of Zork. Right: subdomains of Zork; the Troll (green) and Egg (blue) Quests. Credit: S. Meretzky, The Strong National Museum of Play. Larger versions in Appendix B.</p>
<p>presents the outcome of the action. The ultimate goal of Zork is to collect the Twenty Treasures of Zork and install them in the trophy case. Finding the treasures require solving a variety of puzzles such as navigation in complex mazes and intricate action sequences. During the game, the player is awarded points for performing deeds that bring him closer to the game's goal (e.g., solving puzzles). Placing all of the treasures into the trophy case generates a total score of 350 points for the player. Points that are generated from the game's scoring system are given to the agent as a reward. Zork presents multiple challenges to the player, like building plans to achieve long-term goals; dealing with random events like troll attacks; remembering implicit clues as well as learning the interactions between objects in the game and specific actions. The elimination signal in Zork is given by the Zork environment in two forms, a "wrong parse" flag, and text feedback (e.g. "you cannot take that"). We group these two signals into a single binary signal which we then provide to our learning algorithm. Before we started experimenting in the "Open Zork" domain, i.e., playing in Zork without any modifications to the domain, we evaluated the performance on two subdomains of Zork. These subdomains are inspired by the Zork plot and are referred to as the Egg Quest and the Troll Quest (Figure 2, right, and Appendix B). For these subdomains, we introduced an additional reward signal (in addition to the reward provided by the environment) to guide the agent towards solving specific tasks and make the results more visible. In addition, a reward of -1 is applied at every time step to encourage the agent to favor short paths. When solving "Open Zork" we only use the environment reward. The optimal time that it takes to solve each quest is 6 in-game timesteps for the Egg quest, 11 for the Troll quest and 350 for "Open Zork". The agent's goal in each subdomain is to maximize its cumulative reward. Each trajectory terminates upon completing the quest or after T steps are taken. We set the discounted factor during training to γ = 0.8 but use γ = 1 during evaluation [4]. We used β = 0.5, ℓ = 0.6 in all the experiments. The results are averaged over 5 random seeds, shown alongside error bars (std/3).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of agents in the egg quest.</p>
<p>The Egg Quest: In this quest, the agent's goal is to find and open the jewel-encrusted egg, hidden on a tree in the forest. The agent is awarded 100 points upon successful completion of this task. We experimented with the AE-DQN (blue) agent and a vanilla DQN agent (green) in this quest (Figure 3). The action set in this quest is composed of two subsets. A fixed subset of 9 actions that allow it to complete the Egg Quest like navigate (south, east etc.) open an item and fight; And a second subset consists of N<sub>Take</sub> "take" actions for possible objects in the game. The "take" actions correspond to</p>
<p><sup>4</sup>We adopted a common evaluation scheme that is used in the ALE. During learning and training we use γ &lt; 1 but evaluation is performed with γ = 1. Intuitively, during learning, choosing γ &lt; 1 helps to learn, while during evaluation, the sum of cumulative returns (γ = 1) is more interpretable (the score in the game).</p>
<p>taking a single object and include objects that need to be collected to complete quests, as well as other irrelevant objects from the game dictionary. We used two versions of this action set, $A_{1}$ with $N_{\text{Take}}=200$ and $A_{2}$ with $N_{\text{Take}}=300$. Robustness to hyperparameter tuning: We can see that for $A_{1}$, with T=100, (Figure 3) and for $A_{2}$, with T=200, (Figure 3) Both agents solve the task well. However, for $A_{2}$, with T=100, (Figure 3) the AE-DQN agent learns considerably faster, implying that action elimination is more robust to hyperparameters settings when there are many actions.</p>
<p>The Troll Quest: In this quest, the agent must find a way to enter the house, grab a lantern and light it, expose the hidden entrance to the underworld and then find the troll, awarding him 100 points. The Troll Quest presents a larger problem than the Egg Quest, but smaller than the full Zork domain; it is large enough to gain a useful understanding of our agents’ performance. The AE-DQN (blue) and DQN (green) agents use a similar action set to $A_{1}$ with 200 take actions and 15 necessary actions (215 in total). For comparison, We also included an "optimal elimination" baseline (red) that consists of only 35 actions (15 essential, and 20 relevant take actions). We can see in Figure 5 that AE-DQN significantly outperforms DQN, achieving compatible performance to the "optimal elimination" baseline. In addition, we can see that the improvement of the AE-DQN over DQN is more significant in the Troll Quest than the Egg quest. This observation is consistent with our tabular experiments.</p>
<p>"Open Zork": Next, we evaluated our agent in the "Open Zork" domain (without hand-crafting reward and termination signals). To compare our results with previous work, we trained our agent for 1M steps: each trajectory terminates after $T=200$ steps, and a total of 5000 trajectories were executed . We used two action sets: $A_{3}$, the "Minimal Zork" action set, is the minimal set of actions (131) that is required to solve the game (comparable with the action set used by <em>Kostka et al. (2017)</em>). The actions are taken from a tutorial for solving the game. $A_{4}$, the "Open Zork" action set, includes 1227 actions (comparable with <em>Fulda et al. (2017)</em>). This set is created from action "templates", composed of {Verb, Object} tuples for all the verbs (19) and objects (62) in the game (e.g, open mailbox). In addition, we include a fixed set of 49 actions of varying length (but not of length 2) that are required to solve the game. Table 1 presents the average (over seeds) maximal (in each seed) reward obtained by our AE-DQN agent in this domain while using action sets $A_{3}$ and $A_{4}$, showing that our agent achieves state-of-the-art results, outperforming all previous work. In the appendix, we show the learning curves for both AE-DQN and DQN agents. Again, we can see that AE-DQN outperforms DQN, learning faster and achieving more reward.</p>
<h2>6 Summary</h2>
<p>In this work, we proposed the AE-DQN, a DRL approach for eliminating actions while performing Q-learning, for solving MDPs with large state and action spaces. We tested our approach on the text-based game Zork, showing that by eliminating actions the size of the action space is reduced, exploration is more effective, and learning is improved. We provided theoretical guarantees on the convergence of our approach using linear contextual bandits. In future work, we plan to investigate more sophisticated architectures, as well as learning shared representations for elimination and control which may boost performance on both tasks. In addition, we aim to investigate other mechanisms for action elimination, e.g., eliminating actions that result from low Q-values (Even-Dar, Mannor, and Mansour, 2003). Another direction is to generate elimination signals in real-world domains. This can be done by designing a rule-based system for actions that should be eliminated, and then, training an AEN to generalize these rules for states that were not included in these rules. Finally, elimination signals may be provided implicitly, e.g., by human demonstrations of actions that should not be taken.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>Abbasi-Yadkori, Y.; Pal, D.; and Szepesvari, C. 2011. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems, 2312-2320.</p>
<p>Azizzadenesheli, K.; Brunskill, E.; and Anandkumar, A. 2018. Efficient exploration through bayesian deep q-networks. arXiv.</p>
<p>Bartlett, M. S. 1951. An inverse matrix adjustment arising in discriminant analysis. The Annals of Mathematical Statistics 22(1).</p>
<p>Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M. 2013. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research 47:253-279.</p>
<p>Bertsekas, D. P., and Tsitsiklis, J. N. 1995. Neuro-dynamic programming: an overview. In Decision and Control, 1995., Proceedings of the 34th IEEE Conference on, 560-564. IEEE.</p>
<p>Budzianowski, P.; Ultes, S.; Su, P.-H.; Mrksic, N.; Wen, T.-H.; Casanueva, I.; Rojas-Barahona, L.; and Gasic, M. 2017. Sub-domain modelling for dialogue management with hierarchical reinforcement learning. arXiv preprint.</p>
<p>Chu, W.; Li, L.; Reyzin, L.; and Schapire, R. 2011. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.</p>
<p>Côté, M.-A.; Kádár, Á.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; et al. 2018. Textworld: A learning environment for text-based games. arXiv.</p>
<p>Dalal, G.; Gilboa, E.; and Mannor, S. 2016. Hierarchical decision making in electricity grid management. In International Conference on Machine Learning, 2197-2206.</p>
<p>Dhingra, B.; Li, L.; Li, X.; Gao, J.; Chen, Y.-N.; Ahmed, F.; and Deng, L. 2016. End-to-end reinforcement learning of dialogue agents for information access. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Dulac-Arnold, G.; Denoyer, L.; Preux, P.; and Gallinari, P. 2012. Fast reinforcement learning with large action sets using error-correcting output codes for mdp factorization. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 180-194. Springer.</p>
<p>Dulac-Arnold, G.; Evans, R.; van Hasselt, H.; Sunehag, P.; Lillicrap, T.; Hunt, J.; Mann, T.; Weber, T.; Degris, T.; and Coppin, B. 2015. Deep reinforcement learning in large discrete action spaces. arXiv.</p>
<p>Even-Dar, E., and Mansour, Y. 2003. Learning rates for q-learning. Journal of Machine Learning Research 5(Dec):1-25.</p>
<p>Even-Dar, E.; Mannor, S.; and Mansour, Y. 2003. Action elimination and stopping conditions for reinforcement learning. In Proceedings of the 20th International Conference on Machine Learning.</p>
<p>Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. arXiv.</p>
<p>Glavic, M.; Fonteneau, R.; and Ernst, D. 2017. Reinforcement learning for electric power system decision and control: Past considerations and perspectives. IFAC-PapersOnLine 6918-6927.</p>
<p>He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2015. Deep reinforcement learning with an unbounded action space. CoRR abs/1511.04636.</p>
<p>Hester, T.; Vecerik, M.; Pietquin, O.; Lanctot, M.; Schaul, T.; Piot, B.; Sendonaris, A.; Dulac-Arnold, G.; Osband, I.; Agapiou, J.; et al. 2018. Learning from demonstrations for real world reinforcement learning. AAAI.</p>
<p>Hochreiter, S., and Schmidhuber, J. 1997. Long short-term memory. Neural computation 9(8):17351780 .</p>
<p>Jaderberg, M.; Mnih, V.; Czarnecki, W. M.; Schaul, T.; Leibo, J. Z.; Silver, D.; and Kavukcuoglu, K. 2016. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397.</p>
<p>Kakade, S. M., et al. 2003. On the sample complexity of reinforcement learning. Ph.D. Dissertation, University of London, England.</p>
<p>Kim, Y. 2014. Convolutional neural networks for sentence classification. arXiv preprint.
Kocsis, L., and Szepesvári, C. 2006. Bandit based monte-carlo planning. In European conference on machine learning, 282-293. Springer.</p>
<p>Kostka, B.; Kwiecieli, J.; Kowalski, J.; and Rychlikowski, P. 2017. Text-based adventures of the golovin ai agent. In Computational Intelligence and Games (CIG), 2017 IEEE Conference on. IEEE.</p>
<p>Lagoudakis, M. G., and Parr, R. 2003. Reinforcement learning as classification: Leveraging modern classifiers. In Proceedings of the 20th International Conference on Machine Learning (ICML-03).</p>
<p>Lattimore, T., and Hutter, M. 2012. Pac bounds for discounted mdps. In International Conference on Algorithmic Learning Theory.</p>
<p>Levine, N.; Zahavy, T.; Mankowitz, D. J.; Tamar, A.; and Mannor, S. 2017. Shallow updates for deep reinforcement learning. In Advances in Neural Information Processing Systems, 3138-3148.</p>
<p>Li, J.; Monroe, W.; Ritter, A.; Galley, M.; Gao, J.; and Jurafsky, D. 2016. Deep reinforcement learning for dialogue generation. arXiv.</p>
<p>Li, X.; Chen, Y.-N.; Li, L.; and Gao, J. 2017. End-to-end task-completion neural dialogue systems. arXiv preprint.</p>
<p>Lin, L.-J. 1992. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning 8(3-4).</p>
<p>Lipton, Z. C.; Gao, J.; Li, L.; Chen, J.; and Deng, L. 2016a. Combating reinforcement learning's sisyphean curse with intrinsic fear. arXiv.</p>
<p>Lipton, Z. C.; Gao, J.; Li, L.; Li, X.; Ahmed, F.; and Deng, L. 2016b. Efficient exploration for dialogue policy learning with bbq networks and replay buffer spiking. arXiv.</p>
<p>Liu, B.; Tur, G.; Hakkani-Tur, D.; Shah, P.; and Heck, L. 2017. End-to-end optimization of task-oriented dialogue model with deep reinforcement learning. arXiv preprint.</p>
<p>Machado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.; Hausknecht, M.; and Bowling, M. 2017. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. arXiv.</p>
<p>Mannion, P.; Duggan, J.; and Howley, E. 2016. An experimental review of reinforcement learning algorithms for adaptive traffic signal control. In Autonomic Road Transport Support Systems. Springer. 47-66.</p>
<p>Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems.</p>
<p>Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; et al. 2015. Human-level control through deep reinforcement learning. Nature 518(7540):529-533.</p>
<p>Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. CoRR abs/1506.08941.</p>
<p>Pazis, J., and Parr, R. 2011. Generalized value functions for large action sets. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), 1185-1192.</p>
<p>Peng, B.; Li, X.; Li, L.; Gao, J.; Celikyilmaz, A.; Lee, S.; and Wong, K.-F. 2017. Composite task-completion dialogue system via hierarchical deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Riquelme, C.; Tucker, G.; and Snoek, J. 2018. Deep bayesian bandits showdown. International Conference on Learning Representations.</p>
<p>Serban, I. V.; Sankar, C.; Germain, M.; Zhang, S.; Lin, Z.; Subramanian, S.; Kim, T.; Pieper, M.; Chandar, S.; Ke, N. R.; et al. 2017. A deep reinforcement learning chatbot. arXiv preprint.</p>
<p>Su, P.-H.; Gasic, M.; Mrksic, N.; Rojas-Barahona, L.; Ultes, S.; Vandyke, D.; Wen, T.-H.; and Young, S. 2016. Continuously learning neural dialogue management. arXiv preprint.</p>
<p>Sutton, R. S., and Barto, A. G. 1998. Reinforcement learning: An introduction. MIT press Cambridge.
Tesauro, G. 1995. Temporal difference learning and TD-Gammon. Communications of the ACM $58-68$.</p>
<p>Thrun, S., and Schwartz, A. 1993. Issues in using function approximation for reinforcement learning. In Proceedings of the 1993 Connectionist Models.</p>
<p>Van der Pol, E., and Oliehoek, F. A. 2016. Coordinated deep reinforcement learners for traffic light control. In In proceedings of NIPS.</p>
<p>Van Hasselt, H., and Wiering, M. A. 2009. Using continuous action spaces to solve discrete problems. In Neural Networks, 2009. IJCNN 2009. International Joint Conference on, 1149-1156. IEEE.</p>
<p>Watkins, C. J., and Dayan, P. 1992. Q-learning. Machine learning 8(3-4):279-292.
Wen, Z.; O’Neill, D.; and Maei, H. 2015. Optimal demand response using device-based reinforcement learning. IEEE Transactions on Smart Grid 2312-2324.</p>
<p>Wu, Y.; Schuster, M.; Chen, Z.; Le, Q. V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint.</p>
<p>Yuan, X.; Côté, M.-A.; Sordoni, A.; Laroche, R.; Combes, R. T. d.; Hausknecht, M.; and Trischler, A. 2018. Counting to explore and generalize in text-based games. arXiv preprint arXiv:1806.11525.</p>
<p>Zahavy, T.; Ben-Zrihem, N.; and Mannor, S. 2016. Graying the black box: Understanding dqns. In International Conference on Machine Learning, 1899-1908.</p>
<p>Zahavy, T.; Magnani, A.; Krishnan, A.; and Mannor, S. 2018. Is a picture worth a thousand words? a deep multi-modal fusion architecture for product classification in e-commerce. The Thirtieth Conference on Innovative Applications of Artificial Intelligence (IAAI).</p>
<p>Zelinka, M. 2018. Using reinforcement learning to learn how to play text-based games. arXiv preprint.</p>
<p>Zhao, T., and Eskenazi, M. 2016. Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning. arXiv preprint.</p>
<p>Zrihem, N. B.; Zahavy, T.; and Mannor, S. 2016. Visualizing dynamics: from t-sne to semi-mdps. arXiv preprint arXiv:1606.07112.</p>
<h1>Appendix A Proof of Proposition 1</h1>
<p>Proposition 1. Assume that all state action pairs $(s, a)$ are visited infinitely often unless eliminated according to $\bar{\theta}<em t-1="t-1">{t-1, a}^{T} x(s)-\sqrt{\beta</em>}(\tilde{\delta}) x(s)^{T} \bar{V<em a="a" s_="s,">{t-1, a}^{-1} x(s)}&gt;\ell$. Then, with a probability of at least $1-\delta$, action elimination $Q$-learning converges to the optimal $Q$-function for any valid state-action pairs. In addition, actions which should be eliminated are visited at most $T</em>+1$ times.}(t) \leq 4 \beta_{t} /(u-\ell)^{2</p>
<p>Proof. We start by proving the convergence of the algorithm and then prove the bound on the number of visits of invalid actions.
Denote the MDP as $M$. According to Equation 1, with probability of at least $1-\delta$, elimination by Equation 2 never eliminates a valid action, and thus all of these actions are visited infinitely often. If all of the state-action pairs are visited infinitely often even after the elimination, the Q-learning will converge at all state-action pairs. Otherwise, there are some invalid actions, which are strictly suboptimal, and are visited a finite number of times. In this case, there exists some time $T&lt;\infty$ such that all of these actions are never played for any $t&gt;T$. Define a new MDP $\tilde{M}$, as $M$ without any of the eliminated actions. As these actions are strictly suboptimal, the value of $\tilde{M}$ will be identical to the value of $M$ in all states, and so are the Q -values for any action that survived the elimination. Furthermore, $\tilde{M}$ contains all of the valid states, and their Q-values will be identical those of $M$, as they only depend on the reward in the valid state-action pairs and the value in the next state, both which exist in $\tilde{M}$. For any $t&gt;T, M$ is equivalent to $\tilde{M}$, and all of its state-actions are visited infinitely often. Therefore, the Q-function will converge to the optimal Q-function with probability 1 in all of $\tilde{M}$ 's state-action pairs. Specifically, it will converge in all of valid state-action pairs $(s, a)$, which concludes the first part of the proof.
We'll now prove the sample complexity of any invalid actions. First, note that the confidence bound is strongly related to the number of visits in a state-action pair:</p>
<p>$$
\begin{aligned}
x\left(s_{t}\right)^{T} \bar{V}<em t="t">{t-1, a}^{-1} x\left(s</em>\right) \
&amp; \stackrel{(1)}{\leq} x\left(s_{t}\right)^{T}\left{\lambda I+T_{s, a}(t-1) x\left(s_{t}\right) x\left(s_{t}\right)^{T}\right}^{-1} x\left(s_{t}\right) \
&amp; \stackrel{(2)}{=} \frac{\left|x\left(s_{t}\right)\right|^{2}}{\lambda}-\frac{T_{s, a}(t-1)^{\frac{\left|x\left(s_{t}\right)\right|^{4}}{\lambda^{2}}}}{1+T_{s, a}(t-1)^{\frac{\left|x\left(s_{t}\right)\right|^{2}}{\lambda}}} \
&amp; =\frac{\left|x\left(s_{t}\right)\right|^{2}}{\lambda+T_{s, a}(t-1)\left|x\left(s_{t}\right)\right|^{2}} \leq \frac{1}{T_{s, a}(t-1)}
\end{aligned}
$$}\right) &amp; =x\left(s_{t}\right)^{T}\left{\lambda I+T_{s, a}(t-1) x\left(s_{t}\right) x\left(s_{t}\right)^{T}+\sum_{s^{\prime} \neq s_{t}} T_{s^{\prime}, a}(t-1) x\left(s^{\prime}\right) x\left(s^{\prime}\right)^{T}\right}^{-1} x\left(s_{t</p>
<p>(1) is correct due to the fact that for any positive definite $A$ and positive semidefinite $B$, the difference $A^{-1}-(A+B)^{-1}$ is positive semidefinite. (2) is correct due to the Sherman-Morrison formula (Bartlett, 1951). We note that this bound is not tight because it does not use the correlations between different contexts. In fact, the same bound can be probably achieved by placing a multi-armed bandit algorithm in each state. Deriving a tighter bound that utilizes the correlation between contexts is hard, as it is possible to observe a state that its context is not correlated with other states' contexts. Nevertheless, the confidence bounds for contextual bandits can be used in the non-tabular case, in contrast to a MAB formulation.
This implies that a satisfactory condition for correct elimination is</p>
<p>$$
\begin{aligned}
x\left(s_{t}\right)^{T} \bar{\theta}<em t-1="t-1">{t-1, a}-\sqrt{\beta</em>}(\tilde{\delta}) x\left(s_{t}\right)^{T} \bar{V<em t="t">{t-1, a}^{-1} x\left(s</em> \
\stackrel{(1)}{\geq} u-2 \sqrt{\beta_{t-1}(\tilde{\delta}) x\left(s_{t}\right)^{T} \bar{V}}\right)<em t="t">{t-1, a}^{-1} x\left(s</em>&gt;\ell
\end{aligned}
$$}\right)} \stackrel{(2)}{\geq} u-2 \sqrt{\frac{\beta_{t-1}(\tilde{\delta})}{T_{s, a}(t-1)}</p>
<p>where (1) is correct due to Equation 2 with $\mathbb{E}\left[e\left(s_{t}, a\right)\right]=\theta_{a}^{* T} x\left(s_{t}\right) \geq u$, with probability $1-\delta$, and (2) is correct due to Equation 3. Therefore, if $T_{s, a}(t) \geq 4 \frac{\beta_{t}}{(u-\ell)^{2}}$ then action $a$ in state $s$ is correctly eliminated. We emphasize that the bound does not depend on the algorithm that chooses state-actions, except for the dependency of $\beta_{t}$, through $\hat{V}<em t="t">{t, a}$, in the history. Using the fact that $\beta</em>$ is monotonically increasing with $t$, with probability $1-\delta$, all of the invalid actions are sampled no more than</p>
<p>$$
\begin{aligned}
T_{s, a}(t) &amp; \leq \sum_{\tau=1}^{t} \mathbb{1}\left{T_{s, a}(\tau) \leq 4 \frac{\beta_{\tau}}{(u-\ell)^{2}}\right} \
&amp; \leq \sum_{\tau=1}^{t} \mathbb{1}\left{T_{s, a}(\tau) \leq 4 \frac{\beta_{t}}{(u-\ell)^{2}}\right} \leq 4 \frac{\beta_{t}}{(u-\ell)^{2}}+1
\end{aligned}
$$</p>
<p>If the sub-gaussianity parameter is $R=0$, we have $\beta_{t}=\lambda S^{2}&lt;\infty$, and therefore an arm will be sampled at most a finite number of times $T_{0}=4 \frac{\lambda S^{2}}{(u-\ell)^{2}}+1&lt;\infty$. Otherwise, if the state representations are bounded, i.e. $\forall s,|x(s)|<em t="t">{2} \leq L$, then, using the simpler form of $\beta</em>$, the bound can be written as $\lim <em a="a" s_="s,">{t \rightarrow \infty} \frac{T</em>$, which means an invalid action is sampled a logarithmic number of times.}(t)}{\log \left(\frac{1}{2}\right)} \leq \frac{4 R^{2} d}{(u-\ell)^{2}</p>
<h1>Appendix B Grid world simulations</h1>
<p>In this Section, we experimented with action elimination in a grid world domain with a tabular Q-learning algorithm. We start with the following default configuration (Figure 5). The grid size is 30x30, the number of state categories is $K=10$, and the maximal episode length is $T=150$. If the chosen action is from the same category as the current state, it is performed correctly in probability $p_{c}^{T}=0.75$, and if the state and action types are different, the probability is $p_{c}^{F}=0.5$. We also study the effect of the domain's parameters on the performance of action elimination, by changing these parameters one at a time.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: 30x30 Grid World - the agent starts at the center (green) and needs to navigate to the upper-left corner (blue) while avoiding walls (yellow).</p>
<p>On each of the simulations, the results were filtered by a moving average filter of length 200, which is needed due to the stochastic nature of the domain. The results are averaged over 5 random seeds, shown alongside error bars (std/3).
Since the problem is tabular, we use confidence intervals in the spirit of UCT (Kocsis and Szepesvári, 2006) - denote the empirical mean of the elimination signal by $\bar{e}(s, a)$ and the number of visits in a state-action pair by $N(s, a)$. An action will be eliminated if $\bar{e}(s, a)-\sqrt{\frac{2 \sum_{a} N(s, a)}{N(s, a)}}&gt;\ell \triangleq 0.5$. The Q-function was initially set to 0 , the learning rates were chosen according to (Even-Dar and Mansour, 2003), and we set $\gamma=1$.</p>
<p>We start with comparison between vanilla Q-learning without action elimination (green) and a tabular version of the action elimination Q-learning (blue) (Figure 6). We also include an "optimal elimination" baseline, i.e., a Q-learning agent with one category (red), i.e., only 4 basic navigation actions, which forms an upper bound on performance with multiple categories. In Figure 6(a),6(c), the episode length is $T=150$, and in Figure 6(b) it is $T=300$, to allow sufficient exploration for the vanilla Q-Learning.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Performance of agents in grid world.
We can see that action elimination significantly improves the Q-learning algorithm (blue) over the baseline (green). In Figure 6(b), we increased the number of state categories. We can see that in this case, the action elimination dramatically improves in comparison to the vanilla algorithms, since there are more invalid actions (compare Figure 6(a) with Figure 6(b)). Figure 7(a) present the simulation results with a 40x40 grid world. When compared to Figures 6(a),6(c), with grid of 30x30 and 20x20, respectively, we can conclude that action elimination becomes more effective as the grid size increases. Intuitively, it is relatively easy to reach the goal state on small grids, even if the action space is large, and therefore even random exploration will bring the agent to the goal quickly. From the moment the agent reaches the goal, its policy will be biased towards the goal's direction, and it becomes easier to distinguish between valid and invalid actions.</p>
<p>Next, we make a small modification in the domain and consider a random elimination signal, i.e., if the action does not fit the state category, an elimination signal will be equal 1 in probability $p_{e}^{F}$. If the action and state belong to the same category, the probability is $p_{e}^{T}$. Specifically, we let $p_{e}^{F}$ change between 1 to 0.6 in invalid actions, and $p_{e}^{T}$ between 0 to 0.4 in valid actions. Inspecting Figure 7(b), we observe that only when the elimination signal is almost completely random, the action elimination algorithm does not present superior performance.
Finally, Figure 7(c) present a scenario where valid actions has almost no randomness ( $p_{e}^{T}=0.9$ ), while invalid actions are almost completely random ( $p_{e}^{F}=0.1$ ). Thus, it is easier to identify the invalid actions, and specifically, understand that these actions are suboptimal. The results indeed show that while action elimination Q-learning converges faster than the vanilla Q-learning, the difference between the convergence rates is smaller.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Performance of agents in grid world.
In summary, the tabular simulations showed a significant improvement due to action elimination, especially when the action space is large, the optimal and suboptimal actions are hard to distinct, and the horizon required to reach the goal is large.</p>
<h1>Appendix C Additional figures</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Results in "Open Zork".</p>
<h1>Appendix D Maps of Zork</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: The world of Zork
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Subdomains of Zork; the Troll (green) and Egg (blue) Quests. Credit: S. Meretzky, The Strong National Museum of Play.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ The same amount of steps that were used in previous work on Zork (<em>Fulda et al., 2017; Kostka et al., 2017</em>). For completeness, we also report results for AE-DQN with 2M steps, where learning seemed to converge.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6248 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6248</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6248</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-556bfb22059245c3dd388e97867cd88aa9dd078d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/556bfb22059245c3dd388e97867cd88aa9dd078d" target="_blank">Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work trains Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation, and shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models.</p>
                <p><strong>Paper Abstract:</strong> Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6248.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6248.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prometheus-Vision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prometheus-Vision (13B) — VLM evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source 13B vision-language model fine-tuned on the Perception Collection to act as a VLM-as-a-judge, producing both a 1–5 numeric score and interpretable language feedback conditioned on user-defined, fine-grained rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual instruction following, visual question answering (VQA), image captioning</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Prometheus-Vision 13B (also evaluated at 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>9 undergraduate annotators (proficient in English) scored 45 hand-picked instances (15 each from LLAVA-Bench, VisIT-Bench, Perception-Bench). For feedback-quality pairwise comparisons, 135 feedbacks (3 candidates per instance: GPT-4V, GPT-4, Prometheus-Vision) were rated by the same annotators split into 3 groups using Label Studio.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson, Kendall-Tau, Spearman correlations between model scores and human scores; Pairwise Preference Win-rate for feedback quality; length-bias analysis (response length vs score); evaluator self-consistency (multiple inference samples for GPT-4V).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Prometheus-Vision shows substantially higher agreement with human judges than conventional automatic metrics and outperforms other open-source evaluators. On the 45-sample human-evaluation set (Table 7) Prometheus-Vision (13B) achieved Pearson ≈ 0.674 with human scores (compared to GPT-4V ≈ 0.771 and GPT-4 ≈ 0.734). Across larger multi-benchmark comparisons vs GPT-4V, Prometheus-Vision 13B produced high correlations with GPT-4V (e.g., Pearson ≈ 0.786 on LLAVA-Bench, 0.574 on VisIT-Bench, 0.832 on Perception-Bench in the reported tables) and ranked highest among open-source models on all 8 evaluated benchmarks; the paper also reports that Prometheus-Vision's feedback was judged by annotators to be as good as or better than GPT-4V's feedback 57.78% of the time and as good as or better than GPT-4's 45.93% of the time.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Struggles on text-rich images (graphs, charts, diagrams) leading to lower correlation on VisIT-Bench; reduced effectiveness on short-answer VQA-style tasks (lower correlation on VQA benchmarks) because training data emphasizes longer free-form responses; potential indirect bias from training data augmentation using GPT-4V and from using LLaVA-1.5 as backbone (visual encoder limitations); does not handle AI-generated images (not evaluated); performance depends on the diversity of Perception Collection (currently skewed toward real-world images).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Lower correlation with humans / GPT-4V on VisIT-Bench (text-rich images) relative to other benchmarks; lower correlation on VQA benchmarks (short answers) attributed to mismatch between training response length and evaluation target; though not showing a length bias experimentally, it still gives highest scores to GPT-4V responses (uncertain whether due to GPT-4V-augmented training or true quality differences). Conventional string-similarity metrics diverge widely from human judgments where Prometheus-Vision aligns better with humans, highlighting cases where similarity metrics rate adequate, image-aware responses poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Increase training data diversity to include more text-rich images and short-answer examples; adopt stronger visual encoders / better VLM backbones (use recent architectural advances for text-rich image understanding); augment Perception Collection with AI-generated images for broader coverage; analyze and reduce training-source biases (e.g., mitigate over-reliance on GPT-4V-generated examples); leverage chain-of-thought-style rationale fine-tuning as done here and continue to refine scoring prompt templates (e.g., explicit separators like 'So the overall score is') to reduce inference degeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6248.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6248.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4V</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4V (GPT-4 with vision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A closed-source multimodal (vision+language) model from OpenAI used in this paper as a high-quality reference evaluator; frequently used as the comparison ceiling for VLM evaluation due to its strong image-grounded reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Visual instruction following, visual question answering, image captioning (used as a reference judge across these domains)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4V Preview (sampled multiple times for self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>GPT-4V scores were compared against human annotators (same 45-sample human-evaluation setup) and were sampled multiple times (the paper reports multiple inference samples—GPT-4V sampled up to 6 times in some experiments—to assess self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson, Kendall-Tau, Spearman correlations vs human scores and vs other evaluator models; pairwise feedback preference judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-4V achieved the highest correlation with human scores in the reported 45-sample set (Table 7 Pearson ≈ 0.771) and generally shows high agreement with humans across real-world image benchmarks (e.g., high correlations on LLAVA-Bench and Perception-Bench). However, on VisIT-Bench (text-rich images), GPT-4 (LM-only with caption input) sometimes matched or slightly outperformed GPT-4V, indicating cases where pure-text processing can help when images are highly text-centric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Closed-source nature limits transparency and reproducibility of evaluations; costs and scaling (inference cost) limit wide adoption for large-scale automatic evaluation; shows variability across inference samples (necessitating self-consistency sampling); in some text-rich-image cases GPT-4 (LM) with a caption may outperform GPT-4V, indicating no universal superiority across all image types.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Relative weakness on VisIT-Bench vs GPT-4 (LM) for text-rich images where accurate reading/processing of dense on-image text matters; self-consistency variance across repeated samples necessitates multiple queries to stabilize judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use repeated sampling to estimate self-consistency; complement closed-source GPT-4V judgments with open-source VLM evaluators (e.g., Prometheus-Vision) for transparent analysis; when evaluating text-rich images, include specialized pipelines or stronger OCR-capable visual encoders; provide fine-grained rubrics and reference answers to guide evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6248.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6248.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-as-a-Judge (multistage pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model as a Judge (image→text captioning then LM evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The prior paradigm of evaluating VLM outputs by first converting the image into text (caption) and then feeding that caption plus the response into a language-model-only judge; used in related prior work and as a baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General multi-modal evaluation (image-conditioned response evaluation across instruction following, VQA, captioning)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LM-only judges such as GPT-4 (LM), GPT-3.5-Turbo, Prometheus (LM variants); used in conjunction with an image-to-text step (e.g., LLaVA-1.5 captioning) to provide the LM with visual context.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>In this paper LM-judge baselines were run by first prompting LLaVA-1.5 to generate a caption for the image and supplying that caption to LM evaluators; comparisons to human scores and to VLM evaluators used the same correlation metrics and human annotation setup described above.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson, Kendall-Tau, Spearman correlations against human scores and GPT-4V; comparisons of feedback quality and score consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>LM-as-a-judge approaches lack direct visual input and therefore can suffer degraded alignment with human judgments especially when image understanding nuances are required; some LM judges perform better on text-rich images because processing textual captions can be sufficient, but overall LM-only judges often show lower correlation with human scores than direct VLM evaluators trained for fine-grained multi-modal evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Cannot directly consume image pixels; requires an intermediate caption which introduces error propagation and can miss visual details; needs multiple inference calls (captioning + LM evaluation), increasing cost and latency; tends to be less sensitive to fine-grained, image-grounded criteria unless captions are very precise.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Lower score correlation with humans and GPT-4V in many evaluated benchmarks when compared to VLM-as-a-judge (Prometheus-Vision); failure to capture image-grounded errors that are not reflected in the caption; worse performance on open-ended, long-form, and fine-grained rubrics that require precise grounding to visual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Prefer direct VLM-as-a-judge architectures when possible to remove the captioning bottleneck; if LM-as-judge is used, improve captioning fidelity (better VLM captioners/OCR), ensemble multiple captions, or pass richer multi-turn visual descriptions; fine-tune LM evaluators with image-grounded synthetic data or paired caption+image supervision to reduce error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prometheus: Inducing fine-grained evaluation capability in language models <em>(Rating: 2)</em></li>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Touchstone: Evaluating vision-language models by language models <em>(Rating: 2)</em></li>
                <li>Finegrained human feedback gives better rewards for language model training <em>(Rating: 1)</em></li>
                <li>AlpacaEval: An automatic evaluator of instruction-following models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6248",
    "paper_id": "paper-556bfb22059245c3dd388e97867cd88aa9dd078d",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "Prometheus-Vision",
            "name_full": "Prometheus-Vision (13B) — VLM evaluator",
            "brief_description": "An open-source 13B vision-language model fine-tuned on the Perception Collection to act as a VLM-as-a-judge, producing both a 1–5 numeric score and interpretable language feedback conditioned on user-defined, fine-grained rubrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Visual instruction following, visual question answering (VQA), image captioning",
            "llm_judge_model": "Prometheus-Vision 13B (also evaluated at 7B)",
            "human_evaluation_setup": "9 undergraduate annotators (proficient in English) scored 45 hand-picked instances (15 each from LLAVA-Bench, VisIT-Bench, Perception-Bench). For feedback-quality pairwise comparisons, 135 feedbacks (3 candidates per instance: GPT-4V, GPT-4, Prometheus-Vision) were rated by the same annotators split into 3 groups using Label Studio.",
            "metrics_compared": "Pearson, Kendall-Tau, Spearman correlations between model scores and human scores; Pairwise Preference Win-rate for feedback quality; length-bias analysis (response length vs score); evaluator self-consistency (multiple inference samples for GPT-4V).",
            "reported_differences": "Prometheus-Vision shows substantially higher agreement with human judges than conventional automatic metrics and outperforms other open-source evaluators. On the 45-sample human-evaluation set (Table 7) Prometheus-Vision (13B) achieved Pearson ≈ 0.674 with human scores (compared to GPT-4V ≈ 0.771 and GPT-4 ≈ 0.734). Across larger multi-benchmark comparisons vs GPT-4V, Prometheus-Vision 13B produced high correlations with GPT-4V (e.g., Pearson ≈ 0.786 on LLAVA-Bench, 0.574 on VisIT-Bench, 0.832 on Perception-Bench in the reported tables) and ranked highest among open-source models on all 8 evaluated benchmarks; the paper also reports that Prometheus-Vision's feedback was judged by annotators to be as good as or better than GPT-4V's feedback 57.78% of the time and as good as or better than GPT-4's 45.93% of the time.",
            "llm_specific_limitations": "Struggles on text-rich images (graphs, charts, diagrams) leading to lower correlation on VisIT-Bench; reduced effectiveness on short-answer VQA-style tasks (lower correlation on VQA benchmarks) because training data emphasizes longer free-form responses; potential indirect bias from training data augmentation using GPT-4V and from using LLaVA-1.5 as backbone (visual encoder limitations); does not handle AI-generated images (not evaluated); performance depends on the diversity of Perception Collection (currently skewed toward real-world images).",
            "notable_failure_cases": "Lower correlation with humans / GPT-4V on VisIT-Bench (text-rich images) relative to other benchmarks; lower correlation on VQA benchmarks (short answers) attributed to mismatch between training response length and evaluation target; though not showing a length bias experimentally, it still gives highest scores to GPT-4V responses (uncertain whether due to GPT-4V-augmented training or true quality differences). Conventional string-similarity metrics diverge widely from human judgments where Prometheus-Vision aligns better with humans, highlighting cases where similarity metrics rate adequate, image-aware responses poorly.",
            "mitigation_strategies": "Increase training data diversity to include more text-rich images and short-answer examples; adopt stronger visual encoders / better VLM backbones (use recent architectural advances for text-rich image understanding); augment Perception Collection with AI-generated images for broader coverage; analyze and reduce training-source biases (e.g., mitigate over-reliance on GPT-4V-generated examples); leverage chain-of-thought-style rationale fine-tuning as done here and continue to refine scoring prompt templates (e.g., explicit separators like 'So the overall score is') to reduce inference degeneration.",
            "uuid": "e6248.0",
            "source_info": {
                "paper_title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4V",
            "name_full": "GPT-4V (GPT-4 with vision)",
            "brief_description": "A closed-source multimodal (vision+language) model from OpenAI used in this paper as a high-quality reference evaluator; frequently used as the comparison ceiling for VLM evaluation due to its strong image-grounded reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Visual instruction following, visual question answering, image captioning (used as a reference judge across these domains)",
            "llm_judge_model": "GPT-4V Preview (sampled multiple times for self-consistency)",
            "human_evaluation_setup": "GPT-4V scores were compared against human annotators (same 45-sample human-evaluation setup) and were sampled multiple times (the paper reports multiple inference samples—GPT-4V sampled up to 6 times in some experiments—to assess self-consistency).",
            "metrics_compared": "Pearson, Kendall-Tau, Spearman correlations vs human scores and vs other evaluator models; pairwise feedback preference judgments.",
            "reported_differences": "GPT-4V achieved the highest correlation with human scores in the reported 45-sample set (Table 7 Pearson ≈ 0.771) and generally shows high agreement with humans across real-world image benchmarks (e.g., high correlations on LLAVA-Bench and Perception-Bench). However, on VisIT-Bench (text-rich images), GPT-4 (LM-only with caption input) sometimes matched or slightly outperformed GPT-4V, indicating cases where pure-text processing can help when images are highly text-centric.",
            "llm_specific_limitations": "Closed-source nature limits transparency and reproducibility of evaluations; costs and scaling (inference cost) limit wide adoption for large-scale automatic evaluation; shows variability across inference samples (necessitating self-consistency sampling); in some text-rich-image cases GPT-4 (LM) with a caption may outperform GPT-4V, indicating no universal superiority across all image types.",
            "notable_failure_cases": "Relative weakness on VisIT-Bench vs GPT-4 (LM) for text-rich images where accurate reading/processing of dense on-image text matters; self-consistency variance across repeated samples necessitates multiple queries to stabilize judgments.",
            "mitigation_strategies": "Use repeated sampling to estimate self-consistency; complement closed-source GPT-4V judgments with open-source VLM evaluators (e.g., Prometheus-Vision) for transparent analysis; when evaluating text-rich images, include specialized pipelines or stronger OCR-capable visual encoders; provide fine-grained rubrics and reference answers to guide evaluation.",
            "uuid": "e6248.1",
            "source_info": {
                "paper_title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LM-as-a-Judge (multistage pipeline)",
            "name_full": "Language Model as a Judge (image→text captioning then LM evaluation)",
            "brief_description": "The prior paradigm of evaluating VLM outputs by first converting the image into text (caption) and then feeding that caption plus the response into a language-model-only judge; used in related prior work and as a baseline in this paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "General multi-modal evaluation (image-conditioned response evaluation across instruction following, VQA, captioning)",
            "llm_judge_model": "LM-only judges such as GPT-4 (LM), GPT-3.5-Turbo, Prometheus (LM variants); used in conjunction with an image-to-text step (e.g., LLaVA-1.5 captioning) to provide the LM with visual context.",
            "human_evaluation_setup": "In this paper LM-judge baselines were run by first prompting LLaVA-1.5 to generate a caption for the image and supplying that caption to LM evaluators; comparisons to human scores and to VLM evaluators used the same correlation metrics and human annotation setup described above.",
            "metrics_compared": "Pearson, Kendall-Tau, Spearman correlations against human scores and GPT-4V; comparisons of feedback quality and score consistency.",
            "reported_differences": "LM-as-a-judge approaches lack direct visual input and therefore can suffer degraded alignment with human judgments especially when image understanding nuances are required; some LM judges perform better on text-rich images because processing textual captions can be sufficient, but overall LM-only judges often show lower correlation with human scores than direct VLM evaluators trained for fine-grained multi-modal evaluation.",
            "llm_specific_limitations": "Cannot directly consume image pixels; requires an intermediate caption which introduces error propagation and can miss visual details; needs multiple inference calls (captioning + LM evaluation), increasing cost and latency; tends to be less sensitive to fine-grained, image-grounded criteria unless captions are very precise.",
            "notable_failure_cases": "Lower score correlation with humans and GPT-4V in many evaluated benchmarks when compared to VLM-as-a-judge (Prometheus-Vision); failure to capture image-grounded errors that are not reflected in the caption; worse performance on open-ended, long-form, and fine-grained rubrics that require precise grounding to visual evidence.",
            "mitigation_strategies": "Prefer direct VLM-as-a-judge architectures when possible to remove the captioning bottleneck; if LM-as-judge is used, improve captioning fidelity (better VLM captioners/OCR), ensemble multiple captions, or pass richer multi-turn visual descriptions; fine-tune LM evaluators with image-grounded synthetic data or paired caption+image supervision to reduce error propagation.",
            "uuid": "e6248.2",
            "source_info": {
                "paper_title": "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prometheus: Inducing fine-grained evaluation capability in language models",
            "rating": 2
        },
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Touchstone: Evaluating vision-language models by language models",
            "rating": 2
        },
        {
            "paper_title": "Finegrained human feedback gives better rewards for language model training",
            "rating": 1
        },
        {
            "paper_title": "AlpacaEval: An automatic evaluator of instruction-following models",
            "rating": 1
        }
    ],
    "cost": 0.015948,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prometheus-Vision: <br> Vision-Language Model as a Judge for Fine-Grained Evaluation</h1>
<p>Seongyun Lee ${ }^{1 <em>}$ Seungone Kim ${ }^{1,2 </em>}$ Sue Hyun Park ${ }^{1}$ Geewook Kim ${ }^{1,3}$ Minjoon Seo ${ }^{1}$<br>KAIST AI ${ }^{1}$ NAVER AI Lab ${ }^{2}$ NAVER Cloud AI ${ }^{3}$<br>{seongyun, seungone, suehyunpark, geewook, minjoon}@kaist.ac.kr</p>
<h4>Abstract</h4>
<p>Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15 K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https:// github.com/kaistAI/prometheus-vision.</p>
<h2>1 Introduction</h2>
<p>While recently developed Vision-Language Models (VLMs) are capable of generating long-form text from a combination of an image and instruction, assessing the quality of the output remains a significant challenge (Liu et al., 2023a; Dai et al., 2023; Gao et al., 2023; Ye et al., 2023a; Zhu et al., 2023a; OpenAI, 2023). Traditional metrics, which rely on text-based exact matches or edit distances, fall short in adhering to the granular evaluation criterion of interest and capturing the rich context within the outputs (Agrawal et al., 2023; Mañas et al., 2023; Bai et al., 2023). For instance, as shown in Figure 1, conventional metrics fail to explain what is missing within the response compared to the answer.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Consequently, the role of high-quality human evaluations remains pivotal for a comprehensive assessment. However, human evaluators are prone to biases, and scaling up is expensive in terms of time and cost (Ye et al., 2023c; Kim et al., 2023b).</p>
<p>To address the need for flexible and automatic text evaluation, the 'LM-as-a-Judge' paradigm proposes using language models (LMs) as evaluators, where initial findings suggest its potential to emulate human judgement (Liu et al., 2023a; Zheng et al., 2023; Li et al., 2023; Ye et al., 2023c; Kim et al., 2023d; Zhu et al., 2023b; Bai et al., 2023). However, LMs cannot perceive visual contexts, which necessitates an additional model that could convert the image to text. As a result, such a multistage pipeline could potentially suffer from error propagation and also require multiple inference calls. This situation calls for the direct utilization of VLMs, referred to as VLM-as-a-Judge.</p>
<p>On the other hand, despite GPT-4V's (OpenAI, 2023) potential as an evaluator, its closed-source nature limits transparent evaluation (Kim et al., 2023d). Moreover, our initial tests indicate that open-source VLMs are not effective as evaluators sensitive to granular aspects, demonstrating a low score correlation with both human evaluators and GPT-4V. To address these challenges, we propose Prometheus-Vision, a 13B VLM evaluator that excels at assessing based on fine-grained criteria. As shown in Figure 1, PrometheusVision could evaluate based on the given criteria, pinpointing the differences between the parody artwork and the original masterpiece.</p>
<p>To develop Prometheus-Vision, we construct the Perception Collection, the first multi-modal feedback dataset that includes 15 K fine-grained score rubrics, thus going beyond traditional coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Using the Perception Collection, we fine-tune LLaVA-1.5 to create Prometheus-Vision.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Conventional metrics measure the similarity between the response and ground-truth answer, which is not expressive enough. Moreover, it could not pinpoint what is missing within the response with respect to the evaluation criteria. In contrast, the VLM-as-a-Judge pipeline provides not only the flexibility to adhere to arbitrary evaluation criteria but also provides detailed language feedback that specifically pinpoints the deficiencies.</p>
<p>We achieve our objective by fine-tuning a VLM based on fine-grained criteria with reference materials (e.g., reference answer, score rubric) appended. The resulting evaluator VLM, PrometheusVision, shows a high correlation with both human evaluators and GPT-4V, serving as a testament to its efficiency. The proposed system exhibits a Pearson correlation of 0.786 with human evaluators and 0.639 with GPT-4V on the LLaVA-Bench. These results highlight its potential to serve as an inexpensive yet effective open-source alternative to GPT4 V evaluation. Moreover, when assessing 3,560 instances across 8 benchmarks, PrometheusVision shows the highest correlation with GPT-4V on all the 8 benchmarks among the open-source models and even outperforms GPT-4 (LM judge) on 5 benchmarks.</p>
<p>Our contributions are summarized as follows:</p>
<ul>
<li>We introduce Perception Collection, the first multi-modal feedback dataset that could be used to train an evaluator VLM. In contrast to existing multi-modal feedback, critique, and preference datasets that use coarsegrained criteria, the Perception CollecTION includes 15 K fine-grained criteria that determine the crucial aspect for each instance.</li>
<li>We introduce Prometheus-Vision, the first open-source VLM specialized for evaluation purposes. Prometheus-Vision shows a high correlation with both GPT-4V and human evaluators, indicating its potential to be used as a cheap alternative for GPT-4V evaluation.</li>
</ul>
<h2>2 Related works</h2>
<h3>2.1 Evaluating Vision Language Models</h3>
<p>In prior works, Vision-Language Models (VLMs) are typically evaluated using specific metrics tailored to each task. For image captioning, performance is measured with metrics like BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015), focusing on how well the generated text aligns with reference captions. Similarly, Visual Question Answering (VQA) is evaluated using accuracy metrics based on the exact match between the model's answers and human-annotated answers (Agrawal et al., 2023; Mañas et al., 2023).</p>
<p>However, traditional metrics often fall short of capturing the nuanced details of the response generated by VLMs in complex or subjective situations. A more comprehensive approach has been human evaluation, accounting for contextual and creative aspects not captured by automated metrics. Nonetheless, cost and consistency constraints associated with human evaluations render it a less feasible method for scaling to a lot of instances.</p>
<h3>2.2 Language Model as a Judge for Fine-grained Evaluation</h3>
<p>The difficulty in evaluating long-form responses often arises from the ambiguity in defining what constitutes a good output. For instance, discerning whether a given response is helpful or harmless is often subjective. Recent works have proposed the concept of 'Fine-grained Evaluation', utilizing LM-as-a-judge for assessing granular aspects. Ye et al. (2023c) defines 12 core skill sets that are crucial for evaluating LMs. Kim et al. (2023d) further extends this concept and employs thousands of fine-grained criteria to assess LMs on user-defined criteria. Wu et al. (2023) and Jang et al. (2023) utilize fine-grained criteria to align LMs. Lastly, Kim et al. (2023e) proposes an interactive framework in which users could test LMs on fine-grained criteria.</p>
<p>To the best of our knowledge, we are first to expand the notion of 'Fine-grained Evaluation' for assessing VLMs. Specifically, recent work has proposed to evaluate VLMs using LMs or VLMs (Bai et al., 2023; Ge et al., 2023), yet are still confined to high-level coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. We construct the Perception Collection which encompasses 15 K of fine-grained criteria and use it to train Prometheus-Vision.</p>
<h2>3 The Perception Collection</h2>
<p>In contrast to the language domain, to the best of our knowledge, there do not exist any available feedback, critique, or preference datasets applicable to train an evaluator VLM that could assess in a fine-grained manner. For this purpose, we first construct a comprehensive multi-modal feedback dataset called the Perception Collection.</p>
<p>As shown in Figure 2, each instance in the PerCEPTION COLLECTION consists of five input components (image, instruction, response to evaluate, customized score rubric, reference answer) and two output components (language feedback and score decision). The number of each component in the Perception Collection is shown in Table 1.</p>
<p>Specifically, the five input components are:</p>
<ul>
<li>Image: A real-world image that the user would provide to the VLM.</li>
<li>Instruction: A text instruction that the user would prompt the VLM. It is also related to the provided image.</li>
<li>Response to Evaluate: A text response that the VLM would generate based on the image and instruction. The evaluator VLM has to assess this response.</li>
<li>Customized Score Rubric: A detailed scoring criteria that the VLM should refer to for assessment. We use fine-grained criteria in contrast to coarse-grained ones such as helpfulness, relevance, accuracy, and comprehensiveness. The rubric consists of (1) a description of the criteria and (2) a description of each scoring decision on a scale of 1 to 5.</li>
<li>Reference Answer: A reference answer that would achieve a score of 5 . While this component could be hand-crafted by human annotators, in our experiments, we utilize GPT-4V.</li>
</ul>
<p>Moreover, the two output components are:</p>
<ul>
<li>Feedback: A rationale pinpointing what is good and bad about the response under assessment. Instead of directly providing a scoring decision, this component makes the judgement process more interpretable.</li>
<li>Score: An integer value on a scale of 1 to 5 that represents the quality of the response given the criteria mentioned in the score rubric.</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Previous automatic metrics could not capture whether a VLM's response is aware of <em>aesthetic harmony</em>. With Prometheus-Vision, users could define customized score rubrics that they care about instead of assessing based on coarse-grained criteria such as helpfulness, relevance, accuracy, and comprehensiveness. Each component within the Perception Collection consists of 5 input components: an instruction, a real-world image, a response to evaluate, a customized score rubric, and a reference answer. Based on this, Prometheus-Vision is trained to generate a language feedback and a score decision.</p>
<table>
<thead>
<tr>
<th>Components</th>
<th># Components</th>
<th># Components per Image</th>
</tr>
</thead>
<tbody>
<tr>
<td>Images</td>
<td>5,000</td>
<td>1</td>
</tr>
<tr>
<td>Score Rubrics</td>
<td>15,000</td>
<td>3</td>
</tr>
<tr>
<td>Instructions</td>
<td>30,000</td>
<td>6</td>
</tr>
<tr>
<td>Reference Answers</td>
<td>30,000</td>
<td>6</td>
</tr>
<tr>
<td>Responses</td>
<td>150,000</td>
<td>30</td>
</tr>
<tr>
<td>Feedback &amp; Score</td>
<td>150,000</td>
<td>30</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of each component included in the Perception Collection. Note that the feedback and score are evenly distributed, leading to 30K instances per score between 1 and 5.</p>
<h3>3.1 Perception Collection Construction</h3>
<p>We construct a multi-modal feedback dataset called the Perception Collection. We mainly follow the construction process of [kim2023revisiting]. While creating the Perception Collection, we utilize 5K real-world images sampled from MS COCO 2017 Challenge [lin2014microsoft] and the MMMU benchmark [yue2023mmu].</p>
<p>Concretely, the augmentation process consists of 4 stages: (1) hand-crafting 50 seed score rubrics, (2) brainstorming 15K fine-grained score rubrics, (3) augmenting 30K instructions and reference answers closely tied with the rubric, and (4) augmenting 150K responses and language feedback. We include a detailed analysis of the Perception-Collection in terms of diversity and quality in Appendix A and all the prompts used for augmentation in Appendix F.</p>
<p>Step 1: Hand-Crafting Score Rubrics We first start by writing 50 examples of fine-grained score rubrics that go beyond the coarse-grained counterparts. For 50 images, we write an instruction and the corresponding rubric that pinpoints which aspect to consider during the assessment.</p>
<p>Step 2: Brainstorming Score Rubrics Using GPT-4V, we expand the number of our score rubrics from 50 to 15K. Using an arbitrary image among the 5K pool and the 50 examples as demonstrations, we prompt GPT-4V to generate 3 variants for each image. To ensure quality, we go through an additional stage of prompting GPT-4V to inspect</p>
<p>whether the generated score rubric aligns with the image. If it does not, we iteratively prompt it again until we acquire 3 candidates per image.</p>
<p>Step 3: Augmenting Instructions and Reference Answers related to the Score Rubric Afterwards, we use the 15K score rubrics and prompt GPT-4V to generate 2 novel instructions for each score rubric, leading to a total number of 30K. This process ensures that the instruction is closely tied to the score rubric since the instruction was conditioned on the score rubric.</p>
<p>Step 4: Augmenting Training Instances Lastly, we augment the remaining components which are the response to evaluate, feedback, and scoring decision. We use the score rubric and instruction generated from the previous stages and prompt GPT4 V to write a response that would get a score of $i$ $(1 \leq i \leq 5)$. Importantly, we ensured that there is no length bias (i.e., giving a higher score for longer responses) and included an analysis at Section C. This leads to a total number of 150 K responses and 150 K feedback where each score within between 1 and 5 has an even number of 30 K instances.</p>
<p>We include our analysis of the Perception Collection in terms of its quality, diversity, and whether there is a length bias among score decisions at Appendix A.</p>
<h3>3.2 Fine-tuning a VLM as an Evaluator</h3>
<p>Using the Perception Collection, we use LLaVA-1.5 (7B \&amp; 13B) (Liu et al., 2023a) as our backbone model and train Prometheus-Vision (7B \&amp; 13B). Training on the Prometheus Collection is analogous to Chain-of-Thought finetuning which requires generating a rationale (which is the feedback in our case) and then the score in a sequential manner (Ho et al., 2022; Kim et al., 2023c). We include a fixed phrase 'So the overall score is' in between the feedback and the score which we found to prevent degeneration during inference. The detailed hyper-parameters used during training are included in Appendix D.1.</p>
<h2>4 Experimental Settings</h2>
<h3>4.1 Protocol for Evaluating Evaluator VLMs</h3>
<p>In this section, we explain our experimental setting used to assess the fine-grained judgement capabilities of evaluator VLMs. As it is a non-trivial problem to directly measure 'How well a VLM is evaluating', we indirectly compare with two different
standards: (1) how closely Prometheus-Vision could simulate human evaluators (Section 5.1) and (2) how closely Prometheus-Vision could simulate the best VLM, which is GPT-4V, for nuanced assessment purposes (Section 5.2).</p>
<h3>4.2 Evaluator VLM \&amp; LM Baselines</h3>
<p>We employ 9 VLMs as our evaluator VLM baselines, namely LLAVA-1.5 (7B \&amp; 13B) (Liu et al., 2023a); LLAVA-RLHF (7B \&amp; 13B) (Sun et al., 2023); ShareGPT4V (7B) (Chen et al., 2023); Fuyu (8B) (Bavishi et al., 2023); and GPT4V (OpenAI, 2023) along with PrometheusVISION (7B \&amp; 13B).</p>
<p>In addition, we also compare with using LMs as a judge for evaluating VLMs as in previous work (Bai et al., 2023). We add 4 LMs as our evaluator LM baselines, namely Prometheus (7B \&amp; 13B) (Kim et al., 2023d); GPT-3.5-Turbo (OpenAI, 2022); and GPT-4 (OpenAI, 2023). Since LMs could not receive images as input, we prompt LLaVA-1.5 to generate a caption for the given image and provide the caption as additional input for LM evaluators. In contrast, for VLM evaluator baselines, we directly provide the image as input. The hyper-parameters used to inference evaluator LMs and evaluator VLMs are included in Appendix D.1.</p>
<h3>4.3 Response VLMs</h3>
<p>During our experiments, we utilize 3 different VLMs to sample the outputs that our VLM evaluators would assess. We denote these 3 VLMs as 'Response VLMs'. We utilize Fuyu (8B), LLAVA1.5 (13B), and GPT-4V as our response VLM. The hyper-parameters used to inference response VLMs are included in Appendix D.1.</p>
<h3>4.4 Benchmarks</h3>
<p>Our evaluation benchmarks are mainly divided into 3 categories:</p>
<ul>
<li>Visual Instruction Following Benchmarks: Tasks that require to write a long-form text output given an image and a text instruction. We use LLaVA-Bench (Liu et al., 2023a), VisITBench (Bitton et al., 2023), and a held-out test set of the Perception Collection called the Perception Bench.</li>
<li>Visual Question Answering Benchmarks: Tasks that require to write a text output given</li>
</ul>
<table>
<thead>
<tr>
<th>Benchmarks</th>
<th># Instances</th>
<th># Score Rubrics</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLAVA-Bench</td>
<td>15</td>
<td>15 (Hand-crafted)</td>
</tr>
<tr>
<td>VisIT-Bench</td>
<td>15</td>
<td>15 (Hand-crafted)</td>
</tr>
<tr>
<td>Perception-Bench</td>
<td>15</td>
<td>15 (Hand-crafted)</td>
</tr>
<tr>
<td>Total</td>
<td>45</td>
<td>45</td>
</tr>
</tbody>
</table>
<p>Table 2: The number of the instances and score rubrics included in our evaluation setting in Section 5.1. We randomly sample 15 instances from each benchmark and hand-craft a instance-wise fine-grained score rubric. Each instance originally has an image and an instruction.</p>
<table>
<thead>
<tr>
<th>Benchmarks</th>
<th># Instances</th>
<th># Score Rubrics</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLAVA-Bench</td>
<td>60</td>
<td>60 (Machine-generated)</td>
</tr>
<tr>
<td>VisIT-Bench</td>
<td>500</td>
<td>500 (Machine-generated)</td>
</tr>
<tr>
<td>Perception-Bench</td>
<td>500</td>
<td>500 (Machine-generated)</td>
</tr>
<tr>
<td>OKVQA</td>
<td>500</td>
<td>5 (Machine-generated)</td>
</tr>
<tr>
<td>VQAv2</td>
<td>500</td>
<td>5 (Machine-generated)</td>
</tr>
<tr>
<td>TextVQA</td>
<td>500</td>
<td>5 (Machine-generated)</td>
</tr>
<tr>
<td>COCO-Captions</td>
<td>500</td>
<td>5 (Machine-generated)</td>
</tr>
<tr>
<td>No-Caps</td>
<td>500</td>
<td>5 (Machine-generated)</td>
</tr>
<tr>
<td>Total</td>
<td>3560</td>
<td>1085</td>
</tr>
</tbody>
</table>
<p>Table 3: The number of the instances and score rubrics included in our evaluation setting in Section 5.2. Except for LLaVA-Bench, we randomly sample 500 instances from each benchmark. Each instance originally has an image and an instruction. We additionally add a fine-grained score rubric and reference answer by prompting GPT-4V as explained in Section 3.1.
an image and a text question. Compared to instruction following benchmarks, one notable difference is that we use the short-form answers originated from each dataset as reference answers in the input. We use the test set of the OKVQA dataset <em>Marino et al. (2019)</em>, VQAv2 dataset <em>Goyal et al. (2017)</em>, and TextVQA dataset <em>Singh et al. (2019)</em>.</p>
<ul>
<li>Captioning Benchmarks: Tasks that require to write a text caption of the given image. Similar to the visual question answering benchmarks, the ground truth answers tend to be short compared to the reference responses in the instruction following benchmarks. We use the test set of the COCO-Captions dataset <em>Chen et al. (2015)</em> and No-Caps dataset <em>Agrawal et al. (2019)</em>.</li>
</ul>
<p>The number of instances and score rubrics for each benchmark is shown in Table 2 and Table 3. Note that while the datasets in the VQA and captioning benchmarks originally have ground-truth answers, the instruction following benchmarks inherently does not have a reference answer. Using the same augmentation process mentioned in Section 3.1, we augment a reference answer and a fine-grained score rubric for each instance within the LLaVA-Bench, VisIT-Bench, and Perception-Bench. For the Perception-Bench, which is our held-out test set, we also generate new instructions. For the VQA and captioning benchmarks, we generate 5 score rubrics with the original ground-truth answer in consideration. The authors manually checked the quality of the added components.</p>
<h3>4.5 Setups \&amp; Metrics</h3>
<p>Our evaluation setup is divided into 2 parts.
Setup #1 (Table 2) In Section 5.1, we utilize 45 instances with instance-wise hand-crafted score rubrics ( 15 instances each for LLAVA-BENCH, VisIT-Bench, and Perception-Bench). We ask 9 human annotators proficient in English to provide a scoring decision as Prometheus-Vision. Then, we measure the correlation of the scoring decision by employing Pearson, Kendall-Tau, and Spearman as our metrics. Next, we ask human annotators to compare 2 language feedbacks that are sampled from either GPT-4, GPT-4V, or Prometheus-Vision (13B) and choose which one is better. Then, we measure the Pairwise Preference Win-rate between the 3 candidates. Details of the annotation setting are explained in Appendix D.2.</p>
<p>Setup #2 (Table 3) In Section 5.2, we expand the number of instances and utilize 1,085 fine-grained score rubrics tied across 3,560 instances in total. In this setting, we prompt GPT-4V three times and compare the correlation of the scoring decision by also prompting evaluator VLMs and evaluator LMs three times. As Setup #1, we use Pearson, Kendall-Tau, and Spearman as our metrics.</p>
<h2>5 Experimental Results</h2>
<h3>5.1 Can Prometheus-Vision Closely Simulate Human Evaluators?</h3>
<p>In this subsection, to verify whether PrometheusVision can emulate human evaluators, we measure the correlation between scores annotated by humans and those predicted by evaluator VLMs. The overall results are shown in Figure 3.</p>
<h3>5.1.1 Correlation with Human Evaluators</h3>
<p>Our Prometheus-Vision 13B notably mirrors the high correlation exhibited by leading models</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Pearson Correlation between score decisions from human evaluators and score decisions from either GPT-4V, GPT-4, GPT-3.5-Turbo, Prometheus-13B and Prometheus-Vision-13B on 45 customized score rubrics from LLAVA-Bench, VisIT-Bench, and Perception-Bench. Prometheus-Vision shows a high correlation with human evaluators on instances with real-world images.</p>
<p>GPT-4 and GPT-4V on the LLAVA-Bench and Perception-Bench, achieving correlations of 0.639 and 0.870, respectively. Despite this, although our Prometheus-Vision outperforms GPT-3.5-Turbo and Prometheus 13B with a slightly improved correlation on the VisIT-Bench, it is lower than GPT-4 and GPT-4V.</p>
<p>We posit that this disparity primarily originates from the differing characteristics of the VisIT-Bench and other benchmarks. The former contains a higher proportion of text-rich images, such as graphs and charts, compared to the latter two datasets. Even though the Perception Collection also includes instruction sets for text-rich images, their amount is relatively limited. These inherent limitations in the model architecture of Prometheus-Vision present challenges in processing such text-rich images during inference.</p>
<p>Nevertheless, recent works on vision-language models (Zhang et al., 2023; Ye et al., 2023b; Kim et al., 2022, 2023a) show promising capabilities for handling these image types, providing a better backbone model for future iterations of Prometheus-Vision. In consideration of these findings, the use of text-rich datasets, along with the integration of new methods drawn from recent architectural advancements, could alleviate these limitations.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Pairwise comparison of the quality of the language feedback generated by GPT-4V, GPT-4, and Prometheus-Vision-13B. Results show that Prometheus-Vision's feedback is as good as or better than GPT-4V's feedback 57.78% of the time.</p>
<p>Also, it is worthwhile to compare where GPT-4 (LM Evaluator) and GPT-4V (VLM Evaluator) excel at each benchmark. Similar to Prometheus-Vision, on the VisIT-Bench, GPT-4 shows a slightly higher correlation with human evaluators compared to GPT-4V. This could mainly be because processing text is as important when assessing responses from text-rich images such as diagrams, charts, and graphs. On the other hand, GPT-4V shows a higher correlation with human evaluators on the LLAVA-Bench and Perception-Bench which includes diverse real-world images.</p>
<h3>5.1.2 Comparison of the Quality of the Feedback</h3>
<p>Next, we compare the quality of the language feedback generated by GPT-4, GPT-4V, and Prometheus-Vision 13B across 135 instances by hiring 9 human annotators. The detailed experimental setting is explained in Appendix E and the results are shown in Figure 4.</p>
<p>Surprisingly, The Prometheus-Vision 13B model is capable of generating feedback of a quality comparable to GPT-4. Among the 135 instances, human annotators determine that 57.78% of the time, Prometheus-Vision's feedback is better or as good as GPT-4V's feedback. Also, human annotators determine that 45.93% of the time, Prometheus-Vision's feedback is better or as good as GPT-4's feedback. These results indicate that Prometheus-Vision could also be utilized as a open-source critique model for assisting assessment by humans (Saunders et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluator LM</th>
<th style="text-align: center;">LLAVA-BENCH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VISIT-BENCH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Perception-Bench</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-RLHF 7B</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.337</td>
<td style="text-align: center;">0.374</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-RLHF 13B</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.174</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-1.5 7B</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.602</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.419</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-1.5 13B</td>
<td style="text-align: center;">-0.005</td>
<td style="text-align: center;">0.097</td>
<td style="text-align: center;">0.105</td>
<td style="text-align: center;">0.597</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.270</td>
</tr>
<tr>
<td style="text-align: center;">ShareGPT4V 7B</td>
<td style="text-align: center;">0.366</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.256</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.378</td>
</tr>
<tr>
<td style="text-align: center;">FUYU 8B</td>
<td style="text-align: center;">-0.023</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.052</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.087</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">-2.15E-04</td>
<td style="text-align: center;">4.29E-06</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-TURBO-0613</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.243</td>
<td style="text-align: center;">0.685</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.563</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.417</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus 7B</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.534</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus 13B</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.513</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0613</td>
<td style="text-align: center;">0.712</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.530</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.661</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus-Vision 7B</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.662</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.502</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus-Vision 13B</td>
<td style="text-align: center;">0.786</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.832</td>
<td style="text-align: center;">0.655</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V-PREVIEW</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.824</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.761</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.699</td>
<td style="text-align: center;">0.727</td>
</tr>
</tbody>
</table>
<p>Table 4: Pearson, Kendall-Tau, Spearman correlation with scores sampled from GPT-4V across 3 inferences on visual instruction following benchmarks. Note that GPT-4V was sampled 6 times in total to measure selfconsistency. The best comparable statistics are in bold and second best are underlined among baselines. We include GPT-4V as reference to show its self-consistency when inferenced multiple times.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluator LM</th>
<th style="text-align: center;">OKVQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">VQA v2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TextVQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
<td style="text-align: center;">Pearson</td>
<td style="text-align: center;">Kendall-Tau</td>
<td style="text-align: center;">Spearman</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-RLHF 7B</td>
<td style="text-align: center;">0.562</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.061</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.187</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-RLHF 13B</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.079</td>
<td style="text-align: center;">0.362</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.320</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-1.5 7B</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.464</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.247</td>
</tr>
<tr>
<td style="text-align: center;">LLAVA-1.5 13B</td>
<td style="text-align: center;">0.548</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.408</td>
</tr>
<tr>
<td style="text-align: center;">ShareGPT4V 7B</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.271</td>
</tr>
<tr>
<td style="text-align: center;">FUYU 8B</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.174</td>
<td style="text-align: center;">0.193</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-TURBO-0613</td>
<td style="text-align: center;">0.371</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.374</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.424</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus 7B</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.483</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus 13B</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.145</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-0613</td>
<td style="text-align: center;">0.594</td>
<td style="text-align: center;">0.509</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.723</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.718</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus-Vision 7B</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.485</td>
</tr>
<tr>
<td style="text-align: center;">Prometheus-Vision 13B</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.389</td>
<td style="text-align: center;">0.428</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.523</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4V-PREVIEW</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.810</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.684</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.705</td>
<td style="text-align: center;">0.796</td>
</tr>
</tbody>
</table>
<p>Table 5: Pearson, Kendall-Tau, Spearman correlation with scores sampled from GPT-4V across 3 inferences on visual question answering benchmarks. Note that GPT-4V was sampled 6 times in total to measure self-consistency. We include GPT-4V as reference to show its self-consistency when inferenced multiple times. For all questions, we provided the Evaluator VLM with a fine-grained rubrics.</p>
<h3>5.2 Can Prometheus-Vision Closely Simulate GPT-4 Vision as a Judge?</h3>
<p>In this subsection, to check whether PrometheusVISION could be used as a reliable evaluator on various multi-modal tasks, we compare the correlation between scores predicted by GPT-4V and scores predicted by baselines including PrometheusVision. The results are shown in Tables 4, 5, 6.</p>
<h3>5.2.1 Visual Instruction Following Benchmarks</h3>
<p>The results in Table 4 show that PrometheusVision demonstrates a higher correlation with</p>
<p>GPT-4V compared to that of its backbone model, LLAVA-v1.5, in all 3 benchmarks and 2 model sizes. This indicates that training with PerceptiON COLLECTION enhances the VLM's evaluation capabilities. Furthermore, in the LLAVABENCH and PERCEPTION-BENCH, PROMETHEUSVISION 13B exhibits a higher correlation than the LM evaluators GPT-3.5-Turbo and GPT-4.</p>
<h3>5.2.2 Visual Question Answering Benchmarks</h3>
<p>Table 5 presents the correlation results in the visual question answering (VQA) benchmarks. In this benchmark, Prometheus-Vision significantly outperforms other open-source models, in-</p>
<table>
<thead>
<tr>
<th>Evaluator LM</th>
<th>COCO-CAPTIONS</th>
<th>No CAPS</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Pearson</td>
<td>Pearson</td>
</tr>
<tr>
<td>LLaVA-RLHF 7B</td>
<td>0.148</td>
<td>0.210</td>
</tr>
<tr>
<td>LLaVA-RLHF 13B</td>
<td>0.198</td>
<td>0.171</td>
</tr>
<tr>
<td>LLaVA-1.5 7B</td>
<td>0.248</td>
<td>0.155</td>
</tr>
<tr>
<td>LLaVA-1.5 13B</td>
<td>0.157</td>
<td>0.111</td>
</tr>
<tr>
<td>ShareGPT4V 7B</td>
<td>0.184</td>
<td>0.185</td>
</tr>
<tr>
<td>Fuyu 8B</td>
<td>0.191</td>
<td>0.064</td>
</tr>
<tr>
<td>GPT-3.5-Turbo-0613</td>
<td>0.233</td>
<td>0.242</td>
</tr>
<tr>
<td>Prometheus 7B</td>
<td>0.335</td>
<td>0.165</td>
</tr>
<tr>
<td>Prometheus 13B</td>
<td>0.215</td>
<td>0.279</td>
</tr>
<tr>
<td>GPT-4-0613</td>
<td>0.470</td>
<td>0.427</td>
</tr>
<tr>
<td>Prometheus-Vision 7B</td>
<td>0.434</td>
<td>0.327</td>
</tr>
<tr>
<td>Prometheus-Vision 13B</td>
<td>0.508</td>
<td>0.417</td>
</tr>
<tr>
<td>GPT-4V-PREVIEW</td>
<td>0.579</td>
<td>0.638</td>
</tr>
</tbody>
</table>
<p>Table 6: Pearson, Kendall-Tau, Spearman correlation with scores sampled from GPT-4V across 3 inferences on captioning benchmarks. Note that GPT-4V was sampled 6 times in total to measure self-consistency. We include GPT-4V as reference to show its self-consistency when inferenced multiple times. For all questions, we provide the Evaluator VLM with a fine-grained rubrics.
cluding LLaVA-v1.5. Also, we observe that Prometheus-Vision’s correlation is generally lower in VQA benchmarks compared to visual instruction following benchmarks. We attribute this to the Perception Collection training data, which generally involves longer responses, while the answers in the VQA benchmark are mostly short. Future works could consider adding more diversity to the training data to obtain a stronger VLM evaluator.</p>
<h3>5.2.3 Captioning Benchmarks</h3>
<p>Unlike visual instruction following or VQA benchmarks, captioning benchmarks do not have a direct question but rather require writing a description of a given image in a short sentence. Therefore, we created prompts such as 'Generate a coco-style caption.' and fed them to our evaluator VLM baselines during experiments. The results are shown in Table 6. While most evaluators, including proprietary LMs, show low correlation, PrometheusVision 13B surprisingly stands out by showing a correlation above 0.5 in the COCO-Captions, indicating it could generalize to evaluate other visuallanguage tasks beyond its training data.</p>
<h2>6 Analysis of Potential Biases from VLM Evaluators</h2>
<h3>6.1 Is there a Length Bias?</h3>
<p>Previous works have highlighted a phenomenon known as length bias in models, which refers to a
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Distribution of length of responses by GPT4 V across different scores, as evaluated by GPT-4V and Prometheus-Vision 13B, in all test sets. Each score category on the x-axis is annotated with the quantity of responses that received that particular score from each Evaluator VLM. Individual test set results are in Figure 13.
tendency of evaluator models to prefer longer responses (Li et al., 2023; Dubois et al., 2023; Zheng et al., 2023). This is a critical factor to consider during evaluation, as evaluators with length bias could give higher scores simply based on the length of the response, regardless of its actual content. To verify if this is the case, we plot and analyze the lengths of responses using our results from Section 5.1.</p>
<p>The box plot in Figure 5 showcases GPT-4V and Prometheus-Vision do not indiscriminately favor longer answers, indicating an absence of length bias. This is likely because our experimental setting is in an absolute grading setting where the evaluator VLM assesses the given responses with an absolute score rather than comparing two responses. This also aligns with the previous finding from Zheng et al. (2023) and Kim et al. (2023d). We provide more details of our analysis in Appendix A. 3 and Appendix E.</p>
<h3>6.2 Is there a Self-Enhancement Bias?</h3>
<p>Self-enhancement bias is another type of wellknown bias where evaluators tend to prefer its own response (Zheng et al., 2023). Since Prometheus-Vision is a model specialized for evaluation purposes only, it does not directly suffer from this bias. However, since we train Prometheus-Vision with data augmented from GPT-4V and use LLaVA-v1.5 as our base model, this could indirectly influence Prometheus-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Evaluation of 5 VLMs on (a) LLaVA-Bench and (b) Perception-Bench using either Prometheus-Vision or GPT-4V as an evaluator VLM. Trends show that Prometheus-Vision could closely simulate GPT-4V evaluation. In addition, the open-source nature of Prometheus-Vision provides accessible and transparent evaluation for those developing State-of-the-Art VLMs.</p>
<p>VISION, making things more complicated. To investigate whether there is a self-enhancement bias, we analyze the trends of which score was given to different response VLMs on the LLaVA-Bench and Perception Bench.</p>
<p>Figure 6 illustrates the results. Overall, the results show that Prometheus-Vision and GPT-4V exhibit similar evaluation patterns across the two benchmarks, reinforcing the findings from previous correlation studies with GPT-4V. Notably, Prometheus-Vision gives a higher score to other models compared to its backbone model (LLaVA-v1.5) on the LLaVA-Bench, indicating that evaluator VLMs might not always prefer the responses from its backbone model.</p>
<p>While Prometheus-Vision does give the highest score to GPT-4V, it is hard to determine if this is because Prometheus-Vision was trained on data augmented from GPT-4V, or GPT-4V is distinctively better than the open-source VLMs. We leave analysis of this to future research.</p>
<p>Lastly, the trends from Figure 6 also highlight the potential of our held-out testset, the Perception-Bench, to be used as a testbed for VLM development in future research. Specifically, on the predominant LLaVA-Bench, LLaVA-RLHF shows only a marginal difference of 0.14 points from GPT-4V. However, this gap widens significantly to 1.43 in Perception Bench. Since the Perception Bench was generated based on fine-grained rubrics, its instructions are more complex and extended responses than those of LLaVA-Bench.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we expand the 'LM-as-a-Judge' paradigm to the multi-modal space and introduce 'VLM-as-a-Judge'. We first propose a multi-modal feedback dataset called the Perception Collection, which has unique score criteria for each instance, unlike the existing multi-modal datasets that do not heavily consider the important values to consider during evaluation. Using the Perception Collection, we train Prometheus-Vision, an open-source model specialized for evaluation purposes. The uniqueness of Prometheus-Vision is that it could adhere to user-defined criteria during evaluation. Through experiments, we show that Prometheus-Vision paves way for accessible and transparent evaluation of VLMs. We hope our work could pave the way for more research on open-source evaluators in different modalities.</p>
<h2>Limitations</h2>
<p>One limitation of Prometheus-Vision is that it does not show optimal performance when evaluating instances that include text-rich images including diagrams, charts, and graphs. This is heavily reliant on the performance of the visual encoder used during visual instruction tuning of the backbone model, LLaVA-v1.5 <em>Liu et al. (2023b, a)</em>. In the future, better VLM backbones could possibly resolve this issue. Moreover, another reason might come from the fact that the Perception Collection is heavily skewed towards real-world images, not text-rich images. Adding more feedback data that includes text-rich images could be an interesting line of future work.</p>
<p>Also, our work does not consider cases when images generated by image generation models are given as input. Future work could consider exploring whether VLM evaluators could assess text outputs conditioned on AI-generated images.</p>
<p>Lastly, as mentioned in our motivation for creating the Perception Collection, currently</p>
<p>there are not a lot of multi-modal feedback datasets available for public use, compared to the text-only domain. Investigation of different forms of feedback, preference, and critique datasets would be an interesting line of future work.</p>
<h2>References</h2>
<p>Aishwarya Agrawal, Ivana Kajic, Emanuele Bugliarello, Elnaz Davoodi, Anita Gergely, Phil Blunsom, and Aida Nematzadeh. 2023. Reassessing evaluation practices in visual question answering: A case study on out-of-distribution generalization. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1171-1196.</p>
<p>Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. nocaps: novel object captioning at scale. In Proceedings of the IEEE International Conference on Computer Vision, pages 8948-8957.</p>
<p>Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jingren Zhou. 2023. Touchstone: Evaluating vision-language models by language models. arXiv preprint arXiv:2308.16890.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak Taşırlar. 2023. Introducing our multimodal models.</p>
<p>Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt. 2023. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use. arXiv preprint arXiv:2308.06595.</p>
<p>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. 2023. Sharegpt4v: Improving large multimodal models with better captions. arXiv preprint arXiv:2311.12793.</p>
<p>Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.</p>
<p>Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. 2023. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010.</p>
<p>Wentao Ge, Shunian Chen, Guiming Chen, Junying Chen, Zhihong Chen, Shuo Yan, Chenghao Zhu, Ziyue Lin, Wenya Xie, Xidong Wang, et al. 2023. Mllm-bench, evaluating multi-modal llms using gpt4v. arXiv preprint arXiv:2311.13951.</p>
<p>Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904-6913.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071.</p>
<p>Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2023. Unnatural instructions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14409-14428, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. 2023. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564.</p>
<p>Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. 2022. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV).</p>
<p>Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, and Seunghyun Park. 2023a. Visuallysituated natural language understanding with contrastive reading model and frozen large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Seungone Kim, Se June Joo, Yul Jang, Hyungjoo Chae, and Jinyoung Yeo. 2023b. Cotever: Chain of thought prompting annotation toolkit for explanation verification. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 195-208.</p>
<p>Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon Seo. 2023c. The cot collection: Improving zeroshot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023d. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491.</p>
<p>Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 2023e. Evallm: Interactive evaluation of large language model prompts on user-defined criteria. arXiv preprint arXiv:2309.13633.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.</p>
<p>Lik Xun Yuan. 2023. distilbert-base-multilingual-cased-sentiments-student (revision 2e33845).</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer.</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485.</p>
<p>Oscar Mañas, Benno Krojer, and Aishwarya Agrawal. 2023. Improving automatic vqa evaluation using large language models. arXiv preprint arXiv:2310.02567.</p>
<p>Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages $3195-3204$.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
OpenAI. 2023. GPT-4V(ision) system card. https: //openai.com/research/gpt-4v-system-card.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.</p>
<p>Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. 2019. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8317-8326.</p>
<p>Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, LiangYan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525.</p>
<p>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13484-13508, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. Finegrained human feedback gives better rewards for language model training. In Thirty-seventh Conference on Neural Information Processing Systems.</p>
<p>Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023a. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178.</p>
<p>Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2023b. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration.</p>
<p>Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023c. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928.</p>
<p>Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. 2023. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502.</p>
<p>Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023. Llavar: Enhanced visual instruction tuning for textrich image understanding.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023a. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023b. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Distribution of ROUGE-L similarities in pairs of score rubric descriptions within PERCEPTIONCOLLECTION.</p>
<h2>A Analysis of PerCeption COLLECTION</h2>
<h3>A.1 Diversity of Score Rubrics</h3>
<p>When hand-crafting seed rubrics and generating new fine-grained score rubrics through brainstorming, for each rubric, we tag keywords that best describe the criteria. Figure 11 and Figure 12 show word clouds of keywords in general-purpose rubrics and domain-specific rubrics included in PERCEPTION-COLLECTION, respectively. General-purpose rubrics encourage a broader, more holistic perspective into the image as noted by the prominence of the words 'environmental', 'scene', 'social', <em>etc.</em>. Domain-specific rubrics bring more attention to the visual aspects of the image and data, specifying long-tail subfields of various subjects which are shown by the words 'scientific', 'artistic', 'anatomical', <em>etc.</em>.</p>
<p>Following previous works on machine-generated instructions (Wang et al., 2023; Honovich et al., 2023; Kim et al., 2023d), we quantify the overlap of the generated score rubrics in our training data. Specifically, we compute ROUGE-L similarities between score rubric descriptions for every possible pair within PERCEPTION-COLLECTION. The ROUGE-L distribution is plotted in Figure 7, with the average ROUGE-L score being 0.31 and the distribution being left-skewed. This low similarity score underscores the unique and varied nature of the PERCEPTION COLLECTION.</p>
<h3>A.2 Decisiveness of Score Descriptions</h3>
<p>We examine whether each level of the scoring system in the rubric is clear and distinct. Following</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average sentiment of descriptions for each score in PERCEPTION-COLLECTION Rubrics. Sentiment of +1 signifies positivity, 0 neutrality, and -1 negativity.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Distribution of length of responses scoring from 1 to 5 provided for training.</p>
<p>Kim et al. (2023d), we compute the average sentiment in the description of each score in rubrics within PERCEPTION-COLLECTION. We use a publicly available DeBERTa-distilled DistilBERT for sentiment analysis tasks (Lik Xun Yuan, 2023). The results can be found in Figure 8, where descriptions corresponding to a score of 1 are generally more negative, while those with a score of 5 are more positive. This suggests that the training data is appropriately interpolated according to scores and PROMETHEUS-VISION trained on this dataset can conduct absolute scoring clearly and effectively.</p>
<h3>A.3 Length Bias of Responses per Score Provided for Training</h3>
<p>As explained in Section 3.1, given an instruction, rubric, and reference answer, a response corre-</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Distribution of ROUGE-L scores between score rubric descriptions in PERCEPTION-BENCH and PERCEPTION-COLLECTION.</p>
<p>sponding to score <em>i</em> is generated for all 1 ≤ <em>i</em> ≤ 5 to provide an evaluator model under training responses to practice assessment on. To nullify the tendency of recent LMs to give higher scores to longer responses (Li et al., 2023; Dubois et al., 2023; Zheng et al., 2023), during PERCEPTION-COLLECTION construction, we aim to maintain similar length of responses across the score range (See Appendix F.1 for the exact prompt). The distribution of length of responses by score is plotted in Figure 9. Response lengths are distributed evenly across the score range, with an 417 words in average.</p>
<h2>B Analysis of PERCEPTION-BENCH</h2>
<h3>B.1 Validity of Unseen Score Rubrics</h3>
<p>To ensure that PERCEPTION-BENCH contains rubrics <em>unseen</em> in PERCEPTION-COLLECTION, we plot the ROUGE-L distribution between score rubric descriptions in PERCEPTION-BENCH and PERCEPTION-COLLECTION in Figure 10. The average ROUGE-L similarity between descriptions in our test set and train set is 0.29 and the distribution is left-skewed. We claim that the train-test overlap in our proposed dataset is low and that PERCEPTION-BENCH contains many novel score rubrics.</p>
<h2>C Comparison with conventional metrics</h2>
<p>Traditional VLM response evaluation metrics, which measure similarity solely between the reference answer and the response without considering the image, struggle to account for the varied</p>
<table>
<thead>
<tr>
<th>Evaluator LM</th>
<th>LLaVA-VISIT-PERCEPTION</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Pearson</td>
</tr>
<tr>
<td>ROUGE-1</td>
<td>0.314</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.308</td>
</tr>
<tr>
<td>SPICE</td>
<td>0.340</td>
</tr>
<tr>
<td>METEOR</td>
<td>0.489</td>
</tr>
<tr>
<td>GPT-3.5-TURBO</td>
<td>0.493</td>
</tr>
<tr>
<td>PROMETHEUS 13B</td>
<td>0.450</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.734</td>
</tr>
<tr>
<td>PROMETHEUS-VISION 13B</td>
<td>0.674</td>
</tr>
<tr>
<td>GPT-4V-PREVIEW</td>
<td>0.771</td>
</tr>
</tbody>
</table>
<p>Table 7: Pearson correlation with scores from human on 45 samples from 3 visual instruction following benchmarks. The best comparable statistics are <strong>bolded</strong> and second best <strong>underlined</strong> among baselines.</p>
<p>information in images. Consequently, these conventional metrics can diverge significantly from human evaluations. As shown in Table 7, there is a low Pearson correlation between human-predicted scores and conventional metrics. Notably, even METEOR, the conventional metric with the highest correlation, only achieves around 0.489, whereas PROMETHEUS-VISION 13B demonstrates a higher correlation of 0.674. Moreover, conventional metrics often lack explainability. As Figure 1 indicates, they typically represent response quality with a simple value between 0 and 1. Model response, although it adequately depicts the image without employing expressions used in the reference answer, still receives a low score from conventional metrics due to their inability to perceive the image. In contrast, PROMETHEUS-VISION not only provides a proper numeric score but also generates feedback that elucidates the reasons behind the score. This dual output can be instrumental in identifying ways to improve the model.</p>
<h2>D Experimental Details</h2>
<h3>D.1 Implementation Details and Computation</h3>
<p><strong>Training</strong> We employ LLaVA-1.5 7B / 13B as the backbone VLM for PROMETHEUS-VISION. For the language model component, we utilize vicuna-13b-v1.5, and for the vision encoder, we use clip-vit-large-patch-14-336px. We freeze both the language model and the vision encoder, focusing our training solely on an MLP based alignment network. The training is conducted for one epoch, with a batch size per device set at 32. We set the learning rate at 1e-3, with no weight decay and a warmup ratio of 0.03. A cosine scheduler is utilized as the learning rate scheduler. To enhance training</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Word cloud of keywords in general-purpose score rubrics within Perception-Collection
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Word cloud of keywords in domain-specific score rubrics within Perception-Collection</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Full distribution of length of responses by GPT-4V across different scores, as evaluated by GPT-4V and Prometheus-Vision 13B, in each test set. Each scoring category on the x -axis is annotated with the number of responses that received that particular score from each Evaluator VLM.
efficiency, we incorporate gradient checkpointing and deepspeed zero 2 in our training process.
Inference We use three Response VLMs to generate responses to given images and questions in each dataset. Then, an Evaluator VLM generates feedback and scores indicating how the response might improve given these responses, along with the image, question, reference answer, and a guiding rubric. This approach allows us to measure the correlation between scores from GPT-4V and those from other models. In the process of generating feedback, the model employs sampling with a temperature set to 1.0 and top-p set at 0.9 , while the maximum number of tokens is configured to 2048. Regarding the resources utilized for training and inference, the GPU setup includes 8 NVIDIA A100 80GB. For the CPU, an AMD EPYC 7543 32-Core</p>
<p>Processor is used.</p>
<h2>D. 2 Details in Human Evaluation</h2>
<p>We recruit 9 undergraduate students proficient in English to conduct a human evaluation. The dataset used for the human evaluation is exclusively drawn from the Visual Instruction Tuning Benchmarks. Additionally, we randomly sample 15 items each from LLaVA-Bench, VisIT-Bench, and PerceptionBench, creating a total of 45 problems. For the pairwise feedback quality comparison, we utilize feedback from GPT-4V, GPT-4, and PrometheusVision 13B. Each of the 45 problems is structured to compare two out of the three feedbacks. Consequently, 3 sets of the same 45 problems are prepared, and the 9 participants are divided into 3 groups, with each group evaluating the same set of</p>
<p>problems. We use Label Studio as the evaluation platform ${ }^{1}$. The annotation interface is shown in Figure 18.</p>
<h2>E Length Bias during Evaluation</h2>
<p>We report GPT-4V response length distribution scored by GPT-4V and Prometheus-Vision 13B on individual test sets in Figure 13. Overall trends show that both Evaluator VLMs do not display bias towards lengths in responses during inference.</p>
<h2>F List of Prompts</h2>
<h2>F. 1 Prompts for Perception Collection Creation</h2>
<p>We include the prompts used in the creation of our training dataset, Perception Collection. The Example Criteria include hand-crafted seed rubrics that were sampled and inserted beforehand. Additionally, for fine-grained rubric augmentation, the same prompt is used, but general-purpose rubrics and domain-specific rubrics are augmented separately, ensuring the seed rubrics are also individually incorporated without mixing. Notably, although the prompt does not feature an image insertion, in practice, images are included when calling the GPT-4V API. Detailed information is in the OpenAI API document ${ }^{2}$.</p>
<h2>Prompt for rubric augmentation</h2>
<p>You are helpful and creative rubric generator. You should brainstorm creative and impressive three rubrics used to evaluate the ability of a vision-language model to generate text when given an image.</p>
<p>The rubric must be structured to assess areas that can be answered by viewing the image. It consists of a description explaining specific tasks and criteria for scoring. Here you will see 4 examples of 'criteria', and their scoring rubrics, formatted as JSON.</p>
<p>Criteria 1:
{Example Criteria 1}</p>
<h2>Criteria 2:</h2>
<p>{Example Criteria 2}</p>
<h2>Criteria 3:</h2>
<p>{Example Criteria 3}</p>
<h2>Criteria 4:</h2>
<p>{Example Criteria 4}
Please brainstorm new three criterias and scoring rubrics.
Be creative and create new but useful criteria that people in different settings or industries might find practical.
Please format the output as same as the above examples with no extra or surrounding text. And you should not mention the term like 'Criteria X:' and "'json'". In JSON, all keys and string values must be enclosed in double quotes (""). For example, "key": "value" is a valid format, but key: "value" or 'key': 'value' are not valid.
You should create a diverse rubrics suitable for the given image</p>
<p>Generated criteria:</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Prompt for checking alignment</h2>
<p>You are helpful and creative rubric evaluator. You will be given one image and a rubric used to evaluate the capabilities of a vision-language model based on that image. If the rubric is well-aligned with the given image, you should answer 'align'. However, if the rubric does not fit the given image and there are areas for improvement, you should answer 'misalign'.</p>
<p>The rubric must be structured to assess areas that can be answered by viewing the image. It consists of a description explaining specific tasks and criteria for scoring. Here you will see the rubric, and their scoring rubrics, formatted as JSON.</p>
<p>Rubric:
{Rubric}
Please answer 'align' or 'misalign'. You should generate the output in lowercase.</p>
<p>Alignment:</p>
<h2>Prompt for refining rubric</h2>
<p>You are helpful and creative rubric creator. You will be given one image and a rubric used to evaluate the capabilities of a vision-language model based on that image. If the rubric does not fit the given image and there are areas for improvement, you should make improvements to create a better rubric.</p>
<p>The rubric must be structured to assess areas that can be answered by viewing the image. It consists of a description explaining specific tasks and criteria for scoring. Here you will see the rubric, and their scoring rubrics, formatted as JSON.</p>
<p>Rubric:
{Rubric}
If there are areas that need improvement in the given rubric, improve the rubric that better fits the given image. Maximize your creativity to ensure that the rubric you refine is not too similar to the already existing one.</p>
<p>Please format the output as same as the above examples with no extra or surrounding text. You should generate only one rubric. And you should not mention the term like 'Criteria X:' and "'json'". In JSON, all keys and string values must be enclosed in double quotes (""). For example, "key": "value" is a valid format, but key: "value" or 'key': 'value' are not valid.</p>
<p>Generated rubric:</p>
<h2>Prompt for generating instruction (1)</h2>
<p>Your job is to generate a new novel problem and a response that is related to the given score rubric and image.</p>
<p>The score rubric:
{Rubric}</p>
<ul>
<li>Problem</li>
<li>The problem should inherently be related to the score criteria, score rubric and image given above. Specifically, the score criteria should be the core attributes required to solve the problem.</li>
<li>The problem itself should not be too generic or easy to solve.</li>
<li>Try to make the person who might solve the problem not notice the existence of the score rubric by not explicitly mentioning it, and also provide additional inputs and options if needed.</li>
<li>Assume a situation where a user is interacting with an AI model. The user would try to ask in a first-person point of view, but not using terms like "I", "A User" or "You" in the first sentence.</li>
<li>Do not give a role to the AI, assume that the user is asking a question from his point of view.</li>
<li>Do not include any phrase related to AI model in the problem.</li>
<li>The problem should only be answered by looking at an image, not just by reading the problem.</li>
</ul>
<h2>Prompt for generating instruction (2)</h2>
<ul>
<li>Response</li>
<li>The response should be a response that would get a score of 5 from the score rubric. - The response should be as detailed as possible unless the score rubric is related to conciseness or brevity. It should consist of multiple paragraphs, a list of items, or a step-by-step reasoning process.</li>
<li>The response should look like how a well-prompted GPT-4 would normally answer your problem.</li>
</ul>
<h2>* Format</h2>
<ul>
<li>DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response only.</li>
<li>In front of the problem, append the phrase 'Problem:' and in front of the response, append the phrase 'Response:'.</li>
<li>Write in the order of 'Problem' - 'Response', where the two items are separated by the phrase '[NEXT]'.</li>
<li>Write [END] after you are done.</li>
</ul>
<p>Data Generation:</p>
<h2>Prompt for response and feedback (1)</h2>
<p>Your job is to generate a response that would get a score of {score} and corresponding feedback based on the given score rubric and image. For reference, a reference response that would get a score of 5 is also given.</p>
<p>Instruction:
{instruction}</p>
<p>The score rubric:
${$ rubric $}$</p>
<p>Reference response (Score 5):
${$ response $}$</p>
<ul>
<li>Response</li>
<li>The quality of the score {score} response should be determined based on the score rubric and image, not by its length.</li>
<li>The score {score} response should have the same length as the reference response, composed of {number of sentences} sentences.</li>
<li>Do not explicitly state the keywords of the score rubric inside the response.</li>
</ul>
<h2>Prompt for response and feedback (2)</h2>
<ul>
<li>Feedback</li>
<li>The score {score} feedback should each be an explanation of why the response would get a score of {score}. It should be written based on the generated response, score rubric and image.</li>
<li>The score {score} feedback shouldn't just copy and paste the score rubric, but it should also give very detailed feedback on the content of the corresponding response. - The score {score} feedback should include the phrase 'So the overall score is {score}' in the last sentence.</li>
</ul>
<h2>* Format</h2>
<ul>
<li>DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response only.</li>
<li>In front of the response, append the phrase 'Response:' and in front of the feedback, append the phrase 'Feedback:'.</li>
<li>Write in the order of 'Response' - 'Feedback', where the two items are separated by the phrase '[NEXT]'.</li>
<li>Write [END] after you are done.</li>
</ul>
<p>Data Generation:</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://labelstud.io
${ }^{2}$ https://platform.openai.com/docs/guides/ vision&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
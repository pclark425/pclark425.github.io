<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9050 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9050</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9050</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-271891885</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.08848v1.pdf" target="_blank">PsychoLex: Unveiling the Psychological Mind of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This paper explores the intersection of psychology and artificial intelligence through the development and evaluation of specialized Large Language Models (LLMs). We introduce PsychoLex , a suite of resources designed to enhance LLMs’ proficiency in psychological tasks in both Persian and English. Key contributions include the PsychoLexQA dataset for instructional content and the PsychoLexEval dataset for rigorous evaluation of LLMs in complex psychological scenarios. Additionally, we present the PsychoLexLLaMA model, optimized specifically for psychological applications, demonstrating superior performance compared to general-purpose models. The findings underscore the potential of tailored LLMs for advancing psychological research and applications, while also highlighting areas for further refinement. This research offers a foundational step towards integrating LLMs into specialized psychological domains, with implications for future advancements in AI-driven psychological practice.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9050.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9050.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsychoLexLLaMA (avg, 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsychoLexLLaMA (linear-weight-average, 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B-parameter LLaMA-3.1-derived model fine-tuned and combined with psychology-specific continuous pretraining and supervised fine-tuning (PsychoLex datasets); final weights are a 50/50 linear blend with LLaMA-3.1 Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PsychoLexLLaMA (average)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Derived from LLaMA 3.1 via continuous pretraining on bilingual 'Introduction to Psychology' and supervised fine-tuning on PsychoLexQA using LoRA; final model is linear-weight averaged with LLaMA 3.1 Instruct (50%/50%).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A bilingual (Persian/English) multiple-choice question-answer (MCQA) battery covering broad psychology domains (general, developmental, clinical, psychometrics, cognitive tests, I/O, social, educational, biological foundations, learning, memory, intelligence, personality, disorders).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 48.52%, 1-shot 41.97%, 5-shot 47.05%, Avg 45.85%; English: 0-shot 90.10%, 1-shot 89.03%, 5-shot 90.04%, Avg 89.72%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below typical high English performance observed for strongest models but substantially better in English than Persian; no human baseline reported so direct human comparison not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated on PsychoLexEval in zero-shot, one-shot, and five-shot settings with a consistent generation configuration across models (see Table 2). PsychoLexLLaMA constructed via LoRA continuous pretraining on Hilgard bilingual text, LoRA supervised fine-tuning on PsychoLexQA, and linear weight combination with LLaMA-3.1 Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline reported; performance differs markedly by language (much higher in English), and dataset is MCQA (may not reflect open-ended clinical reasoning). Fine-tuning specifics and prompt templates not exhaustively reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9050.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsychoLexLLaMA (pretrain+sft, 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsychoLexLLaMA (continuous pretraining + supervised fine-tuning, 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B LLaMA-3.1 variant continuously pre-trained on psychology text and then supervised fine-tuned on PsychoLexQA using LoRA (no final linear averaging).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PsychoLexLLaMA (pretrain-sft)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA 3.1 (8B) continuously pre-trained on bilingual psychology textbook (LoRA) and further supervised fine-tuned on PsychoLexQA (LoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Bilingual psychology MCQA dataset covering multiple subfields; used to measure multiple-choice accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 47.30%, 1-shot 43.13%, 5-shot 46.61%, Avg 45.68%; English: 0-shot 88.97%, 1-shot 81.21%, 5-shot 62.03%, Avg 77.40%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs substantially better in English than Persian; one-shot and five-shot impacts vary across languages; no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same evaluation protocol (0/1/5-shot) and generation config for all models. Pretraining and SFT used LoRA; reported pretraining/fine-tuning runtimes and hardware in methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>English 5-shot performance notably lower for this variant (62.03%) indicating instability with added examples; no human comparison provided; dataset is MCQ so limited to closed-response evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9050.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsychoLexLLaMA (avg, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsychoLexLLaMA (linear-weight-average, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter LLaMA-3.1-derived model with psychology-focused continual pretraining and supervised fine-tuning; final model is a 50/50 linear combination with LLaMA-3.1 Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PsychoLexLLaMA (average)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA 3.1 (70B) continuously pre-trained on psychology text and supervised fine-tuned on PsychoLexQA via LoRA; final weights averaged 50% with LLaMA-3.1 Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Bilingual psychology multiple-choice battery spanning clinical, cognitive, developmental, and other psychology domains.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 65.84%, 1-shot 53.06%, 5-shot 69.66%, Avg 62.85%; English: 0-shot 92.13%, 1-shot 91.85%, 5-shot 91.87%, Avg 91.95%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Strong performance in English (≈92% avg), better than PsychoLexLLaMA 8B; Persian performance lower and more variable; no human baseline presented.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated under zero/one/five-shot settings with consistent generation config. Pretraining and SFT used LoRA; linear weight combination applied to produce final model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors note difficulty adding new domain knowledge to very large models without forgetting prior knowledge; human baseline absent; dataset limited to MCQ format.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9050.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PsychoLexLLaMA (pretrain+sft, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PsychoLexLLaMA (continuous pretraining + supervised fine-tuning, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B LLaMA-3.1 variant continuously pre-trained and supervised fine-tuned on psychology corpora using LoRA (without averaging).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PsychoLexLLaMA (pretrain-sft)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA 3.1 (70B) with psychology-focused continual pretraining and supervised fine-tuning using LoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Bilingual multiple-choice evaluation across many psychology subfields designed to test comprehension and application of psychological knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 67.79%, 1-shot 45.34%, 5-shot 68.07%, Avg 60.40%; English: 0-shot 91.45%, 1-shot 90.24%, 5-shot 90.85%, Avg 90.85%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>High English accuracy (~91%); Persian performance more variable; no human baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated in 0/1/5-shot settings with identical generation settings for all models. Pretraining and fine-tuning details (LoRA, runtimes, GPUs) are reported in model methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors discuss challenges with scaling: large models may require much more data to incorporate new domain knowledge and avoid forgetting; human baseline not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9050.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3.1 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned release of the LLaMA 3.1 family used as a baseline and as a component in PsychoLexLLaMA; evaluated at multiple parameter scales in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of LLaMA 3.1 (evaluated at 8B and 70B sizes); used as a baseline and for linear weight combination in PsychoLexLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 70B (both evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Psychology MCQA battery in Persian and English covering general and specialized psychology topics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>8B — Persian: 0-shot 45.89%, 1-shot 41.36%, 5-shot 35.78%, Avg 41.01%; English — 8B: 0-shot 88.97%, 1-shot 89.25%, 5-shot ~87.xx, Avg 88.41. 70B — Persian: 0-shot 70.34%, 1-shot 67.83%, 5-shot 70.40%, Avg 69.52; English (70B): 0-shot ~93.03%, 1-shot ~92.63%, 5-shot ~92.19%, Avg ~92.58 (table formatting imprecise).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA-3.1 Instruct 70B among top performers (especially in English); outperforms many smaller models on English PsychoLexEval; in Persian, performance improved with more shots for some models but varied across sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Baseline models and LLaMA-3.1 variants were evaluated in 0/1/5-shot settings using the shared generation configuration (Table 2). LLaMA-3.1 8B/70B also used in weight-averaging to create PsychoLexLLaMA.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Table formatting introduces minor ambiguities for some reported values; no human baseline provided; cross-language differences substantial (English >> Persian).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9050.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier instruction-tuned variant from the LLaMA family evaluated at multiple scales as a baseline in the PsychoLexEval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLaMA-3 model versions (8B and 70B tested) used as baselines for psychological MCQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 70B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQA covering a broad set of psychology topics in Persian and English.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>8B — Persian: 0-shot 33.88%, 1-shot 10.66%, 5-shot 34.49%, Avg 26.34; English — 8B: 0-shot 85.77%, 1-shot 78.57%, 5-shot 68.22%, Avg 77.52. 70B — Persian: 0-shot 19.54%, 1-shot 9.31%, 5-shot 0.59%, Avg 9.78; English 70B: 0-shot 90.55%, 1-shot 88.58%, 5-shot 76.77%, Avg 85.30.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Strong English performance at larger scale but Persian results are inconsistent and sometimes very low; no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated 0/1/5-shot across PsychoLexEval with consistent generation settings. Some reported Persian 1-shot/5-shot values are unusually low for certain scale/configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Large discrepancies between Persian and English suggest language/data-coverage effects; table entries for some 70B Persian numbers appear very low and may reflect domain/language mismatch or table-formatting issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9050.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight LLM (Qwen2 family) evaluated in this study at multiple parameter scales as a competitor on PsychoLexEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen2 instruction-tuned models evaluated at 7B and 72B in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Psychology multiple-choice test suite in Persian and English assessing conceptual and applied knowledge across psychology subfields.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>7B — Persian: 0-shot 3.55%, 1-shot 6.18%, 5-shot 8.63%, Avg 6.12; English — 7B: 0-shot 89.31%, 1-shot 42.74%, 5-shot 83.76%, Avg 71.94. 72B — Persian: 0-shot 31.37%, 1-shot 5.82%, 5-shot 50.32%, Avg 9.16; English — 72B: 0-shot 91.11%, 1-shot 73.79%, 5-shot 92.29%, Avg 85.73.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs well in English (especially at 72B) but shows unstable and often poor performance in Persian for some configurations; no human baseline reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated in 0/1/5-shot settings with the same generation configuration as other models. Persian performance shows strong sensitivity to shot-setting and model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Large cross-language performance gaps indicate sensitivity to training data language coverage; no human comparison data available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9050.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma 1.1 it</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma 1.1 it</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B open model from the Gemma family evaluated on the PsychoLexEval dataset as an open-source competitor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma 1.1 it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma 1.1 instruction-tuned 7B model evaluated on bilingual psychology MCQ tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice psychology evaluation covering many subfields including cognitive and clinical topics.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 43.07%, 1-shot 40.68%, 5-shot 27.57%, Avg 37.11%; English: 0-shot 84.75%, 1-shot 55.06%, 5-shot 65.86%, Avg 68.56%</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs substantially better in English than Persian; overall lower than the strongest models (LLaMA-3.1 derived and PsychoLex variants) in English.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated under shared 0/1/5-shot protocol; generation configuration common to all models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline; Persian performance drops in 5-shot setting relative to 0/1-shot for this model, indicating potential sensitivity to example selection or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9050.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PersianMind</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PersianMind (Persian-English cross-lingual LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cross-lingual Persian-English LLM evaluated in the paper to benchmark Persian-language handling on the PsychoLexEval dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PersianMind</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A cross-lingual Persian-English LLM (7B) designed to handle Persian-language tasks; evaluated here as part of Persian benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA) - Persian & English</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Persian and English MCQA covering psychology topics (conceptual and applied).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 35.78%, 1-shot 35.96%, 5-shot 24.63%, Avg 32.12%; English: (not listed separately in table block for PersianMind) — primary reported values are for Persian.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>PersianMind demonstrates modest Persian accuracy (≈32% avg); no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations used the same generation configuration and shot-settings (0/1/5). PersianMind is included among Persian-focused models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Only Persian results clearly reported here; dataset and selection biases may affect representativeness; no human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9050.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>c4ai-command-r-v0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>c4ai-command-r-v0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 135B parameter open model included in the evaluation as a large-scale competitor on PsychoLexEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>c4ai-command-r-v0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open model (reported as 135B in table) evaluated on the bilingual PsychoLexEval dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>135B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MCQA spanning many psychology domains; used to measure multiple-choice accuracy of large LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Persian: 0-shot 35.96%, 1-shot 21.75%, 5-shot 46.20%, Avg 34.64%; English: 0-shot ~87.78%, 1-shot 78.06%, 5-shot 75.08%, Avg 80.05% (table formatting imprecise).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Competitive in English with strong zero-shot performance; Persian performance is lower and more variable across shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated identically to other models (0/1/5-shot, consistent generation config).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Some English numbers in the table require careful parsing due to formatting; no human baseline; MCQ format limits assessment of open-ended psychological reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9050.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9050.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aya-23</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aya-23 (Aya family of multilingual models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Multilingual Aya models (evaluated at 8B and 35B) included as part of the cross-model comparisons on PsychoLexEval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aya-23</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Aya family open models (evaluated at 8B and 35B) intended for multilingual capabilities; used here to assess Persian and English psychology MCQ performance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B and 35B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>PsychoLexEval (MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Bilingual MCQA battery for psychology knowledge and comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>8B — Persian: 0-shot 39.64%, 1-shot 41.42%, 5-shot 27.02%, Avg 36.03; English — 8B: 0-shot 73.62%, 1-shot 33.80%, 5-shot 77.05%, Avg 61.49. 35B — Persian: 0-shot 21.07%, 1-shot 10.47%, 5-shot 22.69%, Avg 18.08; English 35B: 0-shot 81.32%, 1-shot 79.02%, 5-shot ~82.xx, Avg 80.78 (table formatting imprecise for 5-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performs substantially better in English than Persian; larger parameter Aya (35B) shows stronger English accuracy than its Persian numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same 0/1/5-shot evaluation and shared generation settings used across models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Persian performance is generally lower across Aya variants; table formatting ambiguity for some English 5-shot values; no human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PsychoLex: Unveiling the Psychological Mind of Large Language Models', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Llama 3 Herd of Models <em>(Rating: 2)</em></li>
                <li>Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT. <em>(Rating: 2)</em></li>
                <li>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. <em>(Rating: 2)</em></li>
                <li>Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. <em>(Rating: 2)</em></li>
                <li>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9050",
    "paper_id": "paper-271891885",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "PsychoLexLLaMA (avg, 8B)",
            "name_full": "PsychoLexLLaMA (linear-weight-average, 8B)",
            "brief_description": "An 8B-parameter LLaMA-3.1-derived model fine-tuned and combined with psychology-specific continuous pretraining and supervised fine-tuning (PsychoLex datasets); final weights are a 50/50 linear blend with LLaMA-3.1 Instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PsychoLexLLaMA (average)",
            "model_description": "Derived from LLaMA 3.1 via continuous pretraining on bilingual 'Introduction to Psychology' and supervised fine-tuning on PsychoLexQA using LoRA; final model is linear-weight averaged with LLaMA 3.1 Instruct (50%/50%).",
            "model_size": "8B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "A bilingual (Persian/English) multiple-choice question-answer (MCQA) battery covering broad psychology domains (general, developmental, clinical, psychometrics, cognitive tests, I/O, social, educational, biological foundations, learning, memory, intelligence, personality, disorders).",
            "llm_performance": "Persian: 0-shot 48.52%, 1-shot 41.97%, 5-shot 47.05%, Avg 45.85%; English: 0-shot 90.10%, 1-shot 89.03%, 5-shot 90.04%, Avg 89.72%",
            "human_baseline_performance": null,
            "performance_comparison": "Below typical high English performance observed for strongest models but substantially better in English than Persian; no human baseline reported so direct human comparison not provided.",
            "experimental_details": "Evaluated on PsychoLexEval in zero-shot, one-shot, and five-shot settings with a consistent generation configuration across models (see Table 2). PsychoLexLLaMA constructed via LoRA continuous pretraining on Hilgard bilingual text, LoRA supervised fine-tuning on PsychoLexQA, and linear weight combination with LLaMA-3.1 Instruct.",
            "limitations_or_caveats": "No human baseline reported; performance differs markedly by language (much higher in English), and dataset is MCQA (may not reflect open-ended clinical reasoning). Fine-tuning specifics and prompt templates not exhaustively reported here.",
            "uuid": "e9050.0",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "PsychoLexLLaMA (pretrain+sft, 8B)",
            "name_full": "PsychoLexLLaMA (continuous pretraining + supervised fine-tuning, 8B)",
            "brief_description": "An 8B LLaMA-3.1 variant continuously pre-trained on psychology text and then supervised fine-tuned on PsychoLexQA using LoRA (no final linear averaging).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PsychoLexLLaMA (pretrain-sft)",
            "model_description": "LLaMA 3.1 (8B) continuously pre-trained on bilingual psychology textbook (LoRA) and further supervised fine-tuned on PsychoLexQA (LoRA).",
            "model_size": "8B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Bilingual psychology MCQA dataset covering multiple subfields; used to measure multiple-choice accuracy.",
            "llm_performance": "Persian: 0-shot 47.30%, 1-shot 43.13%, 5-shot 46.61%, Avg 45.68%; English: 0-shot 88.97%, 1-shot 81.21%, 5-shot 62.03%, Avg 77.40%",
            "human_baseline_performance": null,
            "performance_comparison": "Performs substantially better in English than Persian; one-shot and five-shot impacts vary across languages; no human baseline provided.",
            "experimental_details": "Same evaluation protocol (0/1/5-shot) and generation config for all models. Pretraining and SFT used LoRA; reported pretraining/fine-tuning runtimes and hardware in methods.",
            "limitations_or_caveats": "English 5-shot performance notably lower for this variant (62.03%) indicating instability with added examples; no human comparison provided; dataset is MCQ so limited to closed-response evaluation.",
            "uuid": "e9050.1",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "PsychoLexLLaMA (avg, 70B)",
            "name_full": "PsychoLexLLaMA (linear-weight-average, 70B)",
            "brief_description": "A 70B-parameter LLaMA-3.1-derived model with psychology-focused continual pretraining and supervised fine-tuning; final model is a 50/50 linear combination with LLaMA-3.1 Instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PsychoLexLLaMA (average)",
            "model_description": "LLaMA 3.1 (70B) continuously pre-trained on psychology text and supervised fine-tuned on PsychoLexQA via LoRA; final weights averaged 50% with LLaMA-3.1 Instruct.",
            "model_size": "70B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Bilingual psychology multiple-choice battery spanning clinical, cognitive, developmental, and other psychology domains.",
            "llm_performance": "Persian: 0-shot 65.84%, 1-shot 53.06%, 5-shot 69.66%, Avg 62.85%; English: 0-shot 92.13%, 1-shot 91.85%, 5-shot 91.87%, Avg 91.95%",
            "human_baseline_performance": null,
            "performance_comparison": "Strong performance in English (≈92% avg), better than PsychoLexLLaMA 8B; Persian performance lower and more variable; no human baseline presented.",
            "experimental_details": "Evaluated under zero/one/five-shot settings with consistent generation config. Pretraining and SFT used LoRA; linear weight combination applied to produce final model.",
            "limitations_or_caveats": "Authors note difficulty adding new domain knowledge to very large models without forgetting prior knowledge; human baseline absent; dataset limited to MCQ format.",
            "uuid": "e9050.2",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "PsychoLexLLaMA (pretrain+sft, 70B)",
            "name_full": "PsychoLexLLaMA (continuous pretraining + supervised fine-tuning, 70B)",
            "brief_description": "A 70B LLaMA-3.1 variant continuously pre-trained and supervised fine-tuned on psychology corpora using LoRA (without averaging).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PsychoLexLLaMA (pretrain-sft)",
            "model_description": "LLaMA 3.1 (70B) with psychology-focused continual pretraining and supervised fine-tuning using LoRA.",
            "model_size": "70B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Bilingual multiple-choice evaluation across many psychology subfields designed to test comprehension and application of psychological knowledge.",
            "llm_performance": "Persian: 0-shot 67.79%, 1-shot 45.34%, 5-shot 68.07%, Avg 60.40%; English: 0-shot 91.45%, 1-shot 90.24%, 5-shot 90.85%, Avg 90.85%",
            "human_baseline_performance": null,
            "performance_comparison": "High English accuracy (~91%); Persian performance more variable; no human baseline reported.",
            "experimental_details": "Evaluated in 0/1/5-shot settings with identical generation settings for all models. Pretraining and fine-tuning details (LoRA, runtimes, GPUs) are reported in model methods.",
            "limitations_or_caveats": "Authors discuss challenges with scaling: large models may require much more data to incorporate new domain knowledge and avoid forgetting; human baseline not provided.",
            "uuid": "e9050.3",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Llama-3.1 Instruct",
            "name_full": "LLaMA 3.1 Instruct",
            "brief_description": "An instruction-tuned release of the LLaMA 3.1 family used as a baseline and as a component in PsychoLexLLaMA; evaluated at multiple parameter scales in the study.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1 Instruct",
            "model_description": "Instruction-tuned variant of LLaMA 3.1 (evaluated at 8B and 70B sizes); used as a baseline and for linear weight combination in PsychoLexLLaMA.",
            "model_size": "8B and 70B (both evaluated)",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Psychology MCQA battery in Persian and English covering general and specialized psychology topics.",
            "llm_performance": "8B — Persian: 0-shot 45.89%, 1-shot 41.36%, 5-shot 35.78%, Avg 41.01%; English — 8B: 0-shot 88.97%, 1-shot 89.25%, 5-shot ~87.xx, Avg 88.41. 70B — Persian: 0-shot 70.34%, 1-shot 67.83%, 5-shot 70.40%, Avg 69.52; English (70B): 0-shot ~93.03%, 1-shot ~92.63%, 5-shot ~92.19%, Avg ~92.58 (table formatting imprecise).",
            "human_baseline_performance": null,
            "performance_comparison": "LLaMA-3.1 Instruct 70B among top performers (especially in English); outperforms many smaller models on English PsychoLexEval; in Persian, performance improved with more shots for some models but varied across sizes.",
            "experimental_details": "Baseline models and LLaMA-3.1 variants were evaluated in 0/1/5-shot settings using the shared generation configuration (Table 2). LLaMA-3.1 8B/70B also used in weight-averaging to create PsychoLexLLaMA.",
            "limitations_or_caveats": "Table formatting introduces minor ambiguities for some reported values; no human baseline provided; cross-language differences substantial (English &gt;&gt; Persian).",
            "uuid": "e9050.4",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Llama-3 Instruct",
            "name_full": "LLaMA 3 Instruct",
            "brief_description": "An earlier instruction-tuned variant from the LLaMA family evaluated at multiple scales as a baseline in the PsychoLexEval experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3 Instruct",
            "model_description": "Instruction-tuned LLaMA-3 model versions (8B and 70B tested) used as baselines for psychological MCQA.",
            "model_size": "8B and 70B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "MCQA covering a broad set of psychology topics in Persian and English.",
            "llm_performance": "8B — Persian: 0-shot 33.88%, 1-shot 10.66%, 5-shot 34.49%, Avg 26.34; English — 8B: 0-shot 85.77%, 1-shot 78.57%, 5-shot 68.22%, Avg 77.52. 70B — Persian: 0-shot 19.54%, 1-shot 9.31%, 5-shot 0.59%, Avg 9.78; English 70B: 0-shot 90.55%, 1-shot 88.58%, 5-shot 76.77%, Avg 85.30.",
            "human_baseline_performance": null,
            "performance_comparison": "Strong English performance at larger scale but Persian results are inconsistent and sometimes very low; no human baseline provided.",
            "experimental_details": "Evaluated 0/1/5-shot across PsychoLexEval with consistent generation settings. Some reported Persian 1-shot/5-shot values are unusually low for certain scale/configurations.",
            "limitations_or_caveats": "Large discrepancies between Persian and English suggest language/data-coverage effects; table entries for some 70B Persian numbers appear very low and may reflect domain/language mismatch or table-formatting issues.",
            "uuid": "e9050.5",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Qwen2 Instruct",
            "name_full": "Qwen2 Instruct",
            "brief_description": "An open-weight LLM (Qwen2 family) evaluated in this study at multiple parameter scales as a competitor on PsychoLexEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2 Instruct",
            "model_description": "Qwen2 instruction-tuned models evaluated at 7B and 72B in this paper's experiments.",
            "model_size": "7B and 72B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Psychology multiple-choice test suite in Persian and English assessing conceptual and applied knowledge across psychology subfields.",
            "llm_performance": "7B — Persian: 0-shot 3.55%, 1-shot 6.18%, 5-shot 8.63%, Avg 6.12; English — 7B: 0-shot 89.31%, 1-shot 42.74%, 5-shot 83.76%, Avg 71.94. 72B — Persian: 0-shot 31.37%, 1-shot 5.82%, 5-shot 50.32%, Avg 9.16; English — 72B: 0-shot 91.11%, 1-shot 73.79%, 5-shot 92.29%, Avg 85.73.",
            "human_baseline_performance": null,
            "performance_comparison": "Performs well in English (especially at 72B) but shows unstable and often poor performance in Persian for some configurations; no human baseline reported.",
            "experimental_details": "Evaluated in 0/1/5-shot settings with the same generation configuration as other models. Persian performance shows strong sensitivity to shot-setting and model scale.",
            "limitations_or_caveats": "Large cross-language performance gaps indicate sensitivity to training data language coverage; no human comparison data available.",
            "uuid": "e9050.6",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Gemma 1.1 it",
            "name_full": "Gemma 1.1 it",
            "brief_description": "A 7B open model from the Gemma family evaluated on the PsychoLexEval dataset as an open-source competitor.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemma 1.1 it",
            "model_description": "Gemma 1.1 instruction-tuned 7B model evaluated on bilingual psychology MCQ tasks.",
            "model_size": "7B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Multiple-choice psychology evaluation covering many subfields including cognitive and clinical topics.",
            "llm_performance": "Persian: 0-shot 43.07%, 1-shot 40.68%, 5-shot 27.57%, Avg 37.11%; English: 0-shot 84.75%, 1-shot 55.06%, 5-shot 65.86%, Avg 68.56%",
            "human_baseline_performance": null,
            "performance_comparison": "Performs substantially better in English than Persian; overall lower than the strongest models (LLaMA-3.1 derived and PsychoLex variants) in English.",
            "experimental_details": "Evaluated under shared 0/1/5-shot protocol; generation configuration common to all models.",
            "limitations_or_caveats": "No human baseline; Persian performance drops in 5-shot setting relative to 0/1-shot for this model, indicating potential sensitivity to example selection or prompting.",
            "uuid": "e9050.7",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "PersianMind",
            "name_full": "PersianMind (Persian-English cross-lingual LLM)",
            "brief_description": "A cross-lingual Persian-English LLM evaluated in the paper to benchmark Persian-language handling on the PsychoLexEval dataset.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PersianMind",
            "model_description": "A cross-lingual Persian-English LLM (7B) designed to handle Persian-language tasks; evaluated here as part of Persian benchmarks.",
            "model_size": "7B",
            "test_battery_name": "PsychoLexEval (MCQA) - Persian & English",
            "test_description": "Persian and English MCQA covering psychology topics (conceptual and applied).",
            "llm_performance": "Persian: 0-shot 35.78%, 1-shot 35.96%, 5-shot 24.63%, Avg 32.12%; English: (not listed separately in table block for PersianMind) — primary reported values are for Persian.",
            "human_baseline_performance": null,
            "performance_comparison": "PersianMind demonstrates modest Persian accuracy (≈32% avg); no human baseline provided.",
            "experimental_details": "Evaluations used the same generation configuration and shot-settings (0/1/5). PersianMind is included among Persian-focused models.",
            "limitations_or_caveats": "Only Persian results clearly reported here; dataset and selection biases may affect representativeness; no human baseline provided.",
            "uuid": "e9050.8",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "c4ai-command-r-v0",
            "name_full": "c4ai-command-r-v0",
            "brief_description": "A 135B parameter open model included in the evaluation as a large-scale competitor on PsychoLexEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "c4ai-command-r-v0",
            "model_description": "Large open model (reported as 135B in table) evaluated on the bilingual PsychoLexEval dataset.",
            "model_size": "135B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "MCQA spanning many psychology domains; used to measure multiple-choice accuracy of large LLMs.",
            "llm_performance": "Persian: 0-shot 35.96%, 1-shot 21.75%, 5-shot 46.20%, Avg 34.64%; English: 0-shot ~87.78%, 1-shot 78.06%, 5-shot 75.08%, Avg 80.05% (table formatting imprecise).",
            "human_baseline_performance": null,
            "performance_comparison": "Competitive in English with strong zero-shot performance; Persian performance is lower and more variable across shot settings.",
            "experimental_details": "Evaluated identically to other models (0/1/5-shot, consistent generation config).",
            "limitations_or_caveats": "Some English numbers in the table require careful parsing due to formatting; no human baseline; MCQ format limits assessment of open-ended psychological reasoning.",
            "uuid": "e9050.9",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Aya-23",
            "name_full": "Aya-23 (Aya family of multilingual models)",
            "brief_description": "Multilingual Aya models (evaluated at 8B and 35B) included as part of the cross-model comparisons on PsychoLexEval.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Aya-23",
            "model_description": "Aya family open models (evaluated at 8B and 35B) intended for multilingual capabilities; used here to assess Persian and English psychology MCQ performance.",
            "model_size": "8B and 35B",
            "test_battery_name": "PsychoLexEval (MCQA)",
            "test_description": "Bilingual MCQA battery for psychology knowledge and comprehension.",
            "llm_performance": "8B — Persian: 0-shot 39.64%, 1-shot 41.42%, 5-shot 27.02%, Avg 36.03; English — 8B: 0-shot 73.62%, 1-shot 33.80%, 5-shot 77.05%, Avg 61.49. 35B — Persian: 0-shot 21.07%, 1-shot 10.47%, 5-shot 22.69%, Avg 18.08; English 35B: 0-shot 81.32%, 1-shot 79.02%, 5-shot ~82.xx, Avg 80.78 (table formatting imprecise for 5-shot).",
            "human_baseline_performance": null,
            "performance_comparison": "Performs substantially better in English than Persian; larger parameter Aya (35B) shows stronger English accuracy than its Persian numbers.",
            "experimental_details": "Same 0/1/5-shot evaluation and shared generation settings used across models.",
            "limitations_or_caveats": "Persian performance is generally lower across Aya variants; table formatting ambiguity for some English 5-shot values; no human baseline.",
            "uuid": "e9050.10",
            "source_info": {
                "paper_title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Llama 3 Herd of Models",
            "rating": 2,
            "sanitized_title": "the_llama_3_herd_of_models"
        },
        {
            "paper_title": "Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT.",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_for_persian_a_preliminary_study_focusing_on_chatgpt"
        },
        {
            "paper_title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.",
            "rating": 2,
            "sanitized_title": "exploring_the_frontiers_of_llms_in_psychological_applications_a_comprehensive_review"
        },
        {
            "paper_title": "Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models.",
            "rating": 2,
            "sanitized_title": "psyllm_scaling_up_global_mental_health_psychological_services_with_aibased_large_language_models"
        },
        {
            "paper_title": "Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?",
            "rating": 1,
            "sanitized_title": "khayyam_challenge_persianmmlu_is_your_llm_truly_wise_to_the_persian_language"
        }
    ],
    "cost": 0.0217225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PsychoLex: Unveiling the Psychological Mind of Large Language Models</p>
<p>Mohammad Amin Abbasi m_abbasi1378@comp.iust.ac.ir 
Department of Computer Engineering
University of Science and Technology
TehranIran, Iran</p>
<p>Farnaz Sadat Mirnezami farnaz.mirnezami@gmail.com 
Department of Computer Engineering
University of Guilan
RashtIran</p>
<p>Hassan Naderi naderi@iust.ac.ir 
Department of Computer Engineering
University of Science and Technology
TehranIran, Iran</p>
<p>PsychoLex: Unveiling the Psychological Mind of Large Language Models
1C264278EAEE8A9249BE2652391496A3
This paper explores the intersection of psychology and artificial intelligence through the development and evaluation of specialized Large Language Models (LLMs).We introduce PsychoLex 1 , a suite of resources designed to enhance LLMs' proficiency in psychological tasks in both Persian and English.Key contributions include the PsychoLexQA dataset for instructional content and the Psycho-LexEval dataset for rigorous evaluation of LLMs in complex psychological scenarios.Additionally, we present the Psycho-LexLLaMA model, optimized specifically for psychological applications, demonstrating superior performance compared to general-purpose models.The findings underscore the potential of tailored LLMs for advancing psychological research and applications, while also highlighting areas for further refinement.This research offers a foundational step towards integrating LLMs into specialized psychological domains, with implications for future advancements in AI-driven psychological practice.</p>
<p>Introduction</p>
<p>The rise of Large Language Models (LLMs) has significantly advanced artificial intelligence (AI), providing remarkable capabilities in natural language processing and understanding (Guo et al., 2023;Minaee et al., 2024;Wu et al., 2023).These models have shown proficiency in generating human-like text, translating languages, and engaging 1 https://huggingface.co/collections/aminabbasi/psycholex-66b64e3768da519596e49de9 in sophisticated dialogues (Agrawal, 2023).However, as users increasingly rely on LLMs for psychological and therapeutic questions (Lai et al., 2023), the limitations of these models in specialized domains have become apparent.Notably, there is a critical absence of datasets designed to evaluate and enhance LLMs' performance in the field of psychology.</p>
<p>Despite considerable progress in general AI research, the integration of psychological expertise into LLMs remains underdeveloped.Existing methodologies often lack the depth required to understand and respond accurately to complex psychological inquiries.Moreover, the field is hindered by the lack of comprehensive datasets that include not only questions and answers but also instructional content tailored to psychological contexts.This gap is significant because it restricts the practical applications of LLMs in psychological research, therapy, and education, where nuanced and precise information is essential.</p>
<p>Our research seeks to address this gap by introducing PsychoLex, a suite of resources and models specifically designed for psychological applications in both Persian and English.The primary objectives of this study are to develop and evaluate specialized datasets, namely PsychoLexQA and PsychoLexEval, and to introduce Psycho-LexLLaMA, an LLM developed for psychologyical tasks.These contributions include: (i) Psy-choLexQA, which provides comprehensive instructional content and detailed questions and answers to enhance LLM training; (ii) Psycho-LexEval, a multiple-choice question and answer (MCQA) dataset designed for rigorous evaluation of LLMs in psychological contexts, ensuring they can handle complex psychological queries accurately and contextually;(iii) PsychoLexLLaMA, which improves the performance of LLMs in psychological tasks through continual pre-training and fine-tuning of LLaMA 3.1.(Dubey et al., 2024) Together, these contributions aim to provide robust solutions to existing challenges, enhancing the accuracy and relevance of AI-driven psychological tools and paving the way for future advancements in integrating AI with psychological practice.</p>
<p>The structure of this paper is organized as follows: Section 2 reviews related work in LLMs and their applications in psychology.Section 3 details the datasets developed for this study, including their creation and validation processes.Section 4 discusses the development and fine-tuning of the PsychoLexLLaMA model.Section 5 presents the evaluation methodology and results, comparing PsychoLexLLaMA with other state-of-the-art models.Section 6 provides a comprehensive discussion of the findings, and Section 7 concludes the paper with insights into future research directions and potential applications.</p>
<p>By exploring the intersection of AI and psychology, this paper aims to unveil the psychological capabilities of LLMs and demonstrate their potential to advance both fields significantly.</p>
<p>Related Works</p>
<p>In this section, we review existing research that benchmarks the capabilities of large language models (LLMs) in Persian, followed by studies that explore the integration of LLMs into psychological research and applications.This dual-focus review establishes the context for our work, emphasizing both the linguistic challenges specific to Persian and the broader implications of applying LLMs in the field of psychology.</p>
<p>Benchmarking Large Language Models for Persian</p>
<p>Recent advancements in large language models (LLMs), particularly ChatGPT, have generated significant interest in their evaluation across various languages and tasks.ChatGPT's performance on various Persian natural language processing tasks is evaluated by Abaskohi et al. (2024).they present a comprehensive evaluation of large language models (LLMs) for the Persian language, focusing on models like GPT-3.5-turbo (OpenAI, 2023a), GPT-4 (OpenAI, 2023b), and OpenChat-3.5.This study, which is the first extensive benchmarking effort for Persian, aims to address the challenges posed by Persian as a low-resource language with unique linguistic features.The evaluation covers a broad range of natural language processing (NLP) tasks, including sentiment analysis, question answering, natural language inference, and translation.the study highlights the model's superior performance in multiple-choice questions(MCQs) related to math and general knowledge from the ParsiNLU dataset (Khashabi et al., 2020).These benchmarks are particularly important for assessing the models' reasoning capabilities in Persian.While ChatGPT-4 excels across several benchmarks, its application in psychology has not been tested, underscoring a critical area for future research.</p>
<p>Khayyam Challenge (PersianMMLU)</p>
<p>Recent advancements have focused on optimizing the performance of Large Language Models (LLMs).The PersianMMLU (Ghahroodi et al., 2024) is particularly significant as it concentrates on the Persian language capabilities of these models.It evaluates their proficiency in answering multiple-choice questions across diverse fields such as mathematics, science, logic, and intelligence testing.This comprehensive evaluation involved advanced models like GPT-3.5, GPT-4(OpenAI, 2023b), Aya (Ustun et al., 2024), Per-sianMind (Rostami et al., 2024), mT0 (Muennighoff et al., 2023), mGPT (Shliazhko et al., 2022), andClaude3-haiku (Anthropic, 2024).</p>
<p>The study utilized a robust dataset derived from Iran's national university entrance exams and educational assessments.While GPT-4 emerged as the superior model, its efficacy in psychological applications remains untested.This gap highlights the necessity of our current research, which aims to specifically evaluate the performance of LLMs in psychology-related scenarios.</p>
<p>2.3</p>
<p>Using large language models in psychology Dubey et al. (2024) explores the integration of LLMs, particularly GPT-3 and GPT-4, into psychological research practices.These models' adeptness at text generation, dialogue engagement, persona simulation, and information synthesis provides innovative approaches to studying various psychological subfields.The primary aim is to evaluate the extent to which LLMs can enrich psychological research methodologies.Despite their potential, LLMs often fall short in delivering contextually accurate advice consistently.This study highlights the importance of refining LLMs through fine-tuning and reinforcement learning from human feedback to ensure their practical efficacy in real-world psychological settings.The extensive datasets used to train these models, encompassing diverse sources of human language data, are aimed at tailoring LLMs to better serve both theoretical and applied psychology.</p>
<p>Exploring the Frontiers of LLMs in Psychological Applications</p>
<p>The application of Artificial Intelligence (AI), especially large language models (LLMs), is revolutionizing psychological research.A study by Ke et al. (2024) underscores significant advances in language models and their profound impact on the field of psychology.LLMs like OpenAI's ChatGPT facilitate various research activities, including literature reviews, hypothesis formulation, experiment design, data analysis, and scholarly writing across several psychological domains such as cognitive, behavioral, clinical, educational, developmental, and social psychology.While these models offer substantial benefits, the review also delineates key technical and ethical challenges, including data privacy concerns and inherent limitations of LLMs.The authors advocate for the careful integration of these technologies in psychological research to enhance our understanding of the human mind and improve the methodologies employed in psychological studies.</p>
<p>Dataset</p>
<p>This section outlines the datasets developed to investigate the application of large language models (LLMs) in psychology.We detail the creation and utilization of three pivotal datasets: the foundational pretraining data, the PsychoLexQA dataset for instructional content, and the PsychoLexEval dataset for evaluating model comprehension and performance.</p>
<p>Pretraining Data</p>
<p>For the pretraining phase, we employed "Introduction to Psychology" by Hilgard (1953), a seminal textbook noted for its comprehensive insights into psychology.This text was used in both its Persian and English versions to establish a bilingual foundation for our models.The dataset comprised approximately 1.3 million tokens, offering a rich and diverse corpus that spans a broad spectrum of psychological topics.This extensive pretraining data enabled our models to develop a deep understanding of essential psychological concepts and terminology, facilitating their application in both Persian and English contexts.</p>
<p>PsychoLexQA</p>
<p>For the instructional dataset, we adopted two distinct methodologies to generate detailed and comprehensive instructional content in both Persian and English.Appendix A demonstrates two examples of the PsychoLexQA dataset.</p>
<p>Document-Based Instructions</p>
<p>The first method involved extracting instructional content from "Introduction to Psychology" in both languages.This process was automated using the GPT-4o model, where paragraphs from the textbook were analyzed to grasp key concepts.For each paragraph, the model generated a series of questions and answers aimed at testing material comprehension.Each question was crafted to be clear and precise, with detailed answers provided to ensure a thorough understanding of the discussed psychological concepts.Paragraphs lacking sufficient content for question generation were identified and noted.This method resulted in a dataset containing 7,055 entries.</p>
<p>Self-Instruct</p>
<p>The second method focused on creating structured instructional tasks for various psychological subcategories in both Persian and English.This involved defining tasks such as "Case Study Analysis", "Experiment Design", and "Data Interpretation" across different psychological subfields like Clinical Psychology and Cognitive Psychology.</p>
<p>For each task and subcategory combination, detailed instructions were generated, including task descriptions, optional inputs, and expected outputs.These tasks were presented in a bilingual format, accommodating both Persian and English speakers.The dataset created using GPT-4o comprises a total of 3,001 rows, ensuring extensive coverage of psychological topics.</p>
<p>PsychoLexEval</p>
<p>The PsychoLexEval dataset, a multiple-choice question and answer (MCQA) format in both Persian and English, is designed to assess the comprehension and performance of LLMs in psychology.This section will describe the data collection and review process, the methods employed to ensure quality and compliance, and the broad scope and coverage of this MCQA dataset.</p>
<p>Data Collection</p>
<p>To construct this dataset, we compiled questions from multiple significant sources: (1) Graduate Entrance Exams: questions from psychology entrance exams (2014-2024) that cover advanced topics;</p>
<p>(2) Employment Exams: questions from various job tests, including both specialized and general psychology;</p>
<p>(3) Online Sources: Questions from trusted psychology test websites; (4) GPT-4 Generated Content: questions from Psychology books, covering a wide range of topics.</p>
<p>Filtering and Review</p>
<p>To ensure high quality and legal compliance, we implemented rigorous filtering and review processes for the dataset.Initially, a human review was conducted where a sample of questions was meticulously scrutinized by experts.This step was crucial to ensure that each question was relevant, complete, and clearly articulated.During this phase, we specifically retained only those questions that had exactly four answer options, ensuring consistency and clarity in the evaluation process.Additionally, to avoid any legal complications, we carefully removed any content that potentially violated copyright laws.This step was essential to maintain the integrity of the dataset and ensure that all included materials were legally compliant for use in our research and broader academic dissemination.These measures collectively reinforced the dataset's reliability and adherence to legal standards, providing a robust foundation for evaluating large language models within psychological contexts.</p>
<p>Scope and Coverage</p>
<p>The PsychoLexEval dataset is meticulously designed to evaluate the comprehension and performance of large language models (LLMs) in the field of psychology, ensuring comprehensive coverage across a wide spectrum of psychological fields.It includes general psychology, focusing on basic concepts; developmental psychology, which examines human growth and cognitive development; and clinical psychology, addressing the diagnosis and treatment of mental disorders.Additionally, the dataset encompasses psychometrics, highlighting methods of measuring psychological attributes; cognitive tests for assessing intelligence and aptitude; and industrial and organizational psychology, which looks at behavior and productivity in the workplace.Further expanding its breadth, the dataset covers social psychology, which explores social behaviors and group dynamics; educational psychology, focusing on learning processes and teaching methods; and the needs of exceptional children with special requirements.It also integrates key concepts from "Introduction to Psychology," covering a range of fundamental topics including biological foundations, sensory processes, learning, memory, motivation, emotion, intelligence, personality, and psychological disorders.</p>
<p>By providing such diverse and extensive content, the PsychoLexEval dataset serves as an invaluable resource for researchers in psychology and artificial intelligence.It equips them with a robust tool to deepen insights into psychological phenomena and advance the field by effectively evaluating the capabilities of LLMs across various psychology domains.The dataset comprises 3,430 rows.Appendix A shows an example of the PsychoLexEval dataset.</p>
<p>PsychoLexLLaMA</p>
<p>In this section, we detail the development of Psy-choLexLLaMA, a specialized large language model (LLM) designed explicitly for psychology.Our goal was to surpass the performance of general-purpose models by optimizing our model to require minimal data and hardware resources.We utilized the Transformers2 library for model development The process of constructing our model is illustrated in Appendix A.</p>
<p>Continuous Pre-Training</p>
<p>For continuous pre-training (Zhou et al., 2024), we employed the LoRA technique (Hu et al., 2021) on the bilingual texts of "Introduction to Psychology" by Hilgard.This foundational work was processed in both Persian and English, leveraging the established pretraining data.We utilized LLaMA 3.1 (Dubey et al., 2024) as our base models in two configurations: 8B and 70B.This stage was critical for aligning the base models with psychological content, thereby enhancing their understanding and application of complex psychological concepts efficiently.The pre-training for the 8B model took 8 minutes using a single A100 80GB GPU, while the 70B model required 41 minutes on two A100 80GB GPUs.Table 1 provides a detailed overview of the LoRA training configurations used during this phase.</p>
<p>Supervised Fine-Tuning</p>
<p>The supervised fine-tuning phase was essential for tailoring our models to meet the specific demands of psychological analysis.Utilizing the PsychoLexQA dataset, which includes both instructional content and a comprehensive set of questions and answers, we applied the LoRA technique to further train the pre-trained models.This phase was pivotal in refining the models' abilities to interpret and respond accurately to intricate psychological queries and scenarios within the dataset.The supervised fine-tuning for the 8B model took 22 minutes using a single A100 GPU, while the 70B model required 32 minutes on two A100 GPUs.The LoRA training configurations used during this phase were the same as those in the continuous pre-training.</p>
<p>Linear Weight Combination</p>
<p>To bolster the final model's robustness and preserve the integrity of previous training advances, we implemented a linear weight combination strategy.This involved merging the weights of the LLaMA 3.1 Instruct model with our continuously pre-trained and finely-tuned models.Each model contributed 50% of its weight to the final composite.This method synergistically combined the foundational capabilities of LLaMA with our newly developed psychological expertise, producing a balanced and potent tool adept at handling sophisticated psychological inquiries.Through these meticulous steps, Psycho-LexLLaMA has been meticulously tailored to meet the unique needs of psychological applications.It stands as a robust resource for researchers and practitioners in both psychology and artificial intelligence, providing a reliable platform for further explorations and advancements in these fields.The next sections will evaluate Psycho-LexLLaMA's performance in detail, comparing it with other models to underscore its enhanced capabilities in the realm of psychological research and practice.</p>
<p>Evaluation</p>
<p>In this study, we conducted a comprehensive evaluation of various language models that operate in both Persian and English, focusing on their ability to understand and accurately respond to psychological questions.The models assessed include include Qwen2 (Yang et al., 2024), Aya-23 (Aryabumi et al., 2024), Phi-3 (Abdin et al., 2024), Llama-3, Llama-3.1 (Dubey et al., 2024), Gemma 1.1 (Team et al., 2024), command-r, Per-sianLLaMA (Abbasi et al., 2023), PersianMind (Rostami et al., 2024b), and PsychoLexLLaMA.</p>
<p>Our focus on open-source models was intended to enhance the accessibility and reproducibility of our findings.The generation configuration for all the LLMs evaluated is consistent across the experiments and is detailed in Table 2.</p>
<p>Zero-shot Setting</p>
<p>In the zero-shot setting, models were tested without any prior contextual examples, relying solely on their pre-existing knowledge.This setting evaluated the models' intrinsic ability to generate accurate responses based solely on their training.</p>
<p>One-shot Setting</p>
<p>The one-shot setting involved presenting each model with a single relevant example before it answered a question.This setting was used to assess the impact of a minimal context on the accuracy of the models, providing insights into their ability to leverage new information quickly</p>
<p>Five-shot Setting</p>
<p>In the five-shot setting, models were given five related examples before responding to questions.This scenario tested the models' capacity to utilize more extensive contextual information to enhance their accuracy, offering a deeper understanding of their learning capabilities.</p>
<p>Evaluation Metric</p>
<p>The effectiveness of each model across the zeroshot, one-shot, and five-shot settings was measured using accuracy as the primary metric.Accuracy was defined as the proportion of correct answers provided by the models relative to the total number of questions posed.This rigorous evaluation approach allowed us to discern the strengths and weaknesses of each model in processing and understanding psychological content comprehensively.</p>
<p>Through these methodical evaluations, we aimed to illustrate the varying capabilities of each model under different contextual conditions.This analysis not only sheds light on how models adapt to incremental information but also highlights their potential applicability in psychological settings, where understanding nuanced human behavior is crucial.</p>
<p>Results</p>
<p>This section outlines the outcomes of our evaluation of selected large language models (LLMs) using the PsychoLexEval dataset in both Persian and English.The primary focus was on assessing the models' proficiency in understanding and responding to psychological questions.</p>
<p>Tables 3 and 4 illustrate the accuracy results of the models on the PsychoLexEval dataset for Persian and English, respectively.These tables quantify how effectively each model comprehends and addresses psychology-related questions across languages.</p>
<p>Discussion</p>
<p>The results from Tables 3 and 4 provide significant insights into the performance of various LLMs, showcasing their competencies in both Persian English.Notably, these findings highlight the influence of model architecture and parameter size on handling specialized tasks, such as interpreting and responding to psychology-related questions.</p>
<p>Performance Trends Across Models</p>
<p>The data reveal substantial variability in performance across models and settings.For instance, the Llama-3.1 Instruct with 70B parameters exhibits superior performance in all scenarios, suggesting a positive correlation between larger parameter sizes and enhanced comprehension and response accuracy.This trend is consistent in the English data, where models with larger parameters, such as Llama-3.1 Instruct 70B, also demonstrate robust performance, especially in zero-shot and five-shot settings.</p>
<p>Conversely, models with fewer parameters sometimes perform well in lower-shot settings but typically exhibit decreased performance as the complexity of tasks increases.For example, the Qwen2 Instruct with 7B parameters faces greater challenges in the Persian context than in English, potentially indicating linguistic or dataset-specific hurdles that are more effectively managed by larger models.</p>
<p>Language-Specific Observations</p>
<p>Our evaluation underscores distinct language-specific differences.In Persian, the increase in model accuracy from zero to five shots is more marked, indicating that Persian language models significantly benefit from added context.Conversely, English language models tend to have higher baseline performances, likely reflecting the advantages of more extensive pre-training datasets available in English.</p>
<p>Impact of Training and Fine-Tuning</p>
<p>The results particularly underscore the critical importance of targeted training and fine-tuning, as seen with the PsychoLexLLaMA models.Designed to surpass its predecessor, Llama 3.1, the 70B PsychoLexLLaMA occasionally does not reach its ambitious targets but consistently matches or exceeds the performance of the original Llama 3.1 model.This consistency indicates that while specific enhancements did not universally lead to improvements, they significantly bolstered the model's capabilities.The 70B version, with its vast parameter count, possesses the capacity to acquire a broader knowledge base, making it challenging to add new knowledge without forgetting previously learned information.Consequently, fine-tuning such a large model demands considerably more data to achieve better outcomes due to its complexity.</p>
<p>In contrast, the 8B version of Psycho-LexLLaMA often outperforms larger models, suggesting that precise, domain-specific fine-tuning can yield remarkable effectiveness, even with fewer parameters.This success highlights the potential of smaller models, particularly when equipped with tailored enhancements for specific applications like psychological evaluations.</p>
<p>The varying impacts of scaling between the 8B and 70B versions suggest that while larger models possess a broad knowledge base enhancing their general performance, strategic fine-tuning is crucial for maximizing efficacy in specialized domains.This observation encourages further research into training strategies that optimize both large and small models for specific tasks, ensuring that they not only retain previous knowledge but also effectively integrate new information.</p>
<p>Conclusion</p>
<p>This study has significantly advanced our understanding of how large language models (LLMs) can be effectively tailored for applications within psychology.Through the integration of specialized psychological content, the development of the PsychoLexQA and PsychoLexEval datasets, and the creation of the PsychoLexLLaMA model, we have demonstrated the substantial benefits of targeted model training and fine-tuning.</p>
<p>Our findings indicate that specific pretraining and fine-tuning strategies substantially enhance the performance of LLMs in psychological settings, underscoring the critical role of thoughtful model architecture and training approaches.Notably, while larger models typically show strong performance, our results reveal that even smaller models can achieve exceptional outcomes when subjected to precise, domain-specific adjustments.This suggests a scalable potential for LLMs in psychological applications that can be adapted to different contexts and constraints.</p>
<p>In conclusion, this research not only sheds light on the current capabilities and challenges of using LLMs in psychology but also sets a foundation for future work.It encourages ongoing refinement of these models to improve their relevance and accuracy, thereby enhancing their utility in real-world psychological applications.Moving forward, we anticipate that continued advancements in model training methodologies and evaluation strategies will drive significant progress in the field, making LLMs an indispensable tool in the arsenal of psychological research and practice.</p>
<p>Limitations</p>
<p>This research has demonstrated the potential of customizing large language models (LLMs) for psychological applications.However, it is crucial to recognize several limitations that could impact the scope and applicability of our findings.The PsychoLexQA and PsychoLexEval datasets, fundamental to our study, inherently contain biases due to the selection of textual materials and question design.These biases may restrict the generalizability of our results to broader psychological contexts and populations.Additionally, the reliance freely licensed sources limits the diversity and depth of psychological topics explored, omitting valuable content protected by copyright laws.</p>
<p>Another significant constraint is the dependence on sophisticated hardware for model training.The need for high-performance GPUs poses a considerable barrier, particularly for researchers with limited access to such resources, affecting both the replicability of our results and the broader research community's ability to engage with and expand upon our work.Moreover, while this study aims to enhance model performance with minimal data and hardware resources, achieving optimal efficiency under these constraints remains a challenge.Balancing resource conservation with model capability often requires compromises that may detract from the models' utility in practical psychological applications.</p>
<p>By addressing these limitations, future research can focus on broadening the diversity of training data and developing more resource-efficient modeling techniques, thereby enhancing the practical deployment of LLMs in psychology and related fields.Instruct: Design an experiment to investigate the effects of different types of feedback (positive and negative) on learning outcomes.Include methods for measuring student performance.</p>
<p>A Dataset Samples</p>
<p>Output: The experiment will involve three groups of students: one receiving only positive feedback, another receiving only negative feedback, and a third receiving no feedback at all.Each group will engage in a learning task, such as a math problem-solving activity, with their performance assessed through test scores and self-reported motivation levels.The hypothesis posits that students receiving positive feedback will outperform their peers in both task performance and motivation.This experiment will offer insights into the impact of feedback on learning and educational practices.</p>
<p>Figures 1
1
Figures 1 shows an example of PsychoLexQA dataset based on psychology document.Figure 2 illustrates an example of PsychoLexQA dataset which has been created using a self-instruct method.Figure3shows an example of the Psycho-LexEval dataset that includes a multiple-choice question and answer.Figure4depicts the process involved in constructing the PsychoLexLLaMA.</p>
<p>Figure 1 :
1
Figure 1: Example of PsychoLexQA dataset (Document-based).</p>
<p>Figure 2 :
2
Figure 2: Example of PsychoLexQA dataset (Selfinstruct).</p>
<p>Figure 3 :
3
Figure 3: Example of PsychoLexEval dataset.</p>
<p>Figure 4 :
4
Figure 4: Process of constructing PsychoLexLLaMA model.</p>
<p>Table 1 :
1
LoRA training configurations
LrRankAlphaDropout1e-58160.0</p>
<p>Table 2 :
2
Generation configurations for all evaluated LLMs.</p>
<p>Table 3 :
3
Accuracy of LLMs on the PsychoLexEval dataset in Persian.
AccuracyModel# Param0-shot1-shot5-shotAvgQwen2 Instruct7B03.5506.1808.636.12Gemma 1.1 it7B43.0740.6827.5737.11PersianMind7B35.7835.9624.6332.12Aya-238B39.6441.4227.0236.03Llama-3 Instruct8B33.8810.6634.4926.34Llama-3.1 Instruct8B45.8941.3635.7841.01PsychoLexLLaMA-pretrain-sft8B47.3043.1346.6145.68PsychoLexLLaMA-average8B48.5241.9747.0545.85PersianLLaMA13B20.1318.5219.8919.51Aya-2335B21.0710.4722.6918.08c4ai-command-r-v0135B35.9621.7546.2034.64Llama-3 Instruct70B19.5409.310.59.78Llama-3.1 Instruct70B70.3467.8370.4069.52PsychoLexLLaMA-pretrain-sft70B67.7945.3468.0760.4PsychoLexLLaMA-average70B65.8453.0669.6662.85Qwen2 Instruct72B31.3705.8250.329.16AccuracyModel# Param0-shot1-shot5-shotAvgQwen2 Instruct7B89.3142.7483.7671.94Gemma 1.1 it7B84.7555.0665.8668.56Aya-238B73.6233.8077.0561.49Llama-3 Instruct8B85.7778.5768.2277.52Llama-3.1 Instruct8B88.9789.258788.41PsychoLexLLaMA-pretrain-sft8B88.9781.2162.0377.4PsychoLexLLaMA-average8B90.1089.0390.0489.72Aya-2335B81.3279.028280.78c4ai-command-r-v0135B8778.0675.0880.05Llama-3 Instruct70B90.5588.5876.7785.3Llama-3.1 Instruct70B93 . 0292 . 6392.192.58PsychoLexLLaMA-pretrain-sft70B91.4590.2490.8590.85PsychoLexLLaMA-average70B92.1391.8591.8791.95Qwen2 Instruct72B91.1173.7992.2985.73</p>
<p>Table 4 :
4
Accuracy of LLMs on the PsychoLexEval dataset in English.</p>
<p>https://github.com/huggingface/transformers</p>
<p>. A Abaskohi, S Baruni, M Masoudi, N Abbasi, M H Babalou, A Edalat, S Kamahi, S M Sani, N Naghavian, D Namazifard, P Sadeghi, Y Yaghoobzadeh, 2024</p>
<p>Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT. ArXiv, abs/2404.02403</p>
<p>PersianLLaMA: Towards Building First Persian Large Language Model. M A Abbasi, A Ghafouri, M Firouzmandi, H Naderi, B Minaei-Bidgoli, ArXiv, abs/2312.157132023</p>
<p>M Abdin, S A Jacobs, A A Awan, J Aneja, A Awadallah, H Awadalla, N Bach, A Bahree, A Bakhtiari, H Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. S Agrawal, ArXiv, abs/2303.12810. AnthropicThe Claude 3 Model Family: Opus, Sonnet. Haiku2023. 2024</p>
<p>V Aryabumi, J Dang, D Talupuru, S Dash, D Cairuz, H Lin, B Venkitesh, M Smith, K Marchisio, S Ruder, arXiv:2405.15032Aya 23: Open weight releases to further multilingual progress. 2024arXiv preprint</p>
<p>. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, . . Zhao, Z , 2024The Llama 3 Herd of Models</p>
<p>. O Ghahroodi, M Nouri, M V Sanian, A Sahebi, D Dastgheib, E Asgari, M S Baghshah, M H Rohban, 2024</p>
<p>Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?. ArXiv, abs/2404.06644</p>
<p>Z Guo, R Jin, C Liu, Y Huang, D Shi, Supryadi, L Yu, Y Liu, J Li, B Xiong, D Xiong, ArXiv, abs/2310Evaluating Large Language Models: A Comprehensive Survey. 2023. 19736</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. J E Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, W Chen, ArXiv, abs/2106.096852021</p>
<p>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. L Ke, S Tong, P Cheng, K Peng, ArXiv, abs/2401.015192024</p>
<p>. D Khashabi, A Cohan, S Shakeri, P Hosseini, P Pezeshkpour, M Alikhani, M Aminnaseri, M Bitaab, F Brahman, S Ghazarian, M Gheini, A Kabiri, R K Mahabadi, O Memarrast, A Mosallanezhad, E Noury, S Raji, M S Rasooli, S Sadeghi, </p>
<p>ParsiNLU: A Suite of Language Understanding Challenges for Persian. Y Yaghoobzadeh, Transactions of the Association for Computational Linguistics. 92020</p>
<p>Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. T Lai, Y Shi, Z Du, J Wu, K Fu, Y Dou, Z Wang, ArXiv, abs/2307.119912023</p>
<p>Large Language Models: A Survey. S Minaee, T Mikolov, N Nikzad, M A Chenaghlu, R Socher, X Amatriain, J Gao, ArXiv, abs/2402.061962024</p>
<p>Crosslingual Generalization through Multitask Finetuning. N Muennighoff, T Wang, L Sutawika, A Roberts, S Biderman, T L Scao, M S Bari, S Shen, Z.-X Yong, H Schoelkopf, X Tang, D R Radev, A F Aji, K Almubarak, S Albanie, Z Alyafeai, A Webson, E Raff, C Raffel, Annual Meeting of the Association for Computational Linguistics. 2023. 2023aGpt-3.5.</p>
<p>Openai, Gpt-4 technical report. 2023b</p>
<p>PersianMind: A Cross-Lingual Persian-English Large Language Model. P Rostami, A Salemi, M J Dousti, ArXiv, abs/2401.064662024</p>
<p>mGPT: Few-Shot Learners Go Multilingual. O Shliazhko, A Fenogenova, M Tikhonova, V Mikhailov, A Kozlova, T Shavrina, Transactions of the Association for Computational Linguistics. 122022</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model. A Ustun, V Aryabumi, Z.-X Yong, W.-Y Ko, D D'souza, G Onilude, N Bhandari, S Singh, H.-L Ooi, A Kayid, F Vargus, P Blunsom, S Longpre, N Muennighoff, M Fadaee, J Kreutzer, S Hooker, ArXiv, abs/2402.078272024</p>
<p>Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, A H Awadallah, R W White, D Burger, C Wang, AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. 2023</p>
<p>A Yang, B Yang, B Hui, B Zheng, B Yu, C Zhou, C Li, C Li, D Liu, F Huang, arXiv:2407.10671Qwen2 technical report. 2024arXiv preprint</p>
<p>D.-W Zhou, H.-L Sun, J Ning, H.-J Ye, D.-C Zhan, ArXiv, abs/2401.16386Continual Learning with Pre-Trained Models: A Survey. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7781 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7781</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7781</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-bb5f873632616c2cdc07ef1bb139db0c96c8e5f6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bb5f873632616c2cdc07ef1bb139db0c96c8e5f6" target="_blank">Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs), and reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing real-world scientific research.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem-solving tasks. Their ability to understand and generate human-like text has opened up new possibilities for advancing scientific research, enabling tasks such as data analysis, literature review, and even experimental design. One of the most promising applications of LLMs in this context is hypothesis generation, where they can identify novel research directions by analyzing existing knowledge. However, despite their potential, LLMs are prone to generating ``hallucinations'', outputs that are plausible-sounding but factually incorrect. Such a problem presents significant challenges in scientific fields that demand rigorous accuracy and verifiability, potentially leading to erroneous or misleading conclusions. To overcome these challenges, we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured reasoning process, organizing their output as a chain of ideas (CoI), and includes a KG-supported module for the detection of hallucinations. With experiments on our newly constructed hypothesis generation dataset, we demonstrate that KG-CoI not only improves the accuracy of LLM-generated hypotheses but also reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing real-world scientific research.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7781.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7781.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-CoI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Grounded Chain of Ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-module system that augments LLM hypothesis generation with knowledge graph (KG) evidence, producing a stepwise chain-of-ideas (CoI) and performing KG‑based verification of each reasoning step to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B / mini / (GPT-4o unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KG-CoI evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Generate hypotheses via an LLM augmented with KG neighbors and literature, produce a stepwise chain-of-ideas, verify each step against a domain KG using an LLM verifier (LLM-V), and aggregate per-step correctness into a confidence score; performance compared against Direct, CoT and RAG baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1, Confidence (proportion of CoI steps verified by KG), exact-match ratios for claim verification annotators</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy/F1: standard classification metrics reported as percentages; Confidence = (1/N) * sum(correctness(s_i)) where correctness(s_i) is 1 if a KG triple exists and LLM-V marks the claim verifiable, else 0; exact-match ratios reported for annotator/LLM agreement (unitless proportion).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Constructed hypothesis generation dataset (masked relations from PubTator3; 300 instances: 100 'stimulate', 100 'inhibit', 100 'no_relation')</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Human annotators used to validate claim-verification samples: 100 sampled (claim, triple) instances were manually annotated; two human annotators (H1, H2) produced exact-match agreement values reported in Table 4; these were used to justify selection of LLM-V.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>KG-CoI outperforms baselines; example: GPT-4o KG-CoI accuracy 86.00%, F1 85.83%, average confidence increase ~3.30% over CoT; detailed per-model results in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>KG coverage limits detection (some literature evidence lacks direct KG triples) causing fluctuating confidence; LLM-V selection trades off cost and fidelity; verification considers only direct KG links (no multi-hop verification) which could miss valid indirect support.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7781.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-supported hallucination detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph-Supported Hallucination Detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that verifies each reasoning step (s_i) in the LLM-generated chain-of-ideas by locating entity pairs in a domain KG and using an LLM verifier to decide whether the KG triple supports the claim; used to compute a hypothesis confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-V (implemented with GPT-4o-mini in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o-mini (size/version as provided by vendor)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Claim verification / Hypothesis validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KG triple verification with LLM verifier</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For each CoI step, extract entities (NER), retrieve direct KG triples among them, and call an LLM verifier (LLM-V) to output 1 (verified) or 0 (not verified). The overall hypothesis confidence is the mean of per-step correctness flags.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Confidence (proportion of steps verified), per-step correctness (binary), exact-match agreement between LLM-V and human annotators for verification</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Per-step correctness: binary (0/1). Confidence: mean of per-step correctness across N steps (range 0–1), reported as percentage. Exact-match: proportion of verifier outputs matching human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Uses the same constructed PubTator3-based dataset for hypothesis instances and PubTator3 KG for verification</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>100 (claim, triple) samples manually labeled by two human annotators; exact-match agreement matrix reported comparing H1, H2, GPT-3.5, GPT-4o-mini, GPT-4o (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>LLM-V (GPT-4o-mini) had reasonable agreement with human annotators (exact-match ~0.72–0.82 depending on comparator); chosen as cost-effective verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Verification only considers direct triples in the KG (no multi-hop reasoning); KG incompleteness causes unverifiable but possibly true claims; verifier accuracy depends on LLM-V quality and incurs cost tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7781.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confidence metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-based Hypothesis Confidence Score</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scalar confidence score for a generated hypothesis defined as the mean of binary correctness flags for each step in the chain-of-ideas, reflecting the fraction of reasoning steps supported by KG evidence and LLM-V verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (LLM-G outputs verified by LLM-V)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis reliability score</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Mean per-step KG verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute correctness(s_i) for each CoI step via KG lookup + LLM-V; confidence(H) = (1/N) sum_i correctness(s_i); reported as percentage and used to quantify hallucination rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Confidence (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Confidence ∈ [0,1] (reported as %); fractional proportion of CoI steps verified (1 = all steps verified by KG+LLM-V).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to hypotheses for the 300-instance constructed dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No separate human rating scale for confidence; human annotations used to validate LLM-V behavior used within the confidence pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported confidence values per-model/method in Table 1 (e.g., GPT-4o KG-CoI confidence 44.24% (greedy) and 41.66% (self-consistency)).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Confidence depends on KG completeness and LLM-V judgments; a low score may reflect KG gaps rather than incorrect reasoning; only direct-triple support counted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7781.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constructed dataset (PubTator3 masked)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constructed hypothesis generation dataset derived from PubTator3 (masked relation links)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 300-instance test set created by removing selected edges from the PubTator3 KG to simulate hypothesis generation with incomplete knowledge; balanced across three classes (100 'stimulate', 100 'inhibit', 100 'no_relation').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated across multiple LLMs (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B / mini / (GPT-4o unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Hypothesis generation benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Masked-edge link prediction evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Mask a known PubTator3 relation between two entities; provide subject and object to the model and ask it to predict 'stimulate', 'inhibit' or 'no_relation'; compare predictions to ground truth labels from PubTator3.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1 (class-based), model confidence from KG-supported verification</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy and F1 reported as percentages over 300 instances; dataset is balanced across three classes; ground truth is the removed PubTator3 relation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubTator3-derived 300-instance dataset (constructed by authors)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Dataset construction used publication counts as selection criteria; no human rating of final hypotheses reported beyond verifier annotation experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to produce all main quantitative results (Table 1): e.g., GPT-4o KG-CoI accuracy 86.00%, F1 85.83% on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Dataset simulates novel hypotheses by masking known KG links but still relies on existing KG structure; selection heuristics (publication-count thresholds) may bias examples toward well-studied relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7781.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubTator3 (KG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubTator 3.0 knowledge graph (PubTator3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biomedical knowledge resource built from literature, used here both as the authoritative KG for retrieving neighbor relations and as the source of ground-truth relations masked to build the evaluation dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (knowledge resource)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Knowledge graph / dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KG-based verification and dataset construction</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Used to retrieve k-step neighbor relation chains for query enrichment and to provide ground-truth edges for masked-edge evaluation; also used during KG-supported hallucination detection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used as reference for binary per-step correctness when a triple (head, relation, tail) exists in the KG</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Existence boolean for KG triples used to mark claims as potentially verifiable before LLM-V confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubTator3 (Wei et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Not applicable to PubTator3 itself; human annotation used elsewhere to validate verifier alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Central resource for dataset creation and verification; examples in case studies show neighbor relation retrieval aiding correct hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>KG incompleteness can cause valid claims to be unverified; edges reflect literature-extracted relations which have their own extraction noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7781.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMed (corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubMed biomedical literature corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The large biomedical abstracts corpus used as the textual retrieval source (Lit-R) for literature evidence during LLM context augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Biology</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Text corpus for retrieval-augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Retrieval-augmented generation (lit retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Keywords generated by LLM-E are used to retrieve documents from PubMed via a text retriever; retrieved documents augment LLM-G context for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Indirectly evaluated via downstream accuracy/F1 and confidence improvements when literature info is included/ablated.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Contribution measured by difference in Accuracy/F1 when literature retrieval is included vs ablated (percent change).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed abstracts (as corpus for Lit-R)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None specific; used automatically in retrieval pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Including literature retrieval improved performance on weaker models substantially (ablation: removal of literature caused up to ~9.66% drop on Llama-3.1-8B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Text retrieval might retrieve evidence not directly reflected in KG, causing verifier to mark steps as unverifiable; tokenization issues motivate BM25 choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7781.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 probabilistic retriever</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse lexicon-based retrieval algorithm used as the Lit-R text retriever for PubMed, chosen because it handles domain-specific tokens (e.g., gene names) better than some dense retrievers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25 (retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Information retrieval / Biomedical retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Retrieval method used in evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>BM25-based literature retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Given keywords from LLM-E, BM25 retrieves top-matching PubMed abstracts to include as textual evidence for LLM-G; ablated to test literature contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Measured indirectly via downstream Accuracy/F1 differences with and without BM25-based Lit-R.</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Performance delta (percentage points) in classification metrics attributable to including BM25-retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed abstracts</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Ablation removing literature retrieval (i.e., Lit-R/BM25) caused notable drops in performance, especially for smaller LLMs (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Sparse retrieval can miss semantic matches; dense retrievers may mis-handle biomedical tokens leading to errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7781.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lit-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Literature Retriever (Lit-R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The literature retrieval subsystem in KG-CoI combining query keywords from LLM-E with BM25 over PubMed to obtain textual context for hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pipeline component (supports LLM-G)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Retrieval-augmented LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Retrieval submodule for hypothesis evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Lit-R augmentation ablation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Ablation experiments remove Lit-R to quantify the contribution of literature context to hypothesis accuracy and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1 deltas when Lit-R is removed</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent point change in Accuracy/F1 relative to full KG-CoI baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubMed + BM25 retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Removal of literature information decreased performance across LLMs; effect size largest for smaller models (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Quality of retrieved documents varies; some retrieved literature statements may not map to KG triples leading to unverifiable steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7781.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-R</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Retriever (KG-R)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG retriever that performs breadth-first search for k-step relation chains in the domain KG (PubTator3) to supply neighbor relations R used for query enrichment and LLM context augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KG lookup component</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Knowledge graph augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Retrieval submodule for KG evidence</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>KG neighbor retrieval ablation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrieves k-step neighbor relation chains for entities in queries; ablation (removal of KG relations) quantifies impact on hypothesis generation accuracy/F1.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1 deltas when KG-R is removed</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Percent point change in Accuracy/F1 relative to full KG-CoI. Also used to compute per-step correctness candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>PubTator3 KG</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Removal of KG information causes the largest performance drops for stronger LLMs like Llama-3.1-70B and GPT-4o (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Only direct links retrieved are considered for per-step verification; multi-hop inference beyond k steps may be necessary for some valid reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7781.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baselines: Direct / CoT / RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline prompting and augmentation methods: Direct prompting, Chain-of-Thought (CoT) prompting, Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline evaluation frameworks used for comparison: Direct (single-step answer), CoT (stepwise reasoning prompting), and RAG (literature-augmented generation) to measure gains from KG-CoI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various LLMs tested under each baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B / mini / (GPT-4o unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Prompting and augmentation frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Baseline comparison (Direct, CoT, RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run each LLM under Direct, CoT, and RAG settings (with/without KG augmentation) and compare classification Accuracy, F1, and CI/confidence metrics to assess impact of reasoning and external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1, Confidence</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy/F1 as classification scores (percent); Confidence as KG-verification proportion.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Constructed PubTator3-based dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None beyond verifier annotation experiment</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>KG-CoI improves over Direct, CoT, and RAG across LLMs (Table 1); e.g., Llama-3.1-70B: Direct 72.00% -> KG-CoI 79.33% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>CoT can produce plausible but unverified reasoning; RAG depends on retrieved literature that may not align with KG evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7781.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency ensemble for chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation technique where the model is run multiple times with randomness to generate multiple reasoning chains and a majority-vote/consensus decision is used, tested here at runs N = 1,5,10,15 to evaluate robustness and accuracy improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to multiple LLMs (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B / mini / (GPT-4o unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>LLM reasoning evaluation / Biomedical</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Ensembling method for hypothesis decisions</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Self-consistency ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Run LLM multiple times to generate multiple CoI outputs, aggregate via self-consistency (majority/consensus) to choose final answer; measure how accuracy/F1 scale with number of runs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy, F1 as a function of number of runs</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy/F1 reported for N runs; improvement measured in percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Constructed PubTator3-based dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Self-consistency improves KG-CoI performance for most models (except some anomalous behavior for Llama-3.1-8B), and scaling runs N up to 15 shows consistent gains (Appendix Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Increased runs introduce randomness that may reduce confidence measures; computational cost increases with number of runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7781.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accuracy & F1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accuracy and F1-score (classification metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary automated performance metrics for hypothesis-classification (stimulate / inhibit / no_relation) used throughout experiments to quantify correctness of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reported for each tested LLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (8B, 70B, mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / Machine learning evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Quantitative evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Classification metrics (Accuracy, macro/micro F1 as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compare predicted label against ground-truth masked PubTator3 relation; report overall Accuracy and F1 scores (presented as percentages in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%), F1 (%)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Accuracy = (correct predictions) / (total) * 100. F1 is the harmonic mean of precision and recall; reported as percentage.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Constructed PubTator3-based dataset (300 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>No human scoring for these metrics; computed automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 1 and Table 3 enumerate Accuracy and F1 for methods and ablations; e.g., GPT-4o KG-CoI Acc 86.00%, F1 85.83%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Accuracy/F1 reflect match to masked KG labels (not real-world downstream experimental validation); do not capture novelty or scientific plausibility beyond label correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7781.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-V (GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM verifier agent (implemented with GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent chosen to perform claim verification by judging whether a KG triple supports an LLM-generated claim; selected for a balance of cost and alignment with human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini (used as LLM-V)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o-mini (vendor-defined mini variant)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Automated claim verifier</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>LLM-based claim verification with human agreement benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Sample (claim, relation triple) pairs and have LLM-V decide 'yes'/'no' whether triple supports claim; compare LLM-V outputs to two human annotators to assess alignment and choose verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match ratio (agreement) with human annotators; used within confidence computations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Exact-match = fraction of verifier outputs matching human label; reported as decimal fractions in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>100 sampled (claim, triple) instances from actual hallucination detection process</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Two human annotators (H1, H2) labeled 100 samples; Table 4 reports pairwise exact-match matrix between H1, H2, GPT-3.5, GPT-4o-mini, GPT-4o and prices per 1M tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4o-mini exact-match with H1: 0.74, with H2: 0.72; chosen as cost-effective verifier (price shown in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM-V (GPT-4o-mini) has lower agreement than GPT-4o but acceptable alignment with human annotators, justifying selection on cost-performance tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Verifier alignment imperfect; selection based on limited sample (100 instances) and cost considerations; verifier errors propagate to confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7781.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation studies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Component ablation analysis (KG, literature, query enrichment, chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic removal of KG, literature, query enrichment, or chain-of-thought components from KG-CoI to quantify each component's contribution to hypothesis accuracy and F1 across LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated across multiple LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B / 70B / mini / GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical / System analysis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Ablation evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Ablation testing</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Remove one system component at a time and measure change in Accuracy/F1 relative to the full KG-CoI system to determine importance of each module.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Delta in Accuracy and F1 (percentage points)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Reported percent-point decrease in Accuracy and F1 when component removed (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Constructed PubTator3-based dataset</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>None</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Removal of KG information caused largest drops for some LLMs; removal of literature impacted weaker LLMs most (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Ablations measure impact on automatic metrics but do not directly measure scientific novelty or real-world experimental validity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7781.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7781.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human annotation / exact-match evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotation and exact-match agreement evaluation for LLM-V selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Manual labeling of a 100-sample subset of (claim, triple) pairs by two human annotators to evaluate alignment (exact-match) of candidate LLM verifiers (GPT-3.5, GPT-4o-mini, GPT-4o) with human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human annotators and evaluated LLM verifiers (GPT-3.5, GPT-4o-mini, GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Annotation quality / verifier benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Human evaluation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Exact-match agreement matrix</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute pairwise exact-match ratios between two human annotators and three LLM verifiers for 100 samples; use agreement to assess verifier choice and inter-annotator reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Exact-match ratios (pairwise agreement values)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Exact-match = fraction of samples where two annotators/LLMs gave the same label; values in Table 4 range 0–1.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>100 sampled (claim, triple) instances from hallucination detection outputs</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Two human annotators labeled the samples (H1, H2); pairwise exact-match with GPT-3.5: 0.70–0.82 range shown; price per 1M tokens also reported for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>GPT-4o had best agreement with humans; GPT-4o-mini chosen as cost-effective compromise (exact-match ~0.72–0.80 vs humans).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td>LLM verifiers show lower but reasonable agreement with human annotators; human-human agreement ~0.90, indicating annotator reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small sample size (100) for verifier selection; exact-match does not capture nuanced partial agreement or confidence; cost considerations influenced final verifier choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge <em>(Rating: 2)</em></li>
                <li>Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7781",
    "paper_id": "paper-bb5f873632616c2cdc07ef1bb139db0c96c8e5f6",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "KG-CoI",
            "name_full": "Knowledge Grounded Chain of Ideas",
            "brief_description": "A multi-module system that augments LLM hypothesis generation with knowledge graph (KG) evidence, producing a stepwise chain-of-ideas (CoI) and performing KG‑based verification of each reasoning step to reduce hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)",
            "model_size": "8B / 70B / mini / (GPT-4o unspecified)",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Hypothesis generation",
            "evaluation_method_name": "KG-CoI evaluation pipeline",
            "evaluation_method_description": "Generate hypotheses via an LLM augmented with KG neighbors and literature, produce a stepwise chain-of-ideas, verify each step against a domain KG using an LLM verifier (LLM-V), and aggregate per-step correctness into a confidence score; performance compared against Direct, CoT and RAG baselines.",
            "evaluation_metric": "Accuracy, F1, Confidence (proportion of CoI steps verified by KG), exact-match ratios for claim verification annotators",
            "metric_definition": "Accuracy/F1: standard classification metrics reported as percentages; Confidence = (1/N) * sum(correctness(s_i)) where correctness(s_i) is 1 if a KG triple exists and LLM-V marks the claim verifiable, else 0; exact-match ratios reported for annotator/LLM agreement (unitless proportion).",
            "dataset_or_benchmark": "Constructed hypothesis generation dataset (masked relations from PubTator3; 300 instances: 100 'stimulate', 100 'inhibit', 100 'no_relation')",
            "human_evaluation_details": "Human annotators used to validate claim-verification samples: 100 sampled (claim, triple) instances were manually annotated; two human annotators (H1, H2) produced exact-match agreement values reported in Table 4; these were used to justify selection of LLM-V.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "KG-CoI outperforms baselines; example: GPT-4o KG-CoI accuracy 86.00%, F1 85.83%, average confidence increase ~3.30% over CoT; detailed per-model results in Table 1.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "KG coverage limits detection (some literature evidence lacks direct KG triples) causing fluctuating confidence; LLM-V selection trades off cost and fidelity; verification considers only direct KG links (no multi-hop verification) which could miss valid indirect support.",
            "uuid": "e7781.0",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "KG-supported hallucination detection",
            "name_full": "Knowledge Graph-Supported Hallucination Detection",
            "brief_description": "A module that verifies each reasoning step (s_i) in the LLM-generated chain-of-ideas by locating entity pairs in a domain KG and using an LLM verifier to decide whether the KG triple supports the claim; used to compute a hypothesis confidence score.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM-V (implemented with GPT-4o-mini in experiments)",
            "model_size": "GPT-4o-mini (size/version as provided by vendor)",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Claim verification / Hypothesis validation",
            "evaluation_method_name": "KG triple verification with LLM verifier",
            "evaluation_method_description": "For each CoI step, extract entities (NER), retrieve direct KG triples among them, and call an LLM verifier (LLM-V) to output 1 (verified) or 0 (not verified). The overall hypothesis confidence is the mean of per-step correctness flags.",
            "evaluation_metric": "Confidence (proportion of steps verified), per-step correctness (binary), exact-match agreement between LLM-V and human annotators for verification",
            "metric_definition": "Per-step correctness: binary (0/1). Confidence: mean of per-step correctness across N steps (range 0–1), reported as percentage. Exact-match: proportion of verifier outputs matching human annotation.",
            "dataset_or_benchmark": "Uses the same constructed PubTator3-based dataset for hypothesis instances and PubTator3 KG for verification",
            "human_evaluation_details": "100 (claim, triple) samples manually labeled by two human annotators; exact-match agreement matrix reported comparing H1, H2, GPT-3.5, GPT-4o-mini, GPT-4o (Table 4).",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "LLM-V (GPT-4o-mini) had reasonable agreement with human annotators (exact-match ~0.72–0.82 depending on comparator); chosen as cost-effective verifier.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Verification only considers direct triples in the KG (no multi-hop reasoning); KG incompleteness causes unverifiable but possibly true claims; verifier accuracy depends on LLM-V quality and incurs cost tradeoffs.",
            "uuid": "e7781.1",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Confidence metric",
            "name_full": "KG-based Hypothesis Confidence Score",
            "brief_description": "A scalar confidence score for a generated hypothesis defined as the mean of binary correctness flags for each step in the chain-of-ideas, reflecting the fraction of reasoning steps supported by KG evidence and LLM-V verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (LLM-G outputs verified by LLM-V)",
            "model_size": "various",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Hypothesis reliability score",
            "evaluation_method_name": "Mean per-step KG verification",
            "evaluation_method_description": "Compute correctness(s_i) for each CoI step via KG lookup + LLM-V; confidence(H) = (1/N) sum_i correctness(s_i); reported as percentage and used to quantify hallucination rate.",
            "evaluation_metric": "Confidence (percentage)",
            "metric_definition": "Confidence ∈ [0,1] (reported as %); fractional proportion of CoI steps verified (1 = all steps verified by KG+LLM-V).",
            "dataset_or_benchmark": "Applied to hypotheses for the 300-instance constructed dataset",
            "human_evaluation_details": "No separate human rating scale for confidence; human annotations used to validate LLM-V behavior used within the confidence pipeline.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": false,
            "reported_results": "Reported confidence values per-model/method in Table 1 (e.g., GPT-4o KG-CoI confidence 44.24% (greedy) and 41.66% (self-consistency)).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Confidence depends on KG completeness and LLM-V judgments; a low score may reflect KG gaps rather than incorrect reasoning; only direct-triple support counted.",
            "uuid": "e7781.2",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Constructed dataset (PubTator3 masked)",
            "name_full": "Constructed hypothesis generation dataset derived from PubTator3 (masked relation links)",
            "brief_description": "A 300-instance test set created by removing selected edges from the PubTator3 KG to simulate hypothesis generation with incomplete knowledge; balanced across three classes (100 'stimulate', 100 'inhibit', 100 'no_relation').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated across multiple LLMs (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)",
            "model_size": "8B / 70B / mini / (GPT-4o unspecified)",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Hypothesis generation benchmark",
            "evaluation_method_name": "Masked-edge link prediction evaluation",
            "evaluation_method_description": "Mask a known PubTator3 relation between two entities; provide subject and object to the model and ask it to predict 'stimulate', 'inhibit' or 'no_relation'; compare predictions to ground truth labels from PubTator3.",
            "evaluation_metric": "Accuracy, F1 (class-based), model confidence from KG-supported verification",
            "metric_definition": "Accuracy and F1 reported as percentages over 300 instances; dataset is balanced across three classes; ground truth is the removed PubTator3 relation.",
            "dataset_or_benchmark": "PubTator3-derived 300-instance dataset (constructed by authors)",
            "human_evaluation_details": "Dataset construction used publication counts as selection criteria; no human rating of final hypotheses reported beyond verifier annotation experiment.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Used to produce all main quantitative results (Table 1): e.g., GPT-4o KG-CoI accuracy 86.00%, F1 85.83% on this dataset.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Dataset simulates novel hypotheses by masking known KG links but still relies on existing KG structure; selection heuristics (publication-count thresholds) may bias examples toward well-studied relations.",
            "uuid": "e7781.3",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "PubTator3 (KG)",
            "name_full": "PubTator 3.0 knowledge graph (PubTator3)",
            "brief_description": "A biomedical knowledge resource built from literature, used here both as the authoritative KG for retrieving neighbor relations and as the source of ground-truth relations masked to build the evaluation dataset.",
            "citation_title": "PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge",
            "mention_or_use": "use",
            "model_name": "N/A (knowledge resource)",
            "model_size": "",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Knowledge graph / dataset",
            "evaluation_method_name": "KG-based verification and dataset construction",
            "evaluation_method_description": "Used to retrieve k-step neighbor relation chains for query enrichment and to provide ground-truth edges for masked-edge evaluation; also used during KG-supported hallucination detection.",
            "evaluation_metric": "Used as reference for binary per-step correctness when a triple (head, relation, tail) exists in the KG",
            "metric_definition": "Existence boolean for KG triples used to mark claims as potentially verifiable before LLM-V confirmation.",
            "dataset_or_benchmark": "PubTator3 (Wei et al., 2024)",
            "human_evaluation_details": "Not applicable to PubTator3 itself; human annotation used elsewhere to validate verifier alignment.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Central resource for dataset creation and verification; examples in case studies show neighbor relation retrieval aiding correct hypotheses.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "KG incompleteness can cause valid claims to be unverified; edges reflect literature-extracted relations which have their own extraction noise.",
            "uuid": "e7781.4",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "PubMed (corpus)",
            "name_full": "PubMed biomedical literature corpus",
            "brief_description": "The large biomedical abstracts corpus used as the textual retrieval source (Lit-R) for literature evidence during LLM context augmentation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "N/A (corpus)",
            "model_size": "",
            "scientific_domain": "Biomedical / Biology",
            "theory_type": "Text corpus for retrieval-augmentation",
            "evaluation_method_name": "Retrieval-augmented generation (lit retrieval)",
            "evaluation_method_description": "Keywords generated by LLM-E are used to retrieve documents from PubMed via a text retriever; retrieved documents augment LLM-G context for hypothesis generation.",
            "evaluation_metric": "Indirectly evaluated via downstream accuracy/F1 and confidence improvements when literature info is included/ablated.",
            "metric_definition": "Contribution measured by difference in Accuracy/F1 when literature retrieval is included vs ablated (percent change).",
            "dataset_or_benchmark": "PubMed abstracts (as corpus for Lit-R)",
            "human_evaluation_details": "None specific; used automatically in retrieval pipeline.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Including literature retrieval improved performance on weaker models substantially (ablation: removal of literature caused up to ~9.66% drop on Llama-3.1-8B).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Text retrieval might retrieve evidence not directly reflected in KG, causing verifier to mark steps as unverifiable; tokenization issues motivate BM25 choice.",
            "uuid": "e7781.5",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "BM25",
            "name_full": "BM25 probabilistic retriever",
            "brief_description": "A sparse lexicon-based retrieval algorithm used as the Lit-R text retriever for PubMed, chosen because it handles domain-specific tokens (e.g., gene names) better than some dense retrievers.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond",
            "mention_or_use": "use",
            "model_name": "BM25 (retriever)",
            "model_size": "",
            "scientific_domain": "Information retrieval / Biomedical retrieval",
            "theory_type": "Retrieval method used in evaluation pipeline",
            "evaluation_method_name": "BM25-based literature retrieval",
            "evaluation_method_description": "Given keywords from LLM-E, BM25 retrieves top-matching PubMed abstracts to include as textual evidence for LLM-G; ablated to test literature contribution.",
            "evaluation_metric": "Measured indirectly via downstream Accuracy/F1 differences with and without BM25-based Lit-R.",
            "metric_definition": "Performance delta (percentage points) in classification metrics attributable to including BM25-retrieved documents.",
            "dataset_or_benchmark": "PubMed abstracts",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Ablation removing literature retrieval (i.e., Lit-R/BM25) caused notable drops in performance, especially for smaller LLMs (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Sparse retrieval can miss semantic matches; dense retrievers may mis-handle biomedical tokens leading to errors.",
            "uuid": "e7781.6",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Lit-R",
            "name_full": "Literature Retriever (Lit-R)",
            "brief_description": "The literature retrieval subsystem in KG-CoI combining query keywords from LLM-E with BM25 over PubMed to obtain textual context for hypothesis generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Pipeline component (supports LLM-G)",
            "model_size": "",
            "scientific_domain": "Biomedical / Retrieval-augmented LLMs",
            "theory_type": "Retrieval submodule for hypothesis evaluation",
            "evaluation_method_name": "Lit-R augmentation ablation",
            "evaluation_method_description": "Ablation experiments remove Lit-R to quantify the contribution of literature context to hypothesis accuracy and F1.",
            "evaluation_metric": "Accuracy, F1 deltas when Lit-R is removed",
            "metric_definition": "Percent point change in Accuracy/F1 relative to full KG-CoI baseline.",
            "dataset_or_benchmark": "PubMed + BM25 retrieval",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Removal of literature information decreased performance across LLMs; effect size largest for smaller models (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Quality of retrieved documents varies; some retrieved literature statements may not map to KG triples leading to unverifiable steps.",
            "uuid": "e7781.7",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "KG-R",
            "name_full": "Knowledge Graph Retriever (KG-R)",
            "brief_description": "A KG retriever that performs breadth-first search for k-step relation chains in the domain KG (PubTator3) to supply neighbor relations R used for query enrichment and LLM context augmentation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "KG lookup component",
            "model_size": "",
            "scientific_domain": "Biomedical / Knowledge graph augmentation",
            "theory_type": "Retrieval submodule for KG evidence",
            "evaluation_method_name": "KG neighbor retrieval ablation",
            "evaluation_method_description": "Retrieves k-step neighbor relation chains for entities in queries; ablation (removal of KG relations) quantifies impact on hypothesis generation accuracy/F1.",
            "evaluation_metric": "Accuracy, F1 deltas when KG-R is removed",
            "metric_definition": "Percent point change in Accuracy/F1 relative to full KG-CoI. Also used to compute per-step correctness candidates.",
            "dataset_or_benchmark": "PubTator3 KG",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "Removal of KG information causes the largest performance drops for stronger LLMs like Llama-3.1-70B and GPT-4o (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Only direct links retrieved are considered for per-step verification; multi-hop inference beyond k steps may be necessary for some valid reasoning.",
            "uuid": "e7781.8",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Baselines: Direct / CoT / RAG",
            "name_full": "Baseline prompting and augmentation methods: Direct prompting, Chain-of-Thought (CoT) prompting, Retrieval-Augmented Generation (RAG)",
            "brief_description": "Baseline evaluation frameworks used for comparison: Direct (single-step answer), CoT (stepwise reasoning prompting), and RAG (literature-augmented generation) to measure gains from KG-CoI.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Various LLMs tested under each baseline",
            "model_size": "8B / 70B / mini / (GPT-4o unspecified)",
            "scientific_domain": "Biomedical / LLM evaluation",
            "theory_type": "Prompting and augmentation frameworks",
            "evaluation_method_name": "Baseline comparison (Direct, CoT, RAG)",
            "evaluation_method_description": "Run each LLM under Direct, CoT, and RAG settings (with/without KG augmentation) and compare classification Accuracy, F1, and CI/confidence metrics to assess impact of reasoning and external knowledge.",
            "evaluation_metric": "Accuracy, F1, Confidence",
            "metric_definition": "Accuracy/F1 as classification scores (percent); Confidence as KG-verification proportion.",
            "dataset_or_benchmark": "Constructed PubTator3-based dataset",
            "human_evaluation_details": "None beyond verifier annotation experiment",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "KG-CoI improves over Direct, CoT, and RAG across LLMs (Table 1); e.g., Llama-3.1-70B: Direct 72.00% -&gt; KG-CoI 79.33% accuracy.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "CoT can produce plausible but unverified reasoning; RAG depends on retrieved literature that may not align with KG evidence.",
            "uuid": "e7781.9",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency ensemble for chain-of-thought",
            "brief_description": "An evaluation technique where the model is run multiple times with randomness to generate multiple reasoning chains and a majority-vote/consensus decision is used, tested here at runs N = 1,5,10,15 to evaluate robustness and accuracy improvements.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Applied to multiple LLMs (Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, GPT-4o)",
            "model_size": "8B / 70B / mini / (GPT-4o unspecified)",
            "scientific_domain": "LLM reasoning evaluation / Biomedical",
            "theory_type": "Ensembling method for hypothesis decisions",
            "evaluation_method_name": "Self-consistency ensemble",
            "evaluation_method_description": "Run LLM multiple times to generate multiple CoI outputs, aggregate via self-consistency (majority/consensus) to choose final answer; measure how accuracy/F1 scale with number of runs.",
            "evaluation_metric": "Accuracy, F1 as a function of number of runs",
            "metric_definition": "Accuracy/F1 reported for N runs; improvement measured in percentage points.",
            "dataset_or_benchmark": "Constructed PubTator3-based dataset",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Self-consistency improves KG-CoI performance for most models (except some anomalous behavior for Llama-3.1-8B), and scaling runs N up to 15 shows consistent gains (Appendix Figure 2).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Increased runs introduce randomness that may reduce confidence measures; computational cost increases with number of runs.",
            "uuid": "e7781.10",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Accuracy & F1",
            "name_full": "Accuracy and F1-score (classification metrics)",
            "brief_description": "Primary automated performance metrics for hypothesis-classification (stimulate / inhibit / no_relation) used throughout experiments to quantify correctness of LLM outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Reported for each tested LLM",
            "model_size": "various (8B, 70B, mini, GPT-4o)",
            "scientific_domain": "Biomedical / Machine learning evaluation",
            "theory_type": "Quantitative evaluation metrics",
            "evaluation_method_name": "Classification metrics (Accuracy, macro/micro F1 as reported)",
            "evaluation_method_description": "Compare predicted label against ground-truth masked PubTator3 relation; report overall Accuracy and F1 scores (presented as percentages in tables).",
            "evaluation_metric": "Accuracy (%), F1 (%)",
            "metric_definition": "Accuracy = (correct predictions) / (total) * 100. F1 is the harmonic mean of precision and recall; reported as percentage.",
            "dataset_or_benchmark": "Constructed PubTator3-based dataset (300 instances)",
            "human_evaluation_details": "No human scoring for these metrics; computed automatically.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Table 1 and Table 3 enumerate Accuracy and F1 for methods and ablations; e.g., GPT-4o KG-CoI Acc 86.00%, F1 85.83%.",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Accuracy/F1 reflect match to masked KG labels (not real-world downstream experimental validation); do not capture novelty or scientific plausibility beyond label correctness.",
            "uuid": "e7781.11",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "LLM-V (GPT-4o-mini)",
            "name_full": "LLM verifier agent (implemented with GPT-4o-mini)",
            "brief_description": "An LLM agent chosen to perform claim verification by judging whether a KG triple supports an LLM-generated claim; selected for a balance of cost and alignment with human annotators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini (used as LLM-V)",
            "model_size": "GPT-4o-mini (vendor-defined mini variant)",
            "scientific_domain": "Biomedical claim verification",
            "theory_type": "Automated claim verifier",
            "evaluation_method_name": "LLM-based claim verification with human agreement benchmarking",
            "evaluation_method_description": "Sample (claim, relation triple) pairs and have LLM-V decide 'yes'/'no' whether triple supports claim; compare LLM-V outputs to two human annotators to assess alignment and choose verifier.",
            "evaluation_metric": "Exact-match ratio (agreement) with human annotators; used within confidence computations",
            "metric_definition": "Exact-match = fraction of verifier outputs matching human label; reported as decimal fractions in Table 4.",
            "dataset_or_benchmark": "100 sampled (claim, triple) instances from actual hallucination detection process",
            "human_evaluation_details": "Two human annotators (H1, H2) labeled 100 samples; Table 4 reports pairwise exact-match matrix between H1, H2, GPT-3.5, GPT-4o-mini, GPT-4o and prices per 1M tokens.",
            "automated_falsifiability_check": true,
            "reproducibility_assessment": true,
            "reported_results": "GPT-4o-mini exact-match with H1: 0.74, with H2: 0.72; chosen as cost-effective verifier (price shown in Table 4).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM-V (GPT-4o-mini) has lower agreement than GPT-4o but acceptable alignment with human annotators, justifying selection on cost-performance tradeoff.",
            "limitations_noted": "Verifier alignment imperfect; selection based on limited sample (100 instances) and cost considerations; verifier errors propagate to confidence score.",
            "uuid": "e7781.12",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Ablation studies",
            "name_full": "Component ablation analysis (KG, literature, query enrichment, chain-of-thought)",
            "brief_description": "Systematic removal of KG, literature, query enrichment, or chain-of-thought components from KG-CoI to quantify each component's contribution to hypothesis accuracy and F1 across LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Evaluated across multiple LLMs",
            "model_size": "8B / 70B / mini / GPT-4o",
            "scientific_domain": "Biomedical / System analysis",
            "theory_type": "Ablation evaluation methodology",
            "evaluation_method_name": "Ablation testing",
            "evaluation_method_description": "Remove one system component at a time and measure change in Accuracy/F1 relative to the full KG-CoI system to determine importance of each module.",
            "evaluation_metric": "Delta in Accuracy and F1 (percentage points)",
            "metric_definition": "Reported percent-point decrease in Accuracy and F1 when component removed (see Table 3).",
            "dataset_or_benchmark": "Constructed PubTator3-based dataset",
            "human_evaluation_details": "None",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "Removal of KG information caused largest drops for some LLMs; removal of literature impacted weaker LLMs most (Table 3).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Ablations measure impact on automatic metrics but do not directly measure scientific novelty or real-world experimental validity.",
            "uuid": "e7781.13",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Human annotation / exact-match evaluation",
            "name_full": "Human annotation and exact-match agreement evaluation for LLM-V selection",
            "brief_description": "Manual labeling of a 100-sample subset of (claim, triple) pairs by two human annotators to evaluate alignment (exact-match) of candidate LLM verifiers (GPT-3.5, GPT-4o-mini, GPT-4o) with human judgment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Human annotators and evaluated LLM verifiers (GPT-3.5, GPT-4o-mini, GPT-4o)",
            "model_size": "N/A",
            "scientific_domain": "Annotation quality / verifier benchmarking",
            "theory_type": "Human evaluation protocol",
            "evaluation_method_name": "Exact-match agreement matrix",
            "evaluation_method_description": "Compute pairwise exact-match ratios between two human annotators and three LLM verifiers for 100 samples; use agreement to assess verifier choice and inter-annotator reliability.",
            "evaluation_metric": "Exact-match ratios (pairwise agreement values)",
            "metric_definition": "Exact-match = fraction of samples where two annotators/LLMs gave the same label; values in Table 4 range 0–1.",
            "dataset_or_benchmark": "100 sampled (claim, triple) instances from hallucination detection outputs",
            "human_evaluation_details": "Two human annotators labeled the samples (H1, H2); pairwise exact-match with GPT-3.5: 0.70–0.82 range shown; price per 1M tokens also reported for LLMs.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": true,
            "reported_results": "GPT-4o had best agreement with humans; GPT-4o-mini chosen as cost-effective compromise (exact-match ~0.72–0.80 vs humans).",
            "comparison_to_human_generated": true,
            "comparison_results": "LLM verifiers show lower but reasonable agreement with human annotators; human-human agreement ~0.90, indicating annotator reliability.",
            "limitations_noted": "Small sample size (100) for verifier selection; exact-match does not capture nuanced partial agreement or confidence; cost considerations influenced final verifier choice.",
            "uuid": "e7781.14",
            "source_info": {
                "paper_title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge",
            "rating": 2,
            "sanitized_title": "pubtator_30_an_aipowered_literature_resource_for_unlocking_biomedical_knowledge"
        },
        {
            "paper_title": "Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge",
            "rating": 1,
            "sanitized_title": "graphbased_retriever_captures_the_long_tail_of_biomedical_knowledge"
        }
    ],
    "cost": 0.022355999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models</h1>
<p>Guangzhi Xiong ${ }^{1 <em>}$, Eric Xie ${ }^{1 </em>}$, Amir Hassan Shariatmadari ${ }^{1}$, Sikun Guo ${ }^{1}$, Stefan Bekiranov ${ }^{1}$, Aidong Zhang ${ }^{1}$<br>${ }^{1}$ University of Virginia</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem-solving tasks. Their ability to understand and generate human-like text has opened up new possibilities for advancing scientific research, enabling tasks such as data analysis, literature review, and even experimental design. One of the most promising applications of LLMs in this context is hypothesis generation, where they can identify novel research directions by analyzing existing knowledge. However, despite their potential, LLMs are prone to generating "hallucinations", outputs that are plausiblesounding but factually incorrect. Such a problem presents significant challenges in scientific fields that demand rigorous accuracy and verifiability, potentially leading to erroneous or misleading conclusions. To overcome these challenges, we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured reasoning process, organizing their output as a chain of ideas (CoI), and includes a KG-supported module for the detection of hallucinations. With experiments on our newly constructed hypothesis generation dataset, we demonstrate that KG-CoI not only improves the accuracy of LLM-generated hypotheses but also reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing realworld scientific research.</p>
<h2>Introduction</h2>
<p>Advanced Large Language Models (LLMs) such as GPT4 (OpenAI et al. 2024) have exhibited exceptional performance in a wide range of general machine learning tasks, such as question answering (Hendrycks et al. 2021) and arithmetic computation (Cobbe et al. 2021). Recently, there has been growing interest in harnessing the reasoning capabilities of LLMs within scientific domains. These efforts have shown impressive results, with LLMs tackling complex scientific questions and often achieving, or even exceeding, human-level performance (Nori et al. 2023; Hou and Ji 2023; Stribling et al. 2024). As a result, LLMs are increasingly viewed as promising tools with the potential to significantly advance real-world scientific research, such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>drug discovery, biological sequence analysis, and material design (AI4Science and Quantum 2023).</p>
<p>Specifically, with the capability of processing and synthesizing vast amounts of text, LLMs are well-suited to accelerate the analysis of scientific literature and generate new hypotheses for potential scientific discovery (Qi et al. 2023; Zhou et al. 2024). Traditional scientific research, particularly in natural sciences like biology, typically involves a multi-step, time-consuming process from gathering literature to validating hypotheses. By generating promising research ideas directly from existing literature, LLMs have the potential to significantly streamline and reduce the time required for these labor-intensive tasks.</p>
<p>However, despite their advanced capabilities, LLMs face criticism for generating misinformation or so-called "hallucinations", which are responses that seem plausible but are factually incorrect (Huang et al. 2023). This issue is particularly critical in scientific research, where every reasoning step must be transparent and verifiable. In the context of hypothesis generation for natural sciences, hallucinations can easily arise if the parametric knowledge of LLMs lacks accurate scientific information. Moreover, these hallucinations are particularly challenging to detect in generated hypotheses, as they are often related to potential discoveries that have not yet been explored.</p>
<p>To address the problems mentioned above, we propose a novel system termed KG-CoI (Knowledge Grounded Chain of Ideas), designed to enhance hypothesis generation by incorporating external knowledge from knowledge graphs (KGs). These KGs contain well-organized structured information that has been verified by existing literature. By prompting LLMs to generate a chain of ideas (CoI) through step-by-step reasoning (Wei et al. 2022), our system facilitates in-depth analysis of the input and further verification of the generated content. The KG-CoI system consists of three key modules: KG-guided context retrieval, KG-augmented chain-of-idea generation, and KG-supported hallucination detection. By linking LLM hypothesis generation to KGs, our system aligns the output with well-established scientific knowledge and ensures that the generated hypotheses are grounded in reliable information sources.</p>
<p>To quantitatively demonstrate the effectiveness of our system, we construct a hypothesis generation dataset by masking certain links within a KG and prompting LLMs to hy-</p>
<p>pothesize potential relations without prior knowledge of the facts. Our experiments show that, compared to existing methods for prompting LLMs in hypothesis generation, KG-CoI achieves the highest accuracy in generating hypotheses, underscoring its advantages in real-world scientific research. Moreover, with the KG-supported hallucination detection, we demonstrate the effectiveness of KG-CoI in reducing hallucinations, thereby improving its reliability in natural sciences.</p>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>We present KG-CoI, a novel LLM-enhanced hypothesis generation system that augments the generated hypotheses with external structured knowledge and presents the result as a coherent chain of ideas.</li>
<li>We construct a new dataset to evaluate LLM hypothesis generation and conduct extensive experiments on both open- and close-source LLMs, showing the effectiveness of KG-CoI in hypothesizing scientific knowledge.</li>
<li>We propose a KG-supported hallucination detection method within KG-CoI, which demonstrates the advantage of KG-CoI in reducing hallucinations.</li>
</ul>
<h2>Related Work</h2>
<p>Retrieval-augmented Generation. The integration of external knowledge into large language models (LLMs) has become an increasingly explored area of research, particularly for enhancing the accuracy and reliability of generated content (Gao et al. 2023; Zhao et al. 2024). This approach, known as retrieval-augmented generation (RAG), helps mitigate issues like hallucinations by grounding LLM outputs in relevant and accurate information from external sources (Lewis et al. 2020; Guu et al. 2020; Borgeaud et al. 2022; Izacard and Grave 2020). In various domains, particularly biomedical and scientific research, retrieval-augmented methods have proven effective in improving LLM performance on tasks such as question answering and claim verification (Zakka et al. 2024; Xiong et al. 2024; Liu et al. 2024).</p>
<p>Recent advancements have further refined these methods by incorporating knowledge graphs (KGs) into the retrieval process, addressing limitations such as the omission of rare but crucial information and the overrepresentation of frequently seen concepts. For instance, tools like KRAGEN utilize KGs to enhance LLM capabilities by structuring the retrieval process through graph-based reasoning (Matsumoto et al. 2024). Similarly, hybrid approaches that combine KG-based retrieval with traditional text embedding methods have shown promise in handling the long tail of biomedical knowledge, providing a more balanced and comprehensive retrieval of information (Delile et al. 2024). In our work, we leverage the authoritative knowledge of existing domain-specific KGs for the generation of new scientific hypotheses, which demonstrates the potential of integrating structured knowledge with LLMs to inspire novel insights that might be valuable for scientific research.</p>
<p>Chain-of-thought Based Reasoning. LLM reasoning can be powered by the Chain-of-Thoughts (CoT) prompting (Wei et al. 2022) technique, which involves prompting the</p>
<p>LLM to generate intermediate reasoning steps when answering user questions. This technique helps the LLM generate much more accurate results than standard prompting. While the reasoning paths generated by CoT are a useful mechanism to explain why the LLM generated an answer, some studies show it is not faithful to the LLM's inner reasoning (Liu et al. 2023). Additionally, there can be reasoning steps that sound plausible but factually or logically flawed (Liu et al. 2023). Several studies demonstrate the effectiveness of incorporating external knowledge with CoT prompting to obtain more accurate, factually grounded answers while improving the explainability of reasoning paths (Luo et al. 2023; Trivedi et al. 2022; Wen, Wang, and Sun 2023). Following CoT, advanced frameworks are further proposed to enhance LLM for accurate and reliable outputs (Wang et al. 2022; Yao et al. 2024; Besta et al. 2024), which leverage the internal knowledge of LLMs for complex reasoning.</p>
<p>Hypothesis Generation. Hypothesis generation has been pursued as the task of mining meaningful implicit association between disjoint concepts in decades, where the goal is to predict connections between different concepts identified within scientific literature (Sebastian, Siew, and Orimaye 2017). This process typically involves uncovering meaningful relationships between separate concepts by systematically analyzing existing publications. Traditional methods in this domain often focus on identifying these connections from a static view of the literature. While these approaches are effective and can be rigorously tested, they generally assume that all relevant concepts are pre-existing in the literature and simply need to be linked. This approach lacks the ability to account for the contextual factors that researchers consider important during the hypothesis formation process, and it does not fully engage with the creative and generative aspects of scientific thinking.</p>
<p>Recently, attention has shifted towards using LLMs to generate hypotheses. For example, SciMON (Wang et al. 2023) proposed a framework that utilizes prior scientific literature to fine-tune LLMs for the generation of novel ideas. Additionally, in (Zhou et al. 2024), a method was introduced that uses prompts with LLMs to iteratively generate hypotheses based on provided examples. Our approach differs by utilizing both structured and unstructured domainspecific knowledge from scientific resources while leveraging the reasoning capabilities of LLMs to explore new insights from the current knowledge. Moreover, we focus on the hallucination detection in the generated content, providing an end-to-end system from context retrieval to hypothesis evaluation.</p>
<h2>Methodology</h2>
<p>Figure 1 shows the overview of our KG-CoI system. There are three different modules in KG-CoI, including the KGguided context retrieval, the KG-augmented chain-of-idea generation, and the KG-supported hallucination detection. KG-CoI provides an end-to-end solution to augment the hypothesis generation of LLMs with KG information while providing an explicit analysis of hallucinations in the generated content. The details of each module are introduced in the remaining parts of this section.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: An overview of our proposed KG-CoI for knowledge-grounded hypothesis generation. “KG-R” and “Lit-R” are retrievers for scientific knowledge graphs (KGs) and literature, respectively. “LLM-E”, “LLM-G”, and “LLM-V” are LLM agents for query enrichment, hypothesis generation, and claim verification, respectively.</p>
<h2>KG-guided Context Retrieval</h2>
<p>While retrieval-augmented generation <em>Lewis et al. (2020)</em> provides an opportunity to enhance LLM output with external information from domain-specific corpora, the generation of scientific hypotheses is a challenging task where the relevant information of the new scientific knowledge can hardly be found in the existing literature. Thus, we propose to augment the hypothesis generation capability of LLMs using a KG-guided context retrieval, instead of the vanilla information retrieval from literature.</p>
<p>Knowledge Enhancement with KG. The first key step of our KG-guided context retrieval is to enhance the given question using authoritative knowledge from KG. Specifically, for a given question $\mathcal{Q}$ about the possible scientific facts between two entities $e_{h}$ and $e_{t}$, we can search for all $k$-step relation chains in a knowledge graph $\mathcal{G}$ such that $e_{h}$ and $e_{t}$ are linked by</p>
<p>$e_{h}\stackrel{r_{1}}{\longleftrightarrow}e_{1}\stackrel{r_{2}}{\longleftrightarrow}e_{2}\ldots\stackrel{r_{k}}{\longleftrightarrow}e_{t},$ (1)</p>
<p>where the relations on the bidirectional arrows connect two adjacent entities. The directions of the relations are defined by the KG selected. As shown in Figure 1, KG-CoI uses a KG-based retriever (KG-R) to search for neighbor relations for the given question, which is implemented with a breadth-first search strategy. The relation chains will then be used as external knowledge from KG to augment the context retrieval for the original question. For simplicity, we will refer to the retrieved relations as $\mathcal{R}$ in our later descriptions.</p>
<p>Query Enrichment with KG. Given the input question and the retrieved relations from KG, we propose to enrich the user query by generating keywords using all existing information, which can facilitate further information retrieval from the scientific literature. Formally, given an input question $\mathcal{Q}$ and the retrieved KG relations $\mathcal{R}$, the keywords $w_{1},\cdots,w_{T}$ will be generated as</p>
<p>$w_{1},\cdots,w_{T}=\underset{w_{1}^{<em>},\cdots,w_{T}^{</em>}}{\arg\max}\mathbb{P}<em 1="1">{\textsc{LLM-E}}(w</em>^{<em>},\cdots,w_{T}^{</em>}|\mathcal{Q},\mathcal{R}),$ (2)</p>
<p>where LLM-E is an LLM agent that is required to generate search keywords to help answer the given question using existing information. In our KG-CoI, the query enrichment serves as an important step to naturally fuse the information of KG and the original question for context retrieval. We further test the importance of such a design, which is discussed in our ablation studies.</p>
<p>Information Retrieval with Literature. After the query enrichment step, we perform information retrieval using the generated keywords to retrieve relevant documents from the scientific literature for the original question. For the biological domain explored in this paper, we use PubMed as the source of documents, including all biomedical abstracts in it. For the text retriever on PubMed, we select BM25 <em>Robertson, Zaragoza et al. (2009)</em>, a sparse retriever that is based on lexicon comparisons. BM25 is a suitable choice for biological information retrieval, as domain-specific terms such as gene names may be wrongly tokenized by dense retrievers, leading to misunderstanding. We term a literature retrieval system with both a corpus and a retriever as “Lit-R”, which takes the keywords $w_{1},\cdots,w_{T}$ as the input and outputs relevant documents $\mathcal{D}$ from the given corpus.</p>
<h2>KG-augmented Chain-of-idea Generation</h2>
<p>We then prompt LLMs to perform a KG-augmented chain-of-idea generation, which is the core part of our KG-CoI system. The hypothesis generation of LLMs is augmented by KG from two different perspectives. First, we directly add the retrieved neighbor relations $\mathcal{R}$ from KG to the input context of LLMs, making them knowledgeable of related</p>
<p>information given by KG. Second, the retrieved documents from scientific literature are also provided to the LLMs for the augmentation of text generation. As described in the previous subsection, the retrieval of knowledge from literature is also guided by the relevant KG information, resulting in an implicit impact on the final output.</p>
<p>Additionally, we prompt LLMs to generate their predictions step by step as a chain of ideas (CoI). As discussed in the existing literature, prompting LLMs to have step-by-step thinking would help them dive deep into the given question and organize its answer in a logical way (Wei et al. 2022). Moreover, it is crucial for our system to have a chain of ideas as the output of LLMs, which will be further used in our last module to analyze the hallucination and the confidence of LLMs in the generated contents.</p>
<p>Formally, given the original question $\mathcal{Q}$, the retrieved KG relations $\mathcal{R}$, and the retrieved documents $\mathcal{D}$ from literature, a chain of ideas for the new scientific hypothesis can be generated by</p>
<p>$$
\left{\begin{array}{l}
s_{1}=\arg \max <em 1="1">{s</em>^{<em>}} \mathbb{P}<em 1="1">{\mathrm{LLM}-\mathrm{G}}\left(s</em>^{</em>} \mid \mathcal{Q}, \mathcal{R}, \mathcal{D}\right) \
s_{2}=\underset{s_{2}^{<em>}}{\arg \max } \mathbb{P}<em 2="2">{\mathrm{LLM}-\mathrm{G}}\left(s</em>^{</em>} \mid \mathcal{Q}, \mathcal{R}, \mathcal{D}, s_{1}\right) \
\cdots \
s_{N}=\underset{s_{N}^{<em>}}{\arg \max } \mathbb{P}<em N="N">{\mathrm{LLM}-\mathrm{G}}\left(s</em>^{</em>} \mid \mathcal{Q}, \mathcal{R}, \mathcal{D}, s_{1}, \cdots, s_{N-1}\right) \
\mathcal{H}=\underset{\mathcal{H}^{<em>}}{\arg \max } \mathbb{P}_{\mathrm{LLM}-\mathrm{G}}\left(\mathcal{H}^{</em>} \mid \mathcal{Q}, \mathcal{R}, \mathcal{D}, s_{1}, \cdots, s_{N}\right)
\end{array}\right.
$$</p>
<p>where $s_{1}, \cdots, s_{N}$ are the step-by-step ideas (or step-by-step claims) and $\mathcal{H}$ is the final hypothesis generated given the chain of ideas. LLM-G is an LLM agent designed to generate new scientific hypotheses with the given information. While Formula 3 describes a process of greedy search in the generation of ideas, we also explore the potential of running KG-CoI multiple times with randomness and examine its self-consistency in the answer prediction (Wang et al. 2022), which will be compared in detail in our Experiments.</p>
<h2>KG-supported Hallucination Detection</h2>
<p>While the KG-guided context retrieval and the KGaugmented chain-of-idea generation modules have already enhanced LLMs and provided the final prediction of the hypothesis, it is also important to explore the hallucinations in the generated content and examine how reliable the hypothesis is. With the output organized as a chain of ideas in our KG-CoI system, we propose to verify the correctness of each generated reasoning step using the information from a domain-specific KG.</p>
<p>For each reasoning step $s_{i}$ in the chain of ideas defined in Formula 3, we first identify all $B$ biological entities $e_{i 1}, \cdots, e_{i B}$ in the claim using a named entity recognition (NER) tool tailored for domain-specific research. We then define the correctness of the claim $s_{i}$ given the KG $\mathcal{G}$ as a boolean variable with values 0 and 1 . The correctness of $s_{i}$ will be 1 if $\exists j, k \in{1, \cdots, B}$ and the relation $r_{i j k}$ such that</p>
<p>$$
\left(e_{i j}, r_{i j k}, e_{i k}\right) \in \mathcal{G}
$$</p>
<p>and</p>
<p>$$
\mathrm{LLM}-\mathrm{V}\left(\left(e_{i j}, r_{i j k}, e_{i k}\right), s_{i}\right)=1
$$</p>
<p>LLM-V is an LLM agent designed for claim verification given a triple of head and tail entities as well as their relation, which outputs 1 if the claim can be verified by the relation triple else 0 .</p>
<p>In practice, KG-CoI uses the KG retriever "KG-R" defined in the KG-guided context retrieval phase, leveraging its abilities to find the direct links among entities that appear in the current claim. The retrieved relations will then be examined iteratively to check if any of them can support the claim made by LLMs in their chain of ideas. We only consider the direct link among entities because, in the ideal case, the reasoning steps of the hypothesis generation should be composed of simple scientific facts that can be verified by authoritative KG information. While the chain of ideas enables LLMs to have a deep analysis of the original question, the knowledge it uses for each reasoning step should be simple and easy to verify.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Algorithm</span><span class="w"> </span><span class="mi">1</span>:<span class="w"> </span><span class="nv">The</span><span class="w"> </span><span class="nv">algorithm</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">KG</span><span class="o">-</span><span class="nv">CoI</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">scientific</span><span class="w"> </span><span class="nv">hypoth</span><span class="o">-</span>
<span class="nv">esis</span><span class="w"> </span><span class="nv">generation</span>
</code></pre></div>

<p>Input: A scientific question $\mathcal{Q}$, Knowledge Graph $\mathcal{G}$, Scientific Literature Corpus $\mathcal{C}$, Maximum relation chain length $k$ Output: A scientific hypothesis $\mathcal{H}$ with confidence score</p>
<p>1: Step 1: KG-guided Context Retrieval
2: Retrieve $k$-step relation chains $\mathcal{R}$ from $\mathcal{G}$ for entities in $\mathcal{Q}$ using the KG retriever "KG-R"
3: Generate enriched query keywords $w_{1}, \cdots, w_{T}$ using LLM-E based on $\mathcal{Q}$ and $\mathcal{R}$
4: Retrieve relevant documents $\mathcal{D}$ from $\mathcal{C}$ using a literature retriever "Lit-R" with keywords $w_{1}, \cdots, w_{T}$
5: Step 2: KG-augmented Chain-of-idea Generation
6: Generate step-by-step ideas $s_{1}, \cdots, s_{N}$ for a new hypothesis $\mathcal{H}$ using LLM-G, incorporating $\mathcal{Q}, \mathcal{R}$, and $\mathcal{D}$
7: Formulate the final hypothesis $\mathcal{H}$ based on the chain of ideas
8: Step 3: KG-supported Hallucination Detection
9: for each step $s_{i}$ in $\left{s_{1}, \cdots, s_{N}\right}$ do
10: Identify entities $e_{i 1}, \cdots, e_{i B}$ in $s_{i}$ using NER
11: Check correctness of $s_{i}$ by verifying relations $\left(e_{i j}, r_{i j k}, e_{i k}\right) \in \mathcal{G}$ using LLM-V, $\forall j, k \in{1, \cdots, B}$
12: Assign correctness score correctness $\left(s_{i}\right)$ based on KG validation
13: end for
14: Calculate confidence score using Formula 6
15: Return the final hypothesis $\mathcal{H}$ and its confidence score</p>
<p>After computing the correctness of each reasoning step in the chain of ideas, KG-CoI can summarize the overall confidence of a generated hypothesis as</p>
<p>$$
\operatorname{confidence}(\mathcal{H})=\frac{1}{N} \sum_{i=1}^{N} \operatorname{correctness}\left(s_{i}\right)
$$</p>
<p>The measure of confidence can reflect the hallucinations in the generated content in terms of knowledge from a KG. If</p>
<table>
<thead>
<tr>
<th>LLM</th>
<th>Method</th>
<th>Knowledge</th>
<th>Greedy Search</th>
<th></th>
<th></th>
<th>Self Consistency</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Accuracy</td>
<td>F1</td>
<td>Confidence</td>
<td>Accuracy</td>
<td>F1</td>
<td>Confidence</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>Direct</td>
<td>No</td>
<td>47.00</td>
<td>48.71</td>
<td>00.00</td>
<td>65.00</td>
<td>70.86</td>
<td>00.00</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>CoT</td>
<td>No</td>
<td>56.67</td>
<td>56.61</td>
<td>40.22</td>
<td>56.67</td>
<td>64.92</td>
<td>41.36</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>RAG</td>
<td>Yes</td>
<td>68.67</td>
<td>69.83</td>
<td>37.44</td>
<td>65.00</td>
<td>70.86</td>
<td>38.35</td>
</tr>
<tr>
<td>Llama-3.1-8B</td>
<td>KG-CoI</td>
<td>Yes</td>
<td>70.33</td>
<td>70.42</td>
<td>43.46</td>
<td>66.67</td>
<td>72.63</td>
<td>40.46</td>
</tr>
<tr>
<td>Llama-3.1-70B</td>
<td>Direct</td>
<td>No</td>
<td>72.00</td>
<td>71.94</td>
<td>00.00</td>
<td>71.67</td>
<td>72.00</td>
<td>00.00</td>
</tr>
<tr>
<td>Llama-3.1-70B</td>
<td>CoT</td>
<td>No</td>
<td>73.67</td>
<td>73.77</td>
<td>35.23</td>
<td>71.33</td>
<td>71.58</td>
<td>34.97</td>
</tr>
<tr>
<td>Llama-3.1-70B</td>
<td>RAG</td>
<td>Yes</td>
<td>73.33</td>
<td>73.50</td>
<td>35.02</td>
<td>72.33</td>
<td>73.12</td>
<td>34.58</td>
</tr>
<tr>
<td>Llama-3.1-70B</td>
<td>KG-CoI</td>
<td>Yes</td>
<td>79.33</td>
<td>79.52</td>
<td>30.78</td>
<td>81.00</td>
<td>81.92</td>
<td>26.24</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>Direct</td>
<td>No</td>
<td>70.00</td>
<td>69.45</td>
<td>00.00</td>
<td>69.33</td>
<td>69.71</td>
<td>00.00</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>CoT</td>
<td>No</td>
<td>73.00</td>
<td>72.61</td>
<td>39.28</td>
<td>73.33</td>
<td>73.59</td>
<td>39.21</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>RAG</td>
<td>Yes</td>
<td>76.67</td>
<td>76.55</td>
<td>40.61</td>
<td>76.67</td>
<td>76.70</td>
<td>40.04</td>
</tr>
<tr>
<td>GPT-4o-mini</td>
<td>KG-CoI</td>
<td>Yes</td>
<td>82.67</td>
<td>82.56</td>
<td>43.87</td>
<td>84.00</td>
<td>84.27</td>
<td>44.24</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>Direct</td>
<td>No</td>
<td>73.33</td>
<td>73.40</td>
<td>00.00</td>
<td>74.00</td>
<td>74.37</td>
<td>00.00</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>CoT</td>
<td>No</td>
<td>74.33</td>
<td>74.26</td>
<td>34.41</td>
<td>75.67</td>
<td>75.68</td>
<td>34.93</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>RAG</td>
<td>Yes</td>
<td>75.67</td>
<td>75.97</td>
<td>37.74</td>
<td>74.33</td>
<td>74.74</td>
<td>36.21</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>KG-CoI</td>
<td>Yes</td>
<td>86.00</td>
<td>85.83</td>
<td>44.24</td>
<td>86.33</td>
<td>86.17</td>
<td>41.66</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of our proposed KG-CoI system and baseline methods on hypothesis generation for biological knowledge. "Knowledge" denotes if the method is augmented with external biological knowledge. All scores are percentages.
the confidence of a hypothesis is high, it means most reasoning steps in the chain of ideas can be verified by an external KG, indicating a high probability for the overall analysis to be reliable. If the confidence is low for a generated hypothesis, it shows most reasoning steps are unverifiable, calling for additional cautions when using the hypothesis.</p>
<p>The overall algorithm of our KG-CoI system is presented in Algorithm 1.</p>
<h2>Experiments</h2>
<h2>Experimental Settings</h2>
<p>To simulate the process of generating novel hypotheses, we use the knowledge graph (KG) of PubTator3 (Wei et al. 2024) and remove a set of relations from it to examine the capabilities of LLMs in hypothesizing the hidden relations using other existing knowledge in the KG. The constructed dataset mimics real-world scenarios where LLMs need to analyze existing knowledge and hypothesize new scientific facts, while providing a ground truth for model evaluation and comparison. The constructed hypothesis generation dataset contains 300 instances, where each of the three target classes ("stimulate", "inhibit", "no relation") has 100 instances. More details about the dataset construction can be found in the Appendix.</p>
<p>We choose direct prompting (Direct), chain-of-thought prompting (CoT; Wei et al., 2022), and retrieval-augmented generation (RAG; Lewis et al., 2020) as the baselines for comparison, exploring if KG-CoI helps LLMs better find out the correct potential scientific facts given existing information. Direct and CoT examine if LLMs can make correct predictions based on their own parametric knowledge. RAG shows how well LLMs perform with external knowledge from scientific literature only. For each LLM and setting,
we test its performance with both the greedy search and the self-consistency across five runs (Wang et al. 2022). More ablation studies about different components in KG-CoI are presented in the "Ablation Studies" section.</p>
<p>For our KG-CoI, we implement the KG retriever KG-R using the KG given by PubTator3 (Wei et al. 2024) along with its named entity recognition tool, which can identify the exact entity ID given the term description. We choose the "en_core_sci_sm" model from ScispaCy (Neumann et al. 2019) to extract biological named entities from a complete sentence. The retriever and corpus selected for the literaturebased information retrieval system Lit-R are BM25 (Robertson, Zaragoza et al. 2009) and PubMed, respectively, as mentioned in the Methodology. For LLM agents used in our system, we choose GPT-4o-mini as the LLM-V agent for claim verification, since the hallucination detection tool should be fixed for a fair comparison. More discussion of the choice of LLM-V will be presented in the Appendix. For the implementation of LLM-E and LLM-G agents which are responsible for the generation of the chain of ideas and final hypotheses, we select both commercial and open-source models with various sizes, including Llama-3.1-8B, Llama-3.1-70B, GPT-4o-mini, and GPT-4o.</p>
<p>For LLMs with each setting, we compute the correctness of answers using accuracy and F1 scores. For settings that provide a chain of ideas for hypothesis generation (CoT, RAG, KG-CoI), we evaluate their hallucinations using the proposed KG-supported hallucinating detection tool in our KG-CoI system. The results of the hallucination detection will be summarized as "Confidence", indicating the proportion of claims verified by a given KG in an idea chain. More details about our experimental settings can be found in the Appendix.</p>
<h1>Hypothesis generation question</h1>
<p>Input: Can we hypothesize a key relation between Carbon Tetrachloride and SLC17A5?
Ground Truth: stimulate</p>
<h2>Direct</h2>
<p>Answer: no relation $\mathcal{X}$</p>
<h2>CoT</h2>
<p>Reasoning: Carbon Tetrachloride is a toxic compound that can cause cellular damage, and SLC17A5 is a gene involved in the transport of organic anions, including potentially toxic compounds. There is evidence that SLC17A5 can be involved in the transport of carbon tetrachloride, which can lead to cellular damage and toxicity.
Answer: inhibit $\mathcal{X}$</p>
<h2>RAG</h2>
<p>Retrieved documents: [...]
Reasoning: Carbon Tetrachloride is a toxic compound that can cause damage to the liver and other organs. SLC17A5 is a gene that encodes for a protein called sialin, which is involved in the transport of sialic acid in the lysosome. There is no direct evidence that Carbon Tetrachloride interacts with SLC17A5.
Answer: no relation $\mathcal{X}$</p>
<h2>KG-CoI (Ours)</h2>
<p>Neighbor relations: (Carbon Tetrachloride, cause, Liver Failure), (Liver Failure, associate, SLC17A5), [...]
Retrieved documents: [...]
Reasoning: Carbon Tetrachloride is a chemical that causes liver injury, and SLC17A5 is a gene associated with liver diseases, including liver failure and fibrosis. The relationship between Carbon Tetrachloride and SLC17A5 is that Carbon Tetrachloride causes liver injury, which is associated with SLC17A5.
Answer: stimulate $\boldsymbol{\sim}$</p>
<p>Table 2: An example of how KG-CoI helps Llama-3.1-8B generate correct hypotheses and reduce hallucinations. Verified reasoning steps and misinformation are highlighted. The correct answer is marked with " $\boldsymbol{\sim}$ " and the wrong answers are marked with " $\mathcal{X}$ ".</p>
<h2>Main Results</h2>
<p>Table 1 presents the main results of our experiments, showing how KG-CoI performs compared with other methods using different LLMs. We can observe from the table that KGCoI consistently outperforms all other methods on different LLMs in terms of accuracy and F1. Specifically, the performance of LLMs on hypothesis generation gradually improve with the incorporation of reasoning capabilities (Direct $\rightarrow$ CoT ), knowledge from scientific literature ( $\mathrm{CoT} \rightarrow \mathrm{RAG}$ ), and knowledge from KG (RAG $\rightarrow$ KG-CoI). By comparing different LLMs, we can see that larger models (Llama-3.1-70B, GPT-4o) tend to perform better than smaller ones (Llama-3.1-8B, GPT-4o-mini) when using the same method for hypothesis generation. Interestingly, with the assistance of KG-CoI, the weakest LLM in our experiment (Llama-3.18B) can present an accuracy and F1 score close to the most advanced LLM (GPT-4o) in the "Direct" setting.</p>
<p>Moreover, Table 1 reveals that KG-CoI helps reduce hallucinations in LLM generation with more reasoning steps verified by domain-specific KG. While CoT examines the internal knowledge of LLMs, both RAG and KG-CoI augment the LLM hypothesis generation with external biological knowledge. It can be observed that RAG improves the confidence of hypotheses generation by GPT-4o-mini and GPT-4o, but does not show the same pattern on Llama-
3.1. As the KG only contains authoritative and objective knowledge in the domain, the knowledge in literature may not have an exact match in KG. Thus, the retrieved documents from biological literature sometimes may not be verified by the KG, leading to the fluctuating confidence changes given by RAG in different LLMs. Nevertheless, with additional authoritative knowledge from KG, KG-CoI improves the model confidence on most LLMs examined, with a $3.30 \%$ confidence increase on average compared to the CoT method.</p>
<p>In addition to the results for the greedy search of LLMs on the constructed dataset, we also examine if LLMs benefit from multiple runs on each instance using self-consistency (Wang et al. 2022). While Llama-3.1-8B presents an increased F1 score but a decreased accuracy with multiple runs, self-consistency is shown to be effective for KG-CoI on all other three LLMs in terms of accuracy and F1. On the measurement of hallucinations, it is shown that the comparison of methods in the self-consistency setting presents the same patterns as in the greedy search setting, reflecting the effectiveness of KG-CoI in reducing hallucinations. The results also reveal that compared to greedy search, selfconsistency does not necessarily improve the confidence of generated hypotheses. This may be a result of output uncertainty caused by the introduced randomness in the multiple</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Llama-3.1-8B</th>
<th></th>
<th>Llama-3.1-70B</th>
<th></th>
<th>GPT-4o-mini</th>
<th></th>
<th>GPT-4o</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Accuracy</td>
<td>F1</td>
<td>Accuracy</td>
<td>F1</td>
<td>Accuracy</td>
<td>F1</td>
<td>Accuracy</td>
<td>F1</td>
</tr>
<tr>
<td>KG-CoI</td>
<td>70.33</td>
<td>70.42</td>
<td>79.33</td>
<td>79.52</td>
<td>82.67</td>
<td>82.56</td>
<td>86.00</td>
<td>85.83</td>
</tr>
<tr>
<td>$\times$ KG information</td>
<td>65.00</td>
<td>65.66</td>
<td>73.33</td>
<td>73.24</td>
<td>74.33</td>
<td>74.29</td>
<td>74.67</td>
<td>74.93</td>
</tr>
<tr>
<td>$\times$ Literature information</td>
<td>60.67</td>
<td>59.96</td>
<td>75.33</td>
<td>75.34</td>
<td>76.00</td>
<td>75.53</td>
<td>83.33</td>
<td>83.35</td>
</tr>
<tr>
<td>$\times$ Query enrichment</td>
<td>64.67</td>
<td>65.01</td>
<td>79.00</td>
<td>79.18</td>
<td>78.67</td>
<td>78.51</td>
<td>83.00</td>
<td>83.19</td>
</tr>
<tr>
<td>$\times$ Chain of thoughts</td>
<td>62.00</td>
<td>62.19</td>
<td>77.00</td>
<td>76.99</td>
<td>71.67</td>
<td>72.39</td>
<td>76.00</td>
<td>75.73</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation studies of different components in KG-CoI on various LLMs. "KG-CoI" denotes the full version of our proposed system. " $\times$ " means the removal of specific components in KG-CoI.
trials of the self-consistency setting as opposed to the consistent and deterministic nature of greedy search.</p>
<h2>Case Studies</h2>
<p>To understand how KG-CoI helps LLMs generate the correct hypothesis, we perform case studies on actual instances in our dataset to analyze the effect of components in KG-CoI on the final prediction. Table 2 shows an example where KGCoI helps Llama-3.1-8B find the true relation between Carbon Tetrachloride and SLC17A5. While the direct prompting of the LLM provides a wrong answer, the CoT prompting results in an incorrect answer with hallucinated reasoning steps. As can be observed from the case, LLMs with CoT may hallucinate scientific knowledge that is not verified by existing KG. In contrast, RAG may fail to retrieve useful information from the literature to augment the hypothesis generation, leading to false negatives of the prediction.</p>
<p>By providing LLMs with neighbor information from the domain-specific KG, KG-CoI enables LLMs to reason on objective structured knowledge that may not be explicitly stated in the scientific literature. Table 2 shows the addition of KG knowledge helps Llama-3.1-8B build the reasonable logic chain to link different entities and find out the ground truth relation. Also, the use of both literature and KG information in KG-CoI provides verified knowledge from reliable sources, reducing the hallucinations in the generated content.</p>
<h2>Ablation Studies</h2>
<p>As described in the Methodology, our KG-CoI is a multistep hypothesis generation system with external knowledge from different sources. To illustrate how each component contributes to the entire system, we perform ablation studies to see how different settings affect the performance of the system. We first ablate the source of biological knowledge in KG-CoI. As both KG and literature information are used in our system, we test if the removal of any of them will lead to a performance drop. Specifically, the removal of KG information is performed by discarding the neighbor KG relations in both the query enrichment and the augmented generation steps. The literature information is removed by using the neighbor relations only to augment the hypothesis generation. We further examine a special setting with the removal of query enrichment, retrieving useful documents from literature based on the original question only instead of using
keywords generated given both the question and the neighbor relations. The last setting in our ablation studies involves the removal of the chain of thoughts, testing how the system performs without prompting LLMs to think step-by-step.</p>
<p>The results of our ablation studies are presented in Table 3. In general, the performance of KG-CoI drops with the removal of any component in our ablation studies, which is consistently observed on all LLMs. The importance of different components to the model performance is shown to be diverse in various LLMs. For example, the removal of KG information brings the most dramatic performance decrease on Llama-3.1-70B and GPT-4o, while it is the least important one with the minimal performance change on Llama-3.1-8B. We also discover from Table 3 that, from the LLM with the lowest accuracy (Llama-3.1-8B, 70.33\%) to the highest accuracy (GPT-4o, 86.00\%), the relative importance of the literature information changes from the most important to the least one on corresponding LLMs. While the removal of the literature information causes a performance drop of $9.66 \%$ in Llama-3.1-8B, it only decreases the performance of GPT-4o by $2.67 \%$. Such a result can be interpreted by the fact that more advanced LLMs tend to have read more literature during training, leading to fewer needs for external literature information. The ablation studies demonstrate the necessity of various components in our KG-CoI system, which also provide additional insights into the LLMs used in our experiments.</p>
<p>Additional experimental results can be found in the Appendix.</p>
<h2>Conclusion</h2>
<p>We propose KG-CoI, a systematic approach to enhancing the scientific hypothesis generation capability of LLMs with domain-specific knowledge graphs (KGs), which include KG-guided context retrieval, KG-augmented chain-of-idea generation, and KG-supported hallucination detection. Using a newly constructed hypothesis generation dataset introduced in this work, we demonstrate the effectiveness of KG-CoI in generating correct hypotheses and reducing hallucinations. Our ablation studies and case studies further justify the component designs in our system and illustrate how its predictions will be generated and used in real-world applications. This work paves the potential for researchers to utilize LLMs as a tool to verify results and generate reliable insights for future research.</p>
<h2>References</h2>
<p>AI4Science, M. R.; and Quantum, M. A. 2023. The impact of large language models on scientific discovery: a preliminary study using gpt-4. arXiv preprint arXiv:2311.07361.
Besta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.; Podstawski, M.; Gianinazzi, L.; Gajda, J.; Lehmann, T.; Niewiadomski, H.; Nyczyk, P.; et al. 2024. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 17682-17690.
Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Rutherford, E.; Millican, K.; Van Den Driessche, G. B.; Lespiau, J.-B.; Damoc, B.; Clark, A.; et al. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, 2206-2240. PMLR.
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.
Delile, J.; Mukherjee, S.; Van Pamel, A.; and Zhukov, L. 2024. Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge. arXiv preprint arXiv:2402.12352.
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Y.; Sun, J.; and Wang, H. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997.
Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, 3929-3938. PMLR.
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.; Song, D.; and Steinhardt, J. 2021. Measuring Massive Multitask Language Understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
Hou, W.; and Ji, Z. 2023. Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. BioRxiv.
Huang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; Wang, H.; Chen, Q.; Peng, W.; Feng, X.; Qin, B.; et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232.
Izacard, G.; and Grave, E. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282.
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 9459-9474.
Liu, H.; Soroush, A.; Nestor, J. G.; Park, E.; Idnay, B.; Fang, Y.; Pan, J.; Liao, S.; Bernard, M.; Peng, Y.; et al. 2024. Retrieval augmented scientific claim verification. JAMIA open, 7(1): ooae021.</p>
<p>Liu, Y.; Yao, Y.; Ton, J.-F.; Zhang, X.; Cheng, R. G. H.; Klochkov, Y.; Taufiq, M. F.; and Li, H. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models’ Alignment. arXiv preprint arXiv:2308.05374.
Luo, L.; Li, Y.-F.; Haffari, G.; and Pan, S. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061.
Matsumoto, N.; Moran, J.; Choi, H.; Hernandez, M. E.; Venkatesan, M.; Wang, P.; and Moore, J. H. 2024. KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models. Bioinformatics, 40(6).
Neumann, M.; King, D.; Beltagy, I.; and Ammar, W. 2019. ScispaCy: fast and robust models for biomedical natural language processing. arXiv preprint arXiv:1902.07669.
Nori, H.; King, N.; McKinney, S. M.; Carignan, D.; and Horvitz, E. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375.
OpenAI; Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; Avila, R.; Babuschkin, I.; Balaji, S.; Balcom, V.; Baltescu, P.; Bao, H.; Bavarian, M.; Belgum, J.; Bello, I.; Berdine, J.; Bernadett-Shapiro, G.; Berner, C.; Bogdonoff, L.; Boiko, O.; Boyd, M.; Brakman, A.-L.; Brockman, G.; Brooks, T.; Brundage, M.; Button, K.; Cai, T.; Campbell, R.; Cann, A.; Carey, B.; Carlson, C.; Carmichael, R.; Chan, B.; Chang, C.; Chantzis, F.; Chen, D.; Chen, S.; Chen, R.; Chen, J.; Chen, M.; Chess, B.; Cho, C.; Chu, C.; Chung, H. W.; Cummings, D.; Currier, J.; Dai, Y.; Decareaux, C.; Degry, T.; Deutsch, N.; Deville, D.; Dhar, A.; Dohan, D.; Dowling, S.; Dunning, S.; Ecoffet, A.; Eleti, A.; Eloundou, T.; Farhi, D.; Fedus, L.; Felix, N.; Fishman, S. P.; Forte, J.; Fulford, I.; Gao, L.; Georges, E.; Gibson, C.; Goel, V.; Gogineni, T.; Goh, G.; GontijoLopes, R.; Gordon, J.; Grafstein, M.; Gray, S.; Greene, R.; Gross, J.; Gu, S. S.; Guo, Y.; Hallacy, C.; Han, J.; Harris, J.; He, Y.; Heaton, M.; Heidecke, J.; Hesse, C.; Hickey, A.; Hickey, W.; Hoeschele, P.; Houghton, B.; Hsu, K.; Hu, S.; Hu, X.; Huizinga, J.; Jain, S.; Jain, S.; Jang, J.; Jiang, A.; Jiang, R.; Jin, H.; Jin, D.; Jomoto, S.; Jonn, B.; Jun, H.; Kaftan, T.; Łukasz Kaiser; Kamali, A.; Kanitscheider, I.; Keskar, N. S.; Khan, T.; Kilpatrick, L.; Kim, J. W.; Kim, C.; Kim, Y.; Kirchner, J. H.; Kiros, J.; Knight, M.; Kokotajlo, D.; Łukasz Kondraciuk; Kondrich, A.; Konstantinidis, A.; Kosic, K.; Krueger, G.; Kuo, V.; Lampe, M.; Lan, I.; Lee, T.; Leike, J.; Leung, J.; Levy, D.; Li, C. M.; Lim, R.; Lin, M.; Lin, S.; Litwin, M.; Lopez, T.; Lowe, R.; Lue, P.; Makanju, A.; Malfacini, K.; Manning, S.; Markov, T.; Markovski, Y.; Martin, B.; Mayer, K.; Mayne, A.; McGrew, B.; McKinney, S. M.; McLeavey, C.; McMillan, P.; McNeil, J.; Medina, D.; Mehta, A.; Menick, J.; Metz, L.; Mishchenko, A.; Mishkin, P.; Monaco, V.; Morikawa, E.; Mossing, D.; Mu, T.; Murati, M.; Murk, O.; Mély, D.; Nair, A.; Nakano, R.; Nayak, R.; Neelakantan, A.; Ngo, R.; Noh, H.; Ouyang, L.; O’Keefe, C.; Pachocki, J.; Paino, A.; Palermo, J.; Pantuliano, A.; Parascandolo, G.; Parish, J.; Parparita, E.; Passos, A.; Pavlov, M.; Peng, A.; Perelman, A.; de Avila Belbute Peres, F.; Petrov,</p>
<p>M.; de Oliveira Pinto, H. P.; Michael; Pokorny; Pokrass, M.; Pong, V. H.; Powell, T.; Power, A.; Power, B.; Proehl, E.; Puri, R.; Radford, A.; Rae, J.; Ramesh, A.; Raymond, C.; Real, F.; Rimbach, K.; Ross, C.; Rotsted, B.; Roussez, H.; Ryder, N.; Saltarelli, M.; Sanders, T.; Santurkar, S.; Sastry, G.; Schmidt, H.; Schnurr, D.; Schulman, J.; Selsam, D.; Sheppard, K.; Sherbakov, T.; Shieh, J.; Shoker, S.; Shyam, P.; Sidor, S.; Sigler, E.; Simens, M.; Sitkin, J.; Slama, K.; Sohl, I.; Sokolowsky, B.; Song, Y.; Staudacher, N.; Such, F. P.; Summers, N.; Sutskever, I.; Tang, J.; Tezak, N.; Thompson, M. B.; Tillet, P.; Tootoonchian, A.; Tseng, E.; Tuggle, P.; Turley, N.; Tworek, J.; Uribe, J. F. C.; Vallone, A.; Vijayvergiya, A.; Voss, C.; Wainwright, C.; Wang, J. J.; Wang, A.; Wang, B.; Ward, J.; Wei, J.; Weinmann, C.; Welihinda, A.; Welinder, P.; Weng, J.; Weng, L.; Wiethoff, M.; Willner, D.; Winter, C.; Wolrich, S.; Wong, H.; Workman, L.; Wu, S.; Wu, J.; Wu, M.; Xiao, K.; Xu, T.; Yoo, S.; Yu, K.; Yuan, Q.; Zaremba, W.; Zellers, R.; Zhang, C.; Zhang, M.; Zhao, S.; Zheng, T.; Zhuang, J.; Zhuk, W.; and Zoph, B. 2024. GPT-4 Technical Report. arXiv:2303.08774. Qi, B.; Zhang, K.; Li, H.; Tian, K.; Zeng, S.; Chen, Z.-R.; and Zhou, B. 2023. Large Language Models are Zero Shot Hypothesis Proposers. arXiv preprint arXiv:2311.05965.
Robertson, S.; Zaragoza, H.; et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval, 3(4): 333-389.
Sebastian, Y.; Siew, E.-G.; and Orimaye, S. O. 2017. Emerging approaches in literature-based discovery: techniques and performance review. The Knowledge Engineering Review, 32: e12.
Stribling, D.; Xia, Y.; Amer, M. K.; Graim, K. S.; Mulligan, C. J.; and Renne, R. 2024. The model student: GPT-4 performance on graduate biomedical science exams. Scientific Reports, 14(1): 5670.
Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal, A. 2022. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509.
Wang, Q.; Downey, D.; Ji, H.; and Hope, T. 2023. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint arXiv:2305.14259.
Wang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; Narang, S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.
Wei, C.-H.; Allot, A.; Lai, P.-T.; Leaman, R.; Tian, S.; Luo, L.; Jin, Q.; Wang, Z.; Chen, Q.; and Lu, Z. 2024. PubTator 3.0: an AI-powered literature resource for unlocking biomedical knowledge. Nucleic Acids Research, gkae235.
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: $24824-24837$.
Wen, Y.; Wang, Z.; and Sun, J. 2023. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. arXiv preprint arXiv:2308.09729.</p>
<p>Xiong, G.; Jin, Q.; Lu, Z.; and Zhang, A. 2024. Benchmarking retrieval-augmented generation for medicine. In Findings of the Association for Computational Linguistics: ACL 2024.</p>
<p>Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T.; Cao, Y.; and Narasimhan, K. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.
Zakka, C.; Shad, R.; Chaurasia, A.; Dalal, A. R.; Kim, J. L.; Moor, M.; Fong, R.; Phillips, C.; Alexander, K.; Ashley, E.; et al. 2024. Almanac-retrieval-augmented language models for clinical medicine. NEJM AI, 1(2): AIoa2300068.
Zhao, P.; Zhang, H.; Yu, Q.; Wang, Z.; Geng, Y.; Fu, F.; Yang, L.; Zhang, W.; and Cui, B. 2024. RetrievalAugmented Generation for AI-Generated Content: A Survey. arXiv preprint arXiv:2402.19473.
Zhou, Y.; Liu, H.; Srivastava, T.; Mei, H.; and Tan, C. 2024. Hypothesis Generation with Large Language Models. arXiv preprint arXiv:2404.04326.</p>
<h2>Appendix</h2>
<h2>Data and Code Availability</h2>
<p>Our data and source code are available at https://anonymous. 4open.science/r/KG-CoI-C203/.</p>
<h2>Details of Dataset Construction</h2>
<p>We created a high-quality test set focusing on achieving a balance between creating an environment that mirrors a real-world hypothesis generation task and ensuring effective quantitative evaluation. Each question within the evaluation dataset is constructed from a subgraph extracted from PubTator3 (Wei et al. 2024), containing information supported by a repository of biomedical literature.</p>
<p>In PubTator3, a selected edge between two key nodes was deliberately removed to create a scenario of incomplete information. The model was then asked to hypothesize the relation between the two nodes. This approach accomplishes two objectives: it mimics the process of hypothesizing with incomplete information while expanding upon the current body of knowledge, and it provides a ground truth for effectively measuring the model's performance.</p>
<p>To select edges for our dataset, we focus on two relation types: "stimulate" and "inhibit". These terms are used in place of the labels given in PubTator3, "positive_correlate" and "negative_correlate," to ensure clarity in their definitions and reduce the risk of confusion in model predictions. We began by selecting a random initial node from within PubTator3. From this node, we identified a connected second node through one of its observed relations. Next, we analyzed the relations of this second node to find a third connected node, and continued to traverse the graph in this manner. To select each subsequent node, we sorted the relations of the previous node by the number of publications that observe each relation as a measure of relevance and significance, descending through the most frequently cited relations until a "stimulate" or "inhibit" is found.</p>
<p>Once a relation is selected, we verified whether the opposing relation (i.e., if "stimulate" is chosen, we verify "inhibit", and vice versa) has a similar number of publications. Specifically, for a selection to be valid, the opposite relation must have less than half the number of publications as the selected relation. This process was repeated until 100 samples of stimulate or inhibit relations had been identified. To find "no_relation" pairings, we randomly selected nodes from those involved in previously identified "stimulate" or "inhibit" relations and verified the absence of a direct connection between them, continuing this process until 100 "no_relation" samples were obtained.</p>
<h2>Discussion on Hallucination Detection</h2>
<p>In the implementation of our KG-CoI system, we select GPT-4o-mini as the model for the LLM-V agent to verify if an LLM-generated claim can be supported by an existing relation triple in a domain-specific knowledge graph.</p>
<p>To justify the choice of GPT-4o-mini in the current implementation, we randomly sample 100 instances of (claim, relation triple) from the actual hallucination detection process, and manually annotate if each instance contains a supported claim. In addition to the results from GPT-4o-mini, we also test GPT-3.5 and GPT-4o to see how GPT-4o-mini performs compared with them.</p>
<p>Table 4 presents the exact match ratio of different annotators. Among the three examined LLMs, GPT-4o aligns the best with human annotators while GPT-3.5-turbo performs the worst. However, considering the prices of the LLMs, GPT-4o-mini turns out to be the most cost-effective choice with a low price but a good performance. Thus, we select GPT-4o-mini to be the LLM-V in our experiments for hallucination detection.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">H1</th>
<th style="text-align: center;">H2</th>
<th style="text-align: center;">3.5</th>
<th style="text-align: center;">4o-mini</th>
<th style="text-align: center;">4o</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">H1</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.76</td>
</tr>
<tr>
<td style="text-align: left;">H2</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: left;">3.5</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: left;">4o-mini</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: left;">4o</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">Price</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\$ 0.50$</td>
<td style="text-align: center;">$\$ 0.15$</td>
<td style="text-align: center;">$\$ 5.00$</td>
</tr>
</tbody>
</table>
<p>Table 4: Exact match of different annotations on claim verification. "H1" stands for the first human annotator. "H2" stands for the second human annotator. "3.5", "4o-mini", "4o" denote GPT-3.5-Turbo, GPT-4o-mini, and GPT-4o, respectively. The prices for 1 M input tokens are listed for the tested LLMs.</p>
<h2>Additional Results on Self-consistency Scaling</h2>
<p>In Table 1 of the main paper, we demonstrate that the selfconsistency (Wang et al. 2022) of multiple runs helps KGCoI find out more accurate hypotheses with increased accuracy and F1 scores. To explore if such an improvement grows with the scaling number of runs in self-consistency, we perform further experiments on KG-CoI to evaluate its performance with the number of runs $N$ in self-consistency to be $N=1,5,10,15$.</p>
<p>Figure 2 shows the performance of various LLMs and methods when we increase the number of runs in the selfconsistency setting. From the results, we can observe that KG-CoI consistently outperforms other compared methods when we scale up the runs in self-consistency. Moreover, the LLM performance tends to improve with the increased number of runs, especially in weak models such as Llama-3.1-8B and Llama-3.1-70B. These results demonstrate the potential of KG-CoI to be further improved by increasing the number of runs when using the self-consistency technique.</p>
<h2>Prompt Templates in Experiments</h2>
<p>To improve the reproducibility of our work and facilitate the follow-up research based on KG-CoI, we provide all prompt templates we used in our experiments in this section, which are shown in Figures 3 - 8. For all tested methods, we add a simple example question in the prompts to instruct LLMs to perform this new hypothesis generation task, which was manually designed by human annotators.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of various methods with different numbers of runs used in the self-consistency setting.</p>
<h1>Prompt template for Direct prompting of LLMs</h1>
<p>You are a leading scientist tasked with hypothesizing interactions between two given biochemical entities in the format "Answer: ['relation']" without the quotes. The answer choices are [‘inhibit’], [‘stimulate’], or ['no_relation'].</p>
<p>Example:
Question: Can we hypothesize a key relation between GENE_JAK1 and CHEMICAL_ruxolitinib?
Output your answer in the format "Answer: [your_answer (among 'inhibit', 'stimulate', and 'no_relation')]". Answer: ['inhibit']</p>
<p>Question: ${{$ question }}
Output your answer in the format "Answer: [your_answer (among 'inhibit', 'stimulate', and 'no_relation')]".</p>
<p>Figure 3: Prompt template for Direct prompting of LLMs.</p>
<h2>Prompt template for CoT prompting of LLMs</h2>
<p>You are a leading scientist tasked with hypothesizing interactions between two given biochemical entities in the format Reasoning:, then Answer:. You must strictly follow this structure. Each reasoning step should be verifiable using a knowledge graph. The answer choices are inhibit, stimulate, or no_relation.</p>
<p>Example:
Question: Can we hypothesize a key relation between GENE_JAK1 and CHEMICAL_ruxolitinib?
Output your answer in the format "Reasoning: your_reasoning_steps", then "Answer: [your_answer (among 'inhibit', 'stimulate', and 'no_relation')]".
Reasoning: Ruxolitinib is a JAK1 inhibitor. Additionally, Ruxolitinib has been shown to treat diseases associated with JAK1 mutations.
Answer: ['inhibit']
Question: ${{$ question }}
Output your answer in the format "Reasoning: your_reasoning_steps", then "Answer: [‘inhibit’],", "Answer: ['stimulate']", or "Answer: ['no_relation']".</p>
<p>Figure 4: Prompt template for CoT prompting of LLMs.</p>
<h2>Additional Case Studies</h2>
<p>In addition to the case study of Llama-3.1-8B shown in Table 2 of the main paper, we perform more case studies of other LLMs on the same input, which are presented in Tables 5, 6, 7. Similar to the study of Llama-3.1-8B, the results show that other LLMs also benefit from the additional KG information provided by KG-CoI, helping them hypothesize the correct relation between Carbon Tetrachloride and SLC17A5 that is not found by any other methods compared.</p>
<h1>Prompt template for RAG prompting of LLMs</h1>
<p>You are a leading scientist tasked with hypothesizing interactions between two given biochemical entities in the format Reasoning:, then Answer:. You must strictly follow this structure. Each reasoning step should be verifiable using a knowledge graph. The answer choices are inhibit, stimulate, or no.relation.</p>
<h2>Example:</h2>
<p>Context: context Question: Can we hypothesize a key relation between GENE_JAK1 and CHEMICAL.ruxolitinib?
Output your answer in the format "Reasoning: your.reasoning_steps", then "Answer: [your_answer (among 'inhibit', 'stimulate', and 'no_relation')]".
Reasoning: Ruxolitinib is a JAK1 inhibitor. Additionally, Ruxolitinib has been shown to treat diseases associated with JAK1 mutations.
Answer: ['inhibit']
Question: ${{$ question $}}$
Output your answer in the format "Reasoning: your.reasoning_steps", then "Answer: ['inhibit’],", "Answer: ['stimulate’]", or "Answer: ['no_relation’]". Make sure to include the brackets.</p>
<p>Figure 5: Prompt template for RAG prompting of LLMs.</p>
<h2>Prompt template for LLM-E in KG-Col</h2>
<p>You are an expert tasked with constructing a query to find documents that will answer a given question. Look it up as if you are creating a google search. Your output must only contain the keywords, nothing else.</p>
<p>You are an expert tasked with constructing a query to find documents that will answer a given question. Look it up as if you are creating a google search. Your output must only contain the keywords, nothing else, so do not say "Here are the relevant keywords: " or anything of that nature. Additionally, you are given several relations that may assist in creating the query. Identify which connections from the list are the strongest and use them to construct the query. Your output will be fed directly into the retriever, so ensure it is in natural language format.</p>
<p>Context: ${{$ context $}}$
Question: ${{$ question $}}$</p>
<p>Figure 6: Prompt template for LLM-E in KG-CoI.</p>
<h1>Prompt template for LLM-A in KG-CoI</h1>
<p>You are a leading scientist tasked with hypothesizing interactions between two given biochemical entities in the format Reasoning:, then Answer:. You must strictly follow this structure. Each reasoning step should be verifiable using a knowledge graph. The answer choices are inhibit, stimulate, or no_relation.</p>
<h2>Example:</h2>
<p>Context: context Question: Can we hypothesize a key relation between GENE_JAK1 and CHEMICAL_ruxolitinib?
Output your answer in the format "Reasoning: your_reasoning_steps", then "Answer: [your_answer (among 'inhibit', 'stimulate', and 'no_relation')]".
Reasoning: Ruxolitinib is a JAK1 inhibitor. Additionally, Ruxolitinib has been shown to treat diseases associated with JAK1 mutations.
Answer: ['inhibit']
Question: {{question}}
Output your answer in the format "Reasoning: your_reasoning_steps", then "Answer: ['inhibit'],", "Answer: ['stimulate']", or "Answer: ['no_relation']". Make sure to include the brackets.</p>
<p>Figure 7: Prompt template for LLM-A in KG-CoI.</p>
<h2>Prompt template for LLM-V in KG-CoI</h2>
<p>You are tasked with finding if a relation extracted from a knowledge graph supports a given statement. For example, stating that a chemical has a negative correlation with a gene supports the chemical being an inhibitor, but stating that a chemical is simply 'associated' with the gene does not. Think it through, and use the format Reasoning: ... then Answer: ['...'] to structure your response. Possible answers include ['yes'] or ['no']</p>
<p>Relation: ${{$ relation $}}$
Statement: ${{$ sentence $}}$</p>
<p>Figure 8: Prompt template for LLM-V in KG-CoI.</p>
<h1>Hypothesis generation question</h1>
<p>Input: Can we hypothesize a key relation between Carbon Tetrachloride and SLC17A5?
Ground Truth: stimulate</p>
<h2>Direct</h2>
<p>Answer: no relation $\gamma$</p>
<h2>CoT</h2>
<p>Reasoning: Carbon tetrachloride (CCl4) is a toxic compound that can cause liver damage. SLC17A5 is a gene that encodes a transporter involved in the transport of organic anions, including bile acids. Research has shown that CCl4 can alter the expression of SLC17A5 in the liver, leading to changes in bile acid transport. However, there is no direct evidence that CCl 4 directly inhibits or stimulates SLC17A5. Instead, the interaction is likely indirect, through CCl4induced liver damage and subsequent changes in gene expression.
Answer: no relation $\gamma$</p>
<h2>RAG</h2>
<p>Retrieved documents: [...]
Reasoning: There is no known interaction between Carbon Tetrachloride and SLC17A5. Carbon Tetrachloride is a chemical compound that can cause liver damage, while SLC17A5 is a gene that encodes a protein involved in the transport of sialic acid. There is no established link between the two.
Answer: no relation $\gamma$</p>
<h2>KG-CoI (Ours)</h2>
<p>Neighbor relations: (Carbon Tetrachloride, cause, Liver Failure), (Liver Failure, associate, SLC17A5), [...]
Retrieved documents: [...]
Reasoning: ((@CHEMICAL_Carbon_Tetrachloride has the relation cause with @DISEASE_Liver_Failure), which has the relation associate with @GENE_SLC17A5). - ((@CHEMICAL_Carbon_Tetrachloride has the relation cause with @DISEASE_Chemical_and_Drug_Induced_Liver_Injury), which has the relation associate with @GENE_SLC17A5). - ((@CHEMICAL_Carbon_Tetrachloride has the relation negative_correlate with @CHEMICAL_Silymarin), which has the relation negative_correlate with @GENE_SLC17A5). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Chemical_and_Drug_Induced_Liver_Injury), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Liver_Failure), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Fibrosis), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Liver_Cirrhosis), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation stimulate with @DISEASE_Chemical_and_Drug_Induced_Liver_Injury), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Liver_Diseases), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation stimulate with @DISEASE_Liver_Failure), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation associate with @DISEASE_Fatty_Liver), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride). - ((@GENE_SLC17A5 has the relation negative_correlate with @CHEMICAL_Silymarin), which has the relation negative_correlate with @CHEMICAL_Carbon_Tetrachloride). ((@GENE_SLC17A5 has the relation stimulate with @DISEASE_Liver_Diseases), which has the relation cause with @CHEMICAL_Carbon_Tetrachloride).
Answer: stimulate $\boldsymbol{\sim}$</p>
<p>Table 5: An example of how KG-CoI helps Llama-3.1-70B generate correct hypotheses and reduce hallucinations. Verified reasoning steps and misinformation are highlighted. The correct answer is marked with " $\boldsymbol{\sim}$ " and the wrong answers are marked with " $\gamma$ ".</p>
<h1>Hypothesis generation question</h1>
<p>Input: Can we hypothesize a key relation between Carbon Tetrachloride and SLC17A5?
Ground Truth: stimulate</p>
<h2>Direct</h2>
<p>Answer: no relation $\gamma$</p>
<h2>CoT</h2>
<p>Reasoning: SLC17A5 is a transporter protein involved in the transport of certain organic anions and is linked to the metabolism of various substances. Carbon Tetrachloride is a toxic compound that can cause liver damage and disrupt normal metabolic processes. There is no direct evidence or established interaction between Carbon Tetrachloride and SLC17A5 that suggests a stimulating or inhibiting effect. Therefore, the relationship is likely to be non-existent.
Answer: no relation $\gamma$</p>
<h2>RAG</h2>
<p>Retrieved documents: [...]
Reasoning: Carbon Tetrachloride is known to cause liver damage and can lead to the impairment of various metabolic processes. However, there is no direct evidence or established interaction between Carbon Tetrachloride and the SLC17A5 gene, which is primarily involved in sialic acid transport in lysosomes. Therefore, any hypothesized relation between Carbon Tetrachloride and SLC17A5 would not be supported by existing biochemical knowledge.
Answer: no relation $\gamma$</p>
<h2>KG-CoI (Ours)</h2>
<p>Neighbor relations: (Carbon Tetrachloride, cause, Liver Failure), (Liver Failure, associate, SLC17A5), [...]
Retrieved documents: [...]
Reasoning: Carbon Tetrachloride is known to cause liver damage and is associated with various liver diseases. The gene SLC17A5 has been shown to have a relationship with liver diseases, including those induced by chemical exposure. Specifically, SLC17A5 is associated with conditions like Chemical and Drug-Induced Liver Injury, which is caused by Carbon Tetrachloride. Therefore, SLC17A5 is likely to be stimulated in response to the liver injury caused by Carbon Tetrachloride.
Answer: stimulate $\boldsymbol{\sim}$</p>
<p>Table 6: An example of how KG-CoI helps GPT-4o-mini generate correct hypotheses and reduce hallucinations. Verified reasoning steps and misinformation are highlighted. The correct answer is marked with " $\nu$ "' and the wrong answers are marked with " $\gamma$ ".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hypothesis generation question</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Input: Can we hypothesize a key relation between Carbon Tetrachloride and SLC17A5?</td>
</tr>
<tr>
<td style="text-align: center;">Ground Truth: stimulate</td>
</tr>
<tr>
<td style="text-align: center;">Direct</td>
</tr>
<tr>
<td style="text-align: center;">Answer: no relation $\not$</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning: Carbon Tetrachloride is a hepatotoxic chemical known to cause liver damage. SLC17A5 is a gene encoding a lysosomal sialic acid transporter, primarily involved in cellular transport mechanisms. There is no direct evidence or established pathway indicating that Carbon Tetrachloride interacts with or affects the function of SLC17A5. <br> Answer: no relation $\not$</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
</tr>
<tr>
<td style="text-align: center;">Retrieved documents: [...]</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning: Carbon Tetrachloride is a chemical compound known for its hepatotoxic effects and is commonly used in research to induce liver damage. SLC17A5 is a gene encoding the sialin protein, which is involved in lysosomal transport of sialic acid. There is no known direct interaction or pathway linking Carbon Tetrachloride to the function or expression of SLC17A5. <br> Answer: no relation $\not$</td>
</tr>
<tr>
<td style="text-align: center;">KG-CoI (Ours)</td>
</tr>
<tr>
<td style="text-align: center;">Neighbor relations: (Carbon Tetrachloride, cause, Liver Failure), (Liver Failure, associate, SLC17A5), [...]</td>
</tr>
<tr>
<td style="text-align: center;">Retrieved documents: [...]</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning: The knowledge graph indicates that Carbon Tetrachloride causes various liver diseases, including liver failure, chemical and drug-induced liver injury, fibrosis, liver cirrhosis, and fatty liver. SLC17A5 is associated with these diseases and is shown to stimulate chemical and drug-induced liver injury, liver failure, and liver diseases. Additionally, SLC17A5 has a negative correlation with Silymarin, which in turn has a negative correlation with Carbon Tetrachloride. This suggests that SLC17A5 is involved in the pathways affected by Carbon Tetrachloride, and its activity is likely stimulated by the presence of Carbon Tetrachloride. <br> Answer: stimulate $\boldsymbol{\sim}$</td>
</tr>
</tbody>
</table>
<p>Table 7: An example of how KG-CoI helps GPT-40 generate correct hypotheses and reduce hallucinations. Verified reasoning steps and misinformation are highlighted. The correct answer is marked with " $\checkmark$ " and the wrong answers are marked with " $#$ ".</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
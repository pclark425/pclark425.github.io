<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6061 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6061</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6061</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-df5e83006f703daef292bce5e61121db6e3621ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df5e83006f703daef292bce5e61121db6e3621ad" target="_blank">A Preliminary Evaluation of ChatGPT in Requirements Information Retrieval</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work empirically evaluates how ChatGPT performs on requirements analysis tasks to derive insights into how generative large language model, represented byChatGPT, inﬂuence the research and practice of natural language processing for requirements engineering.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6061.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6061.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chat Generative Pre-trained Transformer (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative large language model (ChatGPT) queried in a zero-shot, single-turn prompt setting via the OpenAI API to perform requirements information retrieval tasks (classification and extraction); outputs are parsed and evaluated against established RE benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) zero-shot Requirements IR evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A zero-shot, end-to-end use of ChatGPT where each individual requirement artifact instance is sent as a single API call (prompt + instance). The model returns natural-language responses (single-word labels + explanations for classification; itemized lists for extraction) which are parsed (regex, lemmatization, human annotation where needed) to produce structured predictions for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>gpt-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Four publicly available RE test sets (single instance per API call): PROMISE NFR statements (249 instances), App reviews NFR multi-label dataset (1800 reviews), Smarthome user stories (100 user stories, 250 ground-truth domain terms), Chinese App descriptions (50 descriptions, manual feature lists). Prompts were short, single-sentence imperative/interrogative instructions crafted per task; evaluation ran in zero-shot single-turn mode.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a literature-distillation method — the approach is zero-shot prompting for classification and extraction on RE artifacts. Key steps: craft concise task prompts, query model per instance, parse natural-language outputs into structured labels/lists, apply lemmatization/human annotation for extraction outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>For classification tasks: single-word labels (or per-label binary answers for multi-labels) plus optional explanation; for extraction tasks: itemized lists of extracted noun terms or feature-describing phrases (natural-language lists that are post-processed to structured sets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic metrics: precision, recall, and F_beta (beta=2 for App review multi-label task, beta=1 otherwise). Comparisons against published baselines (BERT fine-tuning, SVM multi-label baseline, unsupervised noun-phrase extraction, syntactic parsing + BERT). Parsing used regular expressions; extraction results were lemmatized and validated with human annotation following prior practices.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>ChatGPT produced competitive results versus strong baselines overall, with a consistent pattern of higher recall and lower precision on noisier, general artifacts (App reviews) and more balanced precision/recall on specialized artifacts. Key reported comparisons (F_beta): NFR multi-class (average) Baseline 0.84 vs ChatGPT 0.84; App review multi-label (average) Baseline 0.56 vs ChatGPT 0.54; Term extraction (smart home) Baseline 0.77 vs ChatGPT 0.77; Feature extraction (App descriptions) Baseline 0.54 vs ChatGPT 0.75 (ChatGPT outperformed baseline substantially on feature extraction). Qualitative analysis showed ChatGPT extracts more fine-grained features and tends to include general noun phrases when extracting terms.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>PROMISE NFR dataset (249 NFR statements across 15 projects); Jha & Mahmoud App review NFR dataset (1800 reviews); Smarthome Crowd Requirements Dataset (100 user stories, 250 ground-truth terms); Wu et al. App descriptions dataset (50 Chinese descriptions with manual feature lists). All datasets are public and used in prior RE studies (references given in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Zero-shot prompt sensitivity; non-determinism of API responses (mitigated by temperature=0 and parsing); lower precision on noisy, general user-generated text (App reviews) due to ambiguous language and overlapping NFR categories; extracted terms sometimes include overly general phrases (e.g., 'someone'); extraction granularity mismatch (ChatGPT often returns finer-grained features than ground truth); limited number of tasks/datasets (external validity); no instruction-tuning or fine-tuning on the RE datasets in this evaluation (zero-shot gives a lower-bound).</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Compared to: supervised BERT fine-tuning for NFR multi-class classification, supervised SVM (binary relevance) for App review multi-label classification, unsupervised noun-phrase identification for term extraction, and syntactic parsing + supervised BERT for feature extraction. Findings: ChatGPT matched or closely approximated baselines on several tasks and outperformed the baselines on the App-descriptions feature-extraction benchmark (substantially higher F_beta). ChatGPT tended to trade precision for recall (especially on noisy artifacts) relative to supervised baselines.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6061.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6061.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt learning / RE-domain LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt learning and building Requirements-Engineering-domain large language models (RE-domain LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors propose using prompt learning (instruction/prompt engineering) and developing RE-specific LLMs by fine-tuning or instruction-tuning open-source general LLMs on requirements datasets to improve NLP4RE performance and practical applicability (including privacy-preserving fine-tuning for practitioners).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Prompt learning and RE-domain LLMs (proposed direction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A recommended research direction rather than an implemented system: 1) study prompt learning tailored to requirements retrieval tasks to better exploit domain knowledge embedded in general LLMs; 2) build RE-domain LLMs by fine-tuning open-source LLMs with RE corpora and instruction tuning; 3) combine RE-domain LLMs with formal methods and support practitioner fine-tuning under confidentiality constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a described distillation-from-literature approach; recommended techniques include prompt learning/instruction tuning and supervised fine-tuning on RE datasets to internalize RE knowledge, enabling end-to-end classification and extraction tasks without complex NLP pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Expected outputs are improved task-specific predictions (labels, extracted terms/features), domain-adapted conversational assistants for RE, and possibly models that can produce higher-quality structured RE artifacts; no concrete output schema or example reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>No empirical evaluation reported in this paper for these proposed methods. Authors recommend building high-quality benchmark RE datasets for training and comprehensive evaluation, and performing qualitative studies (questionnaires, interviews) with developers for practical usefulness assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>Paper suggests using and extending existing RE datasets (PROMISE NFR, Smarthome requirements, App review/description sets) and calls for creating higher-quality benchmark requirements corpora; no new datasets were released for RE-domain LLM construction in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Privacy and confidentiality when fine-tuning with practitioner data; lack of high-quality, large-scale RE benchmark datasets to train and evaluate RE-domain LLMs; prompt sensitivity; potential need to reconcile fine-grained extraction vs. existing ground-truth granularity; integration with formal RE methods remains an open research challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Authors note that recent open-source models (per community leaderboards such as AlpacaEval and Chatbot Arena Leaderboard) sometimes match or exceed gpt-3.5-turbo on NLP tasks, suggesting opportunity to build RE-domain LLMs from smaller open models. No empirical comparisons are provided in this paper for prompt-learning/fine-tuning approaches versus other literature-synthesis or distillation methods.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A survey of large language models <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks <em>(Rating: 2)</em></li>
                <li>PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models <em>(Rating: 2)</em></li>
                <li>Natural Language Processing for Requirements Engineering: A Systematic Mapping Study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6061",
    "paper_id": "paper-df5e83006f703daef292bce5e61121db6e3621ad",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "ChatGPT (gpt-3.5-turbo)",
            "name_full": "Chat Generative Pre-trained Transformer (gpt-3.5-turbo)",
            "brief_description": "Generative large language model (ChatGPT) queried in a zero-shot, single-turn prompt setting via the OpenAI API to perform requirements information retrieval tasks (classification and extraction); outputs are parsed and evaluated against established RE benchmarks.",
            "citation_title": "Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting",
            "mention_or_use": "use",
            "system_name": "ChatGPT (gpt-3.5-turbo) zero-shot Requirements IR evaluation",
            "system_description": "A zero-shot, end-to-end use of ChatGPT where each individual requirement artifact instance is sent as a single API call (prompt + instance). The model returns natural-language responses (single-word labels + explanations for classification; itemized lists for extraction) which are parsed (regex, lemmatization, human annotation where needed) to produce structured predictions for evaluation.",
            "llm_model_used": "gpt-3.5-turbo",
            "input_type_and_size": "Four publicly available RE test sets (single instance per API call): PROMISE NFR statements (249 instances), App reviews NFR multi-label dataset (1800 reviews), Smarthome user stories (100 user stories, 250 ground-truth domain terms), Chinese App descriptions (50 descriptions, manual feature lists). Prompts were short, single-sentence imperative/interrogative instructions crafted per task; evaluation ran in zero-shot single-turn mode.",
            "distillation_approach": "Not a literature-distillation method — the approach is zero-shot prompting for classification and extraction on RE artifacts. Key steps: craft concise task prompts, query model per instance, parse natural-language outputs into structured labels/lists, apply lemmatization/human annotation for extraction outputs.",
            "output_type": "For classification tasks: single-word labels (or per-label binary answers for multi-labels) plus optional explanation; for extraction tasks: itemized lists of extracted noun terms or feature-describing phrases (natural-language lists that are post-processed to structured sets).",
            "evaluation_methods": "Automatic metrics: precision, recall, and F_beta (beta=2 for App review multi-label task, beta=1 otherwise). Comparisons against published baselines (BERT fine-tuning, SVM multi-label baseline, unsupervised noun-phrase extraction, syntactic parsing + BERT). Parsing used regular expressions; extraction results were lemmatized and validated with human annotation following prior practices.",
            "results": "ChatGPT produced competitive results versus strong baselines overall, with a consistent pattern of higher recall and lower precision on noisier, general artifacts (App reviews) and more balanced precision/recall on specialized artifacts. Key reported comparisons (F_beta): NFR multi-class (average) Baseline 0.84 vs ChatGPT 0.84; App review multi-label (average) Baseline 0.56 vs ChatGPT 0.54; Term extraction (smart home) Baseline 0.77 vs ChatGPT 0.77; Feature extraction (App descriptions) Baseline 0.54 vs ChatGPT 0.75 (ChatGPT outperformed baseline substantially on feature extraction). Qualitative analysis showed ChatGPT extracts more fine-grained features and tends to include general noun phrases when extracting terms.",
            "datasets_or_benchmarks": "PROMISE NFR dataset (249 NFR statements across 15 projects); Jha & Mahmoud App review NFR dataset (1800 reviews); Smarthome Crowd Requirements Dataset (100 user stories, 250 ground-truth terms); Wu et al. App descriptions dataset (50 Chinese descriptions with manual feature lists). All datasets are public and used in prior RE studies (references given in the paper).",
            "challenges_or_limitations": "Zero-shot prompt sensitivity; non-determinism of API responses (mitigated by temperature=0 and parsing); lower precision on noisy, general user-generated text (App reviews) due to ambiguous language and overlapping NFR categories; extracted terms sometimes include overly general phrases (e.g., 'someone'); extraction granularity mismatch (ChatGPT often returns finer-grained features than ground truth); limited number of tasks/datasets (external validity); no instruction-tuning or fine-tuning on the RE datasets in this evaluation (zero-shot gives a lower-bound).",
            "comparisons_to_other_methods": "Compared to: supervised BERT fine-tuning for NFR multi-class classification, supervised SVM (binary relevance) for App review multi-label classification, unsupervised noun-phrase identification for term extraction, and syntactic parsing + supervised BERT for feature extraction. Findings: ChatGPT matched or closely approximated baselines on several tasks and outperformed the baselines on the App-descriptions feature-extraction benchmark (substantially higher F_beta). ChatGPT tended to trade precision for recall (especially on noisy artifacts) relative to supervised baselines.",
            "uuid": "e6061.0"
        },
        {
            "name_short": "Prompt learning / RE-domain LLMs",
            "name_full": "Prompt learning and building Requirements-Engineering-domain large language models (RE-domain LLMs)",
            "brief_description": "Authors propose using prompt learning (instruction/prompt engineering) and developing RE-specific LLMs by fine-tuning or instruction-tuning open-source general LLMs on requirements datasets to improve NLP4RE performance and practical applicability (including privacy-preserving fine-tuning for practitioners).",
            "citation_title": "Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting",
            "mention_or_use": "mention",
            "system_name": "Prompt learning and RE-domain LLMs (proposed direction)",
            "system_description": "A recommended research direction rather than an implemented system: 1) study prompt learning tailored to requirements retrieval tasks to better exploit domain knowledge embedded in general LLMs; 2) build RE-domain LLMs by fine-tuning open-source LLMs with RE corpora and instruction tuning; 3) combine RE-domain LLMs with formal methods and support practitioner fine-tuning under confidentiality constraints.",
            "llm_model_used": null,
            "input_type_and_size": null,
            "distillation_approach": "Not a described distillation-from-literature approach; recommended techniques include prompt learning/instruction tuning and supervised fine-tuning on RE datasets to internalize RE knowledge, enabling end-to-end classification and extraction tasks without complex NLP pipelines.",
            "output_type": "Expected outputs are improved task-specific predictions (labels, extracted terms/features), domain-adapted conversational assistants for RE, and possibly models that can produce higher-quality structured RE artifacts; no concrete output schema or example reported.",
            "evaluation_methods": "No empirical evaluation reported in this paper for these proposed methods. Authors recommend building high-quality benchmark RE datasets for training and comprehensive evaluation, and performing qualitative studies (questionnaires, interviews) with developers for practical usefulness assessment.",
            "results": null,
            "datasets_or_benchmarks": "Paper suggests using and extending existing RE datasets (PROMISE NFR, Smarthome requirements, App review/description sets) and calls for creating higher-quality benchmark requirements corpora; no new datasets were released for RE-domain LLM construction in this work.",
            "challenges_or_limitations": "Privacy and confidentiality when fine-tuning with practitioner data; lack of high-quality, large-scale RE benchmark datasets to train and evaluate RE-domain LLMs; prompt sensitivity; potential need to reconcile fine-grained extraction vs. existing ground-truth granularity; integration with formal RE methods remains an open research challenge.",
            "comparisons_to_other_methods": "Authors note that recent open-source models (per community leaderboards such as AlpacaEval and Chatbot Arena Leaderboard) sometimes match or exceed gpt-3.5-turbo on NLP tasks, suggesting opportunity to build RE-domain LLMs from smaller open models. No empirical comparisons are provided in this paper for prompt-learning/fine-tuning approaches versus other literature-synthesis or distillation methods.",
            "uuid": "e6061.1"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A survey of large language models",
            "rating": 2
        },
        {
            "paper_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2
        },
        {
            "paper_title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",
            "rating": 2
        },
        {
            "paper_title": "PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models",
            "rating": 2
        },
        {
            "paper_title": "Natural Language Processing for Requirements Engineering: A Systematic Mapping Study",
            "rating": 1
        }
    ],
    "cost": 0.012874749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting</h1>
<p>Jianzhang Zhang ${ }^{1}$ | Yiyang Chen ${ }^{1}$ | Nan Niu ${ }^{2}$ | Yinglin Wang ${ }^{3}$ | Chuang Liu ${ }^{1}$</p>
<h4>Abstract</h4>
<p>${ }^{1}$ Department of Management Science and Engineering, Hangzhou Normal University, Hangzhou, Zhejiang, 311121, P.R.China ${ }^{2}$ Department of Electrical Engineering and Computer Science, University of Cincinnati, Cincinnati, Ohio, 45221, USA ${ }^{3}$ Department of Computer Science and Technology, Shanghai University of Finance and Economics, Shanghai, 200433, P.R.China</p>
<h2>Correspondence</h2>
<p>Jianzhang Zhang, Department of Management Science and Engineering, Hangzhou Normal University, Hangzhou, Zhejiang, 311121, P.R.China
Email: zjzhang@hznu.edu.cn</p>
<h2>Funding information</h2>
<p>This paper is supported by the National Natural Science Foundation of China (Grant No.61873080) and Engineering Research Center of Mobile Health Management System of Chinese Ministry of Education (Grant No. 2022GCZXGH04).</p>
<p>Recently, various illustrative examples have shown the impressive ability of generative large language models (LLMs) to perform NLP related tasks. ChatGPT undoubtedly is the most representative model. We empirically evaluate ChatGPT's performance on requirements information retrieval (IR) tasks to derive insights into designing or developing more effective requirements retrieval methods or tools based on generative LLMs. We design an evaluation framework considering four different combinations of two popular IR tasks and two common artifact types. Under zero-shot setting, evaluation results reveal ChatGPT's promising ability to retrieve requirements relevant information (high recall) and limited ability to retrieve more specific requirements information (low precision). Our evaluation of ChatGPT on requirements IR under zero-shot setting provides preliminary evidence for designing or developing more effective requirements IR methods or tools based on LLMs.</p>
<p>KEYWORDS
ChatGPT, Requirements Information Retrieval, Empirical Evaluation, Natural Language Processing for Requirements Engineering</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1 | INTRODUCTION</h1>
<p>Ubiquitous software systems are acting as the fundamental infrastructure in the era of digital economy [1]. Softwaredefined everything has been a broader trend in the information technology community [2]. Among the software lifecycle phases, requirements engineering is the process of eliciting individual stakeholder requirements and needs and developing them into detailed, agreed requirements documented and specified in such a way that they can serve as the basis for all other system development activities [3]. The requirements engineering process plays a significant role on successfully evolving software projects and keeping competitive in the market [4].</p>
<p>Requirements are a verbalization of decision alternatives regarding a system's functionality and quality [5]. Traditional requirements elicitation usually involves users through surveys, interviews, workshops, and focus groups [6]. In the last decade, online platforms, e.g., Google Play and App Store [7], or social media, e.g., Twitter [8], have been serving as prevalent channels for eliciting user requirements. Feature request [9], bug reports [10, 11], and non-functional concerns [12] are common requirements information among others that are mined from user generated content such as App reviews and twitter posts. Besides of user reviews in App sores, App descriptions are also treated as a valuable domain knowledge source for inspiring feature related requirements elicitation and recommendation [13, 14, 15].</p>
<p>The rapid advancement of deep learning methods and techniques leads to the performance breakthroughs on many computer vision and natural language processing (NLP) tasks [16]. Meanwhile, a wide range of automated methods based on machine learning have been proposed to increase the effectiveness of software engineering lifecycle activities [17]. As natural language are generally used to express software requirements [18, 19] and its role in requirements engineering has long been established [20], applying natural language processing for requirements engineering (NLP4RE) [21] has drawn much attention from software engineering research community. Especially, LLMs have achieved new state of the arts in various NLP tasks in the last decade, which motivate SE researchers and practioners to propose and develop more effective methods and tools for supporting RE processing [22, 23].</p>
<p>Generative LLMs have impressed research and industry communities with its capability to perform various NLP related tasks following human prompts in an chat-based and end-to-end manner [24]. The release of ChatGPT (Chat Generative Pre-trained) has draw wide attention due to its potentially profound impact on various domains1. ChatGPT is driven by a generative large language model trained with reinforcement learning from human feedback ${ }^{1}$. Among others, natural language processing (NLP) and software programming are two prominent impacted domains, which can also glimpsed from the highlighted examples in ChatGPT's official blog ${ }^{1}$. This mainly stems from the code pretraining on large scale corpora and instruction tuning by human prompts [25, 26].</p>
<p>On the one hand, natural language requirements artifacts, including specialized (target to developers) and general ones (target to developers and users), spread all over different software repositories and RE phases [27]. For application in practice, additional efforts, e.g. implementation, deployment, and learning to use, are still needed. On the other hand, existing NLP4RE methods and tools are mostly designed for specific tasks to support certain RE phases [21]. With appropriate prompts, ChatGPT can perform almost all NLP tasks from a wide range of domains in a conversational manner, where the prompts can be viewed as human queries in a dialogue.</p>
<p>These facts motivate us to evaluate the effectiveness of ChatGPT in performing different requirements IR tasks to derive insights into more effective requirements IR methods and tools. ChatGPT is the most representative generative LLMs [26] and Requirements IR is the mostly performed NLP4RE task to support the RE process [21], especially in the elicitation and analysis phases [28].</p>
<p>To this end, we empirically evaluate ChatGPT on 4 requirements benchmark datasets covering two popular IR tasks and two common artifact types under a zero-shot setting. Specific requirements IR tasks consist of requirements</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>classification and feature extraction. Requirements artifacts include software requirements specification statements, App descriptions, and App reviews. The zero-shot setting could provide a performance lower bound.</p>
<p>In summary, the main contributions of this work are threefold:</p>
<ul>
<li>We design a framework for evaluating end-to-end chat-based LLMs on performing requirements IR tasks. Specifically, we empirically evaluate ChatGPT on retrieving requirements information from specialized and general artifacts. all of the evaluation materials are publicly available ${ }^{2}$.</li>
<li>The quantitative and qualitative results demonstrates the feasibility and promising potential of employing LLMs to perform various requirements information retrieval tasks in an end-to-end manner.</li>
<li>we suggest possible directions of future efforts on the NLP4RE research and practice based on our evaluation results and the advancement of LLMs.</li>
</ul>
<p>The rest of this paper is structured as follows: Section II details the proposed evaluation framework. Section III presents and analyzes the evaluation results quantitatively and qualitatively as well as threats to validity. Section IV discusses the implications of our evaluation providing insights into designing or developing more effective requirements retrieval methods or tools based on LLMs. Section V presents the related work. In Section VI, we conclude the paper with future work.</p>
<h1>2 | EVALUATION FRAMEWORK</h1>
<p>Figure 1 depicts our proposed evaluation framework. The whole framework consists of four sequential components: tasks selection, data preparation, querying ChatGPT, and results analysis. We elaborate each component of the framework in the following subsections.</p>
<h2>2.1 | Tasks Selection</h2>
<p>Evaluation tasks are formulated based on two dimensions of requirements IR. Specifically, we characterize the tasks with the involved IR techniques and RE artifact types.</p>
<p>For the IR techniques, we select classification and extraction as both of them are most commonly employed techniques to facilitate the RE process. According to the recent survey [21], more than $35 \%$ (36.76\%) of NLP4RE studies utilize classification and extraction as their main techniques. In the context of requirements IR, the meaning of classification is to classify requirements into different categories, e.g., functional and quality categories. The extraction aims to identify key domain abstractions and concepts, such as, domain terms and feature-describing phrases.</p>
<p>For for the artifact types, we select specialized requirements specifications and general natural language documents as they are most dominant input document types for NLP4RE research [21, 28]. We refer the specialized requirements specifications to those textual requirements artifacts that are mainly target to developers, e.g., software requirements specifications (SRS) and user stories. In contrast, general natural language documents are targeted to both developers and users, e.g., App descriptions and reviews.</p>
<p>Combining the above two dimensions, a $2 \times 2$ evaluation task matrix can be formulated as shown in the first part of Figure 1. The horizontal axis denotes IR techniques and the vertical axis denotes artifact types. For instance, the acronym $S-E$ means extracting information from specialized requirements specifications. In our evaluation, we select</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIGURE 1 The Framework for Evaluating ChatGPT on Requirements IR Tasks</p>
<p>SRS statements and user stories to represent specialized requirements specifications. For general natural language documents, we select App descriptions and App reviews. The rationale behind of that selection is that those four requirements artifacts are commonly used in NLP4RE research [21, 9, 13].</p>
<h1>2.2 Data Preparation</h1>
<p>Data preparation consists of testsets selection and prompts construction. Testsets selection aims to determine the dataset used to test the requirements IR ability of ChatGPT. Prompts construction is to craft the natural language queries to ChatGPT in order to instruct ChatGPT perform specific tasks on the selected dataset. Table 1 summarizes the data preparation result including the brief introduction and descriptive text statistics of each dataset. We present the process and results of testsets selection and prompts construction in the remaining part of this subsection.</p>
<h3>2.2.1 | Testsets Selection</h3>
<p>For evaluation purpose, we choose the datasets used by previous studies, which also facilitate compare the performance of ChatGPT and the methods achieving state of the arts on those datasets. Therefore, we collect the testing datasets used by recent NLP4RE studies. The study inclusion criteria include:</p>
<ol>
<li>The study is presented in academic literature published in major RE research publication venues;</li>
<li>The study provides publicly available replication packages containing codes, datasets, and results.</li>
</ol>
<p>TABLE 1 Evaluation testsets and tasks' prompts</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Testsets Introduction</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Avg. Len.</th>
<th style="text-align: center;">TTR</th>
<th style="text-align: center;">LD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Requirements statements <br> NFR multi-class <br> classification (S-C)</td>
<td style="text-align: center;">249 non-functional requirements across <br> 15 projects from the PROMISE NFR <br> dataset. Each requirement statement is la- <br> beled with one of 4 NFR classes, i.e. usabil- <br> ity, security, operational, and performance. <br> ([29], 2020)</td>
<td style="text-align: center;">249</td>
<td style="text-align: center;">23.20</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">App review NFR multi-Label <br> classification (G-C)</td>
<td style="text-align: center;">1800 reviews across various categories <br> from Google Play and Apple App Store. <br> Each review is assigned at least one NFR <br> label from dependability, performance, us- <br> ability, supportability or a miscellaneous la- <br> bel denoting the review not mentioning <br> NFR. ([12], 2019)</td>
<td style="text-align: center;">1800</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: center;">Term extraction from user <br> stories (S-E)</td>
<td style="text-align: center;">100 smarthome user stories. Each user <br> story is stated with the template "As a <br> <Role>, I want my smart home to <Feature>, <br> so that <Benefit>". A set of 250 domain <br> terms are manually extracted from those <br> requirements. ([30], 2022)</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">32.38</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">0.44</td>
</tr>
<tr>
<td style="text-align: center;">Feature extraction from App <br> descriptions (G-E)</td>
<td style="text-align: center;">50 App Descriptions across 10 application <br> categories. For each App description, a list <br> of manually identified feature describing <br> phrases are provided. ([31], 2021)</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">619.98</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.59</td>
</tr>
</tbody>
</table>
<p>The first criterion ensures that the selected dataset is used by primary SE studies. For the venues, we refer to the NLP4RE survey [21] and other related reviews [28, 32, 17]. The second criterion ensures the easy and fair performance comparison. Applying the above two criteria, we finally choose one dataset for each of four evaluation tasks shown in the figure 1. The four datasets are separately non-functional requirements (NFR) statements, App reviews, user stories, and App descriptions. The first three datasets are in English and the last one is in Chinese, which facilitates the evaluation of ChatGPT on retrieving requirements in different languages. The requirements IR tasks performed on the datasets are multi-class classification, multi-label classification, term extraction, and feature extraction respectively. For comparison purpose, only the testset of each dataset is used to evaluating ChatGPT.</p>
<p>In Table 1, the first column shows tasks names that are composed of the specific IR techniques and artifact types. The second column presents the brief introduction and source of the testset selected for each task. The third and forth columns denote two commonly used summary statistics of text dataset [33, 34], i.e., the dataset size and the average number of tokens contained in each text data instance. Two descriptive measures of text content [35] are present in the last columns in order to better understand the characteristics of the different requirements artifacts contained in each dataset. Type-token ratio (TTR) is calculated by dividing the number of unique words by the total number of words in the dataset, denoting the vocabulary diversity. Lexical density(LD) is the percentage of content words in a dataset, suggesting the content richness. On the two classification datasets, specialized statements have a much</p>
<p>higher TTR than App reviews when expressing NFR. The NFR statements are authored by requirements analysts and developers, who should use more specific and unambiguous technical words to state the quality constraints on the software systems [36]. In contrast, when posting App reviews, users, mostly without technical background, tend to use more general yet ambiguous words when expressing their experience of App quality attributes [7, 37, 12]. In the two extraction datasets, similar TTR difference are also observed stemming from analogous reasons. The LD measures of first three datasets are all about 0.44 . The App descriptions dataset has a obviously higher LD of 0.59 because App descriptions are advertisement text containing not only requirements information, e.g., highlighted features, but also diverse requirements irrelevant information, e.g., slogans, membership, and contact information [14, 31]. Those dataset and text content descriptive statistics provide better understanding of the evaluation tasks and are beneficial to investigating ChatGPT's ability to retrieve requirements information.</p>
<h1>2.2.2 Prompts Construction</h1>
<p>The prompts have an significant impact on the output quality thus performance of LLMs on downstream NLP tasks [38, 24]. To interact with ChatGPT, we handcraft the prompts for each task following the peer coding procedure [39] employed by previous SE research [7, 32]. The first two authors independently craft task prompts based on carefully reading and comprehending the papers listed in Table 1 Both of them have the background of SE research and the experience of practical software development. Three basic guidelines are followed when crafting the tasks prompts:</p>
<ol>
<li>Each prompt should be expressed as a single imperative or interrogative sentence;</li>
<li>The words used in each prompt should come from the corresponding reference paper;</li>
<li>Each prompt should describe the input artifact type, task type, and answer scope.</li>
</ol>
<p>The above guidelines ensure that each task prompt has a brief style and contains vital and necessary information. Recent studies [40, 41] show that detailed and longer prompts do not necessarily enhance the LLMs' performance on downstream tasks. We refine the prompts iteratively with the training set of the selected datasets. Finally, prompts are determined as follows:
(1) NFR multi-class classification: Tag one quality label from (Usability, Security, Operational, Performance) for the following non-functional requirement statement.
(2) App review NFR multi-label classification: Does the following App review involve non-functional requirements about usability (security, operational, performance).
(3) Term extraction: Extract all single-word and multi-word noun terms from the following software requirements statement.
(4) Feature extraction: Extract a list of feature describing phrases from the following Chinese App description.</p>
<p>For the App review multi-label classification task, we craft 4 binary relevance prompts aligning with the experimental setup in [12], whose method acts as a baseline in the subsequent evaluation analysis phase.</p>
<h1>2.2.3 Querying ChatGPT</h1>
<p>We automate the querying process by calling the OpenAI API ${ }^{3}$. Every individual instance of each testset is provided to ChatGPT in one API call. Each query is a combination of the task prompt and a requirement instance as illustrated in the right bottom part of Figure 1. This query format and the single-turn manner jointly guarantee the zero-shot setting. Our evaluation period is from 2023-03-16 to 2023-03-28. The parameters of ChatGPT API calling are set as follows:</p>
<ul>
<li>model: gpt-3.5-turbo. The model version we used in our evaluation is gpt-3.5-turbo, which is the latest model during our evaluation period;</li>
<li>temperature: 0 . This parameter determines what sampling temperature to use, between 0 and 2 . Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We set the temperature to 0 to facilitate the reproduction of our evaluation results;</li>
<li>other parameters: default. Other parameters are set to default values except the model version and temperature.</li>
</ul>
<h3>2.2.4 Results Analysis</h3>
<p>ChatGPT's responses to the queries are not fully structured due to its language model nature. In our evaluation, for the classification tasks, each response is composed of a single-word label followed by some explanation sentences. For the extraction tasks, each response is an itemized list. We therefore parse the ChatGPT's responses to obtain structurally organized results. Regular expressions are employed to automatically obtain the predicted labels for the classification tasks. To compare with the ground truth, we perform lemmatization and human annotation respectively for the results of term extraction and feature extraction following the same previous practice [30, 31].</p>
<p>For performance measures, we adopt the commonly used precision, recall, and $F_{\beta}$ [42], which are defined as follows:</p>
<p>$$
\begin{gathered}
\text { Precision }=\frac{T P}{T P+F P} \
\text { Recall }=\frac{T P}{T P+F N} \
F_{\beta}=\left(1+\beta^{2}\right) \frac{\text { Precision } \times \text { Recall }}{\beta^{2} \times \text { Precision }+ \text { Recall }}
\end{gathered}
$$</p>
<p>where $T P, F P$, and $F N$ denotes the number of true positive, false positive, and false positive cases in the retrieval results. To fairly compare the performance of ChatGPT with those of baselines, $\beta$ is set to 2 for App review multi-labels classification and 1 for other three tasks, which are the same as the selected baselines.</p>
<p>The baselines are those methods proposed by the studies listed in column 2 of Table 1, which have achieved state-of-the-art performance on each dataset. The ground truth of the selected datasets also come from those studies.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Introductions of the selected baselines are as follows:</p>
<ul>
<li>For NFR multi-class classification, the baseline employs supervised BERT fine-tuning to build classifiers [29].</li>
<li>For term extraction, the baseline employs unsupervised noun phrases identification with NLP preprocessing and postprocessing [30].</li>
<li>For App review multi-label classification, the supervised SVM algorithm is used by the baseline, where multiple binary relevance sub-tasks are performed to handle the multi-label classification problem [12].</li>
<li>For feature extraction, the baseline combines unsupervised syntactic parsing and supervised BERT fine-tuning classification [31].</li>
</ul>
<p>We discuss the evaluation and comparison results in detail in the next section.</p>
<h1>3 | QUANTITATIVE AND QUALITATIVE ANALYSIS</h1>
<p>Table 2 presents the performance values of ChatGPT and those of the baselines. The values of columns named "Baselines" come from the reported results of aforementioned studies that proposed the baseline methods. For the first two tasks, the second column presents the category labels. For the other two tasks, the second column shows the software application domains. We highlight the highest average performance values with bold font for each task. In addition, we highlight the best values in the last column named " $F_{\beta}$ Measure".</p>
<p>The performance of the feature extraction baseline is re-calculated based on the test results provided by [31]. Because ChatGPT cannot output false negative cases on feature extraction task as what the supervised classifier [31] dose. Therefore, the precision is computed as the percentage of correct features in the extracted ones and the recall is computed as the percentage of correctly extracted features in the ground truth. Besides, an extracted feature is treated as a true positive one only if it is exactly matched with or is a fine-grained sub-feature of a ground truth feature, which follows the evaluation practice in [31]. The results of NFR multi-class classification is the mean of performance on 15 projects, which aligns with the leave-one-project-out cross-validation setting used by the baseline [29].</p>
<h3>3.1 | Quantitative results analysis</h3>
<p>Overall, ChatGPT achieves competitive results compared with those strong baselines on all datasets when balancing precision and recall values (i.e., $F_{\beta}$ ). Specifically, ChatGPT largely outperforms the baseline with a margin of 0.21 on the feature extraction dataset while obtains approximative performance on other three datasets. Except the feature extraction dataset, ChatGPT acts consistently with higher recall and lower precision values with respect to the baselines. In contrast, ChatGPT exhibits much higher precision and a slightly lower recall values than the baseline in the feature extraction dataset with the margins being 0.42 and 0.03 respectively. The comparable recall values without losing much precision (except for the App review multi-labels classification dataset) partially show the effectiveness of ChatGPT in requirements information retrieval activities, where the recall is more preferred than the precision [43]. These overall observations suggest the potential ability of ChatGPT in retrieving useful requirements information from different types of artifacts in multiple languages.</p>
<p>On the two classification datasets, ChatGPT performs better than the baselines in $F_{\beta}$ on most categories except for operational and performance categories. This demonstrates that ChatGPT trained on large scale data encodes some requirements domain knowledge (i.e. the meaning of different NFR categories) as we do not provide any defi-</p>
<p>TABLE 2 Quantitative Evaluation Results of ChatGPT on Requirements IR Tasks</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Categories <br> /domains</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$F_{\beta}$ Measure</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;">ChatGPT</td>
</tr>
<tr>
<td style="text-align: center;">Requirements statements NFR multi-class classification (S-C)</td>
<td style="text-align: center;">Usability</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Security</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Operational</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">App review NFR multi-Label classification (G-C)</td>
<td style="text-align: center;">Dependability</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.69</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Usability</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Performance</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;">0.24</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Supportability</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.54</td>
</tr>
<tr>
<td style="text-align: center;">Term extraction from user stories (S-E)</td>
<td style="text-align: center;">Smart Home</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;">Feature extraction from App descriptions (G-E)</td>
<td style="text-align: center;">business</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">tool</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.78</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">travel</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.77</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">social</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">news</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">navigation</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">music</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">life</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">education</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.82</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">entertainment</td>
<td style="text-align: center;">0.29</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.73</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.75</td>
</tr>
</tbody>
</table>
<p>nition and positive or negative examples in the prompts. ChatGPT achieves much more balanced precision and recall values on the specialized dataset than it does on the App review dataset. This observation indicates that though encoding some NFR knowledge ChatGPT may includes much more false negative instances into the results under a zero-shot setting when classifying more noisy requirements artifacts [32].</p>
<p>On the two extraction datasets, ChatGPT obtains better or comparable $F_{\beta}$ values on various application categories. This further demonstrates ChatGPT's ability of encoding application domain knowledge besides some RE knowledge. The comparable recall values exhibit ChatGPT's potential of extractive information extraction in an end-to-end manner, which does not need a pipeline of NLP modules [31, 30], e.g., lexical and syntactic analysis. The higher precision of ChatGPT on the feature extraction dataset is partially attributed to the fact that ChatGPT tends to extract more fine-grained features, which increases redundancy also enhances precision.</p>
<p>Summary: the quantitative results on our experimental datasets show ChatGPT's competitive ability to retrieve different categories of NFRs and functional feature-describing phrases from specialized and general requirements artifacts across various application domains.</p>
<h1>3.2 | Qualitative results analysis</h1>
<p>We conduct human content analysis [44] on the outputs of ChatGPT considering the textual characteristics of requirements artifacts (last three columns in Table 1).</p>
<p>For the classification task, ChatGPT performs obviously better on the specialized requirements statements than on the general App reviews as shown in Table 2. The App reviews dataset has a larger average length and smaller TTR than the requirements statements dataset suggesting that user reviews contain more noisy topics [32] and use less category-indicating words. These noise and ambiguity of App reviews may present more challenges for ChatGPT causing its lower performance as illustrated by the following false positive and false negative cases:</p>
<p>Review $_{1}$ : Every movement causes the app to crash. Want to open a new drawing Nope, crash. Can't even access my old drawings now. It's a useless paid app until its fixed.</p>
<p>Response $_{1}$ : Yes, the review involves non-functional requirements about performance as the app is crashing frequently and not allowing the user to access their drawings.</p>
<p>Review $<em 2="2">{2}$ : Actually in my area this is not very accurate. I have to rely on other apps. Annoying so fix it.
Response $</em>$ : No, this review does not involve non-functional requirements about dependability. It only mentions the functional requirement of accuracy.</p>
<p>The true category label of Review $<em 1="1">{1}$ is dependability. ChatGPT incorrectly classifies Review $</em>$ as a performance related NFR. This can be mainly attributed to two points. The Review $<em 1="1">{1}$ 's content contains a mixture of topics on features, bug, and price. Besides, some NFR categories are vaguely-defined and very closely related [12], e.g., performance and dependability involved in Review $</em>$. These two reasons lead to the particularly low precision ( 0.06 in Table 2) of ChatGPT on classifying performance related App reviews that constitute only $1.78 \%$ ( 32 out of 1800) of the total testset.</p>
<p>The Review $_{2}$ has a true label of dependability while ChatGPT predicts it as a functional issue according to the general word "accurate". Though the word "accurate" is identified as an indicator word through manual analysis and statistical filtering on the App reviews training dataset [12], ChatGPT do not use that training set in our zero-shot evaluation setting.</p>
<p>For the extraction task, ChatGPT achieves a better performance on the specialized requirements artifacts either as shown in Table 2. As can be seen in the last two rows of Table 1, the user stories have a lower average length and LD due to its fixed template, while they have a higher TTR stemming from the fact that they are collected from crowd workers in an encouraging creativity settings [45].</p>
<p>Further investigating the extracted terms by ChatGPT, we find the average length of extracted terms is equal to that of ground truth terms. However, there are many general noun phrases included into the extraction results, e.g., "someone" and "something similar". These observations partially explain the lower term extraction precision achieved by ChatGPT compared with the baseline which perform some task-specific filtering. The higher term extraction recall shows ChatGPT's potential NLP ability on diverse user requirements. On the feature extraction dataset, ChatGPT tends to extract fine-grained features leading to an average length of 4.35 that is greater than the average length of 2.56 for the ground truth. The following box presents some fine-grained features extracted by ChatGPT with the ground truth phrases in parentheses ${ }^{4}$ :</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Education Apps: online live learning platform (live); modify the chinese definition of a word (modify the chinese definition); bilingual/monolingual mode(bilingual mode)</p>
<p>Travel Apps: enjoy travel discounts (enjoy discounts); share through WeChat (share)
Tool Apps: clean up screenshots, similar pictures and other useless photos (clean up useless pictures); comprehensive internet security guarantee (security guarantee)</p>
<p>The higher LD of the App descriptions dataset owes to its rich content including advertising slogan, contact and subscription information, content provided by the Apps (e.g. entertainment, news, music Apps). ChatGPT achieves a precision of 0.84 , nearly twice the precision of the baseline, showing its stronger ability of discriminate between feature-related and feature-irrelevant information.</p>
<p>Summary: the qualitative results on our experimental datasets further provide evidence for ChatGPT's quantitative performance including the powerful NLP ability and limited RE domain knowledge.</p>
<h1>3.3 | Threats to Validity</h1>
<p>The major threat to internal validity derives from the prompts used to query ChatGPT. Different prompts may lead to variation of ChatGPT's responses thus influencing the performance. To craft a suitable but not perfect prompt for each task, we follow the peer-coding procedure to iterate the prompts refinement process. The first two authors are responsible for prompts construction and both of them have the background of computer science education and practice, which ensures their good understanding of requirements information retrieval. The prompts construction guidelines and the resulted prompts are also confirmed by the last three authors who have rich SE and information systems experience.</p>
<p>Another threat to internal validity is the non-deterministic property of ChatGPT API calling ${ }^{5}$. This means that a slightly different response is returned in every call, even if the prompt stays the same. We mitigate this by two means. On the one hand, the parameter temperature is set to 0 make the outputs mostly deterministic. On the other hand, we employ parsing process to acquire structurized and normalized results from the natural language responses. For example, under the temperature of 0 , ChatGPT may assign a given App review the same category label with slightly different explanations in two individual API calls, which can be also observed in the OpenAI Playground ${ }^{6}$. With regular expressions, we parse the responses to only extract the category label, which avoid the impact of slightly different explanations on our quantitative evaluation results.</p>
<p>To mitigate the threats to construct validity, we choose the commonly used information retrieval measures, which are also widely used in NLP4RE studies, to evaluate the performance of ChatGPT quantitatively.</p>
<p>The limited number of tasks and datasets in our experiments may influence the generality of our results, which constitutes the main threat to external validity. We mitigate this by selecting most common requirements retrieval tasks, i.e., classification and extraction, and public available datasets. Though there is a shortage of benchmark datasets in RE domain [34, 21], we select two typical research datasets for the specialized requirements artifacts. Specifically, the NFR multi-class classification dataset is built upon PROMISE Software Engineering Repository ${ }^{7}$ and the term ex-</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>traction dataset is built upon Smarthome Crowd Requirements Dataset ${ }^{8}$. The volume of App Store data is massive and increases rapidly [46, 47], while the typical benchmark dataset is also relatively absent as most studies usually build datasets separately to evaluate their proposed methods [32]. Therefore, we select two App store datasets published by recent SE studies for general requirements artifacts.</p>
<h1>4 | RESEARCH IMPLICATIONS</h1>
<p>In this work, we empirically evaluate ChatGPT's ability of retrieving requirements information under a zero-shot setting. The quantitative and qualitative results demonstrates ChatGPT's competitive performance compared with the strong baselines. The most significant implication of our evaluation lies to demonstrating the feasibility and promising potential of employing LLMs to perform various requirements information retrieval tasks in an end-to-end manner. The facility of performing multiple requirements IR tasks in an end-to-end manner with a single LLM will largely the efficiency of RE activities, especially the requirements elicitation and analysis. Recent LLMs leaderboards, e.g., AlpacaEval ${ }^{9}$ and Chatbot Arena Leaderboard ${ }^{10}$, display that some open source models with less parameters size have achieved similar or better NLP ability to gpt-3.5-turbo. Therefore, we carefully suggest the following possible directions of future efforts on the NLP4RE research and practice based on our evaluation results and the advancement of LLMs:</p>
<ol>
<li>Prompt learning for specific requirements retrieval and other NLP4RE tasks could be studied to more sufficiently utilize the domain knowledge encoded by general LLMs.</li>
<li>It is necessary to devise RE domain LLMs based on open source general LLMs with RE domain datasets and instructs to further improve the performance of LLMs on various NLP4RE tasks.</li>
<li>To better support the above two research directions, it is also essential to build high-quality benchmark requirements datasets for training and comprehensively evaluating the devised RE domain LLMs.</li>
<li>It is also valuable to investigate how to combine RE domain LLMs and the formal methods to better support requirements modeling and verification efficiently.</li>
<li>RE practitioners, e.g., analysts, could further fine-tune the RE domain LLMs with their own requirements which are usually business confidentiality. The fine-tuned models can provide requirements analysis assistance with an chat-based and end-to-end manner.</li>
</ol>
<h2>5 | RELATED WORK</h2>
<p>NLP techniques have long been applied to facilitate RE process to support human analysts to carry out requirements analysis [48]. Lexical and syntactic analysis are widely used to design domain-adapted methods or tools to support retrieving various requirements information from textual artifacts, such as abstraction [49], terminology [30], and other requirements knowledge [50]. The most commonly used NLP techniques in requirements IR includes tokenization, part of speech tagging, lemmatization, and syntactic parsing, which are often conducted with off-the-shelf NLP tools, e.g., Stanford CoreNLP, GATE, and NLTK. Other than those basic NLP techniques, classification [51], clustering [52, 53], summarization [54], and topic modeling [55] are also employed to mine requirements information from</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>large collections of textual artifacts.
Software requirements specifications are traditionally the most common input document type for NLP4RE research. As the growing of online user communities in the last decade, user generated content have become prevalent in NLP4RE research, e.g., user posts and reviews in Stack Overflow [56], App stores [57], and social media [58]. Feature request [14], bug report [59], and user opinions [60] are the main retrieved requirements information types. In the same time, NLP techniques also achieved rapid advancement with the help of deep learning. Word embeddings and pre-trained language models are also widely used in NLP4RE research, especially in large scale user feedback mining [56, 59, 53, 57].</p>
<p>Since the launch of ChatGPT on November 30, 2022, it has become a hot research topic to evaluate and apply ChatGPT on various domains [61], which can been glanced through the explosive growth of the volume of ChatGPT related papers. In the SE research community, ChatGPT is employed to perform various SE tasks, e.g., bug fixing [62] and unit test generation [63]. Our empirical evaluation of ChatGPT on multiple requirements IR tasks provides both quantitative and qualitative evidence for the promising potential of ChatGPT on retrieving requirements information. Therefore, our study can be used as a starting point to inspire researchers and practitioners to investigate more effective LLMs based NLP4RE methods and tools.</p>
<h1>6 | CONCLUSION AND FUTURE WORK</h1>
<p>In this work, we conduct a preliminary evaluation of ChatGPT on retrieving requirements information, specifically NFR, features, and domain terms. The quantitative and qualitative results demonstrates its promising potential. We further discuss the implications of ChatGPT for the NLP4RE research and practice based on our evaluation results.</p>
<p>In future, we will perform more extensive evaluations of ChatGPT and other open source LLMs on a wider range of NLP4RE tasks and datasets. Also, qualitative case studies with developers, including questionnaires and interviews, shall be conducted to better understand the practical helpfulness of LLMs in facilitating the RE process.</p>
<h2>references</h2>
<p>[1] Wang C, Zhang N, Wang C. Managing privacy in the digital economy. Fundamental Research 2021;1(5):543-551.
[2] Mei H. Understanding "software-defined" from an OS perspective: technical challenges and research issues. Science China Information Sciences 2017;60:1-3.
[3] Pohl K. Requirements engineering: fundamentals, principles, and techniques. Springer Publishing Company, Incorporated; 2010.
[4] Aurum A, Wohlin C. Engineering and managing software requirements, vol. 1. Springer; 2005.
[5] Aurum A, Wohlin C. The fundamental nature of requirements engineering activities as a decision-making process. Information and Software Technology 2003;45(14):945-954.
[6] Maalej W, Nayebi M, Johann T, Ruhe G. Toward data-driven requirements engineering. IEEE software 2015;33(1):48-54.
[7] Pagano D, Maalej W. User feedback in the appstore: An empirical study. In: 2013 21st IEEE international requirements engineering conference (RE) IEEE; 2013. p. 125-134.
[8] Williams G, Mahmoud A. Mining twitter feeds for software user requirements. In: 2017 IEEE 25th International Requirements Engineering Conference (RE) IEEE; 2017. p. 1-10.</p>
<p>[9] Malgaonkar S, Licorish SA, Savarimuthu BTR. Prioritizing user concerns in app reviews-A study of requests for new features, enhancements and bug fixes. Information and Software Technology 2022;144:106798.
[10] Gao C, Zeng J, Lo D, Xia X, King I, Lyu MR. Understanding in-app advertising issues based on large scale app review analysis. Information and Software Technology 2022;142:106741.
[11] Gao C, Zeng J, Wen Z, Lo D, Xia X, King I, et al. Emerging app issue identification via online joint sentiment-topic tracing. IEEE Transactions on Software Engineering 2021;.
[12] Jha N, Mahmoud A. Mining non-functional requirements from app store reviews. Empirical Software Engineering 2019;24:3659-3695.
[13] Ferrari A, Spoletini P. Strategies, Benefits and Challenges of App Store-inspired Requirements Elicitation. In: 2023 IEEE/ACM 45rd International Conference on Software Engineering (ICSE) IEEE; 2023. p. to appear.
[14] Jiang H, Zhang J, Li X, Ren Z, Lo D, Wu X, et al. Recommending new features from mobile app descriptions. ACM Transactions on Software Engineering and Methodology (TOSEM) 2019;28(4):1-29.
[15] Ebrahimi F, Tushev M, Mahmoud A. Classifying mobile applications using word embeddings. ACM Transactions on Software Engineering and Methodology (TOSEM) 2021;31(2):1-30.
[16] LeCun Y, Bengio Y, Hinton G. Deep learning. nature 2015;521(7553):436-444.
[17] Kotti Z, Galanopoulou R, Spinellis D. Machine learning for software engineering: A tertiary study. ACM Computing Surveys 2023;55(12):1-39.
[18] Ferrari A, Dell'Orletta F, Esuli A, Gervasi V, Gnesi S, et al. Natural language requirements processing: a 4D vision. IEEE SOFTWARE 2017;34(6):28-35.
[19] Ferrari A. Natural language requirements processing: from research to practice. In: Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings; 2018. p. 536-537.
[20] Ryan K. The role of natural language in requirements engineering. In: [1993] Proceedings of the IEEE International Symposium on Requirements Engineering IEEE; 1993. p. 240-242.
[21] Zhao L, Alhoshan W, Ferrari A, Letsholo KJ, Ajagbe MA, Chioasca EV, et al. Natural Language Processing for Requirements Engineering: A Systematic Mapping Study. ACM Computing Surveys (CSUR) 2021;54(3):1-41.
[22] Luo X, Xue Y, Xing Z, Sun J. PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models. In: Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering; 2022. p. 1-13.
[23] Amaral O, Azeem MI, Abualhaija S, Briand LC. Nlp-based automated compliance checking of data processing agreements against gdpr. IEEE Transactions on Software Engineering 2023;.
[24] Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A survey of large language models. arXiv preprint arXiv:230318223 2023;.
[25] Wu T, He S, Liu J, Sun S, Liu K, Han QL, et al. A brief overview of ChatGPT: The history, status quo and potential future development. IEEE/CAA Journal of Automatica Sinica 2023;10(5):1122-1136.
[26] Zhou J, Ke P, Qiu X, Huang M, Zhang J. ChatGPT: potential, prospects, and limitations. Frontiers of Information Technology \&amp; Electronic Engineering 2023;p. 1-6.
[27] Vidoni M. A systematic process for Mining Software Repositories: Results from a systematic literature review. Information and Software Technology 2022;144:106791.</p>
<p>[28] Wang Y, Chen J, Xia X, Jiang B. Intelligent Requirements Elicitation and Modeling: A Literature Review. Journal of Computer Research and Development 2021;58(4):683-705.
[29] Hey T, Keim J, Koziolek A, Tichy WF. Norbert: Transfer learning for requirements classification. In: 2020 IEEE 28th International Requirements Engineering Conference (RE) IEEE; 2020. p. 169-179.
[30] Zhang J, Chen S, Hua J, Niu N, Liu C. Automatic Terminology Extraction and Ranking for Feature Modeling. In: 2022 IEEE 30th International Requirements Engineering Conference (RE) IEEE; 2022. p. 51-63.
[31] Wu H, Deng W, Niu X, Nie C. Identifying key features from app user reviews. In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) IEEE; 2021. p. 922-932.
[32] Dąbrowski J, Letier E, Perini A, Susi A. Analysing app reviews for software engineering: a systematic literature review. Empirical Software Engineering 2022;27(2):1-63.
[33] Pustejovsky J, Stubbs A. Natural Language Annotation for Machine Learning: A guide to corpus-building for applications. " O'Reilly Media, Inc."; 2012.
[34] Ferrari A, Spagnolo GO, Gnesi S. Pure: A dataset of public requirements documents. In: 2017 IEEE 25th International Requirements Engineering Conference (RE) IEEE; 2017. p. 502-505.
[35] Bird S, Klein E, Loper E. Natural language processing with Python: analyzing text with the natural language toolkit. " O'Reilly Media, Inc."; 2009.
[36] Ezzini S, Abualhaija S, Arora C, Sabetzadeh M, Briand LC. Using domain-specific corpora for improved handling of ambiguity in requirements. In: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) IEEE; 2021. p. 1485-1497.
[37] Wang Y, Zhang J. An Aspect-Based Unsupervised Approach for Classifying Non-Functional Requirements on Software Reviews. In: New Trends in Intelligent Software Methodologies, Tools and Techniques IOS Press; 2017.p. 766-778.
[38] Liu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys 2023;55(9):1-35.
[39] Saldaña J. The coding manual for qualitative researchers. The coding manual for qualitative researchers 2021;p. 1-440.
[40] Wang Y, Mishra S, Alipoormolabashi P, Kordi Y, Mirzaei A, Naik A, et al. Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks. In: Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing; 2022. p. 5085-5109.
[41] Gao J, Zhao H, Yu C, Xu R. Exploring the feasibility of chatgpt for event extraction. arXiv preprint arXiv:230303836 2023;.
[42] Schütze H, Manning CD, Raghavan P. Introduction to information retrieval, vol. 39. Cambridge University Press Cambridge; 2008.
[43] Berry DM. Empirical evaluation of tools for hairy requirements engineering tasks. Empirical Software Engineering 2021;26(6):111.
[44] Neuendorf KA. The content analysis guidebook. sage; 2017.
[45] Murukannaiah PK, Ajmeri N, Singh MP. Acquiring creative requirements from the crowd: Understanding the influences of personality and creative potential in Crowd RE. In: 2016 IEEE 24th International Requirements Engineering Conference (RE) IEEE; 2016. p. 176-185.
[46] Martin W, Sarro F, Jia Y, Zhang Y, Harman M. A survey of app store analysis for software engineering. IEEE transactions on software engineering 2016;43(9):817-847.</p>
<p>[47] Al-Subaihin AA, Sarro F, Black S, Capra L, Harman M. App store effects on software engineering practices. IEEE Transactions on Software Engineering 2019;47(2):300-319.
[48] Abbott RJ, Moorhead D. Software requirements and specifications: A survey of needs and languages. Journal of Systems and Software 1981;2(4):297-316.
[49] Peng Z, Rathod P, Niu N, Bhowmik T, Liu H, Shi L, et al. Environment-driven abstraction identification for requirementsbased testing. In: 2021 IEEE 29th International Requirements Engineering Conference (RE) IEEE; 2021. p. 245-256.
[50] Lian X, Rahimi M, Cleland-Huang J, Zhang L, Ferrai R, Smith M. Mining requirements knowledge from collections of domain documents. In: 2016 IEEE 24th international requirements engineering conference (RE) IEEE; 2016. p. 156165 .
[51] Maalej W, Kurtanović Z, Nabil H, Stanik C. On the automatic classification of app reviews. Requirements Engineering 2016;21(3):311-331.
[52] Chen K, Zhang W, Zhao H, Mei H. An approach to constructing feature models based on requirements clustering. In: 13th IEEE International Conference on Requirements Engineering (RE'05) IEEE; 2005. p. 31-40.
[53] Nema P, Anthonysamy P, Taft N, Peddinti ST. Analyzing user perspectives on mobile app privacy at scale. In: 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE) IEEE; 2022. p. 112-124.
[54] Tao C, Guo H, Huang Z. Identifying security issues for mobile applications based on user review summarization. Information and Software Technology 2020;122:106290.
[55] Tushev M, Ebrahimi F, Mahmoud A. Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models. In: 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE) IEEE; 2022. p. 762-773.
[56] Gao Z, Xia X, Grundy J, Lo D, Li YF. Generating question titles for stack overflow from mined code snippets. ACM Transactions on Software Engineering and Methodology (TOSEM) 2020;29(4):1-37.
[57] Zhang J, Hua J, Niu N, Chen S, Savolainen J, Liu C. Exploring privacy requirements gap between developers and end users. Information and Software Technology 2023;154:107090.
[58] Guzman E, Alkadhi R, Seyff N. An exploratory study of twitter messages about software applications. Requirements Engineering 2017;22:387-412.
[59] Haering M, Stanik C, Maalej W. Automatically Matching Bug Reports With Related App Reviews. In: 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021 IEEE; 2021. p. 970-981. https://doi.org/10.1109/ICSE43902.2021.00092.
[60] Lin B, Cassee N, Serebrenik A, Bavota G, Novielli N, Lanza M. Opinion mining for software development: a systematic literature review. ACM Transactions on Software Engineering and Methodology (TOSEM) 2022;31(3):1-41.
[61] Fraiwan M, Khasawneh N. A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. arXiv preprint arXiv:230500237 2023;.
[62] Sobania D, Briesch M, Hanna C, Petke J. An Analysis of the Automatic Bug Fixing Performance of ChatGPT. arXiv preprint arXiv:230108653 2023;.
[63] Yuan Z, Lou Y, Liu M, Ding S, Wang K, Chen Y, et al. No More Manual Tests? Evaluating and Improving ChatGPT for Unit Test Generation. arXiv preprint arXiv:230504207 2023;.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://crowdre.github.io/murukannaiah-smarthome-requirements-dataset/
${ }^{9}$ https://tatsu-lab.github.io/alpaca_eval/
${ }^{10}$ https://huggingface.co/spaces/Imsys/chatbot-arena-leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
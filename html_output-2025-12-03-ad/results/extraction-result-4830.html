<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4830 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4830</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4830</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1085ddc5028be0a6f517bde6c44029abc208c63f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1085ddc5028be0a6f517bde6c44029abc208c63f" target="_blank">Associative Recurrent Memory Transformer</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.</p>
                <p><strong>Paper Abstract:</strong> This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4830.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4830.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associative Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based segment-level recurrent model augmented with layerwise associative key-value memory (quasi-linear key-value matrix with DPFP-3 nonlinearity and gamma normalization correction) to store and recall task-specific information over extremely long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Extends the Recurrent Memory Transformer (RMT) with layerwise associative memory blocks that convert memory tokens into keys/values and update a quasi-linear association matrix A and normalization vector z; uses local transformer self-attention for per-segment processing and constant-time segment updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>associative recurrent memory (layerwise key-value associative matrix / fast weights)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>After each segment, memory tokens are mapped to keys k_i and values v_i and stored in a quasi-linear association matrix A using a fast-weights (delta-rule) style update with importance scalars β_i and gamma (γ) correction for z; queries q_j from tokens use A and z to recall associated values y_j which are passed up the layers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval; long-context question answering (BABILong)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Associative retrieval tasks: remember unique key-value pairs and recall values (Remember) and recall latest value for repeated keys (Rewrite). BABILong: QA over extremely long natural contexts where models must find supporting facts distributed across up to tens of millions of tokens, often with distractors and requiring multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Associative Retrieval (Remember/Rewrite); BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Associative retrieval: trained on 50 pairs in Rewrite, ARMT generalizes to 500 memory updates (10x) with high accuracy; Remember task: ARMT stores more key-value pairs (est. via exact-match metric) than RMT given equal recurrent-state float budget. BABILong QA1 (single supporting fact) reported accuracy up to 79.9% at 50M tokens (paper highlights 79.9% on single-fact QA at 50M tokens); on multiple QA subsets ARMT outperforms competitors across many context lengths (examples: QA1: 100% @64k, 99.9% @128k, 99.3% @500k, 98.5% @1M, 89.4% @10M; QA2–QA5 also show higher scores than RMT/Mamba at many lengths; full table in Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>ARMT substantially improves memory capacity and long-range generalization versus RMT and is robust to large numbers of rewrite operations; in associative retrieval ARMT and Mamba both outperform RMT, but ARMT uses its internal associative memory more effectively. On BABILong ARMT outperforms RMT and Mamba across most tasks and lengths, achieving much larger length-generalization (60x reported for ARMT vs 8x for Mamba from the same 16k training length). Ablations show associative memory (not just layerwise recurrent tokens) is critical for the gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Require sequential (non-parallel) processing of segments (no efficient parallel implementation), slower than some parallelizable approaches on short/medium contexts (<300k tokens); initial quasi-linear associative update without gamma-correction suffers catastrophic forgetting under many erase/insert operations; ARMT struggles on language modeling (Wikitext-103) relative to expectations, showing a tendency to keep only the last segment in memory and underperform on LM tasks unless training/optimization is adjusted.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Combining transformer local self-attention with a layerwise associative fast-weights memory yields much larger usable memory capacity and dramatically improved length extrapolation for retrieval-style QA; gamma-correction (adjusting z updates) is necessary to avoid forgetting in many rewrite operations; associative storage (key-value matrix) is more effective than merely increasing recurrent token counts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4830.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4830.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A segment-level recurrent transformer that maintains memory tokens across segments to provide long-context processing via recurrence rather than full attention on the entire context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Segment-level recurrent model where a small set of memory tokens (recurrent states) are passed across segments and attended to by transformer layers; serves as the baseline recurrent memory architecture extended by ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>segment-level recurrent memory tokens</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains a fixed number of memory tokens (recurrent states) that are appended/passed between segments and used as attention sinks so later segments can attend to summarized past information.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval; BABILong long-context QA (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same associative retrieval and long-context QA tasks used to evaluate memory capacity and long-range question answering; RMT is evaluated as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Associative Retrieval; BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline RMT scores (examples from Table 1): QA1 — 99.6% @64k, 99.1% @128k, 96.4% @500k, 94.2% @1M, 76.4% @10M (some longer-length entries not available). On associative retrieval tasks RMT underperforms ARMT and Mamba in capacity and degrades after exceeding training lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RMT provides effective working memory via recurrent tokens but has lower capacity and worse long-length generalization than ARMT and Mamba; adding parallel layerwise memory (PRMT) without associative mechanism does not improve capacity, indicating associative memory is a major contributor to ARMT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Capacity is limited by number of memory tokens; training requires backpropagation through time for many layers; degrades on tasks that require many rewrite operations or extrapolation far beyond training lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Segment-level recurrence is useful as an attention sink and for long contexts, but the plain recurrent-token scheme has limited capacity compared to associative key-value storage; hierarchical/layerwise associative storage can address capacity limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4830.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4830.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear-time sequence model (SSM-style/selective state spaces) designed for efficient long-context processing with parallelizable components and working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A linear-time sequence modeling approach (selective state spaces) with a recurrent working memory/state that supports efficient processing of long contexts; implemented in a parallelized form allowing faster evaluation on many lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>selective state-space working memory (recurrent state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains recurrent state vectors / working memory (state-space model) that compress and carry forward information across tokens/segments in a parallel-friendly manner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval; BABILong long-context QA (competitor)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on associative retrieval tasks (Remember/Rewrite) and BABILong QA tasks as a competitive long-context model.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Associative Retrieval; BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On BABILong: Mamba-130M examples from Table 1: QA1 — 100% @64k, 99.5% @128k, 92.3% @500k (longer lengths limited by implementation); on other QA subsets Mamba can be strong at medium lengths (e.g., QA4/QA5 show high scores at smaller sizes). Mamba shows 8x length-generalization (128k / 16k trained length) compared to ARMT's reported 60x.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mamba shows strong medium-length performance and good working-memory behavior, but ARMT achieves higher capacity and much larger length extrapolation; Mamba and ARMT both outperform RMT on associative retrieval, but ARMT uses associative key-value memory to better utilize recurrent-state budget.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Current Mamba implementation used in this paper has segmentation/implementation limitations preventing evaluation on very long contexts (500k+ tokens) in some settings; may be more parallel but less flexible for extremely long sequential updates compared to ARMT's associative scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>State-space/selective-state memory methods give efficient parallel performance on medium long contexts, but associative key-value recurrent storage can yield higher effective capacity and superior extrapolation on retrieval-style long-context tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4830.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4830.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Memory Recurrent Memory Transformer (PRMT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation of RMT with layerwise recurrent memory tokens (no associative block) designed to test whether layerwise parallel memory alone explains ARMT gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Variant of RMT that passes memory tokens to the next segment in each layer (layerwise recurrent memory) but does not include the associative memory block; used as an ablation to isolate associative memory effects.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>layerwise recurrent memory tokens (non-associative)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Maintains recurrent memory tokens independently at each layer and passes them between segments, without storing them into an associative key-value matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval; capacity ablation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used in ablation experiments on associative retrieval and memory-capacity estimation to compare to ARMT and RMT.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Associative Retrieval (ablation); BABILong (analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PRMT does not improve over RMT; experiments show PRMT performs similarly to RMT and does not account for ARMT's capacity gains (see Fig. 4(b) and ablation discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Layerwise recurrent memory without associative mechanisms (PRMT) fails to reproduce ARMT's capacity gains, indicating associative key-value storage is the critical contributor to improved memorization and long-range generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not increase memory capacity compared to RMT, and therefore cannot provide the same level of long-range recall or rewrite robustness as associative memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Simply adding more or layerwise recurrent tokens is insufficient; an associative key-value memory mechanism is necessary to substantially increase stored-pair capacity and rewrite robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4830.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4830.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV (Reinventing RNNs for the Transformer Era)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-style model with transformer-era training techniques (RWKV family) intended for efficient long-context sequence modeling; mentioned/attempted as a baseline but training failed in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>rwkv: Reinventing rnns for the transformer era</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent (RNN-like) architecture designed for long-context sequence modeling with linear-time recurrence; proposed as an alternative efficient model for long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>recurrent state (RNN-like)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses recurrent-style state updates across tokens/segments to carry forward context; intended to be efficient for long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative retrieval; BABILong (attempted baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Authors attempted to train RWKV-v5 on associative retrieval and BABILong but were unable to obtain reasonable scores with the same training setup.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Associative Retrieval; BABILong (attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>No reliable performance reported; Appendix J states attempts to train RWKV-v5 on these benchmarks failed and the model did not achieve reasonable scores under the same hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RWKV could not be trained successfully for these tasks under the authors' settings, so no direct comparative performance is provided; authors note RWKV training may require careful hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors failed to train RWKV-v5 for either associative retrieval or BABILong with their settings; suggests RWKV training may be sensitive and not directly comparable without further tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Not enough evidence from this study to evaluate RWKV's memory behavior on the examined long-context retrieval tasks; practical training difficulties can preclude baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4830.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4830.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pre-trained generative transformer model used here in few-shot configuration and in combination with retrieval (RAG) as baseline references on BABILong tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Gpt-4 technical report</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (few-shot) / GPT-4 + RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-4 used in few-shot prompting as a baseline strong LM; also evaluated with retrieval-augmentation (RAG) to compare explicit retrieval vs recurrent/associative memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>no persistent memory in few-shot; retrieval-augmented (external retrieval store) for GPT-4 + RAG</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Few-shot GPT-4 relies on prompt context window only; GPT-4 + RAG augments GPT-4 with an external retrieval mechanism to fetch documents from a large corpus at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong long-context QA (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot and retrieval-augmented GPT-4 evaluated on QA subsets of BABILong to measure how few-shot LMs and retrieval help on extremely long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GPT-4 (few-shot) examples: QA1 — 30.0% @64k, 24.0% @128k (other lengths not reported). GPT-4 + RAG examples: QA1 — 50.0% @64k, 56.0% @128k, 50.0% @500k, 56.0% @1M, 16.0% @10M (table entries from Appendix F.1); performance varies and is lower than ARMT at extreme lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Few-shot GPT-4 performs poorly on very long-context retrieval compared to recurrent/associative memory models; retrieval augmentation (RAG) improves GPT-4 performance at some lengths but still lags ARMT for some long-length single-fact QA and fails on extreme-length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Few-shot LMs are limited by prompt window and struggle on extremely long contexts; RAG helps but retrieval pipelines can fail when the problem requires combining many distributed facts or reasoning over many retrieved items (cited in intro), and retrieval accuracy degrades with extreme context lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Explicit retrieval can improve few-shot LM performance but does not fully solve tasks that require reasoning over many distributed facts; recurrent associative memory approaches can outperform retrieval-augmented LMs on extreme-length single/multi-fact QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Mamba: Linear-time sequence modeling with selective state spaces <em>(Rating: 2)</em></li>
                <li>Linear transformers are secretly fast weight programmers <em>(Rating: 2)</em></li>
                <li>In search of needles in a 11m haystack: Recurrent memory finds what llms miss <em>(Rating: 2)</em></li>
                <li>rwkv: Reinventing rnns for the transformer era <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4830",
    "paper_id": "paper-1085ddc5028be0a6f517bde6c44029abc208c63f",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "ARMT",
            "name_full": "Associative Recurrent Memory Transformer",
            "brief_description": "A transformer-based segment-level recurrent model augmented with layerwise associative key-value memory (quasi-linear key-value matrix with DPFP-3 nonlinearity and gamma normalization correction) to store and recall task-specific information over extremely long contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ARMT",
            "agent_description": "Extends the Recurrent Memory Transformer (RMT) with layerwise associative memory blocks that convert memory tokens into keys/values and update a quasi-linear association matrix A and normalization vector z; uses local transformer self-attention for per-segment processing and constant-time segment updates.",
            "memory_type": "associative recurrent memory (layerwise key-value associative matrix / fast weights)",
            "memory_description": "After each segment, memory tokens are mapped to keys k_i and values v_i and stored in a quasi-linear association matrix A using a fast-weights (delta-rule) style update with importance scalars β_i and gamma (γ) correction for z; queries q_j from tokens use A and z to recall associated values y_j which are passed up the layers.",
            "task_name": "Associative retrieval; long-context question answering (BABILong)",
            "task_description": "Associative retrieval tasks: remember unique key-value pairs and recall values (Remember) and recall latest value for repeated keys (Rewrite). BABILong: QA over extremely long natural contexts where models must find supporting facts distributed across up to tens of millions of tokens, often with distractors and requiring multi-hop reasoning.",
            "benchmark_name": "Associative Retrieval (Remember/Rewrite); BABILong",
            "performance_with_memory": "Associative retrieval: trained on 50 pairs in Rewrite, ARMT generalizes to 500 memory updates (10x) with high accuracy; Remember task: ARMT stores more key-value pairs (est. via exact-match metric) than RMT given equal recurrent-state float budget. BABILong QA1 (single supporting fact) reported accuracy up to 79.9% at 50M tokens (paper highlights 79.9% on single-fact QA at 50M tokens); on multiple QA subsets ARMT outperforms competitors across many context lengths (examples: QA1: 100% @64k, 99.9% @128k, 99.3% @500k, 98.5% @1M, 89.4% @10M; QA2–QA5 also show higher scores than RMT/Mamba at many lengths; full table in Appendix F).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "ARMT substantially improves memory capacity and long-range generalization versus RMT and is robust to large numbers of rewrite operations; in associative retrieval ARMT and Mamba both outperform RMT, but ARMT uses its internal associative memory more effectively. On BABILong ARMT outperforms RMT and Mamba across most tasks and lengths, achieving much larger length-generalization (60x reported for ARMT vs 8x for Mamba from the same 16k training length). Ablations show associative memory (not just layerwise recurrent tokens) is critical for the gains.",
            "limitations_or_challenges": "Require sequential (non-parallel) processing of segments (no efficient parallel implementation), slower than some parallelizable approaches on short/medium contexts (&lt;300k tokens); initial quasi-linear associative update without gamma-correction suffers catastrophic forgetting under many erase/insert operations; ARMT struggles on language modeling (Wikitext-103) relative to expectations, showing a tendency to keep only the last segment in memory and underperform on LM tasks unless training/optimization is adjusted.",
            "key_insights": "Combining transformer local self-attention with a layerwise associative fast-weights memory yields much larger usable memory capacity and dramatically improved length extrapolation for retrieval-style QA; gamma-correction (adjusting z updates) is necessary to avoid forgetting in many rewrite operations; associative storage (key-value matrix) is more effective than merely increasing recurrent token counts.",
            "uuid": "e4830.0",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer",
            "brief_description": "A segment-level recurrent transformer that maintains memory tokens across segments to provide long-context processing via recurrence rather than full attention on the entire context.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "use",
            "agent_name": "RMT",
            "agent_description": "Segment-level recurrent model where a small set of memory tokens (recurrent states) are passed across segments and attended to by transformer layers; serves as the baseline recurrent memory architecture extended by ARMT.",
            "memory_type": "segment-level recurrent memory tokens",
            "memory_description": "Maintains a fixed number of memory tokens (recurrent states) that are appended/passed between segments and used as attention sinks so later segments can attend to summarized past information.",
            "task_name": "Associative retrieval; BABILong long-context QA (baseline comparison)",
            "task_description": "Same associative retrieval and long-context QA tasks used to evaluate memory capacity and long-range question answering; RMT is evaluated as a baseline.",
            "benchmark_name": "Associative Retrieval; BABILong",
            "performance_with_memory": "Baseline RMT scores (examples from Table 1): QA1 — 99.6% @64k, 99.1% @128k, 96.4% @500k, 94.2% @1M, 76.4% @10M (some longer-length entries not available). On associative retrieval tasks RMT underperforms ARMT and Mamba in capacity and degrades after exceeding training lengths.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "RMT provides effective working memory via recurrent tokens but has lower capacity and worse long-length generalization than ARMT and Mamba; adding parallel layerwise memory (PRMT) without associative mechanism does not improve capacity, indicating associative memory is a major contributor to ARMT gains.",
            "limitations_or_challenges": "Capacity is limited by number of memory tokens; training requires backpropagation through time for many layers; degrades on tasks that require many rewrite operations or extrapolation far beyond training lengths.",
            "key_insights": "Segment-level recurrence is useful as an attention sink and for long contexts, but the plain recurrent-token scheme has limited capacity compared to associative key-value storage; hierarchical/layerwise associative storage can address capacity limits.",
            "uuid": "e4830.1",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mamba",
            "name_full": "Mamba: Linear-time sequence modeling with selective state spaces",
            "brief_description": "A linear-time sequence model (SSM-style/selective state spaces) designed for efficient long-context processing with parallelizable components and working memory.",
            "citation_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "mention_or_use": "use",
            "agent_name": "Mamba",
            "agent_description": "A linear-time sequence modeling approach (selective state spaces) with a recurrent working memory/state that supports efficient processing of long contexts; implemented in a parallelized form allowing faster evaluation on many lengths.",
            "memory_type": "selective state-space working memory (recurrent state)",
            "memory_description": "Maintains recurrent state vectors / working memory (state-space model) that compress and carry forward information across tokens/segments in a parallel-friendly manner.",
            "task_name": "Associative retrieval; BABILong long-context QA (competitor)",
            "task_description": "Evaluated on associative retrieval tasks (Remember/Rewrite) and BABILong QA tasks as a competitive long-context model.",
            "benchmark_name": "Associative Retrieval; BABILong",
            "performance_with_memory": "On BABILong: Mamba-130M examples from Table 1: QA1 — 100% @64k, 99.5% @128k, 92.3% @500k (longer lengths limited by implementation); on other QA subsets Mamba can be strong at medium lengths (e.g., QA4/QA5 show high scores at smaller sizes). Mamba shows 8x length-generalization (128k / 16k trained length) compared to ARMT's reported 60x.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mamba shows strong medium-length performance and good working-memory behavior, but ARMT achieves higher capacity and much larger length extrapolation; Mamba and ARMT both outperform RMT on associative retrieval, but ARMT uses associative key-value memory to better utilize recurrent-state budget.",
            "limitations_or_challenges": "Current Mamba implementation used in this paper has segmentation/implementation limitations preventing evaluation on very long contexts (500k+ tokens) in some settings; may be more parallel but less flexible for extremely long sequential updates compared to ARMT's associative scheme.",
            "key_insights": "State-space/selective-state memory methods give efficient parallel performance on medium long contexts, but associative key-value recurrent storage can yield higher effective capacity and superior extrapolation on retrieval-style long-context tasks.",
            "uuid": "e4830.2",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PRMT",
            "name_full": "Parallel Memory Recurrent Memory Transformer (PRMT)",
            "brief_description": "An ablation of RMT with layerwise recurrent memory tokens (no associative block) designed to test whether layerwise parallel memory alone explains ARMT gains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "PRMT",
            "agent_description": "Variant of RMT that passes memory tokens to the next segment in each layer (layerwise recurrent memory) but does not include the associative memory block; used as an ablation to isolate associative memory effects.",
            "memory_type": "layerwise recurrent memory tokens (non-associative)",
            "memory_description": "Maintains recurrent memory tokens independently at each layer and passes them between segments, without storing them into an associative key-value matrix.",
            "task_name": "Associative retrieval; capacity ablation",
            "task_description": "Used in ablation experiments on associative retrieval and memory-capacity estimation to compare to ARMT and RMT.",
            "benchmark_name": "Associative Retrieval (ablation); BABILong (analysis)",
            "performance_with_memory": "PRMT does not improve over RMT; experiments show PRMT performs similarly to RMT and does not account for ARMT's capacity gains (see Fig. 4(b) and ablation discussion).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Layerwise recurrent memory without associative mechanisms (PRMT) fails to reproduce ARMT's capacity gains, indicating associative key-value storage is the critical contributor to improved memorization and long-range generalization.",
            "limitations_or_challenges": "Does not increase memory capacity compared to RMT, and therefore cannot provide the same level of long-range recall or rewrite robustness as associative memory approaches.",
            "key_insights": "Simply adding more or layerwise recurrent tokens is insufficient; an associative key-value memory mechanism is necessary to substantially increase stored-pair capacity and rewrite robustness.",
            "uuid": "e4830.3",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RWKV",
            "name_full": "RWKV (Reinventing RNNs for the Transformer Era)",
            "brief_description": "An RNN-style model with transformer-era training techniques (RWKV family) intended for efficient long-context sequence modeling; mentioned/attempted as a baseline but training failed in this study.",
            "citation_title": "rwkv: Reinventing rnns for the transformer era",
            "mention_or_use": "use",
            "agent_name": "RWKV",
            "agent_description": "A recurrent (RNN-like) architecture designed for long-context sequence modeling with linear-time recurrence; proposed as an alternative efficient model for long contexts.",
            "memory_type": "recurrent state (RNN-like)",
            "memory_description": "Uses recurrent-style state updates across tokens/segments to carry forward context; intended to be efficient for long sequences.",
            "task_name": "Associative retrieval; BABILong (attempted baseline)",
            "task_description": "Authors attempted to train RWKV-v5 on associative retrieval and BABILong but were unable to obtain reasonable scores with the same training setup.",
            "benchmark_name": "Associative Retrieval; BABILong (attempted)",
            "performance_with_memory": "No reliable performance reported; Appendix J states attempts to train RWKV-v5 on these benchmarks failed and the model did not achieve reasonable scores under the same hyperparameters.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "RWKV could not be trained successfully for these tasks under the authors' settings, so no direct comparative performance is provided; authors note RWKV training may require careful hyperparameter tuning.",
            "limitations_or_challenges": "Authors failed to train RWKV-v5 for either associative retrieval or BABILong with their settings; suggests RWKV training may be sensitive and not directly comparable without further tuning.",
            "key_insights": "Not enough evidence from this study to evaluate RWKV's memory behavior on the examined long-context retrieval tasks; practical training difficulties can preclude baseline comparisons.",
            "uuid": "e4830.4",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "Large pre-trained generative transformer model used here in few-shot configuration and in combination with retrieval (RAG) as baseline references on BABILong tasks.",
            "citation_title": "Gpt-4 technical report",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (few-shot) / GPT-4 + RAG",
            "agent_description": "GPT-4 used in few-shot prompting as a baseline strong LM; also evaluated with retrieval-augmentation (RAG) to compare explicit retrieval vs recurrent/associative memory approaches.",
            "memory_type": "no persistent memory in few-shot; retrieval-augmented (external retrieval store) for GPT-4 + RAG",
            "memory_description": "Few-shot GPT-4 relies on prompt context window only; GPT-4 + RAG augments GPT-4 with an external retrieval mechanism to fetch documents from a large corpus at inference.",
            "task_name": "BABILong long-context QA (baseline comparisons)",
            "task_description": "Few-shot and retrieval-augmented GPT-4 evaluated on QA subsets of BABILong to measure how few-shot LMs and retrieval help on extremely long contexts.",
            "benchmark_name": "BABILong",
            "performance_with_memory": "GPT-4 (few-shot) examples: QA1 — 30.0% @64k, 24.0% @128k (other lengths not reported). GPT-4 + RAG examples: QA1 — 50.0% @64k, 56.0% @128k, 50.0% @500k, 56.0% @1M, 16.0% @10M (table entries from Appendix F.1); performance varies and is lower than ARMT at extreme lengths.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Few-shot GPT-4 performs poorly on very long-context retrieval compared to recurrent/associative memory models; retrieval augmentation (RAG) improves GPT-4 performance at some lengths but still lags ARMT for some long-length single-fact QA and fails on extreme-length generalization.",
            "limitations_or_challenges": "Few-shot LMs are limited by prompt window and struggle on extremely long contexts; RAG helps but retrieval pipelines can fail when the problem requires combining many distributed facts or reasoning over many retrieved items (cited in intro), and retrieval accuracy degrades with extreme context lengths.",
            "key_insights": "Explicit retrieval can improve few-shot LM performance but does not fully solve tasks that require reasoning over many distributed facts; recurrent associative memory approaches can outperform retrieval-augmented LMs on extreme-length single/multi-fact QA.",
            "uuid": "e4830.5",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2
        },
        {
            "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "rating": 2
        },
        {
            "paper_title": "Linear transformers are secretly fast weight programmers",
            "rating": 2
        },
        {
            "paper_title": "In search of needles in a 11m haystack: Recurrent memory finds what llms miss",
            "rating": 2
        },
        {
            "paper_title": "rwkv: Reinventing rnns for the transformer era",
            "rating": 1
        }
    ],
    "cost": 0.01525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Associative Recurrent Memory Transformer</h1>
<p>Ivan Rodkin ${ }^{1}$ Yuri Kuratov ${ }^{2,1}$ Aydar Bulatov ${ }^{1}$ Mikhail Burtsev ${ }^{3}$<br>${ }^{1}$ Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia<br>${ }^{2}$ AIRI, Moscow, Russia ${ }^{3}$ London Institute for Mathematical Sciences, London, UK<br>{rodkin.id,yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk</p>
<h4>Abstract</h4>
<p>This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of $79.9 \%$. The source code for training and evaluation is available on github.</p>
<h2>1. Introduction</h2>
<p>Memory plays a crucial role in creating models capable of processing extremely long contexts and utilizing remote past information. Starting from RNNs and evolving through LSTM [13] and Memory Networks [24, 28], we are now in the era of Transformer-based [27] Large Language Models [2, 17, 26]. Various methods for extending transformers context length have emerged [4, 19, 31], including approaches based on transformer segment-level recurrence [3, 5, 6, 20] and novel architectures that combine the efficiency of transformer parallelization during training with recurrence at inference [7, 8, 10, 11, 18]. Alternatively, Retrieval-Augmented Generation (RAG) focuses on retrieving information from external storage [1, 12, 23] or self-retrieving from past inputs [21, 29]. However, retrieval fails on complex tasks that require reasoning over multiple pieces of information [16].</p>
<p>In this work we propose the Associative Recurrent Memory Transformer (ARMT) as an extension of the segment-level recurrent model RMT [3] with associative memory. Compared to RWKV [18] and Mamba [10], which use association-based techniques, ARMT benefits from full local selfattention and has constant time and space complexity of processing new segment, similar to RMT. To study ARMT performance we use the BABILong [16] benchmark, because it allows to generate test samples up to 50 million tokens and beyond, compared to other methods. Additionally, we use Associative Retrieval task with multiple key-value pairs to estimate memory capacity of models.</p>
<p>Main contributions of this work include: (1) a novel ARMT architecture for long context processing with segment-level recurrence and associative memory; (2) demonstration that ARMT outcompetes existing memory based models like RMT [3] and Mamba [10] on associative retrieval and long context processing tasks, achieving $80 \%$ accuracy of single fact QA on unprecedented input</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ARMT augments the transformer's layers with associative memory. (a) RMT architecture. (b) ARMT adds associative memory processing to each layer. (c) Associative memory is updated with layerwise memory representations.
size of 50 million tokens; (3) an original method to evaluate memory capacity in associative retrieval task.</p>
<h1>2. Associative Recurrent Memory Transformer</h1>
<p>We extend RMT <a href="Fig. 1a">3</a> by addition of layerwise associative memory $A_{s}^{l}$ over segmented input $X_{s}^{l}$ (Fig. 1b). At every input segment $s$ for each layer $l$ memory tokens $M_{s-1}^{l+1}$ generated for preceding segment are added to $A_{s}^{l}$ (Fig. 1c) used to update input sequence and memory embeddings:</p>
<p>$$
\left[X_{s}^{l+1} ; M_{s}^{l+1}\right]=\operatorname{TrBlock}\left(\operatorname{AssocBlock}\left(\left[X_{s}^{l} ; M_{s}^{l}\right], A_{s}^{l}\right)\right) ; \quad A_{s}^{l}=\operatorname{MemUpdate}\left(A_{s-1}^{l} ; M_{s-1}^{l+1}\right)
$$</p>
<p>The mechanism of associative block (Fig. 1c) is similar to linear transformers [15], but attends only to special memory tokens and is calculated differenty. After each segment, memory tokens are converted to keys and values via linear mapping and then stored in quasi-linear key-value memory [22] using non-linearity $\phi$. Given a memory token $m_{i} \in M_{s}^{l+1}$, we calculate the keys, values, and importance scalars $\beta_{i}$. We then recall the previous association $\bar{v}<em i="i">{i}$ with this key, add the new value $v</em>}$ to the memory, erase the previous value $\bar{v<em i="i">{i}$ associated with $k</em>$, and update the normalization vector.</p>
<p>$$
\begin{gathered}
k_{i}, v_{i}=W_{K} m_{i}, W_{V} m_{i} ; \quad \beta_{i}=\sigma\left(W_{\beta} m_{i}\right) ; \quad A_{0}^{l}=\mathbf{0} ; \quad z_{0}^{l}=\mathbf{0} \
\bar{v}<em s-1="s-1">{i}=\frac{A</em> \
A_{s}^{l}=A_{s-1}^{l}+\sum_{i} \beta_{i}\left(v_{i}-\bar{v}}^{l} \phi\left(k_{i}\right)}{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)} ; \quad \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}<em i="i">{i}\right) \otimes \phi\left(k</em>\right)
\end{gathered}
$$}\right) ; \quad z_{s}^{l}=z_{s-1}^{l}+\sum_{i} \gamma_{i} \phi\left(k_{i</p>
<p>Once we updated $A_{s}^{l}$ with information from previous segment, we recall an association $y_{j}$ for a token $x_{j}$. Associations $y_{j}$ for each token in the segment are then passed to the next transformer layer:</p>
<p>$$
q_{j}=W_{Q} x_{j} ; \quad y_{j}=\frac{A_{s}^{l} \phi\left(q_{j}\right)}{\left(z_{s}^{l}\right)^{T} \phi\left(q_{j}\right)}
$$</p>
<p>For the non-linearity function $\phi$, we used the proposed in [22] DPFP-3 function because, in this paper, it has shown significant superiority over other methods, which is also consistent with our findings.</p>
<p>Note that without $\gamma_{i}\left(\gamma_{i}=1\right)$ this approach suffers from catastrophic forgetting on some tasks. The reason is that while we erase the information $\bar{v}_{i}$ from the $A$-matrix, the corresponding keys</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ARMT demonstrates strong performance on associative memory tasks. (a) The estimated number of pairs, stored in memory after processing the context with key-value pairs. (b) ARMT is more accurate at operations in memory. Being trained only on 50 key-value pairs from Associative Retrieval Rewrite task, ARMT performs accurate even on 500 memory updates. So the observed generalization factor is 10 (500 pairs / 50 pairs). All data are averaged over 3 runs except RMT and PRMT with 2 runs.
remain in the normalization vector $z_{s}$. As shown in our experiments (Fig. 4(a)), this problem becomes significant when performing hundreds of erase-insert operations with associative memory. To overcome this, we propose to take into account the previous keys in $z_{s}$ when updating it (details are in Appendix F.1).</p>
<p>To determine which part contributes the most to ARMT performance, we also studied RMT with layerwise recurrent memory without associative memory block (Parallel Memory RMT, or PRMT; see Fig. 5 in Appendix F.2).</p>
<h1>3. Evaluation of Associative Retrieval and Long Context Memory Retention</h1>
<p>We test memory capacity of ARMT in comparison to recent computationally efficient long-context models Mamba [10] and RMT [3] on the following two variants of associative retrieval. ${ }^{1}$</p>
<p>Remember task requires memorization of all key-value pairs with unique keys from the prior context with subsequent recalling a value corresponding to one of the keys (Fig. 2a). We estimate the total number of key-value pairs stored in memory, based on the exact-match metric (details are in Appendix B). Since ARMT has the same recurrent memory size as Mamba, calculated as the number of floats in recurrent states, it can be concluded that ARMT makes better use of its internal associative memory. Both ARMT and Mamba outperform RMT on this task. PRMT does not improve RMT performance (Fig. 2a). This indicates that the associative memory plays a critical role in ARMT performance compared to RMT. Additionally, we ablated ARMT on normalization correction, as detailed in Appendix F.</p>
<p>In Rewrite task the keys are not unique and the goal is to recall the latest value that corresponds to one of the keys from the prior context. This task evaluates the model's ability to dynamically change the memory storage. The results, shown in Fig. 2b, indicate that ARMT is robust to the number of memory rewrite operations, while RMT and Mamba experience slight degradation after exceeding their training lengths. Notably, ARMT maintains perfect memory recall on lengths exceeding 10 times those used in training.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ARMT sets a record in long-context processing with reasonable performance on 50 million tokens. Accuracy of models on different lengths from Babilong benchmark: panels a-e represent QA1-5 tasks.</p>
<p>We augment the GPT-2 (137M) model with ARMT to solve the BABILong tasks from a recently introduced benchmark for long context processing in a question-answering form [16]. To answer BABILong questions correctly, models have to find multiple relevant facts distributed across long natural contexts with distractor facts, and combine information from relevant facts. We use the exact match metric to evaluate models' performance. As shown in Fig. 3, ARMT outperforms the competitors in the majority of tasks, especially on long sequences. Being trained on 16 k tokens only, it strongly performs up to 50 million tokens on QA1 single supporting fact (Fig. 3a) and up to 10 million tokens on more complex tasks requiring multi-hop reasoning (Fig. 3b-e). We observe 60x length-generalization on these tasks ( $1 \mathrm{M} / 16 \mathrm{k}$ ), while Mamba has 8 x length-generalization (128k / 16k). For Mamba-130m we consider only the lengths up to 128 k due to its implementation limitations (see Appendix H, and Appendices C and D for training details).</p>
<h1>4. Conclusion</h1>
<p>In this work, we propose and evaluate recurrent memory transformer augmented with associative memory mechanism for long-context processing and find that it scales up to an unprecedented 50 million tokens on the BABILong benchmark. ARMT architecture adds an associative memory mechanism for segment-level recurrent model RMT. Based on our evaluation on associative retrieval tasks ARMT demonstrates significant advantage in memory capacity and generalisation compared to</p>
<p>Mamba and RMT. On BABILong benchmark ARMT dominates alternatives on medium sizes up to 500 K tokens and the only approach with high performance across all five tasks in the range of $500 \mathrm{~K}-10 \mathrm{M}$ tokens.</p>
<p>We conclude that ARMT holds great promise for long-range tasks because of its improved memory capacity, its ability to efficiently handle large numbers of rewrite operations with memory, its ability to extract only relevant information from memory during inference, and its generalization to much longer sequences than it was trained on. We also assume that the ARMT can be used for language modeling (Appendices $G$ and $K$ ). Despite the current results, we believe there is potential to enhance its performance on LM task through further research and optimization.</p>
<p>Since all of the results in this study are obtained on relatively small (137M) models, we also assume that the scaling of our methodology and its combination with other techniques can reveal the significant potential for modern large language models. We believe that investigating the properties of recurrent associative memory remains an exciting area of research.</p>
<h1>Acknowledgements</h1>
<p>We are thankful to SberDevices for granting us access to additional computational resources. This work was supported by a grant for research centers, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730324P540002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.</p>
<h2>References</h2>
<p>[1] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[3] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022.
[4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, 2023.
[5] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts, 2023.
[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019 .</p>
<p>[7] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.
[8] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.
[9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.
[10] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
[11] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.
[12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR, 2020.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.
[14] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024.
[15] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020.
[16] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in a 11m haystack: Recurrent memory finds what llms miss, 2024.
[17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,</p>
<p>Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
[18] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023.
[19] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on</p>
<p>Learning Representations, 2023.
[20] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.
[21] Ohad Rubin and Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421, 2023.
[22] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021.
[23] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.
[24] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks, 2015.
[25] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in neural information processing systems, pages 5998-6008, 2017. URL http://papers.nips. cc/paper/7181-attention-is-all-you-need.
[28] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.
[29] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TrjbxzRcnf-.
[30] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023.
[31] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soaring from 4 k to 400k: Extending llm's context with activation beacon, 2024.</p>
<h1>Appendix A. Related Work</h1>
<p>AutoCompressor [5] is a strategy that stacks memory to minimize information loss at the price of quadratic computation cost.</p>
<p>Recurrent Memory Transformers Recent challenges in long-context processing tasks demonstrated recurrent memory superiority over attention mechanism [16, 31] (Fig. 1 (a)). It was shown that this type of memory performs well even in contexts of size 11M [16]. But still, this memory has some issues with capacity and training. Capacity remains limited as the suggested memory states are limited to a small number of memory tokens. The training is still challenging, as the whole training process requires backpropagation through time for hundreds of layers. Our approach is supposed to mitigate these problems by leveraging the association matrix as a connector for different segments. In contrast to RMT, it has different parameters for memory (linear attention projections described in Section 2) and makes this memory hierarchical by creating different association matrices for different layers.</p>
<p>Context explosion prevention In the attention sinks paper [30], authors demonstrated the need for some sinking tokens for attention for efficient extrapolation in long contexts. Recurrent Memory in RMT [3] as well as our model successfully perform this function. In RMT the memory tokens can act as attention sinks while in our model the very association matrix can play this role.</p>
<p>The ARMT can also be thought of as a kind of compressed-memory RMT [3], that attends to all previous memory tokens with layerwise memory.</p>
<p>Recurrent Sequence Models The problem of the quadratic cost of attention mechanism led to the development of recurrent architectures with transformer-like performance [7]. The vast majority of them are at least related to the State-Space Models (SSMs) [8, 10, 18, 25]. Despite comparable performance with transformers on LM tasks, SSMs are known to be less efficient in memorization tasks, especially when the question is asked after the information [14]. Our model performs well even on these types of tasks, because it has the large and flexible storage for keeping the associations in memory, simultaneously having the direct access to the local context via the vanilla attention.</p>
<h2>Appendix B. Memory capacity estimation</h2>
<h2>Theorem:</h2>
<p>Given:</p>
<p>$$
\text { exact_match }=\alpha ; \quad n=\text { number of pairs; } \quad v=\text { number of possible values }
$$</p>
<p>Then the number of memorized pairs can be estimated with the formula:</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h2>Proof:</h2>
<p>We can precisely predict the associated value if we remember k pairs and then extract the key from these pairs. We output the random value if we obtain the key from any other pair. As a result, the following is the mathematical expectation of an exact match:</p>
<p>$$
\alpha=\frac{k}{n} \cdot 1+\frac{n-k}{n} \cdot \frac{1}{v}=\frac{k}{n}\left(1-\frac{1}{v}\right)+\frac{1}{v}=\frac{k(v-1)+n}{n v}
$$</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h1>Additionally:</h1>
<p>$$
k=n \alpha \frac{v-\frac{1}{\alpha}}{v-1}=n \alpha \frac{v \alpha-1}{v \alpha-\alpha}=n \frac{v \alpha-1}{v-1}
$$</p>
<h2>Appendix C. Curriculum learning</h2>
<p>We train all models with curriculum learning. This means we incrementally increase the complexity of the task during the training. In particular, we train all models on short sequences first and then increase the length of the sequences until it reaches the maximum length ( 16 k tokens for babilong experiments, 200 pairs for Associative Retrieval Remember, 50 pairs for Associative Retireval Rewrite, and 1024 tokens for language modeling experiments ( 8 segments, 128 each)).</p>
<h2>Appendix D. Babilong training details</h2>
<p>We consider segments of size 512 for RMT and ARMT to process the long sequences. The curriculum learning process uses the following number of sequences consecutively: $2,3,5,8,16,32$. So the training ends when we finish training on 32 segments, 512 tokens each. We also randomly sample the number of segments during training, as we find it helps the model generalize better.</p>
<h2>Appendix E. Associative Retrieval training details</h2>
<p>Due to the task's simplicity and training efficiency, we are considering small models (about 500k parameters each) for the Associative Retrieval dataset studies. Every model that we compare has four layers. 128 is the hidden dimension. If the memory dimension parameter (state size in Mamba and memory dimension in ARMT) is present in the model, it is assumed to be 32 ; if the contrary is not indicated.</p>
<p>Moreover, if the model supports segmentation (like RMT and ARMT), we use different segments for different key-value pairs. Thus, if we have, for instance, 200 pairs, 200 segments are passed through the model, and after that, in the 201st segment, we expect the model to generate the value. Both keys and values consist of several integers from 0 to 15 .</p>
<p>We also use the curriculum with the following number of key-value pairs: $1,2,3,5,10,20,40$, 50 , and 200. We increase the key size if necessary, so the final key size for remember task is 3 (so we have $16^{3}$ unique keys) and for rewrite task it remains 1 (16 unique keys). For the Remember task, we also consider sampling different numbers of pairs during training for better generalization.</p>
<h2>Appendix F. Ablation</h2>
<h2>F.1. Gamma-correction</h2>
<p>Due to an improper normalization vector $z$ update, the proposed fast-weights technique [22] (also known as delta-rule) does not have the length generalization (Fig. 4(a)). The information in the association matrix $A$ is erased, but not from $z$, which is the source of the issue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">64k</th>
<th style="text-align: center;">128k</th>
<th style="text-align: center;">500k</th>
<th style="text-align: center;">1M</th>
<th style="text-align: center;">10M</th>
<th style="text-align: center;">50M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QA1 - SINGLE SUPPORTING FACT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 + RAG (Few-shot)</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">- $\left(64,8^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.5 \pm 0.2$</td>
<td style="text-align: center;">$92.3 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.3 \pm 0.9$</td>
<td style="text-align: center;">$98.5 \pm 1.0$</td>
<td style="text-align: center;">$89.4 \pm 8.1$</td>
<td style="text-align: center;">$49,6 \pm 40.4\left(79,9^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">QA2 - TWO SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">71,6</td>
<td style="text-align: center;">54,9</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$95.0 \pm 4.2$</td>
<td style="text-align: center;">$86.7 \pm 6.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.8 \pm 0.2$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$84.4 \pm 4.0$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA3 - THREE SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$91.8 \pm 0.3$</td>
<td style="text-align: center;">$81.4 \pm 1.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$90.4 \pm 2.2$</td>
<td style="text-align: center;">$86.0 \pm 4.8$</td>
<td style="text-align: center;">$79.7 \pm 10.8$</td>
<td style="text-align: center;">$72.1 \pm 14.2$</td>
<td style="text-align: center;">$37.0 \pm 10.3$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA4 - TWO ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$99.7 \pm 0.2$</td>
<td style="text-align: center;">$97.6 \pm 2.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.8 \pm 0.3$</td>
<td style="text-align: center;">$91.5 \pm 1.7$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA5 - THREE ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$98.7 \pm 0.1$</td>
<td style="text-align: center;">$97.5 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$99.0 \pm 0.3$</td>
<td style="text-align: center;">$98.7 \pm 0.4$</td>
<td style="text-align: center;">$98.4 \pm 0.3$</td>
<td style="text-align: center;">$97.3 \pm 0.6$</td>
<td style="text-align: center;">$80.9 \pm 7.6$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Exact match metric on QA1-5 Babilong subsets. Each column corresponds to some constant context length. Context includes both noise sentences and facts. * The 50M exact-match on QA1 is measured on 1 best model. ARMT rows are 3 runs averaged. Mamba rows are 2 runs averaged. The metric is marked bold if its $\pm$ std interval intersects the $\pm$ std interval of the best model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Segmentation</th>
<th style="text-align: center;">Memory Capacity</th>
<th style="text-align: center;">Working Memory</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Length extrapolation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mamba</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">RMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">ARMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Models abilities.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) $\gamma$-correction cures the quasi-linear attention memory. Without correction, the quasi-linear attention with delta-rule struggles to extrapolate on unseen amounts of memory updates. (b) Parallel memory doesn't solve the capacity issue. This means that the associative memory plays an important role in increasing the capacity of the memory.</p>
<p>Note that $z$ is the sum of $\phi\left(k_{i}\right)$. Moreover, we recall the information from previous segments using the inner product of $\phi\left(k_{i}\right)$ and $\phi\left(q_{i}\right)$. This means that for accurate recall, all $\phi\left(k_{i}\right)$ should be orthogonal to each other. Therefore, we can expect $z$ to be a sum of approximately orthogonal vectors.</p>
<p>In this sense, we simplify our task to the task of removing the $\phi\left(k_{i}\right)$ from the sum of vectors orthogonal to $\phi\left(k_{i}\right)$, with the exception of the very $\phi\left(k_{i}\right)$. This means that the presence of $\phi\left(k_{i}\right)$ can be measured by computing the inner product between this sum and $\phi\left(k_{i}\right)$ and dividing it by the length of $\phi\left(k_{i}\right)$ (just taking an orthogonal basis component).</p>
<p>After the insertion of the new information into our memory, we expect this sum to include only one $\phi\left(k_{i}\right)$. Therefore, our $\gamma$-coefficient can be computed with the following formula:</p>
<p>$$
\begin{aligned}
&amp; \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}} \
&amp; z_{s}=z_{s-1}+\sum_{i} \gamma_{i} \phi\left(k_{i}\right)
\end{aligned}
$$</p>
<p>The inner product $\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)$ is divided by the square of $\left|\phi\left(k_{i}\right)\right|$ because the gamma will be further multiplied by $\phi\left(k_{i}\right)$.</p>
<p>We also consider detaching the gamma during training, because it seems to converge better in this case.</p>
<h1>F.2. Associative memory ablation</h1>
<p>To understand, if the associative memory important for memorization tasks, we consider another architecture: Parallel-memory RMT (PRMT) Fig. 5</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Parallel recurrent memory transformer. In contrast to RMT, in PRMT memory tokens are passed to the next segment in each layer.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: ARMT performs similarly to RMT on the language modeling task. Wikitext-103 results. The loss on each of the 128 -sized segments on the test dataset (Normalized with Bits-per-byte [9]). The model is trained for a language modeling task on 8 segments, 128 tokens each. Despite the larger estimated capacity, ARMT struggles to solve the LM task well.</p>
<p>It is different from RMT in the hierarchical memory approach, considering the memory shifts layerwise, just like in ARMT. So this architecture can be thought of as RMT with layerwise memory, while ARMT is RMT with layerwise memory organized in association matrices.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Current mamba implementation allows only the first segment to be long. Other tokens have to be processed consecutively one by one.</p>
<h1>Appendix G. Language Modeling experiments</h1>
<p>We utilized the Wikitext-103 dataset to train ARMT and RMT models in order to assess our architecture's performance on real texts. Next, we examined the cross-entropy losses derived from various model segments on the test dataset, as illustrated in Figure 6. In this manner, we can estimate the amount of language data that can be stored in memory.</p>
<p>Nevertheless, we demonstrate that despite having a larger theoretical capacity than RMT, ARMT still performs similarly to RMT in language modeling.</p>
<p>We use the GPT-2 model as the base model for our architecture changes. We consider the RMT and ARMT models' segment sizes to be equal to 128 tokens and train these models to solve the language modeling task on 8 segments, so in total, we train the model to autoregressively predict 1024 tokens. Then we evaluate the models' performance on each of the 15 segments of test texts (1920 tokens).</p>
<h2>Appendix H. Why is mamba slow for long contexts?</h2>
<p>We faced some difficulties in evaluating mamba on long-context (500k+) due to it's specific segmentation abilities shown in Figure 7.</p>
<h2>Appendix I. Associative Retrieval sample structure</h2>
<p>A sample of Remember dataset contains a concatenated context, query, and answer. The context is a set of key-value pairs $(k, v)$, separated by a special token. All keys are sequences of tokens. Tokens in this sequence can intersect, but the whole sequence corresponding to any key is unique in this particular sample. The query is one of the keys in the context. And the answer is a value corresponding to the key from the query. Thus, we can control the number of pairs in the sample and check how many pairs fit in our memory.</p>
<p>This is how the dataset's sample appears:
<key1>:<value1>,<key2>:<value2>, <key3>:<value3>,<key2>-<value2>
The model is thought to be trained to produce the value following the " - " character.</p>
<h1>Appendix J. RWKV-5 model</h1>
<p>We also tried to train the RWKV-v5 [18] model to slove both associative retrieval and babilong tasks (training from scratch for AR tasks and finetuning 430M model for babilong task). Unfortunately, the model was training poorly and hadn't achieved reasonable scores. We used the very same parameters as for training other models. Perhaps the RWKV training process requires accurate adjustments. However, we haven't succeeded.</p>
<h2>Appendix K. Limitations</h2>
<p>The proposed architecture, however, has several drawbacks. The first is the lack of efficient parallel implementation: you have to process all segments consecutively. This doesn't mean that it is slow. It's fast enough to process millions of tokens in a reasonable time. However, on short and medium-length sequences (less than 300k tokens), it is much slower than, for instance, Mamba and RWKV, which have the parallel form. Moreover, as we have shown in Appendix G, it's still challenging to train ARMT to solve the language modeling task well. However, we believe that this problem is not in the very architecture, but in training process: we observe that ARMT tends to keep in memory only the last segment, and therefore struggles to extrapolate on longer sequences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>We did our best but failed to train RWKV model for associative retrieval and BABILong benchmarks, Appendix J.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
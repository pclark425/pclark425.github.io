<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7909 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7909</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7909</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-13bfdd6d1b2cf7222b4ea8bf1b99aefa7243cf53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/13bfdd6d1b2cf7222b4ea8bf1b99aefa7243cf53" target="_blank">Exploring and Verbalizing Academic Ideas by Concept Co-occurrence</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system, has broad prospects and can assist researchers in expediting the process of discovering new ideas.</p>
                <p><strong>Paper Abstract:</strong> Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the emergence of a new idea can be regarded as the fusion of two concepts that co-occur in an academic paper. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7909.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7909.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLM-LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PLM-based Temporal Link Prediction (PLM-LP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal link prediction method that frames concept-concept connection prediction as a masked language modeling task: BERT is fine-tuned to predict a masked relation token ("related"/"unrelated") for a prompted input containing two concepts and a year token, enabling transferable link prediction across many evolving concept co-occurrence graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Exploring and Verbalizing Academic Ideas by Concept Co-occurrence</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PLM-LP</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Represent a candidate concept pair plus a timestamp as a natural-language prompt (including a prefix token 'Existing' or 'Unknown' indicating prior linkage) with a masked token for the relation; fine-tune a masked language model (BERT) to fill the mask with 'related' or 'unrelated', using positive samples from observed co-occurrence edges and hard negative sampling from k-hop neighborhoods across time.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Concept pair names + timestamp token + prompt (derived from evolving concept co-occurrence graphs built from retrieved papers)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Binary relation prediction per pair (related/unrelated) corresponding to predicted new co-occurrence edges; ranked potential connections</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Masked-language-model prompting with temporal token and prefix prompt ('Existing'/'Unknown'); supervised fine-tuning (MLM objective) rather than few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT (masked language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>240 evolving concept co-occurrence graphs constructed from Elasticsearch retrieval over a large internal academic paper database (authors report ~220M papers in their DB); training positives from yearly snapshots 2000–2021 and negatives from k-hop neighborhoods (k=2,d=5 in reported experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy on adjacency matrix, precision/recall/F1 for all edges and for new edges (new edges = edges appearing for first time in test year); human expert plausibility assessments for predicted future pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average (across 240 graphs): accuracy 0.735; all-edges precision 0.970, recall 0.998, F1 0.981; new-edges precision 0.540, recall 0.988, F1 0.560. Human expert evaluation: substantial fraction (e.g., 52.1% in CS, 48.8% geology) of top predicted future pairs deemed reasonable by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Temporal information is encoded only as a year token; negative-sampling choices and graph sparsity affect training; method cannot generate entirely new concepts (only predicts links among existing concept set); predicted verbalizations may lack logical correctness or experimental specificity (hallucination risk) as noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7909.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-Verb</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-based Idea Verbalization (co-occurrence citation quintuple fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence PLM pipeline that verbalizes newly discovered concept connections into natural-language idea sentences by fine-tuning T5 on co-occurrence citation quintuples (two reference paper sentences + two concept tokens -> target idea sentence).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Exploring and Verbalizing Academic Ideas by Concept Co-occurrence</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Idea verbalization via fine-tuned T5 on co-occurrence citation quintuples</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Construct co-occurrence citation quintuples (p_i, p_j, c_u, c_v, p) where p is a target paper that combines concepts c_u and c_v and p_i/p_j are references; concatenate HEAD/Tail markers, concept tokens and the two reference sentences as the input Seq(q) and fine-tune T5 to generate the idea sentence (p). Also perform unsupervised denoising pretraining on a large set of highly-cited papers before supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Sentences from two reference papers tied to each concept + two concept tokens (HEAD/TAIL markers); structured via Seq(q)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language idea sentence(s) describing the combined idea (typically from abstract/introduction style)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Sequence-to-sequence fine-tuning (denoising pretraining followed by supervised MLE fine-tuning on quintuples); beam search decoding at inference</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (sequence-to-sequence pretrained language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>authors experimented with T5-base and T5-large (precise parameter counts not stated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td>Co-occurrence citation quintuples dataset constructed from ~9.5M target papers and ~19.8M reference papers; filtered to ~92,313 high-quality quintuples (73,852 train / 9,230 valid / 9,231 test); denoising pretraining used ~2M highly-cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Automatic NLG metrics: BLEU, METEOR, ROUGE_L; human evaluation via Turing test (domain experts and non-experts); n-gram overlap measures for relevance/plagiarism analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Benchmark: T5-large denoised fine-tuned: BLEU 26.94, METEOR 13.19, ROUGE_L 17.35 (best among tested models). Human Turing tests showed experts often could not reliably distinguish machine-generated idea sentences from human-written ones (accuracy often near random for 2-option tests; <30% for 3-option). n-gram overlap: 1-gram 40.7%, 2-gram 19.9%, 3-gram 13.8% indicating moderate reuse of input tokens but low verbatim copying.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generated idea texts may lack logical rigor, experimental specificity, and correctness especially in natural/exact sciences; hallucination/unverified claims possible; evaluation metrics for idea quality are imperfect; dataset bias toward highly-cited papers and disciplines with richer citation footprints.</td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7909.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperRobot: Incremental draft generation of scientific ideas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that incrementally generates paper drafts (titles, abstracts, conclusions) using domain knowledge graphs as external context to guide text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperrobot: Incremental draft generation of scientific ideas</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Paperrobot: Incremental draft generation of scientific ideas</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2019</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>PaperRobot</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Uses external domain knowledge graphs combined with text generation modules to produce draft scientific text (titles/abstracts/conclusions) incrementally; cited as an approach that uses structured domain knowledge to aid generation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Domain knowledge graph + textual seeds (papers / metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated scientific text drafts (titles, abstracts, conclusions)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Knowledge-enhanced generation using an external structured KB; not described as few-shot prompting in this paper's citations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as prior art for knowledge-enhanced text generation; details and quantitative results are in the original PaperRobot paper (Wang et al., 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7909.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRAW</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DRAW (reader-writer-reviewer for scientific text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework (cited) that composes reader, writer, and reviewer modules to generate scientific texts, used as related work for academic text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to train your agent to read and write</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>How to train your agent to read and write</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Li Liu, Mengge He, Guanghui Xu, Mingkui Tan, Qi Wu</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2021</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DRAW (reader-writer-reviewer)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Proposes modular components—reader to extract knowledge, writer to generate text, and reviewer to critique outputs—to improve generation of scientific texts; cited as an example of automated scientific writing systems.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific articles and extracted knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated scientific text (abstracts, papers)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Modular training with reading/extraction and generation components; not a pure prompt-engineering approach in this citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned as related work; detailed results are in the original DRAW publication.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7909.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specialized large language model trained for scientific knowledge generation and retrieval; the authors note it could be combined with their link predictor to improve explainability of idea verbalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A large pre-trained transformer optimized for scientific text and knowledge tasks; cited as an example of an LLM designed for science that could be integrated with tools for explainable idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Scientific corpora (papers, datasets) used for pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Scientific text generation and knowledge retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Pretraining and prompt-based conditioning; details in the Galactica paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica (proprietary LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as relevant prior work; specific performance details are in the Galactica paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7909.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue-optimized large language model (RLHF/PPO) capable of human-level text generation; the authors cite it as a strong generator that nonetheless requires professional prompts to discover new ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt: Optimizing language models for dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Chatgpt: Optimizing language models for dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>OpenAI</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>ChatGPT (dialogue-optimized LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Large LLM fine-tuned with reinforcement learning from human feedback (e.g., PPO) to produce conversational and instruction-following text; noted by the authors as powerful but prompt-sensitive for idea discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Natural-language prompts (can include paper snippets, summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Natural-language responses and generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Instruction prompting / conversational prompts; RLHF for alignment</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (family of GPT models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Mentioned qualitatively as having strong generation ability; not evaluated quantitatively in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7909.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-BERT (BERT for knowledge graph completion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that adapts BERT to knowledge graph completion tasks by encoding triples as text and fine-tuning for relation prediction; cited as related to PLM-LP's approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Kgbert: Bert for knowledge graph completion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Kgbert: Bert for knowledge graph completion</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Liang Yao, Chengsheng Mao, Yuan Luo</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2019</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>KG-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Encodes knowledge graph triples as natural-language sequences and fine-tunes BERT to predict relations or complete triples; cited as similar in spirit to using PLMs as implicit knowledge structures.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Encoded triple textualizations (head, relation, tail)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted relation or completed triple (structured knowledge completion)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Fine-tuning BERT on textualized KG triples; not prompt-engineered in the sense of few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as prior work; specifics are in the KG-BERT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7909.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LP-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LP-BERT / Multi-task pre-training for semantic network completion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task pretraining language model approach for semantic network (graph) completion cited by the authors when contrasting prior works with their temporal setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-task pre-training language model for semantic network completion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Multi-task pre-training language model for semantic network completion</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Sen Yang Da Li, Kele Xu, Ming Yi, Yukai He, Huaimin Wang</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2022</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LP-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A multi-task pretraining approach that leverages PLMs to perform semantic network completion; cited as related but not directly applicable to temporal evolving graphs as used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Textualized graph elements / semantic network data</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Completed semantic network edges / relations</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Pretraining + fine-tuning on graph completion objectives</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based PLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as precedent; original paper contains details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7909.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-as-KB (Petroni)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models as Knowledge Bases (Petroni et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating that pretrained language models implicitly store factual knowledge and can be probed to retrieve knowledge, motivating treating PLMs as implicit knowledge graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language models as knowledge bases?</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2019</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LM-as-KB probing / cloze-style probing</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Shows that cloze-style prompts can extract factual knowledge from pretrained LMs, framing PLMs as implicit repositories of facts; cited to justify using PLMs as cross-disciplinary implicit knowledge in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Cloze-style prompts / templates derived from facts</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Predicted factual tokens (answers to cloze prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Cloze prompting (masked token prediction) and probing</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Found that LMs store and can recall many facts; details in Petroni et al. (2019).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7909.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7909.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-as-OKG (Wang2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language models are open knowledge graphs (Wang et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis and framing of PLMs as open knowledge graphs that can encode relational knowledge and be used for knowledge extraction/completion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are open knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Language models are open knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>authors</strong></td>
                            <td>Chenguang Wang, Xiao Liu, Dawn Song</td>
                        </tr>
                        <tr>
                            <td><strong>year</strong></td>
                            <td>2020</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LM-as-Open-KG</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Argues that PLMs can function as open knowledge graphs and provides methods to extract relational knowledge from LMs; cited to support the authors' use of PLMs as an interdisciplinary implicit knowledge source.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>Text prompts / templates probing PLM</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Extracted relations / facts from PLM</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_technique</strong></td>
                            <td>Probing and template-based extraction</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Used to motivate PLM usage; details in Wang et al. (2020).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counterpoint</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring and Verbalizing Academic Ideas by Concept Co-occurrence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models as knowledge bases? <em>(Rating: 2)</em></li>
                <li>Language models are open knowledge graphs <em>(Rating: 2)</em></li>
                <li>Paperrobot: Incremental draft generation of scientific ideas <em>(Rating: 2)</em></li>
                <li>How to train your agent to read and write <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Kgbert: Bert for knowledge graph completion <em>(Rating: 2)</em></li>
                <li>Multi-task pre-training language model for semantic network completion <em>(Rating: 2)</em></li>
                <li>Chatgpt: Optimizing language models for dialogue <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7909",
    "paper_id": "paper-13bfdd6d1b2cf7222b4ea8bf1b99aefa7243cf53",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [
        {
            "name_short": "PLM-LP",
            "name_full": "PLM-based Temporal Link Prediction (PLM-LP)",
            "brief_description": "A temporal link prediction method that frames concept-concept connection prediction as a masked language modeling task: BERT is fine-tuned to predict a masked relation token (\"related\"/\"unrelated\") for a prompted input containing two concepts and a year token, enabling transferable link prediction across many evolving concept co-occurrence graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
            "authors": "Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou",
            "year": null,
            "method_name": "PLM-LP",
            "method_description": "Represent a candidate concept pair plus a timestamp as a natural-language prompt (including a prefix token 'Existing' or 'Unknown' indicating prior linkage) with a masked token for the relation; fine-tune a masked language model (BERT) to fill the mask with 'related' or 'unrelated', using positive samples from observed co-occurrence edges and hard negative sampling from k-hop neighborhoods across time.",
            "input_type": "Concept pair names + timestamp token + prompt (derived from evolving concept co-occurrence graphs built from retrieved papers)",
            "output_type": "Binary relation prediction per pair (related/unrelated) corresponding to predicted new co-occurrence edges; ranked potential connections",
            "prompting_technique": "Masked-language-model prompting with temporal token and prefix prompt ('Existing'/'Unknown'); supervised fine-tuning (MLM objective) rather than few-shot prompting",
            "model_name": "BERT (masked language model)",
            "model_size": null,
            "datasets_used": "240 evolving concept co-occurrence graphs constructed from Elasticsearch retrieval over a large internal academic paper database (authors report ~220M papers in their DB); training positives from yearly snapshots 2000–2021 and negatives from k-hop neighborhoods (k=2,d=5 in reported experiments).",
            "evaluation_metric": "Accuracy on adjacency matrix, precision/recall/F1 for all edges and for new edges (new edges = edges appearing for first time in test year); human expert plausibility assessments for predicted future pairs.",
            "reported_results": "Average (across 240 graphs): accuracy 0.735; all-edges precision 0.970, recall 0.998, F1 0.981; new-edges precision 0.540, recall 0.988, F1 0.560. Human expert evaluation: substantial fraction (e.g., 52.1% in CS, 48.8% geology) of top predicted future pairs deemed reasonable by domain experts.",
            "limitations": "Temporal information is encoded only as a year token; negative-sampling choices and graph sparsity affect training; method cannot generate entirely new concepts (only predicts links among existing concept set); predicted verbalizations may lack logical correctness or experimental specificity (hallucination risk) as noted by the authors.",
            "counterpoint": true,
            "uuid": "e7909.0",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "T5-Verb",
            "name_full": "T5-based Idea Verbalization (co-occurrence citation quintuple fine-tuning)",
            "brief_description": "A sequence-to-sequence PLM pipeline that verbalizes newly discovered concept connections into natural-language idea sentences by fine-tuning T5 on co-occurrence citation quintuples (two reference paper sentences + two concept tokens -&gt; target idea sentence).",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
            "authors": "Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou",
            "year": null,
            "method_name": "Idea verbalization via fine-tuned T5 on co-occurrence citation quintuples",
            "method_description": "Construct co-occurrence citation quintuples (p_i, p_j, c_u, c_v, p) where p is a target paper that combines concepts c_u and c_v and p_i/p_j are references; concatenate HEAD/Tail markers, concept tokens and the two reference sentences as the input Seq(q) and fine-tune T5 to generate the idea sentence (p). Also perform unsupervised denoising pretraining on a large set of highly-cited papers before supervised fine-tuning.",
            "input_type": "Sentences from two reference papers tied to each concept + two concept tokens (HEAD/TAIL markers); structured via Seq(q)",
            "output_type": "Natural-language idea sentence(s) describing the combined idea (typically from abstract/introduction style)",
            "prompting_technique": "Sequence-to-sequence fine-tuning (denoising pretraining followed by supervised MLE fine-tuning on quintuples); beam search decoding at inference",
            "model_name": "T5 (sequence-to-sequence pretrained language model)",
            "model_size": "authors experimented with T5-base and T5-large (precise parameter counts not stated in paper)",
            "datasets_used": "Co-occurrence citation quintuples dataset constructed from ~9.5M target papers and ~19.8M reference papers; filtered to ~92,313 high-quality quintuples (73,852 train / 9,230 valid / 9,231 test); denoising pretraining used ~2M highly-cited papers.",
            "evaluation_metric": "Automatic NLG metrics: BLEU, METEOR, ROUGE_L; human evaluation via Turing test (domain experts and non-experts); n-gram overlap measures for relevance/plagiarism analysis.",
            "reported_results": "Benchmark: T5-large denoised fine-tuned: BLEU 26.94, METEOR 13.19, ROUGE_L 17.35 (best among tested models). Human Turing tests showed experts often could not reliably distinguish machine-generated idea sentences from human-written ones (accuracy often near random for 2-option tests; &lt;30% for 3-option). n-gram overlap: 1-gram 40.7%, 2-gram 19.9%, 3-gram 13.8% indicating moderate reuse of input tokens but low verbatim copying.",
            "limitations": "Generated idea texts may lack logical rigor, experimental specificity, and correctness especially in natural/exact sciences; hallucination/unverified claims possible; evaluation metrics for idea quality are imperfect; dataset bias toward highly-cited papers and disciplines with richer citation footprints.",
            "counterpoint": true,
            "uuid": "e7909.1",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "PaperRobot",
            "name_full": "PaperRobot: Incremental draft generation of scientific ideas",
            "brief_description": "A prior system that incrementally generates paper drafts (titles, abstracts, conclusions) using domain knowledge graphs as external context to guide text generation.",
            "citation_title": "Paperrobot: Incremental draft generation of scientific ideas",
            "mention_or_use": "mention",
            "paper_title": "Paperrobot: Incremental draft generation of scientific ideas",
            "authors": "Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, Yi Luan",
            "year": 2019,
            "method_name": "PaperRobot",
            "method_description": "Uses external domain knowledge graphs combined with text generation modules to produce draft scientific text (titles/abstracts/conclusions) incrementally; cited as an approach that uses structured domain knowledge to aid generation.",
            "input_type": "Domain knowledge graph + textual seeds (papers / metadata)",
            "output_type": "Generated scientific text drafts (titles, abstracts, conclusions)",
            "prompting_technique": "Knowledge-enhanced generation using an external structured KB; not described as few-shot prompting in this paper's citations.",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as prior art for knowledge-enhanced text generation; details and quantitative results are in the original PaperRobot paper (Wang et al., 2019).",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.2",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "DRAW",
            "name_full": "DRAW (reader-writer-reviewer for scientific text generation)",
            "brief_description": "A framework (cited) that composes reader, writer, and reviewer modules to generate scientific texts, used as related work for academic text generation.",
            "citation_title": "How to train your agent to read and write",
            "mention_or_use": "mention",
            "paper_title": "How to train your agent to read and write",
            "authors": "Li Liu, Mengge He, Guanghui Xu, Mingkui Tan, Qi Wu",
            "year": 2021,
            "method_name": "DRAW (reader-writer-reviewer)",
            "method_description": "Proposes modular components—reader to extract knowledge, writer to generate text, and reviewer to critique outputs—to improve generation of scientific texts; cited as an example of automated scientific writing systems.",
            "input_type": "Scientific articles and extracted knowledge",
            "output_type": "Generated scientific text (abstracts, papers)",
            "prompting_technique": "Modular training with reading/extraction and generation components; not a pure prompt-engineering approach in this citation.",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned as related work; detailed results are in the original DRAW publication.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.3",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A large language model for science",
            "brief_description": "A domain-specialized large language model trained for scientific knowledge generation and retrieval; the authors note it could be combined with their link predictor to improve explainability of idea verbalization.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "paper_title": "Galactica: A large language model for science",
            "authors": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic",
            "year": 2022,
            "method_name": "Galactica",
            "method_description": "A large pre-trained transformer optimized for scientific text and knowledge tasks; cited as an example of an LLM designed for science that could be integrated with tools for explainable idea generation.",
            "input_type": "Scientific corpora (papers, datasets) used for pretraining",
            "output_type": "Scientific text generation and knowledge retrieval",
            "prompting_technique": "Pretraining and prompt-based conditioning; details in the Galactica paper",
            "model_name": "Galactica (proprietary LLM)",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as relevant prior work; specific performance details are in the Galactica paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.4",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "A dialogue-optimized large language model (RLHF/PPO) capable of human-level text generation; the authors cite it as a strong generator that nonetheless requires professional prompts to discover new ideas.",
            "citation_title": "Chatgpt: Optimizing language models for dialogue",
            "mention_or_use": "mention",
            "paper_title": "Chatgpt: Optimizing language models for dialogue",
            "authors": "OpenAI",
            "year": 2022,
            "method_name": "ChatGPT (dialogue-optimized LLM)",
            "method_description": "Large LLM fine-tuned with reinforcement learning from human feedback (e.g., PPO) to produce conversational and instruction-following text; noted by the authors as powerful but prompt-sensitive for idea discovery.",
            "input_type": "Natural-language prompts (can include paper snippets, summaries)",
            "output_type": "Natural-language responses and generated ideas",
            "prompting_technique": "Instruction prompting / conversational prompts; RLHF for alignment",
            "model_name": "ChatGPT (family of GPT models)",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Mentioned qualitatively as having strong generation ability; not evaluated quantitatively in this paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.5",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "KG-BERT",
            "name_full": "KG-BERT (BERT for knowledge graph completion)",
            "brief_description": "A method that adapts BERT to knowledge graph completion tasks by encoding triples as text and fine-tuning for relation prediction; cited as related to PLM-LP's approach.",
            "citation_title": "Kgbert: Bert for knowledge graph completion",
            "mention_or_use": "mention",
            "paper_title": "Kgbert: Bert for knowledge graph completion",
            "authors": "Liang Yao, Chengsheng Mao, Yuan Luo",
            "year": 2019,
            "method_name": "KG-BERT",
            "method_description": "Encodes knowledge graph triples as natural-language sequences and fine-tunes BERT to predict relations or complete triples; cited as similar in spirit to using PLMs as implicit knowledge structures.",
            "input_type": "Encoded triple textualizations (head, relation, tail)",
            "output_type": "Predicted relation or completed triple (structured knowledge completion)",
            "prompting_technique": "Fine-tuning BERT on textualized KG triples; not prompt-engineered in the sense of few-shot prompting",
            "model_name": "BERT",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as prior work; specifics are in the KG-BERT paper.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.6",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LP-BERT",
            "name_full": "LP-BERT / Multi-task pre-training for semantic network completion",
            "brief_description": "A multi-task pretraining language model approach for semantic network (graph) completion cited by the authors when contrasting prior works with their temporal setting.",
            "citation_title": "Multi-task pre-training language model for semantic network completion",
            "mention_or_use": "mention",
            "paper_title": "Multi-task pre-training language model for semantic network completion",
            "authors": "Sen Yang Da Li, Kele Xu, Ming Yi, Yukai He, Huaimin Wang",
            "year": 2022,
            "method_name": "LP-BERT",
            "method_description": "A multi-task pretraining approach that leverages PLMs to perform semantic network completion; cited as related but not directly applicable to temporal evolving graphs as used in this paper.",
            "input_type": "Textualized graph elements / semantic network data",
            "output_type": "Completed semantic network edges / relations",
            "prompting_technique": "Pretraining + fine-tuning on graph completion objectives",
            "model_name": "BERT-based PLM",
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Cited as precedent; original paper contains details.",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.7",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LM-as-KB (Petroni)",
            "name_full": "Language Models as Knowledge Bases (Petroni et al.)",
            "brief_description": "Work demonstrating that pretrained language models implicitly store factual knowledge and can be probed to retrieve knowledge, motivating treating PLMs as implicit knowledge graphs.",
            "citation_title": "Language models as knowledge bases?",
            "mention_or_use": "mention",
            "paper_title": "Language models as knowledge bases?",
            "authors": "Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",
            "year": 2019,
            "method_name": "LM-as-KB probing / cloze-style probing",
            "method_description": "Shows that cloze-style prompts can extract factual knowledge from pretrained LMs, framing PLMs as implicit repositories of facts; cited to justify using PLMs as cross-disciplinary implicit knowledge in this paper.",
            "input_type": "Cloze-style prompts / templates derived from facts",
            "output_type": "Predicted factual tokens (answers to cloze prompts)",
            "prompting_technique": "Cloze prompting (masked token prediction) and probing",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Found that LMs store and can recall many facts; details in Petroni et al. (2019).",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.8",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "LM-as-OKG (Wang2020)",
            "name_full": "Language models are open knowledge graphs (Wang et al., 2020)",
            "brief_description": "Analysis and framing of PLMs as open knowledge graphs that can encode relational knowledge and be used for knowledge extraction/completion tasks.",
            "citation_title": "Language models are open knowledge graphs",
            "mention_or_use": "mention",
            "paper_title": "Language models are open knowledge graphs",
            "authors": "Chenguang Wang, Xiao Liu, Dawn Song",
            "year": 2020,
            "method_name": "LM-as-Open-KG",
            "method_description": "Argues that PLMs can function as open knowledge graphs and provides methods to extract relational knowledge from LMs; cited to support the authors' use of PLMs as an interdisciplinary implicit knowledge source.",
            "input_type": "Text prompts / templates probing PLM",
            "output_type": "Extracted relations / facts from PLM",
            "prompting_technique": "Probing and template-based extraction",
            "model_name": null,
            "model_size": null,
            "datasets_used": null,
            "evaluation_metric": null,
            "reported_results": "Used to motivate PLM usage; details in Wang et al. (2020).",
            "limitations": null,
            "counterpoint": null,
            "uuid": "e7909.9",
            "source_info": {
                "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 2
        },
        {
            "paper_title": "Language models are open knowledge graphs",
            "rating": 2
        },
        {
            "paper_title": "Paperrobot: Incremental draft generation of scientific ideas",
            "rating": 2
        },
        {
            "paper_title": "How to train your agent to read and write",
            "rating": 2
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Kgbert: Bert for knowledge graph completion",
            "rating": 2
        },
        {
            "paper_title": "Multi-task pre-training language model for semantic network completion",
            "rating": 2
        },
        {
            "paper_title": "Chatgpt: Optimizing language models for dialogue",
            "rating": 1
        }
    ],
    "cost": 0.01973275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring and Verbalizing Academic Ideas by Concept Co-occurrence</h1>
<p>Yi Xu ${ }^{1}$, Shuqian Sheng ${ }^{1}$, Bo Xue ${ }^{1}$, Luoyi Fu ${ }^{1}$; Xinbing Wang ${ }^{1}$, Chenghu Zhou ${ }^{2}$<br>${ }^{1}$ Shanghai Jiao Tong University, Shanghai, China<br>${ }^{2}$ IGSNRR, Chinese Academy of Sciences, Beijing, China<br>{yixu98, susisheng, sappho_x, yiluofu, xwang8}@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Researchers usually come up with new ideas only after thoroughly comprehending vast quantities of literature. The difficulty of this procedure is exacerbated by the fact that the number of academic publications is growing exponentially. In this study, we devise a framework based on concept co-occurrence for academic idea inspiration, which has been integrated into a research assistant system. From our perspective, the fusion of two concepts that co-occur in an academic paper can be regarded as an important way of the emergence of a new idea. We construct evolving concept graphs according to the co-occurrence relationship of concepts from 20 disciplines or topics. Then we design a temporal link prediction method based on masked language model to explore potential connections between different concepts. To verbalize the newly discovered connections, we also utilize the pretrained language model to generate a description of an idea based on a new data structure called co-occurrence citation quintuple. We evaluate our proposed system using both automatic metrics and human assessment. The results demonstrate that our system has broad prospects and can assist researchers in expediting the process of discovering new ideas. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Academic publications have witnessed the evolution and advancement of human civilization. In modern society, out-of-box and interdisciplinary scientific work can get more attention from science funders, industry, and the public (Thurner et al., 2020), where a good idea is the cornerstone of academic research. However, for most researchers, it takes a lot of time to put forward new ideas. For one thing, the number of academic publications is increasing exponentially, and it is difficult for an</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>independent researcher to understand these papers thoroughly. Besides, researchers often focus on their specialized but narrow fields, which makes it a challenge to discover underlying connections beyond their familiar areas (Lahav et al., 2022; Krenn and Zeilinger, 2020). In this work, our purpose is to unveil the profound connections between different academic concepts and ignite researchers' exploration of potential academic ideas while expediting the research process. The two primary goals are idea exploration and verbalization.</p>
<p>For the first goal, we need to understand how new ideas originate. Generally speaking, the emergence of a simple idea is often formed by the interaction between two different concepts rather than from scratch. For example, the combination of convolution and graph neural network contributes to graph convolutional network (Kipf and Welling, 2017). This understanding of idea as connection and combination inspires us to model the process of idea exploration as a link prediction task based on the evolving co-occurrence graph of concepts. Such graphs are constructed according to the cooccurrence relationship of concepts in the papers published in different years. It should be highlighted that there exist numerous factors leading to new ideas in the real world. We provide a possible way as a preliminary exploration.</p>
<p>The second goal, idea verbalization, is carried out after idea exploration to generate fluent and reasonable texts describing an idea, which usually comprises new contents derived from the combination of two different concepts. We retrieve sentences pertaining to concepts from existing publications and then verbalize ideas using the technique of natural language generation. Specifically, We propose a new data structure called co-occurrence citation quintuple (Figure 1), which stores two concepts, their corresponding sentences of papers, and idea texts. The definition is given in section 3.1. The quintuple is an extension of edges</p>
<p>in the evolving concept co-occurrence graph and indicates where an idea comes from. We use such quintuples to train a sequence-to-sequence text generation model.</p>
<p>In our application scenario, there are various types of disciplines. Each of them has distinct characteristics and concepts. Existing methods of link prediction and text generation (Yao et al., 2019; Wang et al., 2019; Krenn and Zeilinger, 2020; Pareja et al., 2020; Da Li et al., 2022) are mostly trained on one dataset by optimizing a set of parameters. Owing to the fact that different datasets require specific training configurations and hyperparameters, such models cannot be transferred to other datasets. Particularly, link prediction models need to set the scale of graphs before training, such as the number of nodes. Moreover, in the field of natural language generation, some works (Wang et al., 2019; Yu et al., 2022) tend to construct domain knowledge bases as external information to generate texts. However, building large knowledge bases for each discipline takes tremendous resources, which is unrealistic. To this end, it is preferable to design general and informative models which can be applied to numerous disciplines.</p>
<p>Thanks to the abundant training corpus of pretrained language models (PLMs) such as BERT (Devlin et al., 2018), T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and GPT (Radford et al., 2018), PLM can be regarded as an implicit knowledge graph (Petroni et al., 2019; Wang et al., 2020), which has the ability of extrapolation. In this work, we integrate the whole academic information into the same representation space by leveraging the capability of PLM to break through disciplinary barriers. For idea exploration, we devise a PLM-based link prediction method, which only needs to train one set of model parameters. For idea verbalization, we use another sequence-to-sequence-based PLM endowed with academic knowledge from millions of highly-cited papers via unsupervised denoising training. Subsequently, we re-train the denoised PLM with co-occurrence citation quintuples in a supervised way. Our contributions are summarized as follows:</p>
<ul>
<li>New insights: we transform the idea generation into two sequential sub-tasks: temporal link prediction and idea verbalization. The former aims to model and predict potential concept connections, while the latter involves expressing these new connections in natural
language.</li>
<li>Publicly-released datasets: we construct 240 evolving concept co-occurrence graphs with 20 high-level disciplines and topics. Each of them includes 23 annual snapshots ranging from 2000 to 2022. For idea verbalization, we propose a new data structure known as the co-occurrence citation quintuple that reveals how ideas appear. We curate nearly 10K high-quality co-occurrence citation quintuples, which originate from 29M papers with high citations.</li>
<li>General system for all disciplines: we design a novel temporal link prediction method and train an idea verbalization model with a large number of academic papers. The two modules are integrated into a system to serve researchers from different fields. Note that the system updates the latest papers to encourage new ideas sustainably. Users are free to enter any academic query.</li>
<li>Systematic experiments: we conduct extensive experiments, including automatic metrics and human assessment, to evaluate the performance of our link prediction method and idea verbalization model. The results show that our system has a promising prospect of helping researchers discover new ideas.</li>
</ul>
<h2>2 Preliminaries</h2>
<h3>2.1 Evolving Concept Co-occurrence Graph</h3>
<p>Given a concept set $C=\left{c_{i}\right}<em j="j">{i=1}^{N}$ consisting of $N$ concepts and a paper corpus $P=\left{p</em>\right}<em p="p">{j=1}^{M}$ consisting of $M$ papers, let $C</em>$ represent the co-occurrence matrix of any two concepts, which is defined as follows:} \subset C$ denote the set of concepts paper $p \in P$ contains. When concepts $c_{u}$ and $c_{v}\left(c_{u} \neq c_{v}\right)$ occur together in the same paper $p$ at the same time, i.e., $c_{u} \in C_{p}, c_{v} \in C_{p}$, it is considered that $c_{u}$ and $c_{v}$ co-occur, that is, there is a connection between the two concepts. Let $\mathcal{A} \in \mathbb{R}^{N \times N</p>
<p>$$
\mathcal{A}\left(c_{u}, c_{v}\right)= \begin{cases}1, &amp; \exists p, c_{u} \in C_{p}, c_{v} \in C_{p} \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>A concept co-occurrence graph is a pair $\mathcal{G}=$ $(C, E)$, where $C$ is a set of concepts, and $E$ is a set of edges representing the co-occurrence relationship between concepts. The co-occurrence matrix</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A quintuple with its text attributes. The dashed line and box represent the texts of paper or concept.
$\mathcal{A}$ is the adjacent matrix of $\mathcal{G}$. Let $G=\left{\mathcal{G}<em t="T_{s">{t}\right}</em>}}^{T_{e}}$ denote a set of concept co-occurrence graphs at different times ranging from $T_{s}$ to $T_{e}, \mathcal{A<em t="t">{t}$ represent the adjacent matrix of $\mathcal{G}</em>$. We call $G$ evolving concept co-occurrence graph. Similar to citation network, $G$ is a strictly evolving network (Skarding et al., 2021) where the connection of concepts has infinite duration. This implies that the edges in $G$ never disappear. Exploring ideas aims to predict future co-occurrence relations in $G$.</p>
<h3>2.2 Co-occurrence Citation Quintuple</h3>
<p>Assuming that paper $p$ contains concept $c_{u}$ and $c_{v}, p$ cites paper $p_{i}$ and $p_{j}\left(p_{i} \neq p_{j}\right)$. Meanwhile, $p_{i}$ contains concept $c_{u}$, and $p_{j}$ contains concept $c_{v}$. Then, for papers $p_{i}, p_{j}$, and $p$, there exist cooccurrence citation relations corresponding to concepts $c_{u}$ and $c_{v}$. Formally, let $R_{p}$ denote the set of reference papers of $p$, and we define the set $Q$ of co-occurrence citation quintuples as:</p>
<p>$$
\begin{aligned}
&amp; Q=\left{\left(p_{i}, p_{j}, c_{u}, c_{v}, p\right) \mid p_{i} \in R_{p}, p_{j} \in R_{p}\right. \
&amp; \left.c_{u} \in C_{p_{i}} \cap C_{p}, c_{v} \in C_{p_{j}} \cap C_{p}, c_{u} \neq c_{v}\right}
\end{aligned}
$$</p>
<p>where $p$ is called target paper, $p_{i}$ and $p_{j}$ are called reference papers. In practice, we bind sentences that mention related concepts to the quintuples, illustrating how an idea existing in $p$ comes up. Figure 1 shows an example of such quintuple, which consists of two concepts text summarization and contrastive learning. In the training process, we use the corresponding texts of $p_{i}, p_{j}, c_{u}$, and $c_{v}$ as input, and our model is expected to generate the idea sentence in $p$, which usually appears in the paper abstract or introduction section.</p>
<h2>3 Datasets and Technical Details</h2>
<h3>3.1 Datasets</h3>
<p>Our work relies on a daily updated database containing more than 220 million academic papers from 19 disciplines published between 1800 and 2023. The database also stores nearly 800 K concept entities with descriptions. See Appendix A for the number of papers in each discipline.</p>
<p>To train our model for temporal link prediction, we first collect 240 essential and common queries from 19 disciplines and one special topic (COVID19). Then, we enter these queries into the paper database to fetch the most relevant papers between 2000 and 2021 with Elasticsearch, a modern text retrieval engine that stores and retrieves papers. Afterward, we use information extraction tools including AutoPhrase (Shang et al., 2018) to identify concepts. Only high-quality concepts that appear in our database will be preserved. Finally, we construct 240 evolving concept co-occurrence graphs, each containing 22 snapshots according to the co-occurrence relationship. The statistics of the concept co-occurrence graphs are provided in Appendix I.</p>
<p>Besides, we construct and release a dataset of co-occurrence citation quintuples, which is used to train text generation model for idea verbalization. We select nearly 9.5 M highly-cited papers ( 500 K per discipline) and their corresponding references (19.7M) to construct quintuples. The process of identifying and processing concepts is similar to constructing the concept co-occurrence graph. Heuristic rules are adopted to filter redundant and noisy sentences, further improving the quality of the quintuples used for idea generation. The statistics and more details of co-occurrence citation quintuples can be found in Appendix B, C, and J.</p>
<h3>3.2 Framework Overview</h3>
<p>The framework of our system in the production environment is illustrated in Figure 2. It starts by receiving the user's query and retrieving the most relevant papers from database to construct an evolving concept co-occurrence graph in a real-time way. Meanwhile, the system maintains two dictionaries for storing the mapping relations between papers and concepts. Then, a BERT-based temporal model predicts potential connections of concepts such as $c_{u}$ and $c_{v}$, which can be regarded as a new idea. Finally, these connected concepts, as well as their corresponding sentences of papers stored in the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our research assistant system. The system starts by receiving the user's query and ends with verbalizing an idea. The left part shows the data retrieval and graph construction module. The upper right part is the temporal link prediction module. The lower right part is the idea verbalization module.</p>
<p>above dictionary, are fed to our pretrained model T5 to verbalize an idea. Our system also allows users to select elements they are interested in to form a group of inputs (p<sub>i</sub>, p<sub>j</sub>, c<sub>u</sub>, c<sub>v</sub>) for idea verbalization. In the following parts, we will introduce two key components in detail.</p>
<h3>3.3 Temporal Link Prediction</h3>
<p>Our system dynamically constructs a unique evolving concept co-occurrence graph for each query according to the papers retrieved by the search engine. Under the circumstance, a general link prediction model with high transferability is required to predict new connections on different graphs, which means there exists only one set of model parameters. We take advantage of the masked language model (MLM) to tackle the link prediction problem on different graphs and propose a new temporal training method called PLM-LP (See Appendix D for the illustration of PLM-LP).</p>
<p>Given a concept pair c<sub>u</sub>, c<sub>v</sub> and a timestamp t, we concatenate these elements and prompt words pro(c<sub>u</sub>, c<sub>v</sub>, t) to obtain the following input sequence x<sup>t</sup><sub>uv</sub>:</p>
<p>x<sup>t</sup><sub>uv</sub> = [CLS] pro(c<sub>u</sub>, c<sub>v</sub>, t): in t, c<sub>u</sub> is [MASK] to c<sub>v</sub>. [SEP],</p>
<p>where pro is a prompt function defined in Equation 3 that generates a description of the given input, [MASK] is the mask token, [CLS] and [SEP] represent the tokens of the beginning and end of the input sequence, respectively. Our model is expected to fill the mask token with a relation token, i.e., "related" and "unrelated", which are taken as the true label to indicate whether the two concepts are connected. Considering that edges in the evolving concept co-occurrence graph do not disappear, we add prompts according to this feature. If there was an edge between c<sub>u</sub> and c<sub>v</sub> before time t, the pro(·) returns the word "Existing", otherwise it returns "Unknown":</p>
<p>$$
\text{pro}(c_u, c_v, t) = \begin{cases}
\text{"Existing"},
\mathcal{A}_{t-1}(c_u, c_v) = 1 \
\text{"Unknown"},
\text{otherwise}
\end{cases}
$$</p>
<p>In the data preprocessing, positive samples $\mathbb{D}^+ = { x^t_{uv} | \mathcal{A}_t(c_u, c_v) = 1, T_s \leq t \leq T_e }$ are directly constructed according to the edges of each year. For negative samples $\mathbb{D}^-$, since the concept co-occurrence graph is sparse, we cannot simply take any two concepts that do not have a connection each year as negative samples, which is unreasonable and will lead to a sharp increase in the number of negative samples. Actually, we only need to focus on the samples in the most difficult cases. Therefore, given a concept c<sub>u</sub> ∈ C and its k-hop neighborhood concepts, we choose concepts that have no connection with c<sub>u</sub> in the next d years to construct negative samples. The set of negative samples is shown as follows:</p>
<p>$$
\mathbb{D}^- = { x^t_{uv} | c_v \in \mathcal{N}<em t_d="t+d">k(c_u), \mathcal{A}</em>
$$}(c_u, c_v) = 0, \tag{4</p>
<p>k ≥ 2, T<sub>s</sub> ≤ t &lt; t + d ≤ T<sub>e</sub> \,</p>
<p>where $\mathcal{N}_k(c_u)$ is the set of concepts at a distance less than or equal to k from c<sub>u</sub>, i.e., the k-hop neighborhood of c<sub>u</sub>. It is worth noting that the negative samples are used to construct input text</p>
<p>sequences with timestamp $t$ rather than $t+d$, and we do not generate negative samples in the last $d$ timestamps. We fine-tune the parameters and vocabulary embeddings of BERT via predicting the masked token. Formally, we compute the crossentropy loss:</p>
<p>$$
\mathcal{L}=-\sum_{d \in \mathcal{D}^{+}\cup \mathcal{D}^{-}} 1_{[M A S K]=y_{d}} \log P\left([M A S K]=y_{d} \mid x_{u v}^{t}\right)
$$</p>
<p>where $y_{d} \in{$ "related", "unrelated" $}$ is the label of the sample. It should be mentioned that KGBERT (Yao et al., 2019) and LP-BERT (Da Li et al., 2022) are similar to PLM-LP, but the settings they adopt are not applicable to the training of temporal data. Nevertheless, the PLM in our method can be replaced by other models.</p>
<h3>3.4 Idea Verbalization</h3>
<p>In our public beta system, we employ T5 (Raffel et al., 2020), a large pretrained sequence-tosequence model for idea verbalization. We select 2 M highly-cited papers for unsupervised denoising training with the language model loss:</p>
<p>$$
\mathcal{L}<em p="p">{l m}=\mathbb{E}</em> ; \theta)]
$$}[-\log P(p \mid \tilde{p</p>
<p>where $\tilde{p}$ represent the corrupted sentence of paper $p$. In the process of fine-tuning, given a co-occurrence citation quintuple $q=\left(p_{i}, p_{j}, c_{u}, c_{v}, p\right)$, we first concatenate $p_{i}, p_{j}, c_{u}$, and $c_{v}$ to a sequence $\operatorname{Seq}(q)$, using $\langle$ HEAD $\rangle,\langle$ TAIL $\rangle,\langle$ SEP $\rangle$ to denote the head, tail of a concept pair, and the separator, respectively, which is shown as follows:</p>
<p>$$
\operatorname{Seq}(q)=\langle\text { HEAD }\rangle c_{u}\langle\text { TAIL }\rangle c_{v}\langle\text { SEP }\rangle p_{i}\langle\text { SEP }\rangle p_{j}
$$</p>
<p>We fine-tune the T5 model to find the optimal parameters $\theta^{*}$ to encode the input sequence and verbalize it into an idea sequence, i.e., the item $p$ in the quintuple. For this purpose, we use the maximum likelihood estimation objective:</p>
<p>$$
\theta^{*}=\underset{\theta}{\arg \max } \prod_{q} P(p \mid \operatorname{Seq}(q) ; \theta)
$$</p>
<p>During the inference process (production environment), we use the predicted connection of concepts $c_{u}, c_{v}$, and their corresponding sentences of papers $p_{i}, p_{j}$ to construct the input sequence, which is encoded by our fine-tuned T5 to generate an idea sequence. Note that the idea verbalization model is also flexible in our framework, and it can be substituted by alternatives such as GPT(Radford et al.,
2018) with another configuration of fine-tuning. We will also provide premium subscribers with GPT-3.5 after the official release of our system.</p>
<h2>4 Evaluation</h2>
<h3>4.1 Analysis of Temporal Link Prediction</h3>
<h3>4.1.1 Results of Link Prediction in 2021</h3>
<p>PLM-LP is compared with 3 temporal model SEMNET (Krenn and Zeilinger, 2020), GCN-GAN (Lei et al., 2019), and EvolveGCN (Pareja et al., 2020), which are suitable for concept co-occurrence graph. SEMNET analyzes graph characteristics to recognize potential new edges with an MLP module. GCN-GAN and EvolveGCN utilize GCN and LSTM to model the structural and temporal information of a graph. In the experiment, their performance is evaluated on our constructed 240 concept co-occurrence graphs, where the last snapshot (the year 2021) is used as the test set. We report the accuracy of the adjacent matrix, precision, recall, and F1 score of all edges and new edges existing in the graph of 2021. New edges do not exist in the past snapshots and only come out in 2021.</p>
<p>Note that PLM-LP is trained with a single set of model parameters on these 240 graphs and then applied to different graphs for the test procedure. The hyper-parameters $k$ and $d$ in PLM-LP are set to 2 and 5, respectively. Apart from our proposed PLM-LP, we also introduce two variants. PLM-LP w/o pro. removes the prompt words $\operatorname{pro}\left(c_{u}, c_{v}, t\right)$. PLM-LP ind. is trained with independent parameters on different graphs. Results of these models in 20 disciplines/topics are provided in Appendix H. The average results are shown in Table 1. It can be observed that all these models are capable of identifying most edges existing in 2021, but the GCN-GAN and EvolveGCN gets undesirable performance to find new edges in 2021. Many cases have been predicted to be unconnected. We believe this is because most graphs are sparse, leading to overfitting. In our scenario, detecting new edges is more important than improving the accuracy of the adjacency matrix. Our proposed method can tackle the issue to a certain extent. As to the variants, it is difficult for PLM-LP w/o pro. to correctly predict all edges in 2021 due to the absence of prompt words. PLM-LP ind. is also inferior to PLM-LP, indicating that PLM can learn interdisciplinary knowledge with a set of training parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">All Edges in 2021</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">New Edges in 2021</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Precision</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">SEMNET</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.519</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.013</td>
</tr>
<tr>
<td style="text-align: center;">GCN-GAN</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.924</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">EvolveGCN</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 5}$</td>
<td style="text-align: center;">$\mathbf{1 . 0 0 0}$</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 5}$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">N/A</td>
</tr>
<tr>
<td style="text-align: center;">PLM-LP w/o pro.</td>
<td style="text-align: center;">0.648</td>
<td style="text-align: center;">0.586</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.474</td>
</tr>
<tr>
<td style="text-align: center;">PLM-LP ind.</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.748</td>
<td style="text-align: center;">0.188</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.195</td>
</tr>
<tr>
<td style="text-align: center;">PLM-LP</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">$\mathbf{0 . 9 9 8}$</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">$\mathbf{0 . 5 4 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Average results of link prediction on different disciplines. The best results are boldfaced. N/A means all cases have been predicted to be negative.</p>
<h3>4.1.2 Human Assessment of Link Prediction in the Future</h3>
<p>We use all graph snapshots, including the year 2021, for training to mine potential connections that may appear in the future. Similarly, we select the top 20 pairs of concepts for each query. See Appendix G for the potential connections of different disciplines. We invited more than 10 experts from the field of computer science and geo-science (geology and geography) to evaluate the predicted results in their corresponding domains. The assessment is based on the experience of experts. The results are shown in Table 2. As expected, at least a third of the potential concept pairs predicted by the system are reasonable in the three disciplines, indicating that PLM-LP is able to explore new concepts across disciplines. We also test random pairs on geo-science, and there are no more than $10 \%$ of reasonable pairs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Disciplines</th>
<th style="text-align: center;">Percentage (\%) of <br> Reasonable Pairs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Computer Science</td>
<td style="text-align: center;">52.1</td>
</tr>
<tr>
<td style="text-align: left;">Geology</td>
<td style="text-align: center;">48.8</td>
</tr>
<tr>
<td style="text-align: left;">Geography</td>
<td style="text-align: center;">34.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage (\%) of reasonable concept pairs based on human assessment.</p>
<h3>4.2 Analysis of Idea Verbalization</h3>
<h3>4.2.1 Benchmark Results</h3>
<p>We release the co-occurrence citation quintuples for idea verbalization, which can be used as a benchmark for natural language generation. Our public beta system adopts PLM such as T5 and BART as the generation models that are fine-tuned on the quintuples. We also apply unsupervised denoising training on T5 with highly-cited papers, which makes the PLM itself learn more academic knowledge. All training and inference processes are carried out on NVIDIA GeForce RTX 3090. In the fine-tuning stage, we employ Adam as the opti-
mizer with 0.01 weight decay. The learning rate is set to $1 \mathrm{e}-4$. For the inference, the beam size is set to 4 . Similar to previous text generation work (Fan et al., 2018; Wang et al., 2019), we use BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE_L (Lin, 2004) to measure the fluency and topic relevance of the generated ideas. Table 3 gives the benchmark results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">METEOR</th>
<th style="text-align: center;">ROUGE_L</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">T5-base</td>
<td style="text-align: center;">25.16</td>
<td style="text-align: center;">12.57</td>
<td style="text-align: center;">16.66</td>
</tr>
<tr>
<td style="text-align: left;">T5-large</td>
<td style="text-align: center;">25.68</td>
<td style="text-align: center;">12.72</td>
<td style="text-align: center;">16.83</td>
</tr>
<tr>
<td style="text-align: left;">T5-base denoise</td>
<td style="text-align: center;">25.72</td>
<td style="text-align: center;">12.54</td>
<td style="text-align: center;">16.74</td>
</tr>
<tr>
<td style="text-align: left;">T5-large denoise</td>
<td style="text-align: center;">26.94</td>
<td style="text-align: center;">13.19</td>
<td style="text-align: center;">17.35</td>
</tr>
<tr>
<td style="text-align: left;">BART-large</td>
<td style="text-align: center;">21.87</td>
<td style="text-align: center;">7.93</td>
<td style="text-align: center;">14.72</td>
</tr>
</tbody>
</table>
<p>Table 3: Benchmark results with different PLMs.
In fact, it is challenging to evaluate long text (Liu et al., 2016; Li et al., 2016), let alone idea verbalization, which may contain new opinions, insights, and methods. Additionally, the new content in the verbalized idea is likely to differ from the target paper in quintuples. Thus, we conduct the following experiments.</p>
<h3>4.2.2 Turing Test</h3>
<p>Similar to previous work (Wang et al., 2019), we recruited more domain experts and non-experts in the field of computer science, geo-science (geology and geography), and medicine to conduct the Turing test. Experts include professors, lecturers, postdoctoral researchers, and graduate students (at least two professors per discipline). Participants are asked to read the machine-generated outputs and human-written texts and choose the real humanwritten text from a set of $N-1$ fake ones. Each participant is given instructions before the test. We also allow participants to use the Internet to retrieve technical terms during the test. For each discipline, there are two different modes of multiple-choice questions, one contains two options per question, and the other contains three options per question. We randomly select 15 questions per test from the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Disciplines</th>
<th style="text-align: center;">Test ID</th>
<th style="text-align: center;"># Cases</th>
<th style="text-align: center;"># Options <br> per Case</th>
<th style="text-align: center;"># Participant</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Computer Science</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Geography \&amp; Geology</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Medicine \&amp; COVID-19</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 4: Settings of Turing test.
question bank for each participant to answer. We conduct six groups of Turing tests, whose experimental settings are shown in Table 4.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Box plot of Turing test. The green triangle represents mean value, and the green line represents median value. The label of the x -axis is composed of the test ID and participant role.</p>
<p>The results are displayed using a box plot in Figure 3. Overall, domain experts are more likely to achieve higher accuracy in these six groups of tests. Also, the results reveal that the accuracy of the 3 -options question is lower than $30 \%$, indicating that it is more difficult for participants to choose the human-written one from 3 options than from 2 options. Moreover, the accuracy of the 2 -option questions is close to or even lower than that of random guessing, which means experts can hardly distinguish between human-written sentences and machine-generated sentences, although they tend to analyze texts from the perspective of logic and accuracy. One of the possible reasons is that the verbalized ideas contain more nonprofessional terms while maintaining fluency and reasonableness, which is more readable than academic papers.</p>
<h3>4.2.3 Relevance \&amp; Plagiarism Analysis</h3>
<p>We calculate the percentage of n-grams in the input sequence which appear in the verbalized idea of test data to analyze how relevant the idea is to the input sequence. Meanwhile, the percentage of n-grams can also be regarded as a plagiarism check. As seen from Table 5, about $40 \%$ of the input 1-grams exist in the output texts, which means the output can combine the knowledge of relevant concepts. Additionally, the percentages of 2 to 5 -grams are all lower than $20 \%$, that is, the verbalized ideas are not simply copied from the input but are paraphrased and fused into new knowledge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">1-gram</th>
<th style="text-align: center;">2-gram</th>
<th style="text-align: center;">3-gram</th>
<th style="text-align: center;">4-gram</th>
<th style="text-align: center;">5-gram</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">9.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Percentage (\%) of n-grams in the test input sequence that appear in the verbalized idea.</p>
<p>We did not provide the n-gram overlap of the target paper $p$. From our perspective, $p$ is more like an inspiration, which teaches our model a possible way of how two concepts can be combined together, rather than generating sentences exactly like $p$.</p>
<h3>4.3 Case Study</h3>
<p>In Appendix E, we provide a page of examples of input sequences, human-written texts, and verbalized ideas according to our test dataset of quintuples. To simulate the real situation, we randomly select cases including new connections PLM-LP predicts, which do not appear in our quintuple dataset. It is worth noting that we only take these two concepts as input and do not enter their corresponding sentences to avoid the impact of potential plagiarism.</p>
<p>Table 6 shows three verbalized ideas. For the first case, we can see our system integrates the critical characteristic of contrastive learning that requires no labels into the task of knowledge graph mining. However, it includes untested experimental results due to the denoising training from numerous papers (especially from paper abstracts and</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Discipline</th>
<th style="text-align: center;">Computer Science</th>
<th style="text-align: center;">Economics</th>
<th style="text-align: center;">Political Science</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Connection</td>
<td style="text-align: center;">knowledge graph \&amp; <br> contrastive learning</td>
<td style="text-align: center;">intellectual capital \&amp; <br> income distribution</td>
<td style="text-align: center;">gender equity \&amp; <br> economic crisis</td>
</tr>
<tr>
<td style="text-align: center;">Verbalized <br> Idea</td>
<td style="text-align: center;">We present a new approach to <br> knowledge graph mining that <br> leverages ontologies. The key <br> idea is to model the domain <br> knowledge as a tree like structure <br> with nodes and edges connected <br> in a directed or unordered graph. <br> This allows us to efficiently learn <br> from large amounts of unlabeled <br> data without having to manually <br> annotate it. Experiments show <br> that this approach outperforms <br> existing approaches such as tree <br> augmented neural networks and <br> SVM for both text classification <br> and image categorization tasks <br> where they only use small subsets <br> of training examples.</td>
<td style="text-align: center;">This paper examines the determin- <br> ants of inequality in income and <br> wealth distributions, with particu- <br> lar emphasis on the role played <br> by intellectual property rights. <br> It is shown that there are two main <br> sources of inequality: unequal <br> income distributions due to <br> ownership of patents and trade <br> secrets, and unequal taxation of <br> these same properties. The first <br> source arises from the fact that most <br> patents are owned by individuals <br> who do not have control over their <br> own economic activities. In contrast, <br> the second source derives from <br> the existence of private property <br> rights which make it possible for an <br> individual to be rich without having to <br> pay taxes on his or her own income.</td>
<td style="text-align: center;">During the global financial and <br> economic crisis, women's employment <br> opportunities declined sharply. These <br> trends are likely to continue during <br> the next few years as more women <br> enter the labor force. From our <br> perspective, there will be an increasing <br> number of women entering the work- <br> force at lower levels of education than <br> men. This trend is expected to continue <br> in the coming years as female participa <br> ation in the labour force continues to <br> increase. The current political and <br> economic climate may make it difficult <br> for women to access higher level <br> education because of the challenges <br> presented by the gender pay gap and <br> the macroeconomic crisis that has <br> gripped much of the developing world <br> since 2007.</td>
</tr>
</tbody>
</table>
<p>Table 6: Case study in computer science, economics, and political science.
introduction section), and we remove them with heuristic rules in the production environment. As to the second case, the verbalized idea mentions that intellectual capital, such as intellectual property rights, is closely related to income distribution. In the last case, our system believes that a gender pay gap exists in developing countries, which is more obvious during the economic crisis. These cases show that our system can well predict and verbalize ideas, and the generated results align with human intuition and value. Nevertheless, more details are required in natural and exact sciences.</p>
<h2>5 Related Work</h2>
<h3>5.1 Graph Technology for Academic Discovery</h3>
<p>There are a few graph technical methods to help researchers find new ideas. SEMNET (Krenn and Zeilinger, 2020) predicts research trends with an MLP in the field of quantum physics via constructing such co-occurrence graphs. Sarica et al. proposes a technology graph to stimulate idea generation in engineering design, which aims to discover new concepts in the white space surrounding a focal design domain according to the semantic distance. Besides, InfraNodus (Paranyushkin, 2019), a commercial tool for people in different industries, generates insights by detecting structural gaps in a text network, which is similar to mind maps.</p>
<h3>5.2 Text Generation</h3>
<p>Pretrained language models, including T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and GPT (Radford et al., 2018) have become the mainstream modules of text generation since they contain billions of parameters and use a large number of corpus for training to achieve good performance. As to text generation for academic research, existing models can only be applied to a few disciplines with much fewer papers than ours. They also require a lot of resources to construct knowledge bases. For instance, PaperRobot (Wang et al., 2019) adopts external domain knowledge graphs to incrementally generate titles, abstracts, and conclusions of a paper. DRAW (Liu et al., 2021a) consists of reader, writer, and reviewer components to generate scientific texts. ChatGPT (OpenAI, 2022) generates human-level texts with proximal policy optimization, but it requires professional prompts to discover new ideas. Galactica (Taylor et al., 2022) is a large language model for science, which can be combined with our link prediction model to enhance its explainability for idea verbalization.</p>
<h2>6 Conclusion</h2>
<p>We model the emergence of a new idea as two sequential processes: temporal link prediction for exploration and text generation for verbalization. To achieve the objectives, we first construct and release two datasets with new data structures, including evolving concept co-occurrence graph and</p>
<p>co-occurrence citation quintuple. Then, we devise a new temporal link prediction method based on the masked language model, which can be applied to various evolving concept co-occurrence graphs of different disciplines. Finally, we finetune a PLM to verbalize ideas using the released quintuples. The pipeline has been integrated into a system free for researchers to obtain inspiration. From the experiments and the feedback of users, our system can provide useful information for idea discovery. In the future, we will release an academic oriented language model with the paradigm of prompt learning and instruction tuning to tackle both link prediction and text generation.</p>
<h2>Limitations</h2>
<p>Based on internal review and user feedback, we summarized the following limitations to improve and iteratively update our system and framework in the future.
Problem Modeling: New concepts appear yearly in the real world, but the current system cannot generate new concepts. Generally, the emergence of new concepts often comes from the fusion of mature technologies. Thus, we model the idea exploration as link prediction. Note that it is not the only pathway to brew new ideas, but we have verified the effectiveness and rationality of this approach in the experiments. In addition, PLM can be taken as an implicit knowledge graph (Petroni et al., 2019; Wang et al., 2020), which is capable of tackling uncovered concepts in the evolving concept graphs. We will continue exploring the potential of PLM in knowledge discovery and innovation.
Logic, Correctness, and Concreteness: Although the verbalized ideas can deceive many experts, they may still lack logic, correctness, and details, especially in natural and exact sciences. It is also a challenge for natural language generation. We plan to use more academic corpus and introduce constraint (Zhang et al., 2020) to alleviate such problems.
Temporal Information: In PLM-LP, we simply take the year information as a token in the input sequence. We conduct additional experiments to show that the temporal information is not sensitive to PLM-LP, which can be attributed to the negative sampling and the nature of the strictly evolving network.
Two Birds One Stone: The current system employs two different PLMs for link prediction and
idea verbalization, respectively. The development of prompt learning (Liu et al., 2021b) reveals that most NLP problems can be regarded as generation problems. In the future, we will introduce new training settings using a single PLM to address link prediction and idea verbalization simultaneously.</p>
<h2>Ethics Statement</h2>
<p>The datasets used in our research are collected through open-source approaches. The whole process is conducted legally, following ethical requirements. As for the Turing Test in our study, all participants are well informed about the purpose of experiments and the usage of test data, and we would not leak out or invade their privacy.</p>
<p>We see opportunities for researchers to apply the system to idea discovery, especially for interdisciplinary jobs. We encourage users to explore different combinations of subjects with the help of our system, making the most of its knowledge storage and thus maximizing the exploration ability of the system.</p>
<p>The main focus of the system is to provide a possible direction for future research, but the effect of human researchers will never be neglected.</p>
<p>The massive data from various disciplines behind the system makes it capable of viewing the knowledge of an area in a multi-dimensional perspective and thus helps promote the development of novel interdisciplinary. However, considering the risks of misinformation generated by NLP tools, the verbalization only contains possible insights into new ideas. Researchers must thoroughly consider whether an idea is feasible or leads to adverse societal effects.</p>
<h2>Acknowledgements</h2>
<p>We would like to express our deepest gratitude to the scientists involved in the Deep-time Digital Earth program, whose contributions have been incredibly valuable. Additionally, we extend our thanks to Zhongmou He, Jia Guo, Zijun Di, Shengling Zhu, Yanpeng Li, Qi Li, Jiaxin Ding and Tao Shi from the IIOT Research Center at Shanghai Jiao Tong University for their unwavering support during the development of our system. This work was supported by NSF China (No.42050105, 62020106005, 62061146002, 61960206002), Shanghai Pilot Program for Basic Research - Shanghai Jiao Tong University.</p>
<h2>References</h2>
<p>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In ACL Workshop.</p>
<p>Sen Yang Da Li, Kele Xu, Ming Yi, Yukai He, and Huaimin Wang. 2022. Multi-task pre-training language model for semantic network completion. arXiv preprint arXiv:2201.04843.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889-898.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations.</p>
<p>Mario Krenn and Anton Zeilinger. 2020. Predicting research trends with semantic and neural networks with an application in quantum physics. Proceedings of the National Academy of Sciences, 117:1910 1916.</p>
<p>Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Duen Horng Chau, Diyi Yang, Eric Horvitz, Daniel S Weld, et al. 2022. A search engine for discovery of scientific challenges and directions. Proceedings of the AAAI Conference on Artificial Intelligence.</p>
<p>Kai Lei, Meng Qin, Bo Bai, Gong Zhang, and Min Yang. 2019. Gcn-gan: A non-linear temporal link prediction model for weighted dynamic networks. In IEEE INFOCOM 2019-IEEE Conference on Computer Communications, pages 388-396. IEEE.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and William B Dolan. 2016. A persona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 994-1003.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out.</p>
<p>Chia-Wei Liu, Ryan Lowe, Iulian Vlad Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122-2132.</p>
<p>Li Liu, Mengge He, Guanghui Xu, Mingkui Tan, and Qi Wu. 2021a. How to train your agent to read and write. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13397-13405.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Dmitry Paranyushkin. 2019. Infranodus: Generating insight using text network analysis. In The world wide web conference, pages 3584-3589.</p>
<p>Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schardl, and Charles Leiserson. 2020. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 5363-5370.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Serhad Sarica, Binyang Song, Jianxi Luo, and Kristin L Wood. 2021. Idea generation with technology semantic network. AI EDAM, 35(3):265-283.</p>
<p>Jingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren, Clare R Voss, and Jiawei Han. 2018. Automated phrase mining from massive text corpora. IEEE</p>
<p>Transactions on Knowledge and Data Engineering, 30(10):1825-1837.</p>
<p>Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. 2021. Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey. IEEE Access, 9:79143-79168.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Stefan Thurner, Wenyuan Liu, Peter Klimek, and Siew Ann Cheong. 2020. The role of mainstreamness and interdisciplinarity for the relevance of scientific papers. PloS one, 15(4):e0230325.</p>
<p>Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967.</p>
<p>Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi Luan. 2019. Paperrobot: Incremental draft generation of scientific ideas. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1980-1991.</p>
<p>Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kgbert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193.</p>
<p>Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. 2022. A survey of knowledge-enhanced text generation. ACM Computing Surveys (CSUR).</p>
<p>Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and William B Dolan. 2020. Pointer: Constrained progressive text generation via insertionbased generative pre-training. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8649-8670.</p>
<h1>A Distribution of Papers</h1>
<p>We are an academic service provider with a sufficient number of high-quality literature data sources (including publications and preprints). These sources are reliable and maintained by a team of professional engineers, ensuring the accuracy and persuasiveness of idea-discovery results. Our database contains more than 220 million academic papers from 19 disciplines between 1800 and 2023 and nearly 800 K concept entities with corresponding descriptions. Figure 4 shows the number of papers in each discipline. Note that there are a large number of interdisciplinary papers. Our system will retrieve relevant papers from this database according to the queries and guide users to discover new ideas.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Number of papers in different disciplines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Item</th>
<th style="text-align: right;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Target Paper</td>
<td style="text-align: right;">$9,500,000$</td>
</tr>
<tr>
<td style="text-align: left;">Reference Paper</td>
<td style="text-align: right;">$19,790,411$</td>
</tr>
<tr>
<td style="text-align: left;">Citation Threshold</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">Concept</td>
<td style="text-align: right;">18,347</td>
</tr>
<tr>
<td style="text-align: left;">Quintuple</td>
<td style="text-align: right;">652,809</td>
</tr>
<tr>
<td style="text-align: left;">High-quality Quintuple</td>
<td style="text-align: right;">92,313</td>
</tr>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: right;">73,852</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: right;">9,230</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: right;">9,231</td>
</tr>
</tbody>
</table>
<p>Table 7: Statistics of co-occurrence citation quintuples.</p>
<h2>B Statistics of Quintuples</h2>
<p>Table 7 shows the statistics of co-occurrence citation quintuples, which originate from 9.5 M target papers and 19.8 M reference papers. Their citations
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Number of concepts and quintuples in different disciplines.
are greater than or equal to 2 . In the data preprocessing, when a paper contains multiple sentences corresponding to a concept, we randomly picked up one sentence to construct a quintuple. We finally obtain 92,313 high-quality instances ( 73,852 for training, 9,230 for validation, and 9231 for testing) after applying a filter mechanism (Appendix C). The distribution of the quintuples and their corresponding concepts are shown in Figure 5. We can see that the numbers of quintuples and concepts of natural science are far more than those of social science, which can be attributed to the paper distribution and citation. In the future, we will lower the citation threshold to get more quintuples of social science.</p>
<h2>C Pipeline of Quintuple Construction</h2>
<p>Figure 6 illustrates the pipeline of constructing quintuples. We select nearly 9.5 M highly cited papers ( 500 K per discipline) and their corresponding references (19.7M) to construct quintuples. We employ AutoPhrase (Shang et al., 2018), an information extraction tool to identify concepts. We execute the process of entity linking and alignment to disambiguate duplicate entities and remove lowquality concepts. Then, we retrieve corresponding sentences of papers that mention these concepts. Relevant sentences will be preserved. Additionally, we apply a rule-based filter to our retrieved contents, where sentences including experimental details, acknowledgments, and sentences with a large number of numerical conclusions, etc., are removed. Finally, we obtain 92,313 quintuples.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Pipeline of constructing quintuples.</p>
<h2>D Framework of PLM-LP</h2>
<p>The framework of the temporal link prediction model PLM-LP is illustrated in Figure 7. We first generate positive and negative samples according to the structure of evolving concept co-occurrence graphs. Note that we add prompt ("Existing" and "Unknown") as the prefix of a sentence. The PLM aims to fill the mask token with a relation token, i.e., "related" and "unrelated". We use a masked language model BERT as the base PLM. We finetune the parameters and vocabulary embeddings of BERT via minimizing cross-entropy loss. Note that we simply take the year information as a token in the input sequence. We conduct experiments to show that the temporal information is not sensitive to PLM-LP. In the future, we will design a novel temporal prompt to capture more temporal information.</p>
<h2>E Examples of Turing Test</h2>
<p>Table 8 shows the examples (2-option questions) used in the Turing Test. All texts presented in the questions originate from the same quintuple, where the human-written text is extracted from the target paper, and the machine-generated text is the idea verbalized by our T5 model according to the concept pair and their corresponding texts. With randomness, repeating the verbalizing process can generate different outputs, which is helpful in preparing questions that need multiple machinegenerated texts. From these examples, we can see that machine-verbalized ideas can easily deceive domain experts.</p>
<h2>F Screenshot of User Interface</h2>
<p>Our system (DeepReport) is available at website https://idea.acemap.cn. Figure 8 and 9 are screenshots of user interface (public beta version). As demonstrated in Figure 8, after the concept "Carbonate Rock" is entered in the searching box, texts relevant to the keyword are presented in the insights box. The system will then dynamically construct an evolving concept co-occurrence graph based on the query result, where each node represents a concept, and relations between concepts are represented by the co-occurrence edges. We provide animations to demonstrate the evolution of the concept graph. The result of temporal link prediction is shown as concept pairs in the lower left New Relations box, and verbalized idea for each pair is shown in a new dialog box. Researchers can select different concept pairs they are interested in and view the corresponding ideas, as illustrated in figure 9. The system also provides network analytic tools such as community detection algorithms and Sankey diagrams for deeper investigation. The response time of the whole system is within 20 seconds.</p>
<h2>G Potential Connections PLM-LP Predicted</h2>
<p>We apply PLM-LP to the constructed 240 evolving concept co-occurrence graphs. We use all graph snapshots, including the year 2021, for training to mine potential connections that may appear in the future. We select the top $K$ pairs of concepts that are most likely to be connected by calculating the difference between the logits of labels, i.e.,</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Framework of our proposed temporal link prediction model PLM-LP.
"related" and "unrelated". Table 9 presents potential connections PLM-LP predicted in 20 disciplines and topics. The connections are shown as concept pairs with \&amp; concatenated. For each discipline, we only display six pairs as examples. In our human assessment, we recruited experts in the field of computer science and geo-science (geology and geography) to evaluate the predicted results in their corresponding domains. Their feedback reveals that at least a third of the potential concept pairs generated by the system are reasonable.</p>
<h2>H Comparison Results of Link Predictions on All Disciplines</h2>
<p>PLM-LP is compared with three up-to-date temporal models: SEMNET (Krenn and Zeilinger, 2020), GCN-GAN (Lei et al., 2019), and EvolveGCN (Pareja et al., 2020), which are applicable to the concept co-occurrence graph. In the experiment, their performance is evaluated on our constructed 240 concept co-occurrence graphs, where the last snapshot (the year 2021) is used as the test set. We report the accuracy of the adjacent matrix, precision, recall, and F1 score of all edges and new edges existing in the graph of 2021. New edges do not exist in the past years and would only come out in 2021. Results of these models in 20 disciplines/topics are provided in Table 10. It should be mentioned that we show the average of 12 evolving concept co-occurrence graphs of each discipline. The results show that GCN-GAN and EvolveGCN are unable to discover new edges. Our proposed PLM-LP is superior to any other models in the task of idea exploration, where the given graphs are strictly evolving network (Skarding et al., 2021).</p>
<h2>I Statistics of Evolving Concept Co-occurrence Graph</h2>
<p>We construct 240 evolving concept co-occurrence graphs (12 graphs per discipline/topics) with Elasticsearch and Autophrase (Shang et al., 2018) according to 240 essential and common queries and relevant papers. Each graph contains 22 temporal snapshots between 2000 and 2021. The statistics of the concept co-occurrence graphs are shown in Tables 11, 12, 13, 14, and 15. These tables provide the corresponding discipline, query, number of nodes (concepts), number of edges in 2021, and selected concepts. We will release the construction code and data set on GitHub for further research, including temporal link prediction, community detection, academic trends analysis, knowledge representation, etc.</p>
<h2>J About the Official Version of DeepReport</h2>
<p>In mid-2023, our DeepReport system underwent a major update, encompassing both data and model improvements. On the data front, we introduced a new version of the quintuple data (V202306), resulting in enhanced quality and a larger-scale dataset. The statistical summary of the new quintuple data (V202306) is presented in Table 16.</p>
<p>Furthermore, we trained a new state-of-the-art model in a specialized domain, which remains internal to our organization. This model, along with the integration of openAI's interface, was implemented to elevate the quality of our online services. The amalgamation of our proprietary large-scale model and the incorporation of openAI's resources empowered our system to provide superior performance and better cater to the needs of our users.</p>
<p>The introduction of the improved quintuple dataset, coupled with the deployment of the new specialized domain model and the utilization of openAI's interface, signifies a significant advancement in our DeepReport system. These updates enable us to deliver more accurate and reliable results, thereby enhancing the overall user experience. We remain committed to further refining our system to ensure it continues to meet the evolving demands of our users.</p>
<h2>K Frequently Asked Questions</h2>
<ul>
<li>Q: Comparing to other concepts graphs, what is the advantage of the concept co-occurrence quintuples? A: This question goes to the core of our work. This allows us to capture not only the co-occurrence relationship between concepts, but also their citation relationships, which can provide additional insights into how ideas are related (or generated) in academic literature.</li>
<li>Q: Why do you think transferring the concept links into natural languages is a necessary step in this assisting process? Your target users are academics. If they couldn't generate a proper idea from the link of concepts into natural language, do you expect the machines could do it better? A: The ultimate goal of our existing and future work is to enable LLM to generate reasonable, interpretable, and traceable ideas, and we now focus on how to use structured knowledge (here we use concept co-occurrence graphs) to guide this process. Therefore, the verbalization process is necessary. Besides, our system is designed to inspire researchers to discover ideas, rather than to replace them. Since we leverage the knowledge of 19 disciplines to train the model, it is valuable for researchers in certain cross-fields to broaden their points of interest.</li>
<li>Q: The evaluation of verbalized ideas cannot measure logic, correctness, and details. Since you have already found the experts, could you think of some possibilities to evaluation the academic quality of these generated ideas trough human assessment? A: From our perspective, the human assessment of verbalized ideas is similar to the process of paper review but not identical, and the latter is more subjective. We should follow three principles,
low resources, high efficiency, and objectivity to evaluate the generated ideas. For instance, predefined criteria and multiple rounds of voting can be of help. Meanwhile, we are trying to train human-in-the-loop-based system to assist experts in idea assessment.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Concept</th>
<th style="text-align: center;">plant disease \&amp; machine learning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">One potential application is the development of mobile disease diagnostics through machine learning <br> and crowdsourcing. Here we announce the release of over 50,000 expertly curated images on healthy and <br> infected leaves of crops plants through the existing online platform PlantVillage. We describe both the <br> data and the platform. These data are the beginning of an on-going, crowdsourcing effort to enable <br> computer vision approaches to help solve the problem of yield losses in crop plants due to infectious diseases.</td>
</tr>
<tr>
<td style="text-align: center;">Machine</td>
<td style="text-align: center;">We present a machine learning approach for disease detection in field images. It employs a deep CNN, <br> which is trained using a supervised learning approach. The proposed approach is able to detect the <br> presence of a number of different plant diseases in field images, including Fusarium wilt, Phytophthora <br> infestans, and Xanthomonas syringae. The results show that the model is robust to changes in lighting, <br> orientation, and crop species, and that it can be used in a real world application.</td>
</tr>
<tr>
<td style="text-align: center;">Concept</td>
<td style="text-align: center;">network intrusion detection \&amp; neural network</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">An intelligent agent based distributed network intrusion detection system is presented. Compared with <br> the current network intrusion detection techniques, IA NIDS uses the parallel technique to reform and <br> detect the coming packet on the application layer, which widens the scale of intrusion detection. It uses <br> intelligent distributed mutual agent technique to enhance the ability of real time response and uses mix <br> detection method to improve the accuracy for detecting DDOS attacks.</td>
</tr>
<tr>
<td style="text-align: center;">Machine</td>
<td style="text-align: center;">We present a novel approach to network intrusion detection based on the use of a neural network model. <br> The neural network model is trained using a variety of anomaly detection techniques and anomaly <br> classification techniques. The model is able to detect a wide range of network intrusions, including denial <br> of service attacks, DDOS attacks, spoofing attacks, and a wide variety of worm attacks. The results <br> demonstrate the effectiveness of the approach by comparing it to a number of state of the art neural <br> network models, and by comparing the results with those of other neural network model based approaches.</td>
</tr>
<tr>
<td style="text-align: center;">Concept</td>
<td style="text-align: center;">metal catalyst \&amp; metal nanoparticles</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">Carbon materials deliver a good opportunity to design catalysts for hydrodeoxygenation by providing <br> chemically inert surface and tunable functional groups to support the metal nanoparticles. The main <br> objective of the research is to give an account on the use of carbon material supported heterogeneous <br> catalysts for the HDO of lignin and related molecules for the synthesis of non petroleum chemical <br> products including hydrocarbons. The surface properties of the catalysts and mechanistic insights into <br> the conversion with key active sites are provided, which will help the designing of new and effective <br> catalysts for this biomass conversion.</td>
</tr>
<tr>
<td style="text-align: center;">Machine</td>
<td style="text-align: center;">The catalysts were characterized by X ray diffraction, scanning electron microscopy, Fourier transform <br> infrared spectroscopy, thermogravimetric analysis, and nitrogen adsorption-desorption isotherms. <br> The results showed that the catalysts exhibited high activity in the hydrodeoxygenation of lignin derived <br> bio oil under mild conditions. Moreover, the catalysts were also applied to the upgradation of bio oil derived <br> from the catalysis isoproanolysis in the organic phase. The high activity of the catalysts was attributed <br> to the synergistic effect of the metal nanoparticle.</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of input concepts, human-written texts, and verbalized ideas according to our test dataset of quintuples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Discipline</th>
<th style="text-align: center;">New Connections</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Art</td>
<td style="text-align: center;">rogue taxidermy \&amp; visual arts avant garde \&amp; early paleozoic post modernism \&amp; human environments</td>
<td style="text-align: center;">claude cahun \&amp; science fiction zhuang zi \&amp; wang guowei west coast \&amp; hip hop</td>
</tr>
<tr>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">spinal cord \&amp; pancreatic cancer arabidopsis thaliana \&amp; heavy chain rna interference\&amp; body mass index RNA</td>
<td style="text-align: center;">grizzly bear \&amp; gene flow splicing variants \&amp; echinococcus granulosus splicing variants \&amp; echinococcus granulosus</td>
</tr>
<tr>
<td style="text-align: center;">Business</td>
<td style="text-align: center;">structural unemployment \&amp; stock market industrial relations \&amp; firm size economic growth \&amp; greenhouse gas emissions</td>
<td style="text-align: center;">copyright law \&amp; knowledge transfer sale constraints \&amp; macroeconomic variables subprime mortgage crisis \&amp; IMF</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">mass spectrometry \&amp; aryl halides capillary electrophoresis \&amp; optical rotation spinodal decomposition \&amp; statistical mechanics</td>
<td style="text-align: center;">phase transition \&amp; density functional theory symmetry breaking \&amp; hydrogen bond canonical ensemble \&amp; condensed matter</td>
</tr>
<tr>
<td style="text-align: center;">Computer</td>
<td style="text-align: center;">implicit bias \&amp; biological inspiration ambient intelligence \&amp; information technology intrusion detection \&amp; social network analysis</td>
<td style="text-align: center;">reading comprehension \&amp; cognitive linguistics graph isomorphism \&amp; ad hoc game theory \&amp; cognitive psychology</td>
</tr>
<tr>
<td style="text-align: center;">Covid-19</td>
<td style="text-align: center;">alternative splicing \&amp; medical genetics psoriatic arthritis \&amp; life expectancy serotonin syndrome \&amp; herpes zoster</td>
<td style="text-align: center;">proton pump inhibitors \&amp; helicobacter pylori allergic rhinitis \&amp; hyperbaric oxygen immunologic memory \&amp; rheumatic diseases</td>
</tr>
<tr>
<td style="text-align: center;">Economics</td>
<td style="text-align: center;">financial crisis \&amp; pension plan social justice \&amp; wealth inequality intellectual capital \&amp; income distribution</td>
<td style="text-align: center;">credit default swap \&amp; idiosyncratic volatility european union \&amp; quantitative easing quality management \&amp; blockchain technology</td>
</tr>
<tr>
<td style="text-align: center;">Engineering</td>
<td style="text-align: center;">NLP \&amp; collective intelligence finite element \&amp; closed form neural network \&amp; software reuse</td>
<td style="text-align: center;">kinetic energy \&amp; stress relief heat exchanger \&amp; tip vortex wave propagation \&amp; monte carlo</td>
</tr>
<tr>
<td style="text-align: center;">Environmental Science</td>
<td style="text-align: center;">saginaw bay \&amp; domestic sewage air pollutant \&amp; night sky brightness meridional overturning circulation \&amp; solar activity</td>
<td style="text-align: center;">lake victoria \&amp; trophic state image segmentation \&amp; stripe rust electrostatic precipitator \&amp; suspended matter</td>
</tr>
<tr>
<td style="text-align: center;">Geography</td>
<td style="text-align: center;">water resources \&amp; conceptual framework air pollution \&amp; underground river landsat thematic mapper \&amp; dry seaso</td>
<td style="text-align: center;">ecosystem services \&amp; ice sheet vadose zone \&amp; loess plateau pm2.5 concentrations \&amp; ecological restoration</td>
</tr>
<tr>
<td style="text-align: center;">Geology</td>
<td style="text-align: center;">massive sulfide \&amp; early carboniferous rock mechanics \&amp; laser scanning radioactive waste \&amp; early cretaceous</td>
<td style="text-align: center;">damping ratio \&amp; hard rock seismic hazard \&amp; coal mining satellite imagery \&amp; impact craters</td>
</tr>
<tr>
<td style="text-align: center;">History</td>
<td style="text-align: center;">public health \&amp; economic growth public service \&amp; internet governance public finance \&amp; environmental governance</td>
<td style="text-align: center;">social movements \&amp; cold war international law \&amp; paradigm shift social security \&amp; digital divide</td>
</tr>
<tr>
<td style="text-align: center;">Materials Science</td>
<td style="text-align: center;">ion exchange \&amp; aqueous solution barium titanate \&amp; molecular sieve pulsed laser deposition \&amp; visible light</td>
<td style="text-align: center;">cathodic protection \&amp; silicon dioxide electron microscope \&amp; manganese dioxide thermal cycling \&amp; finite difference</td>
</tr>
<tr>
<td style="text-align: center;">Mathematics</td>
<td style="text-align: center;">computational fluid dynamics \&amp; integral equation heat transfer \&amp; partial differential equations hubbard model \&amp; phase velocity</td>
<td style="text-align: center;">neural networks \&amp; maximal matching dynamical systems \&amp; particle swarm optimization differential geometry \&amp; heisenberg group</td>
</tr>
<tr>
<td style="text-align: center;">Medicine</td>
<td style="text-align: center;">breast cancer \&amp; neural crest lactobacillus acidophilus \&amp; bone mineral density drug repurposing \&amp; genetic algorithm</td>
<td style="text-align: center;">clinical trials \&amp; traditional chinese femtosecond laser \&amp; connective tissue monoclonal antibody \&amp; hair cell</td>
</tr>
<tr>
<td style="text-align: center;">Philosophy</td>
<td style="text-align: center;">logical positivism \&amp; immanuel kant moral psychology \&amp; traditional chinese western philosophy \&amp; ontological proof</td>
<td style="text-align: center;">filial piety \&amp; critical thinking economic philosophy \&amp; higher education ontological proof \&amp; volunteer activity</td>
</tr>
<tr>
<td style="text-align: center;">Physics</td>
<td style="text-align: center;">particle swarm optimizer \&amp; pattern recognition neural networks \&amp; quantum interference neutron diffraction \&amp; electric field</td>
<td style="text-align: center;">quantum gravity \&amp; baryon number phase diagram \&amp; wave vector electric field \&amp; ray tracing</td>
</tr>
<tr>
<td style="text-align: center;">Political Science</td>
<td style="text-align: center;">conflict resolution \&amp; cultural diplomacy climate change \&amp; civil society gender equity \&amp; economic crisis</td>
<td style="text-align: center;">media literacy \&amp; public policy foreign affairs \&amp; granger causality civic education \&amp; participatory democracy</td>
</tr>
<tr>
<td style="text-align: center;">Psychology</td>
<td style="text-align: center;">emotion regulation \&amp; self awareness family environment \&amp; self concept chronic physical \&amp; emotional disturbance</td>
<td style="text-align: center;">prosocial behavior \&amp; working memory parahippocampal gyrus \&amp; angelman syndrome williams syndrome \&amp; frontal lobe</td>
</tr>
<tr>
<td style="text-align: center;">Sociology</td>
<td style="text-align: center;">public policy \&amp; sexual harassment citizenship behaviors \&amp; adult education household income \&amp; vocational education</td>
<td style="text-align: center;">regional governance \&amp; cultural heritage middle class \&amp; life satisfaction opinion dynamics \&amp; social exclusion</td>
</tr>
</tbody>
</table>
<p>Table 9: Predicted connections of concepts in different disciplines.</p>
<table>
<thead>
<tr>
<th>Disciplines</th>
<th>Method</th>
<th>Accuracy</th>
<th>All Edges in 2021</th>
<th></th>
<th></th>
<th>New Edges in 2021</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
<td>Precision</td>
<td>Recall</td>
<td>F1</td>
</tr>
<tr>
<td>Art</td>
<td>SEMNET</td>
<td>0.454</td>
<td>0.007</td>
<td>0.464</td>
<td>0.116</td>
<td>0.005</td>
<td>0.533</td>
<td>0.006</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.985</td>
<td>1.000</td>
<td>0.891</td>
<td>0.941</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.998</td>
<td>1.000</td>
<td>0.984</td>
<td>0.992</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.706</td>
<td>0.994</td>
<td>1.000</td>
<td>0.997</td>
<td>0.642</td>
<td>1.000</td>
<td>0.671</td>
</tr>
<tr>
<td>Biology</td>
<td>SEMNET</td>
<td>0.490</td>
<td>0.092</td>
<td>0.495</td>
<td>0.131</td>
<td>0.007</td>
<td>0.568</td>
<td>0.014</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.978</td>
<td>1.000</td>
<td>0.857</td>
<td>0.923</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.995</td>
<td>1.000</td>
<td>0.969</td>
<td>0.984</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.834</td>
<td>0.975</td>
<td>0.999</td>
<td>0.983</td>
<td>0.675</td>
<td>0.953</td>
<td>0.691</td>
</tr>
<tr>
<td>Business</td>
<td>SEMNET</td>
<td>0.573</td>
<td>0.117</td>
<td>0.361</td>
<td>0.148</td>
<td>0.010</td>
<td>0.358</td>
<td>0.019</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.968</td>
<td>1.000</td>
<td>0.843</td>
<td>0.914</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.993</td>
<td>1.000</td>
<td>0.963</td>
<td>0.981</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.706</td>
<td>0.979</td>
<td>1.000</td>
<td>0.989</td>
<td>0.523</td>
<td>1.000</td>
<td>0.576</td>
</tr>
<tr>
<td>Chemistry</td>
<td>SEMNET</td>
<td>0.424</td>
<td>0.106</td>
<td>0.654</td>
<td>0.175</td>
<td>0.008</td>
<td>0.660</td>
<td>0.015</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.988</td>
<td>1.000</td>
<td>0.840</td>
<td>0.813</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.994</td>
<td>1.000</td>
<td>0.970</td>
<td>0.985</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.812</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
<td>0.751</td>
<td>1.000</td>
<td>0.752</td>
</tr>
<tr>
<td>Computer Science</td>
<td>SEMNET</td>
<td>0.459</td>
<td>0.083</td>
<td>0.502</td>
<td>0.127</td>
<td>0.005</td>
<td>0.611</td>
<td>0.010</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.980</td>
<td>1.000</td>
<td>0.875</td>
<td>0.932</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.996</td>
<td>1.000</td>
<td>0.977</td>
<td>0.988</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.593</td>
<td>0.993</td>
<td>1.000</td>
<td>0.996</td>
<td>0.383</td>
<td>1.000</td>
<td>0.426</td>
</tr>
<tr>
<td>Covid-19</td>
<td>SEMNET</td>
<td>0.378</td>
<td>0.059</td>
<td>0.617</td>
<td>0.098</td>
<td>0.005</td>
<td>0.689</td>
<td>0.010</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.979</td>
<td>1.000</td>
<td>0.796</td>
<td>0.882</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.995</td>
<td>1.000</td>
<td>0.947</td>
<td>0.973</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.778</td>
<td>0.987</td>
<td>0.998</td>
<td>0.992</td>
<td>0.662</td>
<td>1.000</td>
<td>0.679</td>
</tr>
<tr>
<td>Economics</td>
<td>SEMNET</td>
<td>0.467</td>
<td>0.111</td>
<td>0.624</td>
<td>0.175</td>
<td>0.007</td>
<td>0.660</td>
<td>0.013</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.974</td>
<td>1.000</td>
<td>0.884</td>
<td>0.938</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.994</td>
<td>1.000</td>
<td>0.971</td>
<td>0.986</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.629</td>
<td>0.852</td>
<td>0.997</td>
<td>0.910</td>
<td>0.246</td>
<td>0.941</td>
<td>0.275</td>
</tr>
<tr>
<td>Engineering</td>
<td>SEMNET</td>
<td>0.599</td>
<td>0.104</td>
<td>0.373</td>
<td>0.151</td>
<td>0.010</td>
<td>0.379</td>
<td>0.019</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.967</td>
<td>1.000</td>
<td>0.825</td>
<td>0.903</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.993</td>
<td>1.000</td>
<td>0.961</td>
<td>0.980</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.757</td>
<td>0.959</td>
<td>1.000</td>
<td>0.977</td>
<td>0.513</td>
<td>1.000</td>
<td>0.545</td>
</tr>
<tr>
<td>Environmental Science</td>
<td>SEMNET</td>
<td>0.485</td>
<td>0.110</td>
<td>0.511</td>
<td>0.150</td>
<td>0.007</td>
<td>0.555</td>
<td>0.014</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.970</td>
<td>1.000</td>
<td>0.831</td>
<td>0.907</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.994</td>
<td>1.000</td>
<td>0.965</td>
<td>0.982</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.714</td>
<td>0.958</td>
<td>1.000</td>
<td>0.975</td>
<td>0.451</td>
<td>0.998</td>
<td>0.470</td>
</tr>
<tr>
<td>Geography</td>
<td>SEMNET</td>
<td>0.551</td>
<td>0.088</td>
<td>0.495</td>
<td>0.159</td>
<td>0.005</td>
<td>0.514</td>
<td>0.009</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.981</td>
<td>1.000</td>
<td>0.884</td>
<td>0.958</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.996</td>
<td>1.000</td>
<td>0.979</td>
<td>0.989</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.728</td>
<td>0.983</td>
<td>0.993</td>
<td>0.988</td>
<td>0.449</td>
<td>0.927</td>
<td>0.465</td>
</tr>
<tr>
<td>Geology</td>
<td>SEMNET</td>
<td>0.479</td>
<td>0.081</td>
<td>0.452</td>
<td>0.127</td>
<td>0.005</td>
<td>0.448</td>
<td>0.014</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.975</td>
<td>1.000</td>
<td>0.850</td>
<td>0.918</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.995</td>
<td>1.000</td>
<td>0.965</td>
<td>0.982</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.758</td>
<td>0.998</td>
<td>1.000</td>
<td>0.999</td>
<td>0.623</td>
<td>1.000</td>
<td>0.641</td>
</tr>
<tr>
<td>History</td>
<td>SEMNET</td>
<td>0.566</td>
<td>0.111</td>
<td>0.464</td>
<td>0.150</td>
<td>0.005</td>
<td>0.496</td>
<td>0.009</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.983</td>
<td>1.000</td>
<td>0.894</td>
<td>0.944</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.997</td>
<td>1.000</td>
<td>0.980</td>
<td>0.990</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.781</td>
<td>1.000</td>
<td>0.998</td>
<td>0.999</td>
<td>0.697</td>
<td>1.000</td>
<td>0.700</td>
</tr>
<tr>
<td>Materials Science</td>
<td>SEMNET</td>
<td>0.471</td>
<td>0.090</td>
<td>0.456</td>
<td>0.110</td>
<td>0.005</td>
<td>0.435</td>
<td>0.011</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.968</td>
<td>1.000</td>
<td>0.853</td>
<td>0.920</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.992</td>
<td>1.000</td>
<td>0.965</td>
<td>0.982</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.618</td>
<td>0.900</td>
<td>1.000</td>
<td>0.940</td>
<td>0.252</td>
<td>1.000</td>
<td>0.291</td>
</tr>
<tr>
<td>Mathematics</td>
<td>SEMNET</td>
<td>0.489</td>
<td>0.106</td>
<td>0.477</td>
<td>0.166</td>
<td>0.006</td>
<td>0.448</td>
<td>0.011</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.974</td>
<td>1.000</td>
<td>0.888</td>
<td>0.940</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.995</td>
<td>1.000</td>
<td>0.979</td>
<td>0.990</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.866</td>
<td>0.951</td>
<td>1.000</td>
<td>0.969</td>
<td>0.665</td>
<td>1.000</td>
<td>0.685</td>
</tr>
<tr>
<td>Medicine</td>
<td>SEMNET</td>
<td>0.474</td>
<td>0.108</td>
<td>0.541</td>
<td>0.168</td>
<td>0.007</td>
<td>0.537</td>
<td>0.014</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.970</td>
<td>1.000</td>
<td>0.849</td>
<td>0.917</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.994</td>
<td>1.000</td>
<td>0.971</td>
<td>0.985</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.694</td>
<td>0.999</td>
<td>1.000</td>
<td>0.995</td>
<td>0.447</td>
<td>1.000</td>
<td>0.462</td>
</tr>
<tr>
<td>Philosophy</td>
<td>SEMNET</td>
<td>0.424</td>
<td>0.102</td>
<td>0.586</td>
<td>0.132</td>
<td>0.005</td>
<td>0.755</td>
<td>0.011</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.981</td>
<td>1.000</td>
<td>0.858</td>
<td>0.921</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.996</td>
<td>1.000</td>
<td>0.966</td>
<td>0.982</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.586</td>
<td>0.985</td>
<td>0.984</td>
<td>0.985</td>
<td>0.423</td>
<td>1.000</td>
<td>0.439</td>
</tr>
<tr>
<td>Physics</td>
<td>SEMNET</td>
<td>0.512</td>
<td>0.120</td>
<td>0.629</td>
<td>0.186</td>
<td>0.012</td>
<td>0.618</td>
<td>0.023</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.973</td>
<td>1.000</td>
<td>0.893</td>
<td>0.943</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.993</td>
<td>1.000</td>
<td>0.974</td>
<td>0.987</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.890</td>
<td>0.909</td>
<td>1.000</td>
<td>0.940</td>
<td>0.692</td>
<td>1.000</td>
<td>0.720</td>
</tr>
<tr>
<td>Political Science</td>
<td>SEMNET</td>
<td>0.424</td>
<td>0.106</td>
<td>0.552</td>
<td>0.167</td>
<td>0.005</td>
<td>0.545</td>
<td>0.010</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.976</td>
<td>1.000</td>
<td>0.865</td>
<td>0.926</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.996</td>
<td>1.000</td>
<td>0.975</td>
<td>0.987</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.817</td>
<td>0.999</td>
<td>0.995</td>
<td>0.997</td>
<td>0.673</td>
<td>1.000</td>
<td>0.692</td>
</tr>
<tr>
<td>Psychology</td>
<td>SEMNET</td>
<td>0.485</td>
<td>0.104</td>
<td>0.565</td>
<td>0.167</td>
<td>0.009</td>
<td>0.623</td>
<td>0.011</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.978</td>
<td>1.000</td>
<td>0.864</td>
<td>0.926</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.994</td>
<td>1.000</td>
<td>0.966</td>
<td>0.983</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.645</td>
<td>0.999</td>
<td>1.000</td>
<td>1.000</td>
<td>0.498</td>
<td>0.989</td>
<td>0.503</td>
</tr>
<tr>
<td>Sociology</td>
<td>SEMNET</td>
<td>0.445</td>
<td>0.099</td>
<td>0.567</td>
<td>0.160</td>
<td>0.005</td>
<td>0.613</td>
<td>0.011</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.976</td>
<td>1.000</td>
<td>0.867</td>
<td>0.928</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.996</td>
<td>1.000</td>
<td>0.975</td>
<td>0.987</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.720</td>
<td>0.988</td>
<td>0.994</td>
<td>0.991</td>
<td>0.540</td>
<td>0.942</td>
<td>0.554</td>
</tr>
<tr>
<td>Average</td>
<td>SEMNET</td>
<td>0.478</td>
<td>0.099</td>
<td>0.519</td>
<td>0.146</td>
<td>0.007</td>
<td>0.552</td>
<td>0.013</td>
</tr>
<tr>
<td></td>
<td>GCN-GAN</td>
<td>0.975</td>
<td>1.000</td>
<td>0.860</td>
<td>0.924</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>EvolveGCN</td>
<td>0.995</td>
<td>1.000</td>
<td>0.970</td>
<td>0.985</td>
<td>N/A</td>
<td>0</td>
<td>N/A</td>
</tr>
<tr>
<td></td>
<td>PLM-LF</td>
<td>0.735</td>
<td>0.970</td>
<td>0.998</td>
<td>0.981</td>
<td>0.540</td>
<td>0.988</td>
<td>0.560</td>
</tr>
</tbody>
</table>
<p>Table 10: Results of link prediction on different disciplines/topics. N/A means all cases have been predicted to be negative.</p>
<table>
<thead>
<tr>
<th>Discipline</th>
<th>Query</th>
<th>Num. of Nodes</th>
<th>Num. of Edges (2021)</th>
<th>Selected Concepts</th>
</tr>
</thead>
<tbody>
<tr>
<td>COVID-19</td>
<td>How Is The Effectiveness Of Vaccines For COVID-19</td>
<td>106</td>
<td>815</td>
<td>public health, clinical trial, infectious diseases</td>
</tr>
<tr>
<td>COVID-19</td>
<td>How Many Variants Does COVID-19 Have?</td>
<td>88</td>
<td>370</td>
<td>amino acid, single nucleotide polymorphism, breast cancer</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What Do We Know About Asymptomatic Transmission Of COVID-19?</td>
<td>110</td>
<td>642</td>
<td>public health, polymerase chain reaction, animal states</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What Are The Sequelae Of COVID-19?</td>
<td>116</td>
<td>605</td>
<td>logistic regression, odds ratio, animal states</td>
</tr>
<tr>
<td>COVID-19</td>
<td>Will The COVID-19 Vaccines And Boosters Work On The New Variants?</td>
<td>125</td>
<td>724</td>
<td>adverse event, clinical trials, thermophilic influenzae</td>
</tr>
<tr>
<td>COVID-19</td>
<td>Antibodies And COVID-19</td>
<td>74</td>
<td>402</td>
<td>monoclonal antibody, phage display, amino acid</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What Is The Difference Between COVID-19 And Influenza?</td>
<td>110</td>
<td>1062</td>
<td>public health, polymerase chain reaction, animal states</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What To Do If You Come Into Close Contact With Someone With COVID-19</td>
<td>53</td>
<td>344</td>
<td>public health, severe acute respiratory syndrome, animal states</td>
</tr>
<tr>
<td>COVID-19</td>
<td>Effective Ways To Prevent COVID-19</td>
<td>72</td>
<td>337</td>
<td>public health, risk factor, health care</td>
</tr>
<tr>
<td>COVID-19</td>
<td>Clinical Presentation Of Covid19 In Dementia Patients</td>
<td>130</td>
<td>2034</td>
<td>vascular dementia, frontotemporal dementia, mild cognitive impairment</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What Is The Effectiveness Of Drugs Being Developed To Treat COVID-19 Patients?</td>
<td>128</td>
<td>727</td>
<td>clinical trial, adverse event, animal states</td>
</tr>
<tr>
<td>COVID-19</td>
<td>What Is The Impact Of The San-Cov-2 (Covid19) Pandemic On The Morbidity And Mortality Of High Risk Patients Undergoing Surgery</td>
<td>106</td>
<td>4382</td>
<td>logistic regression, odds ratio, confidence interval</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Local Community Detection With Plans</td>
<td>68</td>
<td>236</td>
<td>social network, complex network, computational complexity</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Bias And Discrimination In Artificial Intelligence</td>
<td>81</td>
<td>857</td>
<td>artificial intelligence, artificial neural network, neural network</td>
</tr>
<tr>
<td>Computer Science</td>
<td>The Development Of Artificial Intelligence In China</td>
<td>104</td>
<td>1156</td>
<td>artificial intelligence, neural network, artificial neural network</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Interpossibility Of Artificial Intelligence</td>
<td>84</td>
<td>1017</td>
<td>artificial intelligence, neural network, artificial neural network</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Current Trends Of Computer Graphics</td>
<td>84</td>
<td>318</td>
<td>user interface, virtual reality, graphical user interface</td>
</tr>
<tr>
<td>Computer Science</td>
<td>How To Improve The Application Of Machine Learning In Product Development</td>
<td>124</td>
<td>1324</td>
<td>machine learning, neural network, support vector machine</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Commercialization Of Artificial Intelligence</td>
<td>92</td>
<td>942</td>
<td>artificial intelligence, neural network, artificial neural network</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Natural Language Processing And Preinstated Model</td>
<td>80</td>
<td>862</td>
<td>natural language, natural language processing, machine translation</td>
</tr>
<tr>
<td>Computer Science</td>
<td>The Development Of Computer Vision</td>
<td>89</td>
<td>494</td>
<td>image processing, machine vision, visual acuity</td>
</tr>
<tr>
<td>Computer Science</td>
<td>The Development Of Graph Neural Network</td>
<td>88</td>
<td>623</td>
<td>neural network, artificial neural network, graph theory</td>
</tr>
<tr>
<td>Computer Science</td>
<td>Is Computer Science Considered Science Or Engineering?</td>
<td>95</td>
<td>801</td>
<td>mechanical engineering, life science, social sciences</td>
</tr>
<tr>
<td>Computer Science</td>
<td>How Will Artificial Intelligence Develop In The Future?</td>
<td>106</td>
<td>1221</td>
<td>artificial intelligence, neural network, artificial neural network</td>
</tr>
<tr>
<td>Geography</td>
<td>A Volcanic Emption As The Earth' S Devastating Force</td>
<td>134</td>
<td>1309</td>
<td>volcanic eruption, volcanic ash, lava flow</td>
</tr>
<tr>
<td>Geography</td>
<td>Features And Qualities Of Coastal Erosion</td>
<td>132</td>
<td>987</td>
<td>remote sensing, climate change, sediment transport</td>
</tr>
<tr>
<td>Geography</td>
<td>Assessment Of Climate Sensitivity</td>
<td>147</td>
<td>1826</td>
<td>climate change, climate sensitivity, greenhouse gas</td>
</tr>
<tr>
<td>Geography</td>
<td>Geography And Economic Development</td>
<td>92</td>
<td>940</td>
<td>economic geography, economic development, economic growth</td>
</tr>
<tr>
<td>Geography</td>
<td>Impact Of Climate Change On Agroenterological Disasters And Paris And Diseases</td>
<td>152</td>
<td>2671</td>
<td>natural disaster, global warming, food security</td>
</tr>
<tr>
<td>Geography</td>
<td>How Human Activities Contribute To Climate Change</td>
<td>157</td>
<td>2688</td>
<td>climate change, global warming, global climate change</td>
</tr>
<tr>
<td>Geography</td>
<td>Effect Of Ice Loss On Sea Level Rise</td>
<td>151</td>
<td>3149</td>
<td>sea level, sea level rise, climate change</td>
</tr>
<tr>
<td>Geography</td>
<td>Animal Extinction And Ways Of Preventing The Human Role In It</td>
<td>107</td>
<td>718</td>
<td>anxiety disorders, prefrontal cortex, medial prefrontal cortex</td>
</tr>
<tr>
<td>Geography</td>
<td>What Is The Impact Of Urban Expansion On Plant Diversity Change In Karst Regions Of Northwest China</td>
<td>118</td>
<td>1268</td>
<td>southwest China, climate change, rural China</td>
</tr>
<tr>
<td>Geography</td>
<td>Assessment Method Of The Sea Turtle Nesting Habitat Of Small Reef Islands</td>
<td>144</td>
<td>1734</td>
<td>sea turtle, green turtle, climate change</td>
</tr>
<tr>
<td>Geography</td>
<td>What Is The Impact Of Sea Level Rise On Ecological Infrastructure?</td>
<td>167</td>
<td>1960</td>
<td>climate change, storm surge, tide gauge</td>
</tr>
<tr>
<td>Geography</td>
<td>What Cause Short Term Sea Level Change In Censuoses?</td>
<td>176</td>
<td>2498</td>
<td>climate change, tars censuoses, early cetsiuoses</td>
</tr>
<tr>
<td>Geology</td>
<td>Composition Of Geosphere</td>
<td>91</td>
<td>383</td>
<td>climate change, radioactive waste, global warming</td>
</tr>
<tr>
<td>Geology</td>
<td>Glacier Mass Change</td>
<td>159</td>
<td>1859</td>
<td>mass balance, climate change, digital elevation model</td>
</tr>
<tr>
<td>Geology</td>
<td>Evolution Of Sedimentary Rock Formation Of A Rock Association Level</td>
<td>111</td>
<td>1749</td>
<td>sedimentary rock, source rock, trace element</td>
</tr>
<tr>
<td>Geology</td>
<td>Superimposed Metamorphism Of Chinese Coal</td>
<td>94</td>
<td>463</td>
<td>coalbed methane, trace element, functional group</td>
</tr>
<tr>
<td>Geology</td>
<td>What Is The Complete Process Of Basin Formation?</td>
<td>107</td>
<td>1414</td>
<td>source rock, tars censuoses, early cetsiuoses</td>
</tr>
<tr>
<td>Geology</td>
<td>Effect Of The Combination Characteristics Of Rock Structural Plans On The Stability Of A Rock-Mass Slope</td>
<td>53</td>
<td>328</td>
<td>finite element, shear strength, open pit</td>
</tr>
<tr>
<td>Geology</td>
<td>How Do Geological Plans Change?</td>
<td>109</td>
<td>980</td>
<td>plate tectonics, north america, climate change</td>
</tr>
<tr>
<td>Geology</td>
<td>A Case Study Assessment Of Sea-Expediction Potential</td>
<td>68</td>
<td>323</td>
<td>case histories, peak ground acceleration, shear wave velocity</td>
</tr>
<tr>
<td>Geology</td>
<td>Global Distribution Of Carbonate</td>
<td>137</td>
<td>2127</td>
<td>climate change, carbon cycle, carbon dioxide</td>
</tr>
<tr>
<td>Geology</td>
<td>What Is The Impact Of Man On Geo-Electronment?</td>
<td>111</td>
<td>582</td>
<td>remote sensing, semisnable development, climate change</td>
</tr>
<tr>
<td>Geology</td>
<td>Differences In The Influence Of The Tectonic Setting Of The Earth On The Formation Of Migrins</td>
<td>109</td>
<td>1427</td>
<td>rare earth element, rare earth, volcanic rock</td>
</tr>
<tr>
<td>Geology</td>
<td>Evolution Of Archean Continental Crust</td>
<td>134</td>
<td>1511</td>
<td>continental margin, oceanic crust, partial melting</td>
</tr>
</tbody>
</table>
<p>Table 11: Statistics of queries and corresponding evolving concept co-occurrence graphs in COVID-19, Computer Science, Geography, and Geology.</p>
<table>
<thead>
<tr>
<th>Discipline</th>
<th>Query</th>
<th>Num. of Nodes</th>
<th>Num. of Edges (2021)</th>
<th>Selected Concepts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mathematics</td>
<td>Brick And Swimerton-Dyer Conjecture</td>
<td>84</td>
<td>427</td>
<td>betula pendula, growing season, silver birch</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Topology And Differential Geometry</td>
<td>82</td>
<td>1100</td>
<td>differential geometry, differential equations, differential forms</td>
</tr>
<tr>
<td>Mathematics</td>
<td>What Is The Geometry Meaning For Rigidity In Biomannian Manifolds?</td>
<td>54</td>
<td>382</td>
<td>momentum geometry, tie group, sectional curvature</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Recent Developments In The Navier-Stokes Problem</td>
<td>37</td>
<td>199</td>
<td>finite element, fluid dynamics, reynolds number</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Recent Developments In Biomann Hypothesis</td>
<td>71</td>
<td>643</td>
<td>stomans hypothesis, axis function, stomans axis function</td>
</tr>
<tr>
<td>Mathematics</td>
<td>When Is A Finite Type Dupto Hypersurface Of A Hypersphere Isoparametric?</td>
<td>43</td>
<td>174</td>
<td>euclidean space, riemannian manifold, vector field</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Numerical Analysis And Scientific Computing</td>
<td>127</td>
<td>2489</td>
<td>scientific computing, numerical methods, numerical analysis</td>
</tr>
<tr>
<td>Mathematics</td>
<td>The Fundamental Group Of A Noncommutative Space</td>
<td>60</td>
<td>643</td>
<td>field theory, quantum mechanics, phase space</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Double Phase Autoscopic Variational Problem</td>
<td>35</td>
<td>471</td>
<td>boundary condition, finite element, phase transition</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Complex Network Analysis Of Nonlinear Time Series</td>
<td>81</td>
<td>879</td>
<td>neural network, artificial neural network, dynamical systems</td>
</tr>
<tr>
<td>Mathematics</td>
<td>P Versus Ng Problem And X Approximation</td>
<td>31</td>
<td>206</td>
<td>approximation algorithm, combinatorial optimization, approximation ratio</td>
</tr>
<tr>
<td>Mathematics</td>
<td>Rationality Of Rigid Quiver Grassmannians</td>
<td>38</td>
<td>227</td>
<td>models space, tie algebra, fixed point</td>
</tr>
<tr>
<td>History</td>
<td>The Silk Road</td>
<td>79</td>
<td>428</td>
<td>silk road, central axis, bombyn mers</td>
</tr>
<tr>
<td>History</td>
<td>When Music Mannerd?</td>
<td>73</td>
<td>425</td>
<td>music education, classical music, musical instrument</td>
</tr>
<tr>
<td>History</td>
<td>China And The West: Society And Culture</td>
<td>94</td>
<td>349</td>
<td>western culture, united states, cultural diversity</td>
</tr>
<tr>
<td>History</td>
<td>Governance In Ancient</td>
<td>59</td>
<td>225</td>
<td>ancient geod, ancient grease, han dynasty</td>
</tr>
<tr>
<td>History</td>
<td>The Domestic Policy Of The European Westear Republic</td>
<td>125</td>
<td>1734</td>
<td>european union, member state, czech republic</td>
</tr>
<tr>
<td>History</td>
<td>What Changed After The October Revolution?</td>
<td>92</td>
<td>720</td>
<td>outdoor revolution, industrial revolution, united states</td>
</tr>
<tr>
<td>History</td>
<td>Capitalism In America</td>
<td>98</td>
<td>1398</td>
<td>latin america, latin american, united states</td>
</tr>
<tr>
<td>History</td>
<td>The Impact Of Maritime Trade On World Civilization.</td>
<td>114</td>
<td>1289</td>
<td>united states, free trade, east axis</td>
</tr>
<tr>
<td>History</td>
<td>The Reign And Life Of Queen Elizabeth</td>
<td>93</td>
<td>600</td>
<td>henry vi, elizabeth ii, edward vi</td>
</tr>
<tr>
<td>History</td>
<td>Governing The New Nation</td>
<td>110</td>
<td>1022</td>
<td>united states, united nations, case study</td>
</tr>
<tr>
<td>History</td>
<td>British Colonial Studies</td>
<td>118</td>
<td>1145</td>
<td>british empire, world war, united states</td>
</tr>
<tr>
<td>History</td>
<td>Social Movements In America</td>
<td>142</td>
<td>2149</td>
<td>social movement, united states, latin america</td>
</tr>
<tr>
<td>Psychology</td>
<td>Lesbian, Gay, Bisexual, Transgender</td>
<td>54</td>
<td>489</td>
<td>sexual orientation, mutual health, united states</td>
</tr>
<tr>
<td>Psychology</td>
<td>Positive Psychology</td>
<td>71</td>
<td>480</td>
<td>positive psychology, mental health, organizational behavior</td>
</tr>
<tr>
<td>Psychology</td>
<td>Psychology And Criminology</td>
<td>77</td>
<td>812</td>
<td>criminal justice, social control, criminal behavior</td>
</tr>
<tr>
<td>Psychology</td>
<td>Personal Perception And Self-Consciousness</td>
<td>82</td>
<td>247</td>
<td>personality trait, self consciousness, college student</td>
</tr>
<tr>
<td>Psychology</td>
<td>The Suicide Intervention</td>
<td>122</td>
<td>1835</td>
<td>suicide prevention, mental health, public health</td>
</tr>
<tr>
<td>Psychology</td>
<td>Mental Health Of Children</td>
<td>120</td>
<td>1779</td>
<td>mental health, health care, mental illness</td>
</tr>
<tr>
<td>Psychology</td>
<td>How Does Cognitivism Differ From Behaviorism?</td>
<td>30</td>
<td>95</td>
<td>cognitive science, information processing, cultural differences</td>
</tr>
<tr>
<td>Psychology</td>
<td>Racism, Bias, And Discrimination</td>
<td>81</td>
<td>684</td>
<td>african american, united states, civil rights</td>
</tr>
<tr>
<td>Psychology</td>
<td>Does Group Polarization Affect The Minority?</td>
<td>94</td>
<td>289</td>
<td>united states, african american, health care</td>
</tr>
<tr>
<td>Psychology</td>
<td>Pain Pressure On Academic Performance</td>
<td>85</td>
<td>799</td>
<td>high school, past group, past pressure</td>
</tr>
<tr>
<td>Psychology</td>
<td>What Is The Role Of Cognitive Flexibility And Inhibition In Complex Dynamic Tasks</td>
<td>101</td>
<td>744</td>
<td>working memory, executive function, frontal cortex</td>
</tr>
<tr>
<td>Psychology</td>
<td>Sex Differences In Functional Connectivity Between Resting State Brain Networks In Autism Spectrum Disorder</td>
<td>133</td>
<td>2477</td>
<td>autism spectrum disorder, magnetic resonance imaging, functional magnetic resonance imaging</td>
</tr>
<tr>
<td>Economics</td>
<td>Globalization And Unemployment</td>
<td>89</td>
<td>1022</td>
<td>financial crisis, economic growth, united states</td>
</tr>
<tr>
<td>Economics</td>
<td>Supply Chain Management</td>
<td>84</td>
<td>1023</td>
<td>chain management, supply chain management, supply chain</td>
</tr>
<tr>
<td>Economics</td>
<td>Volatility And The Cross Section Of Real Estate Equity Returns During Pandemic</td>
<td>58</td>
<td>426</td>
<td>real estate investment trust, financial crisis, asset allocation</td>
</tr>
<tr>
<td>Economics</td>
<td>Human Capital And China'S Future Growth</td>
<td>91</td>
<td>1250</td>
<td>economic growth, human capital, economic development</td>
</tr>
<tr>
<td>Economics</td>
<td>What Critical Approach To Nonclassical Economics Is Superior?</td>
<td>102</td>
<td>1255</td>
<td>classical economics, economic theory, economic growth</td>
</tr>
<tr>
<td>Economics</td>
<td>The Economic Policy Uncertainty</td>
<td>114</td>
<td>1218</td>
<td>economic growth, monetary policy, united states</td>
</tr>
<tr>
<td>Economics</td>
<td>Us Earnings Inequality</td>
<td>101</td>
<td>1024</td>
<td>income inequality, united states, wage inequality</td>
</tr>
<tr>
<td>Economics</td>
<td>What Is The Application Of Fixed Point Theory In Financial And Economic Sciences?</td>
<td>104</td>
<td>1015</td>
<td>financial market, economic theory, financial crisis</td>
</tr>
<tr>
<td>Economics</td>
<td>The Digital Economy</td>
<td>91</td>
<td>1003</td>
<td>digital economy, digital divide, economic growth</td>
</tr>
<tr>
<td>Economics</td>
<td>How Rate Hikes Can Exacerbate Labor-Market Inequality</td>
<td>96</td>
<td>1076</td>
<td>income inequality, united states, human capital</td>
</tr>
<tr>
<td>Economics</td>
<td>Why Do Begins Pay Different Prices For Comparable Products?</td>
<td>74</td>
<td>202</td>
<td>supply chain, online auction, information asymmetry</td>
</tr>
<tr>
<td>Economics</td>
<td>The Impact Of Globalization On Income Distribution In Emerging Economies</td>
<td>124</td>
<td>2384</td>
<td>income distribution, economic growth, developing countries</td>
</tr>
</tbody>
</table>
<p>Table 12: Statistics of queries and corresponding evolving concept co-occurrence graphs in Mathematics, History, Psychology, and Economics.</p>
<table>
<thead>
<tr>
<th>Discipline</th>
<th>Query</th>
<th>Num. of Nodes</th>
<th>Num. of Edges (2021)</th>
<th>Selected Concepts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sociology</td>
<td>Gender Inequality In America</td>
<td>102</td>
<td>1534</td>
<td>gender equality, latin america, economic growth</td>
</tr>
<tr>
<td>Sociology</td>
<td>Abortion And Abortion Rights</td>
<td>95</td>
<td>1174</td>
<td>united states, health care, reproductive rights</td>
</tr>
<tr>
<td>Sociology</td>
<td>Social Networks Addiction</td>
<td>75</td>
<td>742</td>
<td>social network, social networks, social support</td>
</tr>
<tr>
<td>Sociology</td>
<td>Does An Improvement In Rural Infrastructure Contribute To Allovian Poverty In Pakistan?</td>
<td>124</td>
<td>1741</td>
<td>poverty reduction, developing countries, economic growth</td>
</tr>
<tr>
<td>Sociology</td>
<td>Spread Of False News</td>
<td>66</td>
<td>188</td>
<td>false alarm, social media, machine learning</td>
</tr>
<tr>
<td>Sociology</td>
<td>Regional Identity And Regional Change</td>
<td>89</td>
<td>798</td>
<td>european union, east asia, economic development</td>
</tr>
<tr>
<td>Sociology</td>
<td>What Is The Impact Of Intergenerational Mobility On Well-Being In Japan</td>
<td>92</td>
<td>505</td>
<td>united states, human capital, income inequality</td>
</tr>
<tr>
<td>Sociology</td>
<td>Social Media And Marketing</td>
<td>100</td>
<td>1325</td>
<td>social media, media market, social media marketing</td>
</tr>
<tr>
<td>Sociology</td>
<td>Can Populism Contribute To A More Inclusive Citizenship?</td>
<td>97</td>
<td>785</td>
<td>human rights, united states, global citizenship</td>
</tr>
<tr>
<td>Sociology</td>
<td>Advancement Of Women In Science</td>
<td>113</td>
<td>963</td>
<td>united states, national science foundation, career advancement</td>
</tr>
<tr>
<td>Sociology</td>
<td>How Are Functionalism And Conflict Theory Similar?</td>
<td>86</td>
<td>333</td>
<td>conflict revolution, conflict management, international relations</td>
</tr>
<tr>
<td>Sociology</td>
<td>How Do Social Networks Influence Educational Processes</td>
<td>112</td>
<td>1119</td>
<td>social network, social networks, social influence</td>
</tr>
<tr>
<td>Art</td>
<td>Modern Acrobatics</td>
<td>47</td>
<td>135</td>
<td>traditional chinese, chinese traditional, song gansed</td>
</tr>
<tr>
<td>Art</td>
<td>Eastern Asian Art</td>
<td>117</td>
<td>1174</td>
<td>east asia, south asia, central asia</td>
</tr>
<tr>
<td>Art</td>
<td>Surrealist Acrobatics</td>
<td>34</td>
<td>123</td>
<td>under-botest, twentieth century, world war</td>
</tr>
<tr>
<td>Art</td>
<td>Modern Rock Music Trend</td>
<td>89</td>
<td>936</td>
<td>popular music, rock music, music industry</td>
</tr>
<tr>
<td>Art</td>
<td>Interaction Design</td>
<td>83</td>
<td>736</td>
<td>interaction design, interface design, user experience</td>
</tr>
<tr>
<td>Art</td>
<td>Victorian Beauty Standards In Art</td>
<td>72</td>
<td>395</td>
<td>victorian period, victorian era, victorian england</td>
</tr>
<tr>
<td>Art</td>
<td>Are Culturally Vibrant Communities Healthier?</td>
<td>98</td>
<td>489</td>
<td>united states, public health, cultural heritage</td>
</tr>
<tr>
<td>Art</td>
<td>The Role Of Cultural Identity In The Creation Of Art</td>
<td>112</td>
<td>1002</td>
<td>cultural identity, contemporary art, case study</td>
</tr>
<tr>
<td>Art</td>
<td>How Art Develops The Personality Of Human Beings?</td>
<td>108</td>
<td>504</td>
<td>human development, visual arts, personal development</td>
</tr>
<tr>
<td>Art</td>
<td>How Can Graffiti Be Accepted As A Form Of Street Art And Which Attributes Can Be Contributed To Architecture?</td>
<td>75</td>
<td>492</td>
<td>public space, street art, graffiti art</td>
</tr>
<tr>
<td>Art</td>
<td>The Use Of Arts Interventions For Mental Health And Wellbeing In Health Settings</td>
<td>140</td>
<td>1895</td>
<td>mental health, mental illness, public health</td>
</tr>
<tr>
<td>Art</td>
<td>What Is The Difference Between Islamic Art And Christian Art In Terms Of Function In The Middle Ages?</td>
<td>126</td>
<td>1357</td>
<td>middle age, middle race, islamic art</td>
</tr>
<tr>
<td>Business</td>
<td>Employee Motivation</td>
<td>71</td>
<td>683</td>
<td>job satisfaction, human resource, dependent variable</td>
</tr>
<tr>
<td>Business</td>
<td>International Trade Trends In The Usa</td>
<td>113</td>
<td>1417</td>
<td>developing countries, free trade, united states</td>
</tr>
<tr>
<td>Business</td>
<td>How Blockchain And Cryptocurrency Can Revolutionize Business?</td>
<td>85</td>
<td>663</td>
<td>business model, business process, blockchain technology</td>
</tr>
<tr>
<td>Business</td>
<td>What Is The Impact Of Intermediaries On A Negotiation?</td>
<td>82</td>
<td>320</td>
<td>united states, developing countries, european union</td>
</tr>
<tr>
<td>Business</td>
<td>International Business, Further Globalisation Or Backlash?</td>
<td>117</td>
<td>949</td>
<td>international trade, developing countries, business environment</td>
</tr>
<tr>
<td>Business</td>
<td>The Market Growth For Electric Vehicles</td>
<td>92</td>
<td>1599</td>
<td>electric vehicle, hybrid electric vehicle, hybrid electric</td>
</tr>
<tr>
<td>Business</td>
<td>Current Trends In Consumer Behavior</td>
<td>74</td>
<td>336</td>
<td>literature review, social media, online shopping</td>
</tr>
<tr>
<td>Business</td>
<td>Why Is The Importance Of The Correlation Analysis Between The Stock Market Valuation And The Economic Situation Of Business-Exercise Growing?</td>
<td>100</td>
<td>1422</td>
<td>stock market, financial market, capital market</td>
</tr>
<tr>
<td>Business</td>
<td>How To Evaluate Cost Impacts On Reverse Logistics Using An Economic Order Quantity (Eoq) Model With Environmental And Social Considerations</td>
<td>61</td>
<td>330</td>
<td>supply chain management, sustainable development, carbon emission</td>
</tr>
<tr>
<td>Business</td>
<td>Global Unemployment</td>
<td>94</td>
<td>1985</td>
<td>economic growth, united states, unemployment insurance</td>
</tr>
<tr>
<td>Business</td>
<td>Capitalism And Multinational Companies</td>
<td>80</td>
<td>975</td>
<td>multinational corporation, foreign direct investment, human capital</td>
</tr>
<tr>
<td>Business</td>
<td>New Financial Crisis</td>
<td>98</td>
<td>1897</td>
<td>global financial crisis, financial market, financial crises</td>
</tr>
<tr>
<td>Physics</td>
<td>Microfluidics And Microsystems</td>
<td>99</td>
<td>554</td>
<td>cell culture, integrated circuit, sample preparation</td>
</tr>
<tr>
<td>Physics</td>
<td>How Swarm Robotics Can Be Used To Describe Particle System'S Deformation</td>
<td>86</td>
<td>474</td>
<td>particle swarm optimization, mobile robot, swarm intelligence</td>
</tr>
<tr>
<td>Physics</td>
<td>Dark Matter</td>
<td>81</td>
<td>1208</td>
<td>dark matter, direct detection, standard model</td>
</tr>
<tr>
<td>Physics</td>
<td>Galaxy Formation And Evolution</td>
<td>91</td>
<td>1671</td>
<td>star formation, galaxy evolution, galaxy formation</td>
</tr>
<tr>
<td>Physics</td>
<td>Big Bang (Quadratic) Cosmology</td>
<td>70</td>
<td>1092</td>
<td>big bang, cosmological constant, dark matter</td>
</tr>
<tr>
<td>Physics</td>
<td>Global Nonlinear Stability Of Large Dispersive Solutions To The Easiness Equations</td>
<td>55</td>
<td>318</td>
<td>differential equation, caseity problem, partial differential equations</td>
</tr>
<tr>
<td>Physics</td>
<td>How Does The Magnetoresistance Reflect The Information Of Form Surface?</td>
<td>55</td>
<td>564</td>
<td>form surface, magnetic field, form level</td>
</tr>
<tr>
<td>Physics</td>
<td>Condensed Matter Physics And Acoustics</td>
<td>92</td>
<td>1156</td>
<td>condensed matter, condensed matter physics, phase transition</td>
</tr>
<tr>
<td>Physics</td>
<td>The Development Is Quantum Computers</td>
<td>92</td>
<td>1248</td>
<td>quantum computing, quantum computers, quantum computation</td>
</tr>
<tr>
<td>Physics</td>
<td>Optical Physics And Quantum Information Science</td>
<td>113</td>
<td>2218</td>
<td>quantum information, quantum mechanics, quantum physics</td>
</tr>
<tr>
<td>Physics</td>
<td>The Space-Time Geometry Behind The Constant Speed Of Light</td>
<td>118</td>
<td>1826</td>
<td>general relativity, special relativity, cosmological constant</td>
</tr>
<tr>
<td>Physics</td>
<td>Antiferromagnetic Spinwans</td>
<td>35</td>
<td>470</td>
<td>magnetic field, magnetic moment, ground state</td>
</tr>
</tbody>
</table>
<p>Table 13: Statistics of queries and corresponding evolving concept co-occurrence graphs in Sociology, Art, Business, and Physics.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Luoyi Fu is the corresponding author.
${ }^{1}$ The project is publicly available for research purpose https://github.com/xyjigsaw/Kiscovery.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
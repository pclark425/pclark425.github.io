<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9624 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9624</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9624</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-277781732</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.08752v1.pdf" target="_blank">Patience is all you need! An agentic system for performing scientific literature review</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have grown in their usage to provide support for question answering across numerous disciplines. The models on their own have already shown promise for answering basic questions, however fail quickly where expert domain knowledge is required or the question is nuanced. Scientific research often involves searching for relevant literature, distilling pertinent information from that literature and analysing how the findings support or contradict one another. The information is often encapsulated in the full text body of research articles, rather than just in the abstracts. Statements within these articles frequently require the wider article context to be fully understood. We have built an LLM-based system that performs such search and distillation of information encapsulated in scientific literature, and we evaluate our keyword based search and information distillation system against a set of biology related questions from previously released literature benchmarks. We demonstrate sparse retrieval methods exhibit results close to state of the art without the need for dense retrieval, with its associated infrastructure and complexity overhead. We also show how to increase the coverage of relevant documents for literature review generation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9624.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9624.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Patience agentic review system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Patience: An agentic system for performing scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented, multi-agent pipeline that performs retrieval, LLM-driven query expansion, re-ranking, chunk-level summarisation, verification (CoVe) and iterative literature expansion to produce grounded long-form literature reviews and question answers from biomedical corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercially-available large language model from Anthropic used as the central reasoning and prompting engine across the pipeline (query expansion, proposed-answer generation for reranking, chunk summarisation, final answer synthesis, chain-of-thought and verification steps). The paper notes the Claude 3.5 Sonnet family supports very large contexts (example cited: 200K token context length).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Indexed biomedical literature from PubMed (citations: titles and abstracts ~37 million) and the PubMed Central open access subset (full-text articles, ~5 million entries). Evaluation used subsets: LitQA2 questions mapped to PMC OA (103/199 available) and a 200-question subset from PubMedQA; additionally Nature review reference sets for CoVe evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Biology-related research questions drawn from published benchmarks (LitQA2, PubMedQA) and starting questions derived from Nature review abstracts; queries ranged from tightly-scoped multiple-choice style questions to open-ended literature-review prompts (e.g., the role of nervous system in cancer; integrating cryo-EM and cryo-ET).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>A multi-stage agentic pipeline: (1) NER (HunFlair and TrendyGenes) over question to extract gene/disease identifiers; (2) LLM-generated 'must'/'should' search terms with synonyms and boost weights; (3) sparse retrieval against an Elasticsearch index (sparse BM25L-style searches; boosts for recency); (4) chunking of retrieved articles (≈10,000 words per chunk with ≈250-word overlap) and LLM generation of three diverse proposed answers + synonyms; (5) re-ranking of chunks using BM25L scored against the question and each proposed answer, combined with reciprocal rank fusion; (6) LLM-led information extraction: chunk-level summarisation, optional 3-way deduplication of summaries, then final answering synthesising retained summaries with chain-of-thought prompts; (7) Chain-of-Verification (CoVe) extension: plan ~10 verification questions from draft answers, retrieve evidence for those checks, independently fact-check and produce a verified final response; (8) iterative literature expansion by generating follow-up (expansive) questions from initial answers to discover additional relevant/contradictory papers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form literature reviews / grounded narrative answers with cited source attributions and lists of supporting references; intermediate outputs include chunk summaries, proposed answers (for reranking), verification Q&A pairs and expanded question lists.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example (from reranking/proposed-answer stage): 'Kif9 knockdown zebrafish embryos likely display shorter cilia (synonyms: reduced-length cilia, truncated cilia, diminished cilia...) ... Without Kif9, transport of building materials to the ciliary tip may be impaired, resulting in shorter cilia due to insufficient elongation during ciliogenesis.' (one of three diverse, sometimes-contradictory proposed answers used to form BM25L queries and drive reranking).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative retrieval and QA benchmarking: measured recall of benchmark source articles within top-N search results (top 200), measured position of key passages after re-ranking (top 30 chunks), and standard LitQA2 metrics for QA (Accuracy = correct/total, Coverage = attempted/total, Precision = correct/attempted). Additional evaluation: attribution rates (whether final answer was attributed to original source paper), comparisons to PaperQA2 on the same benchmarks, and CoVe evaluation as recall of references used by 10 Nature review articles (primary and secondary references).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Key reported numbers: LLM keyword retrieval found 61.7% ± 1.2 of LitQA2 source articles in top-200 (on PMC OA subset); re-ranking retrieved key passages such that ~83% of key passages were within the first 30 chunks (vs ~900 chunks for baseline). Pipeline-level recall/retention cascade (Table 1): LLM keyword retrieval (top 200) 61.7% ±1.2; re-ranking (top 30 chunks) 57.3% ±1.9; LLM context summarisation & retention 57.0% ±2.4; attribution to answer 50.5% ±0.8. Overall the system achieved ~60.4% coverage (including use of other papers beyond benchmark source) and precision ≈90%; compared to PaperQA2 the system had similar source attribution (50.4% vs 48.2%), lower overall accuracy on LitQA2 (60.4% vs 66%) but higher precision (90% vs 85%). CoVe increased mean recall of primary/secondary references on Nature reviews by ~6%–25%.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrates that sparse retrieval with LLM-driven query expansion and BM25L re-ranking can approach state-of-the-art without dense embedding infrastructure; leverages LLMs for synonyms, query expansion, proposed-answer generation and verification to improve coverage; high precision and better grounding via verification (CoVe); transferable to corpora lacking citation networks; exploits large-context LLM capabilities to summarise full-text chunks; faster than human reviewers at single-paper extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Primary bottleneck is initial retrieval (long tail of search hits); sparse retrieval still lacks semantic nuances and synonym handling without LLM expansion; finite context prevents including many full full-texts verbatim (requires chunking); system's coverage sensitive to the LLM's calibration/caution (Claude 3.5 Sonnet was more conservative than GPT-4o, reducing coverage); execution time (end-to-end) typically ~10–30 minutes; benchmarks (LitQA2) may not reflect real-world multi-paper synthesis needs and often assume a single source paper, which the system sometimes broke; potential for LLM to drop relevant papers during summarisation; risk of contradictory evidence requiring careful handling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failure modes include: initial retrieval missing the true source (major loss of recall occurs at retrieval stage), LLM summariser discarding relevant papers (lowering attribution/recall), proposed-answer generation producing contradictory candidate answers (intentionally to diversify queries) which can complicate synthesis, conservative LLM behaviour reducing coverage, and long-tail search hit distributions causing difficulty finding some relevant papers beyond early ranks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Patience is all you need! An agentic system for performing scientific literature review', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9624.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9624.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (extended)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (extended for verification and diversification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extended Chain-of-Verification workflow that generates verification questions from a draft LLM answer, retrieves evidence for each verification question, independently fact-checks them, and synthesises a final verified response while also expanding the literature searched.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used to generate planning items (key statements), produce follow-up verification questions (~10 per draft), perform retrieval-guided fact checks, and synthesize final verified long-form responses; prompts engineered to use chain-of-thought and to expand literature coverage through questions generated during verification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Applied to retrieval over the same PubMed and PubMed Central OA indices used by the main pipeline; evaluated specifically against the reference lists of 10 recent Nature review articles (primary and secondary references).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Starting questions derived from review abstracts (examples include: integrating cryo-EM and cryo-ET; role of nervous system in cancer; others listed in Table 4). Verification questions are derived from the draft response and focus on validating ~10 key statements.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Draft-answer → planning (identify ~10 key factual statements) → convert statements into verification questions → retrieve evidence for each verification question → independently answer/verify those questions → combine verification answers and original to produce a final verified, diversified response. The process also collects additional references surfaced during the verification retrieval, increasing literature coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Verified long-form responses, lists of verification Q&A pairs, expanded reference lists (primary and secondary), and follow-up questions for further literature exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>Example follow-up/verification questions (from Appendix B) for a cryo-EM integration question: 'What is the highest resolution achieved by single-particle cryo-EM for protein structures?'; 'Can cryo-ET provide high-resolution structural details comparable to single-particle cryo-EM?'; 'Are there limitations or potential artifacts in fitting high-resolution cryo-EM structures into lower-resolution cryo-ET maps?'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measured recall of references from 10 Nature review articles, comparing references found by the draft response alone versus the draft+verification retrieval; counted primary and secondary references recovered within date cutoffs matching review publication.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>CoVe improved mean coverage of primary and secondary references on the evaluated Nature reviews by between ~6% and ~25% (exact per-review numbers reported in Table 3). The fact-checking stage also surfaced additional questions that expanded literature coverage beyond the original retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Improves literature coverage and helps identify contradictory evidence; produces intermediate verification artifacts that make final outputs more grounded; mitigates hallucination by explicit fact-checking steps; expands retrieval beyond citation-network navigation, helping find newer or less-cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Additional computational and execution cost; effectiveness limited by initial retrieval quality (still reliant on the underlying search index); may retrieve many verification-targeted documents that are noisy or only tangentially relevant resulting in more downstream filtering; depends on LLM calibration for verification judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Cases where CoVe fails include: verification retrieval still misses relevant primary evidence (if it's low-ranked), verification answers contradict each other when evidence is mixed or ambiguous, and expansion retrieves tangential or spurious references requiring human adjudication.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Patience is all you need! An agentic system for performing scientific literature review', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9624.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9624.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA2 (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA / PaperQA2 (Retrieval-augmented generative agent for scientific research)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously-published retrieval-augmented LLM agent for scientific research that the authors compare against; in the paper it is described as relying on Semantic Scholar sparse retrieval with a dense reranker and built on GPT-4o for some deployments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (reported in this paper's comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In the paper's discussion PaperQA2 is characterised as using Semantic Scholar's sparse retrieval plus a dense reranker and being built on GPT-4o, leading to higher coverage but lower precision relative to the Patience system in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not exhaustively detailed in this paper; PaperQA2 relies on external document stores (Semantic Scholar) and dense re-ranking infrastructure per the authors' description and cited reference.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>PaperQA2 is used for literature question answering and long-form synthesis tasks across scientific domains (as cited by the authors).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation using external sparse retrieval (Semantic Scholar) and dense re-ranking (cross-encoders), with an LLM-based synthesis step.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form answers and literature syntheses with attributed sources.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reported results in the cited PaperQA2 work; in this paper PaperQA2 is compared on LitQA2-style benchmarks and retrieval/attribution metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>In the authors' comparative summary: PaperQA2 had slightly lower attribution to original source for correct answers (48.2%) compared with Patience (50.4%), higher overall accuracy on LitQA2 (66% vs Patience's 60.4%), but lower precision (85% vs Patience's 90%). The paper attributes some PaperQA2 higher coverage to GPT-4o being less conservative about answering.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Higher coverage on benchmarks (less conservative answering), established use of dense re-ranking and external large-scale retrieval indices (Semantic Scholar) which can increase recall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>May be more prone to giving answers without sufficient grounded context (higher coverage but lower precision); requires dense retrieval and additional infrastructure (embeddings/indexing) that adds complexity and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported tendency to give answers (increase coverage) even when appropriate supporting context is not present in the prompt, which can increase hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Patience is all you need! An agentic system for performing scientific literature review', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research <em>(Rating: 2)</em></li>
                <li>Chain-of-Verification Reduces Hallucination in Large Language Models <em>(Rating: 2)</em></li>
                <li>LongRAG: Enhancing retrieval-augmented generation with long-context llms <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>LAB-Bench: Measuring Capabilities of Language Models for Biology Research <em>(Rating: 1)</em></li>
                <li>The claude 3 model family: Opus, sonnet, haiku <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9624",
    "paper_id": "paper-277781732",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Patience agentic review system",
            "name_full": "Patience: An agentic system for performing scientific literature review",
            "brief_description": "An LLM-augmented, multi-agent pipeline that performs retrieval, LLM-driven query expansion, re-ranking, chunk-level summarisation, verification (CoVe) and iterative literature expansion to produce grounded long-form literature reviews and question answers from biomedical corpora.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Anthropic Claude 3.5 Sonnet",
            "model_description": "A commercially-available large language model from Anthropic used as the central reasoning and prompting engine across the pipeline (query expansion, proposed-answer generation for reranking, chunk summarisation, final answer synthesis, chain-of-thought and verification steps). The paper notes the Claude 3.5 Sonnet family supports very large contexts (example cited: 200K token context length).",
            "model_size": null,
            "input_corpus_description": "Indexed biomedical literature from PubMed (citations: titles and abstracts ~37 million) and the PubMed Central open access subset (full-text articles, ~5 million entries). Evaluation used subsets: LitQA2 questions mapped to PMC OA (103/199 available) and a 200-question subset from PubMedQA; additionally Nature review reference sets for CoVe evaluation.",
            "input_corpus_size": null,
            "topic_query_description": "Biology-related research questions drawn from published benchmarks (LitQA2, PubMedQA) and starting questions derived from Nature review abstracts; queries ranged from tightly-scoped multiple-choice style questions to open-ended literature-review prompts (e.g., the role of nervous system in cancer; integrating cryo-EM and cryo-ET).",
            "distillation_method": "A multi-stage agentic pipeline: (1) NER (HunFlair and TrendyGenes) over question to extract gene/disease identifiers; (2) LLM-generated 'must'/'should' search terms with synonyms and boost weights; (3) sparse retrieval against an Elasticsearch index (sparse BM25L-style searches; boosts for recency); (4) chunking of retrieved articles (≈10,000 words per chunk with ≈250-word overlap) and LLM generation of three diverse proposed answers + synonyms; (5) re-ranking of chunks using BM25L scored against the question and each proposed answer, combined with reciprocal rank fusion; (6) LLM-led information extraction: chunk-level summarisation, optional 3-way deduplication of summaries, then final answering synthesising retained summaries with chain-of-thought prompts; (7) Chain-of-Verification (CoVe) extension: plan ~10 verification questions from draft answers, retrieve evidence for those checks, independently fact-check and produce a verified final response; (8) iterative literature expansion by generating follow-up (expansive) questions from initial answers to discover additional relevant/contradictory papers.",
            "output_type": "Long-form literature reviews / grounded narrative answers with cited source attributions and lists of supporting references; intermediate outputs include chunk summaries, proposed answers (for reranking), verification Q&A pairs and expanded question lists.",
            "output_example": "Example (from reranking/proposed-answer stage): 'Kif9 knockdown zebrafish embryos likely display shorter cilia (synonyms: reduced-length cilia, truncated cilia, diminished cilia...) ... Without Kif9, transport of building materials to the ciliary tip may be impaired, resulting in shorter cilia due to insufficient elongation during ciliogenesis.' (one of three diverse, sometimes-contradictory proposed answers used to form BM25L queries and drive reranking).",
            "evaluation_method": "Quantitative retrieval and QA benchmarking: measured recall of benchmark source articles within top-N search results (top 200), measured position of key passages after re-ranking (top 30 chunks), and standard LitQA2 metrics for QA (Accuracy = correct/total, Coverage = attempted/total, Precision = correct/attempted). Additional evaluation: attribution rates (whether final answer was attributed to original source paper), comparisons to PaperQA2 on the same benchmarks, and CoVe evaluation as recall of references used by 10 Nature review articles (primary and secondary references).",
            "evaluation_results": "Key reported numbers: LLM keyword retrieval found 61.7% ± 1.2 of LitQA2 source articles in top-200 (on PMC OA subset); re-ranking retrieved key passages such that ~83% of key passages were within the first 30 chunks (vs ~900 chunks for baseline). Pipeline-level recall/retention cascade (Table 1): LLM keyword retrieval (top 200) 61.7% ±1.2; re-ranking (top 30 chunks) 57.3% ±1.9; LLM context summarisation & retention 57.0% ±2.4; attribution to answer 50.5% ±0.8. Overall the system achieved ~60.4% coverage (including use of other papers beyond benchmark source) and precision ≈90%; compared to PaperQA2 the system had similar source attribution (50.4% vs 48.2%), lower overall accuracy on LitQA2 (60.4% vs 66%) but higher precision (90% vs 85%). CoVe increased mean recall of primary/secondary references on Nature reviews by ~6%–25%.",
            "strengths": "Demonstrates that sparse retrieval with LLM-driven query expansion and BM25L re-ranking can approach state-of-the-art without dense embedding infrastructure; leverages LLMs for synonyms, query expansion, proposed-answer generation and verification to improve coverage; high precision and better grounding via verification (CoVe); transferable to corpora lacking citation networks; exploits large-context LLM capabilities to summarise full-text chunks; faster than human reviewers at single-paper extraction.",
            "limitations": "Primary bottleneck is initial retrieval (long tail of search hits); sparse retrieval still lacks semantic nuances and synonym handling without LLM expansion; finite context prevents including many full full-texts verbatim (requires chunking); system's coverage sensitive to the LLM's calibration/caution (Claude 3.5 Sonnet was more conservative than GPT-4o, reducing coverage); execution time (end-to-end) typically ~10–30 minutes; benchmarks (LitQA2) may not reflect real-world multi-paper synthesis needs and often assume a single source paper, which the system sometimes broke; potential for LLM to drop relevant papers during summarisation; risk of contradictory evidence requiring careful handling.",
            "failure_cases": "Reported failure modes include: initial retrieval missing the true source (major loss of recall occurs at retrieval stage), LLM summariser discarding relevant papers (lowering attribution/recall), proposed-answer generation producing contradictory candidate answers (intentionally to diversify queries) which can complicate synthesis, conservative LLM behaviour reducing coverage, and long-tail search hit distributions causing difficulty finding some relevant papers beyond early ranks.",
            "uuid": "e9624.0",
            "source_info": {
                "paper_title": "Patience is all you need! An agentic system for performing scientific literature review",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoVe (extended)",
            "name_full": "Chain-of-Verification (extended for verification and diversification)",
            "brief_description": "An extended Chain-of-Verification workflow that generates verification questions from a draft LLM answer, retrieves evidence for each verification question, independently fact-checks them, and synthesises a final verified response while also expanding the literature searched.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Anthropic Claude 3.5 Sonnet",
            "model_description": "Used to generate planning items (key statements), produce follow-up verification questions (~10 per draft), perform retrieval-guided fact checks, and synthesize final verified long-form responses; prompts engineered to use chain-of-thought and to expand literature coverage through questions generated during verification.",
            "model_size": null,
            "input_corpus_description": "Applied to retrieval over the same PubMed and PubMed Central OA indices used by the main pipeline; evaluated specifically against the reference lists of 10 recent Nature review articles (primary and secondary references).",
            "input_corpus_size": null,
            "topic_query_description": "Starting questions derived from review abstracts (examples include: integrating cryo-EM and cryo-ET; role of nervous system in cancer; others listed in Table 4). Verification questions are derived from the draft response and focus on validating ~10 key statements.",
            "distillation_method": "Draft-answer → planning (identify ~10 key factual statements) → convert statements into verification questions → retrieve evidence for each verification question → independently answer/verify those questions → combine verification answers and original to produce a final verified, diversified response. The process also collects additional references surfaced during the verification retrieval, increasing literature coverage.",
            "output_type": "Verified long-form responses, lists of verification Q&A pairs, expanded reference lists (primary and secondary), and follow-up questions for further literature exploration.",
            "output_example": "Example follow-up/verification questions (from Appendix B) for a cryo-EM integration question: 'What is the highest resolution achieved by single-particle cryo-EM for protein structures?'; 'Can cryo-ET provide high-resolution structural details comparable to single-particle cryo-EM?'; 'Are there limitations or potential artifacts in fitting high-resolution cryo-EM structures into lower-resolution cryo-ET maps?'.",
            "evaluation_method": "Measured recall of references from 10 Nature review articles, comparing references found by the draft response alone versus the draft+verification retrieval; counted primary and secondary references recovered within date cutoffs matching review publication.",
            "evaluation_results": "CoVe improved mean coverage of primary and secondary references on the evaluated Nature reviews by between ~6% and ~25% (exact per-review numbers reported in Table 3). The fact-checking stage also surfaced additional questions that expanded literature coverage beyond the original retrieval.",
            "strengths": "Improves literature coverage and helps identify contradictory evidence; produces intermediate verification artifacts that make final outputs more grounded; mitigates hallucination by explicit fact-checking steps; expands retrieval beyond citation-network navigation, helping find newer or less-cited work.",
            "limitations": "Additional computational and execution cost; effectiveness limited by initial retrieval quality (still reliant on the underlying search index); may retrieve many verification-targeted documents that are noisy or only tangentially relevant resulting in more downstream filtering; depends on LLM calibration for verification judgments.",
            "failure_cases": "Cases where CoVe fails include: verification retrieval still misses relevant primary evidence (if it's low-ranked), verification answers contradict each other when evidence is mixed or ambiguous, and expansion retrieves tangential or spurious references requiring human adjudication.",
            "uuid": "e9624.1",
            "source_info": {
                "paper_title": "Patience is all you need! An agentic system for performing scientific literature review",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "PaperQA2 (mentioned)",
            "name_full": "PaperQA / PaperQA2 (Retrieval-augmented generative agent for scientific research)",
            "brief_description": "A previously-published retrieval-augmented LLM agent for scientific research that the authors compare against; in the paper it is described as relying on Semantic Scholar sparse retrieval with a dense reranker and built on GPT-4o for some deployments.",
            "citation_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "mention_or_use": "mention",
            "model_name": "GPT-4o (reported in this paper's comparison)",
            "model_description": "In the paper's discussion PaperQA2 is characterised as using Semantic Scholar's sparse retrieval plus a dense reranker and being built on GPT-4o, leading to higher coverage but lower precision relative to the Patience system in their experiments.",
            "model_size": null,
            "input_corpus_description": "Not exhaustively detailed in this paper; PaperQA2 relies on external document stores (Semantic Scholar) and dense re-ranking infrastructure per the authors' description and cited reference.",
            "input_corpus_size": null,
            "topic_query_description": "PaperQA2 is used for literature question answering and long-form synthesis tasks across scientific domains (as cited by the authors).",
            "distillation_method": "Retrieval-augmented generation using external sparse retrieval (Semantic Scholar) and dense re-ranking (cross-encoders), with an LLM-based synthesis step.",
            "output_type": "Long-form answers and literature syntheses with attributed sources.",
            "output_example": "",
            "evaluation_method": "Reported results in the cited PaperQA2 work; in this paper PaperQA2 is compared on LitQA2-style benchmarks and retrieval/attribution metrics.",
            "evaluation_results": "In the authors' comparative summary: PaperQA2 had slightly lower attribution to original source for correct answers (48.2%) compared with Patience (50.4%), higher overall accuracy on LitQA2 (66% vs Patience's 60.4%), but lower precision (85% vs Patience's 90%). The paper attributes some PaperQA2 higher coverage to GPT-4o being less conservative about answering.",
            "strengths": "Higher coverage on benchmarks (less conservative answering), established use of dense re-ranking and external large-scale retrieval indices (Semantic Scholar) which can increase recall.",
            "limitations": "May be more prone to giving answers without sufficient grounded context (higher coverage but lower precision); requires dense retrieval and additional infrastructure (embeddings/indexing) that adds complexity and cost.",
            "failure_cases": "Reported tendency to give answers (increase coverage) even when appropriate supporting context is not present in the prompt, which can increase hallucination risk.",
            "uuid": "e9624.2",
            "source_info": {
                "paper_title": "Patience is all you need! An agentic system for performing scientific literature review",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PaperQA: Retrieval-Augmented Generative Agent for Scientific Research",
            "rating": 2,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "LongRAG: Enhancing retrieval-augmented generation with long-context llms",
            "rating": 2,
            "sanitized_title": "longrag_enhancing_retrievalaugmented_generation_with_longcontext_llms"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "LAB-Bench: Measuring Capabilities of Language Models for Biology Research",
            "rating": 1,
            "sanitized_title": "labbench_measuring_capabilities_of_language_models_for_biology_research"
        },
        {
            "paper_title": "The claude 3 model family: Opus, sonnet, haiku",
            "rating": 1,
            "sanitized_title": "the_claude_3_model_family_opus_sonnet_haiku"
        }
    ],
    "cost": 0.014551999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PATIENCE IS ALL YOU NEED! AN AGENTIC SYSTEM FOR PERFORMING SCIENTIFIC LITERATURE REVIEW A PREPRINT
April 15, 2025</p>
<p>David Brett david.brett@recursion.com 
Recursion Pharmaceutical
84101Salt Lake CityUTUSA</p>
<p>Anniek Myatt anniek.myatt@recursion.com 
Recursion Pharmaceutical
84101Salt Lake CityUTUSA</p>
<p>PATIENCE IS ALL YOU NEED! AN AGENTIC SYSTEM FOR PERFORMING SCIENTIFIC LITERATURE REVIEW A PREPRINT
April 15, 2025C5559E362EB0F07316F80B77044FD52AarXiv:2504.08752v1[cs.IR]
Large language models (LLMs) have grown in their usage to provide support for question answering across numerous disciplines.The models on their own have already shown promise for answering basic questions, however fail quickly where expert domain knowledge is required or the question is nuanced.Scientific research often involves searching for relevant literature, distilling pertinent information from that literature and analysing how the findings support or contradict one another.The information is often encapsulated in the full text body of research articles, rather than just in the abstracts.Statements within these articles frequently require the wider article context to be fully understood.We have built an LLM-based system that performs such search and distillation of information encapsulated in scientific literature, and we evaluate our keyword based search and information distillation system against a set of biology related questions from previously released literature benchmarks.We demonstrate sparse retrieval methods exhibit results close to state of the art without the need for dense retrieval, with its associated infrastructure and complexity overhead.We also show how to increase the coverage of relevant documents for literature review generation.</p>
<p>Introduction</p>
<p>The use of large language models (LLMs) has grown dramatically in recent years with foundation models displaying impressive capabilities across many applications [1,2], particularly in the context of agentic workflows for the orchestration of tools [3,4].One of these applications is to make use of LLM augmented agents for search and distillation of information for question answering (Q&amp;A) in the biomedical domain [5,6].The focus of building LLM applications for the biomedical domain is largely driven by free data availability via PubMed (titles and abstracts) and PubMed Central (full text journals), where as licensing terms for equivalent data sources for other domains, e.g.chemistry, are more restrictive.</p>
<p>Including explicit information from data sources such as scientific literature in the LLM context for answering expert-level questions is crucial to reduce the likelihood of getting wrong answers ("hallucinations"), particularly given a text extract (e.g PubMedQA [11]).Such benchmarks lack the scope to assess the retrieval capabilities of LLM augmented systems.The LitQA2 benchmark [12] has been developed to support evaluation of Q&amp;A on literature, testing both the retrieval and information extraction abilities of LLM augmented agents.It is still vulnerable to the issues presented by multiple-choice based benchmarks [13,14] and is necessarily a limited measure for the potential effectiveness of a system for real world applications where long form answers are commonly required.</p>
<p>Typically in scientific research questions tend to be open ended and reading a wide selection of literature as part of a review is more important than finding a specific piece of information contained in a single paper, which is what benchmarks such as PubMedQA and LitAQ2 are designed to evaluate.An inherent difficulty with considering the full text of articles in a benchmark is the licensing of those articles.For example not all the of papers used by LitQA2 are openly licensed for machine reading which technically prevents reproducing the results without paying for the papers.</p>
<p>Existing benchmarks do not attempt to consider the coverage of literature used to answer longer form answers/review generation.Typical review papers might have around 100 references as part of answering a question while benchmarks to date are typically answered with a single paper.In this paper we present a benchmark of retrieval coverage not just for a single paper but for the set used within actual review articles.</p>
<p>Agentic systems for retrieval</p>
<p>We define an agentic system as one which is made up of multiple LLM-augmented agents which are orchestrated together to perform a task.There are a number of scientific literature agentic systems for different tasks [15,10].</p>
<p>Many retrieval systems perform a two stage retrieval with an initial search and then re-ranking of the results.In both stages dense retrieval (in which the whole document is encoded as a dense vector) [16,17] and sparse retrieval (in which the vocabulary of the document is encoded) [18,19] might be used.</p>
<p>Considering the first stage of retrieval, a limitation of dense retrieval lies in its sensitivity to the chunk size of the embedded text as well as sensitivity to the choice of encoder.With respect to the former, the correlation of a mention of a term in one section of a paper with a mention in another section might be missed if the sections have been turned into separate text chunks and corresponding embedding vectors.Furthermore for large document corpus it can be expensive to index for dense retrieval.</p>
<p>Sparse retrieval on the other hand lacks the semantic meaning between terms in the vocab of a document.Furthermore it doesn't explicitly handle the issue of synonyms, which query expansion aims to address [20].Dense retrieval effectively performs this query expansion within in the embedding of the model used for encoding the text.</p>
<p>In our system we make use of the LLM to perform the query expansion not just in terms of adding synonyms but also in understanding what vocabulary terms might be necessary in a document to find the answer, rather than just expanding on the question.</p>
<p>Recent studies such as LongRAG [21] have demonstrated that the original techniques for retrieval augmented generation to chunk text into short segments for both retrieval and to provide as prompts for zero shot Q&amp;A [7] are often no longer necessary given the increasing context size of models (for example Claude 3.5 Sonnet has a 200K token context length [22]).Previously, handling long contexts was seen to be an issue for models such as GPT 3.5, which struggled with information retrieval from extended passages [23].Retrieval across long contexts have been demonstrated to be fairly reliable [24,22], however the context size is still finite so putting 20-30 complete full text scientific articles in verbatim is still not feasible.This motivates article chunking and re-ranking based on relevance to answering the question for our use case of literature review generation.</p>
<p>The re-ranking of results is typically done using dense retrieval with powerful techniques such as crossencoders [25,26] where a model encodes the semantic similarity of the question to the document.In our system we introduce a re-ranking based on sparse retrieval using BM25L [27] combining the ranks of multiple queries with expanded synonyms generated by an LLM.</p>
<p>In a further stage for retrieval in our system we let the LLM decide whether to keep or reject a document for use in answering the question, and also summarising the document in context.A similar approach to this is used by PaperQA2 [10].</p>
<p>Our agentic system for retrieval differs to many existing agentic systems in that it is solely based on sparse retrieval with query expansion performed by LLMs rather than making use of any dense retrieval techniques.This is chosen to enable better use of the large context sizes available in newer models [22].</p>
<p>In addition, we introduce a method to expand the relevant literature that is included in an LLM-generated review.This approach involves generating a series of expansive questions based on the initial answer from a retrieval-andanswering round.This helps to uncover more detailed information and identify potentially contradicting research, which is particularly important for literature review generation, where researchers synthesise a broad evidence base and contextualise findings.Often literature expansion has been done by navigating the citation network [15] however that can miss new papers and reinforces existing citation communities which might be guided by research group bias or narrow areas of research.</p>
<p>We introduce an agentic system for literature review generation and describe the specific techniques to select and process full-text literature articles to that aim.We present detailed analysis of how each component contributes to the overall capability of the agentic system using published benchmarks (LitAQ2 and PubMedQA) as well as our own benchmark on literature expansion.</p>
<p>Methodology</p>
<p>Our methodology comprises several key components: document retrieval, information extraction, and verification and diversification.Each component is designed to enhance the system's ability to process and analyse scientific literature effectively.For all descriptions in this methodology the LLM used is Anthropic's Claude 3.5 Sonnet [22].</p>
<p>Document retrieval</p>
<p>There are two primary sources of scientific articles used for this evaluation; PubMed [28] and PubMed Central open access subset [29].PubMed provides citations for ∼37 million for biomedical literature (for articles this typically consists of titles and abstract text).The PubMed Central open access contains ∼5 million full text biomedical literature entries which is available for machine reading.</p>
<p>To index the PubMed citations we perform named entity recognition (NER) for diseases and genes and index the titles and abstracts in an Elasticsearch index [30].Genes/gene products often have poor standardisation in naming across literature.Not only can gene names be ambiguous with other entities but also with other genes.We use a citation network based approach called TrendyGenes [31] that was developed to handle these ambiguities in the NER step.</p>
<p>To index the PubMed Central articles we perform NER using HunFlair [32,33] for genes and diseases.Hunflair provides the classification of spans from which we match the spans to the dictionaries used by TrendyGenes In order to retrieve documents to support asking a question we perform the following steps (also see Figure 1):</p>
<ol>
<li>[NER agent] Run HunFlair [32,33] NER to identify any gene and disease mentions in the question.</li>
</ol>
<p>[Search terms agent]</p>
<p>Use the LLM to generate a set of must and should keyword terms to search for matching documents.For should terms we give a boost measure of low, medium and high (see more in Appendix C).To increase robustness on how the user phrases a question, we generate a set of synonyms for each term which isn't an identifier found by the [NER agent].</p>
<p>[Document retrieval agent]</p>
<p>Using the generated search terms a search template is executed against the Elasticsearch index.This search template also boosts for more recent articles.</p>
<p>[Re-ranking agent]</p>
<p>To focus the LLM on more relevant text passages and reduce the amount of text that is included in the prompt, we first chunk each section of each retrieved article into chunks, preserving complete sentences with overlaps between chunks.Chunks are sized to ∼10000 words with ∼250 word overlaps between chunks.</p>
<p>The LLM is used to generate a set of three diverse proposed answers with rationale and expanded synonyms on keywords (see examples in Appendix D).We then re-rank the article chunks using BM25L [27] with the scores for each proposed answer and the original question we take the combined ranking using the reciprocal rank fusion method [34].The text is tokenized to lowercase, filtered for stop words and numbers, and stemmed using the Porter2 algorithm.This leads to article chunks with relevant information to answering the question being prioritised, which allows to reduce the number of article chunks to be carried forward into the information extraction stage.</p>
<p>Information extraction</p>
<p>There are three stages applied for the information extraction:</p>
<p>1.The first stage takes the text chunk retrieved for each article and produces a summary of the key facts related to the question.If there are no relevant facts the article chunk is rejected.</p>
<ol>
<li>
<p>The second optional stage creates a prompt which takes subsets of summarised articles (typically in sets of 3) to deduplicate facts across them and then removes any article chunks that provides no new information when compared with the other article chunks in the set.</p>
</li>
<li>
<p>The final stage is to pass the remaining summaries into the LLM context in order to get an answer to the question.</p>
</li>
</ol>
<p>In all these stages the prompts are engineered such that the chain of thought technique is used to improve the model output [35].</p>
<p>Verification and diversification</p>
<p>Chain-of-Verification [36] (CoVe) was originally developed for suppressing incorrect information through taking a draft response, planning and fact checking the draft independently and then giving a final verified response.We extend this framework using the flow in figure 1 for the draft response and independent fact checking.The intermediate responses are produced in a similar long form format to the original response.The planning stage identifies ∼10 key statements in the original response, which are converted into questions to verify their validity.The generated answers, the verification of the original statements and the original response go into generating the final response.We find that the additional questions that are generated in the fact checking stage expand the literature coverage beyond that which was encountered in the original question.</p>
<p>3 Results</p>
<p>Document retrieval</p>
<p>We evaluate the document retrieval by determining where in the search ranking the source documents for the benchmarks appear.For evaluating the retrieval on full text articles in the PubMed Central Open Access dataset we can only consider a subset of the LitQA2 benchmark [12] (for which only 103 out of 199 are found in the PubMed Central OA subset).Similarly for evaluating retrieval on PubMed we take the first 200 labelled questions from the PubMedQA benchmark [11] to see where their sources appear in the search ranking.We only consider up to the 200 th ranked search result for both sources in this evaluation.We compare our approach to a baseline using a simpler 'More Like This' query, which leverages TF-IDF to measure document similarity.For LitQA2, we include answer options in the query.In the PubMedQA case we only evaluated a small subset of the test set as it became evident there was little gain in using the LLM keyword approach versus the baseline approach.This is not very surprising given the questions are directly generated from the titles of the articles [11].Figure 2 shows that for the LLM generated keyword approach LitQA2 benchmark 63.6±1.2 source articles were found in the top 200 search hits out of the possible 103 compared with 52 for the simpler approach.For PubMedQA only 154 source articles were found out of the 200 questions tested.The top 200 was chosen as a cutoff for performance reasons, to be able to do the re-ranking in memory.</p>
<p>Re-ranking</p>
<p>For the re-ranking we create a baseline by taking every section in all 200 full-text articles for a given question, as found in figure 2, and then chunk each section as described in 2. In the LitQA2 benchmark [12] there is a key passage of text assigned to each question which is required to answer it.We find the position in the ranking of chunks at which the key passage appears.We contrast this with re-ranking each section across all publications in figure 3. It can be seen that within the first 30 chunks ∼83% of the key passages can be found with the re-ranking, this is in contrast to ∼900 chunks for the baseline.</p>
<p>Information extraction</p>
<p>For the LitQA2 benchmark [12] we consider the split between question sources which are available/not available for machine reading within the PubMed Central OA  [29].In the assessment of the benchmark we follow the same conventions as in the original paper [12]:</p>
<p>• Accuracy = Total correct answers / Total questions • Coverage = Total attempted answers / Total questions • Precision = Total correct answers / Total attempted answers</p>
<p>To get a better understanding of the LLM's comprehension and potential information loss we evaluate how good the model is at answering the question given the source, by just feeding the original article source (table 2).We provide in the LLM context different levels of granularity: the key passage given by the benchmark, the section of the article which contains the key passage, the full article and a context specific summarisation of the full article.</p>
<p>According to the LitQA2 benchmark paper the precision on these all exceed human levels, a finding which we recapitulate here.</p>
<p>Question answering</p>
<p>The LitQA2 benchmark [12] assumes that only the source paper contains the information to achieve the correct answer.We find that this assumption does not hold over our text corpus.</p>
<p>The recall on the source paper for the open access subset is provided in Table 1, from which we can conclude that the highest loss in recall occurs in the initial paper retrieval stage.The LLM context summarisation and retention step allows for the LLM to choose to drop what it considers are irrelevant papers.The final answering stage allows the LLM to attribute the answer to a single paper and this is what leads to our 50.5% final recall.In the table the mean and 95% confidence intervals of the results of 3 repeats are provided.In our evaluated subset, ∼10 answers are correct with different paper attributions, suggesting that the assumption of specificity for the benchmark question might not be correct.</p>
<p>Table 2 provides the evaluation for our agentic literature review system on the LitQA2 benchmark.The assumption of a single source paper containing the answer to a benchmark question equates to a maximum potential accuracy of ∼50% based on the open subset of the benchmark, assuming that the top 30 re-ranked chunks all equate to correct answers.</p>
<p>The coverage is higher than that attributable to the source papers, however additional papers which allowed the LLM to answer the question increased the coverage in total 60.4±0.6%questions with other papers used to answer the questions.Table 3: Recall of references with CoVe</p>
<p>Verification and diversification</p>
<p>In order to benchmark this capability we consider 10 recent review articles published in Nature and the references used with in them.For each review we get an LLM to generate the key question being asked from the abstract of the paper and use that as our starting point (see more in Appendix B).</p>
<p>We perform the retrieval with the date cutoffs set as the publication date of the reviews.The benchmark is then measured as the recall on the references on each paper.We consider the references which either come out of the draft response generation or in the fact checking retrieval process.We consider both the direct references used in the reviews and the secondary references (from which the primary research might come).Table 3 shows that the mean coverage of primary and secondary references on the Nature review articles improves with 6% to 25% with the CoVe approach.</p>
<p>Discussion</p>
<p>We have demonstrated that when compared with the Pa-perQA2 paper [10] we achieved a slightly higher attribution to original source for correct answers of 50.4% compared with 48.2%.When we consider the accuracy of answering the questions overall we achieved 60.4% compared with 66%.However, comparing for precision we exceed PaperQA2 achieving 90% compared with 85%.</p>
<p>In the LAB-Bench [12] paper it can be seen that in answering the benchmark Claude 3.5 Sonnet is significantly more cautious at choosing whether to give an answer compared to GPT-4o (0.12 vs 0.58 coverage of the LitQA2 benchmark).We hypothesise this baseline caution leads to our observed reduced coverage compared with PaperQA2, which is built on GPT-4o, this in part leads to our reduced accuracy overall, however, our precision comes out slightly higher.It should be possible to force the model to take a guess to drive up coverage, but as the retrieval and attribution of our system is similar to that of LitQA2, this would mean the model would make a guess that isn't based on having appropriate information in the context.Given that there is a 25% chance of picking the correct answer in a multiple-choice test, we would be gaming the system rather than testing the system's capability of answering questions based on information from the literature.Therefore, the key area for improvement on the benchmark lies in the initial retrieval stage, both for our system as well as Pa-perQA2 (We make further analysis of this in appendix A).</p>
<p>The LitQA2 benchmark was designed for answering very specific questions with multiple-choice answers, whereas our literature review system was designed to summarise and contextualise a broad literature base around a given research question.Generating meaningful benchmarks is difficult, as was also shown in our results where the benchmark assumption of a question only being answerable from a single paper was broken several times.This also leads to the concern that if more than one paper can provide a relevant answer, there could be papers with contradictory claims that affect the answer.Our system has been designed to include a verification and diversification step to handle such situation, by including a wider range of relevant papers for question answering.Furthermore with the query expansion and synonym handling (particularly of genes/proteins) provided in our methods these are not particularly demonstrated well in the benchmark.The benchmark questions rarely deviate from the keywords used in the passages to generate them such that the retrieval systems are not particularly pushed hard to resolve semantically similar terms.Despite benchmarks necessarily being limited compared to real-world applications, they serve a useful purpose for enabling comparisons and improving methods.</p>
<p>While our methodology does not exceed the accuracy measurement of PaperQA2 [10] it does demonstrate a technique which gets similar levels of attribution and retrieval without the need to exploit the citation network, and without the use of dense retrieval methods.This makes the technique transferable to document stores where this information is not available.</p>
<p>For current agentic systems producing a well-referenced review article there is a strong trade-off between execution time and the quantity of literature processed in such systems.The strength of LLMs to extract information from papers as displayed in table 2 is already very good and for a single paper faster than a typical human.Being able to benchmark the quality of a review for an area of literature is difficult as the generated text is often far larger than you might generate for bench-marking summarisation tasks and the quality is also far more based on human opinion.Furthermore, references used in a review article will be biased to the authors knowledge and affiliations which an agentic system like ours will not be able to reproduce.Even so we were able to demonstrate significant improvement to the literature coverage by use of the CoVe technique.</p>
<p>In reality the end-to-end time to perform such agentic literature reviews typically takes ∼10-30 minutes.This is significantly slower than many of the commercial chat apps (such as ChatGPT) however the quality and grounding of responses (as qualitatively accessed by users) is often a lot better.When compared with asking a human to perform a literature review our agentic literature review system is significantly faster.</p>
<p>Conclusion</p>
<p>We developed a methodology for answering questions from scientific literature and validated the retrieval and answering capabilities of these methods against the LitQA2 benchmark.We have further demonstrated a method for exploring a wider literature space when generating scientific literature reviews using CoVe.We validated this approach by assessing the coverage of relevant literature.Scaling these methods allows for scientists to explore more literature in a shorter amount of time than would be possible previously.</p>
<p>A Further benchmark analysis</p>
<p>In both PaperQA2 and our methods we demonstrate similar initial retrieval.In figure 4 we explore the search rank up to the first 4000 entries.It can be seen that beyond the first 100 entries there is a very long tail.</p>
<p>In figure 5 the aggregated distribution of total search hits based on the generated keyword search are shown for 3 repeats of the LitQA2 benchmark (considering only the available papers).It can be seen that the distribution has a long tail going out into millions of hits.Given both PaperQA2 (which relies on Semantic Scholar's sparse retrieval with a dense reranker) and our sparse retrieval techniques exhibit similar initial retrieval it would suggest that the retrieval issues don't reside solely in the methods alone but potentially the specificity of the questions themselves.Total search hits</p>
<p>B Verification and diversification analysis</p>
<p>For the analysis of the CoVe we use a set of recent Nature reviews to formulate a set of starting questions.The derived starting questions can be seen in table 4.</p>
<p>In the CoVe planning step we get the LLM to propose a series of follow up questions based on the initial answer.For the question How can the integration of cryo-EM and cryoelectron tomography bridge the gap between structural and cell biology to provide a comprehensive understanding of macromolecular interactions within their native cellular context? a subset of the resulting questions are:</p>
<p>• What is the highest resolution achieved by single-particle cryo-EM for protein structures?</p>
<p>• Can cryo-ET provide high-resolution structural details comparable to single-particle cryo-EM?</p>
<p>• What specific cellular interactions or structures can be observed using integrated cryo-EM and cryo-ET that cannot be seen with traditional structural biology methods?</p>
<p>• Are there any limitations or potential artifacts in fitting highresolution cryo-EM structures into lower-resolution cryo-ET maps?</p>
<p>• Can you provide specific examples of transient cellular processes or interactions that have been uniquely revealed by cryo-EM or cryo-ET?</p>
<p>• What is the typical thickness of lamellae prepared by FIB milling for cryo-ET, and how does this compare to the maximum sample thickness for direct cryo-ET imaging?</p>
<p>• What is the highest resolution achieved to date using subtomogram averaging in cryo-ET, and how does it compare to single-particle cryo-EM?</p>
<p>• How does the structural information obtained from the cry-oWriter system compare to that from traditional cryo-EM of purified complexes in terms of resolution and native state preservation?</p>
<p>These questions are nuanced as they are based on information retrieved from the initial round of retrieval and summarisation.</p>
<p>C Search term query expansion</p>
<p>In the search term query expansion we ask the LLM to map to known identifiers found through NER and LLM identified keywords, for which it generates synonyms.</p>
<p>D Initial answers for reranking</p>
<p>In order to perform re-ranking in addition to the question we use 3 plausible answers making use of the LLMs own memory to get sample text in a form which is similar to what a long form answer might look like (even if not correct).These are then used to query the BM25L index.The outputs also include synonyms of key terms to try to overcome some of the semantic short comings typically associated to sparse retrieval.</p>
<p>For example, for the question Relative to wild type zebrafish embryos, those with Kif9 knocked down display what ciliary phenotype?the following answers are given: Kif9 knockdown zebrafish embryos likely display shorter cilia (synonyms/abbreviations: reduced-length cilia, truncated cilia, diminished cilia, abbreviated cilia, stunted cilia) as the ciliary phenotype.Kif9 is a kinesin motor protein (synonyms/abbreviations: molecular motor, motor enzyme, KIF, kinesin-like protein, microtubule-associated motor) that plays a role in intraflagellar transport [synonyms/abbreviations: IFT, ciliary transport, flagellar transport, cilia protein trafficking, axonemal transport].Without Kif9, the transport of building materials to the ciliary tip may be impaired, resulting in shorter cilia due to insufficient elongation during ciliogenesis (synonyms/abbreviations: cilia formation, ciliary assembly, cilia biogenesis, cilia growth, flagella formation).</p>
<p>The ciliary phenotype observed in Kif9 knockdown zebrafish embryos is likely increased beat frequency (synonyms/abbreviations: accelerated ciliary motion, enhanced ciliary beating, rapid cilia oscillation, heightened ciliary activity, faster cilia movement).Kif9 may function as a regulatory protein [synonyms/abbreviations: modulator, control factor, regulator molecule, governing protein, homeostatic agent] that normally dampens ciliary beating.In its absence, the cilia might lack proper regulation, leading to increased beat frequency.This could result in altered fluid flow (synonyms/abbreviations: hydrodynamic changes, fluid dynamics alteration, modified liquid current, shifted fluid movement, disrupted flow patterns) in the zebrafish embryo, potentially affecting development.</p>
<p>Zebrafish embryos with Kif9 knocked down are likely to display wider cilia (synonyms/abbreviations: thicker cilia, enlarged cilia diameter, expanded cilia width, broadened cilia, increased cilia girth) as their ciliary phenotype.Kif9 might be involved in maintaining the structural integrity (synonyms/abbreviations: architectural stability, structural soundness, physical cohesion, conformational robustness, morphological consistency) of the ciliary axoneme (synonyms/abbreviations: ciliary core, central ciliary structure, microtubule scaffold, axonemal structure, ciliary skeleton).Without Kif9, the tight organization of the axonemal microtubules (synonyms/abbreviations: ciliary microtubules, axonemal MTs, cilia structural filaments, flagellar microtubules, cytoskeletal tubules) may be compromised, leading to a wider, less organized ciliary structure.These answers are obviously contradictory but identify the type of key terms which we might expect to see in the initial retrieved literature.</p>
<p>Figure 1 :
1
Figure 1: Data flow to support Q&amp;A</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Search rank with search term agent for benchmark sources in PubMed and PubMed Central (* with 95% CI)</p>
<p>Figure 4 :
4
Figure 4: Search rank with search term agent for benchmark sources in PubMed Central (* with 95% CI)</p>
<p>Figure 5 :
5
Figure 5: Total potential search hits across questions in LitQA2 with search term agent</p>
<p>For</p>
<p>example, for the question Relative to wild type zebrafish embryos, those with Kif9 knocked down display what ciliary phenotype?the following terms are generated: &lt; t e r m s &gt; &lt; must &gt; &lt; t e r m &gt; z e b r a f i s h &lt; / t e r m &gt; &lt; t e r m &gt; Danio r e r i o &lt; / t e r m &gt; &lt; / must &gt; &lt; must &gt; &lt; t e r m &gt; c i l i a &lt; / t e r m &gt; &lt; t e r m &gt; c i l i a r y &lt; / t e r m &gt; &lt; / must &gt; &lt; must &gt; &lt; t e r m &gt;ENSEMBL:ENSG00000088727&lt; / t e r m &gt; &lt; / must &gt; &lt; s h o u l d &gt; &lt; t e r m &gt;embryo &lt; / t e r m &gt; &lt; t e r m &gt; e m b r y o n i c &lt; / t e r m &gt; &lt; b o o s t &gt;MEDIUM&lt; / b o o s t &gt; &lt; / s h o u l d &gt; &lt; s h o u l d &gt; &lt; t e r m &gt;knockdown&lt; / t e r m &gt; &lt; t e r m &gt; k n o c k e d down&lt; / t e r m &gt; &lt; t e r m &gt; s i l e n c i n g &lt; / t e r m &gt; &lt; t e r m &gt; g e n e s i l e n c i n g &lt; / t e r m &gt; &lt; b o o s t &gt;HIGH&lt; / b o o s t &gt; &lt; / s h o u l d &gt; &lt; s h o u l d &gt; &lt; t e r m &gt; p h e n o t y p e &lt; / t e r m &gt; &lt; t e r m &gt; m o r p h o l o g y &lt; / t e r m &gt; &lt; b o o s t &gt;LOW&lt; / b o o s t &gt; &lt; / s h o u l d &gt; &lt; s h o u l d &gt; &lt; t e r m &gt; w i l d t y p e &lt; / t e r m &gt; &lt; t e r m &gt;WT&lt; / t e r m &gt; &lt; b o o s t &gt;MEDIUM&lt; / b o o s t &gt; &lt; / s h o u l d &gt; &lt; / t e r m s &gt; where each <should> and <must> are the terms with their synonyms or the term matched by the NER.The XML structure uses <must> tags for required terms and <should> tags for optional terms that may improve search results.The <boost> values indicate a weighting to apply to should terms in the search scoring.</p>
<p>Table 1 :
1
LitQA2 source article recall dataset
StepRecall of source article (% ± 95% CI)LLM Keyword retrieval (top 200)61.7 ± 1.2Re-ranking (top 30 chunks)57.3 ± 1.9LLM context summarisation and retention57.0 ± 2.4Attribution to answer50.5 ± 0.8
What is the role of the nervous system in cancer development, progression, and metastasis, and how does cancer interact with and affect the nervous system?37138108 Reappraising the palaeobiology of Australopithecus.What is the significance of the genus Australopithecus in human evolution, and how has our understanding of its characteristics and role changed since its initial discovery?37100941 Computational approaches streamlining drug discovery.How are recent advances in computational technologies, particularly in virtual screening and deep learning, reshaping the drug discovery process, and what are the challenges and opportunities associated with these developments?38750233 Decoding the interplay between genetic and non-genetic drivers of metastasis.How do genetic and non-genetic mechanisms contribute to the acquisition of metastatic competencies in cancer cells, and how can new technologies help us better understand these processes?38570716 Bridging structural and cell biology with cryo-electron microscopy.How can the integration of cryo-EM and cryo-electron tomography bridge the gap between structural and cell biology to provide a comprehensive understanding of macromolecular interactions within their native cellular context?38418916 Ion and lipid orchestration of secondary active transport.How do secondary active transporters utilize ion gradients and lipid properties to facilitate the movement of small molecules across cell membranes, and what mechanisms enable their regulation and adaptation to different physiological needs?38326590 A break in mitochondrial endosymbiosis as a basis for inflammatory diseases.How does the breakdown of the endosymbiotic relationship between mitochondria and host cells contribute to cellular signaling and the development of inflammatory and autoimmune diseases?
. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S V Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, E Brynjolfsson, S Buch, D Card, R Castellon, N Chatterji, A Chen, K Creel, J Q Davis, D Demszky, C Donahue, M Doumbouya, E Durmus, S Ermon, J Etchemendy, K Ethayarajh, L Fei-Fei, C Finn, T Gale, L Gillespie, K Goel, N Goodman, S Grossman, N Guha, T Hashimoto, P Henderson, J Hewitt, D E Ho, J Hong, K Hsu, J Huang, T Icard, S Jain, D Jurafsky, P Kalluri, S Karamcheti, G Keeling, F Khani, O Khattab, P W Koh, M Krass, R Krishna, R Kuditipudi, A Kumar, F Ladhak, M Lee, T Lee, J Leskovec, I Levent, X L Li, X Li, T Ma, A Malik, C D Manning, S Mirchandani, E Mitchell, Z Munyikwa, S Nair, A Narayan, D Narayanan, B Newman, A Nie, J C Niebles, H Nilforoshan, J Nyarko, G Ogut, L Orr, I Papadimitriou, J S Park, C Piech, E Portelance, C Potts, A Raghunathan, R Reich, H Ren, F Rong, Y Roohani, C Ruiz, J Ryan, C Ré, D Sadigh, S Sagawa, K Santhanam, A Shih, K Srinivasan, A Tamkin, R Taori, A W Thomas, F Tramèr, R E Wang, W Wang, B Wu, J Wu, Y Wu, S M Xie, M Yasunaga, J You, M Zaharia, M Zhang, T Zhang, X Zhang, Y Zhang, L Zheng, K Zhou, P Liang, 2021On the Opportunities and Risks of Foundation Models," arXiv</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023arXiv</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 62479922023</p>
<p>Chemcrow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 2023</p>
<p>Paperqa: Retrievalaugmented generative agent for scientific research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, arXiv:2312.075592023arXiv preprint</p>
<p>BioRAG: A RAG-LLM Framework for Biological Question Reasoning. C Wang, Q Long, M Xiao, X Cai, C Wu, Z Meng, X Wang, Y Zhou, 2024arXiv</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, S Riedel, D Kiela, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Retrievalaugmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, M Wang, H Wang, 2024</p>
<p>PaperQA: Retrieval-Augmented Generative Agent for Scientific Research. J Lála, O O'donoghue, A Shtedritski, S Cox, S G Rodriques, A D White, 2023arXiv</p>
<p>M D Skarlinski, S Cox, J M Laurent, J D Braza, M Hinks, M J Hammerling, M Ponnapati, S G Rodriques, A D White, LANGUAGE AGENTS ACHIEVE SUPERHUMAN SYNTHESIS OF SCI-ENTIFIC KNOWLEDGE. 92024</p>
<p>PubMedQA: A Dataset for Biomedical Research Question Answering. Q Jin, B Dhingra, Z Liu, W W Cohen, X Lu, 2019arXiv</p>
<p>J M Laurent, J D Janizek, M Ruzo, M M Hinks, M J Hammerling, S Narayanan, M Ponnapati, A D White, S G Rodriques, LAB-Bench: Measuring Capabilities of Language Models for Biology Research. 2024arXiv</p>
<p>Can multiple-choice questions really be useful in detecting the abilities of LLMs?. W Li, L Li, T Xiang, X Liu, W Deng, N Garcia, 2024arXiv</p>
<p>A Myrzakhan, S M Bsharat, Z Shen, Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena. 2024arXiv</p>
<p>AI and Generative AI for Research Discovery and Summarization. M Glickman, Y Zhang, Harvard Data Science Review. 6apr 30 2024</p>
<p>. A Neelakantan, T Xu, R Puri, A Radford, J M Han, J Tworek, Q Yuan, N Tezak, J W Kim, C Hallacy, J Heidecke, P Shyam, B Power, T E Nekoul, G Sastry, G Krueger, D Schnurr, F P Such, K Hsu, M Thompson, T Khan, T Sherbakov, J Jang, P Welinder, L Weng, 2022Text and code embeddings by contrastive pre-training</p>
<p>. M Douze, A Guzhva, C Deng, J Johnson, G Szilvasy, P.-E Mazaré, M Lomeli, L Hosseini, H Jégou, The faiss library," 2024</p>
<p>The probabilistic relevance framework: Bm25 and beyond. S Robertson, H Zaragoza, Found. Trends Inf. Retr. 3Apr. 2009</p>
<p>Splade: Sparse lexical and expansion model for first stage ranking. T Formal, B Piwowarski, S Clinchant, 2021</p>
<p>Study of query expansion techniques and their application in the biomedical information retrieval. A R Rivas, E L Iglesias, L Borrajo, The Scientific World Journal. 201411321582014</p>
<p>Longrag: Enhancing retrieval-augmented generation with long-context llms. Z Jiang, X Ma, W Chen, 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, Transactions of the Association for Computational Linguistics. 202412</p>
<p>I pressure tested gpt-4's 128k context retrieval. G Kamradt, November 2023</p>
<p>Colbert: Using bert sentence embedding in parallel neural networks for computational humor. I Annamoradnejad, G Zoghi, Expert Systems with Applications. 2024249123685</p>
<p>Retrieve to explain: Evidence-driven predictions with language models. R Patel, A Brayne, R Hintzen, D Jaroslawicz, G Neculae, D Corneil, 2024</p>
<p>When documents are very long, bm25 fails!. Y Lv, C Zhai, Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '11. the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '11New York, NY, USAAssociation for Computing Machinery2011</p>
<p>National Library of Medicine. ( Bethesda, Md), August 2024internet</p>
<p>National Library of Medicine. ( Bethesda, Md), August 2024PMC Open Access Subset. internet</p>
<p>. N V Elastic, Elasticsearch, 2024/08/20</p>
<p>TrendyGenes, a computational pipeline for the detection of literature trends in academia and drug discovery. G S Nájera, D N Carlón, D J Crowther, Scientific Reports. 111157472021</p>
<p>HunFlair: an easy-touse tool for state-of-the-art biomedical named entity recognition. L Weber, M Sänger, J Münchmeyer, M Habibi, U Leser, A Akbik, Bioinformatics. 37172021</p>
<p>Hun-Flair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools. M Sänger, S Garda, X D Wang, L Weber-Genzel, P Droop, B Fuchs, A Akbik, U Leser, 2024arXiv</p>
<p>Reciprocal rank fusion outperforms condorcet and individual rank learning methods. G V Cormack, C L A Clarke, S Buettcher, Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09. the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09New York, NY, USAAssociation for Computing Machinery2009</p>
<p>Prompt engineering guidance for claude. Anthropic, August 2024</p>
<p>Chain-of-Verification Reduces Hallucination in Large Language Models. S Dhuliawala, M Komeili, J Xu, R Raileanu, X Li, A Celikyilmaz, J Weston, 2023arXiv</p>            </div>
        </div>

    </div>
</body>
</html>
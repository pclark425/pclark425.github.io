<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7749 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7749</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7749</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-59347f86ce9af155266729e4b0301a29c65abf88</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/59347f86ce9af155266729e4b0301a29c65abf88" target="_blank">Approaching Human-Level Forecasting with Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work develops a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions, and suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.</p>
                <p><strong>Paper Abstract:</strong> Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7749.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7749.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented LM forecasting system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented Language Model Forecasting System (retrieval + reasoning + ensembling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end system that (1) uses LMs to generate search queries, retrieve and summarize news articles, (2) prompts LMs with scratchpads to produce probability forecasts and reasonings, and (3) ensembles multiple LM outputs (trimmed mean) into a final calibrated probability for binary real-world event questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>System (GPT-4-1106-Preview base + fine-tuned GPT-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining retrieval (LM-generated queries, relevancy rating, summarization), reasoning (scratchpad prompts eliciting arguments for/against and calibration checks), and ensembling (trimmed-mean of multiple LM outputs). Fine-tuning: self-supervised selection of model-generated reasonings that outperform crowd forecasts to teach the model effective reasoning strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct numeric probability output from LMs via prompting (explicit instruction to output a number in [0,1]); multiple stochastic/alternative prompts sampled (including temperature T=0.5 for fine-tuned model), then ensembled via trimmed mean; fine-tuning on model outputs that beat crowd (with clipping/averaging with crowd when deviation >0.15) and prompting the model to check for over/underconfidence (historical base-rates). No external post-hoc calibration (binning/isotonic) improved results.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary real-world event outcomes (forecasting questions sourced from forecasting platforms; e.g., whether an event occurs by a given date).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multi-domain (Science & Tech, Healthcare & Biology, Economics & Business, Politics, Security, Sports, etc.); includes a 'Science & Tech' subset but not limited to scientific-discovery-only questions.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Curated dataset compiled from five forecasting platforms (Metaculus, Good Judgment Open, INFER, Polymarket, Manifold): raw 48,754 questions and 7,174,607 user forecasts (2015–2024); curated binary subset: 5,516 binary questions (3,762 train / 840 val / 914 test). Test set restricted to questions opened after model knowledge cutoffs (post-June 1, 2023/2024 depending on model) to avoid pretraining leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Short-to-medium horizons typical of platform questions: average question window ≈70 days; average time to resolution ≈42 days; evaluation uses up to 5 simulated retrieval dates (geometrically spaced between open and close dates).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Primary: Brier score ((forecast - outcome)^2). Secondary: accuracy (binary threshold), RMS calibration error. Standard errors reported (noting SE likely underestimated due to time-series dependence).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>End-to-end system: average Brier score = 0.179 (human crowd aggregate = 0.149); accuracy = 71.5% (crowd = 77.0%). System outperforms best baseline LM (GPT-4-1106-Preview zero-shot Brier = 0.208). Ablations: without fine-tuning Brier = 0.186; without fine-tuning and no retrieval Brier = 0.206. Selective forecasting under certain heuristics can match or beat the crowd in subsets (e.g., when crowd uncertain). Combining system + crowd (weighted) improves crowd Brier from 0.149 to 0.146.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors report the optimized system is 'naturally well calibrated' (RMS calibration error on test ≈ 0.42; human crowd ≈ 0.38). Base models in zero-shot were less calibrated; fine-tuning and ensembling improved calibration without explicit calibration post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Human crowd aggregate (platform aggregates), random/unskilled baseline (0.5 forecast → Brier 0.25), zero-shot and scratchpad prompting across 14 LMs (e.g., GPT-3.5, GPT-4, Claude-2.1, Llama-2 variants, Mistral variants), ablated systems (no retrieval, no fine-tuning), and different ensembling strategies (mean, median, geometric mean, trimmed mean - trimmed mean chosen).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reported limitations include: (1) models hedge and tend to be underconfident (especially low-probability outputs) possibly due to safety training; (2) performance depends strongly on retrieval quality and number of relevant articles; (3) many questions have short horizons or resolve early creating uneven retrieval dates; (4) dataset composition bias (over 80% of recent questions from Manifold and Polymarket) and curation choices; (5) evaluation is retrospective simulation (using historical news up to retrieval date) rather than truly prospective forecasting; (6) fine-tuning selection criterion (keeping only model outputs that beat crowd) risks overconfidence, mitigated by filtering and averaging with crowd; (7) no per-event examples of scientific-discovery forecasts were given, and model knowledge cutoffs constrain which events are truly out-of-sample.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Approaching Human-Level Forecasting with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7749.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7749.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-1106-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-1106-Preview (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art instruction-tuned large language model used as the primary base reasoning model in the system; used with scratchpad prompts to elicit probabilistic forecasts and reasonings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-1106-Preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned transformer LLM (proprietary OpenAI model) used in prompting mode with scratchpads; provides deterministic prompts and outputs numeric probabilities when instructed. Employed for search-query generation, relevance scoring (some steps used GPT-3.5 for cost reasons), and as one of the reasoning models producing forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Prompted direct numeric probability outputs (explicit instruction to return a number between 0 and 1); multiple scratchpad prompts sampled to produce multiple candidate forecasts; ensemble (trimmed mean) across prompts and model samples. No external temperature scaling reported for this base model in the main system (fine-tuned model uses T=0.5 for sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary forecasting questions from platforms (multi-domain).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multi-domain including Science & Tech subset.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on the curated forecasting dataset from Metaculus, GJOpen, INFER, Polymarket, Manifold (test set: 914 binary questions post-cutoff).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Same as system: average question window ≈70 days; 5 retrieval dates geometrically spaced between open and close dates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (primary), accuracy (secondary), calibration plots and RMS calibration error.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Baseline (scratchpad or zero-shot best) for GPT-4-1106-Preview: Brier score ≈ 0.208 (Table 7 shows best zero-shot/scratchpad for the GPT-4 family at ~0.208–0.209). As part of the final system (combined with fine-tuned model and ensembling) it contributes to the system Brier score of 0.179. Ablation with only base GPT-4 (no fine-tuning) yields Brier ≈ 0.186; no IR yields ≈0.206.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Base GPT-4 in zero-shot/scratchpad was less well-calibrated (Figure 3a); within the optimized system and when ensembled/fine-tuned, calibration improves considerably.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against other LMs in baseline table (GPT-3.5, Claude-2.1, Llama-2 variants, Mistral, Gemini-Pro), and against human crowd aggregate and random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pretraining knowledge cutoff (April 2023 for GPT-4-1106-Preview) so cannot know events after cut-off; requires retrieval to reach competitive performance; scratchpad prompts sometimes trigger refusal due to safety training; not fine-tuned in some ablations so less calibrated.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Approaching Human-Level Forecasting with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7749.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7749.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-4-0613 reasoning model (fine-tuned on selected high-performing model reasonings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4 variant fine-tuned in a self-supervised way on model-generated reasonings and probability outputs that outperformed the crowd on the training set, used to produce additional forecasts without scratchpad instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned GPT-4-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned version of GPT-4 (0613 checkpoint) trained on 6,000 most recent selected reasoning–prediction pairs (from a pool of 13,253 candidates) where model outputs beat crowd forecasts; fine-tuning teaches which reasoning method to apply in context and aims to improve calibration and accuracy of probability estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Direct numeric probability outputs; fine-tuned to produce reasonings that lead to accurate probabilities; sampling with temperature T=0.5 for generating multiple forecasts; ensemble with base GPT-4 via trimmed mean. During fine-tuning, targets were averaged with crowd if model deviated >0.15 to mitigate overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary forecasting questions from the curated dataset (as above).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multi-domain including Science & Tech, Healthcare & Biology, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Fine-tuned on selected model outputs from the training split of the curated dataset (6,000 most recent points used for fine-tuning; initial generated pool 73,632, 13,253 selected meeting criteria). Evaluated on the same curated test set (914 binary questions).</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Same dataset horizons (average question window ≈70 days; average resolution ≈42 days); fine-tuning data generated at multiple retrieval dates per question (n=5 retrieval dates, subject to question resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Brier score (primary), accuracy, RMS calibration error.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Fine-tuned GPT-4-0613 used alone: Brier score ≈ 0.182 (Table 5). When combined in the full system with base GPT-4-1106-Preview and ensembling, contributes to final system Brier = 0.179. Fine-tuned GPT-3.5 variants produced similar, slightly worse results (~0.182–0.183).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Fine-tuning and ensembling improved calibration relative to base zero-shot models; overall system RMS calibration error ≈ 0.42 (human crowd 0.38). Authors note standard calibration methods (binning/isotonic) did not improve performance further.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to base GPT-4 (no fine-tuning), fine-tuned GPT-3.5, and other LMs; ablation studies show fine-tuning yields modest but meaningful improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Fine-tuning data selection depended on filtering for outputs that beat the crowd (risk of selection bias/overfitting); budget constraints limited fine-tuning to 6,000 points; potential for overconfidence mitigated by averaging with crowd when deviations large; still hedges on some high-certainty questions due to safety training and retention of conservative outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Approaching Human-Level Forecasting with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7749.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7749.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Forecasting dataset (5 platforms)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curated Forecasting Dataset from Metaculus, Good Judgment Open, INFER, Polymarket, Manifold</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, curated dataset of real-world forecasting questions and recorded individual forecasts from five competitive platforms used to train, validate, and test the LM forecasting system; curated to binary questions and split to avoid pretraining leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Forecasting dataset (Metaculus, GJOpen, INFER, Polymarket, Manifold)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — dataset of platform questions (background, resolution criteria, begin/close/resolve dates) and recorded crowd forecasts; curated into 5,516 binary questions (3,762 train / 840 val / 914 test) after filtering ill-defined items and removing low-volume market items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Binary questions about future real-world events (multi-domain), including Science & Tech category but not limited to scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Multi-domain (Science & Tech, Healthcare & Biology, Economics, Politics, Security, Sports, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Raw: 48,754 questions and 7,174,607 user forecasts (2015–2024). Curated binary subset used for experiments: 5,516 questions (train/val/test splits as above). Test set restricted to questions opened after model knowledge cutoffs to prevent leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td>Question-specific; average question window ≈70 days; average time until resolution ≈42 days; retrieval schedule uses 5 geometrically spaced retrieval dates between begin and close date (excluding dates after early resolution).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Dataset used to compute Brier scores averaged across retrieval dates per question, then averaged across questions; also used for accuracy and RMS calibration error analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Not applicable to dataset itself; dataset supports reported system performance: system Brier = 0.179 (test set), crowd = 0.149.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Dataset provides human crowd aggregates used as strong baselines (platform-specific aggregation mechanisms described: e.g., Metaculus weighted median, GJOpen mean of recent 40%, Polymarket/Manifold market prices).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Curation choices remove some question types; recent questions heavily skewed by platform (Manifold and Polymarket dominance); questions that span knowledge cutoffs are discarded; many questions resolve early creating uneven number of retrieval dates; dataset does not focus specifically on 'scientific discoveries' as a separate, large labeled set.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Approaching Human-Level Forecasting with Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Forecasting future world events with neural networks <em>(Rating: 2)</em></li>
                <li>Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval <em>(Rating: 2)</em></li>
                <li>ForecastQA: A question answering challenge for event forecasting with temporal text data <em>(Rating: 1)</em></li>
                <li>Humans vs large language models: Judgmental forecasting in an era of advanced AI <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7749",
    "paper_id": "paper-59347f86ce9af155266729e4b0301a29c65abf88",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [
        {
            "name_short": "Retrieval-augmented LM forecasting system",
            "name_full": "Retrieval-augmented Language Model Forecasting System (retrieval + reasoning + ensembling)",
            "brief_description": "An end-to-end system that (1) uses LMs to generate search queries, retrieve and summarize news articles, (2) prompts LMs with scratchpads to produce probability forecasts and reasonings, and (3) ensembles multiple LM outputs (trimmed mean) into a final calibrated probability for binary real-world event questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "System (GPT-4-1106-Preview base + fine-tuned GPT-4-0613)",
            "model_description": "Pipeline combining retrieval (LM-generated queries, relevancy rating, summarization), reasoning (scratchpad prompts eliciting arguments for/against and calibration checks), and ensembling (trimmed-mean of multiple LM outputs). Fine-tuning: self-supervised selection of model-generated reasonings that outperform crowd forecasts to teach the model effective reasoning strategies.",
            "model_size": null,
            "probability_estimation_method": "Direct numeric probability output from LMs via prompting (explicit instruction to output a number in [0,1]); multiple stochastic/alternative prompts sampled (including temperature T=0.5 for fine-tuned model), then ensembled via trimmed mean; fine-tuning on model outputs that beat crowd (with clipping/averaging with crowd when deviation &gt;0.15) and prompting the model to check for over/underconfidence (historical base-rates). No external post-hoc calibration (binning/isotonic) improved results.",
            "prediction_target": "Binary real-world event outcomes (forecasting questions sourced from forecasting platforms; e.g., whether an event occurs by a given date).",
            "domain": "Multi-domain (Science & Tech, Healthcare & Biology, Economics & Business, Politics, Security, Sports, etc.); includes a 'Science & Tech' subset but not limited to scientific-discovery-only questions.",
            "dataset_used": "Curated dataset compiled from five forecasting platforms (Metaculus, Good Judgment Open, INFER, Polymarket, Manifold): raw 48,754 questions and 7,174,607 user forecasts (2015–2024); curated binary subset: 5,516 binary questions (3,762 train / 840 val / 914 test). Test set restricted to questions opened after model knowledge cutoffs (post-June 1, 2023/2024 depending on model) to avoid pretraining leakage.",
            "forecasting_horizon": "Short-to-medium horizons typical of platform questions: average question window ≈70 days; average time to resolution ≈42 days; evaluation uses up to 5 simulated retrieval dates (geometrically spaced between open and close dates).",
            "evaluation_metric": "Primary: Brier score ((forecast - outcome)^2). Secondary: accuracy (binary threshold), RMS calibration error. Standard errors reported (noting SE likely underestimated due to time-series dependence).",
            "reported_performance": "End-to-end system: average Brier score = 0.179 (human crowd aggregate = 0.149); accuracy = 71.5% (crowd = 77.0%). System outperforms best baseline LM (GPT-4-1106-Preview zero-shot Brier = 0.208). Ablations: without fine-tuning Brier = 0.186; without fine-tuning and no retrieval Brier = 0.206. Selective forecasting under certain heuristics can match or beat the crowd in subsets (e.g., when crowd uncertain). Combining system + crowd (weighted) improves crowd Brier from 0.149 to 0.146.",
            "calibration_quality": "Authors report the optimized system is 'naturally well calibrated' (RMS calibration error on test ≈ 0.42; human crowd ≈ 0.38). Base models in zero-shot were less calibrated; fine-tuning and ensembling improved calibration without explicit calibration post-processing.",
            "baseline_methods": "Human crowd aggregate (platform aggregates), random/unskilled baseline (0.5 forecast → Brier 0.25), zero-shot and scratchpad prompting across 14 LMs (e.g., GPT-3.5, GPT-4, Claude-2.1, Llama-2 variants, Mistral variants), ablated systems (no retrieval, no fine-tuning), and different ensembling strategies (mean, median, geometric mean, trimmed mean - trimmed mean chosen).",
            "limitations": "Reported limitations include: (1) models hedge and tend to be underconfident (especially low-probability outputs) possibly due to safety training; (2) performance depends strongly on retrieval quality and number of relevant articles; (3) many questions have short horizons or resolve early creating uneven retrieval dates; (4) dataset composition bias (over 80% of recent questions from Manifold and Polymarket) and curation choices; (5) evaluation is retrospective simulation (using historical news up to retrieval date) rather than truly prospective forecasting; (6) fine-tuning selection criterion (keeping only model outputs that beat crowd) risks overconfidence, mitigated by filtering and averaging with crowd; (7) no per-event examples of scientific-discovery forecasts were given, and model knowledge cutoffs constrain which events are truly out-of-sample.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7749.0",
            "source_info": {
                "paper_title": "Approaching Human-Level Forecasting with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4-1106-Preview",
            "name_full": "GPT-4-1106-Preview (OpenAI)",
            "brief_description": "A state-of-the-art instruction-tuned large language model used as the primary base reasoning model in the system; used with scratchpad prompts to elicit probabilistic forecasts and reasonings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4-1106-Preview",
            "model_description": "Instruction-tuned transformer LLM (proprietary OpenAI model) used in prompting mode with scratchpads; provides deterministic prompts and outputs numeric probabilities when instructed. Employed for search-query generation, relevance scoring (some steps used GPT-3.5 for cost reasons), and as one of the reasoning models producing forecasts.",
            "model_size": null,
            "probability_estimation_method": "Prompted direct numeric probability outputs (explicit instruction to return a number between 0 and 1); multiple scratchpad prompts sampled to produce multiple candidate forecasts; ensemble (trimmed mean) across prompts and model samples. No external temperature scaling reported for this base model in the main system (fine-tuned model uses T=0.5 for sampling).",
            "prediction_target": "Binary forecasting questions from platforms (multi-domain).",
            "domain": "Multi-domain including Science & Tech subset.",
            "dataset_used": "Evaluated on the curated forecasting dataset from Metaculus, GJOpen, INFER, Polymarket, Manifold (test set: 914 binary questions post-cutoff).",
            "forecasting_horizon": "Same as system: average question window ≈70 days; 5 retrieval dates geometrically spaced between open and close dates.",
            "evaluation_metric": "Brier score (primary), accuracy (secondary), calibration plots and RMS calibration error.",
            "reported_performance": "Baseline (scratchpad or zero-shot best) for GPT-4-1106-Preview: Brier score ≈ 0.208 (Table 7 shows best zero-shot/scratchpad for the GPT-4 family at ~0.208–0.209). As part of the final system (combined with fine-tuned model and ensembling) it contributes to the system Brier score of 0.179. Ablation with only base GPT-4 (no fine-tuning) yields Brier ≈ 0.186; no IR yields ≈0.206.",
            "calibration_quality": "Base GPT-4 in zero-shot/scratchpad was less well-calibrated (Figure 3a); within the optimized system and when ensembled/fine-tuned, calibration improves considerably.",
            "baseline_methods": "Compared against other LMs in baseline table (GPT-3.5, Claude-2.1, Llama-2 variants, Mistral, Gemini-Pro), and against human crowd aggregate and random baseline.",
            "limitations": "Pretraining knowledge cutoff (April 2023 for GPT-4-1106-Preview) so cannot know events after cut-off; requires retrieval to reach competitive performance; scratchpad prompts sometimes trigger refusal due to safety training; not fine-tuned in some ablations so less calibrated.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7749.1",
            "source_info": {
                "paper_title": "Approaching Human-Level Forecasting with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Fine-tuned GPT-4-0613",
            "name_full": "Fine-tuned GPT-4-0613 reasoning model (fine-tuned on selected high-performing model reasonings)",
            "brief_description": "A GPT-4 variant fine-tuned in a self-supervised way on model-generated reasonings and probability outputs that outperformed the crowd on the training set, used to produce additional forecasts without scratchpad instructions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Fine-tuned GPT-4-0613",
            "model_description": "Fine-tuned version of GPT-4 (0613 checkpoint) trained on 6,000 most recent selected reasoning–prediction pairs (from a pool of 13,253 candidates) where model outputs beat crowd forecasts; fine-tuning teaches which reasoning method to apply in context and aims to improve calibration and accuracy of probability estimates.",
            "model_size": null,
            "probability_estimation_method": "Direct numeric probability outputs; fine-tuned to produce reasonings that lead to accurate probabilities; sampling with temperature T=0.5 for generating multiple forecasts; ensemble with base GPT-4 via trimmed mean. During fine-tuning, targets were averaged with crowd if model deviated &gt;0.15 to mitigate overconfidence.",
            "prediction_target": "Binary forecasting questions from the curated dataset (as above).",
            "domain": "Multi-domain including Science & Tech, Healthcare & Biology, etc.",
            "dataset_used": "Fine-tuned on selected model outputs from the training split of the curated dataset (6,000 most recent points used for fine-tuning; initial generated pool 73,632, 13,253 selected meeting criteria). Evaluated on the same curated test set (914 binary questions).",
            "forecasting_horizon": "Same dataset horizons (average question window ≈70 days; average resolution ≈42 days); fine-tuning data generated at multiple retrieval dates per question (n=5 retrieval dates, subject to question resolution).",
            "evaluation_metric": "Brier score (primary), accuracy, RMS calibration error.",
            "reported_performance": "Fine-tuned GPT-4-0613 used alone: Brier score ≈ 0.182 (Table 5). When combined in the full system with base GPT-4-1106-Preview and ensembling, contributes to final system Brier = 0.179. Fine-tuned GPT-3.5 variants produced similar, slightly worse results (~0.182–0.183).",
            "calibration_quality": "Fine-tuning and ensembling improved calibration relative to base zero-shot models; overall system RMS calibration error ≈ 0.42 (human crowd 0.38). Authors note standard calibration methods (binning/isotonic) did not improve performance further.",
            "baseline_methods": "Compared to base GPT-4 (no fine-tuning), fine-tuned GPT-3.5, and other LMs; ablation studies show fine-tuning yields modest but meaningful improvement.",
            "limitations": "Fine-tuning data selection depended on filtering for outputs that beat the crowd (risk of selection bias/overfitting); budget constraints limited fine-tuning to 6,000 points; potential for overconfidence mitigated by averaging with crowd when deviations large; still hedges on some high-certainty questions due to safety training and retention of conservative outputs.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7749.2",
            "source_info": {
                "paper_title": "Approaching Human-Level Forecasting with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Forecasting dataset (5 platforms)",
            "name_full": "Curated Forecasting Dataset from Metaculus, Good Judgment Open, INFER, Polymarket, Manifold",
            "brief_description": "A large, curated dataset of real-world forecasting questions and recorded individual forecasts from five competitive platforms used to train, validate, and test the LM forecasting system; curated to binary questions and split to avoid pretraining leakage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Forecasting dataset (Metaculus, GJOpen, INFER, Polymarket, Manifold)",
            "model_description": "Not a model — dataset of platform questions (background, resolution criteria, begin/close/resolve dates) and recorded crowd forecasts; curated into 5,516 binary questions (3,762 train / 840 val / 914 test) after filtering ill-defined items and removing low-volume market items.",
            "model_size": null,
            "probability_estimation_method": null,
            "prediction_target": "Binary questions about future real-world events (multi-domain), including Science & Tech category but not limited to scientific discoveries.",
            "domain": "Multi-domain (Science & Tech, Healthcare & Biology, Economics, Politics, Security, Sports, etc.).",
            "dataset_used": "Raw: 48,754 questions and 7,174,607 user forecasts (2015–2024). Curated binary subset used for experiments: 5,516 questions (train/val/test splits as above). Test set restricted to questions opened after model knowledge cutoffs to prevent leakage.",
            "forecasting_horizon": "Question-specific; average question window ≈70 days; average time until resolution ≈42 days; retrieval schedule uses 5 geometrically spaced retrieval dates between begin and close date (excluding dates after early resolution).",
            "evaluation_metric": "Dataset used to compute Brier scores averaged across retrieval dates per question, then averaged across questions; also used for accuracy and RMS calibration error analyses.",
            "reported_performance": "Not applicable to dataset itself; dataset supports reported system performance: system Brier = 0.179 (test set), crowd = 0.149.",
            "calibration_quality": null,
            "baseline_methods": "Dataset provides human crowd aggregates used as strong baselines (platform-specific aggregation mechanisms described: e.g., Metaculus weighted median, GJOpen mean of recent 40%, Polymarket/Manifold market prices).",
            "limitations": "Curation choices remove some question types; recent questions heavily skewed by platform (Manifold and Polymarket dominance); questions that span knowledge cutoffs are discarded; many questions resolve early creating uneven number of retrieval dates; dataset does not focus specifically on 'scientific discoveries' as a separate, large labeled set.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7749.3",
            "source_info": {
                "paper_title": "Approaching Human-Level Forecasting with Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Forecasting future world events with neural networks",
            "rating": 2
        },
        {
            "paper_title": "Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval",
            "rating": 2
        },
        {
            "paper_title": "ForecastQA: A question answering challenge for event forecasting with temporal text data",
            "rating": 1
        },
        {
            "paper_title": "Humans vs large language models: Judgmental forecasting in an era of advanced AI",
            "rating": 1
        }
    ],
    "cost": 0.016454249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Approaching Human-Level Forecasting with Language Models</h1>
<p>Danny Halawi<em><br>UC Berkeley<br>Fred Zhang</em><br>UC Berkeley<br>Chen Yueh-Han*<br>iChun0922ucb@berkeley.edu<br>Jacob Steinhardt<br>jsteinhardt@berkeley.edu<br>UC Berkeley</p>
<p>$k$</p>
<h2>Abstract</h2>
<p>Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters, and in some settings surpasses it. Our work suggests that using LMs to forecast the future could provide accurate predictions at scale and help to inform institutional decision making.</p>
<h2>1 Introduction</h2>
<p>Forecasting events is important in the modern world. Governments rely on economic and geopolitical forecasts for decision-making. Companies hire and invest based on forecasts of market conditions (Armstrong, 2001). In 2020, epidemiological forecasts for COVID-19 prompted national lockdowns across the globe (Adam, 2020).</p>
<p>There are two main approaches to forecasting. Statistical forecasting primarily uses tools from time-series modeling. This methodology typically excels when data are abundant and under minimal distributional shift. By contrast, in judgmental forecasting, human forecasters assign probabilities to future events based on their own judgments, making use of historical data, domain knowledge, Fermi estimates, and intuition. They draw information from diverse sources and reason based on detailed contexts of the task. This enables accurate forecasts even with scarce past observations or under significant distributional shift (Tetlock and Gardner, 2015). We will refer to judgmental forecasting simply as "forecasting".</p>
<p>Since forecasting relies on human effort and expertise, it can be expensive, delayed, or applicable only in specific domains. Moreover, most human forecasts contain little or no explanatory reasoning. These limitations motivate using language models (LMs) to automate forecasting (Hendrycks et al., 2021). Because they can parse and produce texts rapidly, LMs can provide cheap and timely forecasts. Because they are pre-trained on web-scale data, they are endowed with massive, cross-domain knowledge. And because we can elicit their reasonings through prompts, we can examine them to (partially) understand the final forecast.</p>
<p>In this work, we build a LM pipeline for automated forecasting, with a focus on predicting binary outcomes. Our system implements and automates three key components in the traditional forecasting process: (1) retrieval, which gathers relevant information from news sources; (2) reasoning, which weighs available data</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
(a) Our retrieval system. The LM takes in the question and generates search queries to retrieve articles from historical news APIs. Then the LM ranks the articles on relevancy and summarizes the top $k$ articles.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Our reasoning system. The system takes in the question and summarized articles and prompts LMs to generate forecasts. The forecasts are then aggregated into a final forecast using the trimmed mean.</p>
<p>Figure 1: Overview of our retrieval and reasoning systems. Our retrieval system retrieves summarized new articles and feeds them into the reasoning system, which prompts LMs for reasonings and predictions that are aggregated into a final forecast.
and makes a forecast; and (3) aggregation, which ensembles individual forecasts into an aggregated prediction. Each step makes use of an LM or a collection of LMs (either prompted or fine-tuned) (Figure 1).</p>
<p>To optimize and evaluate our system, we collect a large dataset of forecasting questions from 5 competitive forecasting platforms. The test set consists only of (binary) questions published after June 1st, 2023. Since this is after the knowledge cut-off date of our models, this prevents leakage from pre-training. The train set contains questions before June 1st, 2023, which we use for hyperparameter search and fine-tuning our system.</p>
<p>We use a self-supervised approach to fine-tune a LM to make accurate predictions and explanatory reasonings. We first prompt a base LM with various scratchpads to elicit forecasts to questions in our training set. We then fine-tune a new LM on the outputs that outperformed the crowd, which teaches the model what reasoning method to apply in a given context and improves forecasting performance. For hyperparameter search, we identify system configurations, including retrieval and LM prompting strategies, that lead to the best end-to-end performance.</p>
<p>Our optimized system approaches the performance of aggregated human forecasts over the test set, as measured by Brier score, a standard metric in forecasting. To our knowledge, this is the first automated system with forecasting capability that nears the human crowd level, which is generally stronger than individual human forecasters (Section 3.1). We also consider a selective setting where our system uses heuristics, based on the LM's strengths, to decide whether to submit a forecast for a given question and date. In this setting, our system outperforms the human crowd.</p>
<p>To summarize our main contributions:</p>
<ol>
<li>We curate the largest, most recent dataset of real-world forecasting questions to date, for evaluating and optimizing automated forecasting systems.</li>
<li>We build a retrieval-augmented LM system that significantly improves upon the baseline and approaches the human crowd performance on competitive forecasting platforms.</li>
<li>We propose and apply a self-supervised fine-tuning method to improve LM's capability in reasoning about forecasting tasks.</li>
</ol>
<h1>2 Related Work</h1>
<p>Event forecasting. Machine learning systems that make accurate, automated forecasts can help inform human decision-making (Hendrycks et al., 2021). Jin et al. (2021) provided ForecastQA, the first dataset</p>
<table>
<thead>
<tr>
<th>Field</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Question</td>
<td>Will Starship achieve liftoff before Monday, May 1st, 2023?</td>
</tr>
<tr>
<td>Background</td>
<td>On April 14th, SpaceX received a launch license for its Starship spacecraft. A launch scheduled for April 17th was scrubbed due to a frozen valve. SpaceX CEO Elon Musk tweeted: “Learned a lot today, now offloading propellant, retrying in a few days …”</td>
</tr>
<tr>
<td>Resolution Criteria</td>
<td>This question resolves Yes if Starship leaves the launchpad intact and under its own power before 11:59pm ET on Sunday, April 30th.</td>
</tr>
<tr>
<td>Key Dates</td>
<td>Begin Date: 2023-04-17 Close Date: 2023-04-30 Resolve Date: 2023-04-20</td>
</tr>
</tbody>
</table>
<p>Table 1: A sample question with its background, resolution criteria, and key dates. The question resolved early (with a final resolution of Yes). See Table 12 for the complete sample point.</p>
<p>for this task, which contains questions created by crowdworkers based on events from news articles. <em>Zou et al. (2022)</em> introduced Autocast, a benchmark dataset compiled from forecasting competition questions up to 2022. In a competition with a large prize pool, no machine learning system was able to approach the performance of human forecasters on Autocast <em>(Zou et al., 2022)</em>. The knowledge cut-offs of LMs have moved past 2022, necessitating more recent data. In this work, we source questions in 2023–2024, enabling us to apply recent LMs.</p>
<p><em>Yan et al. (2024)</em> built a retrieval system that led to improved accuracy on Autocast. They trained a Fusion-in-Decoder model to directly predict the final (binary) resolution <em>(Izacard and Grave, 2021)</em> and reported accuracy, whereas we elicit both explanatory reasonings and probability forecasts from LMs and measure performance with the standard Brier score metric.</p>
<p><em>Schoenegger and Park (2023); Abolghasemi et al. (2023)</em> evaluated GPT-4 and other LLMs on forecasting tournaments and found that they underperform the human crowd. This observation is in line with ours in Section 3.4. Unlike us, they make little or no efforts to improve these LMs on forecasting.</p>
<p>Finally, there has been recent work on using transformer models or LMs for statistical time-series forecasting <em>(Nie et al., 2023; Gruver et al., 2023; Dooley et al., 2023; Rasul et al., 2023; Jin et al., 2024; Das et al., 2024; Woo et al., 2024)</em>, but this is distinct from our focus on judgmental forecasting.</p>
<p>Information retrieval (IR). IR can improve question-answering capabilities of LMs <em>(Lewis et al., 2020; Shuster et al., 2021; Nakano et al., 2021)</em>. In event forecasting, access to diverse, up-to-date information is crucial <em>(Tetlock and Gardner, 2015)</em>. Thus, a key component of our system is an IR architecture that furnishes the reasoning model with news articles, using LMs for query expansion, relevance ranking and summarization. Beyond our setting, using LMs for IR is an active research topic <em>(Zhu et al., 2024)</em>.</p>
<p>Calibration. Calibration is important for accurate forecasting <em>(Tetlock and Gardner, 2015)</em>. Hence, on competitive forecasting tournaments, forecasters are evaluated by proper scoring rules, such as Brier score <em>(Brier, 1950)</em>, which incentivize calibration <em>(Gneiting and Raftery, 2007)</em>. There is a vast literature on calibration in deep learning; see <em>Gawlikowski et al. (2021); Wang (2023)</em> for surveys.</p>
<h2>3 Preliminaries: Data, Models and Baseline</h2>
<h3>3.1 Dataset</h3>
<p>Data format. Forecasting platforms such as Metaculus, Good Judgment Open, INFER, Polymarket, and Manifold invite participants to predict future events by assigning probabilities to outcomes of a question. Each question consists of a <em>background description, resolution criterion,</em> and 3 timestamps: a <em>begin date</em> when the question was published, a <em>close date</em> when no further forecasts can be submitted, and (eventually) a <em>resolve date</em> when the outcome is determined. A forecast can be submitted between the begin date and min(resolve date, close date). See Table 1 for an example question with these main fields.</p>
<p>Crowd prediction. On any given question, as individual forecasts are submitted, forecasting platforms continuously aggregate them into a crowd prediction; see Section A.3 for details about the aggregation</p>
<table>
<thead>
<tr>
<th>Platform</th>
<th>Train</th>
<th>Validation</th>
<th>Test</th>
<th>Model</th>
<th>Zero-shot</th>
<th>Scratchpad</th>
</tr>
</thead>
<tbody>
<tr>
<td>Metaculus</td>
<td>1,576</td>
<td>230</td>
<td>275</td>
<td>GPT-4-1106-Preview</td>
<td>$\mathbf{0 . 2 0 8}(0.006)$</td>
<td>$\mathbf{0 . 2 0 9}(0.006)$</td>
</tr>
<tr>
<td>GJOpen</td>
<td>806</td>
<td>161</td>
<td>38</td>
<td>Llama-2-13B</td>
<td>$0.226(0.004)$</td>
<td>$0.268(0.004)$</td>
</tr>
<tr>
<td>INFER</td>
<td>52</td>
<td>50</td>
<td>4</td>
<td>Mistral-8x7B-Instruct</td>
<td>$0.238(0.009)$</td>
<td>$0.238(0.005)$</td>
</tr>
<tr>
<td>Polymarket</td>
<td>70</td>
<td>229</td>
<td>300</td>
<td>Claude-2.1</td>
<td>$0.220(0.006)$</td>
<td>$0.215(0.007)$</td>
</tr>
<tr>
<td>Manifold</td>
<td>1,258</td>
<td>170</td>
<td>297</td>
<td>Gemini-Pro</td>
<td>$0.243(0.009)$</td>
<td>$0.230(0.003)$</td>
</tr>
<tr>
<td>All Platforms</td>
<td>$\mathbf{3 , 7 6 2}$</td>
<td>$\mathbf{8 4 0}$</td>
<td>$\mathbf{9 1 4}$</td>
<td>Trimmed mean</td>
<td>$0.208(0.006)$</td>
<td>$0.224(0.003)$</td>
</tr>
</tbody>
</table>
<p>(a) Dataset distribution
(b) Baseline performance of pre-trained models</p>
<p>Table 2: (a) Distribution of our train, validation, and test sets across all 5 forecasting platforms. Importantly, every question in the test set is from June 1, 2023 or later, after the training cut-off of our base LMs. Meanwhile, all questions in the train and validation sets were resolved before June 1, 2023, ensuring no leakage from the tuning process. (b) Baseline performance of pre-trained models on the test set, with 1 standard error (SE) (see full results in Table 7). Random baseline: 0.250 ; human crowd: 0.149 . The results underscore that models are not naturally good at forecasting.
mechanisms. The crowd prediction is a strong benchmark to compete with. For example, Metaculus (2023) shows that an ensemble of all forecasters consistently outperforms using just the top $5,10, \ldots, 30$ best forecasters (based on past scores). In this work, we compare our system performance to the crowd aggregates.</p>
<p>Raw data. We source forecasting questions from the 5 above-mentioned platforms. This yields a total of 48,754 questions and $7,174,607$ user forecasts spanning from 2015 to 2024 . The dataset includes 33,664 binary questions, 9,725 multiple-choice questions, 4,019 numerical questions, and 1,346 questions of other types. The questions cover a wide range of topics across the globe (Figure 10).</p>
<p>The raw dataset contains questions that are ill-defined, overly personal, or of niche interests. Furthermore, recent questions are highly unbalanced, with over $80 \%$ of questions since June 1, 2023 coming from Manifold and Polymarket.</p>
<p>Data curation. To address the above issues, we curate a subset by filtering ill-defined questions and removing questions that received few forecasts or trading volume on Manifold and Polymarket. We focus on predicting binary questions and split multiple-choice questions into binary ones.</p>
<p>To guard potential leakage from LMs' pre-training, we only include questions in the test set that appear after the knowledge cut-off for the models we use (June 1, 2024). All test set questions were opened after the date, and all train and validation questions were resolved before. Questions that span across the date are discarded.</p>
<p>This yields a set of 5,516 binary questions, including 3,762 for training, 840 for validation, and 914 for testing (Table 2a). See Table 12 for a sample data point and Appendix C for details about the curation process.</p>
<h1>3.2 Evaluation</h1>
<p>Retrieval schedule. We can simulate forecasting the future by leveraging the fact that models are only trained up to a cut-off date (Zou et al., 2022). To simulate a forecast for a question that has been resolved, we query a historical news corpus to retrieve articles between the question begin date and a specified retrieval date (Zou et al., 2022; Yan et al., 2024). The retrieval date can be viewed as the "simulated date" of the forecast, as we are mimicking the information the model would have had access to on that date.</p>
<p>To create a set of retrieval dates for each question, we use geometrically increasing time points between the open and close dates. We choose this schedule for two reasons: (1) questions are often most active shortly after they open, and (2) some questions have overly conservative close dates that are long after the question resolves. We use $n=5$ retrieval dates per question; the $k$ th retrieval date is calculated as</p>
<p>$$
\text { retrieval_date }<em _begin="{begin" _text="\text">{k}=\text { date }</em>}}+\left(\text { date <em _begin="{begin" _text="\text">{\text {close }}-\text { date }</em>
$$}}-1\right)^{k / n} \text {. </p>
<p>For questions that resolve before they close, we exclude all dates occurring after the question has been resolved. Under this geometric retrieval schedule, we retain $86 \%$ of retrieval dates on average across all questions (Figure 11b). The average question window in our corpus is approximately 70 days, and the average time until resolution is 42 days.</p>
<p>In our dataset, questions can get resolved long before their official close date. This occurs for questions like "Will $\langle$ event $\rangle$ happen by $\langle$ date $\rangle$ ", where resolving early indicates that the event did occur (see Table 1 for an example). It is tempting to choose retrieval dates with respect to the resolve date so that each question can receive the same number of retrieval dates, e.g. by retrieving at geometric intervals between the open and resolve date. However, this would leak information, since the retrieval date would now depend on the resolve date, which, as we explained, correlates with the resolution.</p>
<p>Metric. Our work focuses on binary questions and uses the Brier score as the performance metric, defined as $(f-o)^{2}$, where $f \in[0,1]$ is the probabilistic forecast and $o \in{0,1}$ is the outcome. The Brier score is a strictly proper scoring rule: assuming the true probability that $o=1$ is $p$, the optimal strategy is to report $f=p$. This is desirable, since improper scoring rules would incentivize reporting distorted probabilities. As a baseline, an (unskilled) forecast of .5 attains a Brier score of .25 .</p>
<p>To compute the final Brier score, we first average the Brier scores across retrieval dates for each question, then average across questions. We also report standard errors; however, note that the computation of standard errors assumes the data are i.i.d., while our data are in fact time-series, so this likely underestimates the true error. Finally, we also measure calibration with root mean square (RMS) calibration error.</p>
<h1>3.3 Models</h1>
<p>We evaluate 14 instruction-tuned LMs: GPT-3.5-Turbo, GPT-3.5-Turbo-1106 (Brown et al., 2020); GPT-4, GPT-4-1106-Preview (OpenAI, 2023); Llama-2-7B, Llama-2-13B, Llama-2-70B (Touvron et al., 2023); Mistral-7B-Instruct, Mistral-8x7B-Instruct (Jiang et al., 2024), Nous Hermes 2 Mixtral-8x7B-DPO, Yi-34B-Chat, Claude-2, Claude-2.1 (Anthropic, 2023), and Gemini-Pro (Gemini Team, 2023); see Section A. 1 for details.</p>
<h3>3.4 Models are not naturally good at forecasting</h3>
<p>As a baseline, we evaluate all 14 LMs with no additional information retrieval. We use zero-shot prompts and scratchpad prompts (Nye et al., 2021). For each prompting strategy, we craft candidate prompts, pick the best prompt on the validation set, and report its Brier scores on the test set. The results are given in Table 2b, where we report the best model in each series; see Table 7 for full statistics. The prompt choices appear in Figure 5 and Figure 6 and further details are in Appendix B.</p>
<p>None of the models are naturally good at forecasting. Most models' scores are around or worse than random guessing (.25). Only the GPT-4 and Claude-2 series beat the unskilled baseline by a large margin ( $&gt;.02$ ). Moreover, while GPT-4-1106-Preview achieves the lowest Brier score of .208, it trails significantly behind the human crowd performance of .149 .</p>
<h2>4 Our System</h2>
<p>As observed in Table 2b, all models perform poorly in the baseline setting. We intuit that models require detailed contexts and up-to-date information to make accurate forecasts. Our system addresses this issue via news retrieval and elicits better reasoning via optimized prompting strategies and fine-tuning.</p>
<h3>4.1 Retrieval</h3>
<p>Our retrieval system consists of 4 steps: search query generation, news retrieval, relevance filtering and re-ranking, and text summarization (Figure 1a).</p>
<p>First, we generate search queries that are used to invoke news APIs to retrieve historical articles. We initially implement a straightforward query expansion prompt (Figure 12a), instructing the model to create queries</p>
<table>
<thead>
<tr>
<th>Criteria</th>
<th>Brier Score $\downarrow$</th>
<th></th>
<th></th>
<th>\% Accuracy $\uparrow$</th>
<th></th>
<th></th>
<th>\% Data Retained $\uparrow$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Ours</td>
<td>Crowd</td>
<td>Aggregate</td>
<td>Ours</td>
<td>Crowd</td>
<td>Aggregate</td>
<td>Forecasts</td>
<td>Questions</td>
</tr>
<tr>
<td>All Questions</td>
<td>$.179_{.003}$</td>
<td>$.149_{.003}$</td>
<td>$\mathbf{. 1 4 6}_{.002}$</td>
<td>$71.5_{.7}$</td>
<td>$77.0_{.7}$</td>
<td>$\mathbf{7 7 . 8}_{.6}$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>Crowd Uncertain</td>
<td>$\mathbf{. 2 3 8}_{.004}$</td>
<td>$.240_{.003}$</td>
<td>$\mathbf{. 2 3 3}_{.002}$</td>
<td>$58.1_{1.3}$</td>
<td>$58.3_{1.3}$</td>
<td>$\mathbf{6 0 . 2}_{1.2}$</td>
<td>$51 \%$</td>
<td>$56 \%$</td>
</tr>
<tr>
<td>Early Retrieval</td>
<td>$.186_{.003}$</td>
<td>$.162_{.004}$</td>
<td>$\mathbf{. 1 5 9}_{.003}$</td>
<td>$70.0_{.9}$</td>
<td>$74.4_{.9}$</td>
<td>$\mathbf{7 5 . 0}_{.8}$</td>
<td>$84 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>5+ Articles</td>
<td>$.175_{.003}$</td>
<td>$.142_{.003}$</td>
<td>$\mathbf{. 1 4 0}_{.002}$</td>
<td>$72.3_{.8}$</td>
<td>$77.7_{.7}$</td>
<td>$\mathbf{7 8 . 7}_{.7}$</td>
<td>$84 \%$</td>
<td>$94 \%$</td>
</tr>
<tr>
<td>All Criteria</td>
<td>$\mathbf{. 2 4 0}_{.005}$</td>
<td>$.247_{.004}$</td>
<td>$\mathbf{. 2 3 7}_{.003}$</td>
<td>$\mathbf{5 8 . 0}_{1.7}$</td>
<td>$54.2_{1.7}$</td>
<td>$\mathbf{5 6 . 6}_{1.7}$</td>
<td>$22 \%$</td>
<td>$43 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: System performance on the test set. "All Questions" shows the Brier score on the full test set. Other rows show selective evaluation when specified criteria are met, averaging over qualifying questions and retrieval dates. "Crowd Uncertain" refers to questions with crowd predictions between 0.3-0.7. "Early Retrieval" refers to the first 3 retrieval dates. "5+ Articles" refers to forecasting when at least 5 relevant articles are retrieved. Finally, "All Criteria" refers to forecasting when the 3 criteria are jointly met. Notably, in every setting the aggregate (average) of our system and crowd prediction is the best. Subscript numbers indicate 1 standard error. We bold entries that outperform the crowd aggregate, and underline the best entry in each category.
based on the question and its background. However, we find that this overlooks sub-considerations that often contribute to accurate forecasting. To achieve broader coverage, we prompt the model to decompose the forecasting question into sub-questions and use each to generate a search query (Min et al., 2019); see Figure 12b for the prompt. For instance, when forecasting election outcomes, the first approach searches directly for polling data, while the latter creates sub-questions that cover campaign finances, economic indicators, and geopolitical events. We combine both approaches for comprehensive coverage.
Next, the system retrieves articles from news APIs using the LM-generated search queries. We evaluate 5 APIs on the relevance of the articles retrieved and select NewsCatcher ${ }^{1}$ and Google News (Section E.2).
Our initial retrieval provides wide coverage at the cost of obtaining some irrelevant articles. To ensure that they do not mislead the model at the reasoning step, we prompt GPT-3.5-Turbo to rate the relevancy of all articles (Figure 14) and filter out low-scoring ones. Since the procedure is costly in run-time and budget, we only present the article's title and first 250 words to the model in context. We validate that this approach achieves high recall and precision while saving $70 \%$ cost (see Section E. 3 for alternative methods and results).
Since LMs are limited by their context window, we summarize the articles. In particular, we prompt GPT-3.5-Turbo to distill the most relevant details from each article with respect to the forecasting question (Figure 13). Finally, we present the top $k$ article summaries to the LM, ordered by their relevancy. We choose the ranking criterion, article count $k$, and summarization prompt based on end-to-end Brier scores over the validation set; see Section 5.2 for the hyperparameter sweep procedure.</p>
<h1>4.2 Reasoning</h1>
<p>Prior work in forecasting has focused on eliciting predictions from models without requiring rationales (Zou et al., 2022; Yan et al., 2024). However, accurately predicting the future is a difficult task that often requires computation beyond a single forward pass. Having the model externalize its reasoning also allows us to understand the explanation for the forecast and improve it accordingly.
We use open-ended scratchpad to structure model's reasoning paths. Our prompt begins with posing the question, providing a description, and specifying resolution criteria and key dates, followed by the top $k$ relevant summaries (Figure 16). To guide the model to reason about the forecasting question, the optimal scratchpad prompt (Figure 15), as identified in Section 5.2, also incorporates four additional components:</p>
<ul>
<li>First, to ensure that the model comprehends the question, we prompt it to rephrase the question. It is also instructed to expand the question with its own knowledge to provide further information. Intuitively, a more detailed and precise phrasing of the question elicits better responses (Deng et al., 2023).</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: Our procedure of generating data for self-supervised training. For each question, the method generates multiple candidate reasoning-prediction pairs and selects those that outperform human aggregates for fine-tuning.</p>
<ul>
<li>Forecasting requires a holistic consideration of the possibilities (Tetlock and Gardner, 2015). We next prompt the model to leverage the retrieved information and its pre-training knowledge to produce arguments for why the outcome may or may not occur.</li>
<li>The model can potentially generate weak arguments. To avoid treating all considerations as equal, it is instructed to weigh them by importance and aggregate them accordingly into an initial forecast.</li>
<li>Finally, to prevent potential bias and miscalibration, the model is asked to check if it is over- or underconfident and consider historical base rates (Tetlock and Gardner, 2015), prompting it to calibrate and amend the prediction accordingly.</li>
</ul>
<p>Base model. We prompt GPT-4-1106-Preview with the best scratchpads (found via hyperparameter sweep), since it consistently gives the lowest Brier scores among the LMs we test (see Section 5.2 on reasoning).</p>
<p>Fine-tuned model. We also prompt a fine-tuned version of GPT-4 that we trained to generate reasonings with accurate predictions (Section 5.1). We prompt it with only the question's basic information (no scratchpad instructions) since our fine-tuned model is trained to reason without prescriptive instructions.</p>
<h1>4.3 Ensembling</h1>
<p>Since the aggregate of predictions is usually superior to individual forecasts (Tetlock and Gardner, 2015), we elicit multiple predictions from the base and fine-tuned models.</p>
<p>We prompt GPT-4-1106-Preview with the optimal scratchpad prompt (Figure 15), along with the 2 next best scratchpad prompts identified in Section 5.2. For our fine-tuned model, we set temperature $T=0.5$ and prompt it 3 times to sample 3 additional forecasts. This gives us 6 forecasts in total: 3 from the base model, and 3 from the fine-tuned model. Given these forecasts, the system ensembles them into a final prediction by taking their trimmed mean, as this performs best on the validation set among the ensemble methods we implement (see Section 5.2 on ensembling).</p>
<p>We provide further details about our system in Appendix D, including hyperparameters and prompt designs.</p>
<h2>5 Optimizing the System</h2>
<p>We now describe the procedure to optimize our retrieval and reasoning system and the results obtained.</p>
<h3>5.1 Fine-tuning a Reasoning Model</h3>
<p>We fine-tune a LM to produce reasonings that lead to accurate forecasts. To generate the data for fine-tuning, we (1) collect a large set of forecasts on the train set, and then (2) select a subset where the model outperforms the human crowd.</p>
<p>Collecting fine-tuning data. To generate the preliminary data, we run our system at each retrieval date in the retrieval schedule and on each question in the train set, multiplied by 16 configurations described below.</p>
<p>First, as a form of data augmentation, we retrieve 2 sets of articles for each question by sampling 2 (distinct) retrieval configurations (Figure 2, left). Specifically, we sample the retrieval prompt, number of queries, and articles per query, twice (Section 4), with relevancy filtering and summarization following the process described in Section 4.1. This results in 2 inputs to the reasoning model per question, each with the same question but a different set of articles.</p>
<p>To increase the chance of attaining a prediction that outperforms the crowd, we generate 4 candidate outputs per input ( 8 total per question) by trying different scratchpad prompts. The first uses the optimal prompt found in Section 5.2 (Figure 15). We then sample 3 other scratchpad prompts, with probability inversely proportional to their Brier score on the validation set. We prompt both Claude-2.1 and GPT-4-Preview, since we find that Claude-2.1 is better on some questions. In total, this gives 16 candidate forecasts per question.</p>
<p>Selecting fine-tuning data. We seek to fine-tune our model on strong forecasts. To select the data, we only keep outputs that give a lower Brier score than the crowd's. However, this can inadvertently cause overconfidence in our fine-tuned model. To mitigate this, we discard pairs where the prediction deviates by more than 0.15 from the crowd prediction, and we also average our prediction with the crowd prediction when constructing the target output.</p>
<p>The resulting fine-tuning data has the following structure (Figure 2, right):</p>
<ul>
<li>The input to the model consists of the question, description, and resolution criteria, followed by summarized articles.</li>
<li>The target output consists of a reasoning and a prediction.</li>
</ul>
<p>Importantly, the fine-tuning input excludes the scratchpad instructions. By doing so, we directly teach the model which reasoning to apply in a given context.</p>
<p>In total, 73,632 reasonings are generated from which 13,253 meet the above desiderata. Finally, we fine-tune GPT-4-06132 on the 6,000 most recent points for 2 epochs, due to budget constraint (Figure 2, right).</p>
<h1>5.2 Hyperparameter Sweep</h1>
<p>Our hyperparameter sweep optimizes an (intermediate) metric over a discrete set of choices, such as prompts and the number of articles presented. We share the key findings below and more details in Appendix E.</p>
<p>Methodology. We divide the hyperparameters into groups of 1-2 and optimize them iteratively. For each group, we select the best configuration based on the average Brier score on the validation set, except for search query generation where we use proxy metrics for efficiency.</p>
<p>We optimize the groups sequentially, fixing the optimal configurations from previous groups while sweeping the current one. The hyperparameters yet to be swept are randomized independently for each input question.</p>
<p>Retrieval. Our retrieval uses LMs for search query generation, relevance rating, and summarization. We independently optimize the prompt choices for search query generation and summarization. The relevance rating prompt is fixed in our system (Figure 14).</p>
<p>For search query generation, we evaluate the prompts by retrieving articles with the generated queries and examining two metrics: (1) the average relevance score across all retrieved articles, and (2) the average relevance score of articles exceeding a relevance threshold of 4 on a 6 -point scale. The 2 high-scoring prompts perform similarly under both metrics and generate queries with little overlap. As a result, we use both prompts (Figure 12) to generate queries and take the union.</p>
<p>For summarization, we run our system end-to-end and pick the top 1 prompt (Figure 13) with respect to the Brier score.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Our system is naturally well calibrated on both (b) validation and (c) test. The crowd is also well calibrated, consistent with Zou et al. (2022)'s findings. In contrast, the base models in the zero-shot setting (a) are less calibrated (Section 3.4).</p>
<p>Reasoning. The reasoning system takes a ranked list of article summaries and prompts LMs to make forecasts. We optimize: (1) the ordering criterion of the summaries (by relevance or recency); (2) the number $k$ of article summaries presented to LMs; and (3) the choice of scratchpad instructions to elicit the forecasts.</p>
<p>For efficiency, we optimize them in 2 independent stages:</p>
<ul>
<li>In the first stage, we jointly optimize (1) and (2). Ranking by relevance and setting $k=15$ achieve the lowest average Brier score.</li>
<li>In the second stage, we optimize (3) the reasoning prompt. We identify the top 3 prompts out of 15 candidates to elicit 3 predictions from our base model in our system; see Figure 15 for the best one.</li>
</ul>
<p>In optimizing the reasoning system, we test both Claude-2.1 and GPT-4-1106-Preview as candidate models for generating forecasts. GPT-4-1106-Preview consistently yields a 0.01-0.03 lower Brier score. Therefore, our final system elicits predictions from GPT-4-1106-Preview and the fine-tuned GPT-4-0613.</p>
<p>Ensembling. We implement 5 ensembling methods, including mean, median, geometric mean, trimmed mean, and a variant of universal self-consistency (USC; Chen et al. (2023)). Trimmed mean performs the best in our evaluation; see Section E.1 for details.</p>
<p>Calibration. Interestingly, our system is naturally well calibrated (Figure 3b), and we find that standard calibration methods such as binning or isotonic regression do not improve performance.</p>
<h1>6 Evaluations</h1>
<p>We evaluate our optimized system on the test set and find that it comes close to human crowd performance (Section 6.1). Next, we analyze its strengths and weaknesses (Section 6.2). Motivated by the observations, we introduce a relaxed setting, where the system may make forecasts selectively (given its identified strengths), and find that our system surpasses the crowd aggregate (Section 6.3). Finally, we demonstrate how our system can be used to complement aggregated human forecasts (Section 6.4).</p>
<h3>6.1 System Nears Human Performance</h3>
<p>We first evaluate the Brier score of our end-to-end system on the test set. Note that all hyperparameters were chosen based on the validation set and all test set questions appear temporally after the validation questions, mirroring the setting of a real-time forecasting competition. In addition to the Brier score, we also report accuracy to compare with past work (Zou et al., 2022; Yan et al., 2024).</p>
<p>As the main result, our averaged Brier score is .179 , while the crowd achieves .149 , resulting in a difference of .03. Our accuracy on the test set is $71.5 \%$, whereas the community scores $77.0 \%$, resulting in a difference of</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Brier Score $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Crowd</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Crowd</td>
</tr>
<tr>
<td style="text-align: center;">Science \&amp; Tech</td>
<td style="text-align: center;">.143 .011</td>
<td style="text-align: center;">.114 .011</td>
<td style="text-align: center;">$82.2_{2.7}$</td>
<td style="text-align: center;">$84.3_{2.6}$</td>
</tr>
<tr>
<td style="text-align: center;">Healthcare \&amp; Biology</td>
<td style="text-align: center;">.074 .015</td>
<td style="text-align: center;">.063 .020</td>
<td style="text-align: center;">$93.8_{4.3}$</td>
<td style="text-align: center;">$90.6_{5.2}$</td>
</tr>
<tr>
<td style="text-align: center;">Economics \&amp; Business</td>
<td style="text-align: center;">.198 .007</td>
<td style="text-align: center;">.147 .009</td>
<td style="text-align: center;">$68.8_{2.1}$</td>
<td style="text-align: center;">$78.3_{1.9}$</td>
</tr>
<tr>
<td style="text-align: center;">Politics \&amp; Governance</td>
<td style="text-align: center;">.172 .006</td>
<td style="text-align: center;">.145 .007</td>
<td style="text-align: center;">$72.6_{1.4}$</td>
<td style="text-align: center;">$78.2_{1.3}$</td>
</tr>
<tr>
<td style="text-align: center;">Education \&amp; Research</td>
<td style="text-align: center;">.163 .024</td>
<td style="text-align: center;">.129 .024</td>
<td style="text-align: center;">$80.6_{6.7}$</td>
<td style="text-align: center;">$77.8_{7.0}$</td>
</tr>
<tr>
<td style="text-align: center;">Arts \&amp; Recreation</td>
<td style="text-align: center;">.221 .010</td>
<td style="text-align: center;">.146 .010</td>
<td style="text-align: center;">$62.4_{2.5}$</td>
<td style="text-align: center;">$76.9_{2.2}$</td>
</tr>
<tr>
<td style="text-align: center;">Security \&amp; Defenses</td>
<td style="text-align: center;">.174 .008</td>
<td style="text-align: center;">.129 .009</td>
<td style="text-align: center;">$71.0_{2.1}$</td>
<td style="text-align: center;">$78.4_{1.9}$</td>
</tr>
<tr>
<td style="text-align: center;">Sports</td>
<td style="text-align: center;">.175 .004</td>
<td style="text-align: center;">.171 .005</td>
<td style="text-align: center;">$73.0_{1.3}$</td>
<td style="text-align: center;">$73.1_{1.3}$</td>
</tr>
<tr>
<td style="text-align: center;">All Categories</td>
<td style="text-align: center;">.179 .003</td>
<td style="text-align: center;">.149 .003</td>
<td style="text-align: center;">$71.5_{7}$</td>
<td style="text-align: center;">$77.0_{7}$</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Platform</th>
<th style="text-align: center;">Brier Score $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Crowd</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Crowd</td>
</tr>
<tr>
<td style="text-align: center;">Metaculus</td>
<td style="text-align: center;">.134 .005</td>
<td style="text-align: center;">.104 .005</td>
<td style="text-align: center;">$80.3_{1.2}$</td>
<td style="text-align: center;">$86.6_{1.1}$</td>
</tr>
<tr>
<td style="text-align: center;">GJOpen</td>
<td style="text-align: center;">.193 .011</td>
<td style="text-align: center;">.157 .013</td>
<td style="text-align: center;">$67.9_{3.4}$</td>
<td style="text-align: center;">$72.6_{3.2}$</td>
</tr>
<tr>
<td style="text-align: center;">INFER</td>
<td style="text-align: center;">.247 .053</td>
<td style="text-align: center;">.310 .086</td>
<td style="text-align: center;">$60.0_{13.1}$</td>
<td style="text-align: center;">$53.3_{13.3}$</td>
</tr>
<tr>
<td style="text-align: center;">Polymarket</td>
<td style="text-align: center;">.172 .005</td>
<td style="text-align: center;">.127 .006</td>
<td style="text-align: center;">$73.6_{1.3}$</td>
<td style="text-align: center;">$79.9_{1.1}$</td>
</tr>
<tr>
<td style="text-align: center;">Manifold</td>
<td style="text-align: center;">.219 .004</td>
<td style="text-align: center;">.200 .005</td>
<td style="text-align: center;">$63.6_{1.3}$</td>
<td style="text-align: center;">$67.9_{1.3}$</td>
</tr>
<tr>
<td style="text-align: center;">All Platforms</td>
<td style="text-align: center;">.179 .003</td>
<td style="text-align: center;">.149 .003</td>
<td style="text-align: center;">$71.5_{7}$</td>
<td style="text-align: center;">$77.0_{7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Results of system evaluation by category (left) and by platform (right). Subscript numbers are 1 standard error. Averaged across all retrieval dates, our optimal system, as described in Section 4, achieves .179 Brier score (human crowd: .149) and accuracy .715 (human crowd: .770).
$5.5 \%$. In comparison with the baseline evaluation (Section 3.4), our system's Brier score (.179) significantly outperforms the best baseline model (.208 with GPT-4-1106-Preview)</p>
<p>In prior work, Zou et al. (2022) evaluated their system on the forecasting dataset Autocast, which consists of questions from 3 of the platforms we use: Metaculus, INFER, and GJOpen. They achieved an accuracy of $65.4 \%$ compared to a community baseline of $92.8 \%$. Yan et al. (2024) later improved this to $67.9 \%$. Our results (Table 4) underscore the significant progress we make in automated forecasting-specifically, we achieve a better accuracy ( $71.5 \%$ ) even though the questions we consider are harder (with a significantly lower crowd accuracy: $77.0 \%$ ).</p>
<p>Further detailed results across different platforms and categories can be found in Table 4. Across categories, our system exhibits noticeable variations: on Sports, our system nearly matches the crowd aggregate, and on Environment \&amp; Energy, it falls much behind. However, we caution against drawing strong conclusions from subcategories, since the sample size is smaller and variation could be due to noise.</p>
<p>Finally, on the test set, we observe again that our system is well calibrated (Figure 3c) with RMS calibration error .42 (human crowd: .38). Interestingly, this is not the case in the baseline evaluations (Section 3.4), where the models are not well calibrated in the zero-shot setting (Figure 3a). Through fine-tuning and ensembling, our system improves the calibration of the base models, without undergoing specific training for calibration.</p>
<h1>6.2 System Strengths and Weaknesses</h1>
<p>We next seek to understand our system's strengths and weaknesses. We will investigate these on the validation set, and later use these insights to improve performance on the test set (Section 6.3).</p>
<p>We find that our system performs best relative to the crowd on the validation set when (1) the crowd is less confident, (2) at earlier retrieval dates, and (3) when it retrieves many articles. Furthermore, we find that our system is well-calibrated.</p>
<p>First, our system significantly outperforms the crowd when the crowd's predictions express high uncertainty. Specifically, when the crowd's predictions are between .3 and .7 , our Brier score is .199 compared to the crowd's .246 . However, our system underperforms the crowd on questions where they are highly certain, likely because it rarely outputs low probabilities (Figure 4b). We hypothesize that this stems from our model's tendency to hedge predictions due to its safety training (see Figure 17 for a qualitative example). Supporting this, our system achieves $7 \%$ higher accuracy on questions where the crowd's prediction is within .05 of 0 or 1 , but the Brier score is worse by .04 .</p>
<p>Next, our system outperforms the crowd on earlier retrieval dates (1, 2, and 3) but not the later ones (4 and 5). Relative to the crowd, our Brier score improves at a slower rate as questions move towards their</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: System strengths. Evaluating on the validation set, we note: (a) When provided enough relevant articles, our system outperforms the crowd. (b) For questions where the crowd is unsure (predictions between .3 and .7), we outperform them (Brier score .199 vs. .246). However, the crowd outperforms our system on questions where they are highly confident, e.g. predicting less than .05. (c) Our system's Brier score is better at the earlier retrieval dates. Finally, our system is well-calibrated (c.f. Figure 3b).
resolution (Figure 4c). This may be due to the aforementioned issue: Our model hedges, even as the evidence becomes more decisive.</p>
<p>With respect to retrieval, our system nears the performance of the crowd when there are at least 5 relevant articles. We further observe that as the number of articles increases, our Brier score improves and surpasses the crowd's (Figure 4a). Intuitively, our system relies on high-quality retrieval, and when conditioned on more articles, it performs better.</p>
<p>Our system is well calibrated on the validation set, with most of the calibration error coming from the system's underconfidence: predictions near 0 are observed to occur less frequently than anticipated, and similarly, events with predictions close to 1 also occur at a higher rate than the model suggests (Figure 3b).</p>
<h1>6.3 System Beats Crowd in the Selective Setting</h1>
<p>In real-word forecasting competitions, forecasters do not have to make predictions on every question in the platform at every possible date. Instead, they typically make predictions on questions where they have expertise or interest in and at times that they choose. Therefore, it is natural to leverage our system's strengths and weaknesses and decide accordingly if we should forecast on a retrieval date $k$ for a question $q$.</p>
<p>Leveraging the insights from Section 6.2, we outperform the crowd by making selective forecasts. Specifically, we report the performance when forecasting only under the conditions identified in Section 6.2:</p>
<ol>
<li>Forecasting only on questions when the crowd prediction falls between .3 and .7. Here, our system attains a Brier score of .238 (crowd aggregate: .240). This comprises $51 \%$ of forecasts and $56 \%$ of questions.</li>
<li>Forecasting only on earlier retrieval dates (1, 2, and 3). Our system's Brier score in this setting is .185 (crowd aggregate: .161). This comprises $66 \%$ of forecasts and $100 \%$ of questions.</li>
<li>Forecasting only when the retrieval system provides at least 5 relevant articles. Under this condition, our system's Brier score is .175 (crowd aggregate: .143). This makes up $84 \%$ of forecasts and $94 \%$ of questions.</li>
<li>Under all three conditions, our system attains Brier score .240 (crowd aggregate: .247). This comprises $22 \%$ of forecasts and $43 \%$ of questions.</li>
</ol>
<p>The gap in Brier score between our system and the crowd shrinks under each heuristic, except the third one (Table 3). Under the first heuristic, we outperform the crowd by a small margin (. 238 vs. .240). This is</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Criteria</th>
<th style="text-align: center;">Brier Score $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">\% Accuracy $\uparrow$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Aggregate</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">Aggregate</td>
</tr>
<tr>
<td style="text-align: center;">Full System</td>
<td style="text-align: center;">.179 .003</td>
<td style="text-align: center;">.146 .002</td>
<td style="text-align: center;">71.5.7</td>
<td style="text-align: center;">77.8.6</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned GPT-4-0613</td>
<td style="text-align: center;">.182 .002</td>
<td style="text-align: center;">.146 .002</td>
<td style="text-align: center;">70.7.7</td>
<td style="text-align: center;">77.4.6</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned GPT-3.5 \&amp; Base GPT-4</td>
<td style="text-align: center;">.181 .002</td>
<td style="text-align: center;">.147 .002</td>
<td style="text-align: center;">70.9.7</td>
<td style="text-align: center;">77.4.6</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tuned GPT-3.5</td>
<td style="text-align: center;">.183 .002</td>
<td style="text-align: center;">.146 .002</td>
<td style="text-align: center;">71.5.7</td>
<td style="text-align: center;">77.4.6</td>
</tr>
<tr>
<td style="text-align: center;">Base GPT-4</td>
<td style="text-align: center;">.186 .002</td>
<td style="text-align: center;">.148 .002</td>
<td style="text-align: center;">70.6.7</td>
<td style="text-align: center;">77.1.6</td>
</tr>
<tr>
<td style="text-align: center;">Base GPT-4; no IR</td>
<td style="text-align: center;">.206 .002</td>
<td style="text-align: center;">.150 .002</td>
<td style="text-align: center;">66.6.7</td>
<td style="text-align: center;">76.9.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study results. The crowd Brier score and accuracy are .146 and $77.0 \%$, respectively. "Aggregate" indicates the weighted average of our system with the crowd prediction. Our full system uses fine-tuned GPT-4-0613 and base GPT-4-1106-Preview (row 1). The system yields similar performance with fine-tuned GPT-3.5 (rows 3-4). Our system exhibits poorer performance without a fine-tuned reasoning model (row 5), and further declines with neither retrieval nor a fine-tuned reasoning model (row 6). Subscript numbers represent one standard error. We bold entries that surpass the crowd aggregate.
valuable as our system can be used to complement the crowd's prediction when there is greater uncertainty. When all three conditions are jointly met, our system beats the crowd significantly (by more than 1.5 standard errors in both Brier score and accuracy).</p>
<h1>6.4 System Complements the Crowd</h1>
<p>Finally, we show that aggregates of our system with the crowd forecasts outperform either one in isolation.
Combining the system's predictions with the crowd using a weighted average-4x weight for the crowd, which we find optimal on the validation set-improves the overall Brier score from .149 to .146 on the full test set (Table 3, top row).
Moreover, our system excels under certain criteria (Section 6.2). It is especially useful in these cases to supplement the crowd prediction. We report these results in Table 3 as well, using an unweighted average (instead of the weighted average above). This outperforms the crowd prediction in all cases: For example, the crowd Brier score is .24 when the prediction is between .3 and .7 , while the system achieves .237 .
Finally, beyond direct score improvements, our system can potentially aid human forecasters by providing effective news retrieval and novel perspectives in reasoning drawn from LM pre-training knowledge. We leave it as a future direction to explore how our system can interactively assist human forecasters.</p>
<h2>7 Ablations</h2>
<p>We conduct 3 ablation studies. The first validates that our performance is not solely due to the power of GPT-4. The last two show the benefits of our retrieval and fine-tuning methods.</p>
<p>Fine-tuning a less capable model. To demonstrate that our system's performance does not hinge on the ability of the base model (i.e., GPT-4), we fine-tune GPT-3.5 on all our fine-tuning data (13,253 samples).
We replace fine-tuned GPT-4 in our system with fine-tuned GPT-3.5, and evaluate using the same methodology as in Section 6.1. We find here that our Brier score is only slightly worse: .182 compared to the previous score of .179 .
No fine-tuning. To demonstrate the gain from fine-tuning (Section 5.1), we evaluate our optimal system, except we only use base GPT-4-Preview-1106 as the reasoning model.
In this setup, the ablated system achieves a Brier score of .186 , which increased on the original score by .007 .
Overall, the results suggest that fine-tuning the reasoning model yields a significant boost to our system's performance.</p>
<p>No fine-tuning and no retrieval. We evaluate our optimal system without any news retrieval and using the base GPT-4-1106-Preview model. The ablated system attains a Brier score of .206.</p>
<p>Recall that in our baseline evaluation (Section 3.4), the lowest Brier score attained by any model is .208. Our ablated system essentially deteriorates to this baseline level. Indeed, without any fine-tuning or retrieval, the only expected advantage of our system over the baseline evaluation setup is its reasoning prompt, found through searching a set of candidate prompts (Section 5). The experiment suggests that this gives fairly a minor improvement.</p>
<h1>8 Conclusion</h1>
<p>Our work presents the first ML system that can forecast at near human levels. We develop a novel retrieval mechanism that uses a LM to determine which information to source and how to evaluate its relevance. We also give a self-supervised fine-tuning method to generate reasonings with accurate predictions.</p>
<p>To facilitate further research, we release our dataset: the largest and most recent forecasting dataset compiled from 5 real-world forecasting competitions. We discuss a few opportunities to improve these systems further.</p>
<p>Iterative self-supervision. With a larger training corpus, our self-supervised fine-tuning approach can be used for iterative self-improvement. Specifically, after fine-tuning a model on its previous optimal predictions and reasonings, we can generate more fine-tuning data by using the same model again, which can be repeated until training data is exhausted.</p>
<p>Data. While our forecasting benchmark is a good initial corpus to train a system, we believe that it is possible to use LMs with later training cut-offs to teach an earlier LM. This could be done by using later LMs to generate questions it knows the answer to but an earlier LM does not (postdiction). In addition, while we source questions from forecasting platforms, it is possible to collect historical data in the wild and re-formulate them as forecasting questions, leading to a larger training set.</p>
<p>Domain-adaptive training. In Section B.3, we observe that in the baseline evaluations, the Brier scores across categories are correlated with models' pre-training knowledge. This suggests that we may be able to specialize models to areas of particular interests by fine-tuning them on domain knowledge.</p>
<p>LMs get better at forecasting naturally. We observe that as LMs improve, they naturally also become better at forecasting. In particular, in Section 3.4, we see that newer generations of models forecast better than older ones. For example, GPT-4-1106, released in 2023, outperforms GPT-4-0613, released in 2021, by .02 with respect to the Brier score. If we were to have fine-tuned the more recent model, we would expect better performance.</p>
<p>At a high level, our results suggest that in the near future, LM-based systems may be able to generate accurate forecasts at the level of competitive human forecasters. We hope that our work paves the way for automated, scalable forecasting that can help to inform institutional decision making.</p>
<h2>Acknowledgments</h2>
<p>We thank Jean-Stanislas Denain, Erik Jones, Ezra Karger, Jacob Pfau and Ruiqi Zhong for helpful discussions, and Jean-Stanislas Denain, Owain Evans, Dan Hendrycks, Horace He and Andy Zou for comments and feedbacks on an early draft of the paper. DH was supported by an award from the C3.ai Digital Transformation Institute. FZ was supported by NSF award CCF-2311648. JS was supported by the National Science Foundation SaTC CORE Award No. 1804794 and the Simons Foundation.</p>
<h2>References</h2>
<p>Abolghasemi, M., Ganbold, O., and Rotaru, K. (2023). Humans vs large language models: Judgmental forecasting in an era of advanced AI. arXiv preprint arXiv:2312.06941.</p>
<p>Adam, D. (2020). Special report: The simulations driving the world's response to COVID-19. Nature, $580(7802): 316-319$.</p>
<p>Anthropic (2023). Model card and evaluations for Claude models. https://www-cdn. anthropic.com/files/ 4zrzovbb/website/5c49cc247484cecf107c699baf29250302e5da70.pdf.</p>
<p>Armstrong, J. S. (2001). Principles of Forecasting: a Handbook for Researchers and Practitioners. Springer.
Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly Weather Review, $78(1): 1-3$.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Chen, X., Aksitov, R., Alon, U., Ren, J., Xiao, K., Yin, P., Prakash, S., Sutton, C., Wang, X., and Zhou, D. (2023). Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311.</p>
<p>Das, A., Kong, W., Sen, R., and Zhou, Y. (2024). A decoder-only foundation model for time-series forecasting. arXiv preprint arXiv:2310.10688.</p>
<p>Deng, Y., Zhang, W., Chen, Z., and Gu, Q. (2023). Rephrase and respond: Let large language models ask better questions for themselves. arXiv preprint arXiv:2311.04205.</p>
<p>Dooley, S., Khurana, G. S., Mohapatra, C., Naidu, S. V., and White, C. (2023). ForecastPFN: Syntheticallytrained zero-shot forecasting. In Advanced in Neural Information Processing Systems (NeurIPS).</p>
<p>Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung, P., Roscher, R., Shahzad, M., Yang, W., Bamler, R., and Zhu, X. X. (2021). A survey of uncertainty in deep neural networks. arXiv preprint arXiv:2107.03342.</p>
<p>Gemini Team (2023). Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805.
Gneiting, T. and Raftery, A. E. (2007). Strictly proper scoring rules, prediction, and estimation. Journal of the American Statistical Association, 102(477):359-378.</p>
<p>Gruver, N., Finzi, M. A., Qiu, S., and Wilson, A. G. (2023). Large language models are zero-shot time series forecasters. In Advanced in Neural Information Processing Systems (NeurIPS).</p>
<p>Hanson, R. (2007). Logarithmic markets coring rules for modular combinatorial information aggregation. The Journal of Prediction Markets, 1(1):3-15.</p>
<p>Hendrycks, D., Carlini, N., Schulman, J., and Steinhardt, J. (2021). Unsolved problems in ML safety. arXiv preprint arXiv:2109.13916.</p>
<p>Izacard, G. and Grave, É. (2021). Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</p>
<p>Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. (2024). Mixtral of experts. arXiv preprint arXiv:2401.04088.</p>
<p>Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q. (2024). Time-LLM: Time series forecasting by reprogramming large language models. In International Conference on Learning Representations (ICLR).</p>
<p>Jin, W., Khanna, R., Kim, S., Lee, D.-H., Morstatter, F., Galstyan, A., and Ren, X. (2021). ForecastQA: A question answering challenge for event forecasting with temporal text data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL).</p>
<p>Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., and Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<p>Manifold (2022). Maniswap. https://manifoldmarkets.notion.site/manifoldmarkets/ Maniswap-ce406e1e897d417cbd491071ea8a0c39.</p>
<p>Metaculus (2023). Wisdom of the crowd vs. the best of the best of the best. https://www.metaculus.com/ notebooks/15760/wisdom-of-the-crowd-vs-the-best-of-the-best-of-the-best.</p>
<p>Min, S., Zhong, V., Zettlemoyer, L., and Hajishirzi, H. (2019). Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<p>Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. (2021). WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. (2023). A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations (ICLR).</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. (2021). Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>OpenAI (2023). GPT-4 technical report. arXiv preprint arXiv:2303.08774.
Polymarket (2023). Polymarket/poly-market-maker: Market Maker Keeper for the polymarket CLOB. https://github.com/Polymarket/poly-market-maker.</p>
<p>Rasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Biloš, M., Ghonia, H., Hassen, N. V., Schneider, A., Garg, S., Drouin, A., Chapados, N., Nevmyvaka, Y., and Rish, I. (2023). Lag-Llama: Towards foundation models for time series forecasting. arXiv preprint arXiv:2310.08278.</p>
<p>Schoenegger, P. and Park, P. S. (2023). Large language model prediction capabilities: Evidence from a real-world forecasting tournament. arXiv preprint arXiv:2310.13014.</p>
<p>Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J. (2021). Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics (Findings of EMNLP).</p>
<p>Tetlock, P. E. and Gardner, D. (2015). Superforecasting: The Art and Science of Prediction. Crown.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Wang, C. (2023). Calibration in deep learning: A survey of the state-of-the-art. arXiv preprint arXiv:2308.01222.</p>
<p>Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D. (2024). Unified training of universal time series forecasting transformers. arXiv preprint arXiv:2402.02592.</p>
<p>Yan, Q., Seraj, R., He, J., Meng, L., and Sylvain, T. (2024). Autocast++: Enhancing world event prediction with zero-shot ranking-based context retrieval. In International Conference on Learning Representations (ICLR).</p>
<p>Zhang, Y., Chen, X., and Park, D. (2018). Formal specification of constant product (xy= k) market maker model and implementation. White paper.</p>
<p>Zhu, Y., Yuan, H., Wang, S., Liu, J., Liu, W., Deng, C., Dou, Z., and Wen, J.-R. (2024). Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107.</p>
<p>Zou, A., Xiao, T., Jia, R., Kwon, J., Mazeika, M., Li, R., Song, D., Steinhardt, J., Evans, O., and Hendrycks, D. (2022). Forecasting future world events with neural networks. In Advances in Neural Information Processing Systems (NeurIPS).</p>
<h1>A Details about Models and Knowledge Accuracy</h1>
<h2>A. 1 Models</h2>
<p>We give a list of detailed information of the models we use below. The weights of the open models are available publicly on Hugging Face, and we primarily use Together AI's serving API to access them. All cut-offs are based on official statements.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Open Weights</th>
<th style="text-align: center;">Knowledge Cut-off</th>
<th style="text-align: right;">Evaluation Cost</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4-1106-Preview</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Apr 2023</td>
<td style="text-align: right;">$\$ 0.01 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (GPT-4-0613)</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Sep 2021</td>
<td style="text-align: right;">$\$ 0.03 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo-Instruct</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Sep 2021</td>
<td style="text-align: right;">$\$ 0.0015 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo-1106</td>
<td style="text-align: left;">OpenAI</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Sep 2021</td>
<td style="text-align: right;">$\$ 0.001 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Claude-1</td>
<td style="text-align: left;">Anthropic</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Dec 2022</td>
<td style="text-align: right;">$\$ 0.024 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2</td>
<td style="text-align: left;">Anthropic</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Dec 2022</td>
<td style="text-align: right;">$\$ 0.024 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2.1</td>
<td style="text-align: left;">Anthropic</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Dec 2022</td>
<td style="text-align: right;">$\$ 0.024 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7B-Chat</td>
<td style="text-align: left;">Meta</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Sep 2022</td>
<td style="text-align: right;">$\$ 0.0002 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13B-Chat</td>
<td style="text-align: left;">Meta</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Sep 2022</td>
<td style="text-align: right;">$\$ 0.00025 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B-Chat</td>
<td style="text-align: left;">Meta</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Sep 2022</td>
<td style="text-align: right;">$\$ 0.0009 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: left;">Mistral AI</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: right;">$\$ 0.0002 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-8x7B-Instruct</td>
<td style="text-align: left;">Mistral AI</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: right;">$\$ 0.0002 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B-DPO</td>
<td style="text-align: left;">NousResearch</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: right;">$\$ 0.0002 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">YI-34B-Chat</td>
<td style="text-align: left;">01.AI</td>
<td style="text-align: left;">Yes</td>
<td style="text-align: center;">June 2023</td>
<td style="text-align: right;">$\$ 0.000776 / 1 \mathrm{~K}$ tokens</td>
</tr>
<tr>
<td style="text-align: left;">Gemini-Pro</td>
<td style="text-align: left;">Google</td>
<td style="text-align: left;">No</td>
<td style="text-align: center;">Early 2023</td>
<td style="text-align: right;">$\$ 0.0005 / 1 \mathrm{~K}$ characters</td>
</tr>
</tbody>
</table>
<p>Table 6: Overview of the LMs we evaluate: A breakdown of the LMs used in our study, including their sources, availability of weights, knowledge cut-off dates, and evaluation costs. The evaluation costs of the open-weight models are based on Together AI's pricing. The knowledge cut-off of Gemini-Pro is claimed to be early 2023 ( $\sim$ April 2023). We are not aware of the exact knowledge cut-offs of the Mistral series, as it is not publicly reported.</p>
<h2>A. 2 Testing Potential Leakage from Post-training</h2>
<p>GPT-4-1106-Preview and GPT-3.5-Turbo-1106, the two models we use in our system, were released in November, 2023. We find no evidence that the post-training phase leaks further information after their knowledge cut-offs (April, 2023 and January, 2021). As a test, we manually query the model on 20 major events in June, 2023-September, $2023^{3}$, such as "Who won the 2023 Turkish presidential election?". For all 20 questions, both models either claim no knowledge or simply hallucinate.</p>
<p>As a sanity check, we also prompt GPT-4-1106-Preview to answer another 20 questions about events during November, 2022-January, 2023, prior to its knowledge cut-off, such as "Which team won the 2022 FIFA World Cup Final?". The model answers all of them correctly.</p>
<h2>A. 3 Crowd Predictions</h2>
<p>On any given question, each platform computes a community prediction that aggregates all individual forecasts. The prediction is dynamically updated and recorded as the forecasts are made. We source the records directly from the platforms (instead of computing them from scratch using the individual forecasts). For binary questions, we provide more details on the aggregation mechanisms as follows.</p>
<ul>
<li>On Metaculus, for a given question, each prediction of a forecaster is marked by $t$ (starting at 1), from their earliest prediction to the latest. The platform computes the crowd prediction of the question by weighted median. The weight of the $t$ th forecast from an individual forecaster is $e^{\sqrt{t}}$, so the more recent</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>forecasts receive higher weights. We remark that the platform also publishes another aggregated forecast called "Metaculus prediction" (which we do not use or compare with in this paper). This differs from the crowd prediction described above and is computed via a proprietary algorithm.</p>
<ul>
<li>GJOpen computes the crowd predictions by the mean of the most recent $40 \%$ of the forecasts from each forecaster.</li>
<li>INFER initializes the crowd prediction to be the mean of all individual forecasts. As the question progresses, it reweights the forecasts, for example, by "putting more weight on the forecasts of individuals with the best track record." ${ }^{4}$ Exact details on the aggregation mechanisms are not found on their website.</li>
<li>Manifold and Polymarket are prediction markets, where the community predictions are the prices (between 0 and 1). The prices are adjusted by their automated market makers, as bets are made. The mechanisms are variants of constant-product market makers (Hanson, 2007; Zhang et al., 2018); see Polymarket (2023); Manifold (2022) for more details.</li>
</ul>
<h1>B Details about Base Evaluations</h1>
<p>In this section, we provide experimental details on our baseline evaluations (Section 3.4).</p>
<h2>B. 1 Evaluation Method</h2>
<p>For both zero-shot and scratchpad prompting, we conduct basic prompt optimization by by crafting 5 candidate zero-shot prompts and 4 candidate scratchpad prompts. We evaluate each prompt on the validation set by comparing Brier scores. Specifically, we randomly select 200 questions from the validation set and calculate the mean Brier scores across the 14 LMs under consideration.</p>
<ul>
<li>The best zero-shot prompt achieves an average Brier score of 0.246 , outperforming the others, which score $0.261,0.276,0.279$, and 0.252 , respectively.</li>
<li>For scratchpad, all prompts yield similar Brier scores. We observe that potentially due to safety training, models can sometimes refuse to answer forecasting questions by simply claiming "I don't know". Therefore, we use the number of "refuse to answer" responses as the deciding metric. The winning scratchpad prompt averages 88 "refuse to answer" responses, while the others average 106, 93, and 94, respectively.</li>
</ul>
<p>The best zero-shot and scratchpad prompts are shown in Figure 5 and Figure 6. In both prompting styles, models are only provided with the question, background, resolution criterion, and question's open and close dates (date_begin and date_end). All the data are sourced from the forecasting platforms and publicly available on the question page to human forecasters. We do no additional news retrieval.</p>
<p>Finally, we use the best prompt of each prompting strategy to forecast on each question in the test set. In Section 3.4, we find that none of the models are naturally good at forecasting. We provide the full results next in Section B.2.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>You are an expert superforecaster, familiar with the work of Tetlock and others. Make a prediction of the probability that the question will be resolved as true. You MUST give a probability estimate between 0 and 1 UNDER ALL CIRCUMSTANCES. If for some reason you can't answer, pick the base rate, but return a number between 0 and 1 .</p>
<p>Question: {question}
Question Background: {background}
Resolution Criteria: {resolution_criteria}</p>
<p>Today's date: {date_begin}
Question close date: {date_end}</p>
<p>Output your answer (a number between 0 and 1) with an asterisk at the beginning and end of the decimal. Do not output anything else.
Answer: ${{$ Insert answer here $}}$</p>
<p>Figure 5: The simple zero-shot prompt used for baseline evaluations. No retrieval is performed. The prompt simply asks the model to make a prediction on a given question from the test set. We add the directive "You MUST ... UNDER ALL CIRCUMSTANCES" to push the model to answer the question, which in some cases it refuses to, potentially due to safety training. See Section 3.4 for results and Appendix B for more details.</p>
<div class="codehilite"><pre><span></span><code>Question: {question}
Question Background:{background}
Resolution Criteria:{resolution_criteria}
</code></pre></div>

<p>Today's date: {date_begin}
Question close date: {date_end}</p>
<p>Instructions:</p>
<ol>
<li>Provide reasons why the answer might be no.
{{ Insert your thoughts $}}$</li>
<li>Provide reasons why the answer might be yes.
${{$ Insert your thoughts $}}$</li>
<li>Aggregate your considerations.
${{$ Insert your aggregated considerations $}}$</li>
<li>Output your answer (a number between 0 and 1) with an asterisk at the beginning and end of the decimal.
${{$ Insert your answer $}}$</li>
</ol>
<p>Figure 6: The scratchpad prompt used for baseline evaluations. No retrieval is performed. The prompt asks the model to make a prediction on a given question from the test set, after making considerations for yes and no. See Section 3.4 for results and Appendix B for more details.</p>
<h1>B. 2 Baseline Evaluation Results</h1>
<p>We now give the full results of our baseline evaluation (Section 3.4) in Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Zero-shot</th>
<th style="text-align: center;">Scratchpad</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">$0.237(0.014)$</td>
<td style="text-align: center;">$0.257(0.009)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo-1106</td>
<td style="text-align: center;">$0.274(0.016)$</td>
<td style="text-align: center;">$0.261(0.010)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (GPT-4-0613)</td>
<td style="text-align: center;">$0.219(0.013)$</td>
<td style="text-align: center;">$0.222(0.009)$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4-1106-Preview</td>
<td style="text-align: center;">$\mathbf{0 . 2 0 8}(0.013)$</td>
<td style="text-align: center;">$\mathbf{0 . 2 0 9}(0.012)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-7B</td>
<td style="text-align: center;">$0.353(0.020)$</td>
<td style="text-align: center;">$0.264(0.011)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-13B</td>
<td style="text-align: center;">$0.226(0.009)$</td>
<td style="text-align: center;">$0.268(0.008)$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-2-70B</td>
<td style="text-align: center;">$0.283(0.014)$</td>
<td style="text-align: center;">$0.282(0.011)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B-Instruct</td>
<td style="text-align: center;">$0.237(0.018)$</td>
<td style="text-align: center;">$0.243(0.008)$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-8x7B-Instruct</td>
<td style="text-align: center;">$0.238(0.018)$</td>
<td style="text-align: center;">$0.238(0.010)$</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B-DPO</td>
<td style="text-align: center;">$0.260(0.022)$</td>
<td style="text-align: center;">$0.248(0.010)$</td>
</tr>
<tr>
<td style="text-align: left;">Yi-34B-Chat</td>
<td style="text-align: center;">$0.238(0.012)$</td>
<td style="text-align: center;">$0.241(0.009)$</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2</td>
<td style="text-align: center;">$0.220(0.013)$</td>
<td style="text-align: center;">$0.219(0.014)$</td>
</tr>
<tr>
<td style="text-align: left;">Claude-2.1</td>
<td style="text-align: center;">$0.220(0.013)$</td>
<td style="text-align: center;">$0.215(0.014)$</td>
</tr>
<tr>
<td style="text-align: left;">Gemini-Pro</td>
<td style="text-align: center;">$0.243(0.019)$</td>
<td style="text-align: center;">$0.230(0.007)$</td>
</tr>
</tbody>
</table>
<p>Table 7: Zero-shot and scratchpad Brier scores on the test set: Brier scores under zero-shot or scratchpad prompts, with 2 standard error (SE) values. Lower is better. Random baseline: 0.250 ; human crowd: 0.149 . All models fall significantly far from human aggregate.</p>
<h2>B. 3 Knowledge Evaluation by Category</h2>
<p>We present an evaluation of model's knowledge about resolved questions on past events and notice variations in performance across categories. To investigate further, we analyzed each model's zero-shot Brier score on the test set by category. This analysis showed a correlation between models' knowledge on the training and validation sets and their Brier scores on the test set across categories. This suggests that domain-adaptive training could be used to improve model performance in categories where its existing knowledge is limited.</p>
<p>First, we assessed pre-trained language model knowledge across categories by evaluating their ability to answer resolved forecasting questions from the train and validation sets. See Table 8 for the results and Figure 7 for the knowledge prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Arts \&amp; Recreation</th>
<th style="text-align: center;">Economics \&amp; Business</th>
<th style="text-align: center;">Education \&amp; Research</th>
<th style="text-align: center;">Environment \&amp; Energy</th>
<th style="text-align: center;">Healthcare \&amp; Biology</th>
<th style="text-align: center;">Politics \&amp; Governance</th>
<th style="text-align: center;">Science \&amp; Tech</th>
<th style="text-align: center;">Security \&amp; Defense</th>
<th style="text-align: center;">Social Sciences</th>
<th style="text-align: center;">Sports</th>
<th style="text-align: center;">Other</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.561</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.311</td>
<td style="text-align: center;">0.454</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.461</td>
<td style="text-align: center;">0.465</td>
<td style="text-align: center;">0.597</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo-1106</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (GPT-4-0613)</td>
<td style="text-align: center;">0.603</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.492</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.377</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.651</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-1106-Preview</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.159</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.487</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-7B</td>
<td style="text-align: center;">0.642</td>
<td style="text-align: center;">0.809</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.081</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13B</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.447</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">0.277</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.486</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-70B</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.229</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.075</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-7B-Instruct</td>
<td style="text-align: center;">0.611</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.191</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.27</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.018</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Mistral-8x7B-Instruct</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.091</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.071</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Mixtral-8x7B-DPO</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.063</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.069</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-Pro</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.136</td>
<td style="text-align: center;">0.124</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2.1</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.205</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2.1</td>
<td style="text-align: center;">0.126</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.405</td>
<td style="text-align: center;">0.071</td>
</tr>
<tr>
<td style="text-align: center;">Gemini-Pro</td>
<td style="text-align: center;">0.155</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.077</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.07</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison of knowledge accuracy across categories and models on the train and validation sets. We list the knowledge accuracy of all base models with respect to all categories in the train and validation set.</p>
<p>We noticed variations in knowledge accuracy across categories. To dig deeper, we analyze the zero-shot Brier score on the test set in Table 9 and assess if there is a correlation between knowledge accuracy on the training and validation sets and zero-shot Brier score on the test set in Table 10.</p>
<p>The potential for domain-adaptive training. We calculate the correlation between the models' knowledge accuracy and their Brier scores of the zero-shot evaluation. Notably, in the Politics \&amp; Governance, Arts \&amp; Recreation, and Education \&amp; Research categories, there exists a strong negative correlation. See the below</p>
<p>Question: {question}
The question was posed on {date_begin} and closed on {date_end}.
Instructions:</p>
<ul>
<li>Please output ' 1 ' if the answer is 'Yes', ' 0 ' if the answer is 'No' or 'IDK' if you don't know the answer. Do not return anything else.</li>
<li>Do not guess.</li>
</ul>
<p>Answer: ${{$ Insert answer here $}}$</p>
<p>Figure 7: The prompt used for evaluating model's knowledge about forecasting questions. It asks the model to answer "Yes" or "No" given its pre-training knowledge and also allows for "IDK" ("I don't know"). See Section B. 3 for the results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Arts \&amp; Recreation</th>
<th style="text-align: center;">Economics \&amp; Business</th>
<th style="text-align: center;">Education \&amp; Research</th>
<th style="text-align: center;">Environment \&amp; Energy</th>
<th style="text-align: center;">Healthcare \&amp; Biology</th>
<th style="text-align: center;">Politics \&amp; Governance</th>
<th style="text-align: center;">Science \&amp; Tech</th>
<th style="text-align: center;">Security \&amp; Defense</th>
<th style="text-align: center;">Sports</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-3.5-Tarbo</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.281</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.178</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.205</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Tarbo-1106</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.343</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.214</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (GPT-4-0613)</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.178</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4-1106-Partive</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.163</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.158</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.167</td>
<td style="text-align: center;">0.173</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-1B</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.431</td>
<td style="text-align: center;">0.359</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.327</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-13B</td>
<td style="text-align: center;">0.260</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.199</td>
</tr>
<tr>
<td style="text-align: center;">Llama-2-550</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.290</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.308</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.212</td>
</tr>
<tr>
<td style="text-align: center;">Metrol-70-Instrset</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.191</td>
</tr>
<tr>
<td style="text-align: center;">Metrol-8-70-Instrset</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.373</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.356</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.353</td>
<td style="text-align: center;">0.179</td>
<td style="text-align: center;">0.227</td>
<td style="text-align: center;">0.189</td>
</tr>
<tr>
<td style="text-align: center;">Metrol-8-70-DP(1)</td>
<td style="text-align: center;">0.367</td>
<td style="text-align: center;">0.315</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: center;">YI-50D-Chat</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.212</td>
</tr>
<tr>
<td style="text-align: center;">Chiatlo-2</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.184</td>
</tr>
<tr>
<td style="text-align: center;">Chiatlo-2.1</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.194</td>
</tr>
<tr>
<td style="text-align: center;">Gemin-1-11v</td>
<td style="text-align: center;">0.381</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.263</td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.189</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of zero-shot Brier scores across categories and models on the test set. This table lists the Brier scores of all base models with respect to the specified categories.</p>
<p>Table 10 for the correlation table. This negative correlation is expected because a higher knowledge accuracy should intuitively correspond to a lower Brier score. As a direction for future research, we propose that domain-adaptive training could be employed to enhance forecasting performance in specific categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Arts \&amp; Recreation</td>
<td style="text-align: right;">-0.417103</td>
</tr>
<tr>
<td style="text-align: left;">Economics \&amp; Business</td>
<td style="text-align: right;">-0.228040</td>
</tr>
<tr>
<td style="text-align: left;">Education \&amp; Research</td>
<td style="text-align: right;">-0.359102</td>
</tr>
<tr>
<td style="text-align: left;">Environment \&amp; Energy</td>
<td style="text-align: right;">-0.135552</td>
</tr>
<tr>
<td style="text-align: left;">Healthcare \&amp; Biology</td>
<td style="text-align: right;">0.162110</td>
</tr>
<tr>
<td style="text-align: left;">Politics \&amp; Governance</td>
<td style="text-align: right;">-0.487266</td>
</tr>
<tr>
<td style="text-align: left;">Science \&amp; Tech</td>
<td style="text-align: right;">-0.091878</td>
</tr>
<tr>
<td style="text-align: left;">Security \&amp; Defense</td>
<td style="text-align: right;">-0.183253</td>
</tr>
<tr>
<td style="text-align: left;">Sports</td>
<td style="text-align: right;">-0.136017</td>
</tr>
</tbody>
</table>
<p>Table 10: Correlation between knowledge accuracy and zero-shot prompt Brier score by category. Categories with an absolute correlation of 0.3 or greater, shown in bold, indicate a high correlation between accuracy on the training and validation set and forecasting performance on the test set. This highlights that in certain domains model's forecasting capabilities are correlated with its pre-training knowledge.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://www.infer-pub.com/frequently-asked-questions&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
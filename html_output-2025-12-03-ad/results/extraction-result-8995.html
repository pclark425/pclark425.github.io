<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8995 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8995</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8995</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-273507329</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.16285v1.pdf" target="_blank">Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?</a></p>
                <p><strong>Paper Abstract:</strong> The current paper presents the development and validation of Self-Score , a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks. Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers. The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system. The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG out-perform those without. All automated LLM agents were observed to perform better than the human control group. Given these re-sults, the study raises concerns about the potential displacement of human workers, especially in areas where AI technologies excel. Ul-timately, SelfScore provides a foundational tool for understanding the impact of AI in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8995.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8995.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CogEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A published benchmark (CogEval) that aims to evaluate large language models' abilities to form cognitive maps and perform planning-related tasks; cited in the paper as related work on cognitive evaluation of LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CogEval (cognitive maps & planning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Benchmark testing LLMs on tasks requiring cognitive mapping and planning (spatial and sequential planning/problem-solving domains).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>This paper only cites CogEval in Related Work; no experimental details, quantitative results, or comparisons from CogEval are reported in the current paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8995.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8995.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logical/Critical-thinking benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmarks focusing on logical and critical thinking (as cited in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper refers to existing benchmark suites that evaluate LLMs on logical and critical thinking problems and notes that such benchmarks report LLM weaknesses in these domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Logical / critical thinking benchmark(s)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Collections of tasks designed to probe logical reasoning and critical thinking capabilities of LLMs (e.g., multi-step reasoning, logic puzzles, inference tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described qualitatively in this paper as 'LLMs struggle' on logical and critical thinking benchmarks; no numerical scores provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper reports that LLMs perform worse on logical/critical-thinking tasks (i.e., below expected human-level competence) according to cited benchmarks, but no direct numeric comparison is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>No experimental protocols or prompt/test adaptations are provided in this paper for those benchmarks; the statements are part of the Related Work summary citing other studies ([32,44] in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The current paper does not reproduce or report primary data from these benchmarks; it only references them to motivate complexity weighting. The paper emphasizes that different LLMs and training data yield differing performance and that the cited observations are qualitative summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval <em>(Rating: 2)</em></li>
                <li>Easy Problems That LLMs Get Wrong <em>(Rating: 2)</em></li>
                <li>Evaluating LLMs at Detecting Errors in LLM Responses <em>(Rating: 2)</em></li>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 1)</em></li>
                <li>Evaluating Language-Model Agents on Realistic Autonomous Tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8995",
    "paper_id": "paper-273507329",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "CogEval",
            "name_full": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "brief_description": "A published benchmark (CogEval) that aims to evaluate large language models' abilities to form cognitive maps and perform planning-related tasks; cited in the paper as related work on cognitive evaluation of LLMs.",
            "citation_title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "CogEval (cognitive maps & planning tasks)",
            "test_description": "Benchmark testing LLMs on tasks requiring cognitive mapping and planning (spatial and sequential planning/problem-solving domains).",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "experimental_details": null,
            "limitations_or_caveats": "This paper only cites CogEval in Related Work; no experimental details, quantitative results, or comparisons from CogEval are reported in the current paper.",
            "uuid": "e8995.0",
            "source_info": {
                "paper_title": "Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Logical/Critical-thinking benchmarks",
            "name_full": "Benchmarks focusing on logical and critical thinking (as cited in this paper)",
            "brief_description": "The paper refers to existing benchmark suites that evaluate LLMs on logical and critical thinking problems and notes that such benchmarks report LLM weaknesses in these domains.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "test_battery_name": "Logical / critical thinking benchmark(s)",
            "test_description": "Collections of tasks designed to probe logical reasoning and critical thinking capabilities of LLMs (e.g., multi-step reasoning, logic puzzles, inference tasks).",
            "llm_performance": "Described qualitatively in this paper as 'LLMs struggle' on logical and critical thinking benchmarks; no numerical scores provided here.",
            "human_baseline_performance": null,
            "performance_comparison": "Paper reports that LLMs perform worse on logical/critical-thinking tasks (i.e., below expected human-level competence) according to cited benchmarks, but no direct numeric comparison is provided in this paper.",
            "experimental_details": "No experimental protocols or prompt/test adaptations are provided in this paper for those benchmarks; the statements are part of the Related Work summary citing other studies ([32,44] in the paper).",
            "limitations_or_caveats": "The current paper does not reproduce or report primary data from these benchmarks; it only references them to motivate complexity weighting. The paper emphasizes that different LLMs and training data yield differing performance and that the cited observations are qualitative summaries.",
            "uuid": "e8995.1",
            "source_info": {
                "paper_title": "Assessing the Performance of Human-Capable LLMs - Are LLMs Coming for Your Job?",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "rating": 2,
            "sanitized_title": "evaluating_cognitive_maps_and_planning_in_large_language_models_with_cogeval"
        },
        {
            "paper_title": "Easy Problems That LLMs Get Wrong",
            "rating": 2,
            "sanitized_title": "easy_problems_that_llms_get_wrong"
        },
        {
            "paper_title": "Evaluating LLMs at Detecting Errors in LLM Responses",
            "rating": 2,
            "sanitized_title": "evaluating_llms_at_detecting_errors_in_llm_responses"
        },
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 1,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Evaluating Language-Model Agents on Realistic Autonomous Tasks",
            "rating": 1,
            "sanitized_title": "evaluating_languagemodel_agents_on_realistic_autonomous_tasks"
        }
    ],
    "cost": 0.00862025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Assessing the Performance of Human-Capable LLMs -Are LLMs Coming for Your Job?
5 Oct 2024</p>
<p>John Mavi johnmavi08@gmail.com 
Luxembourg Tech School A</p>
<p>Sergio Coronado sergio.coronadoarrechedera@men.lu 
Luxembourg Tech School A</p>
<p>Assessing the Performance of Human-Capable LLMs -Are LLMs Coming for Your Job?
5 Oct 2024C8CBCA95C3B07550C75196BEE3193682arXiv:2410.16285v1[cs.CY]BenchmarkLarge Language ModelsLLM Agents
The current paper presents the development and validation of Self-Score, a novel benchmark designed to assess the performance of automated Large Language Model (LLM) agents on help desk and professional consultation tasks.Given the increasing integration of AI in industries, particularly within customer service, SelfScore fills a crucial gap by enabling the comparison of automated agents and human workers.The benchmark evaluates agents on problem complexity and response helpfulness, ensuring transparency and simplicity in its scoring system.The study also develops automated LLM agents to assess SelfScore and explores the benefits of Retrieval-Augmented Generation (RAG) for domain-specific tasks, demonstrating that automated LLM agents incorporating RAG outperform those without.All automated LLM agents were observed to perform better than the human control group.Given these results, the study raises concerns about the potential displacement of human workers, especially in areas where AI technologies excel.Ultimately, SelfScore provides a foundational tool for understanding the impact of AI in help desk environments while advocating for ethical considerations in the ongoing transition towards automation.</p>
<p>Introduction</p>
<p>Automation and efficiency have long been inextricable pursuits of scientific research and societal advancement.During the Industrial Revolution of the 18th and 19th centuries, the development and proliferation of steam power and advancements in production techniques facilitated the widespread establishment of the emerging manufacturing economy, as mass production became easier and more efficient.Later, starting in the 1940s and 1950s, the transition from analog to digital technologies began, gaining momentum in the following decades.With it, unprecedented computational power became more widely available, trivializing previously laborintensive (or even resource-prohibitive) tasks.Now, the precipice of a new technological revolution looms ahead.With the increasing availability of massive amounts of data, Artificial Intelligence (AI) technologies are experiencing an explosion in their efficacy, their applicability, and their accessibility.Chat-GPT was among the fastest growing products in history [23], and retains a large daily active userbase today, two years later [7,8].AI is being increasingly integrated into social media [2,3,33], healthcare [34,37], business and enterprise [5,6,11], and transportation [12,16,21], and these trends show no signs of slowing.Although AI is still in its infancy, efficiency gains are already being observed [19,35].</p>
<p>Technology-driven efficiencies have historically had significant impacts on contemporary society, particularly within the labor market.Mechanization, initiated during the Industrial Revolution, reduced the labor demand in agriculture, leading to a long-term decrease in the number of farmers [14].The advent of digital technologies led to a transition within the workforce, emphasizing the prioritization of knowledge-based roles [13,31].With AI, the potential ramifications stand to be much more substantial.The question then naturally arises, how will these revolutionary technologies shape the future of labor, as those preceding it did?Automated agents are of particular import to this question.These are software systems that autonomously perform tasks or make decisions based on predefined rules or learning algorithms.They operate in a wide range of domains, from customer service or financial trading to programming and development.They are designed to simulate human decision-making without direct intervention.Some agents can incorporate the capability to autonomously carry out actions based on their simulated decision-making [38].</p>
<p>Because automated agents do not rely on direct intervention, coupled with the fact that machines are capable of processing and outputting data more efficiently than humans, these agents stand to drastically change the nature of work.Automated agents might be deployed to oversee rote tasks, such as reporting and administrative work that currently requires human oversight, thereby permitting humans to instead work on more complex, creative, or cognitively demanding tasks for which AI is not currently suited.By streamlining certain processes, resources can be redistributed to significantly increase efficiency throughout the working world.</p>
<p>Key to current implementations of automated agents are Large Language Models (LLMs), as they permit ordinary people to meaningfully interact with computers using tools they already possess: language.LLMs are advanced AI systems trained on vast amounts of text data to understand and generate human-like language.They use deep learning techniques, such as neural networks, to predict and produce coherent text based on a given input, making them useful for a wide array of tasks like translation, summarization, and conversation [4,10].Consequently, they often form the foundation upon which automated agents, or automated LLM agents, are built.</p>
<p>This permits individuals to describe a task to an automated agent using natural language.The agent is then able to leverage its constituent subsystems to autonomously carry out this task and provide its user with an output that is not restricted to natural language text.This flexibility, multi-modality, and versatility distinguishes automated LLM agents from their foundational LLMs.</p>
<p>However, many LLMs are trained on broad, non-domain-specific text in order to maximize the quantity of training data and provide the LLM with the most comprehensive understanding of how natural language works.Retrieval-Augmented Generation (RAG) can be used to mitigate this shortcoming.RAG is an advanced approach in natural language processing (NLP) that combines retrieval of relevant information from external sources with generation of new text based on that information [24,29].</p>
<p>In RAG models, a retrieval component first searches for and pulls in related documents, knowledge, or data from an external database or knowledge base.This information is then fed into a generative model, which uses the retrieved content to create a more informed and accurate response or output.</p>
<p>By incorporating RAG within an automated agent's LLM, the agent can provide more relevant information, specific to its domain and use case [20,25,48].This is particularly relevant when considering the application of automated LLM agents to the help desk and professional consultation industries.Because these industries rely on relevant domainspecific topic knowledge, RAG can be an effective approach to address the shortcomings of broadly trained LLMs.This is significant, as LLM-powered automated agents appear to be well suited to the work required in these domains.</p>
<p>The help desk and professional consultation industries are broad and are present across almost all sectors of the economy.This includes customer support services, IT support services, administrative assistance, after-sales support and assistance, and several other types of support services.Although these service lines may appear to vary, they all rely on domain-specific knowledge, the ability to solve problems, and the ability to handle and mitigate errors to ensure that clients or customers have a good experience.Furthermore, they all rely on communication between a provider and a customer/client.These are all tasks that automated LLM agents are designed to accomplish.However, their ability to do so effectively, particularly when compared to humans, remains unclear.</p>
<p>Answering these questions is significant, as, due to the applicability of automated LLM agents to help desk and professional consultation tasks, it stands to reason that there may be substantial impacts to the human workforce that already exists.In the US, almost 3 million people were employed as customer service representatives in 2023, earning a median hourly wage of $19.08 [9].These positions may face potential redundancies as automation within the sector increases.</p>
<p>In 2021, the global help desk software market was valued at $9.9 billion and is expected to grow at a compound annual growth rate of 9.4%, reaching $26.8 billion by 2032 [40].Much of this growth is being driven by the demand for cloud-based solutions, which further stands to put existing jobs at risk.Moreover, businesses in the US alone risk losing $846 billion worth of sales because of poor customer service [18], as studies have shown that 80% of customers are likely to switch service providers after more than one bad experience with a brand [36].</p>
<p>Because of the high potential human and financial costs associated with the help desk, customer support, and professional consultation industries, it is imperative that the quality of assistance remains high, regardless of the extent to which automation becomes incorporated.Despite trends indicating that the use of AI technologies within these industries is growing [36,43], evidence suggests that clients/customers still prefer to get service from humans, particularly in nuanced or complex cases [43,47].This all serves to highlight the growing need for a quantitative and comparative measure that can assess the performance of both humans and automated LLM agents in providing help desk and professional consultation assistance.</p>
<p>Benchmarking presents such a comparative tool.However, existing LLM benchmarks, as well as those targeting automated LLM agents, do not provide sufficient specific insight into the help desk and consultation industries (more information in Section 1.1).Given the potential impact automated LLM agents may have on the help desk and consultation industries, the development of a targeted benchmark specific to these industries is necessary to facilitate the integration of automated LLM agents into the industry.This gives rise to the primary research question of this study: How can the performance of automated help desk and consultation agents be quantified and thus compared to the performance of human help desk and consultation agents?</p>
<p>The current research presents SelfScore.This novel benchmark seeks to quantify the performance of automated LLM agents and humans when performing help desk and consultation tasks.By deriving its scores from intuitive and understandable math, the benchmark is more comprehensible and transparent than alternatives.This will help facilitate its adoption into domains wherein employees are less likely to have strong computer science or technical backgrounds.</p>
<p>For the current study, the research team developed automated help desk agents using current LLMs to assist with IT-related tasks to assess this newly developed benchmark.This gives rise to the second research question investigated herein: How well do the developed automated LLM agents perform help desk tasks compared to humans?Using the benchmark to quantify the performance of both humans and automated LLM agents, a direct comparison between the two can be drawn.This helps provide insight into the capabilities of current LLMs and the automated LLM agents that can be built upon them in order to assess the potential impact this technology might have on the future of the help desk and professional consultation industries.</p>
<p>Related Work</p>
<p>Currently, several benchmarks exist that aim to quantify the performance of LLMs.Some of these benchmarks focus on the ability of LLMs to engage in logical and critical thinking [32,44].Others prioritize performance on general knowledge questions, ranging in difficulty from elementary school-level to expert-level [22,39].These benchmarks are valuable for understanding the proficiencies of different LLMs on different tasks, and their observations have been considered in the development of SelfScore.For example, [32] notes that LLMs struggle with problems that rely on critical and logical thinking skills, while [22] observes that the largest LLMs are quite proficient in topic and domain knowledge.</p>
<p>However, these benchmarks do not target LLMs as a component of an automated agent.Where existing benchmarks do measure the performance of automated LLM agents [15,27,41,45], they focus on applications that are sufficiently different from the use case outlined in the current research to warrant the establishment of a new benchmark specific to the help desk and professional consultation industries.[15] assesses the capability for automated LLM agents to interact with mobile-based applications, while [45] focuses on automated LLM agents and their performance on games, [41] assesses performance on visual-dependent household tasks, and [27] assesses the capacity of automated LLM agents to create software systems and self-replicate.Although the tasks measured in these benchmarks might share similar underlying dependencies (the ability to navigate applications and the internet, adaptability and critical thinking, and an understanding of computer science and systems, respectively), they are not specific enough to the tasks involved in help desk and professional consultation tasks to permit the assessment of automated LLM agents in this domain.</p>
<p>More closely related are benchmarks that consider "LLMs-as-Agents" [30].However, there remains a distinction between this use case and the use case outlined in the current study.Help desk, customer service, and professional consultation tasks may rely on the use of other services (text-to-speech/speech-to-text, internet browsing, code deployment, document generation, etc.) that fall outside the scope of what many LLMs alone are capable.This is an important aspect of SelfScore, as it evaluates the automated agent's performance in its entirety, including that of other subsystems incorporated within the assessed autonomous LLM agent.</p>
<p>Furthermore, in many cases, these benchmarks evaluate performance on problems that have a well-defined correct answer [15,22,39,41].This is not always the case for help desk and professional consultation problems, where situational context and other nuances will impact a problem's solution.Additionally, this suggests that they are not well suited to assess performance across a longer user-agent interaction, as they assess a single question-answer pair, which is unsuitable for the conversational nature of help desk and professional consultation processes.For benchmarks where this is not the case, their applicability to the help desk and professional industry remains an unresolved limitation for the research goals of the current study.</p>
<p>SelfScore 2.1 Benchmark Criteria</p>
<p>The proposed benchmark assesses an automated help desk agent's ability to respond to a user question of varying complexity in a helpful manner.To accomplish this, one must first define and then calculate both the question's complexity and the helpfulness of the agent's response(s).</p>
<p>The benchmark also assesses the user's interactions with the LLM agent, contextualized by the agent's outputs.This ensures that especially effective or ineffective users do not artificially inflate or deflate the final score of a particular LLM agent.</p>
<p>Complexity.</p>
<p>The benchmark assesses the complexity of a human user's query based on three primary criteria.Grading for these criteria is relative to the easiest and hardest problems in the corresponding domain.For the current study, this domain is computer and IT-related knowledge.However, the benchmark can apply to any LLM agent used for any help desk or professional consultation tasks.The benchmark is specifically tailored to help desk and consultation tasks in non-regulated domains.Though it was not designed with regulated domains in mind, it might also be applicable to these, as the extent of the impact of AI-enabled automation within regulated domains remains to be seen.</p>
<p>The criteria are as follows:</p>
<p>• Critical Thinking: How much critical thinking does this problem require to solve.• Error Handling: How likely is an error to occur while solving this problem, and, should an error occur, how significant will the impact of the error be and how difficult will it be to recover from it.• Topic Knowledge: How much topic knowledge is required to solve the problem.These criteria describe the three most significant independent factors that influence the complexity of a help desk interaction and are equally applicable regardless of the specific domain in which such an interaction occurs.Whether one uses an LLM agent to assist in IT contexts, customer support, or bureaucratic or administrative assistance, it is important to measure the level of necessary critical thinking, error handling, and topic knowledge to address user questions.It is important to note that these criteria are independent.For instance, certain problems might require in-depth topic knowledge, but their solutions might not require significant further critical thinking or elicit substantial errors.Take, for example, a programming problem wherein the rounding of a number does not work as expected.This error might result from the behavior of one of the programming language's rounding functions.In this case, the solution might require the use of a different rounding function.Such a problem would require sufficient topic knowledge of the programming language but would not require further critical thinking or error management.</p>
<p>Furthermore, these criteria display continuous increases from simple help desk problems to challenging ones.As a user's problem becomes more difficult, an LLM agent will require a higher level of at least one, if not more, of the identified criteria to solve it than would be necessary to solve an easier problem.</p>
<p>Moreover, LLMs -and, by extension, automated LLM agentsare suitable tools to address help desk and consultation problems and one can expect their performance on these criteria to improve proportionate to developments in the underlying technology.This makes these criteria suitable metrics by which to assess the overall performance of automated LLM agents for tasks of this type.</p>
<p>2.1.2Helpfulness.The benchmark considers "helpfulness" at three separate points throughout the benchmarking process.</p>
<p>First, the benchmark considers the helpfulness of the initial input question.This refers to the level of information provided by the human user and how useful this information is in identifying and solving the user's current problem.This assessment is necessary as the quality of the initial input question will have an impact on the ability of the LLM agent to address the underlying issue, regardless of how competent the agent itself is.</p>
<p>Next, the benchmark assesses the helpfulness of the user throughout the interaction.This consideration will rate how well a user is able to execute the agent's instructions and will be continuous throughout the interaction between the user and LLM agent.This will primarily be determined by reviewing the user's follow-up dialogue with the agent, following the initial question.As with the first helpfulness assessment, this consideration is necessary to ensure that user competency does not impact an agent's final score.</p>
<p>Finally, the benchmark scores the helpfulness of the agent's outputs.This consideration assesses whether the support agent could provide solutions relevant to the problem.This requires the agent's outputs to be understandable, applicable, and constructive in addressing the user's underlying problem.</p>
<p>The combination of these three separate "helpfulness" measures enables the benchmark to account for differences in user competency and ensures that the benchmark does not erroneously inflate or deflate scores depending on individual user interactions.These measures represent points during the help desk interaction where users can introduce variability.The benchmark must consider this to ensure it only accounts for the LLM agent's abilities.</p>
<p>Additional Considerations.</p>
<p>In addition to considerations regarding complexity and helpfulness, the benchmark can optionally assess other factors that may be relevant to benchmarking an LLM agent's performance, depending on its specific use case.</p>
<p>For agents where operational costs may be a concern, benchmarking can optionally calculate and assess run costs.This permits assessors to weigh potential compromises between an agent's performance using various foundational LLMs and any associated operational costs.In the current research, this consideration only applied to GPT-4 due to limitations of the deployed library LangChain.</p>
<p>For agents that incorporate text-to-speech (TTS) technology, the benchmark may also consider TTS-related aspects of the agent's performance.These include the accuracy of the generated TTS, the comprehensibility of the TTS, and the naturalness of the TTS.These considerations are useful for agents that interact with users via audio interfaces, such as within call center applications, or similar.</p>
<p>Benchmarking Process</p>
<p>The process for benchmarking an LLM agent begins when the agent receives a question input as a problem.This input can originate from either a human user or a supplied testing dataset.At this stage, the benchmark calculates the weighted complexity score and assesses the helpfulness of the user's initial question.Once the LLM agent responds to the initial question, the first "turn" of the interaction concludes.</p>
<p>The benchmark defines "turns" as an individual question-response pair within the user-agent interaction.The "interaction" refers to the complete set of turns that constitute the problem-solving or consultation task between the user and the agent.Turns loop until either the agent solves the problem or exceeds the maximum number of turns, at which point the interaction concludes.Assessors can set this number of turns according to their specific needs.For the current study, researchers set the maximum number of turns equal to 50.</p>
<p>Following the first turn, each subsequent turn involves the following steps:</p>
<p>(1) The benchmark assesses the user's response to the agent for helpfulness.The agent then receives the user response.(2) The agent generates its counter-response.The benchmark assesses this counter-response for LLM helpfulness.The user then receives the counter-response.(3) The benchmark checks to determine if the agent has solved the user's problem.In the affirmative case, the loop terminates, and the current turn ceases here.This check assesses the turn history and the latest LLM response to confirm that the user's problem is, in fact, solved.(4) If the assessor is assessing the per-turn run cost, the benchmark considers this here.There are three cases which control how this occurs.turn.These responses contribute to the assessment of overall helpfulness for both the user and agent across the entire interaction.(7) The benchmark saves all scores, tokens, and other data for this turn for posterity (for instance, if there is a need to recalculate the benchmark scores for the current interaction).(8) If the agent has not solved the user's problem or has not exceeded the maximum number of turns, the loop restarts.</p>
<p>Once the interaction between the user and the agent ends, either by way of solving the user's question or by exceeding the maximum number of turns, the benchmark conducts calculations for final scoring.First, the benchmark calculates the average user and agent helpfulness across the entire interaction.Next, these scores determine the overall quality of the interaction.Finally, the benchmark calculates the interaction's final score by comparing the problem's weighted complexity with the average quality of the interaction.</p>
<p>The subsequent section (2.3 Scoring) provides the formulae for the calculations used throughout the benchmarking process.</p>
<p>Scoring</p>
<p>Benchmark scoring combines quantitative and qualitative measures of the LLM agent's performance.Consequently, subjectivity will play a role in scoring, as qualitative metrics related to problem complexity and response helpfulness are inherently subjective.The benchmark scores these subjective measures using an LLM.This permits the benchmark to efficiently assess on a larger scale than current alternatives.In all cases where the benchmark uses subjective measures, scores are based on a 10-point scale.</p>
<p>Complexity Scoring.</p>
<p>As discussed in Section 2.1.1,the benchmark bases the complexity of a user's initial question on three criteria: Critical Thinking, Error Handling, and Topic Knowledge, assessed relative to the easiest and hardest problems in the corresponding domain.The benchmark consolidates these three criteria into a single weighted complexity score, which aims to capture the overall complexity of the user's problem.</p>
<p>The benchmark calculates the weighted complexity score using the following formula, which maintains the 10-point scale of its constituents:</p>
<p>Weighted Complexity = (0.5 × Critical Thinking) + (0.4 × Error Handling) + (0.1 × Topic Knowledge)</p>
<p>Criterion weights consider the ability for LLMs to perform tasks related to the identified criteria to a high standard.As demonstrated in [22], LLMs are readily able to assist in tasks related to topic knowledge and domain knowledge transfer, thus it is not significantly considered in the weighted complexity score.However, topic knowledge remains an important aspect of addressing help desk and consultation questions and should factor into complexity considerations.</p>
<p>As demonstrated in [32,44], LLMs currently struggle to demonstrate effective logical skills and critical thinking, as well as the ability to foresee and mitigate potential errors [26,28].For this reason, these criteria are more important in determining the weighted complexity scores.Logical skills and critical thinking have been observed to be more challenging for LLMs than error handling, as demonstrated in comparing results from [26,44], hence their increased consideration within the weighted complexity score.This decision ensures that agents that are capable of these more complex tasks are more highly rewarded.</p>
<p>It is important to note that these weights are based on the conclusions of existing research into the capabilities of LLMs.They are not necessarily expected to be conclusive.More work is needed to finetune these weights to observe how weighted complexity might vary as these weights are adjusted.The above weights were used to generate the results of the current study and implementation.</p>
<p>Helpfulness</p>
<p>Scoring.The benchmark independently scores the helpfulness of both the user and the agent every turn on a 10point scale, where a 10 represents a perfectly helpful contribution to the ongoing interaction.</p>
<p>When assessing the user's helpfulness, one should consider whether the user response in this turn demonstrates their ability to follow the previous turn's instructions and communicate relevant new information clearly and concisely.</p>
<p>When assessing the agent's helpfulness, one should consider whether the agent can provide solutions relevant to the current problem, considering the information provided by the user.</p>
<p>For the first turn of the interaction, the assessment of the user helpfulness differs slightly.This distinction is necessary because this turn involves the initial question.As such, there are no previous instructions for the user to follow.Consequently, one should instead consider how helpful this initial question is based on how much potentially helpful information it contains, relevant to solving the problem.</p>
<p>Once the interaction has concluded, the benchmark calculates the overall average helpfulness scores for both the user and the agent using the following formulae: Quality score ensures that the benchmark does not penalize effective agents due to ineffective users, and that effective users do not artificially inflate the score of poor agents.</p>
<p>Final Scoring.</p>
<p>The final benchmark score considers both the weighted complexity score, derived from the user's initial question, as well as the average quality of the entire interaction, using the following formula:
Final Score = Weighted Complexity + Average Quality 2 × 10
The final score provides a theoretical maximum score of 100.This would require both the human user and the LLM agent to be perfectly helpful over the entire interaction on a maximally complex problem in terms of critical thinking, error management, and topic knowledge.</p>
<p>In practice, it is likely difficult for an agent to achieve such a score.However, as agent design and the capabilities of foundational models improve, it seems likely that scores will improve in a continuous manner as agents are able to solve more complex problems and communicate their solutions more effectively to users.</p>
<p>Current Implementation</p>
<p>To assess and validate the proposed benchmark, the research team designed and implemented automated LLM agents to assist in ITdomain help desk tasks.These agents use GPT-4, Mixtral7b, and Mixtral8x7b as foundational models.</p>
<p>The agents received tasks originating from a dataset comprised of Stack Exchange forum posts.The dataset is derived from an anonymized archive dump of all user-contributed content on Stack Exchange, which included posts, users, votes, comments, badges, tags, post history, and post links [42].Posts were selected from the archive dump based first on whether they had an accepted answer.Following this, further selection was based on the number of answers' upvotes to ensure that only high-quality postings were included in the dataset.From this selection, the text content and title of the post, the response, and the number of upvotes were considered relevant going forward.</p>
<p>For the current implementation of the benchmark, the research team used LLMs to assist in a variety of tasks.These tasks include performing large-batch information extraction, assessments of problem complexity, and assessments of user and agent helpfulness, among others.Where applicable, relevant prompts accompany descriptions of LLM use.</p>
<p>Data Pre-Processing and Initial Calculations</p>
<p>First, researchers converted the top entries from the Stack Exchange dataset from XML to JSON format for ease of use and consistency.Selection for the dataset's top entries considered the number of upvotes on a forum posting's answer.This threshold varied depending on whether the agent used RAG or not.For agents that were not using RAG, researchers set the minimum answer upvote to 100, which resulted in a pool of 1,164 entries.For agents using RAG, this threshold was lowered such that the pool was approximately twice as large (total of 2,360 entries).From this pool, random selection determined which entries were used for RAG and which were used for benchmark testing and validation, ensuring a 50/50 split to maintain validation pool size consistency between runs.</p>
<p>Since user questions were occasionally lengthy, an LLM extracted a summarized version of the question and the underlying problem from the posting's solution.</p>
<p>The prompt for question extraction follows: Dumb this question down and summarize it in one or two sentence(s):<Ques-tion>.</p>
<p>Originally, the question extraction prompt used more formal vocabulary (e.g., "simplify this question, " "give a concise version of this question").However, this version provided the best results.</p>
<p>The prompt for underlying problem extraction follows: Extract a problem statement from this post.For example, "The computer is not plugged in", or "The DNS servers are down".Respond with only the problem: <Problem>.</p>
<p>This approach minimizes the level of superfluous details provided to the LLM agent and minimizes required input tokens, where necessary.Furthermore, LLMs are well suited to information extraction tasks, as demonstrated in [46].</p>
<p>During runtime, the calculation for weighted complexity score considers the summarized version of the question, while the solution check (Point 3 in Section 2.2 Benchmarking Process) uses the extracted underlying problem.</p>
<p>To ensure the qualitative assessments are as representative as possible, multiple LLMs perform the assessments for complexity and helpfulness across different testing runs.For example, when using Mixtral8x7b as a foundational model for the agent, one testing run uses GPT-4 to perform qualitative assessment, while another uses Mixtral8x7b, and so on.This permits the direct comparisons of final scores, quality, and complexity and helpfulness assessments to confirm that LLM information extraction and qualitative assessment is robust and suitable for the current task.</p>
<p>Agent Design</p>
<p>The help desk agents share a similar design, differing only in the foundational LLM used for language processing and generation.Figure 1 visualizes the flow between the agent, the user, and the benchmark throughout the interaction.</p>
<p>The agent begins by receiving an input.This input is either the initial problem question or a follow-up question.For the current implementation, the source for these inputs is either a dataset entry (used to provide a human baseline or control assessment) or an LLM generated response which acts as a proxy for the agent's user.This approach facilitates large-scale assessment.In cases where the inputs are audio files, OpenAI's Whisper speech-to-text [1] converts the audio file to a string.Otherwise, the agent receives the input as text directly.</p>
<p>Once the agent receives the input, it passes this input, along with any interaction history, to the inference mechanism of the foundational LLM's backend.The LLM then processes the input to generate a text-based response, which is subsequently output back to the agent.Where relevant, the agent also returns the input and output tokens used.</p>
<p>A script then cleans the LLM's text output to remove unsupported characters by the agent's TTS mechanism.This step is necessary due to the specific TTS component [17] that the current agent implementation uses.The agent then writes the audio file and returns its response as both audio and text.At this point, the agent also returns the input to the benchmark to preserve interaction history.</p>
<p>It is important to note that, in the current implementation, the benchmark manages the interaction's history as opposed to the agent itself.This approach ensures that the benchmark can enforce the maximum number of turns per interaction and can determine when the agent has solved the user's problem.Currently, the agent itself is not able to reset interaction history on its own.This task is managed by the benchmark, which passes history to the agent, as necessary.In another implementation, the agent could exclusively manage history on an interaction-by-interaction basis.</p>
<p>Finally, the turn concludes, and the agent receives a new input from the dataset or from an LLM generated user proxy.The agent then repeats this process until it reaches the maximum number of turns or until it solves the problem.</p>
<p>In the case that an agent does not use RAG, it relies on the following system prompt: A user is having a problem.Respond with simple and helpful instructions most likely to guide the user to a solution.Only provide one solution at a time.Never give instructions to contact external or professional services.Never suggest contacting external or professional services.</p>
<p>Although the last sentence of this prompt is a repeated order, the research team identified that this minimized the chance that the output would suggest contacting external or professional services.</p>
<p>In the case that an agent does use RAG, there is no provided system prompt.Instead, a system prompt is either unnecessary and inferred by RAG data, or the developer explicitly defines it in the agent's backend.</p>
<p>Results</p>
<p>The average final scores for all assessment runs of the benchmark are in the table below.The results shown in the table indicate that agents incorporating RAG performed better than those that did not.The top 3 performing automated LLM agents all achieved similar average final scores, with a difference of only 0.89 between the top performer and third performer's average final scores.An ANOVA indicated that the difference in final scores for the top 3 performers is not statistically significant ( = 2.00,  = 0.135).</p>
<p>However, when considering all assessed agents' final scores, an ANOVA revealed that the observed difference was statistically significant ( = 84.7, &lt; 0.001).Appendix A contains post-hoc test results.These results suggest that the benchmark can effectively discern between differently capable LLMs.Furthermore, they reinforce the observation that agents incorporating RAG are better suited for domain-specific assistance.</p>
<p>The detailed results for the top three performing agents, as well as those from the human baseline, are found below.Pairwise comparisons using Mann-Whitney U tests accompany the result descriptions to determine statistical significance of between group results, where necessary.The Mann-Whitney U statistical test accounts for the non-normal distribution observed in key metric scoring.</p>
<p>Each result name is split into three parts, divided by underscores.The first part refers to the foundational LLM used for the agent, the next part refers to the model used to generate the dataset's complexity scores, and the third part refers to the model used for evaluation of responses during benchmarking.For instance, the results mixtral-8x7b_gpt-4-1106-preview_gpt-4-1106-preview uses Mixtral 8x7b as the agent's foundational LLM, uses GPT-4-1106 preview for the dataset complexity generation, and uses GPT-4-1106 preview for the response evaluation.</p>
<p>Note that the exception to this is human_gpt-4-1106-preview _gpt-4-1106-preview, which represents the current control.These results do not use an LLM agent to generate responses, and instead use the existing human interactions derived from the Stack Exchange forum.This provides a baseline against which to compare the performance of automated LLM agents.In this case, GPT-4 performed benchmarking tasks by assessing complexity and helpfulness.The detailed plots of key insights follow this naming scheme.Figure 2 displays the detailed results of the top performing LLM agent.This agent uses GPT-4 as a foundational model and incorporates RAG.GPT-4 was also used to perform subjective assessments regarding complexity and helpfulness, as well as simulate the responses of the agent's user.Ratings for complexity scores demonstrate a multimodal distribution, with few complexity scores near the extremes.Helpfulness scores for this assessment run are, in general, highly rated.However, average interaction quality scores are generally low.This results in a distribution of final scores that are mostly in the lower half of the possible score range.This distribution of final scores is similar across all assessment runs, however this agent demonstrated higher outlier final scores in the 60s and 70s.</p>
<p>Figure 3 shows the results for the Mixtral 8x7b-based model.These results are generally similar to those displayed in Figure 2. Notably, helpfulness scores were rated higher than in the assessment run depicted in Figure 3, however average interaction quality and final scores were lower, with fewer high score outliers.The bulk of final scores from both assessment runs show a similar distribution, though there were no final scores above 70.</p>
<p>The agent based on Mistral 7b (Figure 4) again displays similar trends to those observed in Figures 2 and 3. Helpfulness scores are higher than the two preceding results.</p>
<p>The human baseline results are displayed in Figure 5 These results are derived directly from the human interactions from the original Stack Exchange dataset.GPT-4 performed benchmarking tasks regarding complexity and helpfulness assessment.Not only   were helpfulness scores significantly lower ( = 4.33 6 ,  &lt; 0.001), but the benchmark scored the human interactions as significantly worse than all tested agents ( = 4.09 6 ,  &lt; 0.001).</p>
<p>Figure 6 displays comparisons of the subjective assessments performed by the LLMs to assess the feasibility of using LLMs for these types of assessments.</p>
<p>These comparisons display a statistically significant difference within the subjective assessments (Complexity:  = 95.4, &lt; 0.001; Helpfulness:  = 76.1,  &lt; 0.001).This result is unsurprising, as training data differs between LLMs.Thus, depending on the content of this training data, LLMs will have differing levels of competency for different domains.The final choice for which LLM to use for benchmarking assessments will eventually depend on the specific domain and use case.</p>
<p>Discussion 4.1 Contributions</p>
<p>The proposed benchmark represents a significant contribution to LLM agent benchmarking by permitting large scale testing of help desk and consultation agents.By using LLMs to conduct the benchmark's required assessments, significantly more data can be considered during scoring due to the rapidity with which LLMs can process written text.Thus, assessors can determine the performance of automated LLM agents across more numerous and more varied interactions than previous benchmarks allowed.</p>
<p>Furthermore, the scoring is simple and intuitive.Final scores are based on a logical 100-point scale and are derived from simple mean calculations.Constituent criteria are all scored on an intuitive 10-point scale based on clear definitions that are comprehensible to non-expert analysts.This approach broadens the benchmark's potential userbase and accessibility, thus facilitating its widespread adoption to a plethora of help desk and professional consultation industries, where existing human staff might not be proficient in AI and LLM technologies or assessment tools.</p>
<p>The current implementation, including the developed automated LLM agents, builds on this contribution by reinforcing the observation that RAG provides significant benefits to help desk-type tasks.Domain-specific RAG material led to significant improvements in the performance of automated LLM agents and their ability to helpfully solve user problems.This result reinforces the observations of [20,25,48] and provides further evidence for the benefits of sourcing relevant additional material to use for RAG when designing this type of LLM agent.</p>
<p>Additionally, the current study's approach of simulating the user side of the interaction facilitates the generation of test sets.This permits LLM agent developers to simulate interactions and assess the performance of their developed agents prior to release.</p>
<p>Based on the results of the current study, automated LLM agents are already capable of solving forum-based IT-related problems better than humans.Although all forum postings in the human baseline were marked as solved, the benchmark scores indicated that the human baseline was still the worst performing cohort of those assessed in the current study.These results indicate a pressing need to investigate the social ramifications of highly capable automated LLM agents on the status quo of the help desk and professional consultation industries.</p>
<p>Limitations</p>
<p>One might argue that a limitation of the current benchmark is its reliance on LLMs for subjective judgments.There is some validity to this criticism, since, as demonstrated in Figure 5, different LLMs will differ in their scoring.Moreover, a single LLM might assign different scores to the same question or response across multiple runs.</p>
<p>However, given the subjective nature of these judgments, it is reasonable to expect that human experts may also reach different conclusions on the complexity and helpfulness scoring for the same interaction.Additionally, human experts are subject to errors arising due to task exhaustion, lack of concentration, or task-related bias.For instance, experts might misjudge the complexity of problems for non-experts due to their familiarity with the domain.Using LLMs for such tasks will help minimize these types of errors.This point is particularly salient considering the current approach's ability to efficiently test a large batch of user-agent interactions.</p>
<p>This limitation might be partially mitigated by explicitly formalizing a rubric and providing examples for each point on the rating scale for each assessment criterion.</p>
<p>Future Work</p>
<p>The logical next step in further assessing the current benchmarking approach is to compare the subjective assessments made by LLMs to those made by relevant human domain experts.This will provide a baseline against which to determine the efficacy of using LLMs for the benchmark's constituent subjective assessments.Results will demonstrate the extent to which assessments made by LLMs align with those made by human domain experts.Should results indicate divergences between human and LLM scoring, these can then be mitigated by the data generated from the future work itself.For instance, this data can be used for RAG or to otherwise finetune the LLMs used in benchmarking assessments.This work will need to be done on a domain-by-domain basis but could help the assessing LLMs make conclusions that are more in line with human assessors, should there be divergences.</p>
<p>Further future work could focus on the collection and benchmarking of live human-agent interactions.This work would differ from the current implementation in that user problems and the subsequent interactions would not be derived from a dataset of forum postings.Instead, live users would interact with an agent in real-time.These crowd-sourced interactions can then be used to assess ready-to-be-deployed agents.This can help provide insight into how the benchmark performs across different points in an agent's development and deployment lifecycle.Moreover, this type of work can focus on collecting data regarding the performance of agents in sectors outside of IT, thus investigating the effectiveness of the benchmark across different domains.</p>
<p>Outside of internet forums, help desk and consultation interactions often occur over the phone or in person.In these cases, the capacity for agents to respond with a high degree of humanness in real-time will be more relevant.Additionally, interactions might be more open-ended than those in online forums.Although the current implementation did not include these types of interactions, the benchmark is designed to account for them.Consequently, data of this nature will help further assess the benchmark.</p>
<p>In addition to the improvements that can be gained through the comparison of human/LLM assessments and broader testing data, the benchmark might benefit from explicit grading rubrics and point-by-point examples.As discussed in 4.2 Limitations, establishing a clear description and example for each criterion point for the helpfulness and complexity assessments may enable different LLMs to reach more consistent conclusions on these subjective measures, both across scoring runs and between different models.</p>
<p>Similarly (as briefly discussed in Section 2.3.1 Complexity Scoring), more work needs to be done to finetune and adjust the weights within the weighted complexity function.The current weights consider what is hard for LLMs and reward those that can better accomplish these difficult tasks.However, it is unclear whether these weights are optimized or whether there might be alternative weights that better describe overall problem complexity.For example, a possible adjustment to the weights could consider what is hard for humans.In this case, topic knowledge would likely be weighted more heavily than critical thinking, as humans do not struggle with logical thinking like LLMs.</p>
<p>Finally, the results of the study indicate the need to investigate the potential impact of increased and highly capable automation on the existing help desk and consultation industries, with a specific focus on the social impact.The results of the current study suggest that workers within the help desk and consultation might be at risk of job replacement.This type of research could involve longitudinal studies into the increasing prevalence of AI technologies within the industry, how these trends have impacted jobs over the last few years, and the development and analysis of predictive models based on this data.Furthermore, information about what other roles human help desk workers fill (documentation, reporting, miscellaneous administrative tasks, etc.), the relative costs of humans to AI agents (including their development and deployment), and other considerations will be necessary to reach conclusions about the long-term impact of AI on this sector.The current benchmark provides a quantitative measurement to facilitate this work, which will be instrumental in conducting these follow-up investigations.</p>
<p>Conclusions</p>
<p>Based on the current results, automated LLM agents are becoming more capable than humans within the investigated subsection of the help desk and professional consultation domain.Should this trend continue, and should AI technologies continue to improve and proliferate, one can expect that jobs in this industry may be heavily impacted.This is significant as the help desk industry (specifically the customer service sector) accounts for 3 million jobs in the US alone.Considering the global help desk software market is already valued at $9.9 billion and is growing at a considerable rate, this impact will be felt worldwide.</p>
<p>These results, in addition to trends indicating that AI is already playing an increasingly large role in the industry, suggest that an impending transitionary period is approaching.Due to the potential human and financial impact of this transition, there exists a collective moral obligation to ensure that it can occur in such a way as to minimize potential harm and to uphold high service standards.</p>
<p>To ensure that this is carried out in such a manner, benchmarks, such as SelfScore, can be leveraged to ensure that newly implemented technologies provide added value to their human counterparts.Due to the importance of high-quality service, it is ill-advised to replace human workers should they perform better than automated alternatives.However, in the contrary case, it is necessary to assess the implications of industry redundancies and the associated broader social impacts.It may prove beneficial to investigate alternative solutions, such as the deployment of automated technologies in a collaborative role with existing human workforces.Final assessment will need to be performed on a case-by-case basis, given the differing capabilities and knowledgebase of LLMs and automated LLM agents.</p>
<p>SelfScore will enable these assessments and provide intuitive and easily comprehensible results, which is of considerable value given the complexity of LLMs and the persistence of the "black box" phenomenon.Furthermore, SelfScore will help facilitate future research into the broader societal and socioeconomic impacts of an ongoing transition towards automation within the help desk and consultation industries.</p>
<p>In developing SelfScore, the research team hopes that the integration of automation technologies can be carried out in a way that is considerate of the potential human and social ramifications, both within the help desk and professional consultation industries and more broadly.Such considerations should remain paramount in future studies on AI and its deployment, ensuring that technological advancements align with ethical standards and societal well-being.</p>
<p>(a) Input and output tokens are the same cost.Calculate the total number of tokens used in this turn, multiplied by the token cost.(b) Input and output tokens are priced differently.Calculate the number of input tokens multiplied by the input token cost and add to the number of output tokens multiplied by the output token cost.(c) Stipulates a set per-turn cost, which is multiplied by the interaction's total number of turns.(5) The benchmark calculates the current turn's quality.This score considers the helpfulness of the user responses and agent responses for this turn.(6) The benchmark saves the user and LLM responses for this</p>
<p>Average</p>
<p>User Helpfulness = Per Turn User Helpfulness Scores Number of Turns Average LLM Helpfulness = Per Turn LLM Helpfulness Scores Number of Turns 2.3.3Quality Scoring.The quality score arises from the helpfulness ratings and describes the overall quality of the interactions.The benchmark uses the following formulae: Average Quality = Average LLM Helpfulness Average Human Helpfulness</p>
<p>Figure 1 :
1
Figure 1: Interaction Flow Scheme.</p>
<p>Figure 2 :
2
Figure 2: GPT-4 + RAG Detailed Results.</p>
<p>Figure 3 :
3
Figure 3: Mixtral 8x7b + RAG Detailed Results.</p>
<p>Figure 4 :
4
Figure 4: Mistral 7b + RAG Detailed Results.</p>
<p>Figure 5 :
5
Figure 5: Human Baseline Detailed Results.</p>
<p>Figure 6 :
6
Figure 6: Comparison of Complexity and Helpfulness Assessments.</p>
<p>Table 1 :
1
Average Final Scores of Agents
Agent LLMAverage Final ScoreGPT-4 1 + RAG29.35Mixtral 8x7b + RAG 28.79Mistral 7b + RAG28.46Mistral 7b28.08Mixtral 8x7b27.70Mixtral 8x7b 223.65GPT-423.40Human23.12
 version: gpt-4-1106-preview. This model is used throughout the current research and is referred to simply as GPT-4.
2 This assessment run of the Mixtral 8x7b agent used GPT-4 for benchmarking tasks, while the preceding run used Mixtral 8x7b for all tasks.
Appendix A Comparison of Groups
Meta prepares AI-powered chatbots in attempt to retain users. Financial Times reports. Reuters. 2023. Aug. 2023</p>
<p>Musk says his AI firm xAI is rolling out chatbot Grok to X Premium+ subscribers. Reuters. 2023. Dec. 2023</p>
<p>What Are Large Language Models (LLMs. 2023</p>
<p>What Is Enterprise AI?. AI in Finance: Applications, Examples &amp; Benefits. 2023. 2024</p>
<p>com Traffic Analytics, Ranking &amp; Audience. August 2024</p>
<p>Customer Service Representatives. Introduction to Large Language Models | Machine Learning. 2024. 202410</p>
<p>Building smarter cars with smarter factories: How AI will change the auto business. Matthias Breunig, Matthias Kässer, Heinz Klein, Jan Paul Stein, 2017. 2017</p>
<p>Digital workplace and culture How digital technologies are changing the workforce and how enterprises can adapt and evolve. Jennifer Buchanan, Beth Kelley, Alicia Hatch, 2017. 2017</p>
<p>Agriculture in the British Industrial Revolution. Mark Cartwright, 2023</p>
<p>Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Jianfeng Liu, Ang Li, Jian Luan, Bin Wang, Rui Yan, Shuo Shang, 10.48550/arXiv.2407.00993arXiv:2407.00993Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents. 2024</p>
<p>Can artificial intelligence help wean city-dwellers from their cars? Reuters. Catherine Early, 2023. Dec. 2023</p>
<p>. 10.5281/zenodo.6334862</p>
<p>Klarissa Fitzpatrick, Customer Service Statistics: 2024 Report. 2024</p>
<p>AI-Driven Productivity Gains: Artificial Intelligence and Firm Productivity. Xueyuan Gao, Hua Feng, 10.3390/su15118934Sustainability. 1589342023. Jan. 2023Multidisciplinary Digital Publishing Institute</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang, 10.48550/arXiv.2312.10997arXiv:2312.109972024</p>
<p>Council Post: Three Ways AI Is Impacting The Automobile Industry. Wendy Gonzalez, Forbes. 2022. 2022Small Business</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 10.48550/arXiv.2009.03300arXiv:2009.03300Measuring Massive Multitask Language Understanding. 2021</p>
<p>ChatGPT sets record for fastest-growing user base -analyst note. Krystal Hu, Reuters. 2023. Feb. 2023</p>
<p>Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. Gautier Izacard, Edouard Grave, 10.48550/arXiv.2007.01282arXiv:2007.012822021</p>
<p>A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph. Cheonsu Jeong, 10.48550/arXiv.2407.19994arXiv:2407.199942024. 19994</p>
<p>Ryo Kamoi, Sarkar Snigdha, Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang, 10.48550/arXiv.2404.03602arXiv:2404.03602Evaluating LLMs at Detecting Errors in LLM Responses. 2024</p>
<p>Megan Kinniment, Lucas Jun, Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, arXiv:2312.11671Elizabeth Barnes, and Paul Christiano. 2024. Evaluating Language-Model Agents on Realistic Autonomous Tasks. </p>
<p>The long but necessary road to responsible use of large language models in healthcare research. C C Jethro, Serena C Y Kwong, Grace C Wang, Giovanni E Nickel, Joseph C Cacciamani, Kvedar, 10.1038/s41746-024-01180-yNPJ Digital Medicine. 71772024. July 2024</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, 10.48550/arXiv.2005.11401arXiv:2005.114012021</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, arXiv:2308.03688Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs as Agents. </p>
<p>Employment impact of digitalisation. Irene Mandl, 2023Technical Report</p>
<p>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval. Ida Momennejad, Hosein Hasanbeig, Felipe Vieira, Hiteshi Sharma, Robert Osazuwa Ness, Nebojsa Jojic, Hamid Palangi, Jonathan Larson, arXiv:2309.151292023</p>
<p>Meta's WhatsApp launches new AI tools for businesses. Dani Morera, Andre Romani, Reuters. 2024. June 2024</p>
<p>Mayo Clinic pairs with Cerebras Systems to help develop AI for health care. Stephen Nellis, Reuters. 2024. Jan. 2024</p>
<p>Experimental evidence on the productivity effects of generative artificial intelligence. Shakked Noy, Whitney Zhang, 10.1126/science.adh2586Science. 3812023. July 2023American Association for the Advancement of Science</p>
<p>O' Keith, Brien, The Future of AI in Customer Service. 2024</p>
<p>Matthew Perrone, Will AI replace doctors who read X-rays, or just make them better than ever? AP News. Health2024. May 2024</p>
<p>Matt Pogla, Auto-GPT vs ChatGPT: How Do They Differ. 2024</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, 10.48550/arXiv.2311.12022arXiv:2311.12022GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark. 2023</p>
<p>Sudip Saha, Help Desk Software Market Size, Shares &amp; Forecast -2032. 2022</p>
<p>ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, arXiv:1912.017342020</p>
<p>Stack Exchange Data Dump. </p>
<p>Tetiana Tsymbal, AI in Customer Service Statistics. 2024. April 2024</p>
<p>Sean Williams, James Huckle, 10.48550/arXiv.2405.19616arXiv:2405.19616Easy Problems That LLMs Get Wrong. 2024. 19616</p>
<p>Yue Wu, Xuan Tang, Tom M Mitchell, Yuanzhi Li, 10.48550/arXiv.2310.01557arXiv:2310.01557SmartPlay: A Benchmark for LLMs as Intelligent Agents. 2024</p>
<p>Large Language Models for Generative Information A Survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen, 10.48550/arXiv.2312.17617arXiv:2312.176172024</p>
<p>AI chatbots are taking over customer service, but most of us would rather wait for a human. Christopher Zara, 2024</p>
<p>Tianjun Zhang, G Shishir, Naman Patil, Sheng Jain, Matei Shen, Ion Zaharia, Joseph E Stoica, Gonzalez, 10.48550/arXiv.2403.10131arXiv:2403.10131RAFT: Adapting Language Model to Domain Specific RAG. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
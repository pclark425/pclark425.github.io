<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6386 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6386</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6386</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-261582815</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.03667v2.pdf" target="_blank">Exploring an LM to generate Prolog Predicates from Mathematics Questions</a></p>
                <p><strong>Paper Abstract:</strong> Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chain-of-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether fine-tuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-of-thought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the combination generation models do not yield significant improvements. The Prolog corpus based on GSM8K and the correspondingly finetuned Prolog generation model based on LLaMA7B are released to the research community.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6386.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6386.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B foundation model (pretrained)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter decoder-only transformer foundation model used as the base model for all experiments in this paper; reported pre-finetune accuracy on GSM8K test set is 11.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llama: Open and efficient foundation language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in this paper; treated as a pretrained foundation model prior to task-specific fine-tuning (original LLaMA pretraining details are outside this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems (elementary school level arithmetic and reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school / multi-step word problems (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>none (this is the pretrained base model prior to task-specific fine-tuning; prior reported evaluation used standard prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K test set</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>11.0% accuracy (pre-fine-tuning, reported prior to this project's fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probes reported for the pretrained model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Baseline poor performance on GSM8K (low accuracy); no detailed per-error analysis provided for pretrained model in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed here beyond noting that fine-tuning and generation strategy affect performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6386.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6386.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (finetuned CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B fine-tuned to produce chain-of-thought (CoT) natural-language solutions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-7B fine-tuned on GSM8K to generate chain-of-thought solution traces (natural-language step-by-step reasoning) and evaluated on GSM8K test set; serves as the baseline fine-tuned model in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on GSM8K solutions presented as chain-of-thought (elementary-school multi-step math problems and stepwise solutions).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic, multi-step reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question → natural-language chain-of-thought solution</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school multi-step (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>fine-tuning on chain-of-thought examples (supervised), generation with beam search at inference</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K test set (final-answer extraction from chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>25.1% accuracy (after fine-tuning and using beam search)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No activation- or attention-level interpretability analyses performed; qualitative observation that fine-tuning with CoT improved performance relative to pretrained baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Chain-of-thought outputs suffer syntax or semantic errors when parsed to retrieve answers; errors classified as parser syntax errors (cannot retrieve answer) or semantic errors (retrieved answer incorrect).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning with CoT substantially improves accuracy over the pretrained baseline (11.0% → 25.1%); beam search generation improved performance compared to sampling.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6386.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6386.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (finetuned Prolog)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B fine-tuned to generate Prolog code representing solutions (then executed by an external Prolog compiler)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLaMA-7B fine-tuned to translate GSM8K natural-language math problems into executable Prolog predicates; generated Prolog is executed by a Prolog compiler to obtain final numeric answers, yielding the best performance among configurations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on GSM8K where target outputs are Prolog programs (obtained by ChatGPT+human correction) encoding the problem solution logic.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and simple symbolic reasoning encoded as logic/program synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question → generated Prolog code (executable)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school multi-step (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>supervised fine-tuning to generate Prolog outputs; Prolog dataset constructed via few-shot ChatGPT prompts (manual correction) and occasional GPT-4 assistance; inference used beam search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K test set (answers computed by executing generated Prolog)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>30.9% accuracy (raw); 32.4% accuracy after manual reclassification of some outputs as correct when accounting for compiler limitations</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Qualitative analysis: many 'syntax errors' are due to Prolog compiler limitations (unsupported operations) rather than flawed semantic intent; no internal activation/attention mechanistic probes were performed. Authors note ~1.5% of samples reclassified as correct after relaxing compiler-strict criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Two primary error types: syntax errors (non-executable code, ~20.6% before revision and ~19.1% after revision) and semantic errors (executable but incorrect outputs; high semantic error rate reported at 48.4%). Specific issues include use of operations unsupported by the compiler (e.g., integer-solution-to-inequality handling) and general question-comprehension errors leading to incorrect logic.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Fine-tuning to produce symbolic/program outputs and delegating computation to an external tool increased accuracy relative to CoT fine-tuning (25.1% → 30.9%), indicating benefit from shifting computation out of the model; further improvements limited by compiler capability and semantic comprehension.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6386.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6386.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (finetuned CoT+Prolog)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B fine-tuned to generate chain-of-thought followed by Prolog code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model fine-tuned to output a natural-language chain-of-thought trace and then a Prolog program; intended to test whether generating reasoning first benefits subsequent code generation, but this configuration degraded performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on GSM8K where targets were concatenated chain-of-thought text followed by Prolog code.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question → chain-of-thought text + Prolog code</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school multi-step (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>supervised fine-tuning producing combined outputs (CoT then code); inference with beam search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K test set (for both CoT extraction and Prolog execution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Code accuracy: 17.7% (substantially below baseline); chain-of-thought accuracy also decreased relative to the CoT-only model (exact CoT accuracy reduction reported but tabulated values referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors qualitatively attribute degradation to 'data contamination' from combining heterogeneous output targets, making token relationships harder to learn; no mechanistic analyses (e.g., probing) were performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Combining CoT and code in a single target caused the model to produce lower-quality outputs (both text and code); this suggests interference in learning distinct output modalities. Observed errors include more syntax errors and incorrect logic in generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Combining outputs did not improve and in fact worsened performance; authors conclude CoT-before-code does not reliably improve code generation in this setup.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6386.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6386.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-7B (finetuned Prolog+CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 7B fine-tuned to generate Prolog code followed by chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Model fine-tuned to output Prolog code first and then a chain-of-thought explanation; code quality nearly matched Prolog-only model and subsequent CoT quality saw a small improvement, suggesting structured code output can help later text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on GSM8K where targets were Prolog code followed by chain-of-thought text.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language question → Prolog code + chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>elementary-school multi-step (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>supervised fine-tuning producing combined outputs (code then CoT); inference with beam search</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on GSM8K test set (Prolog execution for code part; final-answer extraction for CoT part)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Code accuracy: 30.1%; chain-of-thought accuracy increased to 26.2% (slightly above CoT-only fine-tune)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Qualitative observation: producing a clearly structured artifact (code) first may help the model generate more accurate subsequent chain-of-thought text; no detailed mechanistic probes performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still subject to syntax and semantic errors similar to Prolog-only model; non-executable constructs due to compiler limitations and high semantic error rate remain issues.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Prolog-first ordering retained most of the benefit of Prolog-only fine-tuning for code generation, while slightly improving CoT quality downstream.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Llama: Open and efficient foundation language models. <em>(Rating: 1)</em></li>
                <li>Scaling language models: Methods, analysis & insights from training gopher, <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6386",
    "paper_id": "paper-261582815",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "LLaMA-7B (pretrained)",
            "name_full": "LLaMA 7B foundation model (pretrained)",
            "brief_description": "A 7-billion-parameter decoder-only transformer foundation model used as the base model for all experiments in this paper; reported pre-finetune accuracy on GSM8K test set is 11.0%.",
            "citation_title": "Llama: Open and efficient foundation language models.",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Not specified in this paper; treated as a pretrained foundation model prior to task-specific fine-tuning (original LLaMA pretraining details are outside this paper).",
            "benchmark_name": "GSM8K",
            "task_type": "multi-step math word problems (elementary school level arithmetic and reasoning)",
            "problem_format": "natural-language math word problems",
            "difficulty_level": "elementary-school / multi-step word problems (GSM8K)",
            "prompting_method": "none (this is the pretrained base model prior to task-specific fine-tuning; prior reported evaluation used standard prompting)",
            "performance_metric": "accuracy on GSM8K test set",
            "performance_value": "11.0% accuracy (pre-fine-tuning, reported prior to this project's fine-tuning)",
            "internal_analysis": "No internal mechanistic probes reported for the pretrained model in this paper.",
            "failure_modes": "Baseline poor performance on GSM8K (low accuracy); no detailed per-error analysis provided for pretrained model in this paper.",
            "scaling_trend": "Not analyzed here beyond noting that fine-tuning and generation strategy affect performance.",
            "uuid": "e6386.0"
        },
        {
            "name_short": "LLaMA-7B (finetuned CoT)",
            "name_full": "LLaMA 7B fine-tuned to produce chain-of-thought (CoT) natural-language solutions",
            "brief_description": "LLaMA-7B fine-tuned on GSM8K to generate chain-of-thought solution traces (natural-language step-by-step reasoning) and evaluated on GSM8K test set; serves as the baseline fine-tuned model in this work.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on GSM8K solutions presented as chain-of-thought (elementary-school multi-step math problems and stepwise solutions).",
            "benchmark_name": "GSM8K",
            "task_type": "multi-step word problems (arithmetic, multi-step reasoning)",
            "problem_format": "natural-language question → natural-language chain-of-thought solution",
            "difficulty_level": "elementary-school multi-step (GSM8K)",
            "prompting_method": "fine-tuning on chain-of-thought examples (supervised), generation with beam search at inference",
            "performance_metric": "accuracy on GSM8K test set (final-answer extraction from chain-of-thought)",
            "performance_value": "25.1% accuracy (after fine-tuning and using beam search)",
            "internal_analysis": "No activation- or attention-level interpretability analyses performed; qualitative observation that fine-tuning with CoT improved performance relative to pretrained baseline.",
            "failure_modes": "Chain-of-thought outputs suffer syntax or semantic errors when parsed to retrieve answers; errors classified as parser syntax errors (cannot retrieve answer) or semantic errors (retrieved answer incorrect).",
            "scaling_trend": "Fine-tuning with CoT substantially improves accuracy over the pretrained baseline (11.0% → 25.1%); beam search generation improved performance compared to sampling.",
            "uuid": "e6386.1"
        },
        {
            "name_short": "LLaMA-7B (finetuned Prolog)",
            "name_full": "LLaMA 7B fine-tuned to generate Prolog code representing solutions (then executed by an external Prolog compiler)",
            "brief_description": "LLaMA-7B fine-tuned to translate GSM8K natural-language math problems into executable Prolog predicates; generated Prolog is executed by a Prolog compiler to obtain final numeric answers, yielding the best performance among configurations tested.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on GSM8K where target outputs are Prolog programs (obtained by ChatGPT+human correction) encoding the problem solution logic.",
            "benchmark_name": "GSM8K",
            "task_type": "multi-step word problems (arithmetic and simple symbolic reasoning encoded as logic/program synthesis)",
            "problem_format": "natural-language question → generated Prolog code (executable)",
            "difficulty_level": "elementary-school multi-step (GSM8K)",
            "prompting_method": "supervised fine-tuning to generate Prolog outputs; Prolog dataset constructed via few-shot ChatGPT prompts (manual correction) and occasional GPT-4 assistance; inference used beam search",
            "performance_metric": "accuracy on GSM8K test set (answers computed by executing generated Prolog)",
            "performance_value": "30.9% accuracy (raw); 32.4% accuracy after manual reclassification of some outputs as correct when accounting for compiler limitations",
            "internal_analysis": "Qualitative analysis: many 'syntax errors' are due to Prolog compiler limitations (unsupported operations) rather than flawed semantic intent; no internal activation/attention mechanistic probes were performed. Authors note ~1.5% of samples reclassified as correct after relaxing compiler-strict criteria.",
            "failure_modes": "Two primary error types: syntax errors (non-executable code, ~20.6% before revision and ~19.1% after revision) and semantic errors (executable but incorrect outputs; high semantic error rate reported at 48.4%). Specific issues include use of operations unsupported by the compiler (e.g., integer-solution-to-inequality handling) and general question-comprehension errors leading to incorrect logic.",
            "scaling_trend": "Fine-tuning to produce symbolic/program outputs and delegating computation to an external tool increased accuracy relative to CoT fine-tuning (25.1% → 30.9%), indicating benefit from shifting computation out of the model; further improvements limited by compiler capability and semantic comprehension.",
            "uuid": "e6386.2"
        },
        {
            "name_short": "LLaMA-7B (finetuned CoT+Prolog)",
            "name_full": "LLaMA 7B fine-tuned to generate chain-of-thought followed by Prolog code",
            "brief_description": "Model fine-tuned to output a natural-language chain-of-thought trace and then a Prolog program; intended to test whether generating reasoning first benefits subsequent code generation, but this configuration degraded performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on GSM8K where targets were concatenated chain-of-thought text followed by Prolog code.",
            "benchmark_name": "GSM8K",
            "task_type": "multi-step word problems",
            "problem_format": "natural-language question → chain-of-thought text + Prolog code",
            "difficulty_level": "elementary-school multi-step (GSM8K)",
            "prompting_method": "supervised fine-tuning producing combined outputs (CoT then code); inference with beam search",
            "performance_metric": "accuracy on GSM8K test set (for both CoT extraction and Prolog execution)",
            "performance_value": "Code accuracy: 17.7% (substantially below baseline); chain-of-thought accuracy also decreased relative to the CoT-only model (exact CoT accuracy reduction reported but tabulated values referenced).",
            "internal_analysis": "Authors qualitatively attribute degradation to 'data contamination' from combining heterogeneous output targets, making token relationships harder to learn; no mechanistic analyses (e.g., probing) were performed.",
            "failure_modes": "Combining CoT and code in a single target caused the model to produce lower-quality outputs (both text and code); this suggests interference in learning distinct output modalities. Observed errors include more syntax errors and incorrect logic in generated code.",
            "scaling_trend": "Combining outputs did not improve and in fact worsened performance; authors conclude CoT-before-code does not reliably improve code generation in this setup.",
            "uuid": "e6386.3"
        },
        {
            "name_short": "LLaMA-7B (finetuned Prolog+CoT)",
            "name_full": "LLaMA 7B fine-tuned to generate Prolog code followed by chain-of-thought",
            "brief_description": "Model fine-tuned to output Prolog code first and then a chain-of-thought explanation; code quality nearly matched Prolog-only model and subsequent CoT quality saw a small improvement, suggesting structured code output can help later text generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-7B (fine-tuned)",
            "model_family": "decoder-only transformer",
            "model_size": "7B",
            "training_data_description": "Fine-tuned on GSM8K where targets were Prolog code followed by chain-of-thought text.",
            "benchmark_name": "GSM8K",
            "task_type": "multi-step word problems",
            "problem_format": "natural-language question → Prolog code + chain-of-thought",
            "difficulty_level": "elementary-school multi-step (GSM8K)",
            "prompting_method": "supervised fine-tuning producing combined outputs (code then CoT); inference with beam search",
            "performance_metric": "accuracy on GSM8K test set (Prolog execution for code part; final-answer extraction for CoT part)",
            "performance_value": "Code accuracy: 30.1%; chain-of-thought accuracy increased to 26.2% (slightly above CoT-only fine-tune)",
            "internal_analysis": "Qualitative observation: producing a clearly structured artifact (code) first may help the model generate more accurate subsequent chain-of-thought text; no detailed mechanistic probes performed.",
            "failure_modes": "Still subject to syntax and semantic errors similar to Prolog-only model; non-executable constructs due to compiler limitations and high semantic error rate remain issues.",
            "scaling_trend": "Prolog-first ordering retained most of the benefit of Prolog-only fine-tuning for code generation, while slightly improving CoT quality downstream.",
            "uuid": "e6386.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Llama: Open and efficient foundation language models.",
            "rating": 1,
            "sanitized_title": "llama_open_and_efficient_foundation_language_models"
        },
        {
            "paper_title": "Scaling language models: Methods, analysis & insights from training gopher,",
            "rating": 1,
            "sanitized_title": "scaling_language_models_methods_analysis_insights_from_training_gopher"
        }
    ],
    "cost": 0.010679999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring an LM to generate Prolog Predicates from Mathematics Questions</p>
<p>Xiaocheng Yang 
Exploring an LM to generate Prolog Predicates from Mathematics Questions
NYU Shanghai Deans' Undergraduate Research Fund (xy2128), mentor: Yik-Cheung (Wilson) Tam (yt2267)
Recently, there has been a surge in interest in NLP driven by ChatGPT. ChatGPT, a transformer-based generative language model of substantial scale, exhibits versatility in performing various tasks based on natural language. Nevertheless, large language models often exhibit poor performance in solving mathematics questions that require reasoning. Prior research has demonstrated the effectiveness of chainof-thought prompting in enhancing reasoning capabilities. Now, we aim to investigate whether finetuning a model for the generation of Prolog codes, a logic language, and subsequently passing these codes to a compiler can further improve accuracy. Consequently, we employ chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other finetuned LLaMA7B models for the generation of Prolog code, Prolog code + chain-of-thought, and chain-ofthought + Prolog code, respectively. The results reveal that the Prolog generation model surpasses the baseline in performance, while the combination generation models do not yield significant improvements. The Prolog corpus 1 based on GSM8K 2 and the correspondingly finetuned Prolog generation model 3 based on LLaMA7B 4 are released to the research community.</p>
<p>Introduction</p>
<p>Presently, there exists a notable surge in interest in Natural Language Processing (NLP) catalyzed by the advent of ChatGPT. ChatGPT, being a transformerbased generative language model, exhibits versatility in performing a wide range of tasks grounded in natural language. The remarkable achievement of the GPT model can be attributed, in significant part, to its utilization of an exceptionally extensive corpus and a vast parameter set for acquiring features from the corpus. Nevertheless, mere augmentation in the model's size falls short in addressing mathematical inquiries encompassing arithmetic, commonsense, and symbolic reasoning -topics that may appear deceptively simple to individuals [1]. One conceivable explanation for this issue is that generative models overly depend on their training corpus. Its proficiency in specific tasks stems from the presence of sentences closely, and sometimes explicitly, linked to those tasks within the corpus. The GPT model, in turn, assimilates these sentences and effectively memorizes the corresponding answers. However, mathematical problems pose a challenge as they can manifest in various contextual frames, articulated through diverse approaches, and involve distinct numerical values. Consequently, the model is prone to encountering ostensibly unfamiliar mathematical queries, resulting in suboptimal performance.</p>
<p>Therefore, the significance of this project hinges upon the inadequacy of current capabilities in addressing this type of questions. We posit that to surmount this limitation, an NLP model should possess the capability to ingest natural language sentences and produce corresponding logical predicates. These predicates can then be processed by an external tool, distinct from a language model, to ultimately compute the desired result. In this context, we employ the Prolog language, known for its efficacy in performing such tasks, as the external logic tool. In essence, the role of the language model is restricted to semantic parsing and question comprehension, while the logical and computational tasks are delegated to a more precise tool. In this manner, the language model is relieved of the burden of memorizing every conceivable answer to questions, focusing instead on proficiently translating natural language into logic language. This shift in approach has the potential to significantly diminish the model's reliance on an excessively large corpus and enhance its performance in tackling such questions.</p>
<p>Furthermore, a significant issue with neural networks is the limited space for human intervention. It poses a challenge for humans to comprehend the inner workings and exert control over a vast neural network. Through the adoption of this model paradigm, human involvement is facilitated through the control of an external tool responsible for executing logic language, thus augmenting the model's explainability.</p>
<p>The paper's structure is as follows: The concept of chain-of-thought will be discussed in the Related Work section. Subsequently, the Approach section will provide a detailed, step-by-step account of project implementation. The Results section will showcase the performance of the fine-tuned models. Lastly, the Conclusion section presents drawn conclusions and outlines future research directions.</p>
<p>Related Work</p>
<p>In prior research, chain-of-thought prompting has demonstrated its efficacy in enhancing the reasoning capabilities of large language models when compared to conventional prompts that only supply questions and answers [2]. The fundamental concept behind chain-of-thought is to elucidate the intermediate problem-solving steps to the model. Consequently, we are motivated to employ chain-of-thought in finetuning as a foundational benchmark for this project, with the aim of investigating whether fine-tuning for Prolog generation as the output surpasses fine-tuning for chain-of-thought as the output in terms of performance.</p>
<p>Approach</p>
<p>Initially, this study leverages ChatGPT in conjunction with human correction to acquire Prolog code for each sample within the GSM8K dataset. It then employs the Chinese-Vicuna framework to fine-tune the LLaMA model across four data output configurations. Ultimately, the study assesses performance under these configurations to gauge the effectiveness of Prolog code generation for solving mathematical problems.</p>
<p>Base Corpus</p>
<p>The selection of GSM8K as the foundational corpus is based on its high relevance and quality. GSM8K comprises more than 8.5k elementary school-level math problems along with their solutions, articulated in natural language [3]. Each solution employs a chainof-thought approach to address the question, presenting a step-by-step solution culminating in a final answer, which can already be directly used as one output style.</p>
<p>Prolog Code Retrieval</p>
<p>We formulate prompts to extract Prolog code from ChatGPT. We first manually compose 10 examples for integration into the few-shot prompts. Here, we present an illustrative example. Figure 1 depicts a question from GSM8K, while Figure 2 showcases the corresponding Prolog code designed to address it.</p>
<p>In an effort to enhance the accuracy of retrieval results, the prompts incorporate natural language answers from GSM8K. Initially, the gpt-3.5-turbo-16k model is employed due to its cost-effectiveness. We process a pool of 100 samples, selecting 20 of them to construct the prompts, maximizing the utilization of the input token length. Utilizing the newly crafted prompts, we generate codes for all samples, retaining those codes that are both executable and yield accurate outcomes and regenerating codes for the rest. To overcome this bottleneck, we inject randomness into the process by reconfiguring the prompt candidates. The revised prompts comprise two components: the fixed part and the random part. The fixed part retains 8 old candidates. We select the 64 longest correctly generated code pieces, as longer codes often encompass more intricate arithmetic operations and potentially contribute to improved correctness. For each sample, we randomly select five candidates from this group to constitute the random part. This generation process continues iteratively until the bottleneck is encountered once again.</p>
<p>By this stage, the number of remaining questions has dwindled considerably, allowing us to employ gpt-4, which has the potential to further decrease the number of remaining questions to fewer than 100.</p>
<p>Finally, the remaining pieces of Prolog code are finalized through manual completion, followed by a manual verification of both executability and correctness of all the codes.</p>
<p>Finetuning</p>
<p>Owing to VRAM limitations, we employ LoRAs to facilitate the fine-tuning of the LLaMA7B model within the Chinese-Vicuna framework 5 . While the inputs consist of mathematical problems, we explore four different output styles for fine-tuning: chainof-thought, Prolog code, chain-of-thought + Prolog code, and Prolog code + chain-of-thought. This approach yields four fine-tuned 7B models. Throughout the fine-tuning process, the same configuration of hyperparameters is maintained to ensure a fair comparison of the performance of the four models.</p>
<p>Results</p>
<p>All four fine-tuned models undergo performance testing using the identical test set. Beam search is chosen as the generation strategy due to its superior performance in comparison to random sampling. Table 1 and Table 2 present the accuracy results for chain-ofthought and Prolog code, respectively. In the case of a chain-of-thought result, a syntax error is identified when the parser fails to retrieve an answer, whereas a semantic error occurs when the retrieved answer is incorrect. In the case of a Prolog code result, a syntax error indicates non-executability of the code, whereas a semantic error signifies that the executable code produces an incorrect answer.  </p>
<p>Finetuned on Chain-of-Thought</p>
<p>This model results from the fine-tuning of LLaMA7B directly using the natural language answers from GSM8K. Earlier research revealed that LLaMA7B, prior to fine-tuning with the math corpus, achieved an accuracy of 11.0% on GSM8K's test set [4]. Following fine-tuning, LLaMA7B exhibits an accuracy of 25.1%, surpassing its previous performance. The increment in performance can be attributed to both fine-tuning and the adoption of the beam search generation strategy. This suggests that introducing chain-of-thought samples during the fine-tuning phase can positively impact the model's performance. We consider this performance as the baseline.</p>
<p>Finedtuned on Prolog Code Generation</p>
<p>This model undergoes fine-tuning to produce Prolog codes for mathematical questions. The generated Prolog codes are subsequently forwarded to a Prolog compiler for correctness verification. Remarkably, this fine-tuned Prolog generation model attains an impressive accuracy of 30.9%, a substantial improvement over the baseline. This suggests that entrusting the logical and computational aspects to an external tool and relegating the model's role to that of a translational device can effectively enhance its performance in solving math problems that necessitate logical and computational inference. Nonetheless, it has come to our attention that certain outputs categorized as having syntax errors do not necessarily entail critical errors involving ambiguity or semantic flaws. These outputs incorporate operations that the compiler does not support, rendering the codes non-executable. Such issues can be rectified by expanding the compiler's capabilities to encompass a broader range of operations. Therefore, it may be overly stringent to classify these outputs as incorrect. An illustrative example is provided in Figure 3, where the error arises due to the problem of integer solutions to an inequality, which the compiler cannot handle accurately.</p>
<p>Following manual review, approximately 1.5% of the samples are reclassified as correct when the criteria are relaxed, considering the type of samples mentioned earlier as correct. As illustrated in Table 3, the accuracy now stands at 32.4%. While the improvement may not be substantial, it underscores the significance of the reliability of the external tool.</p>
<p>Finetuned on Chain-of-Thought + Prolog Code</p>
<p>This finetuned model generates a chain-of-thought solution followed by a piece of Prolog code. This experiment is motivated by the fact that transformer models utilize the current sequence to generate sub-  Our objective is to assess whether generating chainof-thought solutions first can improve the accuracy of the generated codes. The experiment's outcome reveals that this combination not only diminishes the performance of the chain-of-thought but also lowers the code accuracy to as low as 17.7%, a level even below the baseline. One contributing factor is that this output combination contaminates the data, making it challenging for the model to discern the relationships between tokens during the fine-tuning phase. Another potential explanation is that chain-of-thought may not inherently enhance the quality of code generation during the inference stage.</p>
<p>Finetuned on Prolog Code + Chain-of-Thought</p>
<p>The motivation of this experiment aligns with that of the combination experiment in the previous section.</p>
<p>Our objective is to investigate the impact of Prolog codes on chain-of-thought generation. Interestingly, when Prolog code generation is not influenced by chain-of-thought this time, its quality, achieving an accuracy of 30.1%, closely approximates that of solely generating Prolog codes. Furthermore, code generation appears to exert a marginal, positive influence on chain-of-thought generation that follows. As a result, the accuracy of chain-of-thought rises to 26.2%. This observation suggests that generations characterized by a clear and easily comprehensible structure may aid the model in extracting information that benefits subsequent generated content.</p>
<p>Conclusion</p>
<p>In this study, we utilized ChatGPT to generate Prolog codes for the GSM8K corpus, employing four distinct output settings and subsequently fine-tuning four LLaMA7B models accordingly. Through a comparative analysis of accuracies on the test set, the following conclusions can be drawn. Firstly, fine-tuning undeniably enhances performance in the mathematics question domain. Secondly, the process of generating Prolog codes and subsequently sending them to an external compiler yields superior results compared to chain-of-thought generation. Leaving the logical and computational aspects to an external tool and reducing the model to a translational device can effectively enhance its performance in solving math problems. This approach has the potential to be applied to other domains that involve logical and com-putational reasoning. Thirdly, generating combinations of chain-of-thought and Prolog code does not result in a significant further enhancement of inference performance. So far, the finetuned LLaMA7B for Prolog generation continues to exhibit a 19.1% syntax error rate after revision and a 20.6% before revision. Therefore, future efforts can be directed towards reducing syntax errors and expanding the capabilities of the external tool to support additional operations. Furthermore, given the remarkably high semantic error rate of 48.4%, there is a pressing need for models with enhanced question comprehension capabilities to address this bottleneck.</p>
<p>Figure 1 :Figure 2 :
12One example of a question in GSM8K One example of a piece of Prolog code This iterative process continues until a bottleneck is encountered.</p>
<p>Table 2 :
2The Prolog code part performance of four finetuned models</p>
<p>Table 3 :
3The revised Prolog code part performance of the Prolog model sequent tokens, implying that the content generated initially can influence subsequent content generation.
https://huggingface.co/datasets/Thomas-X-Yang/ gsm8k-prolog 2 https://huggingface.co/datasets/gsm8k 3 https://huggingface.co/Thomas-X-Yang/ Llama-7b-gsm-prolog 4 https://huggingface.co/decapoda-research/ llama-7b-hf
https://github.com/Facico/Chinese-Vicuna
AcknowledgementSpecial thanks are given to Professor Yik-Cheung Tam for mentoring this project, and to NYU Shanghai for providing the platform to support undergraduate research.
. J W Rae, S Borgeaud, T Cai, K Millican, J Hoffmann, F Song, J Aslanides, S Henderson, R Ring, S Young, E Rutherford, T Hennigan, J Menick, A Cassirer, R Powell, G Van Den Driessche, L A Hendricks, M Rauh, P.-S Huang, A Glaese, J Welbl, S Dathathri, S Huang, J Uesato, J Mellor, I Higgins, A Creswell, N Mcaleese, A Wu, E Elsen, S Jayakumar, E Buchatskaya, D Budden, E Sutherland, K Simonyan, M Paganini, L Sifre, L Martens, X L Li, A Kuncoro, A Nematzadeh, E Gribovskaya, D Donato, A Lazaridou, A Mensch, J.-B Lespiau, M Tsimpoukelli, N Grigorev, D Fritz, T Sottiaux, M Pajarskas, T Pohlen, Z Gong, D Toyama, C De Mas, ; A Guy, C Jones, J Bradbury, M Johnson, B Hechtman, L Weidinger, I Gabriel, W Isaac, E Lockhart, S Osindero, L Rimell, C Dyer, O Vinyals, K Ayoub, J Stanway, L Bennett, D Hassabis, K Kavukcuoglu, G Irving, Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las CasasScaling language models: Methods, analysis &amp; insights from training gopher," 2022J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Hen- derson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Pow- ell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Bud- den, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Ne- matzadeh, E. Gribovskaya, D. Donato, A. Lazari- dou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Mas- son d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hecht- man, L. Weidinger, I. Gabriel, W. Isaac, E. Lock- hart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, "Scaling language models: Methods, analysis &amp; insights from train- ing gopher," 2022.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, "Chain-of-thought prompting elicits reasoning in large language models," 2023.</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman, "Training verifiers to solve math word problems," 2021.</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and ef- ficient foundation language models," 2023.</p>            </div>
        </div>

    </div>
</body>
</html>
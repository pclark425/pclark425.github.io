<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8744 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8744</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8744</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271432135</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18219v2.pdf" target="_blank">Recursive Introspection: Teaching Language Model Agents How to Self-Improve</a></p>
                <p><strong>Paper Abstract:</strong> A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8744.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8744.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RISE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Introspection (RISE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning algorithm that converts single-turn problems into a multi-turn MDP and trains an LLM to iteratively detect and correct its own previous responses using on-policy rollouts and reward-weighted supervised learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama2-7B, Llama3-8B, Mistral-7B (evaluated); GPT-3.5 (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to 7B-class instruction-tuned models (Llama2-7B, Mistral-7B) and an 8B Llama3 variant in experiments; fine-tuned with iterative on-policy rollouts and reward-weighted regression objective. (Paper reports experiments primarily on 7B/8B models.)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RISE (Recursive Introspection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Convert dataset to a multi-turn MDP where states are prompt + previous attempts + optional feedback; collect on-policy rollouts, relabel next-turn improved responses via best-of-N sampling from learner or oracle distillation, then train with reward-weighted supervised regression (advantage-weighted variant). At inference, run sequential generate-then-reflect cycles (up to 5 turns in experiments), optionally terminating early with an oracle; final answer obtained via majority voting across turns when no oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and MATH (mathematical reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K: grade-school math word problems; MATH: more challenging mathematical problem set for reasoning evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>On GSM8K (5-turn inference, without oracle majority-vote finalization): LLaMa2-7B increased sequential (maj@1) 5-turn performance by up to ~17.7 percentage points over its 1-turn baseline; Mistral-7B saw improvements up to ~23.9 percentage points in some reported runs. Llama3-8B improved by ~8.2 percentage points and Mistral-7B by ~6.6 percentage points in other reported comparisons. On MATH (5-turn): LLaMa2-7B improved by ~4.6 percentage points and Mistral-7B by ~11.1 percentage points (all reported as absolute percentage-point increases in 5-turn maj@1 or p1 metrics depending on mode). GPT-3.5 improved only ~4.6 percentage points over five turns in the same setup.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base single-turn (m1@t1) accuracies are substantially lower (e.g., Llama2-7B m1@t1 reported ~10.5% on GSM8K in table examples); parallel sampling baselines (maj@5@t1 / pass@K) improve first-turn sampling but RISE's sequential 5-turn maj@1 often outperforms equivalent-budget parallel sampling by ~4–8 percentage points on GSM8K and ~6.5 percentage points on MATH (Mistral-7B example).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Algorithmic fine-tuning to produce iterative behavior: on-policy rollouts from the learner are collected; improved next-turn targets are obtained either via distillation from a stronger model (oracle teacher) or self-distillation (best-of-N samples from the learner); training uses reward-weighted (advantage-weighted) supervised regression so the model learns both from successful and suboptimal trajectories. Inference is sequential generate-then-reflect with optional oracle checks or majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative experimental comparisons show consistent monotonic increases in task accuracy across turns for RISE-trained models (tables/plots across GSM8K and MATH). Ablations show multi-turn training and reward-weighted regression are necessary: (1) training on multi-turn rollouts beats 1-turn oracle-only data, (2) weighted objective outperforms unweighted, (3) on-policy (DAgger-style) data beats purely off-policy expert-only data, and (4) sequential 5-turn performance exceeds parallel sampling (maj@5@t1) given equal sampling budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (a) compute limitations prevented many training iterations — experiments used only 1–2 fine-tuning iterations in practice, limiting self-distillation gains; (b) self-distillation variant requires many iterations/resources to match best-of-N baselines and can be slower to improve; (c) for knowledge-based QA where features are missing, RISE may encourage hallucinations (paper warns iterative conditioning can pick up spurious correlations); (d) occasional failure to correct small localized errors within 5 turns (illustrated qualitative examples and Figure 14); (e) duplicating correct-to-correct data naively can harm performance; (f) RISE depends on carefully designed data collection and weighting — simple SFT on the same oracle data did not reproduce the sequential improvement ability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against prompting-based Self-Refine, GLoRE, self-consistency (majority voting), and V-STaR: RISE produces larger multi-turn improvement than Self-Refine (which often degraded without oracle), outperforms GLoRE's non-oracle results, and exceeds parallel sampling/self-consistency (maj@K) with the same computation budget (RISE's sequential policy yields additional gains). RISE is single-model and end-to-end versus multi-model pipelines like GLoRE or verifier-based V-STaR.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations reported: (1) Multi-turn rollouts (showing learner mistakes + corrections) are superior to one-shot oracle responses; (2) reward-weighted (advantage) regression beats naïve supervised learning on filtered successful responses; (3) using all rollouts (including suboptimal) is better than filtering to successful only; (4) on-policy rollouts (DAgger-style) improve transfer/generalization compared to only off-policy expert data; (5) self-distillation (no external teacher) yields modest 5-turn gains (>1% for some models) but underperforms best-of-N parallel sampling unless more iterations/resources are used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results_extra</strong></td>
                            <td>Weak-to-strong experiment: rollouts from a weaker Llama2-7B when used to train Mistral-7B produced beneficial transfer (weak-to-strong generalization), whereas the reverse (training weak on strong rollouts) performed worse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8744.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8744.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based iterative self-critique and revision method where an LLM is prompted to identify errors in its solution and then rewrite/refine the solution, used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various base LLMs evaluated as baselines (including GPT-3.5 and 7B-class models in this paper’s comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineering approach applied to off-the-shelf LLMs that requests the model to critique its own solution and produce a revised answer; evaluated in multi-turn settings (paper ran 3-turn Self-Refine baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (prompted self-critique & revision)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>One-shot or few-shot prompting that instructs the model to find an error in its previous answer (self-critique) and produce a corrected/refined answer, repeated for a small number of turns (the paper evaluated 3-turn Self-Refine).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and MATH (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning benchmarks used to evaluate whether prompted self-critique improves multi-turn performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-Refine often degraded performance when run without an oracle across models in these evaluations; modest improvements only when oracle early termination was available (exact small gains reported in tables for specific runs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base single-turn performance (examples in paper: Llama2-7B base m1@t1 around 10.5% on GSM8K in an example table); Self-Refine typically did not produce consistent multi-turn gains over the base single-turn for the evaluated models without oracle assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: the model is explicitly prompted to find the error in its previous solution and to rewrite the solution accordingly (no fine-tuning required).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical evaluation in this paper shows Self-Refine baseline largely degrades multi-turn maj@1 performance without oracle; with oracle, only minor improvements observed. The paper cites this as evidence that prompting alone does not reliably induce intrinsic self-improvement in these base models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Degrades performance frequently without external oracle feedback; requires high-quality error traces or oracle hints to be effective; not sufficient to teach model to iteratively correct its own likely mistakes across a distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Directly compared as a baseline; RISE substantially outperforms Self-Refine in enabling monotonic multi-turn accuracy improvements without oracle assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8744.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8744.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (majority-vote over sampled chains-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel-sampling aggregation method that runs multiple independent generations (often chain-of-thought) at one turn and aggregates answers by majority or voting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as a baseline for the same LLMs evaluated (maj@K sampling from first turn)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling multiple independent responses from the model in a single turn and using majority voting (maj@K) to choose a final answer; does not perform sequential edits.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Consistency (parallel sampling + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate K independent candidate answers in a single (first) turn (possibly chain-of-thought), then use majority voting to select the final answer; no sequential correction or conditioning on earlier failures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and MATH (baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard benchmarks where sampling-based ensembles are used to improve single-turn accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>maj@5@t1 (parallel 5-sample majority voting) improves first-turn accuracy relative to pass@1, but RISE's sequential 5-turn maj@1 exceeds maj@5@t1 by ~4–8 percentage points on GSM8K and ~6.5 percentage points on MATH (Mistral-7B example) given equal computational budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-response pass@1 is the base (lower) baseline; self-consistency improves over pass@1 but is outperformed by RISE sequential sampling in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Parallel sampling and majority voting (prompt-level ensemble), not iterative self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper reports consistent advantage of RISE sequential sampling over equal-cost parallel maj@K sampling across multiple models and datasets, implying sequential generation conditioned on prior attempts yields additional gains beyond parallel sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Bounded by the candidate generator: if none of the parallel samples contains the correct answer, majority voting fails; cannot use additional contextual tokens from earlier failed attempts to inform new generations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly as an equal-cost baseline; RISE outperforms it in sequential settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8744.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8744.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RISE-self-distill</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RISE (self-distillation variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of RISE that removes the need for an external teacher by relabeling improved next-turn targets using best-of-N samples from the learner itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B, Llama3-8B (evaluated in self-distillation experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base architecture as RISE experiments; improved responses are chosen by sampling N candidates from the learner and selecting the best via the task's reward function (best-of-N) to relabel the next turn during training.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>RISE (self-distillation)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Collect on-policy rollouts; for each step sample N candidates from current model, pick the candidate with highest reward to serve as supervised target for next step; train with reward-weighted regression.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (self-distillation experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>GSM8K grade-school math problems used to study self-generated supervision effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Self-distillation RISE (one training iteration) produced modest improvements in 5-turn maj@1 over the base 1-turn performance: >1 percentage point gains reported for Llama3-8B and Mistral-7B on GSM8K (exact numbers depend on model and run).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base single-turn performance (m1@t1) for the same models is lower; self-distillation produced only modest 5-turn gains and did not outperform the best-of-N parallel sampling baseline (m5@t1) in the single-iteration regime.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Self-distillation via best-of-N sampling from the learner to relabel improved responses; trained with reward-weighted regression similar to RISE-distill but using no external teacher.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Reported experimental results show that even using only self-generated supervision (one iteration) yields small but positive sequential improvement compared to the base model; authors note additional iterations or online RL would likely increase gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-distillation did not surpass parallel best-of-N sampling (m5@t1) in single-iteration experiments; requires many training iterations or online algorithms to fully close the gap; computationally more expensive to bootstrap from the learner alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to RISE with oracle distillation, self-distillation yields smaller gains when limited to a single training iteration; authors highlight need for more iterations to match oracle-distill performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper notes that self-distillation improves sequential performance modestly in one-iteration experiments and hypothesizes further iterative training would increase benefits; no extensive multi-iteration self-distill ablation was run due to compute limits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8744.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8744.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLoRE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLoRE (Global and Local Refinements)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-model system that trains a student generator, an outcome-based reward model to locate errors, and a refinement model to correct errors at global or local levels; cited and compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Glore: When, where, and how to improve llm reasoning via global and local refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-model system reported in original GLoRE paper (comparison only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLoRE uses separate models: a candidate solution generator, a reward/outcome model to detect errors, and a refinement model for corrections; not reimplemented in this paper but compared to reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>GLoRE (separate error-locator + refinement model)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Use an outcome-based reward model to identify mistakes and a dedicated refinement model to produce corrected answers at different granularities (global/local refinements).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (comparison reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Math reasoning benchmark used in GLoRE paper and compared to RISE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>GLoRE reported little to no improvement in overall accuracy without an oracle; with oracle guidance they reported up to ~10% improvement on a 7B model on GSM8K (reported in GLoRE paper and referenced here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without oracle, GLoRE's global/local refinement gave limited gains or sometimes decreased accuracy according to the comparisons discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Separate reward model to detect errors + separate refinement model to correct them (multi-model pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites GLoRE's reported numbers and contrasts them with RISE's sequential-on-policy single-model improvements (RISE shows larger multi-turn gains without requiring multiple models).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GLoRE relies on multiple models and its non-oracle performance is inferior to RISE; oracle-assisted improvements are notable but require external verification during rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared as a prior multi-model refinement approach; RISE achieves larger sequential improvements in the single-model setting without the multi-model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback. <em>(Rating: 2)</em></li>
                <li>Glore: When, where, and how to improve llm reasoning via global and local refinements. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet. <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection. <em>(Rating: 2)</em></li>
                <li>V-star: Training verifiers for self-taught reasoners. <em>(Rating: 1)</em></li>
                <li>Self-Correct <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8744",
    "paper_id": "paper-271432135",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "RISE",
            "name_full": "Recursive Introspection (RISE)",
            "brief_description": "A fine-tuning algorithm that converts single-turn problems into a multi-turn MDP and trains an LLM to iteratively detect and correct its own previous responses using on-policy rollouts and reward-weighted supervised learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama2-7B, Llama3-8B, Mistral-7B (evaluated); GPT-3.5 (comparison)",
            "model_description": "Applied to 7B-class instruction-tuned models (Llama2-7B, Mistral-7B) and an 8B Llama3 variant in experiments; fine-tuned with iterative on-policy rollouts and reward-weighted regression objective. (Paper reports experiments primarily on 7B/8B models.)",
            "reflection_method_name": "RISE (Recursive Introspection)",
            "reflection_method_description": "Convert dataset to a multi-turn MDP where states are prompt + previous attempts + optional feedback; collect on-policy rollouts, relabel next-turn improved responses via best-of-N sampling from learner or oracle distillation, then train with reward-weighted supervised regression (advantage-weighted variant). At inference, run sequential generate-then-reflect cycles (up to 5 turns in experiments), optionally terminating early with an oracle; final answer obtained via majority voting across turns when no oracle.",
            "task_name": "GSM8K and MATH (mathematical reasoning)",
            "task_description": "GSM8K: grade-school math word problems; MATH: more challenging mathematical problem set for reasoning evaluation.",
            "performance_with_reflection": "On GSM8K (5-turn inference, without oracle majority-vote finalization): LLaMa2-7B increased sequential (maj@1) 5-turn performance by up to ~17.7 percentage points over its 1-turn baseline; Mistral-7B saw improvements up to ~23.9 percentage points in some reported runs. Llama3-8B improved by ~8.2 percentage points and Mistral-7B by ~6.6 percentage points in other reported comparisons. On MATH (5-turn): LLaMa2-7B improved by ~4.6 percentage points and Mistral-7B by ~11.1 percentage points (all reported as absolute percentage-point increases in 5-turn maj@1 or p1 metrics depending on mode). GPT-3.5 improved only ~4.6 percentage points over five turns in the same setup.",
            "performance_without_reflection": "Base single-turn (m1@t1) accuracies are substantially lower (e.g., Llama2-7B m1@t1 reported ~10.5% on GSM8K in table examples); parallel sampling baselines (maj@5@t1 / pass@K) improve first-turn sampling but RISE's sequential 5-turn maj@1 often outperforms equivalent-budget parallel sampling by ~4–8 percentage points on GSM8K and ~6.5 percentage points on MATH (Mistral-7B example).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Algorithmic fine-tuning to produce iterative behavior: on-policy rollouts from the learner are collected; improved next-turn targets are obtained either via distillation from a stronger model (oracle teacher) or self-distillation (best-of-N samples from the learner); training uses reward-weighted (advantage-weighted) supervised regression so the model learns both from successful and suboptimal trajectories. Inference is sequential generate-then-reflect with optional oracle checks or majority voting.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Quantitative experimental comparisons show consistent monotonic increases in task accuracy across turns for RISE-trained models (tables/plots across GSM8K and MATH). Ablations show multi-turn training and reward-weighted regression are necessary: (1) training on multi-turn rollouts beats 1-turn oracle-only data, (2) weighted objective outperforms unweighted, (3) on-policy (DAgger-style) data beats purely off-policy expert-only data, and (4) sequential 5-turn performance exceeds parallel sampling (maj@5@t1) given equal sampling budgets.",
            "limitations_or_failure_cases": "Reported limitations include: (a) compute limitations prevented many training iterations — experiments used only 1–2 fine-tuning iterations in practice, limiting self-distillation gains; (b) self-distillation variant requires many iterations/resources to match best-of-N baselines and can be slower to improve; (c) for knowledge-based QA where features are missing, RISE may encourage hallucinations (paper warns iterative conditioning can pick up spurious correlations); (d) occasional failure to correct small localized errors within 5 turns (illustrated qualitative examples and Figure 14); (e) duplicating correct-to-correct data naively can harm performance; (f) RISE depends on carefully designed data collection and weighting — simple SFT on the same oracle data did not reproduce the sequential improvement ability.",
            "comparison_to_other_methods": "Compared against prompting-based Self-Refine, GLoRE, self-consistency (majority voting), and V-STaR: RISE produces larger multi-turn improvement than Self-Refine (which often degraded without oracle), outperforms GLoRE's non-oracle results, and exceeds parallel sampling/self-consistency (maj@K) with the same computation budget (RISE's sequential policy yields additional gains). RISE is single-model and end-to-end versus multi-model pipelines like GLoRE or verifier-based V-STaR.",
            "ablation_study_results": "Ablations reported: (1) Multi-turn rollouts (showing learner mistakes + corrections) are superior to one-shot oracle responses; (2) reward-weighted (advantage) regression beats naïve supervised learning on filtered successful responses; (3) using all rollouts (including suboptimal) is better than filtering to successful only; (4) on-policy rollouts (DAgger-style) improve transfer/generalization compared to only off-policy expert data; (5) self-distillation (no external teacher) yields modest 5-turn gains (&gt;1% for some models) but underperforms best-of-N parallel sampling unless more iterations/resources are used.",
            "ablation_study_results_extra": "Weak-to-strong experiment: rollouts from a weaker Llama2-7B when used to train Mistral-7B produced beneficial transfer (weak-to-strong generalization), whereas the reverse (training weak on strong rollouts) performed worse.",
            "uuid": "e8744.0",
            "source_info": {
                "paper_title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "A prompting-based iterative self-critique and revision method where an LLM is prompted to identify errors in its solution and then rewrite/refine the solution, used as a baseline.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback.",
            "mention_or_use": "use",
            "model_name": "Various base LLMs evaluated as baselines (including GPT-3.5 and 7B-class models in this paper’s comparisons)",
            "model_description": "Prompt-engineering approach applied to off-the-shelf LLMs that requests the model to critique its own solution and produce a revised answer; evaluated in multi-turn settings (paper ran 3-turn Self-Refine baselines).",
            "reflection_method_name": "Self-Refine (prompted self-critique & revision)",
            "reflection_method_description": "One-shot or few-shot prompting that instructs the model to find an error in its previous answer (self-critique) and produce a corrected/refined answer, repeated for a small number of turns (the paper evaluated 3-turn Self-Refine).",
            "task_name": "GSM8K and MATH (baseline comparisons)",
            "task_description": "Mathematical reasoning benchmarks used to evaluate whether prompted self-critique improves multi-turn performance.",
            "performance_with_reflection": "Self-Refine often degraded performance when run without an oracle across models in these evaluations; modest improvements only when oracle early termination was available (exact small gains reported in tables for specific runs).",
            "performance_without_reflection": "Base single-turn performance (examples in paper: Llama2-7B base m1@t1 around 10.5% on GSM8K in an example table); Self-Refine typically did not produce consistent multi-turn gains over the base single-turn for the evaluated models without oracle assistance.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: the model is explicitly prompted to find the error in its previous solution and to rewrite the solution accordingly (no fine-tuning required).",
            "number_of_iterations": 3,
            "evidence_for_improvement": "Empirical evaluation in this paper shows Self-Refine baseline largely degrades multi-turn maj@1 performance without oracle; with oracle, only minor improvements observed. The paper cites this as evidence that prompting alone does not reliably induce intrinsic self-improvement in these base models.",
            "limitations_or_failure_cases": "Degrades performance frequently without external oracle feedback; requires high-quality error traces or oracle hints to be effective; not sufficient to teach model to iteratively correct its own likely mistakes across a distribution.",
            "comparison_to_other_methods": "Directly compared as a baseline; RISE substantially outperforms Self-Refine in enabling monotonic multi-turn accuracy improvements without oracle assistance.",
            "ablation_study_results": null,
            "uuid": "e8744.1",
            "source_info": {
                "paper_title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency (majority-vote over sampled chains-of-thought)",
            "brief_description": "A parallel-sampling aggregation method that runs multiple independent generations (often chain-of-thought) at one turn and aggregates answers by majority or voting.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "Used as a baseline for the same LLMs evaluated (maj@K sampling from first turn)",
            "model_description": "Sampling multiple independent responses from the model in a single turn and using majority voting (maj@K) to choose a final answer; does not perform sequential edits.",
            "reflection_method_name": "Self-Consistency (parallel sampling + majority vote)",
            "reflection_method_description": "Generate K independent candidate answers in a single (first) turn (possibly chain-of-thought), then use majority voting to select the final answer; no sequential correction or conditioning on earlier failures.",
            "task_name": "GSM8K and MATH (baseline comparisons)",
            "task_description": "Standard benchmarks where sampling-based ensembles are used to improve single-turn accuracy.",
            "performance_with_reflection": "maj@5@t1 (parallel 5-sample majority voting) improves first-turn accuracy relative to pass@1, but RISE's sequential 5-turn maj@1 exceeds maj@5@t1 by ~4–8 percentage points on GSM8K and ~6.5 percentage points on MATH (Mistral-7B example) given equal computational budgets.",
            "performance_without_reflection": "Single-response pass@1 is the base (lower) baseline; self-consistency improves over pass@1 but is outperformed by RISE sequential sampling in the paper's experiments.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Parallel sampling and majority voting (prompt-level ensemble), not iterative self-reflection.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Paper reports consistent advantage of RISE sequential sampling over equal-cost parallel maj@K sampling across multiple models and datasets, implying sequential generation conditioned on prior attempts yields additional gains beyond parallel sampling.",
            "limitations_or_failure_cases": "Bounded by the candidate generator: if none of the parallel samples contains the correct answer, majority voting fails; cannot use additional contextual tokens from earlier failed attempts to inform new generations.",
            "comparison_to_other_methods": "Compared directly as an equal-cost baseline; RISE outperforms it in sequential settings.",
            "ablation_study_results": null,
            "uuid": "e8744.2",
            "source_info": {
                "paper_title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RISE-self-distill",
            "name_full": "RISE (self-distillation variant)",
            "brief_description": "A variant of RISE that removes the need for an external teacher by relabeling improved next-turn targets using best-of-N samples from the learner itself.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B, Llama3-8B (evaluated in self-distillation experiments)",
            "model_description": "Same base architecture as RISE experiments; improved responses are chosen by sampling N candidates from the learner and selecting the best via the task's reward function (best-of-N) to relabel the next turn during training.",
            "reflection_method_name": "RISE (self-distillation)",
            "reflection_method_description": "Collect on-policy rollouts; for each step sample N candidates from current model, pick the candidate with highest reward to serve as supervised target for next step; train with reward-weighted regression.",
            "task_name": "GSM8K (self-distillation experiments reported)",
            "task_description": "GSM8K grade-school math problems used to study self-generated supervision effects.",
            "performance_with_reflection": "Self-distillation RISE (one training iteration) produced modest improvements in 5-turn maj@1 over the base 1-turn performance: &gt;1 percentage point gains reported for Llama3-8B and Mistral-7B on GSM8K (exact numbers depend on model and run).",
            "performance_without_reflection": "Base single-turn performance (m1@t1) for the same models is lower; self-distillation produced only modest 5-turn gains and did not outperform the best-of-N parallel sampling baseline (m5@t1) in the single-iteration regime.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Self-distillation via best-of-N sampling from the learner to relabel improved responses; trained with reward-weighted regression similar to RISE-distill but using no external teacher.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Reported experimental results show that even using only self-generated supervision (one iteration) yields small but positive sequential improvement compared to the base model; authors note additional iterations or online RL would likely increase gains.",
            "limitations_or_failure_cases": "Self-distillation did not surpass parallel best-of-N sampling (m5@t1) in single-iteration experiments; requires many training iterations or online algorithms to fully close the gap; computationally more expensive to bootstrap from the learner alone.",
            "comparison_to_other_methods": "Compared to RISE with oracle distillation, self-distillation yields smaller gains when limited to a single training iteration; authors highlight need for more iterations to match oracle-distill performance.",
            "ablation_study_results": "Paper notes that self-distillation improves sequential performance modestly in one-iteration experiments and hypothesizes further iterative training would increase benefits; no extensive multi-iteration self-distill ablation was run due to compute limits.",
            "uuid": "e8744.3",
            "source_info": {
                "paper_title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GLoRE",
            "name_full": "GLoRE (Global and Local Refinements)",
            "brief_description": "A multi-model system that trains a student generator, an outcome-based reward model to locate errors, and a refinement model to correct errors at global or local levels; cited and compared in this paper.",
            "citation_title": "Glore: When, where, and how to improve llm reasoning via global and local refinements.",
            "mention_or_use": "mention",
            "model_name": "Multi-model system reported in original GLoRE paper (comparison only)",
            "model_description": "GLoRE uses separate models: a candidate solution generator, a reward/outcome model to detect errors, and a refinement model for corrections; not reimplemented in this paper but compared to reported numbers.",
            "reflection_method_name": "GLoRE (separate error-locator + refinement model)",
            "reflection_method_description": "Use an outcome-based reward model to identify mistakes and a dedicated refinement model to produce corrected answers at different granularities (global/local refinements).",
            "task_name": "GSM8K (comparison reported)",
            "task_description": "Math reasoning benchmark used in GLoRE paper and compared to RISE.",
            "performance_with_reflection": "GLoRE reported little to no improvement in overall accuracy without an oracle; with oracle guidance they reported up to ~10% improvement on a 7B model on GSM8K (reported in GLoRE paper and referenced here).",
            "performance_without_reflection": "Without oracle, GLoRE's global/local refinement gave limited gains or sometimes decreased accuracy according to the comparisons discussed in this paper.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Separate reward model to detect errors + separate refinement model to correct them (multi-model pipeline).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites GLoRE's reported numbers and contrasts them with RISE's sequential-on-policy single-model improvements (RISE shows larger multi-turn gains without requiring multiple models).",
            "limitations_or_failure_cases": "GLoRE relies on multiple models and its non-oracle performance is inferior to RISE; oracle-assisted improvements are notable but require external verification during rollouts.",
            "comparison_to_other_methods": "Compared as a prior multi-model refinement approach; RISE achieves larger sequential improvements in the single-model setting without the multi-model complexity.",
            "ablation_study_results": null,
            "uuid": "e8744.4",
            "source_info": {
                "paper_title": "Recursive Introspection: Teaching Language Model Agents How to Self-Improve",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback.",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Glore: When, where, and how to improve llm reasoning via global and local refinements.",
            "rating": 2,
            "sanitized_title": "glore_when_where_and_how_to_improve_llm_reasoning_via_global_and_local_refinements"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet.",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection.",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "V-star: Training verifiers for self-taught reasoners.",
            "rating": 1,
            "sanitized_title": "vstar_training_verifiers_for_selftaught_reasoners"
        },
        {
            "paper_title": "Self-Correct",
            "rating": 1,
            "sanitized_title": "selfcorrect"
        }
    ],
    "cost": 0.019946,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Recursive Introspection: Teaching Language Model Agents How to Self-Improve
26 Jul 2024</p>
<p>Yuxiao Qu yuxiaoq@andrew.cmu.edu 
Carnegie Mellon University
2 UC Berkeley, 3 MultiOn</p>
<p>Tianjun Zhang 
Naman Garg 
Aviral Kumar 
Carnegie Mellon University
2 UC Berkeley, 3 MultiOn</p>
<p>Carnegie Mellon University</p>
<p>Recursive Introspection: Teaching Language Model Agents How to Self-Improve
26 Jul 2024DD23E34CE3A767A6AF177B9CB647CEB9arXiv:2407.18219v2[cs.LG]
A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available.Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake.In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain.Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback.RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt.Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations.Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation.We also find that RISE scales well, often attaining larger benefits with more capable models.Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.</p>
<p>Introduction</p>
<p>A promising approach to utilizing and deploying foundation models, and in particular, large language models (LLMs), is to treat them as general-purpose decision-making machines, or "agents".To be successful, an LLM agent must not just provide plausible completions for input text, but must exhibit interactive, goal-directed behavior to accomplish a given task.Put in abstract terms, this requires mastering two qualities: (a) producing responses that explicitly seek information about the task, followed by (b) making decisions and improving them by "thinking" and verifying them at inference time.For instance, to succeed in using a new coding library, an effective LLM agent should first synthesize programs, then try the most promising subset against a compiler, use the resulting feedback to improve the program, and repeat the process for multiple turns.Having the ability to successfully improve a response in sequential attempts is equivalent to a form of "self-improvement", at test time.</p>
<p>To enable test-time self-improvement, recent approaches attempt to repurpose the knowledge already stored in pre-trained models via few-shot prompting [7,15,31,52,64].Although prompt tuning in conjunction with feedback is effective in eliciting improved responses from capable models, it fails to produce models that can succeed in complex tasks by correcting their own mistakes, such as those that require logical reasoning [21,55].In many of these problems, models contain the "knowledge" needed to answer a challenging prompt, but are not able to elicit that knowledge even when asked to sequentially correct their mistakes.Fine-tuning the LLM on domain-specific question-answering data [6,29,39] can help, but it still does not teach the agent a test-time improvement strategy (see Section 6).A strategy for improving responses over sequential attempts at test time is crucial for tackling challenging prompts, where directly attempting the problem in one shot may largely be futile.</p>
<p>Can we train models to be capable of improving their own responses?If done correctly and on a diverse set of problems and scenarios, this could introduce in an LLM, a general procedure for "how" it can tackle a hard prompt by improving itself as opposed to supervising it with "what" to respond with, which may not generalize as the test prompt becomes out of distribution.Although one straightforward approach to inducing this capability into a model would be to generate data that showcase improvements over multiple sequential turns (potentially from highly capable models), we find that simply imitating these data is not sufficient to enable this capability (Section 6.4).Quite well, this is due to two reasons: First, multi-turn data from a different model would not show improvements in the kinds of errors the learner would make, thereby being irrelevant to the learner [24].Second, often sequential multi-turn data collected from proprietary models is also not of high quality since these models are typically not good at proposing meaningful improvements to their own errors [21] even though they can still provide useful responses to the problem at hand.Therefore, we need a different strategy to endow models with a self-improvement capability.Our key insight is to supervise improvements to the learner's own responses in an iterative fashion, taking inspiration from methods in online imitation learning [36] and reinforcement learning (RL) [45].This supervision can be in the form of oracle responses to the prompt sampled i.i.d.from more capable models, or be generated from the learner itself.</p>
<p>Our contribution is an algorithm RISE: Recursive Introspection (Figure 1) that utilizes these insights to improve the self-improvement capability of an LLM over the course of multiple attempts at a given prompt.In each iteration, our approach bootstraps on-policy rollouts from the learner with better responses at the next turn obtained by running best-of-N (using a success indicator on the task) on multiple revision candidates obtained by sampling from the learner itself or using responses from a more capable model, whichever is more convenient.In this way, we are able to construct rollouts that demonstrate the learner how it can improve its responses under its own distribution.Then, we fine-tune the learner on these data using a reward-weighted regression (RWR [34,35]) objective, that is able to learn from both high-and low-quality parts of such rollouts.By iteratively repeating this procedure, we are able to instill a general self-improvement capability into an LLM.Our results show that LLMs trained via RISE can produce correct responses on more prompts, improving over turns for more challenging prompts.</p>
<p>Even though strong base and intruction-tuned LLMs [23,58] often fail to improve their responses over multiple sequential attempts (even when explicitly told about their mistakes previously), RISE successfully endows similarly-sized LLMs with self-improvement capabilities, resulting in monotonically increasing task performance after each turn.Specifically, on the GSM8K [11] dataset, RISE demonstrates significant improvement over various models.RISE improves the performance of LLaMa3-8B by 8.2% and Mistral-7B by 6.6%, entirely using their own data.RISE attains a 17.7% improvement for LLaMa2-7B over the course of 5-turn introspection (outperforming parallel sampling from the first turn), and a 23.9% improvement for Mistral-7B.In contrast, GPT-3.5 itself only improves by 4.6% over five turns.We see similar trends on the MATH dataset [18], where RISE improves LLaMa2-7B by 4.6% and Mistral-7B by 11.1% over five turns.We also study why and how RISE is able to induce self-improvement abilities and show that this ability generalizes to out-of-distribution prompts as well.These results consistently demonstrate RISE's effectiveness in enhancing mathematical reasoning capabilities for different models.</p>
<p>Related Work</p>
<p>Several prior works build techniques to improve reasoning and thinking capabilities of foundation models for downstream applications.Typically these works focus on building prompting techniques for effective multi-turn interaction with external tools [5,7,14,32,49,54,56], sequentially refining predictions by reflecting on actions [7,15,63], asking the model to verbalize its thoughts [33,52,65], asking the model to critique and revise itself [31,40] or by using other models to critique a primary model's responses [2,12,20,54].Although a subset of this work does improve its own responses, this self-correction ability often requires access to detailed error traces (e.g., execution traces from code compilers [7,31]) in order to succeed.In fact, [21] and Table 1 both indicate that self-improvement guided by the LLM itself (i.e., "intrinsic self-correction") is often infeasible for off-the-shelf LLMs even when they contain the knowledge required to tackle the prompt given, but fine-tuning with RISE induces this capability as we show in this paper.</p>
<p>Beyond prompting, previous work also attempts to fine-tune LLM to obtain self-improvement capabilities [6,39,62].These works attempt to improve reasoning performance by training on self-generated responses [30,46,57,58,60].To achieve this, these works use a combination of learned verifiers [28,47,50], search [13,26,33,38], contrastive prompting on negative data [9,48], and iterated supervised or reinforcement learning (RL) [8,37,59].Although our approach also trains on model-generated data, we aim to introduce a complementary capability to improve performance over sequential turns of interaction, rather than to improve single-turn performance alone.Other work fine-tunes LLMs for multi-turn interaction directly via RL [41,66]: while this is indeed related, single-turn problems posed in multi-turn scenarios require addressing distinct challenges than generic multi-turn RL: (i) sampleefficiency is not a concern since the entire environment is fully characterized by the training dataset of prompts and oracle answers and dynamics are deterministic, and (ii) we need to generalize to novel test prompts.Multi-turn RL focuses on sample efficiency, which is not as critical in our setting, though of course learning to generalize from a limited number of initial states would be appealing.Our main focus is to show that it is possible to train models for self-improvement via appropriately designing multi-turn fine-tuning objectives.This is orthogonal from the choice of training approach (RL or not).</p>
<p>The most related to our work are GLoRE [17] and Self-Correct [53], which train separate models to identify errors and refine incorrect answers of other LLMs.Unlike these works, our approach trains a single model to produce answers and improve them over more than two turns, which is the maximal number of turns studied in these works.We show that doing so successfully requires careful design choices: an iterative on-policy data generation strategy along with a training objective that can learn from both successful and unsuccessful rollouts.From an algorithmic point of view, RISE is similar to online imitation learning [36,44], in that it queries expert supervision on states attained by on-policy rollouts.On-policy distillation for LLMs [1,4] utilizes this idea, but queries an expert to provide completions on partial responses instead of sequential attempts, that we do in this work.</p>
<p>Problem Setup and Preliminaries</p>
<p>The goal of our work is to improve LLM performance over sequential attempts / turns at a given problem.Concretely, given a dataset  = {(  ,  *  )}  =1 of problems   and oracle responses  *  , our goal is to obtain an LLM   (•|[, ŷ1: ,  1: ]) that, given the problem , previous model attempts ŷ1: at the problem, and auxiliary instructions  1: (e.g., instruction to find a mistake and improve the response; or additional compiler feedback from the environment) solves a given problem as correctly as possible.To this end, we encode this goal into the following learning objective that we wish to optimize:
max 𝜋 𝜃 𝐿 ∑︁ 𝑖=1 E 𝑥,𝑦 * ∼𝒟,ŷ 𝑖 ∼𝜋 𝜃 (•|[𝑥,ŷ 1:𝑖−1 ,𝑝 1:𝑖−1 ]) [I (ŷ 𝑖 == 𝑦 * )] . (3.1)
Unlike standard supervised fine-tuning that trains the model  to produce a single response ŷ given , Equation 3.1 trains  to also appropriately react to a given history of responses from its own previous attempts ŷ1:−1 .Equation 3.1 most closely resembles an RL objective, and we will indeed develop our approach by converting a single-turn problem into a multi-turn MDP.Finally, note that prompting-based methods such as Self-Refine [31] can still be viewed as training  to optimize ( * |) but only when only allowed to modulate the prompt   to optimize Equation 3.1.Naturally, since the parameters  are unchanged, this would not be effective in optimizing the objective fully.</p>
<p>RISE: Recursive Introspection for Self-Improvement</p>
<p>Since even strong off-the-shelf models do not exhibit an effective ability to improve themselves when provided with sequential attempts at a given problem [21], a natural next step is to ask how to train models to induce this capability.In this section, we will develop our approach, RISE, for fine-tuning foundation models towards improving their own predictions over multiple turns.Our approach will first convert a problem into a multi-turn MDP, then collect data, and finally run offline reward-weighted supervised learning in this multi-turn MDP to induce this capability.</p>
<p>Converting Single-Turn Problems into a Multi-Turn Markov Decision Process (MDP)</p>
<p>The first step in building our approach is to procedurally construct a multi-turn MDP out of a single-turn dataset of prompts and oracle responses (Figure 2, Left).Given a dataset,  = {(  ,  *  )}, consisting of prompts   and corresponding oracle responses  *  (e.g., math questions and natural language responses to those questions), we will construct an induced MDP ℳ from , and then learn policies in this MDP.An initial state in this MDP is a possible prompt   ∈ .We denote the output response from the foundation model as action .Given a state , the next state can be obtained by concatenating the tokens representing  with the action  proposed by the model, and an additional fixed prompt  that asks the model to introspect, e.g., "this response is not correct, please introspect and correct your answer."(the exact prompt is shown in Appendix D.4).The reward function is a sparse binary indicator of answer correctness at a given state , ([  ,
• • • ], 𝑎) = 1 if and only if 𝑎 = 𝑦 *
 and is obtained from an answer checking function.This construction from dataset  to MDP ℳ is shown below:
𝒟 = {(𝑥 𝑖 , 𝑦 * 𝑖 )} → ℳ : 𝜌(𝑠 0 ) = Unif(𝑥 1 , 𝑥 2 , • • • , 𝑥 𝑁 ) (4.1) 𝑃 (𝑠 ′ |𝑠, 𝑎) = 𝛿 (︀ 𝑠 ′ = concat[𝑠, 𝑎, 𝑓 ] )︀ (4.2) 𝑟(𝑠, 𝑎) = 1 (𝑎 = 𝑦 * 𝑖 if 𝑥 𝑖 ∈ 𝑠) . (4.3)</p>
<p>Learning in the Multi-Turn MDP</p>
<p>With the MDP construction in place, the next step involves training a model to improve itself over the course of a rollout.We subscribe to an offline approach to learning that we describe in the following.</p>
<p>Step 1: Data collection for self-improvement.To ensure that rollout data from this multi-turn MDP is useful for teaching the model how to self-improve, it must satisfy a few desiderata: (1) it must illustrate the mistakes that the learner is likely to make and showcase how to improve upon them in the next attempt, (2) the data must illustrate responses that are relevant to the model given the problem and previous attempts in context, and (3) it must not contain any rollout that degrades in a subsequent turn.</p>
<p>Our data collection strategy (Figure 2, Right) satisfies these desiderata.</p>
<p>In a given round , for a given problem   , we unroll the current model    (•|•) to produce multiple sequential attempts, denoted by    ∼    (•|   ).In problems, where external input (e.g., compiler feedback) is available, we also observe a variable-length, natural language external input,    (e.g., in math problems we ask the model to correct itself).We also observe a scalar reward value (   ,    ), denoted as    in short.Let us denote this dataset of "on-policy" model rollouts as  on-policy := {(   ,    ,    ,    )  =1 }.For each time-step, we construct an improved version of the response    that we will denote by ỹ  .We also record the reward score associated with this improved response as (   , ỹ  ), or r  in short.To obtain an improved version of a response    , we can employ several strategies.Perhaps the most straightforward approach is to query an off-the-shelf more capable model to provide a correct response given the prompt   , the previous response    , and an optional external feedback    .We refer to this as the distillation variant of our approach, since it uses a strong "teacher" model to guide self-improvement (note that this is different from the classic notion of knowledge distillation, and we will in fact show results in Section 6.1 that will help understand the differences).</p>
<p>Don-policy
+ distill := {︁ {︀(︀ 𝑠 𝑖 𝑡 , ỹ𝑖 𝑡 , 𝑓 𝑖 𝑡 , r𝑖 𝑡 )︀}︀ 𝑇 𝑡=1 }︁ |𝒟| 𝑖=1 .(4.4)
The second variant of our approach, which alleviates the need for a teacher model, involves constructing an improved response by sampling multiple times from the learner itself.We refer to this approach as the self-distillation variant.Step 2: Policy improvement.With the aforementioned data construction schemes, we can now train a model on these datasets.While in general, any offline RL approach can be used to train on these data, in our experiments we adopt an approach based on weighted supervised learning [35] due to ease of experimentation and its simplicity.In particular, we perform a weighted supervised regression, where the weights are given by the exponential transformation of the reward values in D.</p>
<p>Reward-weighted RL:
max 𝜃 E 𝑥 𝑖 ∼ D [︃ 𝑇 ∑︁ 𝑡=1 log 𝜋 𝜃 (ỹ 𝑖 𝑡 |𝑠 𝑖 𝑡 ) • exp(𝑟 𝑡 𝑖 /𝜏 ) ]︃ ,(4.6)
where  is a temperature parameter to further expand or narrow the difference between good and bad actions.In our preliminary experiments, we found that Equation 4.6 can often induce a bias towards increasing log likelihoods of responses where rewards are high, prioritizing updates on easy problems where rewards are already high.To address this issue, we apply a slight modification to Equation 4.6 and center the exponentiated rewards around the mean value averaged across all attempts on a given prompt, akin to advantage-weighted regression [34].We find that the use of advantages in place of rewards helps us avoid the "rich-gets-richer" phenomenon with easy problems.and the rollout terminates as soon as the current response is judged to be correct according to the environment's answer verification function.Put in other words, we terminate the rollout as soon as the reward is equal to the reward for the oracle response: (,  test  ) = (,  * ).This protocol invokes queries to the reward function after each turn in the rollout.Since several reward function queries are performed, we refer to this approach as "with oracle".RISE can also be run in a mode that avoids the need to query the answer checker or the reward function within a rollout.In this case, we run full-length rollouts by forcing the model to retry, ignoring the correctness of the response.We then utilize a self-consistency mechanism [51] based on majority voting to decide the candidate response at the end of each turn.Concretely, at the end of each turn , we identify the response by running a majority vote over all response candidates from the previous turns (maj</p>
<p>Inference at
(︁ 𝑦 test 1 , 𝑦 test 2 , • • • , 𝑦 test 𝑗 )︁
), including turn .We call this "without oracle".A schematic illustration of these approach is shown in Figure 3.Most of our evaluations use no oracle.each time the model improves its response, it is allowed to check its answer against an environment and terminate early as soon as a correct answer is found; or (2) without oracle (Right): we ask the model to sequentially revise its own responses j times, and perform majority voting on all candidate outputs from different turns to obtain the final response.If the turn number  is larger than the iteration number , the agent only keeps the most recent history with  interactions to avoid test-time distribution shift.</p>
<p>At iteration , since the agent is able to improve its own response from  to  + 1 when  ≤ , to avoid test time distribution shift, in both modes, we use a size  shift window to store the most recent conversation history when the turn number  is larger than the iteration number .</p>
<p>Practical Algorithm and Implementation Details</p>
<p>A complete algorithmic pseudocode for each approach is shown in Appendix C. We trained 7B models via RISE and found that these models often could not adhere to response style and instructions for improving their responses when generating on-policy data.As a result, before running on-policy data collection, we find it often useful to run an initial phase of supervised fine-tuning on in-domain, multi-turn rollouts generated from a capable model to provide style and instruction-following information to the learner.We call this the "knowledge boosting" stage.We then run on-policy rollouts starting from a boosted model.In each iteration, we generate 1 trajectory for each unique problem.We then run fine-tuning, with hyperparameters and details in Appendix D. For iterative fine-tuning, we find that starting from the base model but training on data from all iterations thus far is more beneficial than continued fine-tuning from the checkpoint obtained in the previous iteration.</p>
<p>When and Why is Self-Improvement Over Turns Possible?</p>
<p>A natural question to ask is why self-improvement with RISE even possible.One might surmise that the model may simply not have enough knowledge to correct its own mistakes if it is unable to correctly answer the problem in the first turn.Then, why is it possible to teach the model to correct its own mistakes?In this section, we provide the reason why this kind of self-improvement is possible, supported with empirical evidence to justify our hypotheses.</p>
<p>Iteratively teaching a model how to make updates on a given response can be crucial when representing the target distribution  * (|) requires more capacity than what the model   affords by conditioning on only the input prompt tokens.When the target distribution requires greater capacity, learning a sequence of conditionals,   ( +1 |,  0: ) followed by marginalization is expected to induce a more  flexible marginal distribution over   given .This hypothesis is akin to the difference between diffusion models [42] and variational autoencoders (VAEs) [25] in image generation: iteratively fitting a sequence of generative distributions over intermediate noisy inputs in a diffusion model gives rise to a more flexible distribution [43] than monolithic variational auto-encoding, even though diffusion models still utilize an evidence lower-bound objective(ELBO).While the diffusion process utilizes hand-designed noise schedules, RISE utilizes the base model itself to induce iterative improvements.</p>
<p>To verify if this hypothesis is true, we tracked the training un-weighted, negative log-likelihood loss (NLL) values for the oracle response  * given the input prompt  marginalized over intermediate steps in a multi-turn rollout, and compared it against the NLL values − log   ( * |) attained by directly attempting to predict the final response in Figure 4 (labeled as "Classic").Concretely, we sampled 256 prompts  and their oracle responses  * and computed the average − log   ( * |) across all , along with a 95% confidence interval for different checkpoints during training.We find that for any given number of epochs (including fractional number of epochs on the x-axis), the NLL value is lower when conditioning on multi-turn data that RISE generates in comparison with oracle responses to the prompts obtained from an expert.This suggests that RISE is able to utilize the computation of tokens from previous turns to model the target distribution.We also measure the average NLL loss on all samples through training, sampled i.i.d.from the training dataset for RISE and classic fine-tuning and observe a similar trend: RISE is able to reduce loss more than the standard approach, attaining lower perplexity values (Figure 5).</p>
<p>Of course, in problems that require "knowledge-based" question answering, it is not possible for the model to produce any meaningful improvements because learning  * (|) is not bounded by insufficient capacity of   (|), but is rather unable to match  * due to the absence of features that are critical to learn the correct mapping from  to .We expect that training with RISE would only incentivize hallucinations in this case [24], since more input tokens appearing from previous attempts would only provide easier ways to pick up on spurious correlations.However, this is not the failure mode on reasoning problems [27], where maj@K rates at turn 1 tend to be higher than pass@1 as we find in our experiments (indicating that performance can be improved by sampling the model itself).In fact, in Figure 6 we also show that the sequential procedure learned by RISE can even solve a significant fraction of problems that were unsolved by pass@B for much larger  in the first turn, indicating that it learns to index into the pre-trained knowledge of the model in a different manner as opposed to simply translating the pass@K performance into the pass@1 performance of the model, that majority of single-turn approaches are believed to be doing.</p>
<p>Figure 6: Fraction of problems unsolved by pass@B at the first turn that sequential 5-turn sampling from RISE solves,</p>
<p>where  = 5 ×  ( is the x-axis).RISE can solve several challenging problems that sampling at the first turn with much larger budgets cannot solve.</p>
<p>Experimental Evaluation</p>
<p>The goal of our experiments is to demonstrate the efficacy of RISE in instilling language models with the ability to self-improve their responses over turns.Our experiments answer the following questions: To this end, we compare RISE to other prior and baseline approaches, and perform ablations on GSM8K [11] and MATH [18].</p>
<p>Baselines, comparisons, and evaluation.We compare RISE to several prior methods that attempt to induce similar self-improvement capabilities: (a) self-refine [21,31] that prompts a base model to critique and revise its mistakes; (b) GloRE [17], which trains a separate reward model to locate errors and a refinement model to improve responses of a base LLM; and(c) self-consistency [51], which runs majority voting on multiple responses from the first turn as a baseline to compare to our sequential strategy.We tried to construct fair comparisons between RISE and these methods by using a similar-sized model [23,58], but differences in the base model, training data, and evaluation setups still prohibits us from performing an apples-to-apples comparison in some cases.Nonetheless, we can still hope to understand the ballpark of improvement by contextualizing our results with these prior works.We also compare to V-STaR [19], but since this is not an fair comparison, we defer it to Appendix B.</p>
<p>We evaluate RISE in both modes at inference time: with and without an oracle (Section 4.3) at the end of five turns.Concretely, these metrics are defined as follows:</p>
<p>• with oracle, "p1@t5": this run terminates the rollout as soon as the response is correct.In other words, this metric allows queries to the final answer verifier at the end of each turn.• without oracle, "m1@t5": this run does not terminate the rollout before five turns, and we compute the maj@1 performance on the candidates produced in each turn as detailed in Section 4.3.</p>
<p>We also compare maj@K performance at the first turn for all the models we train ("m1@t1", "m5@t1").</p>
<p>Approach GSM8K [10] MATH [18] w/o oracle w/ oracle w/o oracle w/ oracle m1@t1 → m5@t1 → m1@t5 p1@t5 m1@t1 → m5@t1 → m1@t5 p1@t5</p>
<p>Self-Refine [31]</p>
<p>→ m1@t3 → p1@t3 → m1@t3 → p1@t3 Base 10.5 22.4 (+11.9)7.1 (-3.4) 13.0 (+2.5) 1.9 5.1 (+3.2) 1.9 (0.0) improvement (in brown) between 1-turn (m5@t1) and 5-turn (m1@t5) performance w/o an oracle on both GSM8K and MATH.This performance gap is even larger when oracle early termination is allowed (p1@t5 w/ oracle).Self-Refine [31] degrades performance across the board when used without an oracle, and attains minor performance improvements when used with an oracle.GLoRE trains a separate refinement model, but still performs worse than RISE; more details about it are in Appendix B.</p>
<p>Using RISE on top of a better base model (Mistral-7B) is also effective (positive improvements with multiple turns), and note the m1@t5 performance of Mistral-7B exceeds even state-of-the-art math models such as Eurus-7B-SFT [58].Simply running single-turn SFT on data utilized by RISE is not effective at inducing a self-improvement capability, implying that the algorithmic design choices in RISE are crucial for performance.Color coding indicates numbers that can be compared to each other.</p>
<p>Does RISE improve performance over multiple turns compared to other approaches?</p>
<p>Main results.We present the comparisons in Table 1.First, note that RISE ("Iteration 1" and "Iteration 2") boosts up the LLama2 base model's five-turn performance by 15.1% and 17.7% respectively with each iteration on GSM8K and 3.4% and 4.6% on MATH, w/o any oracle.Interestingly, we found using prompting-only self-refine [31] largely degrades performance across the board, even with a strong proprietary model, GPT-3.5.The strongest 7B base models, Mistral-7B and Eurus-7B-SFT [58], when coupled with standard prompting, are only able to improve their performance, but only by 5.3% / 11.6% and 0.9% / 4.0% respectively on GSM8K and MATH, which is significantly lower than our approach.The performance of GLoRE improves only by 3.4% on GSM8K (over two turns), but this is still lower than our approach, which improves by 6.3% in two turns and 13.4% in three turns (see Appendix B.1).This indicates that RISE is effective in teaching models how to improve their own errors.To summarize, training with RISE gives the largest performance improvement gains compared to other approaches both with and without the use of an oracle, and these gains are transferred to other base models.</p>
<p>One might also hypothesize that the performance gains with RISE here are largely a result of utilizing queries to an off-the-shelf more capable model for providing supervision and not the algorithmic approach for data collection and training.To address this hypothesis, we store all the data generated by RISE from more capable models and train on this data via standard single-turn SFT ("SFT on oracle data).Since not all of this data are guaranteed to be correct, we also run this experiment on only the correct responses in these oracle data.Observe in Table 1 that this procedure does not still instill self-improvement capabilities, largely preserving or degrading sequential ("maj@1@turn5") performance compared to simply sampling one response in the first turn.This means that the algorithmic design of RISE is critical in enabling it to learn self-improvement capabilities, as opposed to simply the use of expert supervision.</p>
<p>Can RISE Effectively Make Use of Mistakes and Correct Them?</p>
<p>One concern that arises from prior results on self-refinement or self-correction is whether the model can truly correct itself over turns or whether the improvement comes from the effect of sampling more answers and picking the best one.In Table 1, we see that sequentially improving responses via RISE ("maj@1@turn5") outperforms sampling 5 responses in parallel at the first turn and applying a majority vote on them ("maj@5@turn1").Please note that this comparison utilizes an equal number of samples, with the only difference being that these samples are drawn in parallel at the first turn in one case and sequentially at the end of five turns in the other.Comparing maj@5 performance at the end of 1 turn and 5 turns, we observe a consistent 4% to 8% improvement on GSM8K and an 6.5% improvement on MATH (with Mistral-7B model).This means that RISE can imbue models with a self-improvement ability, while running parallel sampling alone on any model cannot endow the same ability.Even the maj@5@turn1 performance of standard single-turn SFT on the data used by RISE is substantially worse than the sequential maj@1@turn5 performance of RISE, implying that the algorithmic protocol of RISE plays a critical underlying role.Finally, we also remark that in Figure 6, we showed that the sequential procedure learned by RISE over five turns could solve a significant fraction of problems that were unsolved by pass@B for much larger values of  ≫ 5 in the first turn, implying that sequential RISE can actually tackle prompts that were not solvable by simply sampling more responses in the first turn.</p>
<p>One might also speculate if these improvements in sequential improvement ability largely come at a cost of reduced improvements in first turn performance.In addition, we also observe that running multiple iterations of RISE still preserves the first turn performance while improving the 5-turn performance.</p>
<p>How Does the Base Model Affect RISE?</p>
<p>The performance of RISE with Llama2-7B on an absolute scale is lower than the best models specifically fine-tuned on math data (e.g., Eurus-7B-SFT or Mistral-7B).However, we find that RISE is still effective on top of Mistral-7B base model.In fact, our performance at the end of five turns outperforms one of the best 7B SFT models, customized to math reasoning.Compare the m1@t5 performance of Eurus-7B-SFT and Mistral-7B in RISE (ours), to find that Mistral-7B + RISE outperforms Eurus-7B-SFT.</p>
<p>Self-Distillation Version of RISE</p>
<p>We also compare the performance of RISE with entirely self-generated data and supervision (Equation 4.4,  = 16) after one iteration directly on top of more capable models: Mistral-7B and Llama-3-8B on GSM8K in Table 2, without any knowledge boosting phase.We find that this variant also improves the 5-turn performance of the base model compared to the first turn: compare "m1@t5" vs "m1@t1" for both the models Llama-3-8B and Mistral-7B, where RISE boosts the sequential self-improvement performance by more than 1% compared to turn 1 performance w/o any oracle.</p>
<p>Of course, we also note that this version of RISE does not outperform the "m5@t1" performance of the  fine-tuned model.We expect this to be largely a function of one single iteration of training.Since the self-distillation version of RISE utilizes best-of-N sampling against the same model to produce supervision for self-improvement, RISE would first have to match the performance of best-of-N sampling before it can start to improve over it via reward maximization.Due to the significant gap between the base model's m5@t1 and m1@t5 performance, we expect that this will take quite a few iterations or a fully online RL algorithm.We did not have computational resources and infrastructure to run multiple iterations, but this is an interesting avenue for future work.In this self-distillation setting, we could also divide the computation between sequential and parallel sampling strategies to get the best results at the end of five turns.Nonetheless, this result shows that even by training on self-generated samples, RISE can actually amplify the sequential sampling performance of the base model.</p>
<p>Does the Performance of RISE Improve with Iterative Training?</p>
<p>Next, we attempt to understand if RISE improves with multiple rounds of training on on-policy data.As shown in Tables 1 and 2, the performance of RISE improves from iteration to iteration constantly.The 5-turn performance of RISE, both with and without an oracle, exhibits a clear improvement with more rounds.This implies that iterative self-training procedures of the form of STaR [61] can also be combined with RISE to train models for self-improvement.This also perhaps serves as a strong hint towards the potential utility of full online reinforcement learning (RL) techniques.</p>
<p>Does RISE Also Improve Sequential Performance on Out-of-Distribution Prompts?</p>
<p>In Table 3, our aim is to evaluate the robustness of the strategy induced by RISE on new, unseen prompts.Specifically, we compare the performance of the RISE model trained with a dataset on evaluation prompts from another dataset.Note in Table 3, these datasets include MATH, GSM8K, and SVAMP.Generally, we observe that the model trained on one dataset is still able to improve the base model's performance on another dataset over the course of sequential five turns.More concretely, while the base Llama2 model largely degrades its turn 1 performance over turn 5 performance, model's trained with RISE enable a positive performance improvement on these out-of-distribution prompts.This means that even though these models have not seen queries similar to the evaluation dataset, simply training with RISE on some kind of mathematical prompts still boosts the efficacy of the self-improvement strategy on a new distribution of test prompts.This finding suggests that RISE is capable of instilling self-improvement procedures that can generalize beyond the distribution of prompts in the fine-tuning data.</p>
<p>What Data Compositions and Data Quantity are Crucial for RISE?</p>
<p>We now study how different data compositions affect the performance of RISE with the goal of answering questions such as should we collect on-policy error correction data like DAgger [36] or should we bias towards high-quality off-policy data?.(a) Data composition for fine-tuning.We first study the necessity of using the interaction of error correction history for training RISE in Figure 7 (Left).We compare two approaches: model trained with oracle answers shown right after the query ("1-turn") and oracle answers shown after intermediate failed attempts ("Multi-turn") in Figure 7 (Left).Even though the latter trains on intermediate responses that may not always be correct, it attains a higher performance than simply training on the correct response for a given prompt.This highlights the importance of training on contexts that include a multi-turn interaction history depicting mistakes from the learner to improve self-improvement capabilities.</p>
<p>(b) Weighted supervised learning vs unweighted supervised learning.Next, we investigate the effect of reward-weighted RL on multi-turn data in RISE as opposed to simply imitating filtered successful data.We find that using all the data leads to improved performance over simply filtering good datain Figure 7 (Right), which reduces sample size.In Figure 7 (Left), we find that reward-weighted training improves performance on later turns, allowing us to better leverage all the sub-optimal data.</p>
<p>(c) On-policy vs off-policy data; self-generated vs. expert data.RISE runs on-policy rollouts and seeks improvements on responses that the learner produces.As shown in Figure 8 (Left), a "DAgger [36]"-style approach that seeks improvements on responses appearing in on-policy rollouts improves performance (green/orange) compared to using expert data alone (blue/pink).Conceptually, this addresses the train-test mismatch between the distribution of context tokens, enabling imitation learning methods to now target the correct distribution.In addition, recent work [24] has shown that LLMs often memorize "unfamiliar" examples generated by oracle models; by training on on-policy rollouts, we should be able to eliminate any such potential issues.Thus, while the model trained via offline imitation is able to reduce loss, these improvements do not generalize to new problems.GSM8K with different data sources."Expert" refers to the use of an oracle model, "On-policy" corresponds to sampling from the learner, and "Best-of-N" means using the best sample out of  from the learner (here  = 16).Right: Comparing RISE with oracle error feedback (pass@1 @ turn k; solid lines) to parallel sampling of 5 responses at turn 1 (pass@k @ turn 1; dashed lines) over number of turns  on the x-axis on GSM8K.Observe that sequential sampling with Iteration 1 and Iteration 2 RISE models consistently outperforms parallel sampling for all values of turn ; and the gap grows as the number of iterations increases.In contrast, this trend is absent for base and SFT models.</p>
<p>Pass@K vs Sequential Sampling via RISE</p>
<p>We now study the performance of sequential sampling with oracle feedback in GSM8K, unlike relying on majority voting as in Table 1.Specifically, we compare the performance of RISE with early termination of evaluation rollouts against pass@5 (not maj@5) performance of the RISE model at the first turn (which makes an equal number of queries to the ground-truth correctness indicator).Access to ground-truth correctness indicator is expected to improve performance for both parallel and sequential sampling unsurprisingly, but we see in Figure 8 (Right) that RISE is able to improve performance more beyond simply sampling more samples at the first turn and computing pass@K, despite this strong assumption of access to an oracle final answer verifier made by the parallel sampling approach.</p>
<p>We would expect parallel sampling via pass@K to be most performant when provided access to oracle answer checking as the model can choose to simply sample  independent responses, if the base model accuracy on this task is reasonable.Pass@K @ turn 1 also upper bounds the first turn accuracy of any procedure that does not query the oracle (e.g., with verifiers, with majority voting, etc.).Hence, access to oracle answer checking for each individual response presents the strongest result one could expect out of parallel sampling, in one turn.On the other hand, sequential sampling produces correlated samples and hence should, in principle, not be able to improve over parallel sampling, unless the model is unable to use the additional tokens and computation provided by the feedback self-improvement prompt to meaningfully correct itself.Since the sequential performance of the model is larger than the parallel performance above, this means that RISE indeed does this successfully.</p>
<p>Error Analysis of RISE over Turns</p>
<p>Following the protocol of Huang et al. [21], in this section, we perform an error analysis of the improvement performed by RISE (without any oracle feedback) to understand how the fraction of incorrect and correct responses changes over turns, when no oracle is used for early termination.We demonstrate this in the form of Venn diagrams in Figure 9. First note that there is a consistent increase in the portion of problems that stay correct and a consistent decrease in the portion of problems that stay incorrect, which means that the model is able to answer more and more problems as we increase the number of turns.Second, there is a consistent decrease in the number of problems that change from being correct to incorrect, which is often also not the case for strong proprietary LLMs such as GPT in Huang et al. [21].We also note that there is a decrease in the total number of incorrect problems that become correct in the subsequent turn, but this is a direct consequence of a shrinkage in the size of the incorrect response set as more problems become correct over turns.This indicates that one can induce "intrinsic" self-improvement (per the terminology of Huang et al. [21]) via fine-tuning with RISE, even though no external environment input is provided during evaluation.Qualitative examples.We also inspect several examples from the GSM8K test set to qualitatively understand the behavior of RISE over turns and observe different behavior patterns, that we show in Appendix B.2.For instance, the trained model may choose to completely rewrite its previous response if it is totally incorrect in order to get to the correct answer or make small edits if the previous response is mostly correct.Another interesting pattern we note is that the model implicitly has the ability to locate errors in previous responses and only refine the erroneous steps.Additionally, the model is tolerant of noisy environmental feedback when there is no oracle-assisted early termination.</p>
<p>Discussion, Future Directions, and Limitations</p>
<p>We presented RISE, an approach for fine-tuning LLMs to be able to improve their own responses over multiple turns sequentially.RISE prescribes an iterative RL recipe on top of on-policy rollout data, with expert or self-generated supervision to steer self-improvement.RISE significantly improves the selfimprovement abilities of 7B models on reasoning tasks (GSM8K and MATH), attaining an improvement over turns that previous work [21] has not observed in strong proprietary models.In addition, RISE outperforms prior approaches that attempt to tackle similar problems of refinement and correction, while being simpler in that it does not require running multiple models and works well with just one model.</p>
<p>Despite these good results, there are still many open questions and limitations.Due to computational constraints, we were not able to perform more than two iterations of training with RISE, and no more than one iteration when the supervision comes from the learner itself.Improving with self-generated supervision will likely require more computation and more iterations, since it will be slower than when using an off-the-shelf expert model.RISE requires running manual iterations and hence, a more "online" variant of RISE is likely the solution in the long run, especially when we wish to scale on-policy learning in a data-efficient manner.Additionally, while our work fine-tunes models on one task at a time, it will be certainly interesting to include data from the protocols specified by RISE into general instruction tuning and post-training pipelines.Given the results that fine-tuning on data prescribed by RISE does not hurt the first-turn performance of any model we trained, we hypothesize that adding this sort of data in general instruction-tuning pipelines should not hurt either, while enabling the sequential self-improvement capability that is largely absent from models today.</p>
<p>Appendices A. Additional Ablations on Data Composition and Weak-to-Strong Generalization</p>
<p>A.1. Inclusion of Correct-to-Correct Data</p>
<p>Intuitively, self-improvement over turns is largely only possible when the model can learn to verify the correctness of its previous response and decide to appropriately modify its response toward correctness.Thus far, the RISE has only trained on data that showed how to convert incorrect responses to correct responses but never illustrated how the model could act on correct responses.To understand if performance can be boosted by also illustrating examples of how the model could act on correct responses, we ran a number of ablations.We took the RISE data generated during Iteration 1 of training on GSM8K with Llama2-7B and modified the multi-turn rollouts to create several cases.First, we duplicated the correct response appearing at the end of every successful multi-turn rollout and trained for one extra turn.This should teach the model that correct responses should not be modified, unlike incorrect responses appearing in previous turns in the rollout.Second, we also ran a variant in which the correct response appearing at the end of every successful rollout is followed by a different correct response.This variant should teach the model that if it chooses to modify a correct response, it must still produce another correct response.</p>
<p>As shown in Table 4, all methods improved performance over the base model, though only appending with a successful rollout with a novel correct response leads to best performance.The default design of RISE in the main paper attains a close second position, and repeating a correct response at the end of a successful rollout largely reduces performance.We suspect that the poor performance of repeating the same correct response is largely a result of inducing spurious correlations due to data duplication.</p>
<p>RISE (Llama2) w/o oracle w/ oracle</p>
<p>m1@t1 → m5@t1 → m1@t5 p1@t5 Boost 32.9 45.</p>
<p>Table 4: Comparison of model performance on GSM8K with different mechanisms of adding correct-to-correct data in RISE.</p>
<p>Values in parentheses indicate improvement over m1@t1, note that appending a successful rollout with a a novel correct response leads to the highest performance gains.</p>
<p>To further investigate self-improvement capabilities, we analyzed the percentage of correct responses changing to incorrect responses in consecutive turns (T to T + 1), as illustrated in Figure 10.Generally, a decreasing trend suggests better self-improvement, while lower absolute values indicate better resistance to noisy feedback.The results reveal unexpected patterns across configurations.The Boost configuration shows the poorest performance, with the highest overall percentages and an increase from turn 4 to 5, suggesting that it struggles to consistently maintain correct responses.Repeating a correct response shows the lowest initial percentage (6.3%)but increases from turn 3 onward, indicating potential issues in extended interactions.Both Default RISE and appending a different correct response demonstrate a favorable trend, steadily decreasing from 12.3% to 3.9% and from 9.8% to 3.3%, respectively, suggesting a good balance between maintaining correct responses and allowing improvements.These findings provide nuanced insights into the stability and self-improvement capabilities of RISE and align with our earlier observation of its superior performance in overall accuracy.</p>
<p>A.2. Weak-to-Strong Generalization: RISE on Weak Model Data Improves Strong Models</p>
<p>In this section, we compare the performance of Llama2 and Mistral-7B with RISE in the weak-to-strong setting [3].Concretely, we are interested in using data generated via RISE with a weak model (Llama2-7B) to train a strong model (Mistral-7B).Our analysis reveals intriguing insights into the transferability of RISE-generated data across models of different capabilities.</p>
<p>RISE w/o oracle w/ oracle</p>
<p>m1@t1 → m5@t1 → m1@t5 p1@t5 Llama2-7B  As shown in Table 5, we find that Mistral-7B + Iteration 1 data generated from Llama2 outperforms training the Llama2-7B model itself on these data (i.e., Llama2-7B + Iteration1) on all the metrics reported with particularly significant improvements in multi-turn reasoning (m1@t5).In fact, training on multi-turn rollouts from Llama2-7B also outperforms training on on-policy Mistral-7B rollouts as well.Interestingly, we observed that training Llama2-7B on multi-turn rollouts from Mistral-7B performs worse than training on on-policy Llama2-7B rollouts, suggesting that Llama2-7B, despite its lower absolute performance, demonstrates more informative mistakes that can be leveraged to better boost the selfimprovement capability.This phenomenon underscores the importance of the quality and nature of errors in the training data, rather than just the overall performance of the model that generates them.These findings collectively suggest that the data generated from a weaker Llama2 model can still be used to induce a self-improvement capability in a stronger model, although the reverse is not true (as is also evident from the fact that using GPT-3.5 rollouts in the boosting phase for training does not improve performance for any model in Table 1).We suspect that this is becaue the reverse poses a much harder learning problem since a weak model need to internalize the mistakes of a stronger model, resulting in hallucinations and memorization [24].Note that training on these data does not degrade single-turn performance either.This hints at an added benefit of training with RISE: weak-to-strong generalization, which can be quite useful in practice when rolling out stronger models is expensive.</p>
<p>B. Additional Results</p>
<p>B.1. Complete Comparisons and Discussion: Extended Version of Table 1</p>
<p>We provide an extended version of Table 1, with a clear explanation of how we implement baselines and a discussion of comparisons.</p>
<p>Approach GSM8K [10] MATH [18] w/o oracle w/ oracle w/o oracle w/ oracle m1@t1 → m5@t1 → m1@t5 p1@t5 m1@t1 → m5@t1 → m1@t5 p1@t5 Self-Refine → m1@t3 → p1@t3 → m1@t3 → p1@t3 Base 10.5 22.4 (+11.9)7.1 (-3.4) 13.0 (+2.5) 1.9 5.1 (+3.2) 1.9 (0.0) 3.1 (+1.2) +Boost 32.9 45.3 (+12.4)26.5 (-6.4) 40.9 (+8.0) 5.5 6.5 (+1.0) 2.9  Observe that RISE attains the biggest performance improvements between 1-turn and 5-turn performance without the use of an oracle on both GSM8K and MATH.This performance gap is even larger when oracle early termination is allowed (5-turn w/ oracle).Self-Refine largely degrades performance across the board.GLoRE trains a separate refinement model, but still performs worse than RISE.</p>
<p>Comparison with Self-Refine [31].</p>
<p>To build a self-refine baseline [31] evaluation, we slightly modified our evaluation pipeline following the self-refine approach.In this setup (Figure 11), the model generates an initial response, and then the environment prompts the model to locate errors in the generated solution and refine its answer based on the initial response and the identified error.</p>
<p>Self-Refine</p>
<p>System: You are an AI language model designed to assist with math problem-solving.In this task, I will provide you with math problems.Your goal is to solve the problem step-by-step, showing your reasoning at each step.After you have finished solving the problem, present your final answer as ∖boxed{Your Answer}.</p>
<p><One-shot Example 17></p>
<p>User: <Query></p>
<p>Agent: <Initial Answer></p>
<p>User: There is an error in the solution above because of lack of understanding of the question.What is the error?To find the error, go through each step of the solution, and check if everything looks good.</p>
<p>Agent: <Critic></p>
<p>User: Now, rewrite the solution in the required format:</p>
<p>Agent: <Refined Answer> Figure 11: Prompt for Self-Refine: We follow the standard pipeline of the original paper, prompt the LLM to refine and correct its previous mistakes.However, our experiments show that without any oracle hint from the environment or human feedback, the self-refine approach leads to a degradation in performance across all models.Only when oracle feedback is available to assist with early termination does the self-refine approach provide a slight performance boost.This highlights the limitation of the self-refine structure in effectively improving model performance without external guidance, which is also observed in [22].</p>
<p>In contrast, the model trained with RISE can attain consistent performance improvements without relying on an oracle.By training the model to iteratively refine its responses, our method enables the model to self-correct and improve its performance over multiple turns.This showcases the effectiveness of our approach in comparison to the self-refine baseline, as it allows for more robust and consistent performance gains without the need for the oracle assistance.</p>
<p>Comparison with GLoRE [17].GLoRE is a multi-model system that relies on a student model to propose drafts, an Outcome-based Reward Model (ORM) or Step-wise ORM to locate errors at different granularity levels, and a Global or Local Refinement Model for adjusting these errors.Since no code was openly available for this approach, in our experiments, we compared to the numbers from the main paper Havrilla et al. [17].While the comparison against GLoRE is already apples-to-oranges since our method only trains a single end-to-end model, while GLoRE trains multiple models.Performance-wise, GLoRE's global and local refinement models show little to no improvement in overall accuracy without an oracle, and even exhibit decreasing accuracy in some cases.However, when an oracle is used to guide the refinement process, GLoRE demonstrates a 10% improvement on the 7B model in the GSM8K dataset.</p>
<p>As anticipated, since we run RISE from a less advanced base model (Llama2 7B), we observe a slightly lower absolute performance compared to GLoRE.However, RISE demonstrates its effectiveness in selfimprovement by sequentially enhancing its performance by an impressive 13.4% within just 3 turns without an oracle feedback, and by a remarkable 23.4% with an oracle on GSM8K.This showcase of RISE's capabilities is particularly noteworthy considering that GLoRE utilizes 3 independent modelsone for generating candidate solutions, one reward model for locating errors, and one refinement model for refinement.</p>
<p>Comparison with V-STaR [19].V-STaR requires training an additional verifier model to rank candidate answers generated by the targeted model, but it does not make any sequential revisions or improvements to a response.While comparing RISE to using a verifier for re-ranking the top 5 responses at the first turn (as a base comparison) would have been informative, we were unable to find this specific result in the original V-STaR paper.The results presented in the official table 6 for V-STaR correspond to running 64 samples, which improves the base model's performance by 35.2% for each prompt during evaluation.In contrast, our method, RISE, after the same amount of finetuning iterations (3 iterations) and using only 5 samples, improves upon the base model by 44.5% (calculated as 55.0% -10.5% = 44.5%).This comparison highlights RISE's efficiency in achieving significant improvements with fewer samples and iterations compared to V-STaR's approach of using a large number of samples without sequential refinement.</p>
<p>Moreover, V-STaR's performance is inherently bounded by the candidate generator's performance.As discussed in Section 5, if there is no correct response among the generated candidates, the problem remains unsolved.In contrast, we show in Figure 6 that RISE can also solve problems that were not solved by majority voting with a much higher budget in the first turn.Furthermore, we believe that combining V-STaR with RISE could lead to even better performance, as RISE can generate better models and a verifier can be complementarily used for filtering.</p>
<p>Comparison with other base models.Mistral-7B [23] and Eurus-7B-SFT [58] are models that exhibit comparable performance to our method in terms of the absolute maj@5 performance.However, it is crucial to note that these base models are fine-tuned using a vast amount of data, including data specifically tuned for math reasoning performance [58], while our model is fine-tuned on a single domain.</p>
<p>That said, we do show that fine-tuning with RISE can still enhance the performance of Mistral-7B models.</p>
<p>To summarize, our method offers several advantages over GLoRE and V-STaR, such as end-to-end error correction with a single model, superior performance with fewer samples, and the ability to solve problems that cannot be solved by random sampling in the first turn.Although our maj@1 performance is lower than GLoRE's base model EI, which is an apples-to-oranges comparison our ultimate 5-turn performance surpasses their best absolute performance in both oracle and non-oracle scenarios.</p>
<p>Compared to other base models like Mistral-7B and Eurus-7B-SFT, our method achieves comparable performance while being fine-tuned on a single domain, and it can be generalized to better base models.</p>
<p>B.2. Qualitative Examples from RISE</p>
<p>In this section, we provide some qualitative examples showing the nature of edits made by RISE on various datasets that we train on.We utilize the model from iteration 2 of RISE.</p>
<p>RISE Example (Small Edits)</p>
<p>Query: Mike plays ping pong for 40 minutes.In the first 20 minutes, he scores 4 points.In the second 20 minutes, he scores 25% more points.How many total points did he score?Turn-1 Response:</p>
<p>Step 1: In the first 20 minutes, Mike scores 4 points.</p>
<p>Step 2: In the second 20 minutes, Mike scores 25% more points than he did in the first 20 minutes.So, Mike scores 4 + 4 = 8 points in the second 20 minutes.</p>
<p>Step 3: To find the total points, we need to add the points scored in the first and second 20 minutes.total_points = 4 + 8 = 12.</p>
<p>Answer: 12 ✗ Turn-2 Response: I apologize for the mistake.Let's solve the problem step by step.</p>
<p>Step 1: In the first 20 minutes, Mike scores 4 points.</p>
<p>Step 2: In the second 20 minutes, Mike scores 25% more points than he did in the first 20 minutes.This means he scores 4 + 4 * 25% = 4 + 1 = 5 points in the second 20 minutes.</p>
<p>Step 3: To find the total points, we add the points scored in the first and second 20 minutes: 4 + 5 = 9 points.Answer: 9 ✓</p>
<p>RISE Example (Big Edits)</p>
<p>Query: In a candy machine, there are 22 more than four times the number of pink gumballs as there are blue gumballs.If there are 12 blue gumballs how many pink ones are there?</p>
<p>Turn-1 Response:</p>
<p>Step 1: Let's assume the number of blue gumballs as x.</p>
<p>Step 2: According to the given information, there are 22 more than four times the number of pink gumballs as there are blue gumballs.So, the number of pink gumballs can be expressed as 4x + 22.</p>
<p>Step 3: We are given that there are 12 blue gumballs, so let's substitute x with 12 in the equation.Answer: 54 ✗</p>
<p>Turn-2 Response:</p>
<p>Let's solve the problem step by step: Step 1: Let's denote the number of blue gumballs as x.</p>
<p>Step 2: We are given that the number of pink gumballs is 22 more than four times the number of blue gumballs.So, the number of pink gumballs can be expressed as 4x + 22.</p>
<p>Step 3: We are also given that there are 12 blue gumballs, so we can substitute x = 12 into the expression for the number of pink gumballs: 4 * 12 + 22 = 48 + 22 = 70 Therefore, there are boxed70 pink gumballs.Answer: 70 ✓ Figure 12: Examples of RISE correct its previous behavior in different modes.Some only make changes to a small part (small edits), some may directly rewrite most of its previous answers (big edits) because the first step in the previous answer is wrong.The mistaken steps of different turns are highlighted in red, and the correct are highlighted in green.This demonstrates shows how RISE can correct its previous answers and finally get to a correct answer.</p>
<p>RISE Example (Noise Tolerance)</p>
<p>Query: Ryan plants 2 flowers a day in his garden.After 15 days, how many flowers does he have if 5 did not grow?</p>
<p>Turn-1 Response:</p>
<p>Step 1: Ryan plants 2 flowers a day for 15 days, so he plants a total of 2 flowers/day * 15 days = 30 flowers in his garden.</p>
<p>Step 2: However, 5 flowers did not grow, so we need to subtract these non-growing flowers from the total count.Step 2: However, 5 flowers did not grow, so we need to add these flowers to the total count, as they were planted but did not grow.that always tells the model, "The answer is incorrect.Please try again.Here's the question: <Query>".We observe that from turn 2 to turn 5, the model is still able to output the correct answer in 3 out of 4 turns.In the response at turn 4, the model performs half of the solution correctly, and only the last step is wrong.In addition, in the turns where the model still outputs the correct answer, it doesn't directly copy the answer from the previous turn but instead comes up with a new paraphrase.This demonstrates that RISE provides the model with tolerance to noisy feedback and does not randomly change its behavior upon receiving a revisiting request.The mistaken steps of different turns are highlighted in red</p>
<p>RISE Example (Error Localization)</p>
<p>Query: Josh decides to try flipping a house.He buys a house for $80,000 and then puts in $50,000 in repairs.This increased the value of the house by 150%.How much profit did he make?Turn-1 Response:</p>
<p>Step 1: Josh buys a house for $80,000.</p>
<p>Step 2: He puts in $50,000 in repairs, which increases the value of the house by 150%.</p>
<p>Step 3: To calculate the increase in value, we multiply the cost of the house by 150% and add it to the cost of the house.increase_in_value = $80,000 * 150% = $120,000.</p>
<p>Step 4: To calculate the profit, we subtract the cost of the house from its new value.profit = increase_in_value -$80,000 = $120,000 -$80,000 = $40,000.Answer: 40, 000 ✗</p>
<p>Turn-2 Response:</p>
<p>Let's solve the problem step-by-step: Step 1: Josh buys a house for $80,000.</p>
<p>Step 2: He puts in $50,000 in repairs, and this increases the value of the house by 150%.</p>
<p>Step 3: The percentage increase in value is 150% of $80,000, which is $120,000 (80,000 * 1.5 = $120,000).</p>
<p>Step 4: To find the actual amount that would be added to the house's value after repairs, we need to subtract the amount he spent on repairs from the increased value, which is $120,000 -$50,000 = $70,000.</p>
<p>Step 5: The profit Josh would make is the difference between the cost of buying the house and the increased value after repairs, which is $70,000 -$80,000 = -$10,000 ($10,000 less than the initial cost).Answer: 0 ✗ Turn-3 Response: Let's solve the problem step-by-step: Step 1: Josh buys a house for $80,000.</p>
<p>Step 2: He puts in $50,000 in repairs.</p>
<p>Step 3: This increases the value of the house by 150%.</p>
<p>Step 4: To find the increase in value, we calculate 150% of the original price of the house: 150% of $80,000 = $120,000.</p>
<p>Step 5: The increase in value after repairs is the original price plus 150% of the original price: $80,000 + $120,000 = $200,000.</p>
<p>Step 6: Josh sells the house for $200,000 and made a profit of $200,000 -$80,000 = $120,000.Answer: 120, 000 ✗ Turn-4 Response: Let's solve the problem step-by-step: Step 1: Josh buys a house for $80,000.</p>
<p>Step 2: He puts in $50,000 in repairs, increasing the value of the house by 150%.</p>
<p>Step 3: The increase in value is 150% of $80,000, which is $120,000 (80,000 * 1.5 = $120,000).</p>
<p>Step 4: The total amount spent, including the purchase of the house and the repairs, is $80,000 + $50,000 = $130,000.</p>
<p>Step 5: The profit Josh made is the increase in value after repairs minus the total amount spent, which is $120,000 -$130,000 = -$10,000.(2) the model doesn't repeat its responses, which is a behavior we notice when evaluating some off-the-shelf models; and (3) the model is making meaningful changes to the incorrect steps.In summary, although the final answer is still incorrect, we observe that through RISE, the model is able to locate the error and perform local computation correctly.The mistaken steps of different turns are highlighted in red, and the correct steps in turn 2 is highlighted in green.end for 11: end for for testing.The training portions of both datasets are used to generate trajectories in each iteration of the RISE method, while the testing portions are held out for evaluating the performance of the models.Additionally, the SVAMP dataset, containing 1,000 problems, is used solely for evaluation purposes to demonstrate the generalizability of our method.</p>
<p>C. Pseudocode</p>
<p>For generating training trajectories, at iteration , the current model is rolled out for a maximum of  steps.Subsequently, an expert model, such as GPT-3.5, is employed to take the conversation history between the current model and the environment and sample 5 responses.Among these responses, the one with the highest reward is chosen, with arbitrary tie-breaking in case of multiple responses with the same highest reward.</p>
<p>D.4. Prompts used for seeding RISE on the GSM8K and MATH tasks</p>
<p>Here we show the prompt we used for data generation and model evaluation: User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students take both mathematics and physics.How many drama club students take neither mathematics nor physics?</p>
<p>Agent:</p>
<p>Step 1: To find the number of drama club students who take neither mathematics nor physics, we first need to determine the number of students taking only math, only physics, and both math and physics.</p>
<p>Step 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20 students take both mathematics and physics.</p>
<p>Step 3: To calculate the number of students taking only math, we subtract the number of students taking both from the total taking math: $36 -20 = 16$.</p>
<p>Step 4: Similarly, to calculate the number of students taking only physics, we subtract the number of students taking both from the total taking physics: $27 -20 = 7$.</p>
<p>Step 5: Now, to find the number of students taking neither math nor physics, we subtract the total number of students taking only math, only physics, and both from the total number of students in the drama club.This gives us $60 -16 -7 -20 = ∖boxed{17}$ students taking neither.Answer: ∖boxed{17}</p>
<p>Self-Refine One-shot example</p>
<p>User: Of the 60 students in the drama club, 36 take mathematics, 27 take physics and 20 students take both mathematics and physics.How many drama club students take neither mathematics nor physics?</p>
<p>Agent:</p>
<p>Step 1: To find the number of drama club students who take neither mathematics nor physics, we first need to determine the number of students taking only math, only physics, and both math and physics.</p>
<p>Step 2: Out of the 60 students in the drama club, 36 take mathematics and 27 take physics, and 20 students take both mathematics and physics.</p>
<p>Step 3: To calculate the number of students taking only math, we subtract the number of students taking both from the total taking math: $36 -20 = 16$.</p>
<p>Step 4: Similarly, to calculate the number of students taking only physics, we subtract the number of students taking both from the total taking physics: $27 -20 = 7$.</p>
<p>Step 5: Now, to find the number of students taking neither math nor physics, we add the number of students taking only math and only physics, then subtract this sum from the total number of students in the drama club.This gives us $60 -(16 + 7) = ∖boxed{37}$ students taking neither.</p>
<p>Answer: ∖boxed{37}</p>
<p>User: There is an error in the solution above because of lack of understanding of the question.What is the error?To find the error, go through each step of the solution, and check if everything looks good.</p>
<p>Agent:</p>
<p>Step 1: Correct.We need to find the number of students taking neither math nor physics.</p>
<p>Step 2: Correct.The given information is accurately stated.</p>
<p>Step 3: Correct.The calculation of students taking only math is correct.</p>
<p>Step 4: Correct.The calculation of students taking only physics is correct.</p>
<p>Step 5: Incorrect.The solution adds the number of students taking only math and only physics, but this does not account for the students taking both subjects.By subtracting this sum from the total number of students, the solution double-counts the students who take both math and physics.</p>
<p>User: Now, rewrite the solution in the required format: Agent: <Same as 16></p>
<p>Figure 1 :
1
Figure 1: Recursive Introspection (RISE).Using iterative multi-round training on on-policy rollouts and supervision from a reward function, RISE trains models that are capable of improving themselves over multiple turns.At inference, we run majority voting on candidate outputs from different turns to obtain the final response.</p>
<p>Figure 2 :
2
Figure 2: Left: Problem formulation.We convert single-turn problems into multi-turn MDPs as discussed in Section 4.1.The state is given by the prompt, history of prior attempts, and optional feedback from the environment.An action is a response generated from the LLM given the state of multi-turn interaction so far.Right: Data collection.We collect data by unrolling the current model  − 1 times followed by an improved version of the response, which is obtained by either (1) self-distillation: sample multiple responses from the current model, and use the best response, or (2) distillation: obtain oracle responses by querying a more capable model.In either case, RISE then trains on the generated data.</p>
<p>Figure 3 :
3
Figure 3: RISE Inference.There are two ways to query the model trained via RISE upon inference: (1) with oracle (Left):</p>
<p>Figure 4 :
4
Figure 4: The probability of the true answer given the prompt.Observe that model trained with RISE has higher probability for the true answer.</p>
<p>Figure 5 :
5
Figure 5: The training perplexity (loss) of fitting only the oracle answer or a sequence of answers.Observe that fitting a sequence of answers (RISE) reduces the loss more than fitting only the oracle answer (Classic).</p>
<p>( 1 )
1
How effectively can RISE improve performance over multiple sequential attempts (i.e., turns) at a given prompt?; (2) Does the performance of RISE improve with more rounds of iterative training?; (3) Does the self-improvement strategy induced by RISE generalize to novel problems that are out of the training domain?and finally; (4) What is the best data composition for training RISE?</p>
<p>the three aspects RISE: (a) the use of multi-turn rollout data for fine-tuning, (b) the use of unsuccessful / suboptimal rollouts via weighted supervised fine-tuning compared to naïve supervised learning, which only utilizes successful rollouts for fine-tuning; and (c) the use of on-policy rollouts and self-generated or oracle data.We will now perform controlled experiments to understand the effect of each of these factors on the overall performance of RISE.</p>
<p>Figure 7 :
7
Figure 7: Left: The importance of multi-turn interaction history and weighted objectives for training RISE.Note thattraining with multi-turn data leads to better self-improvement performance at the end of 5 turns, than one-turn data obtained from the original dataset with oracle answers from another model; also observe that using a weighted objective performs better.Right: The importance of using all rollouts for learning, instead of only successful rollouts or only successful responses in the data.Using all data performs best in our results.</p>
<p>Figure 8 :
8
Figure 8: Left: The importance of data sources used for training.We study the performance of the iteration 1 of RISE on</p>
<p>Figure 9 :
9
Figure 9: Change in the fraction of responses that transition their correctness values over the course of multi-turn rollouts from RISE, w/o oracle.Observe that in general, the fraction of Correct → Correct responses increases; Incorrect → Incorrect responses decreases; and the fraction of Correct → Incorrect responses also decreases, indicating that RISE (w/o any oracle) is able to iteratively improve its responses.</p>
<p>Figure 10 :
10
Figure 10: Percentage of correct responses in turn T that change to being incorrect in turn T+1.This figure illustrates the percentage of correct responses that change to incorrect responses across consecutive turns (T to T+1) for different model configurations.A continuously decreasing trend suggests better self-improvement performance.</p>
<p>Step 3 : 25 ✓Turn- 2
3252
Calculate the number of flowers that actually grew in Ryan's garden.flowers_grew = 30 flowers -5 flowers = 25 flowers.Answer: Response: Apologies for the mistake.Let's correct the solution.Step 1: In 15 days, Ryan plants 2 flowers a day, so he plants a total of 2 flowers/day * 15 days = 30 flowers in his garden.Step 2: Since 5 flowers did not grow, we need to subtract these non-growing flowers from the total count.Step 3: Calculate the number of flowers that actually grew in Ryan's garden.flowers_grew = 30 flowers -5 flowers = 25 flowers.Answer: 25 ✓ Turn-3 Response: Let's reevaluate the solution.Step 1: Ryan plants 2 flowers a day for 15 days, so he plants a total of 2 flowers/day * 15 days = 30 flowers in his garden.</p>
<p>Step 3 : 35 ✗Turn- 4 25 ✓Figure 13 :
33542513
Figure 13: Examples of RISE's tolerance of noisy feedback.In the evaluation where there is no oracle, we use a verifier</p>
<p>RISESystem:Figure 15 :
15
Figure 15: Prompt for RISE: We follow the standard pipeline of math problem solving, prompt the LLM to solve the problem and retry untill exceeds the maximum number of turns</p>
<p>Figure 16 :
16
Figure 16: One-shot Example: Demonstrates how to solve a math problem step-by-step and the expected answer format.</p>
<p>Figure 17 :
17
Figure 17: One-shot Example: Demonstrates the process of self-critique and error correction based on the self-critique.</p>
<p>Concretely, for each state in the dataset,    ∈  on-policy , we sample  responses ỹ
𝑡 [0], ỹ𝑖 𝑡 [1], • • • , ỹ𝑖 𝑡 [𝑁 ] ∼ 𝜋 𝜃 (•|𝑠 𝑖 𝑡 ), and use the best response from these 𝑁 candidates (as measured bythe associated reward values r𝑖 𝑡 [0], • • • , r𝑖 𝑡 [𝑁 ]) to relabel the model response at the next step 𝑡 + 1 in animprovement trajectory. Formally, say ỹ𝑖 𝑡 [𝑚] = arg max 𝑗∈[𝑁 ] 𝑟(𝑠 𝑖 , ỹ𝑖 𝑡 [𝑗]), then we label the responses inthe dataset 𝒟 on-policy at step 𝑡 + 1 with the improved response and its associated reward value r𝑖 𝑡 [𝑚]:Don-policy + self-distillation :={︁ {︀(︀𝑠 𝑖 𝑡+1 , ỹ𝑖 𝑡 [𝑚], 𝑓 𝑖 𝑡+1 , r𝑖 𝑡 [𝑚])︀}︀ 𝑇 −1 𝑡=0}︁ |𝒟| 𝑖=1.(4.5)</p>
<p>Table 1 :
1
RISE
3.1 (+1.2)
vs. other approaches (Self-Refine, GLoRE) and baselines.Observe that RISE attains the biggest performance</p>
<p>Table 2 :
2
RISE with self-distillation on GSM8K.RISE is able to improve 5-turn maj@1 performance of the model with entirely self-generated data and supervision, despite the fact that the base Mistral-7B model does not produce correct answers for several problems.</p>
<p>To understand the utility of different data compositions, we enlist
RISEw/o oraclew/ oraclem1@t1 → m1@t5p1@t5GSM8KLlama2 Base10.511.1 (+0.6) 13.9 (+3.4)Iteration 1 RISE Model trained on MATH19.3 32.6 (+13.3) 48.4 (+29.1)MATHLlama2 Base1.91.4 (-0.5)2.3 (+0.4)Iteration 1 RISE Model trained on GSM8K4.34.4 (+0.1)12.1 (+7.8)SVAMPLlama2 Base29.230.5 (+1.3) 34.0 (+4.8)Iteration 1 RISE Model trained on MATH30.131.4 (+1.2) 45.9 (+15.8)Iteration 1 RISE Model trained on GSM8K42.250.0 (+7.8) 63.6 (+21.4)</p>
<p>Table 3 :
3
Out-of-distribution generalization of RISE.We evaluate model fine-tuned on MATH on the GSM8K test set; model fine-tuned GSM8K on MATH; and the model fine-tuned on a mixture of GSM8K and MATH on the SVAMP data.Observe even though we train on OOD prompts, RISE can still improve sequential performance.</p>
<p>Table 5 : Weak-to-strong generalization on GSM8K.
5
Comparing performance of RISE when training on rollouts generated by Llama2-7B vs Mistral-7B.Note that training the Mistral-7B model on rollouts generated by the weaker Llama2-7B with RISE improves performance compared to using data generated by the Mistral-7B model itself.However, the reverse is not true: training the Llama2 model on Mistral's mistakes leads to worse performance, likely because errors from the Mistral-7B model are harder to comprehend for a worse base model.All values are in % accuracy, and values in parentheses indicate improvement over m1@t1.</p>
<p>Table 6 : Comparing RISE with other approaches (Self-Refine, GLoRE, and V-STaR) and other baseline approaches.
6</p>
<p>Figure14: Examples of RISE not being able to correct its error within 5 steps but demonstrating meaningful error localization behavior.Even in turn 2, it has already solved the problem at step 4 but mistakenly takes another step and reaches the wrong answer.The following turns are unable to correct this small error.Though this problem remains unsolved, we observe that (1) the model is able to stick to the correct steps, where all responses reach an intermediate step of 12000 correctly, except for the last response, where the model tries to modify the answer from one step ahead;
Therefore, Josh made aboxed0 profit.Answer: 0 ✗Turn-5 Response: Let's solve the problem step-by-step: Step 1: Josh buys a house for $80,000. Step 2: He puts in $50,000in repairs. Step 3: The repairs increase the value of the house by 150%. Step 4: The increase in value is $50,000, which is150% of $33,333 (50,000 / 1.5 = $33,333). Step 5: The repairs cost $50,000, and the increase in value is $33,333. Step6: Therefore, Josh's profit is $33,333 -$50,000 = -$16,667. Therefore, Josh made a 16676 profit.Answer: 16676 ✗</p>
<p>Data Collection at Iteration  1:  ′  ←  ′  −1 2: for index  in {1, ..., ||} do for step  ′ in {1, ...,  − 1} do   ′ ← arg max    −1 (•|(   ,    ,   )  ′ −1 =1 +   ′ )    ′ +1 ,    ′ ← env.step(   ′ ,    ′ )    ′ ← arg max π(•|(   ,    ,   )  ′ −1 =1 +   ′ )    ′ +1 ,    ′ ← env.step(   ′ ,    ′ )    ′ ← arg max    (•|(   ,    ,   )  ′ −1 =max{1, ′ − } +   ′ )    ′ +1 ,   ′ ← env.step(   ′ ,    ′ )
3:𝑠 1 ← 𝑥 𝑖4:5:6:7: 8:𝑓 𝑖 𝑇 ′ = retry message + 𝑥 𝑖 if 𝑟 𝑖 𝑇 ′ = 1 then9:break10:end if11:end for12: 13:if 𝑟 𝑖 𝑇 ′ ̸ = 1 then 𝑇 ′ ← 𝑇 ′ + 114:15:16: 17:end if 𝒟 ′ 𝑇 ← 𝒟 ′ 𝑇 ∪{︀(︀𝑠 𝑖 𝑡 , 𝑦 𝑖 𝑡 , 𝑓 𝑖 𝑡 , 𝑟 𝑖 𝑡)︀}︀ 𝑇 ′5:6:𝑓 𝑖 𝑇 ′ = retry message + 𝑥 𝑖7:end for9:ỹ𝑖 𝑇 ′ ← majority voting{𝑦 𝑖 𝑡 } 𝑇 ′ 𝑡=110:
Algorithm 1 =1 18: end for Algorithm 2 Inference at iteration  1: for index  in {1, . . ., ||} do 2:  1 ←   3: for step  ′ in {1, . . .,  } do 4: 8: for step  ′ in {1, . . .,  } do</p>
<p>AcknowledgementsThis work was done at CMU.We thank Fahim Tajwar, Abitha Thankaraj, Amrith Setlur, and Charlie Snell for their feedback and informative discussions.This work was supported by ONR under N000142412206, OpenAI superalignment fast grants, and used the Delta system and JetStream2[16]at the National Center for Supercomputing Applications through CIS240249 and CIS230278, supported by the National Science Foundation.We thank OpenAI for providing GPT-4 credits for academic use.D. Experimental DetailsD.1. Hyperparameters for Fine-Tuning with RISEFor finetuning, we utilize the FastChat codebase, but we customize the loss function to be weighted by reward.The base models are directly loaded from Hugging Face: hrefhttps://huggingface.co/metallama/Llama-2-7b-hfLlama-2-7b-chat-hf and Mistral-7B-Instruct-v0.2.The hyperparameters used for finetuning are specified in Table7.HyperparameterD.2. Inference HyperparametersFor API-based models, such as GPT-3.5, we directly query the official web API provided by OpenAI.In the case of open-source models, we utilize FastChat to serve the model as a web API and interact with the environment through API calls.Serving a 7B model requires a single A100 or A40 GPU.To control the randomness and length of answers generated by the LLMs, we employ the hyperparameters specified in Table8.Hyperparameters/DescriptionD.3. DatasetsThe GSM8K dataset consists of 7,473 problems in the training portion and 1,319 problems in the testing portion.Similarly, the MATH dataset is divided into 7,500 problems for training and 1,000 problems
Gkd: Generalized knowledge distillation for auto-regressive sequence models. Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem, arXiv:2306.136492023arXiv preprint</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu, 2023</p>
<p>Jonathan D Chang, Wenhao Shan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D Lee, Wen Sun, arXiv:2404.08495Dataset reset policy optimization for rlhf. 2024arXiv preprint</p>
<p>A new era in software security: Towards self-healing software via large language models and formal verification. Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain, Youcheng Sun, Mohamed Amine Ferrag, Lucas C Cordeiro, arXiv:2305.147522023arXiv preprint</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, Fireact: Toward language agent fine-tuning. 2023</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, arXiv:2401.013352024arXiv preprint</p>
<p>Contrastive chain-of-thought prompting. Ken Yew, Guizhen Chia, Chen, Anh Luu, Soujanya Tuan, Lidong Poria, Bing, arXiv:2311.092772023arXiv preprint</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman, arXiv:1912.015882019arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D Goodman, arXiv:2404.03683Stream of search (sos): Learning to search in language. 2024arXiv preprint</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Critic: Large Language Models can Self-Correct with Tool-Interactive Critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Jetstream2: Accelerating cloud computing via jetstream. Y David, Jeremy Hancock, John Fischer, Winona Michael Lowe, Marlon Snapp-Childs, Pierce, J Eric Suresh Marru, Matthew Coulter, Brian Vaughn, Nirav Beck, Edwin Merchant, Gwen Skidmore, Jacobs, 10.1145/3437359.3465565Practice and Experience in Advanced Research Computing, PEARC '21. New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Roberta Railneau, arXiv:2402.10963Glore: When, where, and how to improve llm reasoning via global and local refinements. 2024arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>V-star: Training verifiers for self-taught reasoners. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal, arXiv:2402.064572024arXiv preprint</p>
<p>Agentcoder: Multi-agentbased code generation with iterative testing and optimisation. Dong Huang, Qingwen Bu, M Jie, Michael Zhang, Heming Luck, Cui, arXiv:2312.130102023arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, International Conference on Machine Learning. PMLR2022</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Unfamiliar finetuning examples control how language models hallucinate. Katie Kang, Eric Wallace, Claire Tomlin, Aviral Kumar, Sergey Levine, 2024</p>
<p>Auto-encoding variational bayes. P Diederik, Max Kingma, Welling, 2022</p>
<p>Beyond a*: Better planning with transformers via search dynamics bootstrapping. Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, Yuandong Tian, arXiv:2402.140832024arXiv preprint</p>
<p>Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, Houwen Peng, arXiv:2403.04706Common 7b language models already possess strong math capabilities. 2024arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, arXiv:2308.03688Evaluating llms as agents. 2023arXiv preprint</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong, 2023ICLR</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. Xue Bin Peng, Aviral Kumar, Grace Zhang, Sergey Levine, arXiv:1910.001772019arXiv preprint</p>
<p>Reinforcement learning by reward-weighted regression for operational space control. Jan Peters, Stefan Schaal, Proceedings of the 24th international conference on Machine learning. the 24th international conference on Machine learningACM2007</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stephane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. Geoffrey Gordon, David Dunson, Miroslav Dudík, the Fourteenth International Conference on Artificial Intelligence and StatisticsFort Lauderdale, FL, USAApr 201115of Proceedings of Machine Learning Research</p>
<p>Direct nash optimization: Teaching language models to self-improve with general preferences. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, Tengyang Xie, arXiv:2404.037152024arXiv preprint</p>
<p>Branch-solve-merge improves large language model evaluation and generation. Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, Xian Li, arXiv:2310.151232023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Offline rl for natural language generation with implicit language q learning. Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine, arXiv:2206.118712022arXiv preprint</p>
<p>Deep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, Surya Ganguli, 2015</p>
<p>How to train your energy-based models. Yang Song, Diederik P Kingma, 2021</p>
<p>A fast integrated planning and control framework for autonomous driving via imitation learning. Liting Sun, Cheng Peng, Wei Zhan, Masayoshi Tomizuka, Dynamic Systems and Control Conference. American Society of Mechanical Engineers201851913</p>
<p>Inverse reinforcement learning without reinforcement learning. Gokul Swamy, Sanjiban Choudhury, J Andrew Bagnell, Zhiwei Steven Wu, 2024</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv:2402.101762024arXiv preprint</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, arXiv:2211.142752022arXiv preprint</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, arXiv:2212.100012022arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv: Arxiv-2305.162912023arXiv preprint</p>
<p>Math-shepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, CoRR, abs/2312.089352023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, 2022NeurIPS</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, arXiv:2306.022242023arXiv preprint</p>
<p>Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, Anima Anandkumar, arXiv:2306.15626LeanDojo: Theorem Proving with Retrieval-Augmented Language Models. 2023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Advancing llm reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, arXiv:2404.020782024arXiv preprint</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, arXiv:2211.11890Tempera: Test-time prompting via reinforcement learning. 2022arXiv preprint</p>
<p>Niket Tandon, and Uri Alon. Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, arXiv:2402.054032024arXiv preprintIn-context principle learning from mistakes</p>
<p>Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.04406Language agent tree search unifies reasoning acting and planning in language models. 2023arXiv preprint</p>
<p>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar, Archer, arXiv:2402.19446Training language model agents via hierarchical multi-turn rl. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2264 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2264</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2264</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-238359979</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2202.03199v1.pdf" target="_blank">AI Research Associate for Early-Stage Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) has been increasingly applied in scientific activities for decades; however, it is still far from an insightful and trustworthy collaborator in the scientific process. Most existing AI methods are either too simplistic to be useful in real problems faced by scientists or too domain-specialized (even dogmatized), stifling transformative discoveries or paradigm shifts. We present an AI research associate for early-stage scientific discovery based on (a) a novel minimally-biased ontology for physics-based modeling that is context-aware, interpretable, and generalizable across classical and relativistic physics; (b) automatic search for viable and parsimonious hypotheses, represented at a high-level (via domain-agnostic constructs) with built-in invariants, e.g., postulated forms of conservation principles implied by a presupposed spacetime topology; and (c) automatic compilation of the enumerated hypotheses to domain-specific, interpretable, and trainable/testable tensor-based computation graphs to learn phenomenological relations, e.g., constitutive or material laws, from sparse (and possibly noisy) data sets.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2264.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2264.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyPhy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cyber-Physicist (CyPhy) AI Research Associate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI system that automatically enumerates, compiles, and tests mathematically viable physical hypotheses using a topology-aware ontology (I-nets) and an incremental, 'simple-first' search over hypothesis structures, compiling candidates to interpretable tensor computation graphs for parameter estimation from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CyPhy (Cyber-Physicist)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates hypotheses by enumerating interaction-net (I-net) structures (generalized Tonti diagrams) in a directed acyclic search graph where nodes are symbolic I-nets and edges are incremental augmentations (add variable via topological operator, add latent co-chain, add phenomenological link). The search is 'simple-first' (Occam-driven) and guided by an A* algorithm whose cost is a loss combining structural penalty terms (penalizing violations of common Tonti structure and non-parsimonious constructs) and data-fit residual errors for closed-cycle constraints; each complete I-net hypothesis is automatically compiled to either symbolic differential expressions (SymPy) or to numerical tensor-based computation graphs (PyTorch) that implement mimetic/integral discretizations and learn phenomenological parameterizations (e.g., via regression or gradient-based optimization). The system supports multiple interpretations of I-nets (differential, integral, discrete), scale-aware nonlocal phenomenological relations, and noise-robust integral formulations with polynomial underfitting for denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>physics / general scientific discovery / computational physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>early-stage hypothesis generation and initial (in)validation; open-ended exploration with targeted model selection</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Trade-off between structural parsimony and empirical fit is handled explicitly in the objective: total loss = (sum of structural penalties for I-net violations) + (weighted sum of residual errors for each independent constraint). User-specified relative weights control the importance of parsimony vs data fit. The search also prioritizes 'Tonti-like' structures by assigning penalties to nonstandard phenomenological links, thereby implicitly biasing against highly novel (structurally deviant) hypotheses unless supported by strong data-fit improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>A* search over hypothesis DAG optimizing a composite loss (structural penalties + residual errors) with user-tunable weights; within each hypothesis, parameters of phenomenological links are optimized via regression (e.g., LASSO/PDE-FIND) or by training compiled PyTorch computation graphs (gradient-based optimization). Heuristic pruning prevents expansion of incomplete hypotheses whose structural penalty alone exceeds feasible thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared qualitatively to symbolic-regression approaches (PDE-FIND / LASSO-based symbolic regression with finite-difference derivative estimation); differences highlighted between local differential discretizations and integral, mimetic discretizations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative: For noisy ultrasound elastodynamics data, hypotheses interpreted in integral form with larger spatial/temporal neighborhoods and polynomial underfitting (mimetic discretization) produced better, more robust fits than strictly local finite-difference-based symbolic regression; no numeric comparison of novelty vs feasibility trade-offs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In elastodynamics (noisy ultrasound on heterogeneous samples), nonlocal/integral hypothesis interpretations with scale-aware phenomenological relations and mimetic discretization were preferred over local differential forms; this suggests that in heterogeneous media, feasibility (robust estimability from noisy data) favors less-local, scale-aware constitutive parameterizations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2264.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Regression (Schmidt & Lipson 2009)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential rule-based symbolic regression for distilling natural laws</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic-regression approach that searches for compact analytic expressions (laws) from data using evolutionary or search heuristics and scoring based on fit and parsimony.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling Free-Form Natural Laws from Experimental Data.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Symbolic Regression (Schmidt & Lipson)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work: sequential/rule-based symbolic regression that attempts to discover closed-form laws from measurements by searching a space of symbolic expressions and selecting those that minimize error and expression complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific discovery / physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>discovering governing equations and compact analytic relations from data</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2264.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman: A Physics-Inspired Method for Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired symbolic regression method that decomposes problems using dimensional analysis and neural nets to recover analytic formulas from data efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A Physics-Inspired Method for Symbolic Regression.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman (Udrescu & Tegmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as an example of physics-inspired symbolic regression that leverages physics heuristics (dimensional analysis, function decomposition) to find closed-form relations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>physics / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>discovering analytic expressions for physical relations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2264.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Physicist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toward an AI Physicist for Unsupervised Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach to unsupervised learning that aims to partition data into regions explained by different simple physical laws, combining divide-and-conquer with simplification and unification heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toward an AI Physicist for Unsupervised Learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Physicist (Wu & Tegmark)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related work: systems that emulate human strategies (divide-and-conquer, unsupervised partitioning, simplicity priors) to discover piecewise physical laws and simpler representations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific discovery / physics</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>unsupervised discovery of piecewise physical laws</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2264.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PDE-FIND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Data-Driven Discovery of Partial Differential Equations (PDE-FIND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sparse-regression symbolic discovery pipeline that expresses PDEs as sparse linear combinations of candidate nonlinear terms and fits coefficients (e.g., via LASSO) using derivative estimates from data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Data-Driven Discovery of Partial Differential Equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PDE-FIND (Rudy et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used as an example/tool in experiments: after translating I-nets to symbolic ODE/PDE forms, the authors applied LASSO-regularized least squares (PDE-FIND) where each candidate term (including derivatives) is computed from data using finite differences or polynomial approximations to estimate coefficients for linear combinations of candidate terms.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>computational physics / data-driven PDE discovery</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>discovering governing PDEs from data via sparse regression</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>PDE-FIND use highlighted a practical tradeoff: symbolic/differential interpretation + finite-difference derivative estimation is sensitive to noise and discretization error, whereas CyPhy's integral/mimetic discrete interpretation can be more robust; no quantitative novelty-feasibility trade-off numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Sparse regression (LASSO) over a library of candidate terms; requires derivative estimation (finite difference/polynomial approximate).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Used as comparison/tool against CyPhy's integral/mimetic discretization approach.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Qualitative: Finite-difference-based PDE-FIND showed larger errors/less robustness on noisy data; integral/mimetic discretization from CyPhy gave preferable results in the elastodynamics example.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Finite-difference derivative estimation amplifies noise; integral formulations with underfitting/interpolation over larger neighborhoods are more robust for noisy experimental data in heterogeneous materials.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2264.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Networks (Cranmer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Symbolic Physics with Graph Networks / Graph-based physics models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph neural network approaches that learn physical interactions and sometimes symbolic structure by representing systems as graph-structured data (nodes, edges) and enforcing physical constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning Symbolic Physics with Graph Networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physics Graph Networks (Cranmer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work: graph-network-based models that capture relational structure of physical systems and enforce inductive biases from control theory and combinatorial structures to improve interpretability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning for physics / relational learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>learning interactions and dynamics in physical systems; symbolic-structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2264.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discovering Physical Concepts (Iten et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discovering Physical Concepts with Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural-network-based methods to learn latent variables that correspond to physically meaningful concepts from observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Discovering Physical Concepts with Neural Networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physical-Concept Discovery (Iten et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work: methods that train neural nets (often autoencoder-esque) to uncover low-dimensional latent representations that map to interpretable physical quantities.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / physics representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>latent-concept discovery and representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2264.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2264.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural network architectures that incorporate differential-equation residuals into loss functions to enforce physical laws while fitting data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physics-Informed Neural Networks (Raissi et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned as related/hybrid approaches that penalize both prediction error and PDE residuals in loss functions to incorporate known physics into learning, but noted as having built-in ontological biases and limitations in exploring unknown-unknowns.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>computational physics / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>solving forward/inverse PDE problems and physics-constrained learning</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Distilling Free-Form Natural Laws from Experimental Data. <em>(Rating: 2)</em></li>
                <li>AI Feynman: A Physics-Inspired Method for Symbolic Regression. <em>(Rating: 2)</em></li>
                <li>Data-Driven Discovery of Partial Differential Equations. <em>(Rating: 2)</em></li>
                <li>Toward an AI Physicist for Unsupervised Learning. <em>(Rating: 2)</em></li>
                <li>Learning Symbolic Physics with Graph Networks. <em>(Rating: 2)</em></li>
                <li>Discovering Physical Concepts with Neural Networks. <em>(Rating: 2)</em></li>
                <li>Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2264",
    "paper_id": "paper-238359979",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "CyPhy",
            "name_full": "Cyber-Physicist (CyPhy) AI Research Associate",
            "brief_description": "An AI system that automatically enumerates, compiles, and tests mathematically viable physical hypotheses using a topology-aware ontology (I-nets) and an incremental, 'simple-first' search over hypothesis structures, compiling candidates to interpretable tensor computation graphs for parameter estimation from data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CyPhy (Cyber-Physicist)",
            "system_description": "Generates hypotheses by enumerating interaction-net (I-net) structures (generalized Tonti diagrams) in a directed acyclic search graph where nodes are symbolic I-nets and edges are incremental augmentations (add variable via topological operator, add latent co-chain, add phenomenological link). The search is 'simple-first' (Occam-driven) and guided by an A* algorithm whose cost is a loss combining structural penalty terms (penalizing violations of common Tonti structure and non-parsimonious constructs) and data-fit residual errors for closed-cycle constraints; each complete I-net hypothesis is automatically compiled to either symbolic differential expressions (SymPy) or to numerical tensor-based computation graphs (PyTorch) that implement mimetic/integral discretizations and learn phenomenological parameterizations (e.g., via regression or gradient-based optimization). The system supports multiple interpretations of I-nets (differential, integral, discrete), scale-aware nonlocal phenomenological relations, and noise-robust integral formulations with polynomial underfitting for denoising.",
            "research_domain": "physics / general scientific discovery / computational physics",
            "problem_type": "early-stage hypothesis generation and initial (in)validation; open-ended exploration with targeted model selection",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Trade-off between structural parsimony and empirical fit is handled explicitly in the objective: total loss = (sum of structural penalties for I-net violations) + (weighted sum of residual errors for each independent constraint). User-specified relative weights control the importance of parsimony vs data fit. The search also prioritizes 'Tonti-like' structures by assigning penalties to nonstandard phenomenological links, thereby implicitly biasing against highly novel (structurally deviant) hypotheses unless supported by strong data-fit improvements.",
            "optimization_strategy": "A* search over hypothesis DAG optimizing a composite loss (structural penalties + residual errors) with user-tunable weights; within each hypothesis, parameters of phenomenological links are optimized via regression (e.g., LASSO/PDE-FIND) or by training compiled PyTorch computation graphs (gradient-based optimization). Heuristic pruning prevents expansion of incomplete hypotheses whose structural penalty alone exceeds feasible thresholds.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Compared qualitatively to symbolic-regression approaches (PDE-FIND / LASSO-based symbolic regression with finite-difference derivative estimation); differences highlighted between local differential discretizations and integral, mimetic discretizations.",
            "comparative_results": "Qualitative: For noisy ultrasound elastodynamics data, hypotheses interpreted in integral form with larger spatial/temporal neighborhoods and polynomial underfitting (mimetic discretization) produced better, more robust fits than strictly local finite-difference-based symbolic regression; no numeric comparison of novelty vs feasibility trade-offs reported.",
            "domain_specific_findings": "In elastodynamics (noisy ultrasound on heterogeneous samples), nonlocal/integral hypothesis interpretations with scale-aware phenomenological relations and mimetic discretization were preferred over local differential forms; this suggests that in heterogeneous media, feasibility (robust estimability from noisy data) favors less-local, scale-aware constitutive parameterizations.",
            "uuid": "e2264.0"
        },
        {
            "name_short": "Symbolic Regression (Schmidt & Lipson 2009)",
            "name_full": "Sequential rule-based symbolic regression for distilling natural laws",
            "brief_description": "A symbolic-regression approach that searches for compact analytic expressions (laws) from data using evolutionary or search heuristics and scoring based on fit and parsimony.",
            "citation_title": "Distilling Free-Form Natural Laws from Experimental Data.",
            "mention_or_use": "mention",
            "system_name": "Symbolic Regression (Schmidt & Lipson)",
            "system_description": "Mentioned as related work: sequential/rule-based symbolic regression that attempts to discover closed-form laws from measurements by searching a space of symbolic expressions and selecting those that minimize error and expression complexity.",
            "research_domain": "general scientific discovery / physics",
            "problem_type": "discovering governing equations and compact analytic relations from data",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.1"
        },
        {
            "name_short": "AI Feynman",
            "name_full": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "brief_description": "A physics-inspired symbolic regression method that decomposes problems using dimensional analysis and neural nets to recover analytic formulas from data efficiently.",
            "citation_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression.",
            "mention_or_use": "mention",
            "system_name": "AI Feynman (Udrescu & Tegmark)",
            "system_description": "Mentioned in related work as an example of physics-inspired symbolic regression that leverages physics heuristics (dimensional analysis, function decomposition) to find closed-form relations.",
            "research_domain": "physics / symbolic regression",
            "problem_type": "discovering analytic expressions for physical relations",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.2"
        },
        {
            "name_short": "AI Physicist",
            "name_full": "Toward an AI Physicist for Unsupervised Learning",
            "brief_description": "An approach to unsupervised learning that aims to partition data into regions explained by different simple physical laws, combining divide-and-conquer with simplification and unification heuristics.",
            "citation_title": "Toward an AI Physicist for Unsupervised Learning.",
            "mention_or_use": "mention",
            "system_name": "AI Physicist (Wu & Tegmark)",
            "system_description": "Mentioned as related work: systems that emulate human strategies (divide-and-conquer, unsupervised partitioning, simplicity priors) to discover piecewise physical laws and simpler representations.",
            "research_domain": "general scientific discovery / physics",
            "problem_type": "unsupervised discovery of piecewise physical laws",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.3"
        },
        {
            "name_short": "PDE-FIND",
            "name_full": "Data-Driven Discovery of Partial Differential Equations (PDE-FIND)",
            "brief_description": "A sparse-regression symbolic discovery pipeline that expresses PDEs as sparse linear combinations of candidate nonlinear terms and fits coefficients (e.g., via LASSO) using derivative estimates from data.",
            "citation_title": "Data-Driven Discovery of Partial Differential Equations.",
            "mention_or_use": "use",
            "system_name": "PDE-FIND (Rudy et al.)",
            "system_description": "Used as an example/tool in experiments: after translating I-nets to symbolic ODE/PDE forms, the authors applied LASSO-regularized least squares (PDE-FIND) where each candidate term (including derivatives) is computed from data using finite differences or polynomial approximations to estimate coefficients for linear combinations of candidate terms.",
            "research_domain": "computational physics / data-driven PDE discovery",
            "problem_type": "discovering governing PDEs from data via sparse regression",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "PDE-FIND use highlighted a practical tradeoff: symbolic/differential interpretation + finite-difference derivative estimation is sensitive to noise and discretization error, whereas CyPhy's integral/mimetic discrete interpretation can be more robust; no quantitative novelty-feasibility trade-off numbers provided.",
            "optimization_strategy": "Sparse regression (LASSO) over a library of candidate terms; requires derivative estimation (finite difference/polynomial approximate).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Used as comparison/tool against CyPhy's integral/mimetic discretization approach.",
            "comparative_results": "Qualitative: Finite-difference-based PDE-FIND showed larger errors/less robustness on noisy data; integral/mimetic discretization from CyPhy gave preferable results in the elastodynamics example.",
            "domain_specific_findings": "Finite-difference derivative estimation amplifies noise; integral formulations with underfitting/interpolation over larger neighborhoods are more robust for noisy experimental data in heterogeneous materials.",
            "uuid": "e2264.4"
        },
        {
            "name_short": "Graph Networks (Cranmer et al.)",
            "name_full": "Learning Symbolic Physics with Graph Networks / Graph-based physics models",
            "brief_description": "Graph neural network approaches that learn physical interactions and sometimes symbolic structure by representing systems as graph-structured data (nodes, edges) and enforcing physical constraints.",
            "citation_title": "Learning Symbolic Physics with Graph Networks.",
            "mention_or_use": "mention",
            "system_name": "Physics Graph Networks (Cranmer et al.)",
            "system_description": "Mentioned in related work: graph-network-based models that capture relational structure of physical systems and enforce inductive biases from control theory and combinatorial structures to improve interpretability and generalization.",
            "research_domain": "machine learning for physics / relational learning",
            "problem_type": "learning interactions and dynamics in physical systems; symbolic-structure discovery",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.5"
        },
        {
            "name_short": "Discovering Physical Concepts (Iten et al.)",
            "name_full": "Discovering Physical Concepts with Neural Networks",
            "brief_description": "Neural-network-based methods to learn latent variables that correspond to physically meaningful concepts from observational data.",
            "citation_title": "Discovering Physical Concepts with Neural Networks.",
            "mention_or_use": "mention",
            "system_name": "Physical-Concept Discovery (Iten et al.)",
            "system_description": "Mentioned in related work: methods that train neural nets (often autoencoder-esque) to uncover low-dimensional latent representations that map to interpretable physical quantities.",
            "research_domain": "machine learning / physics representation learning",
            "problem_type": "latent-concept discovery and representation learning",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.6"
        },
        {
            "name_short": "Physics-Informed Neural Networks (PINNs)",
            "name_full": "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations",
            "brief_description": "Neural network architectures that incorporate differential-equation residuals into loss functions to enforce physical laws while fitting data.",
            "citation_title": "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations.",
            "mention_or_use": "mention",
            "system_name": "Physics-Informed Neural Networks (Raissi et al.)",
            "system_description": "Mentioned as related/hybrid approaches that penalize both prediction error and PDE residuals in loss functions to incorporate known physics into learning, but noted as having built-in ontological biases and limitations in exploring unknown-unknowns.",
            "research_domain": "computational physics / machine learning",
            "problem_type": "solving forward/inverse PDE problems and physics-constrained learning",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": null,
            "uuid": "e2264.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data.",
            "rating": 2,
            "sanitized_title": "distilling_freeform_natural_laws_from_experimental_data"
        },
        {
            "paper_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression.",
            "rating": 2,
            "sanitized_title": "ai_feynman_a_physicsinspired_method_for_symbolic_regression"
        },
        {
            "paper_title": "Data-Driven Discovery of Partial Differential Equations.",
            "rating": 2,
            "sanitized_title": "datadriven_discovery_of_partial_differential_equations"
        },
        {
            "paper_title": "Toward an AI Physicist for Unsupervised Learning.",
            "rating": 2,
            "sanitized_title": "toward_an_ai_physicist_for_unsupervised_learning"
        },
        {
            "paper_title": "Learning Symbolic Physics with Graph Networks.",
            "rating": 2,
            "sanitized_title": "learning_symbolic_physics_with_graph_networks"
        },
        {
            "paper_title": "Discovering Physical Concepts with Neural Networks.",
            "rating": 2,
            "sanitized_title": "discovering_physical_concepts_with_neural_networks"
        },
        {
            "paper_title": "Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations.",
            "rating": 1,
            "sanitized_title": "physicsinformed_neural_networks_a_deep_learning_framework_for_solving_forward_and_inverse_problems_involving_nonlinear_partial_differential_equations"
        }
    ],
    "cost": 0.0142775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AI Research Associate for Early-Stage Scientific Discovery</p>
<p>Morad Behandish 
Palo Alto Research Center (PARC)
3333 Coyote Hill Road94304Palo Alto, (www.parc.com)California</p>
<p>John T Maxwell Iii 
Palo Alto Research Center (PARC)
3333 Coyote Hill Road94304Palo Alto, (www.parc.com)California</p>
<p>Johan De Kleer 
Palo Alto Research Center (PARC)
3333 Coyote Hill Road94304Palo Alto, (www.parc.com)California</p>
<p>AI Research Associate for Early-Stage Scientific Discovery</p>
<p>Artificial intelligence (AI) has been increasingly applied in scientific activities for decades; however, it is still far from an insightful and trustworthy collaborator in the scientific process. Most existing AI methods are either too simplistic to be useful in real problems faced by scientists or too domain-specialized (even dogmatized), stifling transformative discoveries or paradigm shifts. We present an AI research associate for early-stage scientific discovery based on (a) a novel minimally-biased ontology for physics-based modeling that is context-aware, interpretable, and generalizable across classical and relativistic physics; (b) automatic search for viable and parsimonious hypotheses, represented at a high-level (via domain-agnostic constructs) with built-in invariants, e.g., postulated forms of conservation principles implied by a presupposed spacetime topology; and (c) automatic compilation of the enumerated hypotheses to domain-specific, interpretable, and trainable/testable tensor-based computation graphs to learn phenomenological relations, e.g., constitutive or material laws, from sparse (and possibly noisy) data sets.</p>
<p>Introduction</p>
<p>Data-driven AI methods have been applied extensively in the past few decades to distill nontrivial physicsbased insights (scientific discovery) and to predict complex dynamical behavior (scientific simulation) (Stevens et al. 2020). Notwithstanding their effectiveness and efficiency in classification, regression, and forecasting tasks, statistical learning methods can hardly ever evaluate the soundness of a function fit, explain the reasons behind observed correlations, or provide sufficiently strong guarantees to replace parsimonious and explainable scientific expressions such as differential equations (DE). Hybrid methods such as constructing "physicsinformed/inspired/guided" architectures for neural nets and loss functions that penalize both predication and DE residual errors (Raissi, Perdikaris, and Karniadakis 2019;Wei and Chen 2019;Daw et al. 2020) and graphnets based on control theory and combinatorial structures (Cranmer et al. 2019;Seo and Liu 2019;Sanchez-Gonzalez et al. 2020) are all important steps towards explainability; however, the built-in ontological biases in most machine learning (ML) frameworks prevent them from thinking outside the box to discover not only the known-unknowns, but also unknown-unknowns, during early stages of the scientific process.</p>
<p>Contributions</p>
<p>We present 'cyber-physicist' (CyPhy), our novel AI research associate for early-stage scientific process of hypothesis generation and initial (in)validation, grounded in the most invariable mathematical foundations of classical and relativistic physics. Our framework distinguishes itself from existing rule-based reasoning, statistical learning, and hybrid AI methods by:</p>
<p>(1) an ability to rapidly enumerate and test a diverse set of mathematically sound and parsimonious physical hypotheses, starting from a few basic assumptions on the embedding spacetime topology;</p>
<p>(2) a distinction between non-negotiable mathematical truism (e.g., conservation laws or symmetries), that are directly implied by properties of spacetime, and phenomenological relations (e.g., constitutive laws), whose characterization relies indisputably on empirical observation, justifying targeted use of data-driven methods (e.g., ML or polynomial regression); and</p>
<p>(3) a "simple-first" strategy (following Occam's razor) to search for new hypotheses by incrementally introducing latent variables that are expected to exist based on topological foundations of physics.</p>
<p>Background AI-assisted discovery of scientific knowledge has been an active area of research (Langley 1998) long before the rise of GPU-accelerated deep learning (DL). As computational power and data sources are becoming more ubiquitous, model-based, data-driven, and hybrid AI methods are playing an increasingly more important role in various scientific activities (Kitano 2016;Raghu and Schmidt 2020).</p>
<p>Related efforts to our approach to scientific hypothesis generation and evaluation are mostly engineered after how humans approach scientific discovery, including sequential rule-based symbolic regression (Schmidt and Lipson 2009;Udrescu and Tegmark 2020), latent space representation learning via deep neural net autoencoders Nautrup et al. 2020) and strategic combinations of divide-and-conquer, unsupervised learning, simplification by penalizing description lengths in the loss function, and a posteriori unification by clustering (Wu and Tegmark 2019). While these and other efforts have shown great promise for elevating AI to the role of an autonomous, creative, and insightful collaborator that can offer human scientists a set of viable options to consider, their applications have remained limited to rather basic examples.</p>
<p>On the more domain-specialized end, DL has been widely successful in classification, regression, and forecasting tasks in scientific areas as diverse as turbulence (Miyanawala and Jaiman 2017;Wang et al. 2020), chaotic particle dynamics (Breen et al. 2020), molecular chemistry and materials science (Butler et al. 2018), and protein engineering (Yang, Wu, and Arnold 2019), among others. Most specialized DL architectures are ad hoc, designed (by humans) using narrow, domainspecific, and (by construction) biased knowledge and expertise, stifling innovation and surprise. Moreover, DL models that successfully capture nontrivial patterns in data are often difficult to explain, lack guarantees even within their training space, and poorly extrapolate to out-of-training scenarios (Mehta et al. 2019). Training such models for high-dimensional physics problems requires enormous data, which is either unavailable or too costly to obtain in many experimental sciences.</p>
<p>The Cyber-Physicist</p>
<p>We introduce an AI tool that can bridge multiple levels of abstraction, using a domain-agnostic representation scheme to express a wide range of mathematically viable physical hypotheses by exploiting common structural invariants across physics. Our approach entails:</p>
<p>(a) defining a relatively unbiased ontology rooted in fundamental abstractions that are common to all known theories of classical and relativistic physics;</p>
<p>(b) constructing a constrained search space to enumerate viable hypotheses with postulated invariants, e.g., built-in conservation laws that are consistent with the presupposed spacetime topology; and (c) automatically assembling interpretable ML architectures for each hypothesis, to estimate parameters for phenomenological relations from empirical data.</p>
<p>At the core of (a) is a powerful mathematical abstraction of physical governing equations rooted in algebraic topology and differential geometry (Frankel 2011). This abstraction leads to an ontological commitment to the relationship between physical measurement and basic properties of the embedding spacetime-but nothing more, to leave room for innovation and surprise. This relationship has been shown to be responsible for the analogies and common structure across physics (Tonti 2013), exploited in (b), along with search heuristics based on analogical reasoning. Each viable hypothesis is automatically compiled to an interpretable "computation graph"-tensor-based architecture, akin to a neural net with convolution layers to compute differentiation/integration and (non)linear local operators for constitutive equations-for a given cellular decomposition of embedding spacetime using well-established concepts from cellular homology (Hatcher 2001) and exterior calculus of differential and discrete forms (Bott and Tu 1982;Hirani 2003) that are under-utilized in AI.</p>
<p>Topological Foundations of Physics</p>
<p>The key enabler of our AI framework is a simple type system for (a) physical variables, based on how they are measured in spacetime; and (b) physical relations, based on their (topological vs. metric) nature, and the variables they connect. Following the ground-breaking discoveries by a number of mathematicians, physicists, and electrical engineers (Kron 1963;Roth 1955;Branin 1966) towards a general network theory, Tonti explained the fascinating analogies across classical and relativistic physics in his pioneering life-long work (Tonti 2013) by reframing them in the language of cellular homology, leading to informal classification diagrams. Tonti diagrams can be formalized as directed graphs with strongly typed nodes for variables and edges for relations. The variable are typed as (d 1 , d 2 )forms based on their measurement on d 1  and d 2 dimensional submanifolds (d 1  and d 2 cells) of space and time, respectively. For instance, to model heat transfer in (3+1)D spacetime, temperature is typed as a (0, 1)form because it is measured at spatial points (0cells) and during temporal intervals (1cells), whereas heat flux is typed as a (2, 1)form because it is measured over spatial surfaces (2cells) and during temporal intervals (1cells). In classical calculus, both of these variables reduce to scalar and vector fields, probed at spatial points and at temporal instants, to write down compact pointwise DEs; however, keeping track of the topological and geometric character of DEs is key to a deeper understanding of how known physical theories work, and building on top of it for AI-assisted discovery of new physics grounded in mathematical foundations. The spatiotemporal cells (or embedding manifolds) are further classified as primary or secondary, endowed with inner or outer orientations, respectively, depending on how the variables change sign in a hypothetical reversal of spacetime orientation (Mattiussi 2000). The cells are related by topological duality (Fig. 1 (a)). For example, an inner-oriented curve (1cell,  1 ) sitting in primary space, along which temperature variations are measured, is dual to an outer-oriented surface (2cell,  2 ) sitting in secondary space, over which heat flux is measured, and the two cells are spatially registered and consistently oriented, if we embed them in Figure 1: A topology-aware representation for physics (Tonti 2013): (a) variables associated with spatial and temporal cells of various dimensions give rise to primary forms and secondary forms (also called pseudo-forms); (b) resulting in 32 possible types for spatio-temporal forms, and an underlying structure for fundamental theories of physics. two co-located "copies" of 3D space.</p>
<p>The relations among variables on the Tonti diagrams are typed based on the pairs of variables they relate, as well as the nature of the relation itself:</p>
<p> Topological relations map spatiotemporal forms to forms of one higher dimension in space or time via incidence relations, and are responsible for propagation of information in spacetime through incident cells.  Metric relations locally map forms defined over dual cells to one another based on phenomenological properties, spatial lengths, and temporal durations, and are responsible for local distortion of information.  Algebraic relations are in-place, i.e., map a given form to another from of the same type, and can be used to capture initial/boundary conditions, external source/sink terms, or cross-physics couplings between variables of the same type on different diagrams. The relations are drawn in Fig. 1 as vertical arrows, horizontal (or horizontal-diagonal) arrows, and loops (1cycles), respectively. The interpretation of these relations to symbolic or numerical operations depends on the choice of a cellular decomposition of spacetime on which they operate. For example, using a continuum spacetime with infinitesimal cells, the variables are viewed as differential forms and the topological operators on them are interpreted as exterior derivatives (Bott and Tu 1982). In elementary calculus, these oper-ators give rise to gradient, curl, and divergence in space and partial derivative in time in terms of scalar and vector fields that are proxy to these forms, leading to partial DEs (PDEs). In a discrete (or semi-discrete) setting, on the other hand, the same diagram can be used to produce integral (or integro-differential) equations that capture the same fundamental conservation and constitutive realities, where the variables are viewed as cochains, also called discrete forms (or mixed forms, e.g., discrete in space, differential in time, or vice versa) and the topological operators become co-boundary operators that are fundamental in cellular homology (Hatcher 2001). For example, using a semi-discretization in space with integral quantities associated with 0, 1, 2 and 3cells on a pair of staggered unstructured meshes in 3D, while keeping time as a continuum, the semidiscrete form of the heat equation as a system of ordinary DEs (ODEs) (Fig. 2 (a)). Upon discretization of time, one obtains algebraic equations that can be solved or parameter estimated via tensor-based ML.</p>
<p>It is important to note that 3D meshes in space and 1D time-stepping are not the only ways to provide a combinatorial topology to interpret Tonti diagrams in a discrete setting. Another example is a directed graph representation of lumped-parameter networks such as system models in Modelica or electrical circuits in Spice. The variables in this case are associated with nodes, edges, and meshes (i.e., primitive cycles) and incidence Figure 2: Tonti diagrams are recipes to generate governing equations in different contexts, defined by a continuum, discrete, or semi-discrete setting and a topological embedding of the variables based on how they are measured. The conservation laws in terms of co-boundary operators result directly from assumed properties of space (or spacetime), while constitutive relations must be learned from data (e.g., via regression/ML).</p>
<p>relations are obtained from graph connectivity and edge directions. The same topological operator that leads to a spatial divergence, discretized by a sum of fluxes on the incident faces of a volume in a 3D mesh, also leads to a superposition of forces on interacting planets, sum of currents in/out of junctions in electrical circuits, and superposition of torques on kinematic chains ( Fig. 2 (b, c, d)). Both ODEs and PDEs and their integral or integro-differential forms upon full or semidiscretization can be captured with the same (abstract) operators, and Tonti diagrams serve as recipes to compose them to generate governing equations. </p>
<p>An Ontology for Scientific Process</p>
<p>We present a novel representation, called 'interaction networks' (I-nets), based on a generalization of Tonti diagrams that is expressive and versatile enough to accommodate novel scientific hypotheses, while retaining a basic commitment to philosophical principles such as parsimony (Occam's razor), measurement-driven classification of variables, and separation of non-negotiable mathematical properties of spacetime (homology) from domain-specific empirical knowledge (phenomenology). Data science is employed to help only with the latter.</p>
<p> We conceptualize three levels of abstraction related by inheritance: abstract (symbolic) I-nets  discrete (cellular) I-nets  numerical (tensor-based) I-nets.</p>
<p> At each level, an I-net instance is contextualized by user-defined assumptions on spacetime topology, semantics of physical quantities, and structural restrictions on allowable diagrams based on analogical reasoning and domain-specific insight (if available).</p>
<p> Every I-net instance distinguishes between topological and metric operators; however, it has additional degrees of freedom (beyond Tonti diagrams) for the latter to allow for phenomenological relations among variables that may not be dual to each other.</p>
<p>The latter is motivated by the observation that some existing middle-ground theories use phenomenological relations to capture a combination of topological and metric aspects.</p>
<p>We define an abstract (symbolic) I-net on a single Dspace as a finite collection of primary and/or secondary co-chain complexes that are inter-connected by phenomenological links, as shown in Fig. 4 (a). Each co-chain complex is a sequence of (symbolic) dforms related by (symbolic) co-boundary operators from dforms to (d + 1)forms (0  d  D). The interpretation of d  (d + 1) maps depends on the embedding dimension D; for instance, if D = 1 the only option for the input is d = 0 leading to a simple partial derivative (0  1), whereas for D = 3, we can have d = 0, 1, 2 leading to gradient (0  1), curl (1  2), and divergence (2  3) operations, respectively.  These sequences may represent different (mechanical, electrical, thermal, etc.) domains of physics. Although, for most known physics, each domain's theory appears as one pair of (primary and secondary) sequences in tandem, connected by horizontal (or horizontal-diagonal) constitutive relations leading to Tonti diagrams, we do not make any such restriction when looking for new theories. The cross-sequence links can thus represent both single-physics constitutive relations and mutli-physics coupling interactions. Conservation laws, on the other hand, are represented by a balance between the output of a topological operator and an external source/sink, the latter being represented by a loop.</p>
<p>It is often more convenient to define product spaces (e.g., separate 3D space and 1D time, as opposed to 4D spacetime) in which conservation laws are stated as sums of incoming topological relations being balanced against an external source/sink. To accommodate such representations, we define abstract (symbolic) I-nets on a product of a D 1 space and a D 2 space as multisequences of co-chains, connected by phenomenological links, as before. It is possible to form 2 2 = 4 possible such multi-sequences with various orientation combinations, two of which lead to so-called mechanical and field theories (Tonti 2013), shown in Fig. 1 for (3+1)D spacetime and repeated in Fig. 4 (b) for higher-dimensional pairs of abstract topological spaces. This construction is generalized to products of more than two spaces in a straightforward combinatorial fashion.</p>
<p>Based on the topological context, the semantics for co-boundary operators is unambiguously determined by the dimensions of the two variables (i.e., co-chains) they relate. However, phenomenological links require specifying a parameterization of possibly nonlinear, in-place, and purely metric relations they represent, using unknown parameters that must be learned from data.</p>
<p>Once one or more hypotheses are specified in the language of abstract (symbolic) I-nets with unknown phenomenological parameters (e.g., thermal conductivity in the earlier heat transfer example), the parameters can be optimized to fit the data and the regression error can be used to evaluate the fitness of hypotheses.</p>
<p>A Search for Viable Hypotheses</p>
<p>Having defined a combinatorial representation of viable hypotheses that are partially ordered in terms of complexity, the next step is to generate and test the hypotheses in a "simple-first" fashion. The search space is defined by a directed acyclic graph (DAG) whose nodes (i.e., 'states') represent symbolic I-net instances. The edges (i.e., state transitions) represent generating a new I-net structure by incrementally adding complexity to Figure 5: The search space for the dynamics of a pendulum in 1D time. The complete hypotheses (yellow nodes) correspond to I-net structures that pose new nontrivial equations to be tested against data, whereas incomplete hypotheses (white nodes) have "dangling" branches that are completed in their child states. the parent state. Each action can be one or composition of (a) defining a new symbolic variable, in an existing co-chain complex, by applying a topological operator to an existing variable; (b) defining a new variable in a latent co-chain complex; and (c) adding phenomenological links of prescribed form and unknown parameters, connecting existing variables. The search is guided by a loss function determined by how well the hypotheses represented by these I-net structures explain a given dataset. The algorithm may also be equipped with userspecified heuristic rules to prune the search space or prioritize paths that are perceived as "more likely" due to structural analogies with existing theories.</p>
<p>The input to the search algorithm includes the bare minimum contextual information such as the assumed underlying topology, a preset number of physical domains, and the types of measured variables, e.g., spatiotemporal associations, tensor ranks and shapes, and dimensions/units. The search starts from an "initial" I-net instance (i.e., the 'root') that embodies only measured variable(s) with no initial edges except the ones that are asserted a priori, e.g., loops for initial/boundary conditions or source terms, if applicable. The spatio-temporal types and physical semantics for these variables are provided by the experimentalist.</p>
<p>For example, consider a simple pendulum (Fig. 5 (a)). We have only 1D time, leading to a topological space of inter-connected time instants  0 ,  0 =  0 + /2 and time intervals  1 = ( 0 ,  0 + ),  1 = (  1 ,  1 + ) to which data may be associated. Suppose we are given time series data for angular position ( 0 ). The initial I-net instance is a single symbolic variable for this 0form, which can be differentiated only once in primary 1D time to obtain angular velocity as a 1form: ( 0 )  ( 1 ) = <a href=" 1"></a> at the root of the search DAG ( Fig. 5 (b)). The DAG is expanded by adding new phenomenological links, either between two existing variables, or between an existing variable and one in a newly added latent co-chain sequence (Fig. 5 (c)). In this example, the hypotheses are numbered H-00 (the root) through H-15, enumerating all possible I-net structures formed by at most one latent co-chain complex in 1D time. The user can specify the maximum number of latent variables that the algorithm may consider, to keep the search tractable.</p>
<p>Not every introduction of new variables or relations makes nontrivial statements about physics. For example, the hypothesis H-01 produces a new variable typed as a 1pseudo-form T (  1 ) = f 1 (( *  1 )), where the * operator takes  1 to its dual: * (  0 ,  0 + ) =  0 + /2. However, until this new variable is reached through another path to close a cycle and pose a nontrivial equation, we do not have a complete hypothesis to (in)validate against data. Further down the search DAG, H-08 defines a new variable typed as a 0pseudoform L(  0 ) = f 2 (( *  0 )) where *  0 = (  0  /2,  0 + /2). The co-boundary operation L(  0 )  T (  1 ) = <a href=" 1">L</a>, closes the cycle and produces a commutative diagram (Fig. 5 (c)) leading to:
E H-08 (; f 1 , f 2 ) = f 1 ()   * [f 2 ([])] = 0,
(1) where f 1 , f 2 are selected from restricted function spaces F 1 , F 2 to avoid overfitting (e.g., parameterized by a linear combination of domain-aware basis functions) and their parameters must be determined from data to minimize the residual error E H-08 over the entire period of data collection. A loss function can, for example, be defined as a mean-squared-error (MSE) to penalize violations uniformly over the time series period:
Loss H-08 = min f1F1 min f2F2 E H-08 (; f 1 , f 2 )  1 ,(2)
where   1 is an L 2 norm computed as a temporal integral, i.e., sum of squared errors E 2 H-08 (; f 1 , f 2 ) over time intervals  1 where (1) is evaluated. In this example, it turns out that the best fit is achieved with f 1 () = c 1 sin  and f 2 () = c 2  where c2 /c1 =  g /r. The latent variables L(  0 ) and L(  0 ) turn out to be the familiar notions of angular momentum and torque, respectively, although the software need not know anything about them to generate and test what-if scenarios about their existence and correlations with angular position and velocity. Hence, interpretability of the discovered relationships by a human scientist does not require Figure 6: The hypotheses H-04 and H-08 of Fig. 5 are enumerated and visualized by the software and evaluated against data (split 0.7-0.3 for training/testing). Both energy (first-order) and and torque (second-order) forms of the governing equation are discovered without human intervention. The former was quite unexpected, since its I-net structure does not correspond to a Tonti diagram. The latter has a larger error due to finite difference discretization.</p>
<p>predisposing the AI associate to such interpretations, enabling unexpected discoveries.</p>
<p>In general, every state in the search DAG can be classified as complete or incomplete hypotheses. The former are I-net structures with "dangling" branches that carry no new nontrivial information in addition to their parent states. Every time such a branch is turned into one or more closed cycles by adding enough new variables and/or relations, a new constraint is hypothesized that can be evaluated against data. When adding new dangling branches to the I-net structure, the search algorithm prioritizes actions that produce I-net structures similar to existing Tonti diagrams by assigning a penalty factor to every violation of the common structure (e.g., diagonal phenomenological links connecting non-dual cells). The loss for complete hypotheses can be computed as the sum of penalties for the I-net structure and the sum of residual errors for each of the independent constraints, implied by converging paths, multiplied by use-specified relative weight of the penalties and errors. We use an A* algorithm to search the space of hypotheses. Since we cannot compute the error for incomplete hypotheses, we can only prune them when the increase in their penalty is large enough that it would fail even if it had no error at all.</p>
<p>Generating Symbolic Expressions</p>
<p>One of the practical features of our implementation in Python is its ability to automatically convert I-net instances to symbolic DE expressions in SymPy, when the co-boundary operators are interpreted in a differential setting for infinitesimal cells (  0 + ); for example, equation (1) can be rewritten as a nonlinear ODE:
E H-08 (; f 1 , f 2 ) = f 1 ()   t f 2  (t) .(3)
As a result, the generated hypotheses can be evaluated using any number of existing ML or symbolic regression frameworks that standardize on ODE/PDE inputs. For example, using non-orthogonal basis functions {1, x, x 2 , sin x, cos x} to span both function spaces F 1 , F 2 , we can substitute for both symbolic functions:
f 1 () := c 1 0 + c 1 1  + c 1 2  2 + c 1 3 sin  + c 1 3 cos , (4) f 2 () := c 2 0 + c 2 1 + c 2 2
2 + c 2 3 sin + c 2 3 cos, (5) into (3) to obtain a symbolic second-order (non)linear ODE in SymPy. Next, the software performs algebraic simplification to identify equivalence classes of hypotheses that, despite coming from different I-net structures, lead to the same ODE upon differential interpretation of the I-nets. For ODEs which, after simplification, are linear combinations of nonlinear (differential/algebraic) terms that are computable from data, we can apply symbolic regression to estimate the coefficients from data; for example, we tried LASSO-regularized leastsquares regression in PDE-FIND (Rudy et al. 2017) where each term involving a derivative is evaluated using finite difference or polynomial approximation, whose results are shown in Fig. 6.</p>
<p>There are at least two issues with this approach: First, more sophisticated regression or nonlinear programming methods are needed if the DE has terms that have nested nonlinear functions, i.e., cannot be represented as a linear combination of nonlinear terms because of unknown coefficients embedded within each term. We solve this problem by directly mapping I-net structures to computation graphs in PyTorch, skipping differential interpretation to symbolic DEs altogether.</p>
<p>Second, numerical approximation of symbolic PDEs is a tricky business, as the discrete forms (in 3D space) may not obey the conservation principles postulated by the I-net structure after such approximations. It is difficult to separate discretization errors from modeling errors and noise in data. One of the key advantages of I-nets is the rich geometric information in their type system that is fundamental to physics-compatible and mimetic discretization schemes (Koren et al. 2014;Palha et al. 2014;Lipnikov, Manzini, and Shashkov 2014) that ensure conservation laws are satisfied exactly as a discrete level, regardless of spatial mesh or timestep resolutions. Such information is lost upon conversion to symbolic DEs. Retaining this information is even more important when dealing with noisy data, because discrete differentiation of noisy data (e.g., via finite difference or polynomial fitting) can substantially amplify the noise.</p>
<p>The good news is that we can directly interpret the same I-net instance in integral form to generate equations over larger regions in space and/or time, to make the computations more resilient to noise. For example, in the heat equation, the discrete divergence of heat flux over a single 3cell is replaced by a flux integral over a collection of 3cells, and is equated against the volumetric intgeral of internal energy within the collection. The cancellation of internal surface fluxes (discrete form of Gauss' divergence theorem) is built into the interpretation based on cellular homology. The integrals can be computed using higher-order integration schemes, e.g., using polynomial interpolation with underfitting to filter the noise.</p>
<p>Further details on directly and automatically mapping the abstract (symbolic) I-net structures to discrete (cellular) and numerical (tensor-based) I-net instance (e.g., computation graphs in PyTorch), learning scale-aware phenomenological relations, and physicscompatible discretization and de-noising will be presented in a full paper.</p>
<p>Real-World Scientific Discovery</p>
<p>Figures 7 and 8 illustrate the application of our AI approach to an elastodynamics challenge problem provided by AFRL in the course of the DARPA AI Research Associate (AIRA) program that supported the development of CyPhy. The input is noisy data obtained by ultrasound imaging, measured in (2+1)D spacetime over the surface of several material samples with heterogeneous properties. Figure 7 illustrates the search DAG along with a number of I-net structures for viable hypotheses, each postulating the relevance of a conservation law and existence of a few phenomenological relations. Figure 8 shows the ranking of these hypotheses based on their residual errors when tested against data. Each hypothesis can be interpreted in differential, integral, or integrodifferential forms. The results demonstrate that integral forms applied to wide spatial and temporal neighborhoods (of  25 grid elements along each axis) with high-order polynomial underfitting (up to cubic in each coordinate), resulting in a length/time scale-aware definition of (nonlocal) phenomenological relations as well as physics-compatible (i.e., mimetic) discretization and de-noising, are preferable to strictly local numerical schemes such as finite difference discretization.</p>
<p>Conclusion</p>
<p>Statistical learning methods, despite their accuracy and efficiency in narrow regimes for which they are carefully engineered, are not sufficient to independently acquire deep understandings of the scientific problems they are applied to. Human scientists continue to handle most of knowledge-centric aspects of the scientific process based on domain-specific insight, experience, and expertise.</p>
<p>Our novel approach to early-stage scientific hypothesis generation and testing demonstrates a path forward towards context-aware, generalizable, and interpretable AI for scientific discovery. Our AI associate (CyPhy) distinguishes between non-negotiable mathematical truism, implied by the relationship between measurement and presupposed spacetime topology, and phenomenological realities that are at the mercy of empirical learning. Data-driven regression is targeted at the latter to enable distilling governing equations from sparse and noisy data, while providing deep insights into the mathematical foundations.  </p>
<p>Figure 3
3shows a few other examples of Tonti diagrams for fundamental theories in classical and relativistic physics. The differences amount to (a) topological and metric context; (b) relevant variables and their dimensions/units; and (c) phenomenological relations.</p>
<p>Figure 3 :
3Tonti diagrams capture the common structure responsible for analogies across classical and relativistic physics with a clear distinction between topological and phenomenological relations that follow certain rules.</p>
<p>Figure 4 :
4I-nets are generalizations of Tonti diagrams for finite topological products of finite-dimensional spaces with relaxed rules for feasible phenomenological links to accomodate middle-ground theories.</p>
<p>Figure 7 :
7The search DAG and a number of viable hypotheses to explain ultrasound wavefield in metal parts.</p>
<p>Figure 8 :
8The AI associate discovers the (integral form) of the wave equation as well as the proper length/time scale at which the heterogeneous material properties (in this case, speed of sound) must be defined.
AcknowledgmentThis material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Agreement No. HR00111990029.
Differential Forms in Algebraic Topology. R Bott, L W Tu, Springer Science &amp; Business MediaBott, R.; and Tu, L. W. 1982. Differential Forms in Alge- braic Topology. Springer Science &amp; Business Media.</p>
<p>The Algebraic-Topological Basis for Network Analogies and the Vector Calculus. F H Branin, Symposium on Generalized Networks. Polytechnic Institute of Brooklyn, NYBranin, F. H. 1966. The Algebraic-Topological Basis for Network Analogies and the Vector Calculus. In Symposium on Generalized Networks, 453-491. Polytechnic Institute of Brooklyn, NY.</p>
<p>Newton versus the Machine: Solving the Chaotic Three-body Problem Using Deep Neural Networks. P G Breen, C N Foley, T Boekholt, S P Zwart, Monthly Notices of the Royal Astronomical Society. 4942Breen, P. G.; Foley, C. N.; Boekholt, T.; and Zwart, S. P. 2020. Newton versus the Machine: Solving the Chaotic Three-body Problem Using Deep Neural Networks. Monthly Notices of the Royal Astronomical Society 494(2): 2465- 2470.</p>
<p>Machine Learning for Molecular and Materials Science. K T Butler, D W Davies, H Cartwright, O Isayev, A Walsh, doi:10.1038/ s41586-018-0337-2Nature. 5597715Butler, K. T.; Davies, D. W.; Cartwright, H.; Isayev, O.; and Walsh, A. 2018. Machine Learning for Molecular and Materials Science. Nature 559(7715): 547-555. doi:10.1038/ s41586-018-0337-2.</p>
<p>M D Cranmer, R Xu, P Battaglia, S Ho, arXiv:1909.05862Learning Symbolic Physics with Graph Networks. arXiv preprintCranmer, M. D.; Xu, R.; Battaglia, P.; and Ho, S. 2019. Learning Symbolic Physics with Graph Networks. arXiv preprint arXiv:1909.05862 .</p>
<p>Physics-Guided Architecture (PGA) of Neural Networks for Quantifying Uncertainty in Lake Temperature Modeling. A Daw, R Q Thomas, C C Carey, J S Read, A P Appling, A Karpatne, Proceedings of the 2020 SIAM International Conference on Data Mining. the 2020 SIAM International Conference on Data MiningDaw, A.; Thomas, R. Q.; Carey, C. C.; Read, J. S.; Appling, A. P.; and Karpatne, A. 2020. Physics-Guided Architecture (PGA) of Neural Networks for Quantifying Uncertainty in Lake Temperature Modeling. In Proceedings of the 2020 SIAM International Conference on Data Mining, 532-540. Society for Industrial and Applied Mathematics (SIAM).</p>
<p>The Geometry of Physics: An Introduction. T Frankel, Cambridge University PressFrankel, T. 2011. The Geometry of Physics: An Introduc- tion. Cambridge University Press.</p>
<p>Algebraic Topology. A Hatcher, Cornell UniversityHatcher, A. 2001. Algebraic Topology. Cornell University.</p>
<p>Discrete Exterior Calculus. A N Hirani, California Institute of TechnologyPh.D. dissertationHirani, A. N. 2003. Discrete Exterior Calculus. Ph.D. dis- sertation, California Institute of Technology.</p>
<p>Discovering Physical Concepts with Neural Networks. R Iten, T Metger, H Wilming, L Del Rio, R Renner, Physical Review Letters. 124110508Iten, R.; Metger, T.; Wilming, H.; Del Rio, L.; and Ren- ner, R. 2020. Discovering Physical Concepts with Neural Networks. Physical Review Letters 124(1): 010508.</p>
<p>Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery. H Kitano, AI Magazine. 371Kitano, H. 2016. Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Dis- covery. AI Magazine 37(1): 39-49.</p>
<p>. B Koren, R Abgrall, P Bochev, J E Frank, B Perot, Physics-Compatible Numerical Methods. Journal of Computational Physics. 257Part BKoren, B.; Abgrall, R.; Bochev, P.; Frank, J. E.; and Perot, B. 2014. Physics-Compatible Numerical Methods. Journal of Computational Physics 257(Part B): 1039-1039.</p>
<p>Diakoptics: The Piecewise Solution of Large-Scale Systems. G Kron, MacDonald2Kron, G. 1963. Diakoptics: The Piecewise Solution of Large- Scale Systems, volume 2. MacDonald.</p>
<p>The Computer-Aided Discovery of Scientific Knowledge. P Langley, International Conference on Discovery Science. SpringerLangley, P. 1998. The Computer-Aided Discovery of Scien- tific Knowledge. In International Conference on Discovery Science, 25-39. Springer.</p>
<p>Mimetic Finite Difference Method. K Lipnikov, G Manzini, M Shashkov, Journal of Computational Physics. 257Lipnikov, K.; Manzini, G.; and Shashkov, M. 2014. Mimetic Finite Difference Method. Journal of Computational Physics 257: 1163-1227.</p>
<p>The Finite Volume, Finite Element, and Finite Difference Methods as Numerical Methods for Physical Field Problems. C Mattiussi, Advances in Imaging and Electron Physics. 113Mattiussi, C. 2000. The Finite Volume, Finite Element, and Finite Difference Methods as Numerical Methods for Physical Field Problems. Advances in Imaging and Electron Physics 113: 1-147.</p>
<p>A High-Bias, Low-Variance Introduction to Machine Learning for Physicists. P Mehta, M Bukov, C H Wang, A G R Day, C Richardson, C K Fisher, D J Schwab, Physics Reports. 810Mehta, P.; Bukov, M.; Wang, C. H.; Day, A. G. R.; Richard- son, C.; Fisher, C. K.; and Schwab, D. J. 2019. A High-Bias, Low-Variance Introduction to Machine Learning for Physi- cists. Physics Reports 810: 1-124.</p>
<p>An Efficient Deep Learning Technique for the Navier-Stokes Equations: Application to Unsteady Wake Flow Dynamics. T P Miyanawala, R K Jaiman, H P Nautrup, T Metger, R Iten, S Jerbi, L M Trenkwalder, H Wilming, H J Briegel, R Renner, 10.1016/j.eswa.2008.08.077arXiv:2001.00593Operationally Meaningful Representations of Physical Systems in Neural Networks. arXiv preprintMiyanawala, T. P.; and Jaiman, R. K. 2017. An Effi- cient Deep Learning Technique for the Navier-Stokes Equa- tions: Application to Unsteady Wake Flow Dynamics. arXiv Preprints ISSN 0264-6021. doi:10.1016/j.eswa.2008.08.077. Nautrup, H. P.; Metger, T.; Iten, R.; Jerbi, S.; Trenkwalder, L. M.; Wilming, H.; Briegel, H. J.; and Renner, R. 2020. Op- erationally Meaningful Representations of Physical Systems in Neural Networks. arXiv preprint arXiv:2001.00593 .</p>
<p>Physics-Compatible Discretization Techniques on Single and Dual Grids, with Application to the Poisson Equation of Volume Forms. A Palha, P P Rebelo, R Hiemstra, J Kreeft, M Gerritsma, Journal of Computational Physics. 257Palha, A.; Rebelo, P. P.; Hiemstra, R.; Kreeft, J.; and Ger- ritsma, M. 2014. Physics-Compatible Discretization Tech- niques on Single and Dual Grids, with Application to the Poisson Equation of Volume Forms. Journal of Computa- tional Physics 257: 1394-1422.</p>
<p>M Raghu, E Schmidt, arXiv:2003.11755A Survey of Deep Learning for Scientific Discovery. arXiv preprintRaghu, M.; and Schmidt, E. 2020. A Survey of Deep Learn- ing for Scientific Discovery. arXiv preprint arXiv:2003.11755 .</p>
<p>Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 378Raissi, M.; Perdikaris, P.; and Karniadakis, G. E. 2019. Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems In- volving Nonlinear Partial Differential Equations. Journal of Computational Physics 378: 686-707.</p>
<p>An Application of Algebraic Topology to Numerical Analysis: On the Existence of a Solution to the Network Problem. J P Roth, Proceedings of the National Academy of Sciences. 417Roth, J. P. 1955. An Application of Algebraic Topology to Numerical Analysis: On the Existence of a Solution to the Network Problem. Proceedings of the National Academy of Sciences 41(7): 518-521.</p>
<p>Data-Driven Discovery of Partial Differential Equations. S H Rudy, S L Brunton, J L Proctor, J N Kutz, Science Advances. 341602614Rudy, S. H.; Brunton, S. L.; Proctor, J. L.; and Kutz, J. N. 2017. Data-Driven Discovery of Partial Differential Equa- tions. Science Advances 3(4): e1602614.</p>
<p>A Sanchez-Gonzalez, J Godwin, T Pfaff, R Ying, J Leskovec, P W Battaglia, arXiv:2002.09405Learning to Simulate Complex Physics with Graph Networks. arXiv preprintSanchez-Gonzalez, A.; Godwin, J.; Pfaff, T.; Ying, R.; Leskovec, J.; and Battaglia, P. W. 2020. Learning to Simu- late Complex Physics with Graph Networks. arXiv preprint arXiv:2002.09405 .</p>
<p>Distilling Free-Form Natural Laws from Experimental Data. M Schmidt, H Lipson, Science. 3245923Schmidt, M.; and Lipson, H. 2009. Distilling Free-Form Nat- ural Laws from Experimental Data. Science 324(5923): 81- 85.</p>
<p>S Seo, Y Liu, arXiv:1902.02950Differentiable Physics-Informed Graph Networks. arXiv preprintSeo, S.; and Liu, Y. 2019. Differentiable Physics-Informed Graph Networks. arXiv preprint arXiv:1902.02950 .</p>
<p>. R Stevens, V Taylor, J Nichols, A B Maccabe, K Yelick, D Brown, AI for Science. Argonne National Laboratory(ANLTechnical reportStevens, R.; Taylor, V.; Nichols, J.; Maccabe, A. B.; Yelick, K.; and Brown, D. 2020. AI for Science. Technical report, Argonne National Laboratory(ANL).</p>
<p>The Mathematical Structure of Classical and Relativistic Physics: A General Classification Diagram. Modeling and Simulation in Science, Engineering and Technology. Birkhuser. E Tonti, 9781461474210Tonti, E. 2013. The Mathematical Structure of Classical and Relativistic Physics: A General Classification Diagram. Modeling and Simulation in Science, Engineering and Tech- nology. Birkhuser. ISBN 9781461474210.</p>
<p>AI Feynman: A Physics-Inspired Method for Symbolic Regression. S M Udrescu, M Tegmark, Science Advances. 6162631Udrescu, S. M.; and Tegmark, M. 2020. AI Feynman: A Physics-Inspired Method for Symbolic Regression. Science Advances 6(16): eaay2631.</p>
<p>Towards Physics-Informed Deep Learning for Turbulent Flow Prediction. R Wang, K Kashinath, M Mustafa, A Albert, R Yu, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningWang, R.; Kashinath, K.; Mustafa, M.; Albert, A.; and Yu, R. 2020. Towards Physics-Informed Deep Learning for Tur- bulent Flow Prediction. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 1457-1466.</p>
<p>Physics-Inspired Convolutional Neural Network for Solving Full-Wave Inverse Scattering Problems. Z Wei, X Chen, IEEE Transactions on Antennas and Propagation. 679Wei, Z.; and Chen, X. 2019. Physics-Inspired Convolutional Neural Network for Solving Full-Wave Inverse Scattering Problems. IEEE Transactions on Antennas and Propaga- tion 67(9): 6138-6148.</p>
<p>Toward an AI Physicist for Unsupervised Learning. T Wu, M Tegmark, arXiv:1810.10525arXiv preprintWu, T.; and Tegmark, M. 2019. Toward an AI Physicist for Unsupervised Learning. arXiv preprint arXiv:1810.10525 .</p>
<p>Machine-Learning-Guided Directed Evolution for Protein Engineering. K K Yang, Z Wu, F H Arnold, 10.1038/s41592-019-0496-6Nature Methods. 168Yang, K. K.; Wu, Z.; and Arnold, F. H. 2019. Machine- Learning-Guided Directed Evolution for Protein Engineer- ing. Nature Methods 16(8): 687-694. doi:10.1038/s41592- 019-0496-6.</p>            </div>
        </div>

    </div>
</body>
</html>
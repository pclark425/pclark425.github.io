<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4382 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4382</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4382</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-272279984</p>
                <p><strong>Paper Title:</strong> Leveraging LLMs for Efficient Topic Reviews</p>
                <p><strong>Paper Abstract:</strong> : This paper presents the topic review (TR), a novel semi-automatic framework designed to enhance the efficiency and accuracy of literature reviews. By leveraging the capabilities of large language models (LLMs), TR addresses the inefficiencies and error-proneness of traditional review methods, especially in rapidly evolving fields. The framework significantly improves literature review processes by integrating advanced text mining and machine learning techniques. Through a case study approach, TR offers a step-by-step methodology that begins with query generation and refinement, followed by semi-automated text mining to identify relevant articles. LLMs are then employed to extract and categorize key themes and concepts, facilitating an in-depth literature analysis. This approach demonstrates the transformative potential of natural language processing in literature reviews. With an average similarity of 69.56% between generated and indexed keywords, TR effectively manages the growing volume of scientific publications, providing researchers with robust strategies for complex text synthesis and advancing knowledge in various domains. An expert analysis highlights a positive Fleiss’ Kappa score, underscoring the significance and interpretability of the results.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4382.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4382.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Topic Review (TR) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-automatic literature-review pipeline that leverages LLMs, transformer-based topic models (BERTopic), dense embeddings, dimensionality reduction, and clustering to extract, label, and synthesize themes across large sets of papers with human-in-the-loop selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Topic Review (TR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Modular pipeline that: (1) embeds document abstracts with BAAI/bge-large-en-v1.5, (2) reduces dimensions with UMAP, (3) clusters with HDBSCAN, (4) extracts cluster-level features via class-based TF-IDF (c-TF-IDF) as in BERTopic, and (5) refines topic representations and generates labels using an instruction-tuned LLM (SOLAR-10.7B-Instruct) and TextGeneration modules. The pipeline supports KeyBERT for keyword extraction and a prompt-template driven LLM labeling stage; final topic selection/validation is semi-automatic with expert raters.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SOLAR-10.7B-Instruct-v1.0 (SOLAR 10.7B, instruction-tuned; paper also uses token-generation config settings), plus text-generation modules; embeddings from BAAI/bge-large-en-v1.5 (dense embedding model).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval and clustering (BGE embeddings → UMAP → HDBSCAN) combined with c-TF-IDF keyword extraction (KeyBERT) and LLM-based labeling/generation (instruction-tuned SOLAR used to produce topic labels and select representative docs).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Cluster-centric synthesis: documents are grouped into topics, cluster-level summaries/keywords are produced (c-TF-IDF, KeyBERT), and an LLM is used to generate concise topic labels and representative-document summaries; human-in-the-loop selection finalizes topics and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Initial corpus: 19,643 documents (experiment filtered and analyzed subsets, e.g., Topic 6 contained 399 documents; 267 had both author and index keywords used in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Knowledge management / literature review process; experiment highlighted topics in medical systematic reviews and biomedical text mining as well.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Clustered topic labels, keyword lists, representative documents for each topic, visualizations (UMAP, hierarchical cluster visual), and numeric similarity/evaluation statistics (cosine similarities, Fleiss' Kappa).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Cosine similarity between KeyBERT-generated keywords and Scopus/index/author keywords (BERT embeddings), Shapiro–Wilk normality test, paired t-test, Fleiss' Kappa for inter-rater agreement; descriptive stats (mean, median, std).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>For Topic 6: mean cosine similarity between KeyBERT keywords and Scopus index keywords = 0.6956 (median 0.7007, std 0.0531); vs averaged baseline topics mean = 0.6323. Author-keyword similarity for Topic 6 mean = 0.6971 (std 0.0717) vs baseline mean = 0.5958. Paired t-test p-values: 0.0135 (index keywords) and 0.0224 (author keywords). Fleiss' Kappa scores for human ratings: meaningfulness κ=0.0402, importance κ=0.0650 (both 'slight agreement').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baseline was KeyBERT output averaged across four other selected topics (Topics 9, 44, 51, 63) drawn from the same Scopus query; also compared to indexed and author keywords from the documents themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>TR (via KeyBERT+BERTopic on Topic 6) produced significantly higher cosine similarity to indexed and author keywords than baseline averaged topics (index: 0.6956 vs 0.6323; author: 0.6971 vs 0.5958) with statistically significant paired t-tests (p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining dense embeddings, UMAP+HDBSCAN clustering, c-TF-IDF (BERTopic), KeyBERT keyword extraction, and an instruction-tuned LLM for labeling yields coherent, higher-similarity topic clusters for domain-focused subsets; semi-automatic human selection remains necessary for final topic relevance and interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Low inter-rater agreement on topic meaningfulness/importance (Fleiss' κ near 0.04–0.07), sensitivity to hyperparameters (HDBSCAN min cluster size, UMAP n_neighbors/components, LLM temperature), variability in LLM text generation (inconsistency), dependency on library/tool versions, computational cost (GPU A100 used), and partial reliance on human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Pipeline applied to a large initial corpus (≈19.6k documents) and processed on an NVIDIA A100 (40 GB); authors discuss using 4-bit quantization (bitsandbytes) for large models and note that SOLAR-10.7B and BGE embeddings enable handling large numbers of documents, but no explicit empirical scaling curves are reported. Authors recommend hyperparameter optimization and note practical scaling via embedding+clustering reduces per-document generative load.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs for Efficient Topic Reviews', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4382.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4382.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTopic+SOLAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTopic integrated with SOLAR-10.7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of BERTopic (transformer-based neural topic modeling with c-TF-IDF) with an instruction-tuned LLM (SOLAR-10.7B-Instruct) to refine topic representations, select representative documents, and generate topic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural topic modeling with a class-based TF-IDF procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERTopic + SOLAR-10.7B-Instruct integration</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>BERTopic pipeline: embeddings → UMAP → HDBSCAN → c-TF-IDF to produce cluster topics; augmented by SOLAR-10.7B-Instruct used to interpret clusters, produce natural-language labels/follow-up summaries, and select representative documents based on similarity to c-TF-IDF topic representations. The SOLAR model provides instruction-following generation to enhance topic representation and label quality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SOLAR-10.7B-Instruct (10.7 billion parameter LLM; instruction-tuned variant).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Cluster-level representation via c-TF-IDF plus LLM-driven selection/description; embedding similarity between documents and topic centroids used to pick representative docs.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Topic refinement and natural-language label generation via LLM conditioned on cluster keywords and representative documents (semi-automatic synthesis across the documents in a cluster).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied within TR to clusters derived from corpora up to 19,643 documents; specific cluster (Topic 6) contained 399 documents (267 with index and author keywords used in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General academic literature (applied to knowledge management and medical/biomedical subtopics in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined topic labels, representative-document selection, improved topic descriptions and keyword summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same cluster-level metrics as TR (cosine similarity of keywords, t-tests, human ratings), qualitative evaluation by experts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>When used within TR, contributed to topic-representation quality that led to KeyBERT cosine similarities ~0.696 for Topic 6 vs ~0.632 baseline; no isolated numeric performance metric reported solely for BERTopic+SOLAR beyond integrated pipeline results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared indirectly to traditional topic methods (LDA, Top2Vec) in literature review; within paper, BERTopic-based pipeline compared against averaged-topic baselines for keyword similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Pipeline shows improved thematic capture for selected topic (Topic 6) relative to averaged-topic baseline; precise attribution to SOLAR component not separately quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Instruction-tuned LLM can improve topic interpretability and selection of representative documents when combined with BERTopic's cluster representations; transformer-based topic models outperform older probabilistic topic models in capturing semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Need for quantization and memory management for large LLMs (authors use 4-bit quantization), potential variability from generative model temperature settings, and dependency on correct hyperparameterization of BERTopic components.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors used quantization (bitsandbytes 4-bit) and A100 GPU to enable running SOLAR-10.7B; they discuss that SOLAR is efficient for large document numbers but do not present explicit scaling curves.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs for Efficient Topic Reviews', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4382.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4382.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KeyBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KeyBERT (Minimal Keyword Extraction with BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple keyword-extraction technique that uses BERT-style embeddings to find keywords most similar to document embeddings; used here to generate topic-specific keywords for comparison with indexed and author keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minimal Keyword Extraction with BERT.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KeyBERT (keyword extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generates candidate keywords/phrases for documents or clusters by embedding documents and candidate n-grams with a transformer embedding model and selecting terms with highest cosine similarity to document/cluster embeddings; here applied per-cluster and compared against Scopus index and author keywords.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>No generative LLM for KeyBERT itself; uses BERT-style sentence/phrase embeddings (in this paper embeddings from BAAI/bge-large-en-v1.5).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based keyword extraction (highest cosine-similarity n-grams to document/cluster embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Produces keyword sets per cluster which are compared (cosine similarity) against indexed/author keywords to validate topic quality; used as input to LLM labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied to 267 documents within Topic 6 that contained both author and index keywords (from a larger corpus of 19,643 documents).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied to academic literature clusters (knowledge management, systematic review topics, biomedical subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Keyword lists for documents and clusters used for evaluation and topic labeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Cosine similarity between KeyBERT output and Scopus index/author keywords (BERT embeddings), descriptive statistics, Shapiro–Wilk, paired t-test.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>KeyBERT-generated keywords for Topic 6 had mean cosine similarity 0.6956 to index keywords and 0.6971 to author keywords (n=267 documents); differences vs averaged selected topics were statistically significant (p=0.0135 and p=0.0224).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Averaged KeyBERT outputs from four other selected topics (9, 44, 51, 63) used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>KeyBERT for Topic 6 outperformed averaged-topic baseline (index: 0.6956 vs 0.6323; author: 0.6971 vs 0.5958).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>KeyBERT combined with the embedding+clustering pipeline produces keywords that align well with indexed and author-specified keywords for focused topics, serving as a useful automatic validation signal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality depends on embedding model choice; KeyBERT is limited to keyword extraction and does not perform multi-document synthesis or claim aggregation by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Applied at cluster and document scale in a corpus of thousands of documents; performance depends on embedding throughput but no explicit scaling benchmarks given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs for Efficient Topic Reviews', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4382.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4382.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoQuery-PLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic query generation using pre-trained language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach (2023) that uses pre-trained language models to automatically generate or refine search queries for literature retrieval, improving precision/recall relative to traditional or manual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automatic query generation using pre-trained language models</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach that leverages pre-trained language models to construct, refine, or expand search queries for retrieving relevant literature; cited in the paper as improving IR metrics (precision, recall, F-measure) and sometimes outperforming manual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Pre-trained language models (not further specified in this paper; referenced work from 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Query-generation via generative PLM conditioned on seed terms or examples, then IR retrieval against bibliographic databases.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis system per se — it supports document retrieval which feeds downstream synthesis/pipeline stages (e.g., TR).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Intended to improve retrieval across systematic-search tasks; original referenced studies report variable corpus sizes (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic reviews / information retrieval for biomedical and other academic domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated or refined search queries; indirectly leads to improved retrieval sets used for later synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported in referenced work to include precision, recall, F-measure for retrieval tasks (not quantified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited claim: automatic query generation with pre-trained language models significantly surpasses traditional models in precision, recall, and F-measures in some evaluations (no numeric values provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional query-generation methods and manual expert queries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Cited as better in some cases, but no detailed numbers or experimental details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using PLMs to generate or refine queries can increase retrieval effectiveness and reduce manual query-engineering effort, which benefits downstream automated literature-review pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper; general concerns include retrieval bias from generated queries, potential for overfitting query terms to model biases, and dependence on quality of PLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in detail; likely scales with the search space and retrieval engine rather than PLM size in the cited summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs for Efficient Topic Reviews', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4382.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4382.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASReview (ML-aided pipeline for screening in systematic reviews)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, active-learning machine-learning pipeline to assist title/abstract screening in systematic reviews, reducing human workload and improving screening efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An open source machine learning framework for efficient and transparent systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ASReview (ML-aided systematic review screening)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Active-learning based screening tool that iteratively trains classifiers with reviewer feedback to prioritize likely-relevant citations, enabling reviewers to find relevant studies faster and reduce the number of abstracts to manually screen.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Primarily traditional ML classifiers / active learning (SVMs, ensemble methods, etc.); not specifically an LLM in the referenced description, but cited as ML-aided pipeline relevant to automated literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Active-learning classification for relevance ranking (title/abstract screening), feature-based ML or embedding-based classifiers depending on configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesizer of claims/theories — it accelerates study selection which supports downstream synthesis tasks (meta-analysis, topic modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Designed for large-scale systematic reviews; used across datasets ranging from hundreds to thousands of citations (no specific number presented in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Systematic reviews across multiple domains, including medicine and social sciences.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked list of candidate studies for reviewer screening; selected set of included/excluded studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Screening efficiency metrics (reduction in number to screen), precision/recall of included studies, time saved (as reported in cited literature, not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Cited as significantly improving efficiency and quality of systematic reviews and meta-analyses in prior work; this paper references ASReview as a representative ML-aided pipeline but provides no new numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Manual screening and static (non-active) ML-assisted approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Reported improvements in efficiency in cited literature (no numeric comparison in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active learning pipelines like ASReview reduce human screening workload and integrate naturally into semi-automatic literature-review workflows such as TR, though they do not perform multi-document synthesis themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Class imbalance, need for reviewer-in-the-loop, potential bias from initial seed selection, not a standalone synthesis/system for generating theories.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed to scale to thousands of citations; actual performance depends on classifier and feature choice—no explicit scaling curves in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Leveraging LLMs for Efficient Topic Reviews', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural topic modeling with a class-based TF-IDF procedure. <em>(Rating: 2)</em></li>
                <li>An open source machine learning framework for efficient and transparent systematic reviews. <em>(Rating: 2)</em></li>
                <li>Automatic query generation using pre-trained language models <em>(Rating: 2)</em></li>
                <li>Unraveling the landscape of large language models: A systematic review and future perspectives. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4382",
    "paper_id": "paper-272279984",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "TR",
            "name_full": "Topic Review (TR) framework",
            "brief_description": "A semi-automatic literature-review pipeline that leverages LLMs, transformer-based topic models (BERTopic), dense embeddings, dimensionality reduction, and clustering to extract, label, and synthesize themes across large sets of papers with human-in-the-loop selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Topic Review (TR)",
            "system_description": "Modular pipeline that: (1) embeds document abstracts with BAAI/bge-large-en-v1.5, (2) reduces dimensions with UMAP, (3) clusters with HDBSCAN, (4) extracts cluster-level features via class-based TF-IDF (c-TF-IDF) as in BERTopic, and (5) refines topic representations and generates labels using an instruction-tuned LLM (SOLAR-10.7B-Instruct) and TextGeneration modules. The pipeline supports KeyBERT for keyword extraction and a prompt-template driven LLM labeling stage; final topic selection/validation is semi-automatic with expert raters.",
            "llm_model_used": "SOLAR-10.7B-Instruct-v1.0 (SOLAR 10.7B, instruction-tuned; paper also uses token-generation config settings), plus text-generation modules; embeddings from BAAI/bge-large-en-v1.5 (dense embedding model).",
            "extraction_technique": "Embedding-based retrieval and clustering (BGE embeddings → UMAP → HDBSCAN) combined with c-TF-IDF keyword extraction (KeyBERT) and LLM-based labeling/generation (instruction-tuned SOLAR used to produce topic labels and select representative docs).",
            "synthesis_technique": "Cluster-centric synthesis: documents are grouped into topics, cluster-level summaries/keywords are produced (c-TF-IDF, KeyBERT), and an LLM is used to generate concise topic labels and representative-document summaries; human-in-the-loop selection finalizes topics and interpretation.",
            "number_of_papers": "Initial corpus: 19,643 documents (experiment filtered and analyzed subsets, e.g., Topic 6 contained 399 documents; 267 had both author and index keywords used in evaluation).",
            "domain_or_topic": "Knowledge management / literature review process; experiment highlighted topics in medical systematic reviews and biomedical text mining as well.",
            "output_type": "Clustered topic labels, keyword lists, representative documents for each topic, visualizations (UMAP, hierarchical cluster visual), and numeric similarity/evaluation statistics (cosine similarities, Fleiss' Kappa).",
            "evaluation_metrics": "Cosine similarity between KeyBERT-generated keywords and Scopus/index/author keywords (BERT embeddings), Shapiro–Wilk normality test, paired t-test, Fleiss' Kappa for inter-rater agreement; descriptive stats (mean, median, std).",
            "performance_results": "For Topic 6: mean cosine similarity between KeyBERT keywords and Scopus index keywords = 0.6956 (median 0.7007, std 0.0531); vs averaged baseline topics mean = 0.6323. Author-keyword similarity for Topic 6 mean = 0.6971 (std 0.0717) vs baseline mean = 0.5958. Paired t-test p-values: 0.0135 (index keywords) and 0.0224 (author keywords). Fleiss' Kappa scores for human ratings: meaningfulness κ=0.0402, importance κ=0.0650 (both 'slight agreement').",
            "comparison_baseline": "Baseline was KeyBERT output averaged across four other selected topics (Topics 9, 44, 51, 63) drawn from the same Scopus query; also compared to indexed and author keywords from the documents themselves.",
            "performance_vs_baseline": "TR (via KeyBERT+BERTopic on Topic 6) produced significantly higher cosine similarity to indexed and author keywords than baseline averaged topics (index: 0.6956 vs 0.6323; author: 0.6971 vs 0.5958) with statistically significant paired t-tests (p &lt; 0.05).",
            "key_findings": "Combining dense embeddings, UMAP+HDBSCAN clustering, c-TF-IDF (BERTopic), KeyBERT keyword extraction, and an instruction-tuned LLM for labeling yields coherent, higher-similarity topic clusters for domain-focused subsets; semi-automatic human selection remains necessary for final topic relevance and interpretation.",
            "limitations_challenges": "Low inter-rater agreement on topic meaningfulness/importance (Fleiss' κ near 0.04–0.07), sensitivity to hyperparameters (HDBSCAN min cluster size, UMAP n_neighbors/components, LLM temperature), variability in LLM text generation (inconsistency), dependency on library/tool versions, computational cost (GPU A100 used), and partial reliance on human judgment.",
            "scaling_behavior": "Pipeline applied to a large initial corpus (≈19.6k documents) and processed on an NVIDIA A100 (40 GB); authors discuss using 4-bit quantization (bitsandbytes) for large models and note that SOLAR-10.7B and BGE embeddings enable handling large numbers of documents, but no explicit empirical scaling curves are reported. Authors recommend hyperparameter optimization and note practical scaling via embedding+clustering reduces per-document generative load.",
            "uuid": "e4382.0",
            "source_info": {
                "paper_title": "Leveraging LLMs for Efficient Topic Reviews",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "BERTopic+SOLAR",
            "name_full": "BERTopic integrated with SOLAR-10.7B-Instruct",
            "brief_description": "Integration of BERTopic (transformer-based neural topic modeling with c-TF-IDF) with an instruction-tuned LLM (SOLAR-10.7B-Instruct) to refine topic representations, select representative documents, and generate topic labels.",
            "citation_title": "Neural topic modeling with a class-based TF-IDF procedure.",
            "mention_or_use": "use",
            "system_name": "BERTopic + SOLAR-10.7B-Instruct integration",
            "system_description": "BERTopic pipeline: embeddings → UMAP → HDBSCAN → c-TF-IDF to produce cluster topics; augmented by SOLAR-10.7B-Instruct used to interpret clusters, produce natural-language labels/follow-up summaries, and select representative documents based on similarity to c-TF-IDF topic representations. The SOLAR model provides instruction-following generation to enhance topic representation and label quality.",
            "llm_model_used": "SOLAR-10.7B-Instruct (10.7 billion parameter LLM; instruction-tuned variant).",
            "extraction_technique": "Cluster-level representation via c-TF-IDF plus LLM-driven selection/description; embedding similarity between documents and topic centroids used to pick representative docs.",
            "synthesis_technique": "Topic refinement and natural-language label generation via LLM conditioned on cluster keywords and representative documents (semi-automatic synthesis across the documents in a cluster).",
            "number_of_papers": "Applied within TR to clusters derived from corpora up to 19,643 documents; specific cluster (Topic 6) contained 399 documents (267 with index and author keywords used in evaluation).",
            "domain_or_topic": "General academic literature (applied to knowledge management and medical/biomedical subtopics in experiments).",
            "output_type": "Refined topic labels, representative-document selection, improved topic descriptions and keyword summaries.",
            "evaluation_metrics": "Same cluster-level metrics as TR (cosine similarity of keywords, t-tests, human ratings), qualitative evaluation by experts.",
            "performance_results": "When used within TR, contributed to topic-representation quality that led to KeyBERT cosine similarities ~0.696 for Topic 6 vs ~0.632 baseline; no isolated numeric performance metric reported solely for BERTopic+SOLAR beyond integrated pipeline results.",
            "comparison_baseline": "Compared indirectly to traditional topic methods (LDA, Top2Vec) in literature review; within paper, BERTopic-based pipeline compared against averaged-topic baselines for keyword similarity.",
            "performance_vs_baseline": "Pipeline shows improved thematic capture for selected topic (Topic 6) relative to averaged-topic baseline; precise attribution to SOLAR component not separately quantified.",
            "key_findings": "Instruction-tuned LLM can improve topic interpretability and selection of representative documents when combined with BERTopic's cluster representations; transformer-based topic models outperform older probabilistic topic models in capturing semantics.",
            "limitations_challenges": "Need for quantization and memory management for large LLMs (authors use 4-bit quantization), potential variability from generative model temperature settings, and dependency on correct hyperparameterization of BERTopic components.",
            "scaling_behavior": "Authors used quantization (bitsandbytes 4-bit) and A100 GPU to enable running SOLAR-10.7B; they discuss that SOLAR is efficient for large document numbers but do not present explicit scaling curves.",
            "uuid": "e4382.1",
            "source_info": {
                "paper_title": "Leveraging LLMs for Efficient Topic Reviews",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "KeyBERT",
            "name_full": "KeyBERT (Minimal Keyword Extraction with BERT)",
            "brief_description": "A simple keyword-extraction technique that uses BERT-style embeddings to find keywords most similar to document embeddings; used here to generate topic-specific keywords for comparison with indexed and author keywords.",
            "citation_title": "Minimal Keyword Extraction with BERT.",
            "mention_or_use": "use",
            "system_name": "KeyBERT (keyword extraction)",
            "system_description": "Generates candidate keywords/phrases for documents or clusters by embedding documents and candidate n-grams with a transformer embedding model and selecting terms with highest cosine similarity to document/cluster embeddings; here applied per-cluster and compared against Scopus index and author keywords.",
            "llm_model_used": "No generative LLM for KeyBERT itself; uses BERT-style sentence/phrase embeddings (in this paper embeddings from BAAI/bge-large-en-v1.5).",
            "extraction_technique": "Embedding-based keyword extraction (highest cosine-similarity n-grams to document/cluster embeddings).",
            "synthesis_technique": "Produces keyword sets per cluster which are compared (cosine similarity) against indexed/author keywords to validate topic quality; used as input to LLM labeling.",
            "number_of_papers": "Applied to 267 documents within Topic 6 that contained both author and index keywords (from a larger corpus of 19,643 documents).",
            "domain_or_topic": "Applied to academic literature clusters (knowledge management, systematic review topics, biomedical subsets).",
            "output_type": "Keyword lists for documents and clusters used for evaluation and topic labeling.",
            "evaluation_metrics": "Cosine similarity between KeyBERT output and Scopus index/author keywords (BERT embeddings), descriptive statistics, Shapiro–Wilk, paired t-test.",
            "performance_results": "KeyBERT-generated keywords for Topic 6 had mean cosine similarity 0.6956 to index keywords and 0.6971 to author keywords (n=267 documents); differences vs averaged selected topics were statistically significant (p=0.0135 and p=0.0224).",
            "comparison_baseline": "Averaged KeyBERT outputs from four other selected topics (9, 44, 51, 63) used as baseline.",
            "performance_vs_baseline": "KeyBERT for Topic 6 outperformed averaged-topic baseline (index: 0.6956 vs 0.6323; author: 0.6971 vs 0.5958).",
            "key_findings": "KeyBERT combined with the embedding+clustering pipeline produces keywords that align well with indexed and author-specified keywords for focused topics, serving as a useful automatic validation signal.",
            "limitations_challenges": "Quality depends on embedding model choice; KeyBERT is limited to keyword extraction and does not perform multi-document synthesis or claim aggregation by itself.",
            "scaling_behavior": "Applied at cluster and document scale in a corpus of thousands of documents; performance depends on embedding throughput but no explicit scaling benchmarks given.",
            "uuid": "e4382.2",
            "source_info": {
                "paper_title": "Leveraging LLMs for Efficient Topic Reviews",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AutoQuery-PLM",
            "name_full": "Automatic query generation using pre-trained language models",
            "brief_description": "A referenced approach (2023) that uses pre-trained language models to automatically generate or refine search queries for literature retrieval, improving precision/recall relative to traditional or manual queries.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Automatic query generation using pre-trained language models",
            "system_description": "Approach that leverages pre-trained language models to construct, refine, or expand search queries for retrieving relevant literature; cited in the paper as improving IR metrics (precision, recall, F-measure) and sometimes outperforming manual queries.",
            "llm_model_used": "Pre-trained language models (not further specified in this paper; referenced work from 2023).",
            "extraction_technique": "Query-generation via generative PLM conditioned on seed terms or examples, then IR retrieval against bibliographic databases.",
            "synthesis_technique": "Not a synthesis system per se — it supports document retrieval which feeds downstream synthesis/pipeline stages (e.g., TR).",
            "number_of_papers": "Intended to improve retrieval across systematic-search tasks; original referenced studies report variable corpus sizes (not specified here).",
            "domain_or_topic": "Systematic reviews / information retrieval for biomedical and other academic domains.",
            "output_type": "Generated or refined search queries; indirectly leads to improved retrieval sets used for later synthesis.",
            "evaluation_metrics": "Reported in referenced work to include precision, recall, F-measure for retrieval tasks (not quantified in this paper).",
            "performance_results": "Cited claim: automatic query generation with pre-trained language models significantly surpasses traditional models in precision, recall, and F-measures in some evaluations (no numeric values provided in this paper).",
            "comparison_baseline": "Traditional query-generation methods and manual expert queries.",
            "performance_vs_baseline": "Cited as better in some cases, but no detailed numbers or experimental details provided in this paper.",
            "key_findings": "Using PLMs to generate or refine queries can increase retrieval effectiveness and reduce manual query-engineering effort, which benefits downstream automated literature-review pipelines.",
            "limitations_challenges": "Not detailed in this paper; general concerns include retrieval bias from generated queries, potential for overfitting query terms to model biases, and dependence on quality of PLM outputs.",
            "scaling_behavior": "Not discussed in detail; likely scales with the search space and retrieval engine rather than PLM size in the cited summaries.",
            "uuid": "e4382.3",
            "source_info": {
                "paper_title": "Leveraging LLMs for Efficient Topic Reviews",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "ASReview",
            "name_full": "ASReview (ML-aided pipeline for screening in systematic reviews)",
            "brief_description": "An open-source, active-learning machine-learning pipeline to assist title/abstract screening in systematic reviews, reducing human workload and improving screening efficiency.",
            "citation_title": "An open source machine learning framework for efficient and transparent systematic reviews.",
            "mention_or_use": "mention",
            "system_name": "ASReview (ML-aided systematic review screening)",
            "system_description": "Active-learning based screening tool that iteratively trains classifiers with reviewer feedback to prioritize likely-relevant citations, enabling reviewers to find relevant studies faster and reduce the number of abstracts to manually screen.",
            "llm_model_used": "Primarily traditional ML classifiers / active learning (SVMs, ensemble methods, etc.); not specifically an LLM in the referenced description, but cited as ML-aided pipeline relevant to automated literature review.",
            "extraction_technique": "Active-learning classification for relevance ranking (title/abstract screening), feature-based ML or embedding-based classifiers depending on configuration.",
            "synthesis_technique": "Not a synthesizer of claims/theories — it accelerates study selection which supports downstream synthesis tasks (meta-analysis, topic modeling).",
            "number_of_papers": "Designed for large-scale systematic reviews; used across datasets ranging from hundreds to thousands of citations (no specific number presented in this paper).",
            "domain_or_topic": "Systematic reviews across multiple domains, including medicine and social sciences.",
            "output_type": "Ranked list of candidate studies for reviewer screening; selected set of included/excluded studies.",
            "evaluation_metrics": "Screening efficiency metrics (reduction in number to screen), precision/recall of included studies, time saved (as reported in cited literature, not quantified here).",
            "performance_results": "Cited as significantly improving efficiency and quality of systematic reviews and meta-analyses in prior work; this paper references ASReview as a representative ML-aided pipeline but provides no new numeric evaluation.",
            "comparison_baseline": "Manual screening and static (non-active) ML-assisted approaches.",
            "performance_vs_baseline": "Reported improvements in efficiency in cited literature (no numeric comparison in this paper).",
            "key_findings": "Active learning pipelines like ASReview reduce human screening workload and integrate naturally into semi-automatic literature-review workflows such as TR, though they do not perform multi-document synthesis themselves.",
            "limitations_challenges": "Class imbalance, need for reviewer-in-the-loop, potential bias from initial seed selection, not a standalone synthesis/system for generating theories.",
            "scaling_behavior": "Designed to scale to thousands of citations; actual performance depends on classifier and feature choice—no explicit scaling curves in this paper.",
            "uuid": "e4382.4",
            "source_info": {
                "paper_title": "Leveraging LLMs for Efficient Topic Reviews",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural topic modeling with a class-based TF-IDF procedure.",
            "rating": 2,
            "sanitized_title": "neural_topic_modeling_with_a_classbased_tfidf_procedure"
        },
        {
            "paper_title": "An open source machine learning framework for efficient and transparent systematic reviews.",
            "rating": 2,
            "sanitized_title": "an_open_source_machine_learning_framework_for_efficient_and_transparent_systematic_reviews"
        },
        {
            "paper_title": "Automatic query generation using pre-trained language models",
            "rating": 2,
            "sanitized_title": "automatic_query_generation_using_pretrained_language_models"
        },
        {
            "paper_title": "Unraveling the landscape of large language models: A systematic review and future perspectives.",
            "rating": 1,
            "sanitized_title": "unraveling_the_landscape_of_large_language_models_a_systematic_review_and_future_perspectives"
        }
    ],
    "cost": 0.017232749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging LLMs for Efficient Topic Reviews
30 August 2024</p>
<p>Bady Gana bady.gana@pucv.cl 0009-0004-9840-0586
Escuela de Ingeniería Informática
Universidad Católica de Valparaíso
2241, 2362807ValparaísoAvenida BrasilPontificia, Chile</p>
<p>Andrés Leiva-Araos 0000-0002-2480-3022
Instituto de Data Science
Facultad de Ingeniería
Universidad del Desarrollo
Av. La Plaza 680</p>
<p>Las Condes
7610615SantiagoChile</p>
<p>Héctor Allende-Cid hector.allende-cid@iais.fraunhofer.de 0000-0003-3047-8817
Escuela de Ingeniería Informática
Universidad Católica de Valparaíso
2241, 2362807ValparaísoAvenida BrasilPontificia, Chile</p>
<p>Knowledge Discovery
Fraunhofer-Institut für Intelligente Analyse-und Informationssysteme (IAIS)
Schloss Birlinghoven 153757Sankt AugustinGermany</p>
<p>Lamarr Institute for Machine Learning and Artificial Intelligence
53115DortmundGermany</p>
<p>José García jose.garcia@pucv.cl 0000-0003-3126-8352
Escuela de Ingeniería en Construcción y Transporte
Pontificia</p>
<p>Universidad Católica de Valparaíso
2147, 2340000ValparaísoAv. BrasilChile</p>
<p>Leveraging LLMs for Efficient Topic Reviews
30 August 2024FA81726D7604954C10F6AB93CC5460D910.3390/app14177675Received: 7 August 2024 Revised: 21 August 2024 Accepted: 28 August 2024NLPLLMknowledge managementtransformer-based topic models
This paper presents the topic review (TR), a novel semi-automatic framework designed to enhance the efficiency and accuracy of literature reviews.By leveraging the capabilities of large language models (LLMs), TR addresses the inefficiencies and error-proneness of traditional review methods, especially in rapidly evolving fields.The framework significantly improves literature review processes by integrating advanced text mining and machine learning techniques.Through a case study approach, TR offers a step-by-step methodology that begins with query generation and refinement, followed by semi-automated text mining to identify relevant articles.LLMs are then employed to extract and categorize key themes and concepts, facilitating an in-depth literature analysis.This approach demonstrates the transformative potential of natural language processing in literature reviews.With an average similarity of 69.56% between generated and indexed keywords, TR effectively manages the growing volume of scientific publications, providing researchers with robust strategies for complex text synthesis and advancing knowledge in various domains.An expert analysis highlights a positive Fleiss' Kappa score, underscoring the significance and interpretability of the results.</p>
<p>Introduction</p>
<p>Recent advances in literature review processes have increasingly integrated machine learning (ML) and text-mining tools to enhance the efficiency and effectiveness of systematic reviews [1].These developments include significant improvements in query generation, information extraction, and refinement techniques, which aid in identifying relevant articles and developing efficient search strategies [2].Furthermore, automated text classification, including traditional ML methods and zero-shot classification techniques, has improved accuracy, precision, and recall in systematic review screening, reducing human error and workload [3,4].The integration of ML and natural language processing (NLP) has further advanced literature review processes, facilitating a more efficient synthesis of scholarly literature while addressing the challenges of interpreting complex academic jargon and nuanced expressions critical for high-quality data extraction [5][6][7][8].However, these advances also highlight the challenges of understanding the contextual and semantic intricacies of scholarly articles, which are essential to accurately evaluate and synthesize research findings [9][10][11].</p>
<p>To address these issues, the integration of large language models (LLMs) and topic analysis algorithms shows promising potential [9,10].LLMs, with their advanced capabilities in understanding and generating human-like text, can greatly enhance the identification and classification of key themes and concepts within large volumes of literature [11,12].This not only aids in overcoming the semantic complexities but also assists in distilling vast amounts of data into coherent and relevant information streams.Topic analysis, in conjunction with LLMs, can further refine the process by clustering related research works [13], thereby streamlining the literature review process and enhancing its representativeness.</p>
<p>The integration of LLMs and advanced tools like neural topic modeling with a classbased TF-IDF procedure (BERTopic) into the scientific literature review process represents a significant paradigm shift from traditional methods [14,15].Despite the advancements achieved with techniques such as probabilistic latent semantic analysis (PLSA) and latent dirichlet allocation (LDA) [16,17], LLMs and BERTopic offer unprecedented capabilities in understanding and analyzing complex contexts and semantic nuances in large volumes of text.This approach surpasses the limitations of methods that do not consider semantics and context [18,19], paving the way for more efficient, in-depth, and nuanced literature reviews.In this context, we propose a novel semi-automatic bibliographic review method that leverages the power of LLMs and transformer-based topic models to analyze and synthesize scientific literature.This approach not only promises to enhance the coverage of systematic reviews but also addresses the challenges of managing the ever-increasing volume of scientific publications, particularly in rapidly evolving fields such as medicine, cybersecurity, and the construction industry, among other technical areas.</p>
<p>Building on the advanced capabilities of LLMs and transformer-based topic models, this article introduces a new framework for conducting scientific literature reviews, a significant leap from traditional review methodologies.This innovative approach is designed to address the growing complexities and voluminous nature of contemporary research publications.The key contributions of this article are as follows:</p>
<p>•</p>
<p>A novel framework for semi-automatic literature review processes is proposed, utilizing the synergistic potential of LLMs and BERTopic.This framework is tailored to enhance the depth and breadth of literature analyses, ensuring comprehensive coverage across scholarly databases.</p>
<p>•</p>
<p>A single case study within this framework is developed and presented.This case study encompasses a specific domain demonstrating the applicability and robustness of the proposed method.• Specific metrics to assess the quality of the identified topics have been defined, contributing to the validation and continuous improvement of the framework and literature review process.• Through surveys and statistical tests such as Fleiss' Kappa, the case study has been rigorously evaluated by subject-matter experts.This expert validation, along with the statistical analysis, underscores the effectiveness of the framework in extracting relevant and profound insights from extensive scientific literature.</p>
<p>The integration of these elements marks a significant stride in modernizing literature review processes, offering a scalable solution to the challenges posed by the exponential growth of scientific information.</p>
<p>The rest of the paper is organized as follows: Section 2 presents an overview of ML and text-mining techniques applied to systematic literature review processes.The proposed framework and the developed experiments are detailed in Section 3. Insights from the selected bibliometric analysis, along with the statistical evaluations by experts, are disclosed in Section 4. Finally, Section 5 discusses the main conclusions and potential future directions.</p>
<p>Literature Review</p>
<p>In the context of ML applications in the literature review process, there is a growing integration of ML and text-mining tools to enhance the efficiency and efficacy of systematic reviews.In recent years, the authors emphasize advancements in query generation and refinement techniques.</p>
<p>Ref. [20] discusses an automatic query generation approach using pre-trained language models, significantly surpassing traditional models in precision, recall, and F-measures, and sometimes even manual queries.In parallel, ref. [21] analyzes query logs from a specialized tool, revealing intuitive user behaviors in query formulation for medical systematic reviews.Similarly, another research trend illustrates the effectiveness of semi-automated text-mining tools in identifying relevant articles and developing efficient search strategies.Ref. [22] assesses the utility of such tools in a systematic review of diagnostic test accuracy, yielding additional relevant articles but with varied precision.Complementarily, in [23], the authors focus on using text-mining software for medical subject headings (MeSH) term identification, effectively adding new tools to the Systematic Review Toolbox and highlighting the strategy's efficiency.Moreover, ref. [24] examines the quality and nature of search methods in Campbell systematic reviews, emphasizing the importance of adhering to Methodological Expectations of Campbell Collaboration Intervention Reviews (MECCIR) and Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [25] guidelines for comprehensive and reproducible searches.These studies collectively reflect a paradigm shift towards leveraging ML and text mining for optimizing literature review processes in systematic reviews.</p>
<p>In recent years, Automated Text Classification (ATC) in systematic review has had a significant step toward more efficient, accurate, and less labor-intensive research methodologies.Traditional ML and zero-shot classification methods have shown promise, achieving competitive accuracy, precision, and recall across various datasets, thereby reducing human error and workload in abstract screening [3].A paradigm shift towards semi-automated screening is observed, where decision support tools integrate multiple ML algorithms and feature selection techniques, demonstrating high effectiveness in areas like internet-based randomized controlled trials [4].Hybrid feature selection rule measures (HFSRM) and logistic regression classifiers have been effectively applied in HIV literature and diagnostic test accuracy (DTA) reviews, significantly reducing screening time and effort [26][27][28].Support vector machines (SVMs) and ensemble methods, trained interactively by reviewers, have been instrumental in distinguishing relevant from irrelevant citations, though class imbalance remains a challenge [29,30].The efficiency of ATC is further exemplified in food safety systematic reviews, where combinations of ML algorithms reduced the volume of articles for expert review while maintaining relevance [29].Moreover, advancements in NLP have facilitated prioritizing title and abstract screening, significantly reducing workload and bias in literature reviews [31,32].In [33], the authors propose a ML-aided pipeline, ASReview, to significantly improve the efficiency and quality of systematic reviews and meta-analyses.This tool, designed to address the inefficiencies of manual screening, leverages active learning to systematically and accurately filter relevant studies from vast amounts of scientific literature.</p>
<p>In terms of citation classification and research evaluation, several studies collectively contribute to a deeper understanding and advancement in citation classification and research evaluation, leveraging ML and NLP techniques for more efficient and comprehensive analysis of scholarly literature.Ref. [5] discusses the development of computer models using ML to predict long-term citation counts, addressing the limitation of traditional citation count methods, which cannot evaluate articles at publication time due to the cumulative nature of citations.Ref. [6] presents a classification approach for deep learning models in citation recommendation, emphasizing the need for a systematic study of these models' strengths, weaknesses, and evaluation metrics.Ref. [7] introduces the 'theory-as-discourse' approach for review article writing, utilizing ML for manuscript selection, thereby enhancing review article representativeness.Ref. [7] discusses the use of a hybrid document classification method termed "supervised clustering", which combines the benefits of supervised and unsupervised learning for effective document retrieval.Ref. [8] focuses on a meta-analysis of citation classification, exploring ML and NLP methods for analyzing linguistic features in citation context, highlighting the importance of identifying citation types for research evaluation.</p>
<p>The burgeoning field of automated literature review and scientific article analysis represents a significant stride in efficiently managing the ever-increasing volume of scientific literature.This advancement is underscored by [9], highlighting the challenge posed by the data deluge in the world wide web era and the consequential growth in scientific literature.It emphasizes the shift from manual to automated literature review processes, underscoring the necessity for computational techniques to handle the extensive data efficiently.Ref. [10] further elaborates on this, pointing out the transformative role of AI in revolutionizing traditional research practices, particularly in literature review processes in information systems research.The discussion extends to the need for AI in automating laborious aspects of literature review, echoing [9] emphasis on the increasing indispensability of automated methods.</p>
<p>In a similar vein, ref.</p>
<p>[11] delves into the challenges presented by the volume and complexity of scientific literature, advocating for systematic, replicable, and rigorous literature reviews augmented by computational techniques.This aligns with the [12] perspective, which proposes a general framework to automate systematic literature reviews by employing AI tools, thereby addressing the creative and technical aspects of the process.Even though the analyzed literature is very recent, due to the rapid development of new techniques, certain artifacts integrating various computational techniques for semiautomated literature review models, like in [13], appear obsolete as of today.</p>
<p>The recent surge in transformer-based topic models, such as BERTopic, has significantly expanded research horizons, broadening the scope of investigation to encompass a diverse array of disciplines.In [34], the authors employed the BERTopic algorithm to conduct a comprehensive analysis of the data; the analysis revealed four main clusters of topics in LLM research: "language and NLP", "education and teaching", "clinical and medical applications", and "speech and recognition techniques".In medical research, BERTopic's utility was evident in bone regeneration studies, where it identified 372 research topics, highlighting the growing importance of areas like 3D printing and extracellular vesicles.This application underlines BERTopic's capability to set up automated screening routines, tracking progress in innovative medical fields [35].In the financial sector, BERTopic has been employed for news impact analysis, significantly improving news selection for finance researchers by analyzing 38,240 news articles.This approach, compared against models like latent dirichlet allocation (LDA) and Top2Vec, demonstrated BERTopic's superiority in data interpretation and minimal preprocessing requirements, enhancing the feasibility and usability of the news impact analysis process [36].Additionally, BERTopic's robustness in topic modeling was further proven through its application in identifying and analyzing interdisciplinary topics in Library &amp; information science (LIS).By extracting, filtering, and analyzing topics based on disciplinary diversity and cohesion, BERTopic facilitated the exploration of interdisciplinary research's evolutionary path, indicating that growth in LIS primarily occurs in interdisciplinary topics [37].In [38], the authors investigate the efficacy of BERTopic for text clustering, comparing it with LDA and Top2Vec on Weibo and Twitter data, finding BERTopic superior in topic separation and understanding of text data structure, with at least 34.2% better performance in Chinese and English clustering.</p>
<p>In Table 1, we can observe a comparison of the contributions of previous studies with our work.</p>
<p>Collectively, establish a clear trajectory towards the increasing integration of AI and computational methods in the literature review process, reflecting a paradigm shift necessitated by the digital age's data challenges.</p>
<p>To the best of our knowledge, from the reviewed literature, we observe a significant effort in the application of techniques for the development of bibliographic reviews and linguistic summaries to uncover hidden patterns in data.This trend is not only a response to the sheer volume of data but also an evolution towards more sophisticated, efficient, and accurate scientific literature analysis methods.Our work in enhancing the efficiency of systematic literature review processes through artificial intelligence stands as a significant contribution to the field.While current efforts facilitate human effort and do not fully replace it, our approach notably advances the accuracy, speed, and comprehensiveness of literature reviews.This not only underscores the potential of AI in academic research but also paves the way for more sophisticated, AI-driven methodologies in future literature review processes.ML-Aided pipeline for screening in systematic reviews 2021</p>
<p>Proposes a machine learning-aided pipeline, ASReview, to significantly improve the efficiency and quality of systematic reviews and meta-analyses.</p>
<p>Select relevant studies, Extract data from selected studies [33] Systematic reviews automation based on query log analysis 2022</p>
<p>Analyzes query logs from a specialized tool to reveal intuitive user behaviors in query formulation for medical systematic reviews.</p>
<p>Conduct exhaustive literature search [21] Automatic query generation using pre-trained language models 2023</p>
<p>Discusses an automatic query generation approach using pre-trained language models, significantly surpassing traditional models in precision, recall, and F-measures.</p>
<p>Conduct exhaustive literature search [20] Utility of Text-Mining Tools 2023</p>
<p>Assesses the utility of semi-automated text-mining tools in systematic reviews of diagnostic test accuracy, yielding additional relevant articles with varied precision.</p>
<p>Select relevant studies [22] MeSH Term Identification 2023 Focuses on using MeSH term identification, highlighting the strategy's efficiency.</p>
<p>Conduct exhaustive literature search [23] BERTopic Analyze and interpret data [34][35][36][37][38] Our work 2024</p>
<p>Proposes an advanced systematic review process incorporating AI techniques for improved accuracy, speed, and comprehensiveness in literature reviews.</p>
<p>Semi-automatic exhaustive literature search and screening framework</p>
<p>Framework</p>
<p>The purpose of this section is to explain the details of the framework.The section is divided into four parts.The core components of the framework, which will be detailed in Section 3.1, the parameters and configuration of the framework set forth in Section 3.2, followed by the metrics used to evaluate the obtained results, discussed in Section 3.3, and finally, the proposed experiment to validate the effectiveness of the proposal, presented in Section 3.4.</p>
<p>Core Components</p>
<p>In this section, the core components of the proposed framework are detailed.Figure 1 displays the framework's flowchart.Initially, the abstracts from each document are processed by the framework.This framework is comprised of five principal components, each designed to optimize specific aspects of the literature review process.The framework begins by embedding abstracts using the FlagEmbedding model BGElarge-en-v1.5 [39], transforming documents into dense vectors for various tasks such as retrieval, classification, and clustering (Section 3.1.1).Following this, UMAP is applied for dimensionality reduction to manage high-dimensional data complexity while preserving crucial structures (Section 3.1.2).Clustering is then performed using HDBSCAN to identify coherent topics, minimizing noise, and grouping relevant documents (Section 3.1.3).Feature extraction is enhanced by a class-based TF-IDF (c-TF-IDF) [14], which improves thematic relevance and distinctiveness (Section 3.1.4).Finally, the integration of BERTopic with the SOLAR-10.7Bmodel [40] refines topic representation, providing a comprehensive analysis for NLP tasks (Section 3.1.5).This approach optimizes performance and significantly enhances the quality of thematic representation in literature reviews.</p>
<p>This cohesive approach ensures that the proposed framework not only optimizes performance but also significantly enhances the quality and depth of thematic representation in literature reviews.</p>
<p>Embedding</p>
<p>Bge-large-en-v1.5 is a transformer-based model developed by the Beijing Academy of Artificial Intelligence (BAAI) specifically for generating embeddings that capture the semantic content of text.The model is designed to efficiently process large-scale text data by leveraging its architecture to capture long-range dependencies within the text.This makes bge-large-en-v1.5 particularly effective in understanding the contextual relationships between words, which is essential for a wide range of NLP applications.</p>
<p>The effectiveness of bge-large-en-v1.5 is attributed not only to its optimized architecture and extensive training on a large dataset but also to its specific design, which efficiently processes large-scale text data.Bge-large-en-v1.5 employs a transformer-based architecture, which is known for its ability to capture long-range dependencies in text, making it particularly effective at understanding the contextual relationships between words.This model achieves a balance between efficiency and representation quality, making it well-suited for NLP applications that require both high accuracy and processing speed.</p>
<p>One of the key strengths of bge-large-en-v1.5 is its ability to generate dense, lowdimensional embeddings that capture the semantic relationships between words with high precision.Unlike traditional word embeddings such as Word2Vec [41] or GloVe [42], which rely on fixed-dimensional word vectors, bge-large-en-v1.5'stransformer-based approach allows for dynamic, context-sensitive embeddings.This results in more accurate thematic representations, especially in complex texts where the meaning of a word can vary depending on its context.</p>
<p>Additionally, bge-large-en-v1.5'sadaptability across different usage contexts is evidenced by its robust performance in various evaluation tasks.In the English Massive Text Embedding Benchmark (MTEB) benchmark [43], bge-large-en-v1.5 outperforms significantly larger models like GPT Sentence Embeddings for Semantic Search (SGPT) Bloom [44], which has 7.1 billion parameters.This is achieved by optimizing the architecture to reduce computational overhead while maximizing representational fidelity.The model's ability to maintain high performance with fewer parameters compared to larger models highlights its efficiency, making it an ideal choice for embedding within the proposed framework.</p>
<p>The transformer architecture used by bge-large-en-v1.5 also offers advantages over traditional methods, such as better handling of polysemy (multiple meanings of a word) and improved capture of word order and syntactic structure.These characteristics are critical for tasks that involve nuanced language understanding, further justifying the selection of bge-large-en-v1.5 for this framework.</p>
<p>Dimensional Reduction</p>
<p>Dimensionality reduction is a critical step in the proposed framework, aimed at simplifying high-dimensional embeddings to facilitate efficient clustering and further analysis.The framework utilizes Uniform Manifold Approximation and Projection (UMAP) [45] as the default technique due to its effectiveness in preserving both local and global structures within reduced dimensions.UMAP excels at maintaining the integrity of these structures, which is essential for ensuring accurate clustering of topics, as it helps retain the nuanced relationships between data points that might be lost with other dimensionality reduction techniques.</p>
<p>UMAP operates by constructing a high-dimensional graph representation of the data and then optimizing a low-dimensional graph that is as structurally similar as possible to the original.This method allows UMAP to capture both local data point relationships (i.e., how close data points are to their neighbors) and global data structures (i.e., how clusters of data are related to one another).This dual capability makes UMAP particularly effective for applications where understanding the global and local relationships within data is crucial, such as topic modeling in large text corpora.</p>
<p>One of UMAP's key characteristics is its computational efficiency, particularly when embedding in dimensions higher than two.Unlike t-distributed stochastic neighbor embedding (t-SNE) [46], which requires global normalization and the use of spatial trees that scale poorly with dimensionality, UMAP avoids these computational bottlenecks.UMAP's optimization process is significantly faster, making it more scalable for large datasets and suitable for real-time applications.This efficiency is not only beneficial for visualization purposes but is also critical for machine learning tasks such as clustering or anomaly detection, where high-dimensional data needs to be managed effectively without compromising on the quality of the resulting embeddings.</p>
<p>Additionally, UMAP's flexibility allows it to adapt to various data structures better than other techniques like LargeVis [47], especially on non-local scales where it excels.This makes UMAP a versatile tool in the framework, capable of providing high-quality, lowdimensional representations that are crucial for subsequent clustering and analysis tasks.The framework further enhances its adaptability by allowing the incorporation of other dimensionality reduction techniques to meet the specific needs of different analyses.This flexibility ensures that the framework can handle a wide range of datasets and analytical requirements.</p>
<p>Clustering</p>
<p>Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDB-SCAN) [48] is a clustering algorithm that extends the capabilities of DBSCAN by introducing a hierarchical approach.Unlike traditional clustering algorithms that require a predefined number of clusters, HDBSCAN is able to discover clusters of varying densities, which makes it particularly effective in dealing with real-world data where cluster densities can vary significantly.</p>
<p>HDBSCAN works by first constructing a minimum spanning tree of the distanceweighted graph, which represents the data points.It then performs a process known as "condensed clustering", where the tree is pruned to remove edges that represent noise, and the remaining tree structure reveals the clusters.This method allows HDBSCAN to model noise explicitly as outliers, meaning that points that do not clearly belong to any cluster are not forced into a cluster.This approach significantly improves the quality of the clustering by ensuring that only coherent, high-density regions of data are grouped together, which is crucial for accurate topic representation in NLP tasks.</p>
<p>One of the key advantages of HDBSCAN is its ability to produce a soft clustering output, where each data point is assigned a probability of belonging to a cluster.This soft clustering approach is particularly useful in situations where data points may be ambiguously located between clusters, as it provides a more nuanced understanding of the data structure compared to hard clustering methods like k-means.Additionally, HDBSCAN automatically determines the number of clusters based on the data, removing the need for the user to specify this parameter, which can be challenging in complex datasets.</p>
<p>HDBSCAN is employed in the proposed framework primarily due to its efficiency in identifying structures of varying densities, which is vital for obtaining accurate topic representations from the reduced embeddings produced by UMAP.The combination of UMAP and HDBSCAN allows for the discovery of meaningful clusters in high-dimensional data, where traditional clustering methods might struggle.This is particularly important in NLP tasks, where the semantic relationships between topics can vary in complexity and density.</p>
<p>The framework also maintains flexibility by allowing the integration of other clustering models as needed.This modularity ensures that the framework can adapt to new developments in clustering techniques, keeping it aligned with the latest trends in NLP and data science.</p>
<p>In studies conducted by [15,49], it was shown that reducing the dimensionality of high-dimensional embeddings with UMAP can enhance the performance of clustering algorithms like k-means and HDBSCAN, both in terms of accuracy and runtime.The combination of UMAP for dimensionality reduction and HDBSCAN for clustering has been proven effective across various NLP tasks, as demonstrated in several studies.For instance, ref. [50] highlighted the application of machine learning in improving water infrastructure integrity and quality, where UMAP was used to enhance the performance of clustering techniques.Additionally, in [51], HDBSCAN was used alongside UMAP to cluster short social media texts represented by FastText [52] and bidirectional encoder representations from transformers (BERT) embeddings, revealing trending topics within communities.Similarly, ref. [53] applied this approach to identify arguments in realworld datasets, while [54] used it to analyze tweets related to COVID-19 vaccines.Lastly, ref. [55] leveraged the UMAP-HDBSCAN combination for topic modeling in agricultural instructional sentences.</p>
<p>This robust combination of UMAP and HDBSCAN within the framework ensures that the clustering process is both efficient and effective, yielding high-quality topic representations essential for advanced NLP tasks.</p>
<p>Feature Extraction</p>
<p>To achieve an accurate representation of topics, the traditional term frequency-inverse document frequency (TF-IDF) method is adjusted to a class-based version, known as c-TF-IDF.This approach enhances the traditional TF-IDF by considering the uniqueness of documents within a group compared to others, resulting in a more refined and meaningful representation of topics.</p>
<p>The c-TF-IDF approach, as implemented in BERTopic, modifies the classic TF-IDF technique to operate at the cluster level rather than on individual documents.In this method, each cluster is treated as a single document by concatenating all the documents within that cluster.The term frequency within this aggregated cluster (or "class") is calculated in relation to the entire set of clusters with the goal of identifying terms that are particularly distinctive within each cluster.The inverse frequency is then adjusted by calculating the logarithm of the average number of words per class divided by the frequency of the term across all classes.</p>
<p>This adjustment allows c-TF-IDF to emphasize terms that are unique to specific clusters, rather than those that are common across the entire dataset.By focusing on the differences between clusters, c-TF-IDF effectively identifies keywords that define the thematic essence of each cluster, which might be diluted or overlooked by traditional TF-IDF methods that operate on individual documents.</p>
<p>One of the significant advantages of c-TF-IDF over the traditional TF-IDF is its ability to provide a clearer thematic structure within large text datasets.Traditional TF-IDF can struggle with sparse data when applied to large document collections, often leading to the identification of terms that are too generic or not representative of the specific themes within clusters.In contrast, c-TF-IDF ensures that the identified terms are directly related to the distinctive content of each cluster, thus improving the relevance and specificity of the topics generated.</p>
<p>By leveraging c-TF-IDF, BERTopic facilitates a deeper and more accurate understanding of the thematic structure in large text datasets.This approach not only improves the quality and relevance of the identified topics but also enhances the interpretability of the results, making it easier to understand the underlying patterns in the data.</p>
<p>Topic Representation</p>
<p>This proposed framework, by combining the modularity of BERTopic with the advanced capabilities of SOLAR-10.7B-Instruct-v1.0,establishes itself as a versatile and scal-able tool for literature analysis.Its design not only simplifies and streamlines the literature review process but also ensures that the conclusions drawn accurately and deeply reflect the evolving landscape of knowledge.With this approach, we aim to provide researchers and academics with a more efficient and effective methodology for synthesizing and analyzing large volumes of textual information.The architecture of SOLAR-10.7B-Instruct-v1.0,designed for superior performance across a variety of NLP tasks, facilitates the adaptability and effectiveness of the framework in diverse literature review applications.SOLAR 10.7B-Instruct, a large language model with 10.7 billion parameters, is noted for its exceptional performance on a variety of NLP tasks.This model uses an innovative approach called depth up-scaling (DUS), which allows LLMs to scale efficiently while maintaining their simplicity and effectiveness.A variant of SOLAR 10.7B, known as SOLAR 10.7B-Instruct, has been specifically tuned to follow complex instructions, outperforming models such as Mixtral-8x7B-Instruct [40].The inclusion of SOLAR 10.7B in BERTopic significantly improves the representation of topics by leveraging its ability to interpret and follow complex instructions, which is especially useful for selecting representative documents based on their similarity to c-TF-IDF representations of topics.SOLAR 10.7B's efficiency in handling a large number of documents and its ability to select those that are sufficiently diverse further strengthen its application in BERTopic.</p>
<p>Parameter Settings and Configurations</p>
<p>The parameter settings and configurations are crucial to ensure efficiency and accuracy in handling large models and performing complex NLP tasks.In the proposed approach for literature analysis using the Hugging Face Transformers library, several components are carefully configured to tailor the pipeline to the specific needs of each researcher.</p>
<p>The configuration starts with the extraction of the documents to be analyzed from a query, which will be transformed because of the embedding model.</p>
<p>Then the model quantization must be defined, using the bitsandbytes library for 4-bit quantization, which is essential for handling large models with improved GPU memory efficiency at low computational cost [56].</p>
<p>Key parameters include load in 4bit to enable quantization, bnb 4bit quant type as nf4 for a normalized float 4 quantization type, bnb 4bit use double quant for a double quantization, and bnb 4bit compute dtype set to bfloat16.In text generation, a GenerationConfig object is set following the guidelines of the transformers library.Do sample is activated, a temperature of 0.1 is set to control variability in the responses and a repetition penalty of 1.1 is set to reduce repetitions.The max new tokens limit is set to 50, achieving a balance between length and coherence of the generated text [57].</p>
<p>The template prompt (see Figure 2) serves as a structured model for generating instructions aimed at labeling topics.It is divided into three main components: the personality of the system generating the prompts, an example prompt illustrating how to craft a specific labeling task, and a general template for creating prompts.The system's personality is depicted as a helpful, respectful, and honest assistant.The example prompt provides a detailed scenario, including sample documents and keywords related to a topic, and instructs the user to generate a concise label for the topic based on the provided information.Meanwhile, the main template offers placeholders for documents and keywords, maintaining consistency in the labeling process by prompting users to create short labels for the topics.Through this structured approach, the prompt facilitates the generation of clear and consistent instructions for topic labeling tasks.</p>
<p>The SentenceTransformer model is used to convert texts into embeddings, specifically choosing the "BAAI/bge-large-en-v1.5"version.This allows an accurate and efficient numerical representation of the abstracts.In dimensionality reduction and clustering, UMAP is implemented with parameters such as n neighbors 10, n components 6, and metric cosine.For clustering, HDBSCAN is used, where the minimum cluster size is dynamic, allowing the researcher to adjust it according to the specific needs of the study.This flexibility is key to handling various dataset sizes and clustering levels.Representation models such as KeyBERTInspired, Maximal Marginal Relevance with 0.3 diversity, and TextGeneration with SOLAR 10.7B are integrated to improve the quality and relevance of the identified themes.Each with specific configurations that allow fine adaptability of the analysis.Finally, the BERTopic configuration includes adjustable submodels and hyperparameters such as top n words.This, together with the ability to train and transform the model with different sets of abstracts and embeddings, ensures that researchers can custom calibrate the pipeline, adapting it to the diverse requirements of their research projects.</p>
<p>Metrics</p>
<p>In the context of text analysis for the generation of clustered topics, it is essential to have metrics that evaluate the similarity between the indexed key terms.This is because these keywords are essential for the understanding and thematic organization of the documents.The comparison between the indexed keywords, for example, those coming from the documents and those generated by tools such as KeyBERT, allows to evaluate the quality of the clustered topic generation process provides a quantitative measure of the similarity between the sets of keywords, thus helping to determine the effectiveness of the topic generation process.</p>
<p>•</p>
<p>Similarity based on indexed keywords: In this study, the cosine-based similarity technique using BERT embeddings is employed to compare the relationship between Scopus indexed keywords and keywords generated by the KeyBERT tool [58].This technique relies on the vector representation of texts, where each word is converted into a numerical vector in a high-dimensional space.The similarity between two texts is calculated by evaluating the angle between the vectors representing these texts.The closer the angle is to zero, the higher the similarity between the texts.This approach allows for a quantitative and accurate comparison of keywords, facilitating the evaluation of the quality of the clustered topic generation process.Additionally, statistical tests are conducted to determine the significance of the observed similarity results, using a p-value threshold to assess whether the results are statistically significant at a 95% confidence level.• Thematic coherence analysis: In our study, we assess the relevance of clustered themes derived from the TR process, utilizing a survey administered to seven expert raters.The evaluation focuses on two key parameters: meaningfulness and importance; both evaluated as high, medium, and low.Meaningfulness pertains to the extent to which the AI-generated custom names for each topic cluster accurately and significantly represent concepts within the fields of SLR, ML, and NLP, reflecting the depth and relevance of these topics in the broader area of knowledge.Importance evaluates the perceived significance of each custom name, considering its impact, influence, or critical value within the area of knowledge research.To quantify the agreement among raters regarding these evaluations, we employ the Fleiss' κ score.Fleiss' Kappa (κ) is a statistical measure for assessing the reliability of agreement between a fixed number of raters when assigning categorical ratings to a number of items.It is expressed as:
κ = P − Pe 1 − Pe
where P is the proportion of times that raters agree (observed agreement proportion) and Pe is the hypothetical probability of chance agreement.Using Fleiss' Kappa, one can determine if the agreement level is better than chance, with a κ of 1 indicating perfect agreement and a κ less than or equal to 0 suggesting no agreement beyond chance.</p>
<p>Experiment</p>
<p>In this section, we describe the application of the configured pipeline to perform a literature review based on the provided initialization of the research.</p>
<p>Our experiment started with the formulation of a specific research topic.From this topic, we designed a precise query to extract relevant papers from the Scopus database.The query was carefully constructed using a combination of keywords related to the literature review and automation.</p>
<p>In this case, they are items from the following scopus query (See Figure 3) results in 19,643 documents.In this section, we describe the application of the configured pipeline to perform a 475 literature review based on the provided initialization of the research.</p>
<p>476</p>
<p>Our experiment started with the formulation of a specific research topic.From this 477 topic, we designed a precise query to extract relevant papers from the Scopus database.The 478 query was carefully constructed using a combination of keywords related to the literature 479 review and automation.</p>
<p>480</p>
<p>In this case they are items from the following scopus query (See</p>
<p>Q1: ( literature AND review OR slr OR overview OR survey OR insights OR screening ) AND ( semi-automated OR semi-automation OR automation ) OR ( machine AND learning OR deep AND learning )</p>
<p>The query is built to capture relevant documents for literature review and the automa-483 tion process.Key terms such as "literature review", "SLR" (Systematic Literature Review), 484 "overview," "survey," "insights," and "literature screening" are used to identify studies re-485 lated to literature review.Additionally, terms like "semi-automated," "semi-automation," 486 and "automation" are included to search for research addressing partial or total automation 487 of the review process.Terms related to ML, such as "machine learning" and "deep learn-488 ing," are also incorporated to broaden the search scope and capture works utilizing ML 489 techniques in the context of literature review.The combination of these terms in the query 490 aims to ensure the comprehensiveness and relevance of the retrieved documents to support 491 the ongoing research.</p>
<p>492</p>
<p>Our experiment applied TR to Scopus documents to conduct a literature review on 493 knowledge management.We aimed to evaluate the framework's effectiveness in synthesiz-494 ing and analyzing a large number of academic papers.Specifically, we assessed its ability 495 to perform a comprehensive and systematic review, determine the precision and coherence 496 of issues identified, analyze the efficiency of information extraction and insight generation, 497 validate its adaptability to various knowledge domains and research topics, and identify 498 potential areas for improvement in future research and applications.</p>
<p>499</p>
<p>The parameter used to configure HDBSCAN is min cluster size, in this case, it has 500 been set to 50 in this experiment, which means that any group identified by the algorithm 501 The query is built to capture relevant documents for literature review and the automation process.Key terms such as "literature review", "SLR", (Systematic Literature Review), "overview", "survey", "insights", and "literature screening" are used to identify studies related to literature review.Additionally, terms like "semi-automated", "semi-automation", and "automation" are included to search for research addressing partial or total automation of the review process.Terms related to ML, such as "machine learning" and "deep learning", are also incorporated to broaden the search scope and capture works utilizing ML techniques in the context of literature reviews.The combination of these terms in the query aims to ensure the comprehensiveness and relevance of the retrieved documents to support the ongoing research.</p>
<p>Our experiment applied TR to Scopus documents to conduct a literature review on knowledge management.We aimed to evaluate the framework's effectiveness in synthesizing and analyzing a large number of academic papers.Specifically, we assessed its ability to perform a comprehensive and systematic review, determine the precision and coherence of issues identified, analyze the efficiency of information extraction and insight generation, validate its adaptability to various knowledge domains and research topics, and identify potential areas for improvement in future research and applications.</p>
<p>The parameter used to configure HDBSCAN is minimum cluster size; in this case, it has been set to 50 in this experiment, which means that any group identified by the algorithm that has less than 50 data points will not be considered as a valid cluster.</p>
<p>Hardware</p>
<p>An Nvidia A100 GPU (40GB) was utilized for data processing and executing the analysis algorithms on Google Colaboratory.The use of this GPU allowed for significant acceleration in the calculations required for the application of the literature analysis framework, thereby contributing to enhancing the efficiency and speed of the process.</p>
<p>Results and Discussion</p>
<p>In this section, we present the outcomes of experiment 3.4, which focused on assessing the performance of the configured framework for literature review and clustered topic analysis.The section is structured into two parts: Results in Section 4.1 and Discussion in Section 4.2.</p>
<p>Results and Analysis</p>
<p>The results of our analysis are organized into three sections.First, Visualization Section 4.1.1provides graphical representations of the data, aiding in the visualization of trends, patterns, and relationships within the analyzed information.Second, Metrics Section 4.1.2present quantitative measures, offering numerical insights into various aspects of the analysis.Finally, Expert Analysis, as referred to in Section 4.1.3,offers interpretation and insights from domain experts, providing nuanced perspectives and qualitative evaluations based on deep domain knowledge.</p>
<p>Our focus was on assessing the effectiveness of the proposed framework for literature review and clustered topic analysis.We began by formulating a specific research topic and designing a precise query (see Section 3.4) to extract relevant papers from the Scopus database, resulting in 19,643 documents.The experiment then applied the framework to conduct a literature review on knowledge management within the context of the literature review.Our objectives included evaluating the framework's ability to conduct a comprehensive and systematic review, assessing the precision and coherence of identified topics, and analyzing the efficiency in extracting relevant information.</p>
<p>Visualization</p>
<p>In this section, we focus on presenting a variety of visualizations that shed light on the findings of our study regarding the effectiveness of a configured framework for literature review and clustered topic analysis.The query we employed (See Section 3.4) was specifically designed to capture relevant documents for literature review and the automation process.We delve into an exploratory analysis of these findings, delving into the depths of the framework output to reveal hidden insights and facilitate a nuanced interpretation of the data.From the 19,643 documents, two categories emerge: the 65 topics identified as clusters and, on the other hand, the category of "outliers", comprising 5848 document that are not deemed individually sufficient to form a cluster.Hence, the numerical disparity underscores that 13,793 documents are appropriately grouped.</p>
<p>From the 65 correctly grouped knowledge clusters, the aim of the experiment is to identify the topics most relevant to the initial query, which primarily focuses on literature review, literature screening, and associated automation processes.To achieve this, we need to identify them from the output topics of the framework.Since this is a semi-automatic step, it involves evaluating the topics that best align with the focus of the researcher based on their initial query.In this way, the following topics (see Table 2) can be identified.The proximity of these topics suggests a convergence in the field of text processing applied to medicine and biomedicine.This could indicate an active area of research where new techniques are being explored and developed to automate and enhance the analysis of medical documents.For this reason, it is ideal to explore visualizations that allow the selection of the most suitable topic according to the initial query.</p>
<p>From the 2D dimensionality reduction representation using UMAP, the spatial distribution of documents can be observed.The following visualization (see Figure 4a) provided an organized structure for exploring and understanding the thematic diversity of the retrieved documents.Significantly, we selected topic 6 (see Figure 4b), titled "Automated Methods for Literature Screening in Medical Systematic Reviews".For our investigation, Topic 6 aligns closely with the objectives of our experiment (see Section 3.4), which aimed to assess the framework's ability to conduct a comprehensive and systematic review of literature related to literature review practices, including automation processes.By focusing on a specific topic within the broader context of literature review and automation, we can draw meaningful conclusions about the framework's adaptability across different knowledge domains.Additionally, we remarked Topic -1 as outlier documents (see Figure 4c), this topic involves the documents that did not manage to form a cluster, thus deviating significantly from the rest of the topics.</p>
<p>To effectively analyze Topic 6, a visualization such as the hierarchical cluster visualization (see Figure 5) is essential.This visualization plays a crucial role in our analysis by providing a hierarchical representation of the relationships between topics.</p>
<p>It allows us to explore how closely related topics are organized within the hierarchy, shedding light on their inter-cluster distances.Topic 6 appears to exhibit a close relationship with Topic 29, focusing on "Biomedical Text Mining for Gene and Protein Extraction".This observation is discerned through their proximity in the hierarchical cluster, indicating potential thematic similarities or shared characteristics despite their distinct subject matters.The framework demonstrates several significant advantages that underscore its utility and effectiveness in the realm of literature review and clustered topic analysis.This is exemplified in the Bubble Cloud (see Figure 6), which presents a representation showcasing the most frequent words in Topic 6.This visualization effectively illustrates how the proposed framework adeptly captures the knowledge landscape within this specific domain.</p>
<p>This visualization lies in its ability to highlight key themes inherent to the query.Each bubble contains terms closely associated with the systematic review process, such as "systematic review", "machine learning", and related concepts like "method", "article", and "research".By encapsulating these essential elements within the bubble cloud, the visualization provides a briefly yet comprehensive overview of the thematic focus within Topic 6.The visualizations presented in this section offer a deep understanding of the implications of the configured framework for literature review and clustered topic analysis.By connecting the query employed in Scopus with the emerging topics, these visualizations provide researchers with valuable tools for navigating and interpreting vast amounts of scholarly literature.Through the exploration of specific topics aligned with the study's objectives and the comparison and contrast of thematic relationships among clusters, researchers can gain deep insights into the structure and coherence of the analyzed corpus.</p>
<p>Evaluation of Topic Generation Quality</p>
<p>The Evaluation of Topic Generation Quality section aims to assess and validate the effectiveness of the topic generation process by comparing the automatically generated keywords with those indexed in Scopus and provided by the authors in the documents.This comparison allows us to measure the accuracy and relevance of the generated topics, ensuring that the thematic organization and representation of the documents are consistent with the original content.By using advanced NLP techniques, this section provides a quantitative analysis that supports the reliability and quality of the topic modeling approach employed in this study.</p>
<p>Of the 399 documents initially obtained in Topic 6, 267 containing both author keywords and index keywords were filtered.These documents were specifically selected for their likelihood of providing more complete and detailed information on the topic in question, thus allowing a more accurate and relevant selection for the literature analysis.In this sense, the perspective is based on techniques used for research and systematic reviews (see Figure 6).</p>
<p>To further analyze the quality and consistency of topic generation, we conducted an experiment focused on comparing the cosine similarity between the keywords generated by KeyBERT for Topic 6 and the keywords indexed and provided by the authors in the same set of documents.This comparison allowed us to directly measure how well the automatically generated topics align with the established thematic content.</p>
<p>Moreover, we expanded the analysis by comparing the KeyBERT output for documents within Topic 6 against the average KeyBERT-generated keywords from four other selected topics (Topics 9, 44, 51, and 63).These topics were chosen as baselines because they are related to the general scope of the Scopus query but are distinct from Topic 6, offering a meaningful comparison point.To determine if there is a significant difference in topic identification between KeyBERT's output for Topic 6 and the averaged results from the other four topics, we conducted a series of statistical analyses.We first applied the Shapiro-Wilk test to check for normality in the data.After confirming that the data followed a normal distribution, we performed paired t-Tests to assess the significance of the differences in cosine similarity.The results indicated statistically significant differences, with p-values suggesting that KeyBERT's performance in generating topics for Topic 6 is distinct when compared to the baseline topics.</p>
<p>The results presented in Table 3 provide an overview of the cosine similarity between the KeyBERT-generated keywords and those indexed and authored in the documents analyzed.The descriptive statistics indicate that for the index keywords in Topic 6, the mean cosine similarity is 0.6956, with a median of 0.7007 and a standard deviation of 0.0531.In contrast, the mean for the averaged selected topics (Topics 9, 44, 51, and 63) is slightly lower at 0.6323, with a median of 0.6309 and a smaller standard deviation of 0.0245, suggesting less variability in similarity scores across these topics.A similar trend is observed for the author keywords, where Topic 6 shows a mean cosine similarity of 0.6971, a median of 0.7064, and a standard deviation of 0.0717.The selected topics have a lower mean of 0.5958, a median of 0.6019, and a standard deviation of 0.0432, again indicating more consistency but lower overall similarity compared to Topic 6.The Shapiro-Wilk test results confirm the normality of the data across all categories, with p-values exceeding the 0.05 threshold, thus justifying the use of the paired t-Test for further analysis.The t-Test paired p-values, 0.0135 for index keywords and 0.0224 for author keywords, indicate statistically significant differences between the cosine similarities of Topic 6 and the averaged selected topics.These results suggest that the KeyBERT model effectively captures the thematic essence of Topic 6, resulting in higher similarity scores compared to the baseline topics.</p>
<p>The findings reported in Table 3 underscore the effectiveness of KeyBERT in generating relevant and accurate keywords for Topic 6, as evidenced by the statistically significant differences in cosine similarity when compared to other topics.The higher mean and median values for Topic 6, along with the significant t-Test results, demonstrate that the model's performance is superior when applied to documents specifically related to Topic 6.This suggests that KeyBERT is well-suited for tasks requiring precise topic modeling and keyword generation in similar contexts.</p>
<p>Expert Analysis</p>
<p>The evaluation of both meaningfulness and importance yields Fleiss' Kappa scores of 0.0402 and 0.0650, respectively (see Table 4).These scores fall into the "slight agreement" range according to [59] scale, indicating a minimal consensus among raters with a marginally higher agreement on the importance of topics.In both cases, the P j values-representing proportions of ratings across high, medium, and low categories for both dimensions-combined with the proportions of agreement beyond chance P e , reveal nuanced perspectives among the raters.Specifically, the P j values suggest raters are somewhat more aligned in recognizing topics of high importance, as reflected in the slightly higher Fleiss' Kappa score for importance.This marginal difference in consensus levels between meaningfulness and importance underscores the subjective nature of evaluating AI-generated topic clusters and highlights the challenge of achieving substantial inter-rater reliability.Our evaluation of these metrics focuses on the diversity of interests and prior knowledge of the different topics inherent to each rater.For instance, raters who are dedicated to conducting research in the field of NLP consistently produced higher evaluations in both dimensions than those who work in more generic areas such as ML.</p>
<p>Discussion</p>
<p>In this section, we critically examine the strengths and weaknesses of the framework discussed.</p>
<p>The researcher bears the responsibility for selecting the final topics, a decision that should harmonize with the research topic.Each chosen topic embodies a cohesive collection of documents interlinked by thematic content, thereby facilitating a meticulous exploration of particular domains within the realms of knowledge management and literature review.</p>
<p>This translates into greater results identifying themes and trends in large academic datasets.Together, these benefits position the framework as a powerful and versatile tool for the synthesis and analysis of academic literature, offering researchers a solid framework for exploring and understanding knowledge in their respective fields of study, which facilitates its adoption by researchers with different levels of expertise.</p>
<p>The flexibility of the framework to adjust parameters such as the minimum cluster size in HDBSCAN, the temperature of the generative model, and the modularity of BERTopic allows tailoring the analysis to the specific needs of each research, ensuring customized and relevant results.Defining the minimum cluster size parameter in HDBSCAN is a trialand-error process, where finding the optimal cluster size required to adequately represent a specific topic can be challenging.This reliance on manual experimentation can slow down the process and affect the reproducibility of the results.</p>
<p>Despite the promising results, the framework has certain shortcomings that deserve consideration.The temperature of the generative model can fluctuate, leading to a certain lack of consistency in text generation.While this variability can be beneficial for exploring different perspectives, it can also make it difficult to interpret the results consistently.</p>
<p>The framework is subject to constant evolution of the underlying libraries and tools, such as the transformers library, which can lead to dependency on specific code versions and require continuous adaptations to maintain pipeline functionality.These limitations highlight the need for further research and refinement to improve the robustness and efficiency of the framework in future applications.</p>
<p>It is also essential to compare the obtained results, particularly the keywords from Topic 6, with those generated by the KeyBERT model.As presented in Table 3.These findings underscore the effectiveness of the KeyBERT model in capturing the thematic essence of Topic 6, demonstrating that it outperforms in generating relevant and accurate keywords compared to other topics.This suggests that the framework's integration with KeyBERT is particularly well-suited for tasks requiring precise topic modeling and keyword generation, making it a valuable tool for academic literature analysis in specialized domains.Thus, while the framework offers substantial advantages in terms of flexibility and adaptability, its effectiveness is further enhanced when used in conjunction with models like KeyBERT, particularly for topics that demand high precision in keyword generation.</p>
<p>Conclusions and Future Work</p>
<p>In this work, we present a framework that integrates NLP and topic generation tools into a cohesive system.This framework simplifies the topic modeling process and ensures adaptability across various contexts, including academic research, market analysis, and social network analysis.We assessed the applicability of our framework using documents extracted from Scopus, starting with the formulation of a specific research topic.This involved crafting a query, reviewing the topic, and applying filters to assemble a relevant document collection.</p>
<p>We conducted an in-depth analysis to evaluate the quality of the topic generation process using KeyBERT.By comparing the cosine similarity between the automatically generated keywords for Topic 6 and those indexed and provided by the authors, as well as comparing these with the averaged results from Topics 9, 44, 51, and 63, we assessed the model's performance.The statistical analyses, including the Shapiro-Wilk test for normality and the paired t-Test, revealed that Topic 6 exhibited significantly higher cosine similarity scores for both index keywords and author keywords compared to the baseline topics.These findings highlight the effectiveness of KeyBERT in accurately capturing the thematic essence of documents, especially when applied to specific, well-defined topics like Topic 6.Additionally, the evaluation of meaningfulness and importance yielded Fleiss' Kappa scores of 0.0402 and 0.0650, respectively, reflecting slight agreement among raters and suggesting the need for enhanced consistency.</p>
<p>Looking ahead, systematic hyperparameter optimization in BERTopic-through methods such as grid search, random search, or Bayesian optimization-is crucial for refining model performance.This approach is essential for advancing NLP by ensuring that derived topics are relevant and well-understood, which is vital for managing extensive and nuanced textual data.</p>
<p>Furthermore, developing robust, automated metrics for clear and interpretable indicators remains an ongoing research area.If human evaluation continues to be necessary, our research highlights the importance of precise evaluative criteria or improved rater training to enhance consistency and reliability in assessing topic significance, particularly in systematic literature reviews.</p>
<p>Figure 1 .
1
Figure 1.Framework flow diagram.This diagram shows the pipeline from document extraction to finding the filtered topics, optionally repeating the process for them.</p>
<p>Figure 2 .
2
Figure 2. Template prompt: System + Example + Main.Based on [57].</p>
<p>Figure 3 .
3
Figure 3. Scopus Query Statement.</p>
<p>Figure 4 .
4
Figure 4. Clusters output (D1 and D2 representing the dimensions reduced by 2D-UMAP.These dimensions, D1 and D2, are the axes that best preserve the topological structure of the data.),(a) All the clusters, (b) Topic 6, and (c) The outliers documents.</p>
<p>Figure 5 .
5
Figure 5. Topic hierarchy.The proximity of documents in the hierarchy is directly related to the similarity of the vectors representing them.The colors represent hierarchical clusters.Interpreting this diagram allows for more solid conclusions than those that can be obtained from the visualization provided in Figure 4a.</p>
<p>Figure 6 .
6
Figure 6.Bubble Cloud of most frequent words in topic 6.This visualization presents the frequency of up to 2 n-grams in topic 6.</p>
<p>Table 1 .
1
Comparison of Related Work.
TopicYearScopeSLR Focus TaskCitationClassic machine learning algorithms in text mining for abstract screening2007-2022Explores the use of SVMs and ensemble methods in distinguishing relevant citations, with a focus on overcoming class imbalance.Select relevant studies, Assess study quality[29,30]Hybrid Feature Selection with supervised learning for automatic ranking of articles2016-2018Discusses the application of HFSRM and logistic regression classifiers in DTA reviews, significantly reducing screening time and effort.Assess study quality, Analyze and interpret data[26-28]Adherence of search methods to SLR frameworks2021Examines the quality and nature of search methods in Campbell systematic reviews, emphasizing adherence to MECCIR and PRISMA guidelines.Develop review protocol, Conduct exhaustive literature search[24]</p>
<p>Pewhere P is the proportion of times that raters agree (observed agreement proportion) 469 and Pe is the hypothetical probability of chance agreement.Using Fleiss' Kappa, one 470 can determine if the agreement level is better than chance, with a κ of 1 indicating 471 perfect agreement and a κ less than or equal to 0 suggesting no agreement beyond 472 chance.
Version August 21, 2024 submitted to Appl. Sci.12 of 24fixed number of raters when assigning categorical ratings to a number of items. It is 467expressed as:468κ =P − Pe 1 −
473 3.4.Experiment 474</p>
<p>Table 2 )
2
results in 481 19643 documents.</p>
<p>482</p>
<p>Table 2 .
2
Scopus Query Statement.</p>
<p>Table 2 .
2
Top Topics Identified in the Study.
Topic Custom Name</p>
<p>Table 3 .
3
Cosine similarity comparison between index keywords and author keywords with KeyBERT output; this process is applied to 267 documents that contain both types of keywords.The table includes descriptive statistics, Shapiro-Wilk test results for normality, and t-Test paired p-values to assess the significance of differences.
Index KeywordsAuthor KeywordsKeyBERT (6)Mean of Selected TopicsKeyBERT (6)Mean of Selected TopicsMin Max Mean Median Std Dev0.4956 0.8149 0.6956 0.7007 0.05310.5333 0.6940 0.6323 0.6309 0.02450.3577 0.8600 0.6971 0.7064 0.07170.3071 0.6847 0.5958 0.6019 0.0432Shapiro-Wilk p-value t-Test Paired p-value0.19090.01350.21100.45510.02240.2572</p>
<p>Table 4 .
4
Expert analysis results.Meaningfulness and importance parameters.
Context ParameterAverage ScoreσPePFleiss' κMeaningfulness2.4230.650.420.540.0402Importance2.3550.680.400.440.0650
Acknowledgments: Bady Gana is supported by National Agency for Research and Development (ANID)/Scholarship Program/DOCTORADO NACIONAL/2024-21240115. Bady Gana is supported by Beca INF-PUCV.José García is supported by VINCI-DI:039.463/2024.Data Availability Statement:The original data[60]and code presented in this study are openly available in the accessed date 19 August 2024 GitHub repository at[61].Conflicts of Interest:The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.AbbreviationsThe following abbreviations are used in this manuscript:Symbol
Automating systematic literature reviews with natural language processing and text mining: A systematic literature review. G Sundaram, D Berleant, Proceedings of the International Congress on Information and Communication Technology. the International Congress on Information and Communication TechnologyLondon, UK; SingaporeSpringerFebruary 2023. 2023</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. J De La Torre-López, A Ramírez, J R Romero, 10.1007/s00607-023-01181-x2023105</p>
<p>Aceves-Martins, M. A novel application of machine learning and zero-shot classification methods for automated abstract screening in systematic reviews. C F Moreno-Garcia, C Jayne, E Elyan, 10.1016/j.dajour.2023.100162Decis. Anal. J. 2023, 6, 100162</p>
<p>Automatic text classification to support systematic reviews in medicine. J G Adeva, J P Atxa, M U Carrillo, E A Zengotitabengoa, 10.1016/j.eswa.2013.08.047Expert Syst. Appl. 412014</p>
<p>Using content-based and bibliometric features for machine learning models to predict citation counts in the biomedical literature. L Fu, C Aliferis, 10.1007/s11192-010-0160-5Scientometrics. 852010</p>
<p>Deep learning in citation recommendation models survey. Z Ali, P Kefalas, K Muhammad, B Ali, M Imran, 10.1016/j.eswa.2020.113790Expert Syst. Appl. 1622020. 113790</p>
<p>Understanding the elephant: The discourse approach to boundary identification and corpus construction for theory review articles. K R Larsen, D Hovorka, A Dennis, J D West, 10.17705/1jais.00556J. Assoc. Inf. Syst. 202019</p>
<p>A meta-analysis of semantic classification of citations. S N Kunnath, D Herrmannova, D Pride, P Knoth, 10.1162/qss_a_00159Quant. Sci. Stud. 22021</p>
<p>Information extraction from scientific articles: A survey. Z Nasar, S W Jaffry, M K Malik, 10.1007/s11192-018-2921-5Scientometrics. 1172018</p>
<p>Artificial intelligence and the conduct of literature reviews. G Wagner, R Lukyanenko, G Paré, J. Inf. Technol. 372022</p>
<p>Computational literature reviews: Method, algorithms, and roadmap. D Antons, C F Breidbach, A M Joshi, T O Salge, 10.1177/1094428121991230Organ. Res. Methods. 262023</p>
<p>A roadmap toward the automatic composition of systematic literature reviews. Da Silva Júnior, E M Dutra, M L , 10.47909/ijsmc.52Iberoam. J. Sci. Meas. Commun. 2021</p>
<p>Towards an Integrative Approach for Automated Literature Reviews Using Machine Learning. C Tauchert, M Bender, N Mesbah, P Buxmann, Proceedings of the 53rd Hawaii International Conference on System Sciences. the 53rd Hawaii International Conference on System SciencesMaui, HI, USAJanuary 20202020</p>
<p>M Grootendorst, Bertopic, arXiv:2203.05794Neural topic modeling with a class-based TF-IDF procedure. arXiv 2022. </p>
<p>Machine learning techniques applied to construction: A hybrid bibliometric analysis of advances and future directions. J Garcia, G Villavicencio, F Altimiras, B Crawford, R Soto, V Minatogawa, M Franco, D Martínez-Muñoz, V Yepes, 10.1016/j.autcon.2022.104532Autom. Constr. 1422022. 104532</p>
<p>Unsupervised learning by probabilistic latent semantic analysis. T Hofmann, 10.1023/A:1007617005950Mach. Learn. 422001</p>
<p>Latent dirichlet allocation. D Blei, A Ng, M Jordan, J. Mach. Learn. Res. 32003</p>
<p>Learning the parts of objects by non-negative matrix factorization. D Lee, H Seung, 10.1038/44565Nature. 4011999</p>
<p>Learning topic models-going beyond SVD. S Arora, R Ge, A Moitra, 10.1109/FOCS.2012.49Proceedings of the 2012 IEEE 53rd Annual Symposium on Foundations of Computer Science. the 2012 IEEE 53rd Annual Symposium on Foundations of Computer ScienceNew Brunswick, NJ, USA20-23 October 2012</p>
<p>Towards semantic-driven boolean query formalization for biomedical systematic literature reviews. M Pourreza, F Ensan, 10.1016/j.ijmedinf.2022.104928Int. J. Med. Inform. 1702023. 104928</p>
<p>The Impact of Query Refinement on Systematic Review Literature Search: A Query Log Analysis. H Scells, C Forbes, J Clark, B Koopman, G Zuccon, Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval. the 2022 ACM SIGIR International Conference on Theory of Information RetrievalMadrid, SpainJuly 2022</p>
<p>Investigation of text-mining methodologies to aid the construction of search strategies in systematic reviews of diagnostic test accuracy-A case study. H O'keefe, J Rankin, S A Wallace, F Beyer, 10.1002/jrsm.1593Res. Synth. Methods. 142023</p>
<p>A mapping exercise using automated techniques to develop a search strategy to identify systematic review tools. A Sutton, H O'keefe, E E Johnson, C Marshall, 10.1002/jrsm.1665Res. Synth. Methods. 142023</p>
<p>PROTOCOL: Searching and reporting in Campbell Collaboration systematic reviews: An assessment of current methods. S Young, A Bethel, C Keenan, K Ghezzi-Kopel, E Moreton, D Pickup, Z A Premji, M Rogers, B C Viinholt, 10.1002/cl2.1208Campbell Syst. Rev. 172021. e1208</p>
<p>The PRISMA 2020 statement: An updated guideline for reporting systematic reviews. M J Page, J E Mckenzie, P M Bossuyt, I Boutron, T C Hoffmann, C D Mulrow, L Shamseer, J M Tetzlaff, E A Akl, S E Brennan, 10.1136/bmj.n71BMJ. 2021n71. [CrossRef</p>
<p>Data sampling and supervised learning for HIV literature screening. H Almeida, M J Meurs, L Kosseim, A Tsang, 10.1109/TNB.2016.2565481IEEE Trans. Nanobiosci. 152016</p>
<p>CLEF ehealth 2017 task 2: Logistic regression for automatic article ranking. C Norman12, M Leeflang, A Névéol, Limsi@, Proceedings of the CEUR Workshop Proceedings: Working Notes of CLEF 2019: Conference and Labs of the Evaluation Forum. the CEUR Workshop Proceedings: Working Notes of CLEF 2019: Conference and Labs of the Evaluation ForumLugano, Switzerland9-12 September 2019</p>
<p>Technology Assisted Reviews by Stacking Active and Static Learning. C R Norman, M M Leeflang, A Névéol, Limsi, Clef, Proceedings of the CLEF 2018-Working Notes of CLEF 2018 Conference and Labs of the Evaluation Forum. the CLEF 2018-Working Notes of CLEF 2018 Conference and Labs of the Evaluation ForumAvignon, France2018. September 20182</p>
<p>Automatic classification of literature in systematic reviews on food safety using machine learning. L M Van Den Bulk, Y Bouzembrak, A Gavai, N Liu, L J Van Den Heuvel, H J Marvin, 10.1016/j.crfs.2021.12.010Curr. Res. Food Sci. 2022</p>
<p>Classifier Ensemble for Biomedical Document Retrieval. M Torii, H Liu, LBM (Short Papers); CEUR-WS. Washington, DC, USA200717</p>
<p>Natural language processing was effective in assisting rapid title and abstract screening when updating systematic reviews. X Qin, J Liu, Y Wang, Y Liu, K Deng, Y Ma, K Zou, L Li, X Sun, 10.1016/j.jclinepi.2021.01.010J. Clin. Epidemiol. 1332021</p>
<p>Improvement of intervention information detection for automated clinical literature screening during systematic review. T Tsubota, D Bollegala, Y Zhao, Y Jin, T Kozu, 10.1016/j.jbi.2022.104185J. Biomed. Inform. 1342022. 104185</p>
<p>An open source machine learning framework for efficient and transparent systematic reviews. R Van De Schoot, J De Bruin, R Schram, P Zahedi, J De Boer, F Weijdema, B Kramer, M Huijts, M Hoogerwerf, G Ferdinands, 10.1038/s42256-020-00287-7Nat. Mach. Intell. 32021</p>
<p>Unraveling the landscape of large language models: A systematic review and future perspectives. Q Ding, D Ding, Y Wang, C Guan, B Ding, 10.1108/JEBDE-08-2023-0015J. Electron. Bus. Digit. Econ. 2023</p>
<p>Modeling new trends in bone regeneration, using the BERTopic approach. S Guizzardi, M T Colangelo, P Mirandola, C Galli, 10.2217/rme-2023-0096Regen. Med. 182023</p>
<p>Leveraging State-of-the-Art Topic Modeling for News Impact Analysis on Financial Markets: A Comparative Study. W Chen, F Rabhi, W Liao, I Al-Qudah, 10.3390/electronics121226052023. 260512</p>
<p>Identifying interdisciplinary topics and their evolution based on BERTopic. Z Wang, J Chen, J Chen, H Chen, Scientometrics. Berlin/Heidelberg, GermanySpringer2023</p>
<p>Experimental Comparison of Three Topic Modeling Methods with LDA, Top2Vec and BERTopic. L Gan, T Yang, Y Huang, B Yang, Y Y Luo, L W C Richard, D Guo, Artificial Intelligence and Robotics, Proceedings of the 8th International Symposium, ISAIR 2023. Beijing, China; SingaporeSpringer21-23 October 2023. 2023</p>
<p>S Xiao, Z Liu, P Zhang, N Muennighoff, C-Pack, arXiv:2309.07597v2Packaged Resources to Advance General Chinese Embedding. arXiv 2023. </p>
<p>D Kim, C Park, S Kim, W Lee, W Song, Y Kim, H Kim, Y Kim, H Lee, J Kim, arXiv:2312.15166B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling. arXiv 2023. 10</p>
<p>Efficient Estimation of Word Representations in Vector Space. T Mikolov, K Chen, G Corrado, J Dean, Proceedings of the Workshop at ICLR. the Workshop at ICLRScottsdale, AZ, USA2-4 May 2013</p>
<p>Glove: Global Vectors for Word Representation. J Pennington, R Socher, C D Manning, Proceedings of the EMNLP. the EMNLPDoha, QatarOctober 201414</p>
<p>N Muennighoff, N Tazi, L Magne, N Reimers, Mteb, arXiv:2210.07316Massive Text Embedding Benchmark. arXiv 2022. </p>
<p>N Muennighoff, Sgpt, arXiv:2202.08904GPT Sentence Embeddings for Semantic Search. arXiv 2022. </p>
<p>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. L Mcinnes, J Healy, 10.21105/joss.00861J. Open Source Softw. 38612018</p>
<p>Visualizing data using t-SNE. L Van Der Maaten, G Hinton, J. Mach. Learn. Res. 92008</p>
<p>Visualizing Large-scale and High-dimensional Data. J Tang, J Liu, M Zhang, Q Mei, 10.1145/2872427.2883041WWW '16, Proceedings of the 25th International Conference on World Wide Web. Montréal, QC, Canada; Geneva, Switzerland11-15 April 2016. 2016International World Wide Web Conferences Steering Committee</p>
<p>Density-Based Clustering Based on Hierarchical Density Estimates. R Campello, D Moulavi, J Sander, Advances in Knowledge Discovery and Data Mining, Proceedings of the 17th Pacific-Asia Conference, PAKDD 2013. Gold Coast, AustraliaApril 2013</p>
<p>. Proceedings, Ii; Part, Springer, 10.1007/978-3-642-37456-2_1420137819Berlin/Heidelberg, Germany</p>
<p>Considerably Improving Clustering Algorithms Using UMAP Dimensionality Reduction Technique: A Comparative Study. M Allaoui, M L Kherfi, A Cheriet, 2020SpringerBerlin/Heidelberg, Germany</p>
<p>J García, A Leiva-Araos, E Diaz-Saavedra, P Moraga, H Pinto, Yepes, 10.3390/app132212497Machine Learning Techniques in Water Infrastructure Integrity and Quality: A Review Powered by Natural Language Processing. 2023. 1249713</p>
<p>Improving the Performance of HDBSCAN on Short Text Clustering by Using Word Embedding and UMAP. M S Asyaky, R Mandala, 10.1109/ICAICTA53211.2021.9640285Proceedings of the 2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA). the 2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)Bandung, IndonesiaSeptember 2021</p>
<p>Enriching Word Vectors with Subword Information. P Bojanowski, E Grave, A Joulin, T Mikolov, 10.1162/tacl_a_00051Trans. Assoc. Comput. Linguist. 52016</p>
<p>M Färber, A Steyer, arXiv:2112.00160Towards Full-Fledged Argument Search: A Framework for Extracting and Clustering Arguments from Unstructured Text. arXiv 2021. </p>
<p>Text Clustering of COVID-19 Vaccine Tweets. U David, M Karabatak, 10.1109/ISDFS55398.2022.9800754Proceedings of the 2022 10th International Symposium on Digital Forensics and Security (ISDFS). the 2022 10th International Symposium on Digital Forensics and Security (ISDFS)Istanbul, TurkeyJune 2022</p>
<p>Bertopic and NER Stop Words for Topic Modeling on Agricultural Instructional Sentences. T Gelar, A N Sari, 10.2991/978-94-6463-364-1_14Proceedings of the International Conference on Applied Science and Technology on Engineering Science 2023 (iCAST-ES 2023). the International Conference on Applied Science and Technology on Engineering Science 2023 (iCAST-ES 2023)Tarakan, Indonesia; Paris, FranceAtlantis Press20-22 October 2024. 2024</p>
<p>T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Qlora, arXiv:2305.14314Efficient Finetuning of Quantized LLMs. arXiv 2023. </p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:2005.14165Language Models are Few-Shot Learners. 2020</p>
<p>Minimal Keyword Extraction with BERT. M Grootendorst, Keybert, 2020. 28 November 2023</p>
<p>The measurement of observer agreement for categorical data. J R Landis, G G Koch, 10.2307/2529310Biometrics. 331977</p>
<p>Leveraging LLMs for Efficient Topic Reviews. Gana Castillo, B P , 2024. 19 August 2024</p>
<p>Topic-Modeling-BERTopic-SOLAR. B Gana Castillo, 2024. 19 August 2024</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
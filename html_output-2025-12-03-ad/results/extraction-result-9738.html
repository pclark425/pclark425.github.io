<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9738 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9738</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9738</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278740946</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.12301v1.pdf" target="_blank">Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge</a></p>
                <p><strong>Paper Abstract:</strong> LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with empirical human distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9738.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9738.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConceptualLoss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual information loss from single-point LLM judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies that replacing human judgment distributions with single-point LLM outputs (or otherwise uncalibrated LLM judgments) causes information loss: it removes diversity, disagreement, uncertainty and subjectivity present in human annotations, and LLMs tend to be overconfident and skewed toward few options.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General evaluation / LLM-as-a-Judge paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various LLMs (open-source and closed-source evaluated: Qwen2.5-7B, LLaMA3.1-8B, GPT-4o, GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Models produce a probability distribution over discrete judgment labels by extracting logits for judgment tokens (top-5 logits used), with prompts instructing single-label outputs for each evaluation (e.g., Likert 1-5 or entailment labels); prior approaches often trained LLMs to output single deterministic labels (single-point alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotations form empirical label distributions: datasets have N annotators per sample (e.g., SNLI/MNLI: 5 annotators; Summeval: expert and crowd ratings on Likert 1–5; MT-Bench: human reviewer(s) choosing A/B/Tie). The empirical p(x) distribution is derived from these annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>KL divergence between human empirical distribution p(x) and model predicted distribution q_theta(x) (primary), and top-1 Accuracy comparing argmax(q) to argmax(p). Metrics are used throughout; qualitative statement that raw models often have KL > 2.0 without alignment (Table 1 & text).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of distributional information (disagreement, uncertainty, subjectivity) when collapsing to a single-point; decreased reliability and comprehensiveness of evaluations; LLM outputs are often overconfident and concentrated on few options (biased/skewed distributions); potential loss in explainability because explanations correspond to a single sampled judgment rather than the full predicted distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Qualitative and empirical demonstrations: the paper notes 'collapsing this diversity into a single decision overlooks valuable information such as disagreement, uncertainty, and subjectivity' (Introduction/Related Work). Empirically, many raw models exhibit large KL divergence from human distributions (text states KL typically exceeding 2.0 without alignment), illustrating substantial misalignment and thus information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>More capable models (e.g., GPT-4o) naturally produce predicted distributions closer to human annotations than weaker models (text notes correlation between model capability and alignment). Also, distributional alignment (the paper's method) can recover much of the lost information by training models to match human distributions and by using adversarial training for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Introduction; Related Work (LLM-as-a-Judge); Preliminary; Table 1; Limitations; Ethical Consideration</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9738.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9738.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLI_Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison on Natural Language Inference (SNLI/MNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On NLI tasks (SNLI and MNLI), unaligned LLMs produce judgment distributions that diverge from human annotators (high KL), and single-point supervision reduces but does not fully capture human uncertainty — distribution alignment substantially lowers KL while maintaining or slightly improving accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Natural Language Inference (SNLI, MNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Evaluated models include GPT-4o-mini, GPT-4o (raw), Qwen2.5, and LLaMA3.1 (fine-tuned variants for alignment comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompts from LogiEval (modified): given premise and hypothesis, model chooses one label (entailment/neutral/contradiction). Predictions are converted to a probability distribution over these 3 labels by extracting logits for judgment tokens (top-5 logits).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Each NLI instance annotated by 5 human annotators; empirical distribution p(x) derived from annotator labels; datasets subsampled (10k from MNLI to match scale).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>KL divergence and Accuracy. Representative reported values (Table 1): Qwen2.5 single-point on SNLI KL=0.72 Acc=92.6%; Qwen2.5 distribution (ours) SNLI KL=0.31 Acc=93.0%. For MNLI Qwen2.5 single-point KL=0.74 Acc=89.2%; distribution KL=0.23 Acc=89.0%. Raw models often exhibit much larger KL (text: 'KL typically exceeding 2.0' without alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Single-point judgments lose annotator disagreement and uncertainty in NLI labels; raw LLMs can produce miscalibrated/overconfident distributions that do not reflect human label variability, reducing fidelity to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Empirical: raw models show substantially higher KL to human distributions (Table 1 and discussion). Single-point alignment reduces KL but still does not match the finer-grained human distributions; distribution alignment lowers KL further (Qwen2.5 example above).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>High-capability LLMs (GPT-4o raw) show better initial alignment (lower raw KL) than weaker models, indicating model capability mitigates but does not eliminate the loss. The paper's distribution alignment method recovers distributional information and preserves accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Preliminary (dataset details); Experiment Setup (Datasets, Prompts); Table 1; 'Correlation between Model Capability and Alignment Performance' in Results</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9738.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9738.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SummEval_Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison on Summarization quality ratings (Summeval Likert metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For summarization evaluation (Summeval, four 1–5 Likert dimensions), LLMs as judges often misrepresent the human rating distribution (high KL for raw models); single-point alignment provides modest gains, but explicit distribution alignment better captures the human rating distribution and uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization quality evaluation (fluency, coherence, consistency, relevance) using Summeval</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Qwen2.5, LLaMA3.1, GPT-4o, GPT-4o-mini evaluated (open- and closed-source models).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompts adapted from G-Eval: models are asked to output only a single Likert score (1–5) following detailed metric instructions; logits for judgment tokens are converted into a distribution over the 5-point scale (top-5 logits used).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Summeval summaries rated by experts and crowdworkers on 1–5 Likert scale across four dimensions; each dimension treated independently; dataset contains 5,104 samples with multiple annotators forming p(x).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>KL divergence and Accuracy. Representative reported values (Table 1): Qwen2.5 single-point KL=0.74 Acc=45.8%; Qwen2.5 distribution (ours) KL=0.52 Acc=46.6%. Raw closed-source models can show very large KL on SummEval (e.g., GPT-4o-mini raw KL=5.23 Acc=25.6% in Table 1), demonstrating severe misalignment.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of subtle subjective nuance and rating variability in human Likert scores when using single-point or unaligned LLM judgments; LLMs can be overconfident and fail to represent multimodal human opinion spread across rating bins; this leads to large KL divergence and poor fidelity on subjective, fine-grained metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete divergence: GPT-4o-mini raw KL=5.23 on Summeval (Table 1), with very low accuracy (25.6%), illustrating how some LLMs severely misalign with human rating distributions and fail to capture human variability.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Distribution alignment reduces KL substantially and slightly improves accuracy, suggesting that training to match human distributions can recover much of the lost nuance. Also, more capable models tend to be less misaligned initially.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Experiment Setup (Datasets: Summeval); Prompts (Appendix A); Table 1; Overall Performance; Robustness Analysis (Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9738.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9738.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MTBench_Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison on human preference judgments (MT-Bench A vs B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On preference comparisons (MT-Bench), raw and single-point LLM judgments produce distributions that diverge from human preferences; distribution alignment reduces KL and achieves slightly better top-1 agreement while better representing 'Tie' / disagreement cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Human preference comparison for dialogue/model responses (MT-Bench)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Qwen2.5, LLaMA3.1, GPT-4o, GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompts adapted from MT-Bench: present two model responses (A and B) and ask for a choice (a, b, tie). Model logits for tokens corresponding to a/b/tie are converted to a distribution (top-5 logits constrained).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>MT-Bench contains pairwise comparisons with at least one human reviewer per model pair (A vs B); human preference (A, Tie, B) forms empirical distribution p(x).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>KL divergence and Accuracy. Representative reported values (Table 1) for Qwen2.5: single-point KL=0.83 Acc=63.0%; distribution KL=0.68 Acc=63.6%. Table 3 shows distributional alignment yields lower KL across perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges can underrepresent ties and nuanced preference splits; unaligned LLMs may overcommit to a side and not reflect human indecision; single-point supervision masks annotator disagreement and reduces fidelity to human preference distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Quantified divergence: single-point baseline KL higher than distribution-aligned models (Table 1 and Table 3). The paper highlights that single-point methods collapse disagreement and ignore the distributional nature of human preference judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Distribution alignment recovers much of the preference distribution and improves robustness under label perturbations (Table 3). High-capacity LLMs again tend to be closer to human distributions initially.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Experiment Setup (Datasets: MT-Bench); Table 1; Robustness Analysis (Table 3); Overall Performance</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9738.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9738.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Explainability_Loss</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explainability limitation when LLMs act as judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes a limitation: generated explanations from a distribution-trained judge correspond to a single sampled judgment and therefore do not explain the full predicted distribution, reducing interpretability of distributional judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General evaluation explainability</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Distribution-trained LLMs (as proposed in paper) and baseline models</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Models can be prompted to produce an explanatory string, but the paper's framework focuses on distributional outputs (probability vectors) rather than textual explanations of the distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators provide distributions; human explanations (if any) are not expanded in the datasets used.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Not applicable (explainability limitation is qualitative); described in 'Limitations' section.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When using LLMs as judges, generated explanations typically map to a single sampled label rather than summarizing the full uncertainty/variance encoded in the predicted distribution, so users lose comprehensive interpretability of model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Explicit statement in Limitations: 'the model's explainability is limited, as the generated explanations only correspond to a single sampled judgment rather than interpreting the entire predicted distribution.'</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Paper suggests future work will improve explainability; distribution alignment itself improves fidelity to human distributions but does not solve explanation-of-distribution issue.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Limitations</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9738.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9738.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bias_Risk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Risk of misaligned/overconfident automated judgments amplifying human biases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors warn that automated judge systems can reflect and amplify biases present in the human data used for alignment, and miscalibrated or overconfident LLM judgments can produce biased or unfair decisions in high-stakes or subjective domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Ethical/societal impacts of LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>All LLMs considered in study (open- and closed-source)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Evaluation pipelines aligning LLM outputs to human empirical distributions; ethical considerations apply regardless of specific prompt setup.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotation distributions used as targets; possible dataset biases discussed qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Qualitative warning; robustness metrics (KL under perturbation) also used to analyze sensitivity to annotation noise (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Potential amplification of biases from training data, overconfidence leading to unjustified certainty, and failure to capture nuanced fairness considerations from diverse human perspectives.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Ethical Consideration section warns that 'Misaligned or overconfident evaluations may lead to biased decisions, potentially reflecting and amplifying biases subtly present within the human data used for the alignment process itself.'</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Adversarial training and distributional alignment are proposed to increase robustness to sampling noise and reduce overfitting to limited annotations; nevertheless, the paper acknowledges residual risks and the need for careful dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Ethical Consideration; Robustness Analysis; Conclusion</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, Llms-as-judges: A comprehensive survey on llm-based evaluation methods. 2024 <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge <em>(Rating: 2)</em></li>
                <li>Improving llm-as-a-judge inference with the judgment distribution <em>(Rating: 2)</em></li>
                <li>Geval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9738",
    "paper_id": "paper-278740946",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "ConceptualLoss",
            "name_full": "Conceptual information loss from single-point LLM judgments",
            "brief_description": "The paper identifies that replacing human judgment distributions with single-point LLM outputs (or otherwise uncalibrated LLM judgments) causes information loss: it removes diversity, disagreement, uncertainty and subjectivity present in human annotations, and LLMs tend to be overconfident and skewed toward few options.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General evaluation / LLM-as-a-Judge paradigm",
            "llm_judge_model": "Various LLMs (open-source and closed-source evaluated: Qwen2.5-7B, LLaMA3.1-8B, GPT-4o, GPT-4o-mini)",
            "llm_judge_setup": "Models produce a probability distribution over discrete judgment labels by extracting logits for judgment tokens (top-5 logits used), with prompts instructing single-label outputs for each evaluation (e.g., Likert 1-5 or entailment labels); prior approaches often trained LLMs to output single deterministic labels (single-point alignment).",
            "human_evaluation_setup": "Human annotations form empirical label distributions: datasets have N annotators per sample (e.g., SNLI/MNLI: 5 annotators; Summeval: expert and crowd ratings on Likert 1–5; MT-Bench: human reviewer(s) choosing A/B/Tie). The empirical p(x) distribution is derived from these annotations.",
            "agreement_metric": "KL divergence between human empirical distribution p(x) and model predicted distribution q_theta(x) (primary), and top-1 Accuracy comparing argmax(q) to argmax(p). Metrics are used throughout; qualitative statement that raw models often have KL &gt; 2.0 without alignment (Table 1 & text).",
            "losses_identified": "Loss of distributional information (disagreement, uncertainty, subjectivity) when collapsing to a single-point; decreased reliability and comprehensiveness of evaluations; LLM outputs are often overconfident and concentrated on few options (biased/skewed distributions); potential loss in explainability because explanations correspond to a single sampled judgment rather than the full predicted distribution.",
            "examples_of_loss": "Qualitative and empirical demonstrations: the paper notes 'collapsing this diversity into a single decision overlooks valuable information such as disagreement, uncertainty, and subjectivity' (Introduction/Related Work). Empirically, many raw models exhibit large KL divergence from human distributions (text states KL typically exceeding 2.0 without alignment), illustrating substantial misalignment and thus information loss.",
            "counterexamples_or_caveats": "More capable models (e.g., GPT-4o) naturally produce predicted distributions closer to human annotations than weaker models (text notes correlation between model capability and alignment). Also, distributional alignment (the paper's method) can recover much of the lost information by training models to match human distributions and by using adversarial training for robustness.",
            "paper_reference": "Introduction; Related Work (LLM-as-a-Judge); Preliminary; Table 1; Limitations; Ethical Consideration",
            "uuid": "e9738.0",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "NLI_Comparison",
            "name_full": "Comparison on Natural Language Inference (SNLI/MNLI)",
            "brief_description": "On NLI tasks (SNLI and MNLI), unaligned LLMs produce judgment distributions that diverge from human annotators (high KL), and single-point supervision reduces but does not fully capture human uncertainty — distribution alignment substantially lowers KL while maintaining or slightly improving accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Natural Language Inference (SNLI, MNLI)",
            "llm_judge_model": "Evaluated models include GPT-4o-mini, GPT-4o (raw), Qwen2.5, and LLaMA3.1 (fine-tuned variants for alignment comparisons).",
            "llm_judge_setup": "Prompts from LogiEval (modified): given premise and hypothesis, model chooses one label (entailment/neutral/contradiction). Predictions are converted to a probability distribution over these 3 labels by extracting logits for judgment tokens (top-5 logits).",
            "human_evaluation_setup": "Each NLI instance annotated by 5 human annotators; empirical distribution p(x) derived from annotator labels; datasets subsampled (10k from MNLI to match scale).",
            "agreement_metric": "KL divergence and Accuracy. Representative reported values (Table 1): Qwen2.5 single-point on SNLI KL=0.72 Acc=92.6%; Qwen2.5 distribution (ours) SNLI KL=0.31 Acc=93.0%. For MNLI Qwen2.5 single-point KL=0.74 Acc=89.2%; distribution KL=0.23 Acc=89.0%. Raw models often exhibit much larger KL (text: 'KL typically exceeding 2.0' without alignment).",
            "losses_identified": "Single-point judgments lose annotator disagreement and uncertainty in NLI labels; raw LLMs can produce miscalibrated/overconfident distributions that do not reflect human label variability, reducing fidelity to human judgments.",
            "examples_of_loss": "Empirical: raw models show substantially higher KL to human distributions (Table 1 and discussion). Single-point alignment reduces KL but still does not match the finer-grained human distributions; distribution alignment lowers KL further (Qwen2.5 example above).",
            "counterexamples_or_caveats": "High-capability LLMs (GPT-4o raw) show better initial alignment (lower raw KL) than weaker models, indicating model capability mitigates but does not eliminate the loss. The paper's distribution alignment method recovers distributional information and preserves accuracy.",
            "paper_reference": "Preliminary (dataset details); Experiment Setup (Datasets, Prompts); Table 1; 'Correlation between Model Capability and Alignment Performance' in Results",
            "uuid": "e9738.1",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "SummEval_Comparison",
            "name_full": "Comparison on Summarization quality ratings (Summeval Likert metrics)",
            "brief_description": "For summarization evaluation (Summeval, four 1–5 Likert dimensions), LLMs as judges often misrepresent the human rating distribution (high KL for raw models); single-point alignment provides modest gains, but explicit distribution alignment better captures the human rating distribution and uncertainty.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization quality evaluation (fluency, coherence, consistency, relevance) using Summeval",
            "llm_judge_model": "Qwen2.5, LLaMA3.1, GPT-4o, GPT-4o-mini evaluated (open- and closed-source models).",
            "llm_judge_setup": "Prompts adapted from G-Eval: models are asked to output only a single Likert score (1–5) following detailed metric instructions; logits for judgment tokens are converted into a distribution over the 5-point scale (top-5 logits used).",
            "human_evaluation_setup": "Summeval summaries rated by experts and crowdworkers on 1–5 Likert scale across four dimensions; each dimension treated independently; dataset contains 5,104 samples with multiple annotators forming p(x).",
            "agreement_metric": "KL divergence and Accuracy. Representative reported values (Table 1): Qwen2.5 single-point KL=0.74 Acc=45.8%; Qwen2.5 distribution (ours) KL=0.52 Acc=46.6%. Raw closed-source models can show very large KL on SummEval (e.g., GPT-4o-mini raw KL=5.23 Acc=25.6% in Table 1), demonstrating severe misalignment.",
            "losses_identified": "Loss of subtle subjective nuance and rating variability in human Likert scores when using single-point or unaligned LLM judgments; LLMs can be overconfident and fail to represent multimodal human opinion spread across rating bins; this leads to large KL divergence and poor fidelity on subjective, fine-grained metrics.",
            "examples_of_loss": "Concrete divergence: GPT-4o-mini raw KL=5.23 on Summeval (Table 1), with very low accuracy (25.6%), illustrating how some LLMs severely misalign with human rating distributions and fail to capture human variability.",
            "counterexamples_or_caveats": "Distribution alignment reduces KL substantially and slightly improves accuracy, suggesting that training to match human distributions can recover much of the lost nuance. Also, more capable models tend to be less misaligned initially.",
            "paper_reference": "Experiment Setup (Datasets: Summeval); Prompts (Appendix A); Table 1; Overall Performance; Robustness Analysis (Table 3)",
            "uuid": "e9738.2",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MTBench_Comparison",
            "name_full": "Comparison on human preference judgments (MT-Bench A vs B)",
            "brief_description": "On preference comparisons (MT-Bench), raw and single-point LLM judgments produce distributions that diverge from human preferences; distribution alignment reduces KL and achieves slightly better top-1 agreement while better representing 'Tie' / disagreement cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Human preference comparison for dialogue/model responses (MT-Bench)",
            "llm_judge_model": "Qwen2.5, LLaMA3.1, GPT-4o, GPT-4o-mini",
            "llm_judge_setup": "Prompts adapted from MT-Bench: present two model responses (A and B) and ask for a choice (a, b, tie). Model logits for tokens corresponding to a/b/tie are converted to a distribution (top-5 logits constrained).",
            "human_evaluation_setup": "MT-Bench contains pairwise comparisons with at least one human reviewer per model pair (A vs B); human preference (A, Tie, B) forms empirical distribution p(x).",
            "agreement_metric": "KL divergence and Accuracy. Representative reported values (Table 1) for Qwen2.5: single-point KL=0.83 Acc=63.0%; distribution KL=0.68 Acc=63.6%. Table 3 shows distributional alignment yields lower KL across perturbations.",
            "losses_identified": "Using LLMs as judges can underrepresent ties and nuanced preference splits; unaligned LLMs may overcommit to a side and not reflect human indecision; single-point supervision masks annotator disagreement and reduces fidelity to human preference distributions.",
            "examples_of_loss": "Quantified divergence: single-point baseline KL higher than distribution-aligned models (Table 1 and Table 3). The paper highlights that single-point methods collapse disagreement and ignore the distributional nature of human preference judgments.",
            "counterexamples_or_caveats": "Distribution alignment recovers much of the preference distribution and improves robustness under label perturbations (Table 3). High-capacity LLMs again tend to be closer to human distributions initially.",
            "paper_reference": "Experiment Setup (Datasets: MT-Bench); Table 1; Robustness Analysis (Table 3); Overall Performance",
            "uuid": "e9738.3",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Explainability_Loss",
            "name_full": "Explainability limitation when LLMs act as judges",
            "brief_description": "The paper notes a limitation: generated explanations from a distribution-trained judge correspond to a single sampled judgment and therefore do not explain the full predicted distribution, reducing interpretability of distributional judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "General evaluation explainability",
            "llm_judge_model": "Distribution-trained LLMs (as proposed in paper) and baseline models",
            "llm_judge_setup": "Models can be prompted to produce an explanatory string, but the paper's framework focuses on distributional outputs (probability vectors) rather than textual explanations of the distribution.",
            "human_evaluation_setup": "Human annotators provide distributions; human explanations (if any) are not expanded in the datasets used.",
            "agreement_metric": "Not applicable (explainability limitation is qualitative); described in 'Limitations' section.",
            "losses_identified": "When using LLMs as judges, generated explanations typically map to a single sampled label rather than summarizing the full uncertainty/variance encoded in the predicted distribution, so users lose comprehensive interpretability of model uncertainty.",
            "examples_of_loss": "Explicit statement in Limitations: 'the model's explainability is limited, as the generated explanations only correspond to a single sampled judgment rather than interpreting the entire predicted distribution.'",
            "counterexamples_or_caveats": "Paper suggests future work will improve explainability; distribution alignment itself improves fidelity to human distributions but does not solve explanation-of-distribution issue.",
            "paper_reference": "Limitations",
            "uuid": "e9738.4",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Bias_Risk",
            "name_full": "Risk of misaligned/overconfident automated judgments amplifying human biases",
            "brief_description": "The authors warn that automated judge systems can reflect and amplify biases present in the human data used for alignment, and miscalibrated or overconfident LLM judgments can produce biased or unfair decisions in high-stakes or subjective domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Ethical/societal impacts of LLM-as-a-Judge",
            "llm_judge_model": "All LLMs considered in study (open- and closed-source)",
            "llm_judge_setup": "Evaluation pipelines aligning LLM outputs to human empirical distributions; ethical considerations apply regardless of specific prompt setup.",
            "human_evaluation_setup": "Human annotation distributions used as targets; possible dataset biases discussed qualitatively.",
            "agreement_metric": "Qualitative warning; robustness metrics (KL under perturbation) also used to analyze sensitivity to annotation noise (Table 3).",
            "losses_identified": "Potential amplification of biases from training data, overconfidence leading to unjustified certainty, and failure to capture nuanced fairness considerations from diverse human perspectives.",
            "examples_of_loss": "Ethical Consideration section warns that 'Misaligned or overconfident evaluations may lead to biased decisions, potentially reflecting and amplifying biases subtly present within the human data used for the alignment process itself.'",
            "counterexamples_or_caveats": "Adversarial training and distributional alignment are proposed to increase robustness to sampling noise and reduce overfitting to limited annotations; nevertheless, the paper acknowledges residual risks and the need for careful dataset construction.",
            "paper_reference": "Ethical Consideration; Robustness Analysis; Conclusion",
            "uuid": "e9738.5",
            "source_info": {
                "paper_title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, Llms-as-judges: A comprehensive survey on llm-based evaluation methods. 2024",
            "rating": 2,
            "sanitized_title": "haitao_li_qian_dong_junjie_chen_huixue_su_yujia_zhou_qingyao_ai_ziyi_ye_yiqun_liu_llmsasjudges_a_comprehensive_survey_on_llmbased_evaluation_methods_2024"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge",
            "rating": 2,
            "sanitized_title": "beyond_correlation_the_impact_of_human_uncertainty_in_measuring_the_effectiveness_of_automatic_evaluation_and_llmasajudge"
        },
        {
            "paper_title": "Improving llm-as-a-judge inference with the judgment distribution",
            "rating": 2,
            "sanitized_title": "improving_llmasajudge_inference_with_the_judgment_distribution"
        },
        {
            "paper_title": "Geval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 1,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        }
    ],
    "cost": 0.01311725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge</p>
<p>Luyu Chen luyu.chen@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Zeyu Zhang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Haoran Tan 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Quanyu Dai 
Huawei Noah's Ark Lab</p>
<p>Hao Yang 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Zhenhua Dong 
Huawei Noah's Ark Lab</p>
<p>Xu Chen xu.chen@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China</p>
<p>Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge
CBC84AC7E3830FE56CFF2072DD5E684D
LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments.However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations.This approach leads to information loss and decreases the reliability of evaluations.To address this limitation, we propose a novel training framework that explicitly aligns the LLMgenerated judgment distribution with empirical human distributions.Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process.Furthermore, considering that empirical distributions may derive from limited human annotations, we incorporate adversarial training to enhance model robustness against distribution perturbations.Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with improved alignment quality, evaluation accuracy, and robustness.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have demonstrated remarkable progress across various tasks, such as natural language understanding [1,2], reasoning [3][4][5], and evaluation [6,7].One of their most significant applications is for automatic judgment, which employs LLMs to evaluate specific targets based on predefined criteria or instructions [8,9].This LLM-as-a-Judge paradigm offers significant advantages in efficiency and flexibility due to its capability to efficiently handle large-scale data and adapt to diverse evaluation tasks.Therefore, using LLMs as judges has emerged as a promising alternative to conventional human evaluations [10].</p>
<p>Most previous works adopt single-point judgment with LLMs, which just outputs a single result for each sample [11][12][13].Although this paradigm is straightforward, it overlooks the inherent diversity of human evaluations.In real-world scenarios, human evaluations are rarely deterministic.Instead, they tend to follow a distribution with diverse perspectives and inherent uncertainties [14,15].Therefore, replacing this distributional human evaluations with a single-point LLM judgment may cause information loss [15], which limits the comprehensiveness and reliability of evaluations.</p>
<p>In order to empower LLM judgment with the diversity and uncertainty of human evaluations, an intuitive approach is to generate a judgment distribution based on LLMs.Although LLMs can inherently provide probability distributions over output tokens, previous studies have shown that they are often overconfident and skewed towards a few options [16].Besides, most current LLM training approaches focus on single-point alignment, aiming to maximize the probability of generating a specific correct or desired output [17,18], as illustrated in Figure 1(a).This focus inherently limits their ability to capture the diversity and uncertainty present in human evaluations, hindering effective † Corresponding author.</p>
<p>Preprint.Under review.(b) Distribution Alignment: by using this approach, the models are trained to produce judgment distributions that align with the human evaluation distributions.</p>
<p>distribution alignment.Therefore, it is necessary to design an explicit distributional alignment framework to align LLMs' output with human evaluation distributions, as illustrated in Figure 1(b).</p>
<p>To address the above challenges, we design a novel framework that explicitly aligns the output distributions of LLMs with human evaluation distributions.Specifically, we propose a distributional alignment objective that leverages the Kullback-Leibler (KL) divergence [19] to minimize the discrepancy between the model's predicted distribution and the empirical distribution derived from human annotations.Besides, we introduce a hybrid loss function that combines the primary KL divergence objective with an auxiliary cross-entropy loss to improve the training stability.It combines the distributional advantages of KL divergence and the stability of single-point alignment.To further mitigate the risk of overfitting caused by limited human annotations, we propose an adversarial training strategy to improve model robustness.Specifically, we apply the worst-case perturbation to the empirical distributions during optimization, encouraging the model to align with any plausible distribution within the bounded perturbation set.Our major contributions are presented as follows:</p>
<p>• Explicit Distribution Alignment Framework.We propose a novel framework to explicitly align the distribution of LLM judgment with human evaluation distributions, thereby effectively capturing the uncertainty and diversity inherent in human evaluations.</p>
<p>• Robust Distribution Alignment Methodology.By introducing an adversarial optimization strategy that leverages distribution perturbations during training, we significantly enhance the fidelity and robustness of model alignment with real human evaluation distributions.</p>
<p>• Extensive Experimental Validations.Experiments across diverse LLM backbones and evaluation tasks demonstrate that our approach consistently surpasses existing closed-source LLMs and substantially outperforms conventional single-point alignment methods in multiple aspects.</p>
<p>2 Related Work</p>
<p>LLM-as-a-Judge</p>
<p>LLMs are increasingly used as automated evaluators (i.e., LLM-as-a-Judge) [11-13, 20, 21] due to their efficiency, scalability, and generalization capabilities [8,9].Previous works typically utilize LLMs to produce a single-point deterministic evaluation, such as binary consistency judgments [20] and Likert-scale ratings [21].However, these approaches neglect the inherent variability observed in human evaluations, where humans often present diverse opinions, resulting in evaluation distributions [14].Collapsing this diversity into a single decision overlooks valuable information [15] such as disagreement, uncertainty, and subjectivity.To address this limitation, we generate probability distributions from LLMs for evaluations.We propose an explicit alignment method to better match the distributions generated by LLMs with the actual distributions provided by human annotators.</p>
<p>Distributional Reward Models</p>
<p>To model diverse human preferences, distributional reward models in Reinforcement Learning from Human Feedback (RLHF) [18] aim to output a distribution over reward values rather than a single scalar reward [22][23][24].Existing research in this area typically employs methods that model preference score distributions using mean and variance [22,23], or adopting quantile regression techniques to achieve finer-grained preference modeling [24].These approaches often infer reward distributions from human preferences indirectly and necessitate architectural modifications that may decrease the general capabilities of LLMs [25].Different from previous studies, our proposed approach directly leverages the explicit distributions derived from human evaluations, while it can also preserve the inherent language generation capabilities of LLMs.</p>
<p>Adversarial Training</p>
<p>Adversarial training can enhance model robustness by exposing models to worst-case perturbations in training phase, which has been widely adopted in various fields, such as computer vision [26,27] and natural language processing [28,29].It is often formulated as a min-max optimization problem with two adversarial stages.Specifically, the maximization stage identifies the worst-case perturbation, and the minimization stage updates the model parameters to minimize loss under these perturbations [30].This iterative procedure enables the model to learn more robust and reliable decision boundaries.</p>
<p>Common optimization algorithms employed in adversarial training include the Fast Gradient Sign Method (FGSM) [31] and Projected Gradient Descent (PGD) [32], both of which have shown strong stability and effectiveness.In this study, we adopt adversarial training to enhance the robustness of LLMs, in order to better align model predictions with human evaluation distributions.</p>
<p>Preliminary</p>
<p>Consider a dataset D, where each sample x ∈ D is annotated independently by N human annotators.Each annotator assigns labels from a discrete set of categories C = {1, 2, . . ., C}.We define the empirical distribution of human judgments (i.e., human evaluation distribution) for a given sample x as the vector p(x) ∈ R C , whose i-th component is given by:
p i (x) = 1 N N j=1 I(y j = i), ∀i ∈ C,(1)
where y j denotes the label from the j-th annotator for sample x, and I(•) is the indicator function.</p>
<p>Correspondingly, let θ denote the parameters of the LLM.For an input sample x, the model outputs a normalized probability distribution over the C categories.This distribution is represented by the vector q θ (x), where each component q θ,i (x) indicates the predicted probability for category i. Formally, the probability vector q θ (x) is defined as:
q θ (x) ∈ {q ∈ R C | q i ≥ 0, ∀i ∈ C, C i=1 q i = 1}.(2)
In classical evaluation settings [33,34], a single deterministic reference label r(x) ∈ {0, 1} C is often used, typically defined by selecting the most frequent human annotation:
r i (x) = 1, if i = arg max k p k (x), 0, otherwise.(3)
The primary objective of our method is to optimize the model parameters θ so that the predicted judgement distribution q θ (x) closely aligns with the human judgment distribution p(x).Our proposed explicit distributional alignment enables the model to better capture nuanced human judgments, thereby resulting in more informative and representative evaluations.</p>
<p>Methodology 4.1 Overview</p>
<p>To overcome the limitations of single-point judgments and better reflect the inherent diversity and uncertainty in human evaluations, we propose a novel training framework that explicitly aligns model-generated probability distributions with real-world human evaluation distributions.Given an input, we extract logits corresponding to the judgment token to obtain the predicted distribution.Our training involves two main steps, as demonstrated in Figure 2(a).First of all, we generate a worst-case perturbation around the empirical distribution to enhance robustness.Then, we compute the hybrid loss between the model prediction and the perturbed distribution, and update the model parameters accordingly.This approach mitigates the inherent limitation of empirical human distributions, which serve only as imperfect estimates of the true underlying distributions, as illustrated in Figure 2(b).</p>
<p>By aligning model predictions with all plausible distributions within the perturbation set, our method promotes more robust and faithful distributional alignment.</p>
<p>Human Distribution Alignment via Hybrid Loss</p>
<p>To achieve effective alignment between the model's output distribution and the human judgment distribution, we propose a hybrid loss function.This loss function combines KL divergence for distribution alignment with an auxiliary cross-entropy objective for training stability.First, to explicitly encourage distributional alignment, we introduce the KL divergence loss:
L KL (θ) = 1 |D| x∈D D KL (p(x) ∥ q θ (x)) ,(4)
where D KL (• ∥ •) denotes the KL divergence.This objective promotes a fine-grained alignment between the model's predicted distribution and the human evaluation distribution.Second, to improve training stability and guide learning with more direct supervision, we also include an auxiliary cross-entropy loss with respect to the deterministic label r(x):
L CE (θ) = 1 |D| x∈D CE(q θ (x), r(x)).(5)
This hybrid design mirrors the philosophy of knowledge distillation [35], where student models are trained using both soft targets (through KL divergence from a teacher model) and hard labels (via cross-entropy with ground truth).This approach leverages the rich informational content provided by the teacher's outputs and the direct guidance of true labels, improving both learning fidelity and convergence stability.Embracing this principle, our hybrid loss function blends these two objectives via a weighting factor α ∈ [0, 1]:
L Hybrid (θ) = α • L KL (θ) + (1 − α) • L CE (θ).(6)
This hybrid approach ensures stable training using cross-entropy while also achieving nuanced distributional alignment through KL divergence.As a result, it effectively captures both consensus and diversity in human annotations.</p>
<p>Robust Alignment via Adversarial Training</p>
<p>In practice, due to the limited number of human annotations, we only have empirical approximations of the human judgment distribution, as illustrated in Figure 2(b).Directly aligning the model output q θ (x) with these empirical approximations p(x) results in the model overfitting to sampling noise or artifacts, reducing the robustness of alignment with the true underlying distribution.</p>
<p>To address this challenge, we introduce adversarial training into our distribution alignment framework.Specifically, we define a perturbation set E around the empirical annotation distribution p(x) and identify the worst-case perturbed distribution p ′ (x) within this set.Aligning our model with this worstcase distribution ensures robustness against any plausible perturbation within E, thereby improving alignment with the human judgment distribution.This transformation converts our objective into a min-max optimization problem as follows:
θ * = arg min θ max p ′ (x)∈E [α • D KL (p ′ (x) ∥ q θ (x)) + (1 − α) • CE(q θ (x), r(x))] .(7)
This adversarial training process consists of two alternating steps:</p>
<ol>
<li>
<p>Adversarial Distribution Generation: For a fixed model parameter θ and each sample x in the batch, find the worst-case distribution p ′ (x) within the perturbation set E that maximizes the loss.</p>
</li>
<li>
<p>Model Update: Update model parameters θ to minimize the adversarially perturbed loss.</p>
</li>
</ol>
<p>Through this adversarial training procedure, we explicitly model the worst-case scenarios of the true underlying human judgment distribution, thereby ensuring robust and stable alignment.By accounting for potential annotation noise and sampling artifacts, the model becomes less sensitive to empirical inaccuracies, thus enhancing its generalization performance and practical applicability.</p>
<p>Implementing Adversarial Training via Projected Gradient Descent</p>
<p>To solve the inner maximization equation in Equation ( 7), we adopt Projected Gradient Descent (PGD) to iteratively search for the worst-case perturbation within a constrained space.Specifically, we seek an adversarial distribution p ′ (x) that maximizes the KL divergence from the model prediction q θ (x), while remaining close to the original human distribution p(x) and preserving the properties of a valid probability distribution.</p>
<p>We define the feasible perturbation set E as the intersection of two convex sets as
E = ∆ C ∩ B ϵ * (p(x)), where ∆ C = {p ′ ∈ R C | C i=1 p ′ i = 1, p ′ i ≥ 0}
is the C-dimensional probability simplex, and B ϵ * (p(x)) is an ℓ 2 ball of radius ϵ * centered at the original human distribution p(x).Based on the feasible perturbation set, we design the optimization procedure as follows.First of all, we initialize the unperturbed distribution as p (0) = p(x).Then, we conduct the gradient ascent by updating the current iterate in the direction of the gradient of the KL divergence as:
y (t+1) = p (t) + η • ∇ p (t) D KL (p (t) ∥ q θ (x)) ,(8)
where η denotes the step size controlling the gradient ascent magnitude.After that, we project the updated distribution back onto the feasible set E with the equation:
p (t+1) = Π E (y (t+1) ) = arg min p ′ ∈E ∥p ′ − y (t+1) ∥ 2 2 .(9)
This projection step is a convex Quadratically Constrained Quadratic Program (QCQP) [36], as it minimizes a convex quadratic objective over the intersection of two convex sets: the simplex and the ℓ 2 ball.Notably, the intersection E is guaranteed to be non-empty since the original distribution p(x) ∈ E by definition.Consequently, this projection problem is well-posed and can be efficiently solved using off-the-shelf convex optimization solvers such as CVXPY [37].</p>
<p>5 Experiments</p>
<p>Experiment Setup</p>
<p>Datasets.We select four representative datasets to cover diverse evaluation tasks [15], including Natural Language Inference (NLI), qualitative evaluation with Likert ratings, and human preference understanding.The selected datasets are introduced as follows:</p>
<p>• Natural Language Inference (SNLI [38]/MNLI [39]).These are classic benchmarks for the NLI task.Each instance consists of a pair of sentences, with five annotators judging their logical relationship (entailment, neutral, contradiction).We randomly sample an equal number of instances from MNLI (10,000 each) to maintain a comparable data scale with SNLI.</p>
<p>• Qualitative Assessment (SummEval [33]).This dataset originates from the CNN/DailyMail news summarization task and includes summaries generated by various automatic summarization models.Each summary is rated by experts and crowdsourced workers on a 1-5 Likert scale across four dimensions: fluency, coherence, consistency, and relevance.We treat each dimension as an independent evaluation instance, totaling 5,104 samples.• Human Preference Understanding (MT-Bench [40]).This dataset assesses the human performance of six LLMs on 80 open-ended multi-turn dialogue questions.For each model pair (A vs. B)'s responses, at least one human reviewer annotates their preference (A, Tie, B).</p>
<p>All datasets are split into training and test sets at an 8:2 ratio in our experiments.To facilitate the reproduction, we present the detailed prompts that are used in all the tasks in Appendix A.</p>
<p>Baselines.We compare our proposed approach against two baseline methods.</p>
<p>(1) Raw Model:</p>
<p>We directly evaluate the pretrained LLMs without any task-specific fine-tuning.This baseline aims to measure the inherent alignment between pretrained models and human judgment distributions, reflecting the model's original capability to approximate human judgment without explicit training or adjustment.</p>
<p>(2) Single-point Alignment: We adopt the traditional supervised fine-tuning strategy [18], using only the most frequent human annotation label as the supervision target.This baseline evaluates the effectiveness of conventional single-point alignment methods.</p>
<p>Models.</p>
<p>Our study evaluates both open-source (Qwen2.5-7B[41], LLaMA3.1-8B[42]) and closedsource (GPT-4o, GPT-4o-mini) language models.Open-source models are utilized without additional tuning, whereas the chosen closed-source models, recognized for their strong performance, are tested both with and without further training.</p>
<p>Training Details.We fine-tune all selected open-source models using a uniform hyperparameter configuration to ensure a fair comparison.Specifically, we employ the AdamW [43] optimizer with a learning rate of 5 × 10 −5 and train each model for 2 epochs.To enhance model robustness, we incorporate adversarial training, setting the perturbation step size to 0.05 and performing 5 gradient ascent steps per training iteration.Additionally, we conduct a hyperparameter search for two critical parameters: the weight parameter α, chosen from the set {0, 0.2, 0.4, 0.6, 0.8, 1.0}, and the perturbation radius parameter ϵ, selected from the set {0.0, 0.05, 0.1, 0.15, 0.2, 0.25}.We conduct all experiments on one NVIDIA A100-40G GPU.</p>
<p>Evaluation Metrics.We employ two primary metrics to measure the alignment between modelpredicted and human-annotated distributions:</p>
<p>(1) KL Divergence: This metric quantifies the discrepancy between the model's predicted distribution q θ (x) and the human distribution p(x).Lower KL divergence indicates closer alignment:
KL(p(x)||q θ (x)) = i p(x) log p(x) q θ (x) .(10)
(2) Accuracy: Accuracy measures whether the model's most probable predicted label aligns with the most frequent label in the human judgment distribution:
Accuracy = 1 |D| x∈D I arg max i∈C q θ,i (x) = arg max j∈C p j (x) .(11)
Here, |D| denotes the total number of test samples, q θ,i (x) represents the model-predicted probability for category i, and p j (x) denotes the human judgment distribution for category j.</p>
<p>Extracting Model Predictions.We extract model predictions by retrieving logits corresponding to the judgment token associated with potential judgment labels (e.g., contradiction, 1, 5).These logits are converted into probabilities via softmax normalization, and synonymous tokens are mapped to the standard label space.To maintain consistency across experiments, we limit the extraction to the top-5 logits because OpenAI's API restricts the number of logits returned.Besides, the logits beyond the fifth highest are generally negligible, with values often falling below 1e-6.</p>
<p>Overall Performance</p>
<p>We evaluate the effectiveness of our distribution alignment method across four benchmark datasets, including SNLI, MNLI, Summeval, and MT-Bench.The primary experimental results are presented in Table 1.Our findings highlight three key aspects as follows: (2) Superiority of Our Proposed Method.Compared to conventional single-point alignment methods, our approach consistently achieves better distribution alignment across different datasets and LLM backbones.It significantly reduces KL divergence while maintaining accuracy, demonstrating its generalization ability and effectiveness in aligning model outputs with human-labeled distributions.</p>
<p>(3) Correlation between Model Capability and Alignment Performance.We observe a positive correlation between a model's inherent capability and its alignment performance.More capable models, such as GPT-4o, not only achieve higher accuracy but also yield predicted distributions closer to human annotations than weaker models like GPT-4o-mini and Qwen2.5.This suggests that stronger models naturally produce judgments distributions more consistent with human evaluations.</p>
<p>In conclusion, our method demonstrates superior performance in distribution alignment compared to raw models and conventional single-point alignment approaches.It reduces KL divergence while maintaining accuracy across multiple datasets and various LLM backbones, presenting the effectiveness of our method in aligning model outputs with human judgment distributions.</p>
<p>Ablation Study</p>
<p>To better understand the contribution of each component in our method, we conduct an ablation study, whose results are summarized in Table 2.We observe that removing any single component consistently degrades alignment performance, resulting in increased KL divergence.This indicates that all three components complement each other and collectively enhance distributional alignment.According to the results, we find that KL divergence loss is the most crucial.Removing it leads to a significant increase in KL divergence, which confirms its essential role in human judgment alignment by penalizing deviations from the target distribution.Besides, adversarial training also contributes to improving the performance.By introducing perturbations during training, the model can align better with human distributions even under worst-case distributional shifts, thereby improving robustness and generalization.In addition, removing cross-entropy loss results in only a slight increase in KL divergence.Although its effect on distributional alignment is limited, extensive experiments in Section 5.4 demonstrate that a small amount of auxiliary CE loss can stabilize training.</p>
<p>Impact of Hyper-parameters</p>
<p>We further perform experiments on how the weighting parameter α and perturbation radius ϵ affect model alignment performance.Using Qwen2.5 as the base model, we evaluate its alignment performance across all four datasets.Lower KL divergence indicates better alignment between the model prediction and the human judgment distribution.The results are presented in Figure 3.</p>
<p>Impact of Weighting Parameter α.The weighting parameter α is responsible to balance balances KL divergence and CE losses, thereby affecting the performance of alignment.According to the results, we observe that increasing α from 0 to approximately 0.8 consistently enhances the alignment performance across all datasets.However, further increasing α from 0.8 to 1.0 causes a noticeable performance decline, indicating that KL divergence is more effective than cross-entropy in distribution alignment.However, a small proportion of cross-entropy loss proves beneficial by stabilizing training.</p>
<p>Impact of Perturbation Radius ϵ.We further explore the influence of the perturbation radius ϵ in our method.Each line in As the perturbation parameter ϵ increases, the alignment performance exhibits a general improvement.However, the performance gains gradually diminish with increasing perturbation magnitudes, and this trend is particularly evident on the MNLI dataset.</p>
<p>These results have verified our earlier statements: KL divergence serves as the primary mechanism for alignment, while incorporating a minor component of cross-entropy loss can further improve the training stability.It further underscores the importance of combining both components.Besides, the integration of moderate adversarial perturbations further boosts alignment performance by increasing</p>
<p>Robustness Analysis</p>
<p>To further analyze the robustness of our method, we conduct extensive experiments by adding random perturbations to the target label distributions in the test set.Specifically, our random perturbations δ are ranged from 0.00 to 0.25.The experiments are performed on all four datasets based on Qwen2.5, and we use the hyper-parameters identified in Section 5.4 with α = 0.8 and ϵ = 0.25.</p>
<p>As shown in Table 3, our full method consistently achieves the lowest KL divergence across all perturbation levels and datasets, outperforming both the single-point alignment baseline and the variant without adversarial training.These results suggest that incorporating adversarial training enables the model to effectively align with all plausible distributions within the perturbation set, thereby improving robustness and fidelity in distributional alignment.</p>
<p>Conclusion</p>
<p>In this paper, we propose a distribution alignment framework that explicitly aligns the model outputs with the human evaluation distribution, aiming for more nuanced evaluation results.Specifically, we employ KL divergence as the main objective to minimize the discrepancy between model predictions and target distributions.Furthermore, we introduce a hybrid loss function, incorporating an auxiliary cross-entropy loss to stabilize training.Finally, adversarial training is utilized to further enhance alignment performance by increasing the model's robustness against distributional shifts.Experiments demonstrate that our distribution alignment method outperforms existing single-point alignment approaches and exhibits strong generalization and robustness across different models and datasets.</p>
<p>Limitations</p>
<p>Although our approach can effectively align model outputs with human judgment distributions, it exhibits two notable limitations.First, the model's explainability is limited, as the generated explanations only correspond to a single sampled judgment rather than interpreting the entire predicted distribution.Second, suitable training datasets remain scarce due to high human annotation costs.Most existing datasets contain only a single annotation per instance.Therefore, improving model alignment across diverse tasks requires constructing more datasets with richer human evaluation data.</p>
<p>In future work, we will further improve the explainability and efficiency of our proposed method.</p>
<p>A Prompts</p>
<p>Summeval.These prompts are reused from G-Eval [34] with slight modifications.Specifically, the output label region has been explicitly specified, and the model is instructed to directly output evaluation results without providing explanations.</p>
<p>Prompts Used for Summeval for Coherence Evaluation</p>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Coherence (1-5) -the collective quality of all sentences.We align this dimension with the DUC quality question of structure and coherence whereby "the summary should be well-structured and well-organized.The summary should not just be a heap of related information, but should build from sentence to a coherent body of information about a topic."</p>
<p>Evaluation Steps:</p>
<ol>
<li>
<p>Read the news article carefully and identify the main topic and key points.2. Read the summary and compare it to the news article.Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.</p>
</li>
<li>
<p>Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.</p>
</li>
</ol>
<p>Example:</p>
<p>Source Text: {Document} Summary: {Summary} Evaluation Form(scores ONLY): Make a selection from "1", "2", "3", "4", "5".Only write the answer with a single score, do not write reasons.</p>
<p>-Coherence:</p>
<p>Prompts Used for Summeval for Fluency Evaluation</p>
<p>You will be given one summary written for a news article.</p>
<p>Your task is to rate the summary on one metric.</p>
<p>Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.</p>
<p>Evaluation Criteria:</p>
<p>Fluency (1-5): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.</p>
<p>-1: Poor.The summary has many errors that make it hard to understand or sound unnatural.</p>
<p>-2: Below Average.The summary has several noticeable errors that significantly impact readability, though some parts can be understood with effort.</p>
<p>-3: Fair.The summary has some errors that affect the clarity or smoothness of the text, but the main points are still comprehensible.</p>
<p>-4: Good.The summary has minor errors that do not significantly interfere with understanding; it reads relatively smoothly.</p>
<p>-5: Excellent.The summary has few or no errors and is easy to read and follow, with natural-sounding language throughout.</p>
<p>Example:</p>
<p>Summary: {Summary} Evaluation Form(scores ONLY): Make a selection from "1", "2", "3", "4", "5".Only write the answer with a single score, do not write reasons.</p>
<p>-Fluency:</p>
<p>arXiv:2505.12301v1 [cs.AI] 18 May 2025 conversations and choose one of the following:</p>
<p>Figure 1 :
1
Figure 1: Comparison between single-point alignment and distribution alignment.(a) Single-point Alignment: in this method, LLMs are trained to generate outputs that exactly match the desired text.(b)Distribution Alignment: by using this approach, the models are trained to produce judgment distributions that align with the human evaluation distributions.</p>
<p>Figure 2 :
2
Figure 2: Overview of our proposed framework.(a) Training framework: We generate adversarial perturbations of the empirical human distribution and optimize the hybrid loss.(b) Motivation: Illustrates the relationship between the empirical, perturbed, and true underlying distributions.Robust alignment mitigates the deviation problem in the empirical human distribution.</p>
<p>Figure 3 :
3
Figure 3: Effect of weighting parameter α and perturbation radius ϵ on KL divergence across four datasets.Lower values indicate better alignment between model predictions and human distributions.According to the results, we find that KL divergence loss is the most crucial.Removing it leads to a significant increase in KL divergence, which confirms its essential role in human judgment alignment by penalizing deviations from the target distribution.Besides, adversarial training also contributes to improving the performance.By introducing perturbations during training, the model can align better with human distributions even under worst-case distributional shifts, thereby improving robustness and generalization.In addition, removing cross-entropy loss results in only a slight increase in KL divergence.Although its effect on distributional alignment is limited, extensive experiments in Section 5.4 demonstrate that a small amount of auxiliary CE loss can stabilize training.</p>
<p>Figure 3
3
corresponds to a specific value of perturbation radius, and we use the blue line (ϵ = 0) to represent training without adversarial perturbations.The results demonstrate that the method without adversarial training commonly performs the worst.It indicates that adversarial training can enhance the model's generalization capability, thereby improving alignment performance.</p>
<p>Table 1 :
1
Main results comparing raw models, single-point alignment, and our distribution alignment method across four datasets.KL indicates KL divergence, and Acc denotes top-1 accuracy.
SNLIMNLISummevalMT-BenchModelMethodKL↓Acc↑KL↓ Acc↑ KL↓Acc↑KL↓Acc↑GPT-4o-miniRaw model2.13 87.0% 1.88 84.9% 5.23 25.6% 5.63 62.3%GPT-4oRaw model1.75 85.5% 1.16 84.2% 2.82 35.2% 2.48 68.5%Raw model2.08 83.1% 1.77 83.5% 4.94 22.7% 3.36 62.0%Qwen2.5Single-point0.72 92.6% 0.74 89.2% 0.74 45.8% 0.83 63.0%Distribution (Ours) 0.31 93.0% 0.23 89.0% 0.52 46.6% 0.68 63.6%Raw model0.90 64.9% 0.67 70.5% 3.60 29.5% 1.58 53.4%LLaMA3.1Single-point0.75 91.7% 0.66 89.2% 0.59 45.9% 0.82 61.4%Distribution (Ours) 0.32 91.8% 0.26 89.2% 0.51 47.8% 0.72 63.6%</p>
<p>Table 2 :
2
Ablation study of our proposed method.We analyze the contribution of adversarial training (Adv), KL divergence loss (KL), and cross-entropy loss (CE) on MNLI and Summeval datasets.
ComponentsQwen2.5LLaMA3.1MethodMNLISummevalMNLISummevalAdv KL CEKL↓Acc↑KL↓Acc↑KL↓ Acc↑ KL↓ Acc↑Raw Model---1.77 83.5% 4.94 22.7% 0.67 70.5% 3.60 29.5%Single-point--✓ 0.74 89.2% 0.74 45.8% 0.66 89.2% 0.59 45.9%Ours (Full)✓✓✓ 0.23 89.0% 0.52 46.6% 0.26 89.2% 0.51 47.8%Ours w/o Adv-✓✓ 0.25 89.0% 0.64 46.6% 0.32 89.6% 0.62 45.9%Ours w/o KL✓-✓ 0.78 88.4% 0.75 45.8% 0.65 89.2% 0.65 45.4%Ours w/o CE✓✓-0.23 89.0% 0.54 46.0% 0.33 88.7% 0.58 48.0%(1) Necessity of Distribution Alignment. Without specific alignment training, both open-sourceand closed-source models demonstrate substantial divergence between their predictions and humanjudgment, with KL divergence typically exceeding 2.0. This indicates that current models inherentlyproduce judgment distributions that are misaligned with human evaluations, highlighting the necessityfor additional training of distribution alignment.</p>
<p>Table 3 :
3
The robustness analysis of our distribution alignment method under varying label perturbation levels (δ).The KL divergences are reported across different datasets and models, with lower KL divergence indicating better performance in distribution alignment.
KL Divergence at different perturbation levels (δ)DatasetMethodδ = 0.00 δ = 0.05 δ = 0.10 δ = 0.15 δ = 0.20 δ = 0.25Single-point0.7150.7170.7080.6920.7200.709SNLIOurs w/o Adv0.3340.3360.3330.3270.3440.346Ours (Full)0.3240.3250.3230.3170.3350.336Single-point0.7420.7440.7430.7450.7530.757MNLIOurs w/o Adv0.2710.2720.2720.2760.2830.293Ours (Full)0.2430.2450.2450.2510.2560.264Single-point0.7430.7480.7520.7780.8090.836SummevalOurs w/o Adv0.6390.6430.6490.6690.7070.735Ours (Full)0.5250.5290.5390.5650.5880.612Single-point0.8330.8330.8370.8320.8360.848MT-BenchOurs w/o Adv0.8310.8300.8340.8290.8320.846Ours (Full)0.6750.6760.6780.6750.6770.689
the model's robustness against the shifts in real-world human evaluation distributions.rmance by increasing the model's robustness against distributional shifts.</p>
<p>Prompts Used for Summeval for Consistency EvaluationYou will be given a news article.You will then be given one summary written for this article.Your task is to rate the summary on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Evaluation Criteria:Consistency(1)(2)(3)(4)(5)-the factual alignment between the summary and the summarized source.A factually consistent summary contains only statements that are entailed by the source document.Annotators were also asked to penalize summaries that contained hallucinated facts.Evaluation Steps:1. Read the news article carefully and identify the main facts and details it presents.2. Read the summary and compare it to the article.Check if the summary contains any factual errors that are not supported by the article.3. Assign a score for consistency based on the Evaluation Criteria.Example:Source Text: {Document} Summary: {Summary} Evaluation Form(scores ONLY): Make a selection from "1", "2", "3", "4", "5".Only write the answer with a single score, do not write reasons.-Consistency: Prompts Used for Summeval for Relevance Evaluation You will be given one summary written for a news article.Your task is to rate the summary on one metric.Please make sure you read and understand these instructions carefully.Please keep this document open while reviewing, and refer to it as needed.Evaluation Criteria:Relevance (1-5) -selection of important content from the source.The summary should include only important information from the source document.Annotators were instructed to penalize summaries which contained redundancies and excess information.Evaluation Steps:1. Read the summary and the source document carefully.2. Compare the summary to the source document and identify the main points of the article.3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.4. Assign a relevance score from 1 to 5.Example:Source Text: {Document} Summary: {Summary} Evaluation Form(scores ONLY): Make a selection from "1", "2", "3", "4", "5".Only write the answer with a single score, do not write reasons.-Relevance: MT-Bench.These prompts are reused from MT-Bench[40]with slight modifications.Prompts Used for MT-BenchHuman: For this task, you will be shown two conversations between a user and an AI assistant, labeled A and B. Your goal is to evaluate which response (A or B) better follows the user's instructions and more helpfully answers their question.Response length should not unduly influence your decision.Make a selection from "a", "b", "tie".Only write the answer with a single word, do not write reasons.</Instructions> </PrefJudgment> Assistant: NLI Tasks.For SNLI and MNLI datasets, prompts are reused from LogiEval[44]with slight modifications.Prompts Used for NLI Tasks You will be given a premise and a hypothesis.Your task is to determine whether the hypothesis logically follows from the premise.Choose only one of the following labels and output your answer with ONLY the label (one word), ensuring there are no spaces or other characters in the answer.Possible labels: entailment: The hypothesis follows logically from the information contained in the premise.neutral: It is not possible to determine whether the hypothesis is true or false without further information.contradiction: The hypothesis is logically false from the information contained in the premise.Read the following premise and hypothesis thoroughly and select the correct answer from the three answer labels.Premise: {premise}Hypothesis: {hypothesis} Make a selection from "entailment", "neutral", "contradiction".Only write the answer with a single word, do not write reasons.B Ethical ConsiderationOur work explores the alignment of LLM-generated evaluation distributions with human judgment distributions, aiming to enhance the accuracy, diversity, and robustness of automatic evaluations.This has positive societal implications by potentially reducing the reliance on costly and time-consuming human evaluations, enabling scalable and fairer assessments in applications such as education, content moderation, and peer review.However, automated judgment systems also carry inherent risks.Misaligned or overconfident evaluations may lead to biased decisions, potentially reflecting and amplifying biases subtly present within the human data used for the alignment process itself.Such outcomes are particularly detrimental in high-stakes or subjective domains where fairness is paramount and the nuanced complexities of human judgment are not easily replicated or may be overlooked by automated systems.
Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Yifan Dong, Chen Du, Yushuo Yang, Zhipeng Chen, Jinhao Chen, Ruiyang Jiang, Yifan Ren, Xinyu Li, Zikang Tang, Peiyu Liu, Jian-Yun Liu, Ji-Rong Nie, Wen, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc202033Ilya Sutskever, and Dario Amodei</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022ISBN 9781713871088</p>
<p>Reflexion: language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Reasoning with large language models, a survey. Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki Van Stein, Thomas Back, 2024</p>
<p>Evaluating and improving tool-augmented computation-intensive math reasoning. Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, Yiqun Liu, Llms-as-judges: A comprehensive survey on llm-based evaluation methods. 2024</p>
<p>A survey on llm-as-a-judge. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni, Jian Guo, 2025</p>
<p>Lima: less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>Flask: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Hyeongbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, ICLR 2024. International Conference on Learning Representations (ICLR). 2024</p>
<p>Judgelm: Fine-tuned large language models are scalable judges. Lianghui Zhu, Xinggang Wang, Xinlong Wang, 2025</p>
<p>Wildbench: Benchmarking llms with challenging tasks from real users in the wild. Yuntian Bill Yuchen Lin, Khyathi Deng, Abhilasha Chandu, Valentina Ravichander, Nouha Pyatkin, Ronan Dziri, Yejin Le Bras, Choi, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. Amos Tversky, Daniel Kahneman, science. 18541571974</p>
<p>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge. Aparna Elangovan, Lei Xu, Jongwoo Ko, Mahsa Elyasi, Ling Liu, Sravan Babu Bodapati, Dan Roth, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Towards measuring the representation of subjective global opinions in language models. Durmus Esin, Karina Nguyen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, First Conference on Language Modeling. 2024</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>On information and sufficiency. The annals of mathematical statistics. Solomon Kullback, Richard A Leibler, 195122</p>
<p>Chatgpt as a factual inconsistency evaluator for text summarization. Zheheng Luo, Qianqian Xie, Sophia Ananiadou, 2023</p>
<p>Human-like summarization evaluation with chatgpt. Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, Xiaojun Wan, 2023</p>
<p>Distributional preference learning: Understanding and accounting for hidden context in RLHF. Anand Siththaranjan, Cassidy Laidlaw, Dylan Hadfield-Menell, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Aligning crowd feedback via distributional preference reward modeling. Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin, Ruiming Deik, Yong Tang, Liu, ICML 2024 Workshop on Models of Human Feedback for AI Alignment. 2024</p>
<p>Quantile regression for distributional reward models in rlhf. Nicolai Dorka, arXiv:2409.101642024arXiv preprint</p>
<p>Improving llm-as-a-judge inference with the judgment distribution. Victor Wang, Michael Jq Zhang, Eunsol Choi, arXiv:2503.030642025arXiv preprint</p>
<p>Frequency domain adversarial training for robust volumetric medical segmentation. Asif Hanif, Muzammal Naseer, Salman Khan, Mubarak Shah, Fahad Shahbaz Khan, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2023</p>
<p>Self-adaptive adversarial training for robust medical segmentation. Fu Wang, Zeyu Fu, Yanghao Zhang, Wenjie Ruan, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2023</p>
<p>Adversarial training methods for semisupervised text classification. Takeru Miyato, Andrew M Dai, Ian Goodfellow, International Conference on Learning Representations. 2017</p>
<p>Adversarial examples for evaluating reading comprehension systems. Robin Jia, Percy Liang, 10.18653/v1/D17-1215Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSeptember 2017</p>
<p>A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies. Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, Xu-Yao Zhang, 10.1016/j.patcog.2022.108889.URLhttps://www.sciencedirect.com/science/article/pii/S0031320322003703Pattern Recognition. 0031-32031311088892022</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, arXiv:1412.65722014arXiv preprint</p>
<p>Towards deep learning models resistant to adversarial attacks. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu, International Conference on Learning Representations. 2018</p>
<p>Summeval: Re-evaluating summarization evaluation. Wojciech Alexander R Fabbri, Bryan Kryściński, Caiming Mccann, Richard Xiong, Dragomir Socher, Radev, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Geval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsDecember 2023Singapore</p>
<p>Distilling the knowledge in a neural network. Hinton, Deep Learning and Representation Learning Workshop in Conjunction with NIPS. 2014</p>
<p>Quadratic minimisation problems in statistics. C J Albers, F Critchley, J C Gower, 10.1016/j.jmva.2009.12J. Multivar. Anal. 0047-259X1023March 2011</p>
<p>10.1016/j.jmva.2009.12.018URL. </p>
<p>CVXPY: A Python-embedded modeling language for convex optimization. Steven Diamond, Stephen Boyd, Journal of Machine Learning Research. 17832016</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, 10.18653/v1/D15-1075Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSeptember 2015</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel Bowman, 10.18653/v1/N18-1101Proceedings of the 2018 Conference of the North American Chapter. Long Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20181</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>An Yang, Baosong Yang, Beichen Zhang, arXiv:2412.15115Qwen2.5 technical report. 2024arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, 2024</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 2019</p>
<p>Evaluating the logical reasoning ability of chatgpt and gpt-4. Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
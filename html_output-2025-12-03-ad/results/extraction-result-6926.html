<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6926 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6926</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6926</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-247187974</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.00600v1.pdf" target="_blank">Dual Embodied-Symbolic Concept Representations for Deep Learning</a></p>
                <p><strong>Paper Abstract:</strong> Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs. Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space. Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space. The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing. As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning. To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching. Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration. We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6926.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6926.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual embodied-symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual embodied-symbolic concept representations (dual-level model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-level model proposing that conceptual knowledge is represented both as modality-specific embodied feature vectors (sensorimotor-derived) and as amodal, language-specific symbolic graphs/embeddings (word / knowledge-graph embeddings), which interact during conceptual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Dual embodied-symbolic concept representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid (embodied simulation + symbolic / relational vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Claims concepts have two linked functional representations: (1) embodied representations as modality-specific feature vectors (e.g., visual embeddings for images), and (2) symbolic representations as amodal concept/knowledge-graph embeddings (word vectors / KG embeddings). The two levels interact to drive processing, with linguistic shortcuts used when distributional information suffices and sensorimotor simulation used when more detailed representation is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains how people can use cheap linguistic distributional shortcuts for many tasks yet recruit sensorimotor simulation when precision/detail is required; supports tasks requiring grounding, few-shot transfer, image-text matching and bridging between perception and knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>review of behavioral and computational studies (cites behavioral category-production, neuroimaging/meta-analytic evidence for modality-specific and amodal hubs, and computational modeling work integrating embeddings and KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Descriptive synthesis; uses evidence from category production tasks, neuroimaging/meta-analytic comparisons of modality-specific vs amodal hub activations, and computational model applications (knowledge distillation, image-text matching, scene-graph/CSKG bridging).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Synthesis concludes that both embodied (feature-vector) and symbolic (graph/word-vector) representations are inherent to human conceptual processing and that dual representations support better performance in computational tasks (e.g., few-shot CIL, image–text matching) when used jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Paper notes abstract concepts are challenging for purely embodied accounts and that symbolic/distributional shortcuts can dominate behavior in many tasks; no direct contradictory empirical refutation cited against dual-level hybrid view in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Daniel T. Chang (paper provided)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6926.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual Coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Coding of Knowledge in the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework mapping sensorimotor-derived representations to embodied level and language-derived representations to symbolic level, supporting the idea of two complementary representational formats for concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dual Coding of Knowledge in the Human Brain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Dual-coding framework</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid (sensorimotor simulation + linguistic/amodal coding)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Posits that conceptual knowledge is stored in two complementary systems: one grounded in sensorimotor/perceptual systems (embodied representations) and another in linguistic/amodal systems (symbolic representations), which together form concept meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for how perceptual experience and language jointly contribute to concept representation and retrieval; explains facilitation when both codes are available.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>literature review / theoretical synthesis (Neuroscience & cognitive studies cited)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Meta-analytic and literature-based synthesis across behavioral and neuroimaging studies (as cited in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Supports the existence of both embodied and language-derived representations and their interaction in conceptual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Not discussed in depth in this paper; other studies show distributional (linguistic) information can sometimes fully predict behavior (linguistic shortcut), indicating variable reliance on the two codes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Y. Bi, 2021</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6926.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linguistic shortcut hypothesis (as applied to conceptual processing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesis that people will preferentially use computationally cheaper linguistic distributional information (word co-occurrence) when it suffices for the task, resorting to sensorimotor simulation only when finer-grained representation is required.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Linguistic shortcut hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>processing-level heuristic / representational strategy (symbolic vs embodied use)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally, the mind selects between using distributional linguistic representations (cheap, amodal) and embodied sensorimotor simulations (costly, detailed) depending on task demands; the two formats coexist but are used adaptively.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains empirical patterns in category production (which exemplars are named first/more frequently), and predicts when linguistic vs sensorimotor information will drive responses.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment (category production study)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Category production task (participants list members of a named category); predictors: 11-dimensional sensorimotor similarity and corpus-derived linguistic proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Both sensorimotor similarity and linguistic proximity predict order/frequency in category production, with linguistic proximity having an effect above and beyond sensorimotor similarity — supporting the linguistic shortcut idea.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Sensorimotor similarity also predicts category production, indicating linguistic shortcuts are not exclusive and both information types interact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>B. Banks, C. Wingfield, L. Connell, 2021</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6926.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid semantic hub model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid models combining modality-specific, multimodal systems and amodal semantic hub areas</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Accounts proposing that modality-specific cortical regions represent features, multimodal regions integrate features, and amodal semantic hubs represent overarching supramodal conceptual information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Varieties of Abstract Concepts and Their Grounding in Perception or Action</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Hybrid modality-specific + amodal semantic hub model</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational network / hub-and-spoke hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally, conceptual content is distributed across modality-specific feature systems (the 'spokes'), multimodal integration zones, and amodal hub regions that encode supramodal conceptual structure; hybrid organization explains both perceptual grounding and abstract concept representation.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains neuroimaging findings showing both modality-specific activations and amodal hub responses; accounts for representation of both concrete and abstract concepts through different mixes of modality-specific and language/social/emotional systems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>neuroimaging and literature review (empirical neurocognitive studies synthesized)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Neuroimaging studies (fMRI/MEG) and theoretical review comparing activations across tasks and concept types; also behavioral comparisons across concept types.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Available evidence best fits hybrid models combining modality-specific and multimodal circuits with amodal conceptual hubs; abstract concepts recruit additional systems (emotional, introspective, social, linguistic).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Challenges remain for purely embodied theories to account for abstract concepts; hybrid models attempt to accommodate this but precise mapping of hub functions is ongoing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>M. Kiefer, M. Harpaintner, 2020</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6926.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributional linguistic model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributional linguistic (word co-occurrence) model / word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Models that derive word meaning from patterns of word co-occurrence in large corpora (e.g., Word2Vec, GloVe), producing dense semantic vectors where proximity encodes distributional similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual and Affective Multimodal Models of Word Meaning in Language and Mind</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributional semantics / word-embedding models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector (high-dimensional distributional space)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally represents words/concepts as vectors in a high-dimensional space learned from corpora where semantic similarity is captured by geometric proximity; can be used as amodal symbolic representations of concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for many semantic similarity and association effects, supports language-driven conceptual predictions and can serve as a cheap representational code for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral model-comparison study (computational simulation compared to human behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Model comparison against human behavioral data for representation of concrete and abstract concepts (multimodal vs unimodal models compared).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Unimodal distributional models capture some aspects of meaning, but multimodal models that add visual/affective information better capture human representations, especially for basic-level concepts within a superordinate category.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Purely corpus-derived word embeddings fail to capture some semantic relational structure and perceptual/affective dimensions; multimodal augmentation improves fit to human data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>S. D. Deyne, D. Navarro, G. Collell, A. Perfors, 2021</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6926.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Knowledge Graph Embedding (KGE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge graph embedding approaches (TransE, RESCAL, ConvE, GCN-based KGE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that embed nodes (concepts) and relations of a knowledge graph into low-dimensional continuous vector spaces to capture relational semantics; families include translation-based, semantic-matching, CNN-based and GCN-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Graph Embeddings and Explainable AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Knowledge graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational network / low-dimensional vector embeddings for graph structure</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally represents relational conceptual knowledge as vectors for concepts and relations learned to preserve KG triple structure (e.g., TransE enforces h + r ≈ t), and more advanced models capture neighborhood, path, type, and substructure information (GNN/GCN).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Provides precise structural representation of symbolic conceptual knowledge enabling link prediction, reasoning, and integration with modality-specific embeddings for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational model development and evaluation (benchmarks on KG tasks; referenced surveys and empirical ML papers)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Graph embedding training and evaluation on KG tasks (link prediction, entity/relation classification) and downstream tasks; comparisons of model families on benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Translation models capture simple relational regularities; semantic-matching and NN-based models (ConvE, GCNs) better capture deeper semantics and can incorporate node/edge attributes for improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Translation-based models and simple triple-only approaches neglect intrinsic higher-order associations and richer graph structure; GNN-based approaches improve but require more information and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Federico Bianchi et al., 2020</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6926.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Neural Bridging (GB-Net)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Bridging Network for scene graph ⇄ commonsense KG integration (GB-Net)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that treats scene graphs (SGs) as image-conditioned embodiments of commonsense knowledge graphs (CSKGs) and iteratively builds bridges between SG and CSKG embeddings via message passing/GNN to enrich scene-graph generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging Knowledge Graphs to Generate Scene Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Scene-graph ⇄ CSKG bridging via GNN (GB-Net)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>relational network / graph-structured grounded representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Representationally casts SGs and CSKGs as special KGs and learns interconnected embeddings via dynamic message passing and bridge-edge inference so that scene concepts and relations link to their commonsense counterparts.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>By grounding scene-graph nodes/edges to CSKG counterparts, the method enriches visual concept representations with commonsense knowledge, improving scene-graph generation and downstream visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational experiment / method paper (GNN-based model evaluated on scene graph generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Graph neural network training with iterative message passing and bridge inference; evaluation on scene graph generation benchmarks comparing enriched vs baseline models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Formulating SG generation as a graph-bridging inference problem and fusing SG and CSKG embeddings via GNN increases the connectivity between visual and symbolic knowledge and improves SG embedding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Current work considers primarily co-occurrence bridging and may not yet fully model all relation types or long-tail concepts; scalability and reliance on CSKG coverage are practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>A. Zareian, S. Karaman, S.-F. Chang, 2020</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6926.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Embodied-symbolic fusion (image-text matching)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied-symbolic fused representation for image-text matching (Scene Concept Graph expansion + image-concept fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational approach that augments visual embeddings with expanded concept embeddings derived from aggregated scene graphs (SCG) to reduce the semantic gap between images and text for image-text retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge Aware Semantic Concept Expansion for Image-Text Matching</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Embodied-symbolic fused representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid (visual feature vectors fused with symbolic concept vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally, images are represented as fused vectors combining embodied visual embeddings and symbolic concept/semantic embeddings (detected concepts expanded via an SCG), while text is represented via word embeddings; similarity is computed between these fused representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Fusing symbolic concept embeddings (expanded via SCG) with visual embeddings captures higher-level semantics and commonsense co-occurrence relations, improving image–text matching and retrieval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational experiment / method paper (image-text retrieval benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Image-text matching formulated as a ranking problem; model jointly learns image-concept fused embeddings and text embeddings with a combined loss (matching loss + concept prediction loss); evaluated on retrieval metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Concept-enhanced image embeddings that combine visual and expanded symbolic concept embeddings reduce semantic gap and improve matching accuracy compared to visual-only embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>SCG expansion in the cited work only accounts for co-occurrence and may miss richer relation types; dependency on accurate concept detection can limit gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>B. Shi, L. Ji, P. Lu, Z. Niu, N. Duan, 2019</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6926.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal KG (IKRL/DKRL/MMKG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Knowledge Graphs and multimodal KG embedding approaches (IKRL, DKRL, MMKG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that ground KG symbols with multimodal data (images, text descriptions, audio, video) by adding modality-specific embeddings (e.g., image embeddings, description encodings) to KG embeddings to achieve symbol grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-Modal Knowledge Graph Construction and Application: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Multimodal knowledge graph embedding / symbol grounding in KGs</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>hybrid (KG embedding + modality-specific embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally represents concepts/relations as KG vectors augmented or paired with modality-specific vectors (image encodings, textual descriptions), enabling triples like (s, r, d) where d is multimodal data (e.g., 'hasImage') to ground symbols in perceptual data.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Grounding KG nodes/relations in multimodal perceptual data improves representativeness and discriminability of concept representations and supports multimodal retrieval, reasoning and improved KG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>computational methods and survey of empirical systems (IKRL, DKRL examples and survey evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Training combined KG + modality encoders (CNNs for images, CNN/RNN for descriptions) and evaluating on KG completion/link prediction and multimodal retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Models that incorporate multimodal information (IKRL, DKRL) produce richer concept embeddings and improve KG task performance relative to structure-only embeddings; symbol grounding enhances cross-modal mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Selecting representative/discriminative multimodal exemplars and scaling to wide-coverage KGs remain challenges; integration strategies vary in effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Xiangru Zhu et al., 2022</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6926.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6926.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Category production task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Category production (behavioral paradigm for concept structure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A behavioral experimental paradigm where participants are given a category name and asked to list member concepts; used to probe how conceptual knowledge is organized and accessed from long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Category production paradigm (evidence-gathering task)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>behavioral experimental paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Functionally tests which representational formats (sensorimotor similarity vs linguistic distributional proximity) predict order and frequency of produced category members, revealing the relative contributions of embodied and symbolic information.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>If linguistic proximity predicts production above sensorimotor similarity, distributional representations play an independent role; if sensorimotor measures predict production, embodied features are important.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>behavioral experiment with computational predictors (sensorimotor 11-dim measures and corpus-derived linguistic proximity)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Participants name members of a presented category; analyses predict order and frequency using sensorimotor similarity and word co-occurrence-based linguistic proximity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Both sensorimotor and linguistic distributional measures predict production order/frequency, with linguistic proximity exerting additional predictive power beyond sensorimotor similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Neither modality alone accounts for all variance—both contribute and interact; results support hybrid accounts rather than pure embodied-only or symbolic-only accounts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>B. Banks, C. Wingfield, L. Connell, 2021</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Dual Embodied-Symbolic Concept Representations for Deep Learning', 'publication_date_yy_mm': '2022-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dual Coding of Knowledge in the Human Brain <em>(Rating: 2)</em></li>
                <li>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production <em>(Rating: 2)</em></li>
                <li>Varieties of Abstract Concepts and Their Grounding in Perception or Action <em>(Rating: 2)</em></li>
                <li>Visual and Affective Multimodal Models of Word Meaning in Language and Mind <em>(Rating: 2)</em></li>
                <li>Knowledge Graph Embeddings and Explainable AI <em>(Rating: 2)</em></li>
                <li>Bridging Knowledge Graphs to Generate Scene Graphs <em>(Rating: 2)</em></li>
                <li>Multi-Modal Knowledge Graph Construction and Application: A Survey <em>(Rating: 2)</em></li>
                <li>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning <em>(Rating: 1)</em></li>
                <li>Knowledge Aware Semantic Concept Expansion for Image-Text Matching <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6926",
    "paper_id": "paper-247187974",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "Dual embodied-symbolic",
            "name_full": "Dual embodied-symbolic concept representations (dual-level model)",
            "brief_description": "A dual-level model proposing that conceptual knowledge is represented both as modality-specific embodied feature vectors (sensorimotor-derived) and as amodal, language-specific symbolic graphs/embeddings (word / knowledge-graph embeddings), which interact during conceptual processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_name": "Dual embodied-symbolic concept representation",
            "theory_type": "hybrid (embodied simulation + symbolic / relational vectors)",
            "theory_description": "Claims concepts have two linked functional representations: (1) embodied representations as modality-specific feature vectors (e.g., visual embeddings for images), and (2) symbolic representations as amodal concept/knowledge-graph embeddings (word vectors / KG embeddings). The two levels interact to drive processing, with linguistic shortcuts used when distributional information suffices and sensorimotor simulation used when more detailed representation is needed.",
            "functional_claims": "Explains how people can use cheap linguistic distributional shortcuts for many tasks yet recruit sensorimotor simulation when precision/detail is required; supports tasks requiring grounding, few-shot transfer, image-text matching and bridging between perception and knowledge.",
            "evidence_source": "review of behavioral and computational studies (cites behavioral category-production, neuroimaging/meta-analytic evidence for modality-specific and amodal hubs, and computational modeling work integrating embeddings and KGs)",
            "experimental_paradigm": "Descriptive synthesis; uses evidence from category production tasks, neuroimaging/meta-analytic comparisons of modality-specific vs amodal hub activations, and computational model applications (knowledge distillation, image-text matching, scene-graph/CSKG bridging).",
            "key_result": "Synthesis concludes that both embodied (feature-vector) and symbolic (graph/word-vector) representations are inherent to human conceptual processing and that dual representations support better performance in computational tasks (e.g., few-shot CIL, image–text matching) when used jointly.",
            "supports_theory": true,
            "counter_evidence": "Paper notes abstract concepts are challenging for purely embodied accounts and that symbolic/distributional shortcuts can dominate behavior in many tasks; no direct contradictory empirical refutation cited against dual-level hybrid view in the paper.",
            "citation": "Daniel T. Chang (paper provided)",
            "uuid": "e6926.0",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Dual Coding",
            "name_full": "Dual Coding of Knowledge in the Human Brain",
            "brief_description": "A framework mapping sensorimotor-derived representations to embodied level and language-derived representations to symbolic level, supporting the idea of two complementary representational formats for concepts.",
            "citation_title": "Dual Coding of Knowledge in the Human Brain",
            "mention_or_use": "mention",
            "theory_name": "Dual-coding framework",
            "theory_type": "hybrid (sensorimotor simulation + linguistic/amodal coding)",
            "theory_description": "Posits that conceptual knowledge is stored in two complementary systems: one grounded in sensorimotor/perceptual systems (embodied representations) and another in linguistic/amodal systems (symbolic representations), which together form concept meaning.",
            "functional_claims": "Accounts for how perceptual experience and language jointly contribute to concept representation and retrieval; explains facilitation when both codes are available.",
            "evidence_source": "literature review / theoretical synthesis (Neuroscience & cognitive studies cited)",
            "experimental_paradigm": "Meta-analytic and literature-based synthesis across behavioral and neuroimaging studies (as cited in the paper).",
            "key_result": "Supports the existence of both embodied and language-derived representations and their interaction in conceptual processing.",
            "supports_theory": true,
            "counter_evidence": "Not discussed in depth in this paper; other studies show distributional (linguistic) information can sometimes fully predict behavior (linguistic shortcut), indicating variable reliance on the two codes.",
            "citation": "Y. Bi, 2021",
            "uuid": "e6926.1",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Linguistic shortcut hypothesis",
            "name_full": "Linguistic shortcut hypothesis (as applied to conceptual processing)",
            "brief_description": "Hypothesis that people will preferentially use computationally cheaper linguistic distributional information (word co-occurrence) when it suffices for the task, resorting to sensorimotor simulation only when finer-grained representation is required.",
            "citation_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "mention_or_use": "mention",
            "theory_name": "Linguistic shortcut hypothesis",
            "theory_type": "processing-level heuristic / representational strategy (symbolic vs embodied use)",
            "theory_description": "Functionally, the mind selects between using distributional linguistic representations (cheap, amodal) and embodied sensorimotor simulations (costly, detailed) depending on task demands; the two formats coexist but are used adaptively.",
            "functional_claims": "Explains empirical patterns in category production (which exemplars are named first/more frequently), and predicts when linguistic vs sensorimotor information will drive responses.",
            "evidence_source": "behavioral experiment (category production study)",
            "experimental_paradigm": "Category production task (participants list members of a named category); predictors: 11-dimensional sensorimotor similarity and corpus-derived linguistic proximity.",
            "key_result": "Both sensorimotor similarity and linguistic proximity predict order/frequency in category production, with linguistic proximity having an effect above and beyond sensorimotor similarity — supporting the linguistic shortcut idea.",
            "supports_theory": true,
            "counter_evidence": "Sensorimotor similarity also predicts category production, indicating linguistic shortcuts are not exclusive and both information types interact.",
            "citation": "B. Banks, C. Wingfield, L. Connell, 2021",
            "uuid": "e6926.2",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Hybrid semantic hub model",
            "name_full": "Hybrid models combining modality-specific, multimodal systems and amodal semantic hub areas",
            "brief_description": "Accounts proposing that modality-specific cortical regions represent features, multimodal regions integrate features, and amodal semantic hubs represent overarching supramodal conceptual information.",
            "citation_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "mention_or_use": "mention",
            "theory_name": "Hybrid modality-specific + amodal semantic hub model",
            "theory_type": "relational network / hub-and-spoke hybrid",
            "theory_description": "Functionally, conceptual content is distributed across modality-specific feature systems (the 'spokes'), multimodal integration zones, and amodal hub regions that encode supramodal conceptual structure; hybrid organization explains both perceptual grounding and abstract concept representation.",
            "functional_claims": "Explains neuroimaging findings showing both modality-specific activations and amodal hub responses; accounts for representation of both concrete and abstract concepts through different mixes of modality-specific and language/social/emotional systems.",
            "evidence_source": "neuroimaging and literature review (empirical neurocognitive studies synthesized)",
            "experimental_paradigm": "Neuroimaging studies (fMRI/MEG) and theoretical review comparing activations across tasks and concept types; also behavioral comparisons across concept types.",
            "key_result": "Available evidence best fits hybrid models combining modality-specific and multimodal circuits with amodal conceptual hubs; abstract concepts recruit additional systems (emotional, introspective, social, linguistic).",
            "supports_theory": true,
            "counter_evidence": "Challenges remain for purely embodied theories to account for abstract concepts; hybrid models attempt to accommodate this but precise mapping of hub functions is ongoing.",
            "citation": "M. Kiefer, M. Harpaintner, 2020",
            "uuid": "e6926.3",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Distributional linguistic model",
            "name_full": "Distributional linguistic (word co-occurrence) model / word embeddings",
            "brief_description": "Models that derive word meaning from patterns of word co-occurrence in large corpora (e.g., Word2Vec, GloVe), producing dense semantic vectors where proximity encodes distributional similarity.",
            "citation_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "mention_or_use": "mention",
            "theory_name": "Distributional semantics / word-embedding models",
            "theory_type": "feature-based vector (high-dimensional distributional space)",
            "theory_description": "Functionally represents words/concepts as vectors in a high-dimensional space learned from corpora where semantic similarity is captured by geometric proximity; can be used as amodal symbolic representations of concepts.",
            "functional_claims": "Accounts for many semantic similarity and association effects, supports language-driven conceptual predictions and can serve as a cheap representational code for many tasks.",
            "evidence_source": "behavioral model-comparison study (computational simulation compared to human behavior)",
            "experimental_paradigm": "Model comparison against human behavioral data for representation of concrete and abstract concepts (multimodal vs unimodal models compared).",
            "key_result": "Unimodal distributional models capture some aspects of meaning, but multimodal models that add visual/affective information better capture human representations, especially for basic-level concepts within a superordinate category.",
            "supports_theory": null,
            "counter_evidence": "Purely corpus-derived word embeddings fail to capture some semantic relational structure and perceptual/affective dimensions; multimodal augmentation improves fit to human data.",
            "citation": "S. D. Deyne, D. Navarro, G. Collell, A. Perfors, 2021",
            "uuid": "e6926.4",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Knowledge Graph Embedding (KGE)",
            "name_full": "Knowledge graph embedding approaches (TransE, RESCAL, ConvE, GCN-based KGE)",
            "brief_description": "Methods that embed nodes (concepts) and relations of a knowledge graph into low-dimensional continuous vector spaces to capture relational semantics; families include translation-based, semantic-matching, CNN-based and GCN-based approaches.",
            "citation_title": "Knowledge Graph Embeddings and Explainable AI",
            "mention_or_use": "mention",
            "theory_name": "Knowledge graph embedding",
            "theory_type": "relational network / low-dimensional vector embeddings for graph structure",
            "theory_description": "Functionally represents relational conceptual knowledge as vectors for concepts and relations learned to preserve KG triple structure (e.g., TransE enforces h + r ≈ t), and more advanced models capture neighborhood, path, type, and substructure information (GNN/GCN).",
            "functional_claims": "Provides precise structural representation of symbolic conceptual knowledge enabling link prediction, reasoning, and integration with modality-specific embeddings for grounding.",
            "evidence_source": "computational model development and evaluation (benchmarks on KG tasks; referenced surveys and empirical ML papers)",
            "experimental_paradigm": "Graph embedding training and evaluation on KG tasks (link prediction, entity/relation classification) and downstream tasks; comparisons of model families on benchmarks.",
            "key_result": "Translation models capture simple relational regularities; semantic-matching and NN-based models (ConvE, GCNs) better capture deeper semantics and can incorporate node/edge attributes for improved performance.",
            "supports_theory": true,
            "counter_evidence": "Translation-based models and simple triple-only approaches neglect intrinsic higher-order associations and richer graph structure; GNN-based approaches improve but require more information and computation.",
            "citation": "Federico Bianchi et al., 2020",
            "uuid": "e6926.5",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Graph Neural Bridging (GB-Net)",
            "name_full": "Graph Bridging Network for scene graph ⇄ commonsense KG integration (GB-Net)",
            "brief_description": "A method that treats scene graphs (SGs) as image-conditioned embodiments of commonsense knowledge graphs (CSKGs) and iteratively builds bridges between SG and CSKG embeddings via message passing/GNN to enrich scene-graph generation.",
            "citation_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "mention_or_use": "mention",
            "theory_name": "Scene-graph ⇄ CSKG bridging via GNN (GB-Net)",
            "theory_type": "relational network / graph-structured grounded representation",
            "theory_description": "Representationally casts SGs and CSKGs as special KGs and learns interconnected embeddings via dynamic message passing and bridge-edge inference so that scene concepts and relations link to their commonsense counterparts.",
            "functional_claims": "By grounding scene-graph nodes/edges to CSKG counterparts, the method enriches visual concept representations with commonsense knowledge, improving scene-graph generation and downstream visual reasoning.",
            "evidence_source": "computational experiment / method paper (GNN-based model evaluated on scene graph generation tasks)",
            "experimental_paradigm": "Graph neural network training with iterative message passing and bridge inference; evaluation on scene graph generation benchmarks comparing enriched vs baseline models.",
            "key_result": "Formulating SG generation as a graph-bridging inference problem and fusing SG and CSKG embeddings via GNN increases the connectivity between visual and symbolic knowledge and improves SG embedding quality.",
            "supports_theory": true,
            "counter_evidence": "Current work considers primarily co-occurrence bridging and may not yet fully model all relation types or long-tail concepts; scalability and reliance on CSKG coverage are practical limitations.",
            "citation": "A. Zareian, S. Karaman, S.-F. Chang, 2020",
            "uuid": "e6926.6",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Embodied-symbolic fusion (image-text matching)",
            "name_full": "Embodied-symbolic fused representation for image-text matching (Scene Concept Graph expansion + image-concept fusion)",
            "brief_description": "A computational approach that augments visual embeddings with expanded concept embeddings derived from aggregated scene graphs (SCG) to reduce the semantic gap between images and text for image-text retrieval.",
            "citation_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "mention_or_use": "mention",
            "theory_name": "Embodied-symbolic fused representation",
            "theory_type": "hybrid (visual feature vectors fused with symbolic concept vectors)",
            "theory_description": "Functionally, images are represented as fused vectors combining embodied visual embeddings and symbolic concept/semantic embeddings (detected concepts expanded via an SCG), while text is represented via word embeddings; similarity is computed between these fused representations.",
            "functional_claims": "Fusing symbolic concept embeddings (expanded via SCG) with visual embeddings captures higher-level semantics and commonsense co-occurrence relations, improving image–text matching and retrieval performance.",
            "evidence_source": "computational experiment / method paper (image-text retrieval benchmarks)",
            "experimental_paradigm": "Image-text matching formulated as a ranking problem; model jointly learns image-concept fused embeddings and text embeddings with a combined loss (matching loss + concept prediction loss); evaluated on retrieval metrics.",
            "key_result": "Concept-enhanced image embeddings that combine visual and expanded symbolic concept embeddings reduce semantic gap and improve matching accuracy compared to visual-only embeddings.",
            "supports_theory": true,
            "counter_evidence": "SCG expansion in the cited work only accounts for co-occurrence and may miss richer relation types; dependency on accurate concept detection can limit gains.",
            "citation": "B. Shi, L. Ji, P. Lu, Z. Niu, N. Duan, 2019",
            "uuid": "e6926.7",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Multimodal KG (IKRL/DKRL/MMKG)",
            "name_full": "Multimodal Knowledge Graphs and multimodal KG embedding approaches (IKRL, DKRL, MMKG)",
            "brief_description": "Approaches that ground KG symbols with multimodal data (images, text descriptions, audio, video) by adding modality-specific embeddings (e.g., image embeddings, description encodings) to KG embeddings to achieve symbol grounding.",
            "citation_title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
            "mention_or_use": "mention",
            "theory_name": "Multimodal knowledge graph embedding / symbol grounding in KGs",
            "theory_type": "hybrid (KG embedding + modality-specific embeddings)",
            "theory_description": "Functionally represents concepts/relations as KG vectors augmented or paired with modality-specific vectors (image encodings, textual descriptions), enabling triples like (s, r, d) where d is multimodal data (e.g., 'hasImage') to ground symbols in perceptual data.",
            "functional_claims": "Grounding KG nodes/relations in multimodal perceptual data improves representativeness and discriminability of concept representations and supports multimodal retrieval, reasoning and improved KG tasks.",
            "evidence_source": "computational methods and survey of empirical systems (IKRL, DKRL examples and survey evidence)",
            "experimental_paradigm": "Training combined KG + modality encoders (CNNs for images, CNN/RNN for descriptions) and evaluating on KG completion/link prediction and multimodal retrieval tasks.",
            "key_result": "Models that incorporate multimodal information (IKRL, DKRL) produce richer concept embeddings and improve KG task performance relative to structure-only embeddings; symbol grounding enhances cross-modal mapping.",
            "supports_theory": true,
            "counter_evidence": "Selecting representative/discriminative multimodal exemplars and scaling to wide-coverage KGs remain challenges; integration strategies vary in effectiveness.",
            "citation": "Xiangru Zhu et al., 2022",
            "uuid": "e6926.8",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        },
        {
            "name_short": "Category production task",
            "name_full": "Category production (behavioral paradigm for concept structure)",
            "brief_description": "A behavioral experimental paradigm where participants are given a category name and asked to list member concepts; used to probe how conceptual knowledge is organized and accessed from long-term memory.",
            "citation_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "mention_or_use": "mention",
            "theory_name": "Category production paradigm (evidence-gathering task)",
            "theory_type": "behavioral experimental paradigm",
            "theory_description": "Functionally tests which representational formats (sensorimotor similarity vs linguistic distributional proximity) predict order and frequency of produced category members, revealing the relative contributions of embodied and symbolic information.",
            "functional_claims": "If linguistic proximity predicts production above sensorimotor similarity, distributional representations play an independent role; if sensorimotor measures predict production, embodied features are important.",
            "evidence_source": "behavioral experiment with computational predictors (sensorimotor 11-dim measures and corpus-derived linguistic proximity)",
            "experimental_paradigm": "Participants name members of a presented category; analyses predict order and frequency using sensorimotor similarity and word co-occurrence-based linguistic proximity.",
            "key_result": "Both sensorimotor and linguistic distributional measures predict production order/frequency, with linguistic proximity exerting additional predictive power beyond sensorimotor similarity.",
            "supports_theory": true,
            "counter_evidence": "Neither modality alone accounts for all variance—both contribute and interact; results support hybrid accounts rather than pure embodied-only or symbolic-only accounts.",
            "citation": "B. Banks, C. Wingfield, L. Connell, 2021",
            "uuid": "e6926.9",
            "source_info": {
                "paper_title": "Dual Embodied-Symbolic Concept Representations for Deep Learning",
                "publication_date_yy_mm": "2022-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dual Coding of Knowledge in the Human Brain",
            "rating": 2,
            "sanitized_title": "dual_coding_of_knowledge_in_the_human_brain"
        },
        {
            "paper_title": "Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production",
            "rating": 2,
            "sanitized_title": "linguistic_distributional_knowledge_and_sensorimotor_grounding_both_contribute_to_semantic_category_production"
        },
        {
            "paper_title": "Varieties of Abstract Concepts and Their Grounding in Perception or Action",
            "rating": 2,
            "sanitized_title": "varieties_of_abstract_concepts_and_their_grounding_in_perception_or_action"
        },
        {
            "paper_title": "Visual and Affective Multimodal Models of Word Meaning in Language and Mind",
            "rating": 2,
            "sanitized_title": "visual_and_affective_multimodal_models_of_word_meaning_in_language_and_mind"
        },
        {
            "paper_title": "Knowledge Graph Embeddings and Explainable AI",
            "rating": 2,
            "sanitized_title": "knowledge_graph_embeddings_and_explainable_ai"
        },
        {
            "paper_title": "Bridging Knowledge Graphs to Generate Scene Graphs",
            "rating": 2,
            "sanitized_title": "bridging_knowledge_graphs_to_generate_scene_graphs"
        },
        {
            "paper_title": "Multi-Modal Knowledge Graph Construction and Application: A Survey",
            "rating": 2,
            "sanitized_title": "multimodal_knowledge_graph_construction_and_application_a_survey"
        },
        {
            "paper_title": "Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning",
            "rating": 1,
            "sanitized_title": "semanticaware_knowledge_distillation_for_fewshot_classincremental_learning"
        },
        {
            "paper_title": "Knowledge Aware Semantic Concept Expansion for Image-Text Matching",
            "rating": 1,
            "sanitized_title": "knowledge_aware_semantic_concept_expansion_for_imagetext_matching"
        }
    ],
    "cost": 0.0185565,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Dual Embodied-Symbolic Concept Representations for Deep Learning</p>
<p>Daniel T Chang dtchang43@gmail.com 
Dual Embodied-Symbolic Concept Representations for Deep Learning
C9D883FB721C375593C778749DBE936F
Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs.Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for few-shot class incremental learning, and embodied-symbolic fused representation for image-text matching.Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.</p>
<p>Introduction</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations [1][2]: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concepts in the form of concept graphs.For concrete concepts, the two levels are associated / connected.The embodied level corresponds to sensorimotor-derived knowledge representations of the dual-coding framework [4]; the symbolic level corresponds to language-derived knowledge representations of that framework.</p>
<p>Embodied concept representations are modality specific and exist in the form of feature vectors in a feature space.</p>
<p>Symbolic concept representations, on the other hand, are amodal and language specific, and exist in the form of word / knowledge-graph embeddings in a concept / knowledge space.Traditionally, they are learned separately and used independently.For example, deep learning for computer vision learns embodied concept representations in the form of visual feature vectors for image classification, whereas deep learning for natural language processing learns symbolic concept representations in the form of semantic word vectors for sentiment analysis.The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5][6][7].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system.Further, they typically interact to drive conceptual processing.</p>
<p>As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.To demonstrate their usage and value, we discuss two important use cases: embodied-symbolic knowledge distillation for fewshot class incremental learning (CIL) and embodied-symbolic fused representation for image-text matching.The first use case [16] demonstrates that embodied-symbolic knowledge distillation mitigates both the catastrophic forgetting problem for CIL and the overfitting problem for few-shot learning.The second use case [17] demonstrates that embodied-symbolic fused representation closes the semantic gap between images and text leading to improved performance when matching an image with text.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.We discuss two important examples of such integration: scene graph generation with knowledge graph bridging, and multimodal knowledge graphs.The first example [19] presents a unified formulation of scene graphs (SGs) and commonsense knowledge graphs (CSKGs), where a SG is seen as an image-conditioned embodiment of a CSKG.The second example [21] grounds symbols in knowledge graphs (KGs) to corresponding image, text, sound and video data and maps symbols to their corresponding referents with meanings in the physical world, which is a key step towards the realization of human-level AI.</p>
<p>Human Conceptual System: Symbolic and Embodied</p>
<p>The human conceptual system comprises both simulated information of sensorimotor experience (i.e., embodied representations) and linguistic distributional information of how words are used in language (i.e., symbolic representations) [5].Further, the linguistic shortcut hypothesis predicts that people will use computationally cheaper linguistic distributional information where it is sufficient for the task in question.However, people will resort to sensorimotor simulation to provide a more detailed, precise conceptual representation when required.Therefore, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing.</p>
<p>In the following we discuss three recent findings in cognitive science that demonstrate the crucial role of dual embodiedsymbolic concept representations in human cognition.</p>
<p>Category Production</p>
<p>A common way of testing how concepts are structured and accessed from long-term memory is with a category production task, whereby a participant is presented with a category name such as 'animal', and asked to name concepts belonging to that category.In a category production study [5], measures of sensorimotor similarity between a category and member concept (based on an 11-dimensional representation of sensorimotor strength) and linguistic proximity between the category name and member-concept name (based on word co-occurrence derived from a large corpus) are used to test category production performance.Both measures predict the order and frequency of category production, but linguistic proximity has an effect above and beyond sensorimotor similarity.</p>
<p>Sensorimotor and linguistic distributional information, therefore, are found to offer an explanation to the mechanisms driving responses in category production tasks [5].In terms of linguistic distributional information (i.e., symbolic information), it is evident that the shared linguistic contexts between member-concept names and category name (e.g., between cat / dog and 'animal') in corpus-derived linguistic space is an effective predictor of category membership.In terms of sensorimotor information (i.e., embodied information), as suggested by many theories of conceptual structure, categorical distinctions emerge from common features (e.g., fur, four-legged) of member concepts that we perceive when interacting with the world.Furthermore, sensorimotor and linguistic distributional information typically interact to drive conceptual processing, and the linguistic shortcut hypothesis is the process by which people arrive at the most-frequently-named and first-named member concepts of a category.</p>
<p>Grounding of Concrete and Abstract Concepts</p>
<p>Concrete concepts (e.g., animal) have perceivable referents.In general, their semantic content can be clearly characterized, and their conceptual taxonomies can be unequivocally defined.Evidence obtained with concrete concepts [6] suggests interplay between modality-specific, multimodal and amodal semantic hub regions.Modality-specific and multimodal regions represent conceptual features, whereas amodal semantic hubs code conceptual information in an overarching supramodal fashion.The available evidence is most consistent with hybrid models of conceptual representations combining modality-specific and multimodal circuits with amodal conceptual hubs, i.e., combining embodied and symbolic representations.</p>
<p>Abstract concepts (e.g., justice) have referents which cannot be directly perceived, like mental or emotional states, abstract ideas, social constellations and scientific theories.Their semantic content is highly variable across individuals and contexts.As such, abstract concepts are a particular challenge for embodied / grounded cognition theories [6] because at the first glance, it is hard to imagine how abstract concepts, without a referent which can be perceived or acted upon, could be grounded in the sensorimotor brain systems.Embodied / grounded cognition theories, however, have been refined [6] in order to account for the representation of abstract concepts.Besides sensorimotor information due to metaphoric mapping or due to relations to classes of situations, the relevance of emotional, introspective, social and linguistic information has been stressed.</p>
<p>As is the case with concrete concepts, the findings with regard to abstract concepts [6] can be accommodated best by hybrid models of conceptual representations assuming an interaction between modality-specific, multimodal and amodal hub areas.Modality-specific systems include the sensorimotor systems, but also the modal systems involved in the processing of emotions, introspections, mentalizing, social constellations, and language.The latter modal systems are probably more important for abstract than for concrete concepts.</p>
<p>Meaning in Language and Mind</p>
<p>There are different theories on how much linguistic and sensorimotor representations contribute to meaning [7].In embodied theories, meaning is based on the relation between words and internal bodily states corresponding to multiple modalities, such as vision, olfaction, and perhaps even internal affective states.By contrast, symbolic theories suggest that the meaning of word (e.g., rose) can be derived in a recursive fashion by considering its relation to the meaning of words in its linguistic context (e.g., red, flower).Current theories of semantics tend to posit that both symbolic and embodied information contribute to meaning.</p>
<p>A study [7] is made to evaluate how well different kinds of models account for people's representations of both concrete and abstract concepts.The models include unimodal linguistic models as well as multimodal models which combine linguistic with perceptual or affective information.There are two types of linguistic models considered: the distributional linguistic model which derives word meaning from word co-occurrences derived from large language corpora, and the word association model which measures the meaning of a word as a distribution of respectively weighted associative links encoded in a large semantic network.The study demonstrates that both visual and affective multimodal models better capture behavior that reflects human representations, especially for basic-level concepts that belong to the same superordinate category.The conclusion is that multimodal information (i.e., symbolic and embodied information) is important for capturing both abstract and concrete concepts.</p>
<p>Symbolic Concept Representations</p>
<p>Embodied concept representations (i.e., modality-specific concept-oriented feature representations) have been discussed previously [1][2][3].In particular, we recommend their learning using exemplar-based contrastive self-supervised learning (CSSL) [3] since it is concept (class) centric and it supports class incremental learning.</p>
<p>In the following we discuss symbolic concept representations (i.e., amodal, language-specific concept graphs) since they are an integral part of dual embodied-symbolic concept representations, but we have not discussed them previously.They come in two forms: word embedding for representing concepts (namely, word semantics in natural language corpora or texts), and knowledge graph embedding for representing concept graphs (a.k.a.knowledge graphs), i.e., conceptual knowledge (e.g.commonsense knowledge).</p>
<p>Note that we will not discuss either word embedding learning or knowledge-graph embedding learning since they are of secondly importance to the focus of this paper.They are discussed in [9][10] and [12][13][14] respectively.</p>
<p>Word Embedding</p>
<p>Word embeddings [8][9] are dense representations of words in semantic vector spaces generated from language corpora or texts, in which semantically similar words have similar embedding vectors.Word embeddings have played an important role for tasks of natural language processing including complex and pertinent ones such as information retrieval and sentiment analysis.They are efficient to learn, highly scalable for large corpora (thousands of millions of words).</p>
<p>Two most widely used word embedding models [8][9][10] are Word2Vec and GloVe.Word2Vec employs two model architectures: Continuous Bag-of-Words (CBOW) model which aims to predict the occurrence of a word given other words that constitute its context, and Skip Gram (SG) model which deals with predicting a context given the word.Word2Vec considers only local word co-occurrences.GloVe is based on a model that reduces the dimensionality of a global cooccurrence matrix of the word-word type in a corpus, with the statistics of the entire corpus captured directly by the model.</p>
<p>GloVe focuses on global word co-occurrences.</p>
<p>For deep-learning based natural language processing [10] word embedding is the basic building block that maps words in the input sentences into continuous space vectors, and usually used (pretrained) in the first layer of a neural network.</p>
<p>Based on the word embedding, complex networks such as recurrent neural networks (RNNs) can be used for feature extraction and build, for example, context-aware word embedding or phrase / sentence embedding.</p>
<p>Note that the methods that generate word embedding based purely on information in a language corpus or text fail to take advantage of the semantic relational structure that exits between words in concurrent contexts.To overcome this limitation, the corpus or text is enhanced with extra morphological, syntactic, semantic and domain knowledge from knowledge sources (e.g., Wikipedia, Wordnet) to generate knowledge-aware word embedding [11].</p>
<p>Knowledge Graph Embedding</p>
<p>The knowledge graph (KG) [12][13][14][15] (a.k.a.concept graph) is a representation of conceptual knowledge (specifically, structured relational information) in the form of concepts and relations between them.We can view a KG as a set of statements (facts) having the form of subject-predicate-object triples, using the notation (h, r, t) (head, relation, tail) to identify a statement.We can also view a KG as a directed labeled graph, where nodes represent concepts and edges represent relations between concepts.</p>
<p>Knowledge graph embedding (KGE) [12][13][14][15] is a widely adopted approach to KG representation in which concepts and relations are embedded in low-dimensional continuous vector spaces.Most methods create a vector for each concept and each relation.These embeddings are generated in a way to capture latent properties of the semantics in the KG: similar concepts and similar relations will be represented with similar vectors.KGE offers precise, effective and structural representation of symbolic conceptual information.</p>
<p>Most of the KGE approaches rely mainly on the use of the subject-predicate-object triples present in the KG to generate the vector representations, (h, r, t), for (head, relation, tail) [12][13][14][15].These approaches can be broadly classified into two groups: translation-based models and semantic matching models.Translation-based models (e.g., TransE) are based on learning the translations from the head concept to the tail concept.They use distance-based measures to generate the similarity score for a pair of concepts and their relations.The training objective is to achieve, mathematically, h + r ~ t.</p>
<p>Semantic matching models (e.g., RESCAL) use a multiplicative approach and represent the relations as matrices / tensors in the vector space.For example, RESCAL relies on a tensor factorization approach upon the 3-dimensional tensor generated by considering subject-predicate-object as the 3 dimensions of the tensor.Note that these models only consider each individual fact, while their intrinsic associations are neglected, which is not sufficient for capturing deeper semantics for better embedding.They, therefore, cannot meet the requirements of KGE.</p>
<p>New approaches leverage the graph nature of KG, and use neural network (NN) based models for various tasks [14][15].</p>
<p>When treated as a graph, KG can be seen as a heterogeneous graph, with the logical relations of more importance than the graph structure.NN-based models can consider the type of concept or relation, neighborhood / substructure information, path information, and temporal information.The use of convolutional neural networks (CNNs) or attention mechanisms also helps to generate better embeddings.Examples of CNN-based models include ConvE and ConvKB [14].</p>
<p>Graph neural networks (GNNs) are neural networks that can be directly applied to graphs, and provide an easy way to generate node-level, edge-level, and graph-level embeddings.They have a somewhat universal architecture in common, referred to as Graph Convolutional Networks (GCNs) which use deep, multi-layer processing known as message passing.</p>
<p>Examples of GCN-based models include RGCN and SACN [14].</p>
<p>GCNs have a strong ability to mine the underlying semantics of KGs [14].In general, GCN-based models can incorporate additional information, such as node types, relation types, node attributes and substructures, to generate better embeddings.For example, if the node information of a multi-hop domain can be aggregated, the accuracy of the model in specific tasks can be greatly improved.A knowledge distillation approach suitable for few-shot CIL is proposed in [16], which is based on the use of dual To mitigate the overfitting problem, multiple visual embeddings for classes are generated, where each is designed specifically for a group of classes.The semantic word vectors are used to separate classes into several groups.The number of groups / visual embeddings is defined by the superclass (cluster) knowledge obtained from the semantic word vector space.</p>
<p>In the semantic space, there is a semantic word vector for each class.The set of superclasses is attained from the semantic word vector space representations of the base classes, and are then held fixed.</p>
<p>For the first (base) task, involving base classes, the steps for obtaining groups / visual embeddings are:</p>
<p> Train the network backbone on the base classes, which is then kept frozen.</p>
<p> Apply k-means clustering, where k = N, on base semantic word vectors and assign a superclass (cluster) label to each base class.</p>
<p> Train N embeddings on the base task using superclass labels as group identity.</p>
<p>For other subsequent (novel) task, involving novel classes, the cluster centers (obtained in the base task) are used to assign superclass labels to novel classes.To assign a superclass label to novel classes, the minimum Euclidean distance between the semantic word vector of a novel class and cluster centers is used.Hence, given a novel class, there is a selection of groups / visual embeddings that each may be more or less suited.</p>
<p>An attention module is used to merge multiple visual embeddings of a class to generate its final visual embedding, i.e., visual vector.A mapping module is then used to project the visual vector from the visual space into the semantic space to align the visual vector with its associated semantic word vector.For the first (base) task, training involves attention loss and classification loss; for other subsequent (novel) task, training involves attention loss, distillation loss (i.e., alignment loss) and classification loss.</p>
<p>Embodied-Symbolic Fused Representation for Image-Text Matching</p>
<p>Image and text matching [17] is an important vision-language cross-modality task for many applications including image retrieval and caption.Before calculating the similarity between an image and text, a matching model needs to obtain a rich representation of the image (and text) first.Most of the current image-text matching models utilize pre-trained neural networks to extract feature embeddings as the representation of images.The image embeddings, however, fail to extract highlevel semantic information.So the semantic gap between images and text leads to limited performance when matching an image with text.</p>
<p>Learning semantic concepts is useful to enhance image representation and can significantly improve the performance of both image-to-text and text-to-image retrieval.Frequently co-occurred concepts in the same image (scene), e.g.bedroom and bed, can provide commonsense knowledge to discover other semantic-related concepts.[17] uses a Scene Concept Graph (SCG) to support this by aggregating image scene graphs and extracting frequently co-occurred concepts as commonsense knowledge.Moreover, it proposes a novel model to incorporate this knowledge to improve image-text matching.Specifically, semantic concepts are detected from images and then expanded by the SCG to include commonly-related concepts (which may be occluded or long-tailed).Afterwards, it fuses their representations with the image embeddings, as semantic-enhanced image embeddings, to use (with text embeddings) for image-text matching.</p>
<p>The model uses dual embodied-symbolic concept representations.The visual embodied representation exists in the form of image embeddings.There are two symbolic representations.The text symbolic representation exists in the form of (context-aware) word embeddings.The concept symbolic (semantic) representation for images exists in the form of concept embeddings.Finally, there is an embodied-symbolic (image-concept) fused representation existing in the form of conceptenhanced image embeddings.</p>
<p>The scene graph [17] of an image is a graph consisting of concepts and relations between them.It can be represented as a set of triples of <subject, relation, object>.The SCG is constructed from scene graphs by aggregating co-occurred <subject, object> pairs from scene graphs of all images.To generate the concept embeddings for an image, first, a concept detection module (a multi-label image classification model) is used to extract semantic concepts from the image on a small concept vocabulary.With the SCG in hand, a concept expansion module is then used to expand the semantic concepts to include commonly-related concepts.Finally, a concept prediction module is used to predict relevant concepts from these and generate the concept embeddings.The concept embeddings of an image are fused with its visual embedding, by the imageconcept fusion module, to generate a concept-enhanced image representation.</p>
<p>The image-text matching problem is formulated as a ranking model.Given the input image and the text, the output is the similarity score of matching their respective visual and language representations, with the visual representation being an image-concept (embodied-symbolic) fused representation and the language representation being word embeddings.To learn image and text matching as well as image-relevant semantic concepts jointly in an end-to-end fashion, the loss function consists of two parts: image-text matching loss and concept prediction loss.</p>
<p>Deep Learning and Symbolic AI Integration</p>
<p>Symbolic AI and deep learning both have strength and weakness, which tend to be each other's opposites.A significant challenge today [18] is to effect a reconciliation.Symbolic AI is based on manipulation of abstract compositional representations whose elements stand for concepts and relations.Therefore, to facilitate reconciliation, a key objective for deep learning is to develop architectures capable of discovering concepts and relations in raw data, and learning how to represent them.</p>
<p>An excellent example for doing this for image data has been discussed in 5 Embodied-Symbolic Fused Representation for Image-Text Matching.(Note that only co-occurred concepts are considered in [17].However, their relations will also be considered in future work.)As discussed there, dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration.</p>
<p>In the following, we discuss two additional important examples of deep learning and symbolic AI integration.</p>
<p>Scene Graph Generation with Knowledge Graph Bridging</p>
<p>Scene graphs (SGs) are powerful representations that extract semantic concepts and their relations from images, which facilitate visual comprehension and reasoning.(For an example usage, see 5 Embodied-Symbolic Fused Representation for Image-Text Matching.)A SG can be represented as a set of triples of <subject, relation, object>.On the other hand, commonsense knowledge graphs (CSKGs) [20] are rich repositories that encode how the world is structured (i.e., commonsense knowledge), and how common concepts are related and interact.</p>
<p>GB-Net (Graph Bridging Network) [19] presents a unified formulation of these two constructs, where a SG is seen as an image-conditioned embodiment of a CSKG.Based on this perspective, the SG generation is formulated as the inference of a bridge between the SG and CSKG, where each concept or relation in the SG must be linked to its corresponding concept or relation in the CSKG.Specifically, both SG and CSKG are defined as special types of knowledge graph (KG): GB-Net fuses the SG embedding and CSKG embedding through a dynamic message passing and bridging algorithm using a graph neural networks (GNN).The method iteratively propagates messages to update nodes, then compares nodes to update bridge edges, and repeats until the two graphs are carefully connected.This results in the SG embedding with bridges to the CSKG embedding.</p>
<p>Multimodal Knowledge Graphs</p>
<p>Knowledge graphs (KGs) have found great use in a wide range of applications including text understanding, recommendation system, natural language question answering, and image understanding (see 5 Embodied-Symbolic Fused Representation for Image-Text Matching and 6.1 Scene Graph Generation with Knowledge Graph Bridging).More and more KGs have been created, covering common sense knowledge [20] (see 6.1 Scene Graph Generation with Knowledge Graph Bridging), lexical knowledge, encyclopedia knowledge, taxonomic knowledge, and geographic knowledge.</p>
<p>Most of the existing KGs are represented with pure symbols, denoted in the form of text, without grounding to the physical world experience.However, both symbolic and embodied representations are inherent to the functioning of the human conceptual system, and they typically interact to drive conceptual processing (see 2 Human Conceptual System: Symbolic and Embodied).Therefore, it is necessary to ground symbols in KGs to corresponding image, text, sound and video data and map symbols to their corresponding referents with meanings in the physical world.That is, the multi-modalization</p>
<p>Conclusion</p>
<p>Motivated by recent findings from cognitive neural science, we advocate the use of a dual-level model for concept representations: the embodied level consists of concept-oriented feature representations, and the symbolic level consists of concept graphs (a.k.a.knowledge graphs).The human conceptual system comprises both embodied representations and symbolic representations, which typically interact to drive conceptual processing.As such, we further advocate the use of dual embodied-symbolic concept representations for deep learning.That is, deep learning should learn from data not only modality-specific embodied representations such as image embeddings, text embeddings, etc., but also the corresponding amodal symbolic (semantic) representation as knowledge graph embeddings, with links to commonsense knowledge graphs.</p>
<p>Dual embodied-symbolic concept representations are the foundation for deep learning and symbolic AI integration, which is an important direction for deep learning and AI since their integration reinforces each other's strength, compensates each other's weakness, and takes a major step toward human-level AI (e.g., grounding in experience / data, understanding, reasoning, explanation, etc.).</p>
<p>4</p>
<p>Embodied-Symbolic Knowledge Distillation for Few-Shot CIL In CIL [2], a model learns tasks continually, with each task containing a batch of new classes.In few-shot CIL [3], only the training set of the first (base) task may have large-scale training data for base classes, while other subsequent (novel) task just contains few-shot instances for novel classes.Few-shot CIL requires transferring knowledge (i.e., knowledge distillation) from old classes to new classes in solving the catastrophic forgetting problem, which is generic to CIL.The additional challenge of few-shot learning is the overfitting problem.</p>
<p>embodied-symbolic concept (class) representations.The embodied (visual) representation exists in the form of visual vectors.Semantic word vectors (Word2Vec and GloVe) are used as the symbolic (semantic) representation to facilitate knowledge distillation, which are generated from unsupervised learning on an unannotated text corpus.The semantically guided network does not add new parameters while adding new classes incrementally.The knowledge distillation process only includes semantic word vectors of novel classes in addition to the base classes, which are used to help the network remembering base class training, generalizing to novel classes, and generating well-separated embodied representation (visual vectors) of classes.Note that though the network may not have had the opportunity to see instances of novel classes, nevertheless, novel classes may very well share semantic properties with base classes it has seen.For example, if hyena is a novel class, many typical hyena attributes like 'face', 'body', etc., may have been seen by the network from base classes.</p>
<p></p>
<p>A KG is a set of nodes of type concept (C) or relation (R), and a set of directed, weighted edges (Ε) between the nodes. A CSKG is a type of KG with commonsense concept (CC) nodes and commonsense relation (CR) nodes.Commonsense edges are of four types: CC-&gt;CC, CC-&gt;CR, CR-&gt;CC, and CR-&gt;CR. A SG is a type of KG with scene concept (SC) nodes and scene relation (SR) nodes.Scene edges are of four types: SC-&gt;SR (subjectOf), SC-&gt;SR (objectOf), SR-&gt;SC (hasSubject), and SR-&gt;SC (hasObject). The SG and CSKG are connected through four types of bridge edges: SC-&gt;CC, SR-&gt;CR, CC-&gt;SC and CR-&gt;SR.GB-Net uses dual embodied-symbolic concept representations.The visual embodied representation for images exists in the form of image embeddings.There are two symbolic concept representations.The CSKG symbolic representation exists in the form of CSKG embedding (of commonsense nodes and edges).The SG symbolic (semantic) representation for images exists in the form of SG embedding (of scene nodes and edges).The CSKG embedding and the SG embedding are interconnected through the bridge edges.</p>
<p>Acknowledgement: Thanks to my wife Hedy (郑期芳) for her support.of KGs[21]is an inevitable key step towards the realization of human-level AI, which results in Multimodal Knowledge Graphs (MMKGs)[12,15,21].To support symbol grounding in MMKG[21], one can take multimodal data as particular attribute values of concepts or relations.This can be denoted in a triple (s, r, d), where s denotes a concept or relation, d denotes one of its corresponding multimodal data, and the relation r is, e.g., "hasImage" when d is an image.Symbol grounding, therefore, can be divided into concept grounding and relation grounding.As an example, concept grounding aims to find representative, discriminative and diverse images for visual concepts.A major challenge is to find representative images for a visual concept from a group of relevant images.The representativeness and discriminativeness of images can be scored in terms of results of cluster-based methods, such as K-means, based on visual embeddings.The captions of images can also be utilized to evaluate the representativeness and discriminativeness of images, at the semantic level, based on text embeddings.IKRL and DKRL are two well-known examples of MMKG. IKRL (Image-embodied Knowledge RepresentationLearning)[12,15]provides a method to integrate images inside the scoring function of the KG embedding model (TransE).Essentially, IKRL uses multiple images for each concept and use the AlexNet CNN to generate embeddings for the images.These embeddings are then selected and combined with the use of attention to be finally projected in the KG embedding space.DKRL (Description-Embodied Knowledge Representation Learning)[12,15], on the other hand, includes the description of concepts in the representation.It uses a CNN to encode the concept description into a vector representation and uses this representation in the loss function.DKRL learns two embeddings for each concept, one that is structure-based (i.e., KG, like TransE) and one that is based on the concept descriptions.The two embeddings are interconnected / integrated.The above discussion, though brief, shows that MMKG uses dual embodied-symbolic concept representations.The KG symbolic representation exists in the form of KG embedding.There are various embodied representations, depending on the multimodal data involved.For image data, the visual embodied representation exists in the form of image embedding.For text (descriptions), the textual embodied representation exists in the form of text embedding.The KG embedding and the modality-specific embedding(s) are interconnected / integrated.
Concept-Oriented Deep Learning. T Daniel, Chang, arXiv:1806.017562018arXiv preprint</p>
<p>Concept Representation Learning with Contrastive Self-Supervised Learning. T Daniel, Chang, arXiv:2112.056772021arXiv preprint</p>
<p>Exemplar-Based Contrastive Self-Supervised Learning with Few-Shot Class Incremental Learning. T Daniel, Chang, arXiv:2202.026012022arXiv preprint</p>
<p>Dual Coding of Knowledge in the Human Brain. Y Bi, Trends in Cognitive Sciences. 2510October 2021</p>
<p>Linguistic Distributional Knowledge and Sensorimotor Grounding both Contribute to Semantic Category Production. B Banks, C Wingfield, L Connell, Cogn. Sci. 45e130552021</p>
<p>Varieties of Abstract Concepts and Their Grounding in Perception or Action. M Kiefer, M Harpaintner, Open Psychol. 20202</p>
<p>Visual and Affective Multimodal Models of Word Meaning in Language and Mind. S D Deyne, D Navarro, G Collell, A Perfors, Cogn, Sci. 45e129222021</p>
<p>A Systematic Literature Review on Word Embeddings. L Gutiérrez, B Keith, Proc. Int. Conf. Softw. Process Improvement. Int. Conf. Softw. ess Improvement2018</p>
<p>U Naseem, I Razzak, S K Khan, M Prasad, arXiv:2010.15036A Comprehensive Survey on Word Representation Models: From Classical to State-of-the-art Word Representation Language Models. 2020</p>
<p>M Zhou, N Duan, S Liu, H.-Y Shum, Progress in Neural NLP: Modeling, Learning, and Reasoning," in Engineering. 20206</p>
<p>Incorporating Extra Knowledge to Enhance Word Embedding. A Roy, S Pan, Proceedings of the 29th International Joint Conference on Artificial Intelligence. the 29th International Joint Conference on Artificial Intelligence202020</p>
<p>Knowledge Graph Embeddings and Explainable AI. Federico Bianchi, Gaetano Rossiello, Luca Costabello, Matteo Palmonari, Pasquale Minervini, arXiv:2004.148432020arXiv preprint</p>
<p>A Survey of Knowledge Graph Embedding and Their Applications. Shivani Choudhary, Tarun Luthra, Ashima Mittal, Rajat Singh, arXiv:2107.078422021arXiv preprint</p>
<p>A Survey on Knowledge Graph Embeddings for Link Prediction. Meihong Wang, Linling Qiu, Xiaoli Wang, Symmetry. 202113485</p>
<p>Application and Evaluation of Knowledge Graph Embeddings in Biomedical Data. Mona Alshahrani, Maha A Thafar, Magbubah Essack, PeerJ Computer Science. 72021</p>
<p>Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, Mehrtash Harandi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2021</p>
<p>Knowledge Aware Semantic Concept Expansion for Image-Text Matching. B Shi, L Ji, P Lu, Z Niu, N Duan, IJCAI. 2019</p>
<p>Reconciling Deep Learning with Symbolic Artificial Intelligence: Representing Objects and Relations. Marta Garnelo, Murray Shanahan, Current Opinion in Behavioral Sciences. 292019</p>
<p>Bridging Knowledge Graphs to Generate Scene Graphs. A Zareian, S Karaman, S F Chang, arXiv:2001.023142020arXiv preprint</p>
<p>CSKG: The Commonsense Knowledge Graph. F Ilievski, P Szekely, B Zhang, Extended Semantic Web Conference (ESWC). 2020</p>
<p>Multi-Modal Knowledge Graph Construction and Application: A Survey. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, Nicholas Jing Yuan, arXiv:2202.057862022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
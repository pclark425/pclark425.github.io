<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6493 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6493</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6493</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276408537</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.10906v1.pdf" target="_blank">PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> â€”Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort. In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs). In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators. This work introduces PCGRLLM , an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques. We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach. Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks. The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model. Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6493.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6493.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-2024-08-06 with Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of OpenAI's gpt-4o-2024-08-06 using Chain-of-Thought (CoT) prompting to iteratively generate and refine reward functions for a PCGRL text-to-reward task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate short story instructions into reward functions; train PCGRL agents and measure whether generated levels match instruction-specified player encounters (enemies/keys/door).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.156</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ToT, GoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.173</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT provides consistent step-by-step reasoning but limited exploration of the reward-function space; performs reasonably but is outperformed by tree-based exploration (ToT) in iterative refinement. Paper reports CoT peaks at 0.156 (iteration 6) and benefits from feedback and self-alignment but less than ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6493.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6493.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o + ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-2024-08-06 with Tree-of-Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of gpt-4o using Tree-of-Thoughts (ToT) prompting to expand multi-branch reasoning and backtrack during iterative reward-function refinement for PCGRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate story instructions into reward functions and iteratively refine via rollouts and feedback, evaluating generated levels against instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.329</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, GoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.173</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT achieved the highest reported accuracy for gpt-4o (0.329), demonstrating the value of branching, multi-path reasoning and backtracking for exploring reward-function design space; authors highlight ToT's fault-tolerance and superior iterative refinement performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6493.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6493.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o + GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-2024-08-06 with Graph-of-Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of gpt-4o using Graph-of-Thoughts (GoT) prompting where multiple thought nodes and auxiliary examples are used to guide reward-function refinement for PCGRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-08-06</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph-of-Thoughts (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>graph-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate and refine reward functions from textual story input using graph-structured multi-node reasoning and auxiliary examples, then evaluate produced levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.111</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, ToT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.218</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>GoT was intended to improve sample efficiency by leveraging multiple thoughts and auxiliary inputs, but in experiments it underperformed ToT for gpt-4o; authors attribute reduced performance in some settings to overload from too much auxiliary information and difficulty reasoning when provided many full reward examples at once.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6493.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6493.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama3.2 + CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llama3.2-90b-instruct with Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Meta's llama3.2-90b-instruct using CoT prompting on the PCGRL text-to-reward generation task, showing stronger zero-shot performance and high improvement with combined feedback/self-alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama3.2-90b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>90B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert story instruction to reward function, train PCGRL agent, and measure whether generated levels satisfy instruction-defined encounters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.472</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ToT, GoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.089</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>llama3.2 had better zero-shot/base performance than gpt-4o in this task; CoT combined with feedback and self-alignment reached the highest reported value for this model (0.472 in FB&SA condition), indicating that model zero-shot ability interacts with which PE techniques provide greatest benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6493.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6493.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama3.2 + ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llama3.2-90b-instruct with Tree-of-Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of llama3.2 using ToT prompting for iterative reward function design in PCGRL; shows improvements but not always the top method for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama3.2-90b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>90B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively refine LLM-generated reward functions via branching reasoning and training rollouts, measure alignment of generated levels to instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.363</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, GoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.02</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT improves exploration and fault tolerance; however, for llama3.2 the best results were achieved by CoT with FB&SA, suggesting interactions between base model capabilities and the most effective PE approach.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6493.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6493.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>llama3.2 + GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>llama3.2-90b-instruct with Graph-of-Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of llama3.2 applying GoT prompting to combine multiple candidate reward functions and auxiliary nodes for sample-efficient refinement in PCGRL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama3.2-90b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>90B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Graph-of-Thoughts (GoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>graph-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-to-reward generation (PCGRL Dungeon narrow 16x16)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use graph-structured multi-thought expansion and auxiliary best/worst thought examples to refine reward functions and evaluate produced levels against instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>multi-label accuracy (average over inferred levels)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.383</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, ToT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.089</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>GoT provided sample-efficiency benefits in principle, but effectiveness depended strongly on quality and quantity of auxiliary examples; authors found that carefully curated auxiliary inputs (e.g., adding a single worst thought) could help, while too much example content could degrade reasoning and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>Eureka: Humanlevel reward design via coding large language models <em>(Rating: 2)</em></li>
                <li>Text2reward: Automated dense reward function generation for reinforcement learning <em>(Rating: 2)</em></li>
                <li>ChatPCG: Large language model-driven reward design for procedural content generation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6493",
    "paper_id": "paper-276408537",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "gpt-4o + CoT",
            "name_full": "gpt-4o-2024-08-06 with Chain-of-Thought prompting",
            "brief_description": "Evaluation of OpenAI's gpt-4o-2024-08-06 using Chain-of-Thought (CoT) prompting to iteratively generate and refine reward functions for a PCGRL text-to-reward task.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-08-06",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Translate short story instructions into reward functions; train PCGRL agents and measure whether generated levels match instruction-specified player encounters (enemies/keys/door).",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.156,
            "comparison_target_method": "ToT, GoT",
            "performance_difference": -0.173,
            "statistical_significance": false,
            "analysis_notes": "CoT provides consistent step-by-step reasoning but limited exploration of the reward-function space; performs reasonably but is outperformed by tree-based exploration (ToT) in iterative refinement. Paper reports CoT peaks at 0.156 (iteration 6) and benefits from feedback and self-alignment but less than ToT.",
            "ablation_study_present": true,
            "uuid": "e6493.0",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "gpt-4o + ToT",
            "name_full": "gpt-4o-2024-08-06 with Tree-of-Thoughts prompting",
            "brief_description": "Evaluation of gpt-4o using Tree-of-Thoughts (ToT) prompting to expand multi-branch reasoning and backtrack during iterative reward-function refinement for PCGRL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-08-06",
            "model_size": null,
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Translate story instructions into reward functions and iteratively refine via rollouts and feedback, evaluating generated levels against instruction.",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.329,
            "comparison_target_method": "CoT, GoT",
            "performance_difference": 0.173,
            "statistical_significance": false,
            "analysis_notes": "ToT achieved the highest reported accuracy for gpt-4o (0.329), demonstrating the value of branching, multi-path reasoning and backtracking for exploring reward-function design space; authors highlight ToT's fault-tolerance and superior iterative refinement performance.",
            "ablation_study_present": true,
            "uuid": "e6493.1",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "gpt-4o + GoT",
            "name_full": "gpt-4o-2024-08-06 with Graph-of-Thoughts prompting",
            "brief_description": "Evaluation of gpt-4o using Graph-of-Thoughts (GoT) prompting where multiple thought nodes and auxiliary examples are used to guide reward-function refinement for PCGRL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-08-06",
            "model_size": null,
            "reasoning_method_name": "Graph-of-Thoughts (GoT)",
            "reasoning_method_type": "graph-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Generate and refine reward functions from textual story input using graph-structured multi-node reasoning and auxiliary examples, then evaluate produced levels.",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.111,
            "comparison_target_method": "CoT, ToT",
            "performance_difference": -0.218,
            "statistical_significance": false,
            "analysis_notes": "GoT was intended to improve sample efficiency by leveraging multiple thoughts and auxiliary inputs, but in experiments it underperformed ToT for gpt-4o; authors attribute reduced performance in some settings to overload from too much auxiliary information and difficulty reasoning when provided many full reward examples at once.",
            "ablation_study_present": true,
            "uuid": "e6493.2",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "llama3.2 + CoT",
            "name_full": "llama3.2-90b-instruct with Chain-of-Thought prompting",
            "brief_description": "Evaluation of Meta's llama3.2-90b-instruct using CoT prompting on the PCGRL text-to-reward generation task, showing stronger zero-shot performance and high improvement with combined feedback/self-alignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama3.2-90b-instruct",
            "model_size": "90B",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Convert story instruction to reward function, train PCGRL agent, and measure whether generated levels satisfy instruction-defined encounters.",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.472,
            "comparison_target_method": "ToT, GoT",
            "performance_difference": 0.089,
            "statistical_significance": false,
            "analysis_notes": "llama3.2 had better zero-shot/base performance than gpt-4o in this task; CoT combined with feedback and self-alignment reached the highest reported value for this model (0.472 in FB&SA condition), indicating that model zero-shot ability interacts with which PE techniques provide greatest benefit.",
            "ablation_study_present": true,
            "uuid": "e6493.3",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "llama3.2 + ToT",
            "name_full": "llama3.2-90b-instruct with Tree-of-Thoughts prompting",
            "brief_description": "Evaluation of llama3.2 using ToT prompting for iterative reward function design in PCGRL; shows improvements but not always the top method for this model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama3.2-90b-instruct",
            "model_size": "90B",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Iteratively refine LLM-generated reward functions via branching reasoning and training rollouts, measure alignment of generated levels to instructions.",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.363,
            "comparison_target_method": "CoT, GoT",
            "performance_difference": -0.02,
            "statistical_significance": false,
            "analysis_notes": "ToT improves exploration and fault tolerance; however, for llama3.2 the best results were achieved by CoT with FB&SA, suggesting interactions between base model capabilities and the most effective PE approach.",
            "ablation_study_present": true,
            "uuid": "e6493.4",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "llama3.2 + GoT",
            "name_full": "llama3.2-90b-instruct with Graph-of-Thoughts prompting",
            "brief_description": "Evaluation of llama3.2 applying GoT prompting to combine multiple candidate reward functions and auxiliary nodes for sample-efficient refinement in PCGRL.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "llama3.2-90b-instruct",
            "model_size": "90B",
            "reasoning_method_name": "Graph-of-Thoughts (GoT)",
            "reasoning_method_type": "graph-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Text-to-reward generation (PCGRL Dungeon narrow 16x16)",
            "task_description": "Use graph-structured multi-thought expansion and auxiliary best/worst thought examples to refine reward functions and evaluate produced levels against instruction.",
            "performance_metric": "multi-label accuracy (average over inferred levels)",
            "performance_value": 0.383,
            "comparison_target_method": "CoT, ToT",
            "performance_difference": -0.089,
            "statistical_significance": false,
            "analysis_notes": "GoT provided sample-efficiency benefits in principle, but effectiveness depended strongly on quality and quantity of auxiliary examples; authors found that carefully curated auxiliary inputs (e.g., adding a single worst thought) could help, while too much example content could degrade reasoning and performance.",
            "ablation_study_present": true,
            "uuid": "e6493.5",
            "source_info": {
                "paper_title": "PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Eureka: Humanlevel reward design via coding large language models",
            "rating": 2,
            "sanitized_title": "eureka_humanlevel_reward_design_via_coding_large_language_models"
        },
        {
            "paper_title": "Text2reward: Automated dense reward function generation for reinforcement learning",
            "rating": 2,
            "sanitized_title": "text2reward_automated_dense_reward_function_generation_for_reinforcement_learning"
        },
        {
            "paper_title": "ChatPCG: Large language model-driven reward design for procedural content generation",
            "rating": 2,
            "sanitized_title": "chatpcg_large_language_modeldriven_reward_design_for_procedural_content_generation"
        }
    ],
    "cost": 0.013707999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning
15 Feb 2025</p>
<p>In-Chang Baek inchang.baek@gm.gist.ac.kr 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Sung-Hyun Park 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Sam Earle 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Zehua Jiang 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Noh Jin-Ha 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Julian Togelius 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>Kyung-Joong Kim 
Gwangju Institution of Science and Technology (GIST)
New York University</p>
<p>PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning
15 Feb 20253C19A70D7335268273FEB0988D2AC9ECarXiv:2502.10906v1[cs.AI]large language modelprocedural content generationprompt engineeringreinforcement learningreward generation
Reward design plays a pivotal role in the training of game AIs, requiring substantial domain-specific knowledge and human effort.In recent years, several studies have explored reward generation for training game agents and controlling robots using large language models (LLMs).In the content generation literature, there has been early work on generating reward functions for reinforcement learning agent generators.This work introduces PCGRLLM, an extended architecture based on earlier work, which employs a feedback mechanism and several reasoning-based prompt engineering techniques.We evaluate the proposed method on a story-to-reward generation task in a two-dimensional environment using two state-of-the-art LLMs, demonstrating the generalizability of our approach.Our experiments provide insightful evaluations that demonstrate the capabilities of LLMs essential for content generation tasks.The results highlight significant performance improvements of 415% and 40% respectively, depending on the zero-shot capabilities of the language model.Our work demonstrates the potential to reduce human dependency in game AI development, while supporting and enhancing creative processes.</p>
<p>I. INTRODUCTION</p>
<p>The design of reward functions plays a pivotal role in the training of deep reinforcement learning (DRL) agents and evolutionary algorithms for procedural content generation (PCG) in games [1].Reward functions guide agent behaviors and define the objectives that align generated content with desired outcomes, such as game difficulty or aesthetic appearance.Traditionally, designing reward functions relies heavily on researchers' game-specific knowledge and time-consuming reward shaping process.In the procedural content generation via RL (PCGRL) literature [2], the controllability of reward function has been achieved by parameterization of reward function in two-and three-dimensional level generation tasks [3], [4].This significant human dependency not only requires significant time and resources but also introduces barriers to accessibility and scalability of game AIs.Additionally, the controllability of RL-based generative models has been dependent on pre-defined environmental features.Therefore, This work was supported by GIST-IREF from Gwangju Institute of Science and Technology(GIST).</p>
<p>Â§ Corresponding author LLM â„³</p>
<p>Instruct (Story)</p>
<p>Agent  reward generation is necessary to alleviate the dependency on humans and dependency on controllable features.
Feedback Alignment Environment
Recent advancements in LLMs have shown their potential to mitigate these challenges by leveraging pre-trained expert knowledge from large datasets.Several studies have explored LLM-based reward generation approaches in robotic control [5]- [7] and the gameplay [8], [9] domain, utilizing LLMs' reasoning and coding capabilities.One such approach, ChatPCG [10], introduced an early-stage LLM-driven reward generation method for PCGRL, which transforms high-level game descriptions into low-level reward function code.This work proposed a reward fine-tuning method, self-alignment, to align the reward function for a specific environment.However, a limitation of this approach is the absence of a refinement process that incorporates results of the trained agent.As a result, it is uncertain whether the trained policy accurately reflects the intended reward function generation conditions.</p>
<p>To address these limitations, we propose an improved architecture based on prior work [10], PCGRLLM, a feedbackbased reward generation framework for content generation.refining processes: self-alignment and feedback.The language model generates a reward function with a brief story instruction and a PCGRL model is trained with the generated function.To ensure that the RL agent generates content that satisfies the instruction, the language model provides feedback and updates the reward function in the next iteration.</p>
<p>The feedback allows the RL agent to improve its policy by observing the actual outcomes of the trained agent, while the LLM generates rewards that can be effectively incorporated into the RL agent's training.Specifically, our contributions are threefold:</p>
<p>â€¢ Enhancing reward generation architecture: The feedback mechanism enhances the reward generation pipeline, enabling the policy to align more effectively with the given instructions.We evaluate the proposed framework with a text-to-reward task, which evaluates reward functions based on how they reflect a given textual prompt.The PCGRL agent is trained with LLM-generated reward functions, and the quality of these reward functions is evaluated based on the agent-generated content.The task is demonstrated in a two-dimensional level generation environment [2], [11], with a brief story input, such as: "The player needs to obtain a key and escape through the door.To pick up the key, the player encounters bat monsters."To generate reward values for content generation, it is essential to design reward functions iteratively through effective planning and to employ reasoning to assess the causal relationship between the reward function and the resulting content.From the perspective of content generation, we evaluate the reasoning capabilities of LLMs-code generation, content evaluation, reflection, and key extraction-by conducting extensive evaluations.</p>
<p>II. BACKGROUND</p>
<p>A. Prompt Engineering</p>
<p>Prompt engineering (PE) has become an effective methodology to enhance the performance of large language models (LLMs) in a gradient-free manner, with various techniques developed to improve their logical reasoning and planning abilities.These techniques can be described as utilizing different structures to extend thought processes, as illustrated in Fig. 2. Compared to traditional input-output (IO) methods that query results in a single step, these structured approaches show superior performance in solving problems sequentially.</p>
<p>These techniques can be broadly categorized into three approaches based on chain, tree, and graph structures:</p>
<p>Chain-of-Thought (CoT) [12], [13] expands reasoning process with multiple steps to solve a problem in step-bystep.This method is particularly effective for problems requiring long-horizon reasoning.An enhanced variant, CoT with self-consistency (CoT-SC) [14], selects the most consistent response through a majority vote mechanism.</p>
<p>Tree-of-Thoughts (ToT) [15] expands reasoning in multiple directions, increasing the scope of exploration.This faulttolerant method, equipped with backtracking capability, not only broadens the exploration space but also allows recovery by redirecting to alternative paths when a wrong direction is taken, providing resilience against getting trapped in local optima.The multi-path reasoning requires a fitness function to evaluate and select nodes for expansion, ensuring efficient exploration of the solution space.</p>
<p>Graph-of-Thoughts (GoT) [16] is an extended version of ToT, designed to improve the sample efficiency of reasoning by leveraging multiple thoughts generated during node expansion.When expanding a node, GoT retrives related nodes to enhance the reasoning process and ensure efficient exploration of the solution space.</p>
<p>B. Reward Generation Problem</p>
<p>The reward design problem [17] refers to the process of searching for a reward function (R) that maximizes the fitness score F (Ï€ trained ), where Ï€ trained represents a policy trained using R. A DRL agent takes an action a in a state s âˆˆ S and receives a reward value r = R(s|a).The reward function can be designed as a sparse reward based on task success criteria or as an immediate dense reward for actions.Recently, there has been significant interest in LLMs for reward generation, particularly in the robotics domain, where these models benefit from extensive human-level prior knowledge.Compared to using LLMs as policies, LLM-based reward generation is advantageous for reducing inference costs of large models and improving performance in low-level control tasks [18], [19].</p>
<p>For instance, studies such as [6], [7], [20] have demonstrated the use of LLMs to generate reward functions for training DRL agents in robotics.By providing detailed environment descriptions and task-specific rules, these approaches facilitate reward generation for robotic tasks.Building on these efforts, Eureka [5] and Text2Reward [21] introduced advanced techniques such as evolutionary search and reward reflection to achieve human-level reward design performance.These methods also incorporate mechanisms to evolve reward functions using human feedback or preferences.Notably, LLMs take a leading role in reward function generation, either fully automating the process or assisting human creativity in designing effective rewards.While most of these studies focus on robot control tasks by generating dense rewards for solving complex tasks, there remains a need to explore their potential applications in domains like content generation.</p>
<p>Meanwhile, the reasoning processes used to evolve reward functions have predominantly relied on conventional PE methods, such as CoT, which focus on incremental reasoning expansions and limit the exploration of the reward function space.Recognizing these constraints, this study aims to advance state-of-the-art PE techniques to significantly expand the scope of reward space exploration, unlocking broader possibilities for reward function optimization across diverse domains, including content generation.</p>
<p>C. LLM-driven Reward Generation for Procedural Content Generation</p>
<p>Reward design has been a crucial aspect of content generation, but it remains heavily reliant on expert knowledge and is often time-consuming.Traditionally, reward functions have been either fixed through manually crafted code [2] or conditionally generated based on predefined features, such as path length, determined by experts [3].This dependency on predefined conditions limits the diversity of content that can be generated and constrains the flexibility of reward design.Furthermore, the need for domain-specific knowledge about the game reduces the accessibility of content generation algorithms, making them less adaptable to diverse applications.</p>
<p>To address this problem, ChatPCG [10] introduced an early stage architecture which leverages LLMs to automate the generation and refinement of reward functions.The previous work demonstrated reward function generation for enriching multiplayer game content [22], training a generator agent to maximize role differentiation in a given game context.Chat-PCG proposed CoT-based self-alignment techniques to align the reward function to the specific scale of game variables, ensuring the outputs of reward function align to the LLMgenerated insight.</p>
<p>However, ChatPCG has an architectural limitation, namely the absence of feedback, as it lacks a refinement process based on the trained policy.It is challenging to predict content generation outcomes from an arbitrary reward function since small parts of code lead to significant changes on outputs.To address this issue, this paper improves upon the previous architecture by incorporating self-feedback mechanisms and enhancing the exploration of reward space using PEs.Through the enhanced architecture, the processes of reward generation and validation are fully automated, reducing human dependency in reward function design and supporting design creativity.III.PRELIMINARIES A. Procedural Content Generation via RL PCGRL [2]-a DRL-based content generation method-is a machine learning-based content generation methods.The generator agent is trained with a hand-crafted reward function and gets a positive reward when the content gets closer to the goal condition.The benefits of PCGRL stem from its data-free nature and computational efficiency during inference, making it well suited for real-time content generation in games [23].</p>
<p>Originally introduced for 2D level generation in games such as Sokoban and Zelda [2], PCGRL has been expanded through subsequent research.These advancements include support for conditional metric inputs [3], the ability to freeze specific tiles during generation [11], applications in 3D Minecraft level generation [4], and integration with vectorized multiplayer game skill data [22].</p>
<p>In PCGRL, the level design process is framed as a Markov Decision Process (MDP), where level generation is learned through a trial-and-error approach.At each step t, the agent observes the game level as a state s t , selects an action a t to modify a tile of the level, and transitions to a new state s t+1 .The agent then receives a reward: r t = R(s t , s t+1 ), determined by a reward function (R) that evaluates the transition between states.In PCGRL, reward function design requires identifying a computable target based on the given generation objective and appropriately combining different weights.When multiple sub-functions need to be combined to achieve the desired artifact, designing a reward function in a single attempt is highly challenging.Instead, the reward functions has been refined by humans through multiple attempts, with iterative modifications made based on the observed results.Therefore, the reward design process involves iteratively combining functions to generate a reward function that produces game-like levels while satisfying the given conditions.</p>
<p>B. The 2D Level Generation Environment</p>
<p>This study uses the PCGRL-Jax [11] environment, a GPU-accelerated implementation of the widely used twodimensional level generation framework [2], [3].The selected environment ensures a deterministic reward setting compared to the stochastic reward signal used in the previous study [10], so that the generated reward function is relatively accurately  Each episode begins with a randomly initialized 16 Ã— 16 matrix derived from a predefined tile set.The tile set consists of seven types: Empty , Wall , Player , Bat , Scorpion , Spider , Key , and Door .Each tile type is represented numerically in the matrix to indicate its presence.The agent can modify five types of tiles, except for two unchangeable Player and Door tiles, along with the 3 Ã— 3 area of unchangeable tiles surrounding the tiles.The two unchangeable tiles are randomly spawned on the opposite corners of the level in the initial state.The observation space is defined as a 2D array representing the integer tile numbers, along with a channel the location of the tile to be modified.The discrete action space includes five actions, each corresponding to the specific tile type that replaces the tile at the modification location.The reward for the agent is determined by an LLM-generated reward function, implemented using JAX-compatible functions [24].</p>
<p>IV. STORY-BASED REWARD FUNCTION GENERATION TASK</p>
<p>Recent advancements in text-based generative models have showcased the potential for translating textual descriptions into diverse domains such as human-like motion [25], high-fidelity images [26], music composition [27], and game content generation.The gaming domain has also benefited significantly from text-based generative approaches.For example, textconditioned generative models have been applied to specific tasks such as generating Super Mario Bros levels [28] or Sokoban puzzles [29], where models synthesize playable and contextually relevant game content.Extending beyond level design, recent works have explored generating entire games from textual descriptions [30], [31], thereby transforming abstract narratives into interactive environments and mechanics.The generated content is evaluated to ensure it aligns with the instructions (i.e., textual conditioning).</p>
<p>Our method is evaluated on the text-to-reward generation task, which aims to bridge narrative-driven descriptions with a trainable reward function.This evaluation checks whether the generated content satisfies the given text instructions, such as ensuring that the player encounters specific conditions during gameplay-for example, encountering Bat and Spider as required objectives.The two input instructions used in this work are as follows:</p>
<p>â€¢ "The player needs to obtain a key and escape through the door.To pick up the key, the player encounters bat monsters."â€¢ "The player needs to obtain a key and escape through the door.To pick up the key, the player encounters bat and spider monsters."</p>
<p>We measure coherence-based accuracy by evaluating how well the player's experience along the path to the door aligns with the given instructional conditions.To measure the game entities (keys and enemies) encountered by the player, we adopted a deterministic pathfinding algorithm for evaluation.We evaluate the generated levels based on how well they align with specific gameplay scenarios, placing emphasis on ensuring a coherent player experience.</p>
<p>V. PROPOSED METHOD</p>
<p>Our proposed framework PCGRLLM, an improved reward generation framework for PCG, employs a three-step sequential approach: (1) refine the reward function through feedback, (2) align the reward function to the environment and train the agent, and (3) provide feedback to the reward function based on the generated content.Fig. 3 illustrates the comprehensive architecture of the proposed framework.While the prior study [10] was the first to incorporate self-alignment into the reward generation task in content generation domain, this work extends the framework by incorporating feedback feature and a refinement process, forming an outer loop that enhances the overall system's adaptability and performance.The following subsections detail the process of refining the reward function to align with the instruction inputs, with the corresponding pseudo-code provided in Algorithm 1. // Step 2: Self-alignment (Section V-B)
8:
R â€² âˆ¼ M(p reward , l, R, F b, f, Aux)
9:
for alignment step z = 1 to N align do 10:
r env 1:N â† Rollout(Ï€ random , Env, R â€² ) 11:
R â€²â€² â† N (p align , R â€² , r env 1:M )</p>
<p>12:</p>
<p>R â€² â† R â€²â€²</p>
<p>13:</p>
<p>end for 14:
Ï€ trained â† Train(Ï€ untrained , Env, R â€² ) 15: s T â† Rollout(Ï€ trained ) â–· Generate content 16: f â† F (s T ) â–· Evaluate fitness 17: // Step 3: Feedback (Section V-C) 18: F b â† M(p feedback , l, R â€² , s T ) 19:
G.update(R â€² , s T , f ) 20: end for 21: Output: Refined reward function R â€²</p>
<p>A. Reward Refinement</p>
<p>The reward refinement process is an iterative procedure aimed at enhancing the reward function through feedback to better align it with the given instruction.It takes as input a textual description of the game environment, the objective of the reward generation task, textual instructions, and accessible variables from the environment.For this study, two concise textual stories, as detailed in Section IV, were used as input for the textual instructions.The process involves revising the parent function, defined as the previously generated reward function, and progresses iteratively, with each iteration denoted as y.Depending on the presence of a prior iteration, the process operates in one of two phases: initializing the reward function or refining it based on feedback from previous iterations.</p>
<p>Initial generation (y = 1) The initial generation phase begins with a template reward function, which is an empty function containing only parameter definitions.The parameters of the reward function are the previous and current level arrays, while the level size is intentionally excluded to prevent the LLM from generating hard-coded functions.In the first iteration, the LLM generates conceptual ideas to outline the components of the reward function.The prompt includes the progress of the feedback iteration, "{current iter.} of {max iter.}", to utilize the planning capabilities of LLMs.</p>
<p>Continued refinement (y â‰¥ 2) In subsequent iterations, the process retrieves a previously generated reward function from the reward function archive (G) to serve as the parent function for refinement.A pair consisting of the parent reward function (R) and its corresponding feedback (F b) is sampled and provided to the LLM.The method for retrieving the parent reward function (R) varies depending on the prompt engineering method, as detailed in following section (Section V-D).</p>
<p>Once the parent function and feedback are identified, they are passed to the LLM (M).Using a refinement prompt (p refine ), the LLM generates an improved reward function (R â€² ) that incorporates the parent function and feedback.The improved reward function addresses the shortcomings or problematic aspects of the previous reward function, making it better aligned with the textual instructions.</p>
<p>B. Self-alignment to the Environment</p>
<p>The self-alignment process ensures that the generated reward function (R) produces the trainable reward signals in the environment, reflecting the intend of LLM.By allowing the reward function to briefly interact with the training environment in a few-shot manner, this process refines the function to align with the designed insights.Overly narrow reward ranges or excessively large reward values can hinder effective training DRL agent.This iterative process enables the reward function to generate the intended content while providing meaningful feedback for each refinement.</p>
<p>Specifically, one episode is simulated using a random agent to collect distributed reward values directly from interactions with the environment, denoted as r env 1:M .The mean and variance of these reward values are calculated to verify whether the designed reward function outputs values within the intended range.Based on these evaluations, the LLM adjusts the reward function, producing an updated version (R â€²â€² ) along with the actual reward values (r env 1:M ), the current reward function (R â€² ), and the alignment prompt (p align ).This process, repeated N align times, incrementally refines the reward function by modifying weights or formulas to consider the scale and sparsity of the reward signal.</p>
<p>C. Feedback from Generated Contents</p>
<p>Feedback is an essential process for refining the reward function to reflect the actual output of the trained policy.The LLM reasons about the causal relationship between the reward function and the generated content to identify inconsistencies.It evaluates whether the generated content aligns with the instruct conditions by analyzing discrepancies, such as the positions or counts of important tiles within the level.Based on this analysis, the model formulates a plan to refine the reward function, guiding its next iteration toward better alignment with the intended design objectives and the policy's outputs.</p>
<p>First, the terminal states (s T ) are collected as generated content by inferring the trained policy (Ï€ trained ).Then, the LLM generates feedback by reasoning over the text-formatted levels or rendered images of the terminal states (s T ) along with the current reward function (R â€² ) and a feedback prompt (p feedback ).To prevent hallucination and maintain precision, the number of feedback points is limited to one per iteration.</p>
<p>Once the feedback (F b) is generated, the updated reward function and its associated feedback are added to the reward function archive (G) for use in the next iteration.This iterative process ensures that the reward function progressively aligns with both the intended objectives and the actual outputs of the trained policy, enabling the generation of increasingly optimal content.</p>
<p>D. Reasoning-based Prompt Engineering for Reward Improving</p>
<p>We employed various state-of-the-art PE techniques to iteratively refine the reward function in a step-by-step manner.The process of generating reward functions is inherently a trial-and-error approach, which presents a significant challenge as it does not guarantee consistent improvements.Such an algorithm must effectively identify and address errors while maintaining the flexibility to explore alternative solutions, ensuring convergence toward improved reward designs.To address this challenge, we adopted backtracking-enabled and branching PE methods, such as ToT and GoT, to expand the exploration of the reward space.Fig. 2 illustrates the detailed procedure of thought node expansion and the utilization of auxiliary information in GoT.</p>
<p>Thought node A feedback iteration unit is represented by a single thought node in the reasoning process, comprising three steps: reward refinement, self-alignment, and feedback.The expansion method varies depending on the PE approach.CoT employs a simple chain structure with a maximum breadth (N breadth ) of 1, while ToT and GoT utilize tree and graph structures, respectively, with a maximum breadth.For parent node selection, CoT expands from the latest unique node, whereas ToT and GoT expand from the node with the highest fitness score, provided its number of child nodes remains within the maximum breadth.</p>
<p>Reward evaluation The fitness score, essential for determining the parent node in ToT and GoT, is measured within the range [0, 1] and evaluates how well the generated content satisfies the instruction input.It can be determined using heuristics or self-evaluation with LLMs.In this study, we use a heuristic approach based on accuracy to mitigate the influence of subjective LLM evaluations.Specifically, we calculate the average accuracy of 30 instruct-level pairs.</p>
<p>Auxiliary information GoT utilizes auxiliary inputs, incorporating the parent thought node along with the top-2 reward nodes and their corresponding fitness scores.This approach is motivated by the need to improve sample efficiency, as generating and training reward functions incur significant computational costs.By comparing the fitness value (f ), the LLM can identify reward functions that best satisfy the instruct and discern their advantageous elements to combine them, facilitating more informed reasoning and improved outcomes.</p>
<p>VI. EXPERIMENT A. Experiment Setting</p>
<p>Reward generation The number of self-feedback iterations was set to N feedback = 6, and the self-alignment iterations were set to N align = 5.For generating the LLM-based reward, OpenAI's gpt-4o-2024-08-06 [32] served as the primary backend language model.The breadth for the ToT and GoT methods was set to N breadth = 2, balancing the depth of the thought nodes with exploration.The fitness function utilized an accuracy-based evaluation metric, calculated as the average score over 30 inferenced levels.To minimize variability in LLM responses, the temperature parameter, which governs stochasticity, was set to 0. The prompts used in the experiments are noted in Appendix D.</p>
<p>While we attempted to control the LLM's determinism, we still observed variance in first-iteration zero-shot accuracy across different prompt engineering and feedback types.Subtle differences in prompt structure (e.g., placeholders for auxiliary data) can affect on the reasoning process.We focus on how accuracy improves over subsequent iterations rather than focusing on zero-shot performance.Environment The Dungeon problem features seven game tiles and uses the Narrow representation for the environment setting [2].The level size was set to 16 Ã— 16, and the agent was configured to scan the entire level three times per episode.The discrete action space consisted of five actions to modify the level, targeting five modifiable tiles: empty, wall, and three enemy types.At the start of each episode, the player and one door were randomly placed along one of the four edges of the level, and the nearby 3 Ã— 3 tiles were masked as unmodifiable.</p>
<p>RL training The DRL models were trained using proximal policy optimization (PPO) [33] for 50 million timesteps using PureJaxRL [34] implementation and the hyperparameters detailed in Appendix C. Each input instruction and PE method was repeated five times, and the results were averaged.All experiments were conducted on RTX 8000 GPU machines.Accuracy measures how well the generated level aligns with the given instructions.We assess whether the player encounters the specified key enemy tiles during gameplay, following the given scenario.The ground truth (g) is defined such that among the three enemy tile set (E)-Bat, Scorpion, and Spider-those mentioned in the instruction are treated as positive, while the others are considered negative.The prediction (p) is defined as the enemies encountered by the player while traversing from the key to the door.The floodfill pathfinding algorithm is employed to determine viable solutions by identifying routes that allow the player to reach key and door tiles.Fig. 5 visually illustrates how viable solutions are extracted from the game level and demonstrates the process of counting the enemies encountered.We define enemies within the 5 Ã— 5 area around the solution path as those subject to convolution, categorizing them as positive predictions while others are negative.The multi-label accuracy is measured as shown in Eq. 1:</p>
<p>B. Evaluation Criteria
Accuracy = 1 |E| eâˆˆE I(g e = p e )(1)
For example, if the instruction states, "... the player encounters bat monsters," and the generated level is shown in Fig. 5b, the ground truth is [1, 0, 0], where only the Bat is a positive label.If the prediction is [1, 1, 0], where both Bat and Scorpion are marked as positive, the accuracy is calculated as 0.67.The accuracy is measure on each instruction-level pair and the averaged with five runs.The detailed evaluation algorithm is described in Appendix A.</p>
<p>C. Experimental Result</p>
<p>We conducted three key experiments to evaluate the effectiveness of the proposed framework and to investigate the capabilities of LLMs in generating reward functions for PCG tasks.The focus is on assessing the LLM's reasoning and planning abilities in identifying and resolving issues within reward functions, as well as its reliability in objectively inferring scores to evaluate the suitability of generated content.The following research questions were posed to guide the experimental analysis:</p>
<p>â€¢ RQ1.How do self-alignment and feedback mechanisms enhance reward generation?â€¢ RQ2.How does reasoning-based prompt engineering enhance reward generation?â€¢ RQ3.Is LLM has capability of self-evaluation on content fitness? 1) Architectural Ablation Study (RQ1): Table I presents the results of an ablation study analyzing the role of selfalignment (SA) and feedback (FB) mechanisms in improving the accuracy of reward function generation using the gpt-4o model.The performance improvement was consistent across all PE methods, highlighting the significant role of feedback in aligning generated reward functions with task objectives.Specifically, PCGRLLM outperformed both the zero-shot generated reward function (0.031) and the self-alignment-only approach [10] (0.045) by achieving a score of 0.187 with FB, representing an improvement of approximately 415.5% (0.045â†’0.187).This substantial performance improvement can be attributed to the model's relatively limited zero-shot generation capabilities.</p>
<p>To evaluate the generalization of the proposed method, we additionally employed Meta's llama3.2-90b-instruct[35], which exhibits better zero-shot generation performance.The llama3.2 model achieved a 40.5% improvement (0.289â†’0.406) with FB&amp;SA.The results from llama3.2 suggest that the proposed method is capable of improving rewards regardless of the LLM's zero-shot performance.These results demonstrate that feedback is essential for improving reward functions, however, combining feedback with selfalignment does not necessarily guarantee performance improvement.We discuss the importance of feedback quality in Section VII-A and auxiliary experiment on vision-input feedback generation is described on Appendix B.</p>
<p>2) Effect on Reasoning-based Prompt Engineering (RQ2): This section examines the role of reasoning-based prompts in guiding LLMs to design effective reward functions.Table II compares the accuracy of three methods-CoT, ToT, and GoT-over six iterations.Fig. 6 illustrates the accuracy values over six iterations, highlighting the initial, best, and the last accuracy values.ToT achieves the highest accuracy of 0.329 at iteration 6, reflecting its ability to effectively leverage structured reasoning.CoT peaks at 0.156 in the same iteration while maintaining a consistent trajectory overall.In contrast, GoT reaches a modest accuracy of 0.095 by iteration 2, showing decreasement on later iterations.These findings highlight the necessity of selecting prompt strategies that correspond to the level of informational importance.Of the various approaches, ToT proves to be the most effective for iterative refinement, highlighting the value of structured reasoning in improving LLM-generated reward signals while also providing fault tolerance for flawed rewards through its backtracking mechanism.</p>
<p>3) Content Evaluation Ability Analysis (RQ3): Accurately measuring the fitness of generated content plays a critical role in determining the correct direction for improving the reward function.Due to the potential for hallucinations in LLMs, the PEs studies [15], [16] employed heuristics as fitness functions to evalute the trained policy.We treat the heuristic as an oracle representing the maximum performance and compare it against the evaluation scores produced by the LLM when used to improve the reward function.To evaluate content, we provided the LLM with generated content and evaluation metrics, requesting to return a score value scaled between 0 and 1 according to the explanation of accuracy measurement.The results presented in Table III highlight a clear contrast between the heuristic (oracle) and LLM-based evaluation methods.On average, the heuristic evaluation demonstrated a significant performance improvement of +0.172 when feedback was applied, showcasing its ability to effectively utilize feedback for performance enhancement.Conversely, the LLMbased self-evaluation exhibited a decline in performance with feedback, resulting in an average decrease of -0.039.This outcome indicates that the inaccuracies in self-evaluation led to inaccurate directions for improving the reward function.</p>
<p>VII. DISCUSSION</p>
<p>A. Specificity of Feedback</p>
<p>In RQ1 experiment, we observed that the involvement of feedback influences on the reward function improvement.To further investigate this, we conducted an experiment to evaluate whether the quality of feedback impacts performance.This experiment benchmarks three different feedback types-No Feedback, Generic, and Specific feedback-on performance across multiple iterations.The general feedback provides two lines of tips to enhance the reward function that are unrelated to the generated content, while specific feedback represents the default setting in this study.To isolate the effect of fitness evaluation, the experiment was conducted within the CoT framework.</p>
<p>As shown in Table IV, the performance comparison across feedback quality types highlights the significant impact of feedback specificity on performance improvement as iterations progress.Additionally, Fig. 7 illustrates the relative accuracy change (âˆ†y) over iterations for each feedback type, emphasizing the role of specific feedback in driving more consistent performance gains.</p>
<p>According to the data, the no feedback and generic feedback conditions exhibit occasional improvements in âˆ†y; however,  the overall changes remain mostly consistent and insignificant.Due to the lack of actionable guidance for improvement in the no feedback and generic feedback conditions, subsequent iterations tend to produce results similar to those of the first iteration, with minimal iterative improvement.In contrast, specific feedback demonstrates more notable performance improvements in some iterations.Particularly, significant performance leaps are observed in âˆ†y 2â†’3 and âˆ†y 5â†’6 .These findings clearly indicate that tailored feedback plays a critical role in optimizing performance as iterative tasks progress.</p>
<p>B. Auxiliary Information Analysis</p>
<p>In RQ2 experiment, we observed that providing more fewshot data in GoT resulted in lower performance compared to ToT.To investigate whether the number or quality of the fewshot data affects the improvement of the reward function, we conducted an in-depth experiment.Fig. 8 compares the type and quantity of few-shot examples provided as auxiliary data.Providing best thoughts (high fitness values) would expand the upper bound of performance, while worst thoughts (low fitness values) guide the model to avoid falling below the lower bound of performance.</p>
<p>The results indicate that, while there is no consistent accuracy trend based on the number of auxiliary data, specific settings demonstrate higher performance tendencies.For example, adding one worst thought to the GoT baseline setting improves performance (0.244), surpassing CoT (0.156).This highlights that providing excessive information does not necessarily enhance the effectiveness of an LLM, as it may struggle to reason logically when overloaded with multiple pieces of information in a single turn.This suggests that presenting entire reward examples at once can distract the LLM from key focal points, making it more advantageous to provide the parent reward function and a summarized revision direction separately.Therefore, the ability to retrieve relevant information and adopt a divide-and-conquer approach is crucial for improving reward generation and represents a valuable area of study.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>This study introduces an advanced reward generation architecture for game content generation.PCGRLLM frames the reward function refinement as thought units and incorporates various PE methods to enhance exploration within the reward space.The extended architecture analyzes the content generated by the trained policy and incorporates policy feedback into reward refinement.The results indicate that feedbackbased reflection significantly improves the reward function, highlighting the critical role of feedback quality.The state-ofthe-art PEs, such as ToT and GoT, has capability to enhance reward function through a fault-tolerant reasoning process and sample efficiency.The generality of the framework is evaluated using two popular foundation LLMs, demonstrating substantial improvements in low and zero-shot generation performance.</p>
<p>Furthermore, this work investigates two essential abilities of LLMs for game content generation: content evaluation and vision-based content reasoning.The experimental results suggest that content evaluation using language models remains a challenging problem, despite it plays a crucial role in guiding the direction of reward refinement.Future work aims to enhance content evaluation performance by developing objective assessment methods, such as few-shot retrieval techniques, to achieve more balanced score distributions.This improvement would reduce reliance on human experts, paving the way for an end-to-end LLM framework.for t âˆˆ kernel_tiles do end for 17: end for 18: return Encountered enemies p Algorithm 2 describes the process of detecting enemies encountered from the player to door positions.For each key k âˆˆ K, the algorithm calculates the path Ï„ Sâ†’k from the start S to the key and the path Ï„ kâ†’D from the key to the door D using the flood-fill pathfinding algorithm.Determine whether there is connectivity between the player, key, and door, and whether there is a single unique key on the path, defining this as the solution trajectory.Using the coordinates of the trajectory path, perform a convolution operation with a 5 by 5-sized kernel to detect the presence of enemies.Update the prediction (p) based on the types of detected enemies.</p>
<p>B. Vision-based Feedback Analysis</p>
<p>The state-of-the-art Vision-Language Models (VLMs), such as gpt-4o with vision capabilities, can process visual inputs to perform tasks like question answering based on a given image.This section investigate the reasoning capabilities of VLMs, specifically evaluating whether they have been trained on and can effectively reason about the distribution of gamerendered images.Fig. 9 illustrates the difference between two input methods: textual input, where a 2D array is represented as plain text, and image input, where the same array is processed in visual form.We evaluated how the modality of information provided when generating feedback influences changes in accuracy performance.</p>
<p>Table V provides quantitative evidence of this improvement.In a chain-of-thought (CoT) reasoning framework, both textual and image inputs show performance gains with the addition of feedback (+FB).For textual input, the accuracy improves from 0.033 to 0.156, marking a gain of +0.122.Similarly, for image input, the accuracy increases from 0.031 to 0.157, with a comparable gain of +0.126.These results suggest that  feedback is highly effective across both modalities, leading to substantial performance enhancements.Moreover, the comparable improvements indicate that both text and image inputs benefit similarly from feedback, emphasizing the importance of leveraging feedback mechanisms in multimodal frameworks to align reward functions with complex, goal-oriented tasks.This also highlights the potential for evaluating and providing feedback on high-dimensional outputs, such as gameplay videos, to improve the quality of generated content in complex scenarios.</p>
<p>C. Hyperparameters</p>
<p>Fig. 1 :
1
Fig. 1: An overview of the reward generation process: (1) instructions guide the LLM, (2) outputs direct the agent, (3) environment interactions refine rewards, and (4) feedback analyzes content for improvement.</p>
<p>Fig. 1 Fig. 2 :
12
Fig. 2: The architectural comparison of three prompt engineering techniques, along with details of the thought nodes.Each thought node includes a reward function (R) and a fitness value (f ), which represent the evaluated score of the contents trained by the agent using the reward function.In the Tree-and Graph-of-Thought methods, the parent node is selected based on the fitness value.</p>
<p>like weights to adjust the reward values, ensuring the logic remains intact.Focus on reward scale and sparsity.</p>
<p>Fig. 3 :
3
Fig. 3: Architecture of PCGRLLM framework."Message icons " indicate the use of language model (M) in the context.Refer to Section V for detailed description.</p>
<p>6 Fig. 4 : 6 :
646
Fig.4: The generated level images are from the iterative reward generation process based on the given instructions.Each map corresponds to an iteration (y), which represents the number of times the reward has been generated and revised by LLMs, and is produced by an agent trained using these LLM-generated reward functions.The asterisk (*) denotes that the generated level satisfies the given instructions.</p>
<p>Fig. 5 :
5
Fig. 5: (a) The dotted lines represent the solutions that achievable to the key.(b) The player enemy encounters enemies within yellow dotted box.</p>
<p>Fig. 6 :
6
Fig. 6: Accuracy comparison across iterations for different prompt engineering types.Error bars represent 95% confidence intervals, with accuracy values annotated at key points.</p>
<p>Fig. 7 :
7
Fig. 7: Accuracy change (âˆ†y) across iterations for Specific, Generic, and No Feedback types.Each bar represents the relative change (âˆ†y nâ†’n+1 ) between consecutive iterations.Positive values indicate improvement, while negative values represent decline.</p>
<p>Fig. 8 :
8
Fig. 8: Accuracy based on the number of auxiliary data in the GoT reward refinement process, categorized by best (columns) and worst (rows) thoughts determined by fitness values.</p>
<p>Algorithm 2 Ï„k 5 â†
25
Enemy Encounter Detection Logic Require: Path finding algorithm P, enemy tiles E Require: Player position P , door position D, key position list K, generated level S T 1: p â† [False, False, False] â–· Initialize prediction array 2: for key k âˆˆ K do 3: Ï„ Sâ†’k â† P(S, k), Ï„ kâ†’D â† P(k, D) 4: if Ï„ Sâ†’k = âˆ… or Ï„ kâ†’D = âˆ… or xâˆˆÏ„ Sâ†’k [x âˆˆ K] P â†’D â† Ï„ Sâ†’k + Ï„ kâ†’D â–· Solution trajectory 8: 5x5 kernel to match enemy tiles 9: for (x, y) âˆˆ Ï„ P â†’D do 10: kernel_tiles â† {S T [x + i, y + j] | (i, j) âˆˆ k 5 } 11:</p>
<p>Fig. 9 :
9
Fig. 9: Comparison of input methods: (a) textual input, where the 2D array is represented as plain text, and (b) image input, where the array is converted into an image format for processing.</p>
<p>TABLE I :
I
Ablation study results for framework architecture, comparing accuracy across prompt engineering methods.
LLM (M)gpt-4o-2024-08-06llama3.2-90b-instructBase+ SA [10]+ FB+ FB &amp; SABase+ SA [10]+ FB+ FB &amp; SAPECoT0.0330.0100.1560.1170.3000.2980.2390.472ToT0.0280.0580.3290.1030.3030.2620.2360.363GoT0.0330.0670.0760.1110.1490.3080.3070.383Mean0.0310.0450.1870.1100.2510.2890.2600.406</p>
<p>TABLE II :
II
Performance between reasoning-based prompt engineering
PEIteration (y i )123456CoT0.033 0.007 0.092 0.109 0.093 0.156ToT0.028 0.172 0.259 0.124 0.113 0.329GoT0.0330.0950.053 0.074 0.042 0.076</p>
<p>TABLE III :
III
Results of an ablation study comparing the performance of heuristic and LLM-based evaluators when feedback is applied to each PE method.
Fitness (F )Heuristic (Oracle)LLM (Self-evaluate)Base+FBâˆ†Base+FBâˆ†PEToT0.0280.329+0.301 0.1060.071-0.034GoT0.0330.076+0.044 0.0620.018-0.044Mean0.0300.203+0.172 0.0840.044-0.039</p>
<p>TABLE IV :
IV
Performance between feedback quality type
Feedback TypeIteration (y i )123456No Feedback0.086 0.063 0.071 0.088 0.0920.099Generic0.034 0.0330.0520.039 0.018 0.031Specific0.033 0.007 0.092 0.109 0.0930.156âˆ†y 1â†’2âˆ†y 2â†’3âˆ†y 3â†’4âˆ†y 4â†’5âˆ†y 5â†’6Iteration</p>
<p>TABLE V :
V
Ablation study results for framework architecture, comparing accuracy across prompt engineering methods and scenarios.
FeedbackTextImageInputBase+FBâˆ†Base+FBâˆ†PECoT0.0330.156+0.122 0.0310.157+0.126</p>
<p>TABLE VI :
VI
Hyperparameters and DRL agent network architecture used in the experiments.
ParameterValuePCGRL Agent SettingBaselinePPOÎ» GAE0.95Epochs size10Rollout length128Minibatch size4Clipping coefficient (Ïµ)0.2Learning rate0.0001Value loss coefficient0.5Entropy coefficient0.01Maximum gradient norm0.5Î³0.99Maximum steps50,000,000Network ArchitectureConv layers
[31,31,3]â†’ [16, 15, 15] [16, 15, 15] â†’ [8, 8, 8] Actor network [4096] â†’ [64] â†’ [2] Critic network [3844] â†’ [64] â†’ [1]</p>
<p>APPENDIXA. Accuracy Evaluation MethodThe accuracy measurement involves two sequential steps:(1) identifying solution trajectories and (2) determining the types of encountered enemies.The encountered enemies are identified based on the solution trajectories, simulating player traversal along these paths.Fig.5aillustrates the process of determining solution trajectories.There are three possible solutions corresponding to the three keys in the level.The trajectory for Key 1 (blue) passes through Key 2 (brown), rendering it ineligible as an independent solution.The algorithm excludes duplicated paths and prioritizes the shortest paths, based on the assumption that players prefer the shortest traversal routes.In contrast, the trajectories leading to Key 2 (brown) and Key 3 (green) do not overlap with those of other keys, making them independent solutions.As a result, the number of solutions is two, corresponding to Key 2 (brown) and Key 3 (green).D. Natural Language PromptReward RefinementINSTRUCTION PCG Agent Reward Function Generation TaskYou are a reward function engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.PCGRL EnvironmentThe gym-pcgrl project provides an environment for Procedural Content Generation via Reinforcement Learning (PCGRL), where agents learn to generate game levels.One of the environments, "binary," involves creating maps using given tile types.In the "narrow" representation, the agent modifies the map by scanning through it, changing tiles one by one.This localized approach encourages more precise level design.The agent receives rewards based on how well the generated level meets the predefined objectives.If the agent generates a level that aligns closely with the goal-such as creating a functional and balanced map-it receives a higher reward.This reward-driven mechanism incentivizes the agent to improve its design strategies over time, learning to create optimal levels through trial and error.Reward FunctionThe reward function is a function that calculates the reward value for the agent based on the playtested results.The array is a 2D array with the shape of (height, width) to represent the game level.The level is represented with tile numbers.The tile number is an integer value.Tile NumberTarget ScenarioThe 'Player' needs to obtain the Key and escape through the 'Door'.To pick up the key, the player must encounter one of the monsters: BAT.The players can figure out all of the monsters when they play the level several times.Ensure that the generated map includes a reasonable number of monsters and keys.First, create a reward function that minimizes the generation of monsters other than those mentioned above.Second, design the reward function so that the map does not contain an excessive number of monsters or keys.Changes Made -Increased the reward for bats and scorpions to 5.0 to encourage their presence more strongly.Self-AlignmentINSTRUCTION-Increased the penalty for spiders to 7.0 to discourage their presence more strongly.-Increased the reward for keys to 4.0 to encourage collecting keys.-Increased the penalty for having more than 2 keys to 4.0 to discourage excessive key collection.-Increased the penalty for having more than 3 total monsters to 6.0 to discourage excessive monster presence.FeedbackINSTRUCTIONEvaluate why the current reward function is producing these game levels Analyze how the reward structure influences the design decisions made by the AI.For each of the following aspects, explain potential issues and how modifying the reward system could lead to better outcomes.The Following was Generated Based on the Current Reward Function Review the generated levels and identify common problems in the contents.Previous Reward Code {previous reward code}Evaluating the Generated Levels:Step-by-Step Analysis 1. Review multiple generated levels and pinpoint common problems in the contents.2. Decide on one specific problem and analyze why the current reward function causes this issue.-If PCGRL fails to train, it could indicate a sparse reward problem.Consider the reward value and frequency in the previous implementation.-Another issue could be overfitting on specific actions, such as repeatedly placing a tile excessively, leading to excessive rewards for certain actions.-Since predicting reward values from a complex function can be difficult, a simpler reward function might resolve training problems.3. Pinpoint problematic parts in the reward function and propose changes.Provide the changed part only of the reward function.RESPONSE Feedback:Step-by-Step Analysis 1. Review of Generated Levels and Common Problems:-Imbalance in Monster Types: Across the generated levels, a common issue is the lack of a balanced distribution of monster types.Some levels have an overrepresentation of one type of monster, while others lack diversity in monster types.-Key and Door Placement Issues: Another issue is the placement of keys and doors.In some levels, keys are placed in locations that are not easily accessible or logical in terms of gameplay flow.Analysis of the Current Reward Function:-Lack of Incentive for Balance:The current reward function provides a reward for having at least one of each required monster type (BAT, SCORPION, SPIDER), but it does not sufficiently incentivize a balanced distribution of these monsters.The penalty for imbalance is relatively small, which may not be enough to encourage diversity.
Deep learning for procedural content generation. J Liu, S Snodgrass, A Khalifa, S Risi, G N Yannakakis, J Togelius, Neural Computing and Applications. 3312021</p>
<p>Pcgrl: Procedural content generation via reinforcement learning. A Khalifa, P Bontrager, S Earle, J Togelius, Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment. the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment202016</p>
<p>Learning controllable content generators. S Earle, M Edwards, A Khalifa, P Bontrager, J Togelius, 2021 IEEE Conference on Games (CoG). IEEE2021</p>
<p>Learning controllable 3d level generators. Z Jiang, S Earle, M Green, J Togelius, Proceedings of the 17th International Conference on the Foundations of Digital Games. the 17th International Conference on the Foundations of Digital Games2022</p>
<p>Eureka: Humanlevel reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv:2310.129312023arXiv preprint</p>
<p>Learning reward for physical skills using large language model. Y Zeng, Y Xu, arXiv:2310.140922023arXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. W Yu, N Gileadi, C Fu, S Kirmani, K.-H Lee, M G Arenas, H.-T L Chiang, T Erez, L Hasenclever, J Humplik, arXiv:2306.086472023arXiv preprint</p>
<p>Auto mc-reward: Automated dense reward design with large language models for minecraft. H Li, X Yang, Z Wang, X Zhu, J Zhou, Y Qiao, X Wang, H Li, L Lu, J Dai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024435</p>
<p>Online intrinsic rewards for decision making agents from large language model feedback. Q Zheng, M Henaff, A Zhang, A Grover, B Amos, arXiv:2410.230222024arXiv preprint</p>
<p>Chatpcg: Large language model-driven reward design for procedural content generation. I.-C Baek, T.-H Park, J.-H Noh, C.-M Bae, K.-J Kim, 2024 IEEE Conference on Games (CoG). 2024</p>
<p>Scaling, control and generalization in reinforcement learning level generators. S Earle, Z Jiang, J Togelius, 2024 IEEE Conference on Games (CoG). IEEE2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. M Besta, N Blach, A Kubicek, R Gerstenberger, M Podstawski, L Gianinazzi, J Gajda, T Lehmann, H Niewiadomski, P Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438690</p>
<p>Where do rewards come from. R L Lewis, S Singh, A G Barto, Proceedings of the International Symposium on AI-Inspired Biology. the International Symposium on AI-Inspired Biology2010</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE202311530</p>
<p>Self-refined large language model as automated reward function designer for deep reinforcement learning in robotics. J Song, Z Zhou, J Liu, C Fang, Z Shu, L Ma, arXiv:2309.066872023arXiv preprint</p>
<p>Text2reward: Automated dense reward function generation for reinforcement learning. T Xie, S Zhao, C H Wu, Y Liu, Q Luo, V Zhong, Y Yang, T Yu, arXiv:2309.114892023arXiv preprint</p>
<p>Raidenv: Exploring new challenges in automated content balancing for boss raid games. H.-C Jeon, I.-C Baek, C -M. Bae, T Park, W You, T Ha, H Jung, J Noh, S Oh, K.-J Kim, IEEE Transactions on Games. 2023</p>
<p>Searchbased procedural content generation: A taxonomy and survey. J Togelius, G N Yannakakis, K O Stanley, C Browne, IEEE Transactions on Computational Intelligence and AI in Games. 332011</p>
<p>JAX: composable transformations of Python+NumPy programs. J Bradbury, R Frostig, P Hawkins, M J Johnson, C Leary, D Maclaurin, G Necula, A Paszke, J Vanderplas, S Wanderman-Milne, Q Zhang, 2018</p>
<p>Generating diverse and natural 3d human motions from text. C Guo, S Zou, X Zuo, S Wang, W Ji, X Li, L Cheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Hierarchical text-conditional image generation with clip latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, arXiv:2204.06125202213arXiv preprint</p>
<p>A Agostinelli, T I Denk, Z Borsos, J Engel, M Verzetti, A Caillon, Q Huang, A Jansen, A Roberts, M Tagliasacchi, arXiv:2301.11325Musiclm: Generating music from text. 2023arXiv preprint</p>
<p>Mariogpt: Open-ended text2level generation through large language models. S Sudhakaran, M GonzÃ¡lez-Duque, M Freiberger, C Glanois, E Najarro, S Risi, Advances in Neural Information Processing Systems. 202336227</p>
<p>Level generation through large language models. G Todd, S Earle, M U Nasir, M C Green, J Togelius, Proceedings of the 18th International Conference on the Foundations of Digital Games. the 18th International Conference on the Foundations of Digital Games2023</p>
<p>A text-to-game engine for ugc-based role-playing games. L Zhang, X Peng, S Yang, F Wang, arXiv:2407.081952024arXiv preprint</p>
<p>Word2world: Generating stories and worlds through large language models. M U Nasir, S James, J Togelius, arXiv:2405.066862024arXiv preprint</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017arXiv preprint</p>
<p>Discovered policy optimisation. C Lu, J Kuba, A Letcher, L Metz, C Schroeder De Witt, J Foerster, Advances in Neural Information Processing Systems. 202235468</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B RoziÃ¨re, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023. 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 1, 6, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2], [2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 1, 3, 1, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2], [2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 22, 2, 4, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2], [2, 1, 5, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2], [2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2], [2, 2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2], [2, 2, 2, 1, 2, 1, 2, 2, 6, 2, 2, 1, 2, 2, 2, 2], [2, 1, 1, 2, 6, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2], [2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2], [2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2], [2, 1, 8, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 2], [2, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1arXiv preprint2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]] :1 :2 :3 :4 :5 :6 :8 :7</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3811 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3811</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3811</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-90.html">extraction-schema-90</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <p><strong>Paper ID:</strong> paper-97c52051e0d5557fbf0f378f5501ebba19277b53</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/97c52051e0d5557fbf0f378f5501ebba19277b53" target="_blank">Transforming the bootstrap: using transformers to compute scattering amplitudes in planar N=4 super Yang–Mills theory</a></p>
                <p><strong>Paper Venue:</strong> Machine Learning: Science and Technology</p>
                <p><strong>Paper TL;DR:</strong> This work shows that transformers can be applied successfully to problems in theoretical physics that require exact solutions.</p>
                <p><strong>Paper Abstract:</strong> We pursue the use of deep learning methods to improve state-of-the-art computations in theoretical high-energy physics. Planar N=4 Super Yang–Mills theory is a close cousin to the theory that describes Higgs boson production at the Large Hadron Collider; its scattering amplitudes are large mathematical expressions containing integer coefficients. In this paper, we apply transformers to predict these coefficients. The problem can be formulated in a language-like representation amenable to standard cross-entropy training objectives. We design two related experiments and show that the model achieves high accuracy ( >98%) on both tasks. Our work shows that transformers can be applied successfully to problems in theoretical physics that require exact solutions.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3811.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3811.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large numbers of scholarly input papers, including details of the methods, domains, results, benchmarks, and challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-symbol bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoder-decoder Transformer models applied to symbol-coefficient prediction in the amplitude bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper applies sequence-to-sequence Transformer models to predict integer coefficients (and zero/nonzero structure) of symbol terms that represent multi-loop scattering amplitudes in planar N=4 Super Yang–Mills theory, treating keys (letter sequences) and integer coefficients as token sequences and training with cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\mathcal{N}=4$ Super Yang-Mills Theory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder–decoder Transformer (small bespoke models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder–decoder Transformers (bidirectional encoder + autoregressive decoder) with between 1 and 8 layers per side, 8–16 attention heads, model hidden dimensions d in {256,512,1024}, and total parameter counts reported between ~4.5 million and ~245 million; learnable positional encodings used. Integer coefficients tokenized in base-1000 with explicit sign token. Models trained with Adam (lr=1e-4) on GPUs (NVIDIA V100/A100).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Symbolic coefficient prediction / symbolic-relationship extraction (interpretable symbolic regression-like task applied to symbol representations of amplitudes)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Theoretical high-energy physics (scattering amplitudes / planar $\mathcal{N}=4$ Super Yang–Mills form factors)</td>
                        </tr>
                        <tr>
                            <td><strong>input_data_type</strong></td>
                            <td>Symbol 'keys' (sequences of letters from a fixed alphabet) and integer coefficients forming the symbol of the amplitude; also compressed representations (quad and octuple suffix compression) and parent-coefficient lists (strike-two parents) for cross-loop prediction experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Sequence-to-sequence supervised training: encode 2L-letter keys (or compressed key representations) and decode integer coefficients (tokenized in base-1000 with sign). Two sub-tasks: (a) binary classification zero vs nonzero, (b) predicting exact nonzero integer coefficients. Compression schemes (quad and octuple) are used to reduce key-space. Cross-loop experiments use lists of 'strike-two' parent coefficients (coefficients at loop L-1 formed by striking two letters) as input to predict coefficients at loop L. Evaluation of learned linear relations (integrability, final-entry, triple-adjacency) is used to probe learning dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Integer coefficients associated with symbol keys (i.e., symbolic-expression terms), plus auxiliary binary zero/nonzero predictions; relation-satisfaction outputs (whether predicted coefficients satisfy known linear relations).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Three-gluon form-factor symbol dataset from the amplitude-bootstrap literature (planar $\mathcal{N}=4$ SYM) up to loop orders L=1..7 (data source cited as ref. [14] in the paper). Variants: full uncompressed symbol, quad-compressed representation (8 final-quad tokens), and (not used here) octuple compression for higher loops.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Element-wise accuracy (binary for zero/nonzero; exact coefficient correctness per element), magnitude-accuracy (correct absolute value), sign-accuracy (correct sign), relation-satisfaction rate (whether predicted coefficients obey specified linear relations), and epoch-based learning curves (reported with confidence intervals derived from binomial statistics on test-set size).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>High element-wise predictive performance on domain data: zero-vs-nonzero classification near 99.9% (e.g., L=5: 99.96% after one epoch; L=6: 99.91% after one epoch). Exact nonzero coefficient prediction: L=5 best run reached 43% after 1 epoch, 95% after 7 epochs, 99% after 16 epochs, 99.5% after 47 epochs; L=6 best run reached 95% after 66 epochs, 98% after 88 epochs, 99.3% after 199 epochs. Quad-compressed representation: L=6 reached 95% in 43 epochs and 99% after 192 epochs (with larger models required), and L=7 quad-trained models reached ~99.1% test-set accuracy with 4-layer d=1024 models. Learning dynamics show a reproducible two-phase pattern: the model first rapidly learns magnitudes (absolute values) of coefficients, then more slowly learns signs. Models also learn many known linear relations (integrability, final-entry, triple-adjacency) progressively during training. Mixed-loop training (combining lower- and higher-loop data) can improve generalization to higher-loop elements when only a small subset of the higher-loop data is available.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scaling: naive training cost grows extremely rapidly with loop order (naive symbol size ~6^{2L}); even with compression, training time and data size become large (L=7 symbol is many millions of terms). Domain shift: coefficient magnitudes grow with loop order (distribution shift), complicating cross-loop generalization. Nontrivial zeros (zeros not explained by simple adjacency/prefix-suffix rules) are plentiful at higher loops and hard to handle. Sign prediction is markedly harder and requires larger model capacity and more data. Length/generalization failure: models trained exclusively at one sequence length (loop) failed to generalize to other lengths (e.g., L=5-trained models evaluated on L=6 achieve at most ~3% accuracy), and can produce nonsensical outputs when length-generalization fails. Compression tradeoffs: quad compression reduces dataset size but removes symmetries and final-entry relations, making the learning task harder and requiring larger models/more passes over data. Model-capacity thresholds: small models sometimes remain stuck near 50% total accuracy because they fail to learn signs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to the traditional amplitude-bootstrap computational pipeline: Transformers provide a complementary approach that can learn to predict coefficients (an otherwise-large system of integer linear equations) and exploit patterns to reduce amount of solved data; traditional bootstrap faces exponential computational growth (authors cite ~16x cost per loop, making L>9 infeasible). Paper situates work relative to other AI/symbolic-math uses of Transformers (prior works cited applying Transformers to symbolic mathematics, polylog simplification, symbolic algebra, reinforcement learning for string vacua), but claims novelty in applying Transformers specifically within the amplitude-bootstrap paradigm. Empirically, small domain-specific Transformers (tens of millions of parameters) suffice here, unlike very large general LLMs; no direct head-to-head numeric comparison to general-purpose LLMs or classical symbolic-regression packages is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_counterexamples</strong></td>
                            <td>Failed length-generalization: models trained only at L=5 do not generalize to L=6 (accuracy ~3%); small models and inadequate data cause failure to learn sign (overall accuracy stalls around 50%), and occasionally models output syntactically invalid coefficient tokens (e.g., '+++') when failing. Quad compression sometimes increases number of training passes needed and demands larger models, showing that compression is not purely beneficial. Cross-loop prediction experiments (predicting loop L coefficients from L-1 'strike-two' parents) are proposed and set up (input is ordered list of parent coefficients), but explicit prediction results for that next-loop task are not reported in the provided text (experiment description appears but outcome not included in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Transforming the bootstrap: using transformers to compute scattering amplitudes in planar N=4 super Yang–Mills theory', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3811",
    "paper_id": "paper-97c52051e0d5557fbf0f378f5501ebba19277b53",
    "extraction_schema_id": "extraction-schema-90",
    "extracted_data": [
        {
            "name_short": "Transformer-symbol bootstrap",
            "name_full": "Encoder-decoder Transformer models applied to symbol-coefficient prediction in the amplitude bootstrap",
            "brief_description": "This paper applies sequence-to-sequence Transformer models to predict integer coefficients (and zero/nonzero structure) of symbol terms that represent multi-loop scattering amplitudes in planar N=4 Super Yang–Mills theory, treating keys (letter sequences) and integer coefficients as token sequences and training with cross-entropy.",
            "citation_title": "Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\\mathcal{N}=4$ Super Yang-Mills Theory",
            "mention_or_use": "use",
            "model_name": "Custom encoder–decoder Transformer (small bespoke models)",
            "model_description": "Encoder–decoder Transformers (bidirectional encoder + autoregressive decoder) with between 1 and 8 layers per side, 8–16 attention heads, model hidden dimensions d in {256,512,1024}, and total parameter counts reported between ~4.5 million and ~245 million; learnable positional encodings used. Integer coefficients tokenized in base-1000 with explicit sign token. Models trained with Adam (lr=1e-4) on GPUs (NVIDIA V100/A100).",
            "task_type": "Symbolic coefficient prediction / symbolic-relationship extraction (interpretable symbolic regression-like task applied to symbol representations of amplitudes)",
            "domain": "Theoretical high-energy physics (scattering amplitudes / planar $\\mathcal{N}=4$ Super Yang–Mills form factors)",
            "input_data_type": "Symbol 'keys' (sequences of letters from a fixed alphabet) and integer coefficients forming the symbol of the amplitude; also compressed representations (quad and octuple suffix compression) and parent-coefficient lists (strike-two parents) for cross-loop prediction experiments.",
            "method_description": "Sequence-to-sequence supervised training: encode 2L-letter keys (or compressed key representations) and decode integer coefficients (tokenized in base-1000 with sign). Two sub-tasks: (a) binary classification zero vs nonzero, (b) predicting exact nonzero integer coefficients. Compression schemes (quad and octuple) are used to reduce key-space. Cross-loop experiments use lists of 'strike-two' parent coefficients (coefficients at loop L-1 formed by striking two letters) as input to predict coefficients at loop L. Evaluation of learned linear relations (integrability, final-entry, triple-adjacency) is used to probe learning dynamics.",
            "output_type": "Integer coefficients associated with symbol keys (i.e., symbolic-expression terms), plus auxiliary binary zero/nonzero predictions; relation-satisfaction outputs (whether predicted coefficients satisfy known linear relations).",
            "benchmark_or_dataset": "Three-gluon form-factor symbol dataset from the amplitude-bootstrap literature (planar $\\mathcal{N}=4$ SYM) up to loop orders L=1..7 (data source cited as ref. [14] in the paper). Variants: full uncompressed symbol, quad-compressed representation (8 final-quad tokens), and (not used here) octuple compression for higher loops.",
            "evaluation_metrics": "Element-wise accuracy (binary for zero/nonzero; exact coefficient correctness per element), magnitude-accuracy (correct absolute value), sign-accuracy (correct sign), relation-satisfaction rate (whether predicted coefficients obey specified linear relations), and epoch-based learning curves (reported with confidence intervals derived from binomial statistics on test-set size).",
            "results_summary": "High element-wise predictive performance on domain data: zero-vs-nonzero classification near 99.9% (e.g., L=5: 99.96% after one epoch; L=6: 99.91% after one epoch). Exact nonzero coefficient prediction: L=5 best run reached 43% after 1 epoch, 95% after 7 epochs, 99% after 16 epochs, 99.5% after 47 epochs; L=6 best run reached 95% after 66 epochs, 98% after 88 epochs, 99.3% after 199 epochs. Quad-compressed representation: L=6 reached 95% in 43 epochs and 99% after 192 epochs (with larger models required), and L=7 quad-trained models reached ~99.1% test-set accuracy with 4-layer d=1024 models. Learning dynamics show a reproducible two-phase pattern: the model first rapidly learns magnitudes (absolute values) of coefficients, then more slowly learns signs. Models also learn many known linear relations (integrability, final-entry, triple-adjacency) progressively during training. Mixed-loop training (combining lower- and higher-loop data) can improve generalization to higher-loop elements when only a small subset of the higher-loop data is available.",
            "limitations_or_challenges": "Scaling: naive training cost grows extremely rapidly with loop order (naive symbol size ~6^{2L}); even with compression, training time and data size become large (L=7 symbol is many millions of terms). Domain shift: coefficient magnitudes grow with loop order (distribution shift), complicating cross-loop generalization. Nontrivial zeros (zeros not explained by simple adjacency/prefix-suffix rules) are plentiful at higher loops and hard to handle. Sign prediction is markedly harder and requires larger model capacity and more data. Length/generalization failure: models trained exclusively at one sequence length (loop) failed to generalize to other lengths (e.g., L=5-trained models evaluated on L=6 achieve at most ~3% accuracy), and can produce nonsensical outputs when length-generalization fails. Compression tradeoffs: quad compression reduces dataset size but removes symmetries and final-entry relations, making the learning task harder and requiring larger models/more passes over data. Model-capacity thresholds: small models sometimes remain stuck near 50% total accuracy because they fail to learn signs.",
            "comparison_to_other_methods": "Compared qualitatively to the traditional amplitude-bootstrap computational pipeline: Transformers provide a complementary approach that can learn to predict coefficients (an otherwise-large system of integer linear equations) and exploit patterns to reduce amount of solved data; traditional bootstrap faces exponential computational growth (authors cite ~16x cost per loop, making L&gt;9 infeasible). Paper situates work relative to other AI/symbolic-math uses of Transformers (prior works cited applying Transformers to symbolic mathematics, polylog simplification, symbolic algebra, reinforcement learning for string vacua), but claims novelty in applying Transformers specifically within the amplitude-bootstrap paradigm. Empirically, small domain-specific Transformers (tens of millions of parameters) suffice here, unlike very large general LLMs; no direct head-to-head numeric comparison to general-purpose LLMs or classical symbolic-regression packages is provided.",
            "notable_counterexamples": "Failed length-generalization: models trained only at L=5 do not generalize to L=6 (accuracy ~3%); small models and inadequate data cause failure to learn sign (overall accuracy stalls around 50%), and occasionally models output syntactically invalid coefficient tokens (e.g., '+++') when failing. Quad compression sometimes increases number of training passes needed and demands larger models, showing that compression is not purely beneficial. Cross-loop prediction experiments (predicting loop L coefficients from L-1 'strike-two' parents) are proposed and set up (input is ordered list of parent coefficients), but explicit prediction results for that next-loop task are not reported in the provided text (experiment description appears but outcome not included in excerpt).",
            "uuid": "e3811.0",
            "source_info": {
                "paper_title": "Transforming the bootstrap: using transformers to compute scattering amplitudes in planar N=4 super Yang–Mills theory",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.011891249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transforming the Bootstrap: Using Transformers to Compute Scattering Amplitudes in Planar $\mathcal{N}=4$ Super Yang-Mills Theory</h1>
<p>Tianji Cai ${ }^{a * \dagger}$, Garrett W. Merz ${ }^{b <em>} \dagger$, François Charton ${ }^{c </em>}$, Niklas Nolte ${ }^{c}$, Matthias Wilhelm ${ }^{d}$, Kyle Cranmer ${ }^{h}$, Lance J. Dixon ${ }^{a}$<br>${ }^{a}$ SLAC National Accelerator Laboratory<br>${ }^{b}$ Data Science Institute, University of Wisconsin-Madison<br>${ }^{c}$ FAIR, Meta<br>${ }^{d}$ Niels Bohr Institute, University of Copenhagen<br>tianji@slac.stanford.edu, garrett.merz@wisc.edu, fcharton@meta.com, nolte@meta.com, matthias.wilhelm@nbi.ku.dk, kyle.cranmer@wisc.edu, lance@slac.stanford.edu Sept 2024</p>
<h4>Abstract</h4>
<p>We pursue the use of deep learning methods to improve state-of-the-art computations in theoretical high-energy physics. Planar $\mathcal{N}=4$ Super Yang-Mills theory is a close cousin to the theory that describes Higgs boson production at the Large Hadron Collider; its scattering amplitudes are large mathematical expressions containing integer coefficients. In this paper, we apply Transformers to predict these coefficients. The problem can be formulated in a language-like representation amenable to standard cross-entropy training objectives. We design two related experiments and show that the model achieves high accuracy ( $&gt;98 \%$ ) on both tasks. Our work shows that Transformers can be applied successfully to problems in theoretical physics that require exact solutions.</p>
<h2>1. Introduction</h2>
<p>Particle physics at the energy frontier is entering an exciting new era of high-precision experiments, ushered in by the high-luminosity upgrade of the Large Hadron Collider (LHC). Exploiting the full physics potential of the experimental data requires substantial improvements in the predictions of Standard Model (SM) [1] processes, both as backgrounds to new physics, and for measuring Higgs boson couplings and other SM parameters.</p>
<p>Many ingredients are necessary for these predictions, see e.g. [2] for a review. At the heart of all such calculations are scattering amplitudes - the fundamental quantummechanical building blocks for transition probabilities between asymptotic states. The</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Sample Feynman diagrams for the process $g g \rightarrow H g$ at two loops (left) and eight loops (right) in QCD. The same diagrams contribute in SYM, where the Higgs boson $H$ and top quark $(t)$ triangle is replaced by a particular local operator in the theory, and the process is referred to as a form factor.
conventional way to compute scattering amplitudes uses Feynman diagrams (see Figure 1 for examples), which graphically organize a series of terms in a perturbative expansion. Performing high-precision calculations in the theory of quantum chromodynamics (QCD) requires Feynman diagrams containing at least two loops [3-10]. Each loop represents intermediate-state virtual particles whose unobserved momenta must be integrated over. Each successive order of precision demands the addition of another loop to the diagram. Unfortunately, the number of possible Feynman diagrams, and thus the number of integrals that must be performed, grows factorially with loop order, quickly making these calculations intractable.</p>
<p>A recently-developed alternative technique, known as the amplitude bootstrap [11-13], attempts to directly construct candidate solutions for multi-loop amplitudes. It has mainly been applied so far to a simpler relative of QCD, called planar $\mathcal{N}=4$ super-YangMills theory (SYM). The amplitude bootstrap circumvents many of the computational and numerical challenges that arise from the Feynman diagram approach. It leverages the rich, yet highly constrained, analytical structure of amplitudes that arises from the particular recurrent features of the integrals involved. Using this technique, the form of the amplitude can be determined a priori, and the finite-dimensional solution space at a given loop order can be strongly constrained through a large system of linear relations with integer coefficients. Many of the linear relations are found by analyzing the lower-loop results. A small number of additional constraints, derived from behavior in physical limits, can then be applied in order to obtain a unique solution.</p>
<p>The amplitude bootstrap allows for the computation of amplitudes up to eight loops in SYM [14], vs. two loops using traditional Feynman diagram methods for the same quantity in QCD [15], as depicted in Figure 1. However, when using the bootstrap technique, the number of linear relations to be solved and the number of unknown coefficients both increase by a factor of about 4 at each subsequent loop order [14]. Since</p>
<p>it is not possible to determine the minimal set of independent equations in advance, the number of relations can be several times larger than the number of unknown coefficients. The computational cost of generating and solving the equations thus increases by at least a factor of $4^{2}=16$ at each loop order. Performing computations by this method becomes infeasible beyond about loop $L=9$. This situation necessitates the development of new methods that may exploit hitherto unobserved patterns in the data in order to simplify the computation of scattering amplitudes at higher loops.</p>
<p>Notably, amplitudes in SYM can be expressed as sets of tokenizable "words" with integer coefficients; see Section 2. We refer to these words as keys which index into the coefficients, and to a key-coefficient pair as an element. Exploiting the many linear relations between these elements amounts to solving a large system of linear equations (as explained above), where the integer solutions are hard to discover, but easy to verify.</p>
<p>In our case, verification includes first checking that the proposed expression is a legitimate function in the appropriate space (see Appendix A), and then comparing its behavior in a physical limit to the predictions of the form-factor operator product expansion (FFOPE) [16,17]. The latter constraint is stringent enough to fix the answer uniquely [14].</p>
<p>Since both the keys and the integers can be represented as sequences of tokens, we can train deep learning models such as Transformers to predict the coefficient associated with each key.</p>
<p>Transformers [18] are incredibly versatile neural network architectures that employ an attention mechanism [19] to learn complex nonlinear relationships between input features. They have revolutionized fields ranging from natural language processing [20] and computer vision [21] to formal symbolic mathematics [22, 23]. Inspired by these many recent successes, we apply Transformers to two sets of experiments; see Section 3 for our model setup.</p>
<p>We first show in Section 4 that Transformers can accurately predict elements of the solution at a given loop order when trained on other elements at the same loop. In Section 5, we show that the model is able to do this even when data is presented in a highly compressed format. To make these predictions successfully, many features of the complex relationships between individual terms must be learned by the model. In the subsequent Section 6, we explore how a number of the known linear relations are learned as a function of training epoch, and use this information to draw conclusions about the learning dynamics of the model. In Section 7, we show that augmenting a small amount of training data at a given loop with data from a lower loop improves performance, and discuss prospects for future multi-loop experiments.</p>
<p>The mathematical structure of the problem hints that some elements of the solution at higher loops may be determined using related elements of the solution at lower loops. Our second goal is therefore to discover this relationship implicitly, assuming it exists. In Section 8, we train Transformers to predict coefficients of terms at loop $L+1$, given a set of coefficients of potentially related terms at loop $L$. We also perform a number of ablations to determine conditions under which this relationship is no longer learnable.</p>
<p>This study allows us to uncover certain features of the cross-loop relationship, which may prove crucial in further developing the bootstrap program.</p>
<p>We emphasize that the goal of these experiments is not to optimize performance. Rather it is to show that properties of amplitudes can be learned by AI models, and to probe the learning dynamics.</p>
<p>Additionally, this paper contains several Appendices. In Appendix A, we further describe the mathematical formalism by which scattering amplitudes are expressible in a language-like fashion, via their symbols, which are sums of pairs of keys and integer coefficients. In Appendix B, we give a more comprehensive list of linear relations between symbol terms and evaluate them as a function of training epoch. In Appendix C, we perform a number of architecture ablations and evaluate their effects on the coefficient prediction task of Section 4. In Appendix D, we perform additional ablation experiments in the manner of Section 8 in order to further characterize the correspondence between elements at different loop orders.</p>
<h1>1.1. Related Work</h1>
<p>The amplitude bootstrap program has a long tradition, dating back to ref. [11]. A recent review can be found in ref. [24], and the specific data we use is from ref. [14]. Our work supplements the traditional approach by offering a novel problem-solving framework, where human intelligence is augmented by artificial intelligence to further push the state-of-the-art for amplitude calculations.</p>
<p>In a similar spirit, a number of recent works have also leveraged deep learning to tackle analytical calculations in theoretical physics. In particular, a sequence-to-sequence Transformer has been used to compute the squared amplitude of a particle interaction symbolically [25]; deep reinforcement learning has been applied to explore the landscape of string vacua [26]; and Transformers have been employed to simplify polylogarithms [27], which are complicated mathematical functions entering multi-loop amplitudes similar to those we study (see Section 2.1). However, no previous work has used Transformers to perform computations in the amplitude bootstrap paradigm.</p>
<p>Methodologically, our work is closely related to recent works using Transformers for symbolic mathematical data. For example, Transformers have been taught to perform mathematical tasks such as solving differential equations [28], learning recurrent sequences [29], and finding the greatest common divisor of number pairs [30]. A comparable approach has also been used to solve linear algebra tasks [23], including eigenvector decomposition and matrix inversion, which share many structural similarities with our amplitude bootstrap method. Additionally, our first experiment, in which we use some elements of a scattering amplitude to predict others, can be framed as a tensor completion problem - at a fixed loop order, the model must learn to fill in unseen elements of the solution based on elements it has seen at training time. This task is similar in some respects to low-rank matrix completion [31], where Transformers have previously been employed [32].</p>
<h1>2. Three-Gluon Form Factors in Planar $\mathcal{N}=4$ SYM Theory</h1>
<p>The amplitude bootstrap program has seen substantial success in planar $\mathcal{N}=4$ superYang-Mills theory [33]. Similar to QCD, SYM contains gluons which self-interact; but instead of including quarks, it contains four gluinos and six scalars. The gluons, gluinos and scalars are all massless, and all transform into each other under the $\mathcal{N}=4$ supersymmetry. They all have the same number of internal "color" degrees of freedom. We take the number of colors $N_{c}$ to infinity, and refer to the Feynman diagrams that contribute to the scattering of these massless particles as planar. As a theoretical laboratory or model system for QCD, SYM allows us to see much further into the perturbative expansion than QCD. For example, a class of SYM amplitudes was recently computed to eight loops [14,34]. These amplitudes, referred to as three-gluon form factors $\mathcal{F}<em 3="3" _mathrm_gFF="\mathrm{gFF">{3 \mathrm{gFF}}$, involve three massless gluons and a massive color-singlet operator. The operator couples to gluons very similarly to how the Higgs boson does in the limit of a very heavy top quark. Thus $\mathcal{F}</em>$ is the SYM analog of the QCD process $g g \rightarrow H g$, which is known only to two loops [15]. In fact, part of the QCD form-factor result (the so-called "highest-weight" part) is identical to the SYM result [35,36]. Figure 1 shows sample Feynman diagrams for this process to the current highest calculable loop order in QCD and SYM, respectively.}</p>
<p>In this work, we focus exclusively on the three-gluon form factors $\mathcal{F}_{3 \mathrm{gFF}}$ in planar $\mathcal{N}=4$ SYM. The known results up to loop $L=7$ are used for model training and evaluation.</p>
<h3>2.1. Symbols: A Simple Language for Amplitudes</h3>
<p>The three-gluon form factors, like many other amplitudes in SYM, can be expressed in terms of functions called generalized polylogarithms. They are multiple, iterated integrations of rational functions. In SYM, the calculation of scattering amplitudes at loop order $L$ requires integrals that are iterated $2 L$ times. Such amplitudes $\mathcal{F}^{(L)}$ can be characterized by another mathematical object known as the symbol [37]:</p>
<p>$$
\mathcal{S}\left[\mathcal{F}^{(L)}\right]=\sum_{l_{i_{1}}, \ldots, l_{i_{2 L}} \in \mathcal{L}<em i__1="i_{1">{m}} C^{l</em>
$$}}, \ldots, l_{i_{2 L}}} l_{i_{1}} \otimes \cdots \otimes l_{i_{2 L}</p>
<p>Here $\mathcal{L}<em 1="1">{m}=\left{l</em>$. Symbol terms encode information about the derivatives and discontinuities of the polylogarithms, and do not correspond directly to individual Feynman diagrams. More details about the map from generalized polylogarithms to symbols are given in Appendix A.}, \ldots, l_{m}\right}$ is the symbol alphabet containing $m$ letters $l_{i}$, which are in turn functions of the particles' four-momenta, and $C^{l_{i_{1}}, \ldots, l_{i_{2 L}}}$ is a $2 L$-fold tensor of integer coefficients, most of which are zero. In other words, a solution for the symbol at loop $L$ can be represented by $m^{2 L}$ integers, with each sequence of $2 L$ letters (i.e., $l_{i_{1}} \otimes \cdots \otimes l_{i_{2 L}}$ ) serving as a key indexing into the integer-valued tensor $C^{l_{i_{1}}, \ldots, l_{i_{2 L}}</p>
<p>The alphabet of $\mathcal{F}_{3 \mathrm{gFF}}$ is one of the simplest among all amplitudes and contains only</p>
<p>six letters, i.e., $m=6$ :</p>
<p>$$
\mathcal{L}_{3 \mathrm{gFF}}={a, b, c, d, e, f}
$$</p>
<p>cf. Appendix A. These letters are Lorentz-invariant functions of the gluons' four-momenta. Via their definition they transform under a dihedral symmetry with two generators:
cycle: ${a, b, c, d, e, f} \rightarrow{b, c, a, e, f, d}$, and flip: ${a, b, c, d, e, f} \rightarrow{b, a, c, e, d, f}$.
The $L$-loop form factor $\mathcal{F}<em 3="3" _mathrm_gFF="\mathrm{gFF">{3 \mathrm{gFF}}^{(L)}$ is invariant under dihedral transformations for any $L$.
As concrete examples, the symbols for $\mathcal{F}</em>$ at loops $L=1$ and $L=2$ contain only 6 and 12 nonvanishing terms, respectively:}}^{(L)</p>
<p>$$
\begin{aligned}
\mathcal{S}\left[\mathcal{F}<em 3="3" _mathrm_gFF="\mathrm{gFF">{3 \mathrm{gFF}}^{(1)}\right]=(-2)[b \otimes d+c \otimes e+a \otimes f+b \otimes f+c \otimes d+a \otimes e] \
\mathcal{S}\left[\mathcal{F}</em>\right]=8[b \otimes d \otimes d \otimes d+c \otimes e \otimes e \otimes e+a \otimes f \otimes f \otimes f \
\quad+b \otimes f \otimes f \otimes f+c \otimes d \otimes d \otimes d+a \otimes e \otimes e \otimes e] \
+16[b \otimes b \otimes b \otimes d+c \otimes c \otimes c \otimes e+a \otimes a \otimes a \otimes f \
\quad+b \otimes b \otimes b \otimes f+c \otimes c \otimes c \otimes d+a \otimes a \otimes a \otimes e]
\end{aligned}
$$}}^{(2)</p>
<p>Usually, we omit the tensor product " $\otimes$ " and use for example "bd" as shorthand for " $b \otimes d$ ", calling it a word or a key. For $L=1$, the key bd then indexes into the tensor $C^{l_{1}, l_{2}}=C^{b, d}$, mapping onto the integer coefficient -2 . As another example, the key ab never appears in $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(1)}\right]$, and therefore ab maps onto a coefficient of 0 . The invariance of the form factor under dihedral transformations (3) relates all terms with the same coefficients in eq. (4) to one another. Hence at $L=1(L=2)$ there are really only 1 (2) nonzero terms to predict.</p>
<p>One can recover the iterated integral representation from the symbol, up to constants that can be recovered in a similar fashion. For the one-loop form factor given in eq. (4), its iterated integral representation is</p>
<p>$$
\mathcal{F}<em 2="2">{3 \mathrm{gFF}}^{(1)}=2\left[\operatorname{Li}</em>}(1-b c)+\operatorname{Li<em 2="2">{2}(1-c a)+\operatorname{Li}</em>(1-a b)\right]
$$</p>
<p>where $\operatorname{Li}<em 0="0">{2}(x)=-\int</em>\right]$ (in a different normalization) is given in eq. (4.32) of ref. [35].}^{x} d t \ln (1-t) / t$. The iterated integral representation of the two-loop form factor $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(2)</p>
<p>In general, $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(L)}\right]$ is a sum of $6^{2 L}$ elements (i.e., the monomials in eq. (4)) containing a key which is a $2 L$-sequence of letters, and an associated integer coefficient. This large number of $6^{2 L}$ elements can be substantially reduced via a set of conditions on the symbol that restrict which letters can appear next to each other. Therefore, most of the $6^{2 L}$ coefficients are zero, and most zeros can be accounted for by the following two simple rules:</p>
<ul>
<li>adjacency rule: any key including one of the subsequences ad, da, de (or their dihedral images) has zero coefficient [14].</li>
<li>prefix/suffix rule: any key beginning with d, e or f or ending with a, b or c has zero coefficient.</li>
</ul>
<p>We call such zero coefficients trivial zeros. Table 1 records the actual numbers of nonzero coefficients in the symbol at different loop orders, as well as the naive $6^{2 L}$ counts and the counts excluding trivial zeros. The table shows that, at loop $L=6$ and higher, about half of the terms allowed by the above two simple rules still have a zero coefficient. We refer to such terms as nontrivial zeros.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Loop</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">6</th>
<th style="text-align: center;">7</th>
<th style="text-align: center;">8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Total $\left(6^{2 L}\right)$</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">1,296</td>
<td style="text-align: center;">46,656</td>
<td style="text-align: center;">$1.7 \cdot 10^{6}$</td>
<td style="text-align: center;">$6.0 \cdot 10^{7}$</td>
<td style="text-align: center;">$2.2 \cdot 10^{9}$</td>
<td style="text-align: center;">$7.8 \cdot 10^{10}$</td>
<td style="text-align: center;">$2.8 \cdot 10^{12}$</td>
</tr>
<tr>
<td style="text-align: left;">W/O trivial zeros</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">1,830</td>
<td style="text-align: center;">32,838</td>
<td style="text-align: center;">589,254</td>
<td style="text-align: center;">$1.1 \cdot 10^{7}$</td>
<td style="text-align: center;">$1.9 \cdot 10^{8}$</td>
<td style="text-align: center;">$3.4 \cdot 10^{9}$</td>
</tr>
<tr>
<td style="text-align: left;">Total nonzero</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">636</td>
<td style="text-align: center;">11,208</td>
<td style="text-align: center;">263,880</td>
<td style="text-align: center;">$4.9 \cdot 10^{6}$</td>
<td style="text-align: center;">$9.3 \cdot 10^{7}$</td>
<td style="text-align: center;">$1.7 \cdot 10^{9}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Elements in the symbol $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}^{(L)}\right]$ for loops $L=1$ to 8 .</p>
<p>Figure 2 shows the distribution of magnitudes of nonzero integer coefficients of $\mathcal{S}\left[\mathcal{F}_{3 \mathrm{gFF}}\right]$ at loops $L=4,5,6$ on a log scale. Note that the magnitudes of the coefficients grow quickly from lower to higher loops, a domain shift which may pose a challenge when attempting to train a model to generalize across loop orders.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Histograms of the symbol coefficients for the three-gluon form factor at 4,5 , and 6 loops. The horizontal axis is the base 10 logarithm of the magnitude of the coefficient. The vertical axis is the (arbitrarily normalized) frequency with which coefficient magnitudes occur in the form factor.</p>
<h1>2.2. Linear Relationships among Symbol Elements</h1>
<p>In addition to the trivial zeros defined above, there exist many linear correlations in the symbol which highly constrain the values of many coefficients. We study three types of linear relations in this work - the integrability relations, the multiple-final-entry relations, and the triple-adjacency relation.</p>
<p>Many of these constraints are inspired by empirical observations and have deep physical roots yet to be understood. However, some are based on rather elementary mathematical considerations. For example, one important constraint is functional integrability: a random multi-variate symbol is not the symbol of any function, because mixed partial derivatives must commute. This requirement correlates large sets of</p>
<p>coefficients with specific adjacent letter pairs. One such integrability relation reads</p>
<p>$$
F^{a, b}+F^{a, c}-F^{b, a}-F^{c, a}=0
$$</p>
<p>which correlates the coefficients of four terms in the symbol at a time. Here $F^{a, b}$ is the abbreviation of $C^{l_{1}, \ldots, l_{i-1}, a, b, l_{i+2}, \ldots, l_{2 L}}$, the coefficient corresponding to the key " $\ldots \mathrm{ab} \ldots$..." where the letter pair ab can appear in any pair of adjacent positions (or slots) in the key. The remaining letters (indicated by "...") may take any values but must be the same for each of the terms in the relation. In other words, eq. (6) can be written equivalently as</p>
<p>$$
\begin{aligned}
&amp; C^{l_{1}, \ldots, l_{i-1}, a, b, l_{i+2}, \ldots, l_{2 L}}+C^{l_{1}, \ldots, l_{i-1}, a, c, l_{i+2}, \ldots, l_{2 L}} \
&amp; -C^{l_{1}, \ldots, l_{i-1}, b, a, l_{i+2}, \ldots, l_{2 L}}-C^{l_{1}, \ldots, l_{i-1}, c, a, l_{i+2}, \ldots, l_{2 L}}=0
\end{aligned}
$$</p>
<p>for any choice of $l_{1}, \ldots, l_{i-1}, l_{i+2}, \ldots, l_{2 L} \in \mathcal{L}_{\text {3gFF }}$, and it remains valid for any position $i$ in the $2 L$-length keys.</p>
<p>A concrete instance of relation (7) at five loops is</p>
<p>$$
C^{c, a, b, c, a, b, d, c, c, d}+C^{c, a, c, c, a, b, d, c, c, d}-C^{c, b, a, c, a, b, d, c, c, d}-C^{c, c, a, c, a, b, d, c, c, d}=0
$$</p>
<p>where the relevant adjacent letter pairs are underlined and the four integer coefficients are $72,-88,-72,56$, respectively. We thus have $72+(-88)-(-72)-56=0$, satisfying eq. (8). Another integrability relation correlates 14 coefficients at a time, the longest linear relation currently known.</p>
<p>While the integrability and triple-adjacency relations occur in all adjacent pairs of slots in the key, the final-entry conditions relate only sets of keys that have the same beginnings, but different suffixes.</p>
<p>In general, these linear relationships can serve as an excellent probe of the learning dynamics of the models, informing us about which properties of the symbol are learned at different training stages. We explore a subset of them in Section 6, and give a more comprehensive discussion in Appendix B.</p>
<h1>2.3. Compact Symbol Representations</h1>
<p>With increasing loop order, the number of elements in the symbol becomes very large, as can be seen in Table 1: at 8 loops, the symbol has around 1.7 billion terms. A compact symbol representation is therefore necessary at high loops. We derive one such compact representation by noticing that the number of independent suffixes for keys of nonzero coefficients is very limited.</p>
<p>Applying the multiple-final-entry relations (see Appendix B) and dihedral symmetry, one notices that all terms in the symbol can be related to terms ending in the following 8 sequences of four letters: dddd, bbbd, bdbd, bbdd, dbdd, fbdd, dbbd, and cddd. We thus create a new quad representation where 8 new tokens are added to represent these 8 suffixes. All keys at a given loop order are then represented by their first $2 L-4$ letters plus one of the eight quad suffix letters.</p>
<p>Represented in the quad format, there are only 391,570 keys for the loop $L=6$ symbol, in contrast to the 5 million keys in the original uncompressed format. Furthermore, compressing the data using the quad representation naturally removes the dihedral symmetry and all multiple-final-entry relations that involve only the last four entries. It is therefore interesting to see how the model will perform when it is presented with the more "efficient" quad format, versus having to learn these relations from the full symbol.</p>
<p>An even more compact octuple representation can be achieved by considering the last 8 letters in each element. There are 93 possible final-entry octuples after factoring out dihedral symmetry. This representation gives 16,971 keys in the $L=6$ symbol, 312,463 in the $L=7$ symbol, and 5.6 million in the $L=8$ symbol. In the current study, we do not use the octuple representation, since the highest loop order under consideration is $L=7$. However, the octuple representation may become necessary in the future to push past $L=8$, the highest loop order currently known.</p>
<h1>3. Implementation Details</h1>
<p>In this section, we briefly discuss the default architecture and tokenization scheme for the later experiments.</p>
<p>Due to the discrete nature of our problem, all tasks are framed as sequence-tosequence translation problems: coefficients and keys are both encoded as sequences of tokens, and the model is trained to minimize the cross-entropy of the probability distribution for the predicted coefficient sequence with the ground-truth solution. At loop $L$, keys are encoded as sequences of $2 L$ letter tokens, e.g., 'a, a, b, d, d, c, e, e'. While several recent works have explored different ways to tokenize integers [38,39], we simply encode coefficients as sequences of numerical tokens in base 1000; e.g., 12334 as ' + , 12, 334 ', with the sign first [29, 40]. To preserve syntax, zero coefficients are arbitrarily assigned a sign token of '+'.</p>
<p>In most experiments, we use encoder-decoder Transformers, which contain a bidirectional Transformer encoder and an autoregressive Transformer decoder linked by a cross-attention mechanism [18]. Both encoder and decoder have the same number of layers (up to 8), the same number of attention heads ( 8 or 16), and the same dimension $(d=256, d=512$ or $d=1024)$. For all models, the tokenizer dimension and Transformer dimension $d$ are the same (i.e., 256, 512, or 1024). Henceforth, we describe a model with $N$ layers in the encoder and $N$ layers in the decoder as an $N$-layer Transformer.</p>
<p>Overall, our models have between 4.5 and 245 million trainable parameters, and the best performance on many of our experiments is obtained with models with fewer than 35 million parameters. In contrast, many popular large language models [41,42] have tens of billions of parameters. Following similar observations on AI-for-mathematics applications [23], we are able to achieve very good results with these small Transformers trained on domain-specific data. In all experiments except where noted, we use the smallest model that can perform the task with large ( $&gt;98 \%$ ) accuracy, where accuracy</p>
<p>is defined per element rather than per individual token.
In the default model, we use a learnable positional encoding in both the encoder and decoder [18]; alternative schemes are explored in Appendix C. The optimizer is Adam [43], with a learning rate of $10^{-4}$ and a flat learning rate schedule. We do not observe significant performance improvements from changes to the learning rate schedule or addition of learning-rate warmup. We believe this is due to the small size of the Transformers we use: in our experience, warmup becomes more useful for larger models.</p>
<p>All models are implemented in PyTorch [44] and trained on a single NVIDIA V100 GPU with 32 GB of memory, or on larger architectures (A100).</p>
<p>Throughout the paper, we define an epoch as a pass over 300,000 key-coefficient pairs, as opposed to the more common definition of one full pass over all training data. This makes the notion of epoch size more comparable between different experiments that employ different amounts of data at different loops. At the end of each epoch, the model is evaluated on a held-out test set. The train-test split is performed randomly. In all experiments, metrics such as accuracy are quoted on the held-out test set, not the full symbol. For example, for a training set of 100,000 elements and a test set of 1,000 elements, an accuracy of $90 \%$ refers to predicting 900 of those 1,000 elements correctly.</p>
<p>Accuracies are measured on a test set of $N_{\text {test }}$ randomly sampled elements. The accuracy for each element is a binary variable (correct $=1$, wrong $=0$ ), so we may assume that the errors are binomial. Applying the central limit theorem to the average, we have a $95 \%$ confidence interval of $1.96 \times \sqrt{a(1-a) / N_{\text {test }}} \approx 2 \sqrt{(1-a) / N_{\text {test }}}$ due to statistical error, for an accuracy $a \approx 1$. For $a=99 \%$ and a test set size of $N_{\text {test }}=10,000$, this interval is $0.2 \%$. There is also uncertainty in the accuracy due to the model's initialization.</p>
<p>Because the goal of these experiments is not to optimize final accuracy, we do not perform any fine-grained hyperparameter scans. For this reason, we do not typically have a separate validation set in addition to the test set. However, in order to determine whether the specific choice of training and test set might play a role, in Appendix C. 2 we repeat one of our experiments for a few different train/test splits using the same set of hyperparameters chosen for the main work. We find qualitatively similar performance.</p>
<h1>4. Predicting Symbol Coefficients from Keys</h1>
<p>In the first experiment, we train Transformers to predict coefficients from their keys. The models are trained on a fraction of the symbol at a given loop, $L=5$ or 6 , and are tasked to predict the remaining terms. Because most coefficients are zero, we split the problem into two separate tasks: predicting whether the coefficient corresponding to a given key is zero, and predicting a nonzero coefficient from the associated key.</p>
<p>We first train 1-layer Transformers with dimension $d=256$ and 8 attention heads (i.e., 4.5 million parameters) to predict whether coefficients are zero or nonzero. We construct a dataset consisting of all nonzero-coefficient elements in the symbol plus an equal number of zero-coefficient elements. Here, zero-coefficient elements are selected</p>
<p>randomly from the pool of all possible zeros, and the majority of zeros are thus trivial (as defined in Section 2.1). We explore an alternative prescription to handle the nontrivial zeros in Section 6.</p>
<p>At $L=5$, the model correctly classifies $99.96 \%$ of elements in a test set of 10,000 examples, after only one epoch (corresponding to observing only $57 \%$ of the symbol). At $L=6$, the model correctly classifies $99.91 \%$ of the test set after one epoch, i.e., after observing only $3 \%$ of the symbol, and $99.97 \%$ after two epochs (i.e., after observing $6 \%$ of the symbol). Distinguishing nonzero coefficients from these mostly trivial zeros thus appears to be a very simple problem for Transformers.</p>
<p>Predicting nonzero coefficients from their keys proves to be more difficult, necessitating larger models. We train 2-layer Transformers with dimension $d=512$ and 8 attention heads, for loops $L=5$ and $L=6$. At $L=5$, models are trained on 163,880 nonzero-coefficient elements (i.e., $62 \%$ of the symbol), and tested on 100,000 elements. At $L=6$, models are trained on $4,816,466$ nonzero-coefficient elements (i.e., $98 \%$ of the symbol), and are again tested on 100,000 elements.</p>
<p>At $L=5$, the best model (out of four initializations, or seeds) correctly predicts $43 \%$ of the coefficients from the test set after only one epoch. Accuracy is $95 \%$ after 7 epochs, $99 \%$ after 16 epochs and $99.5 \%$ after 47 epochs. At $L=6$, the best model (again out of four initializations) correctly predicts $95 \%$ of the coefficients in the test set after 66 epochs, $98 \%$ after 88 epochs and $99.3 \%$ after 199 epochs. We show accuracy as a function of epoch in Figure 3.</p>
<p>At both $L=5$ and 6 , learning proceeds in two qualitative phases: first the magnitudes of the coefficients are learned, then the signs. At $L=5$, after two epochs, $97 \%$ of magnitudes are correctly predicted, but the signs are predicted at near chance level ( $50 \%$ ). After 5 epochs, $99 \%$ of magnitudes are predicted correctly, but only $78 \%$ of signs are predicted correctly. By epoch $10,98 \%$ of signs are predicted correctly. For $L=6$, training follows the same pattern, but proceeds more slowly: the model learns the magnitudes of coefficients during the first 20 epochs. Accuracy then saturates around $50 \%$, while the model predicts the magnitudes of coefficients with more than $95 \%$ accuracy, but predicts their signs at near chance level. Finally, from epoch 40 to 70, the model learns to correctly predict the signs of the coefficients.</p>
<p>Until the sign is learned, the model strongly prefers to predict one sign over the other in each epoch: predictions may for example flip from $98 \%$ ' - ' signs in one epoch to $90 \%$ ' + ' signs in the next. This preference gradually diminishes with epoch, decreasing to within a few percent of the true proportion of positive and negative signs (which is very close to $50 \%$ positive and $50 \%$ negative) roughly at the midpoint of the second step (epoch 7 for $L=5$; epoch 60 for $L=6$ ). We indicate this behavior for one representative run in the lower portion of Figure 3. These fluctuations are not restricted to a particular subset of magnitudes; when the true or predicted magnitude is restricted to a given value, similar gradually-decreasing fluctuations are observed.</p>
<p>Furthermore, when models are trained to predict only the magnitudes of nonzero coefficients at $L=6$ (by setting all signs to ' + '), the best model can do so at $97.7 \%$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Accuracy vs. epoch on the nonzero coefficient-from-key task at loop $L=5$ (left) and $L=6$ (right), for four model initializations shown in different colors. The bottom plots show the balance of predicted signs vs. epoch, with $+(-)$ indicating $100 \%$ ( $0 \%$ ) positive signs. Initially the model fluctuates between strongly favoring one sign or the other before more accurately predicting the mix of signs for individual terms.
accuracy after 50 epochs. However, when models are trained to predict only the signs of the nonzero coefficients (by setting all magnitudes to ' 1 '), they exhibit random guessing behavior even after 100 epochs. These results suggest that learning the magnitude of the coefficient may be a prerequisite for learning the sign.</p>
<p>In summary, our results indicate that Transformers trained on a small fraction of the symbol can predict coefficients from their keys with very high accuracy.</p>
<h1>5. Quad Representation of Symbols</h1>
<p>At higher loops, a more compact representation of the form factor symbol is necessary for efficient training. For example, the loop $L=7$ symbol has 93 million nonzero-coefficient elements, almost 19 times as many as the $L=6$ symbol. How long might it take to train on such a large symbol? At loop $L=5$, performing the nonzero coefficient-from-key prediction task to $&gt;99 \%$ accuracy takes about 22 passes through the training set and 0.7 hours. At loop $L=6$, the same task takes 11 passes through the training set and over 54 hours. The time to reach $&gt;99 \%$ accuracy scales at least linearly with the number of elements. If we assume this scaling continues to $L=7$, it will take at least $54 \times 19 \approx 1000$ hours, or 43 days.</p>
<p>Compressing the data using the quad representation can significantly improve the</p>
<p>model training speed by reducing the number of nonzero-coefficient elements to a more manageable 7.3 million. Under the linear scaling paradigm, we would predict a more manageable 80 hours, or under 4 days.</p>
<p>However, learning coefficients from keys in the quad representation is a harder problem than in the full representation. The quad representation eliminates many of the obvious symmetries in the symbol: it removes both the dihedral symmetry and relations involving up to four final entries. Thus, many potential sources of correlation between the training and test sets are no longer present, and the model is forced to learn more subtle correlations between coefficients and keys in order to correctly predict the coefficients in the test set. Larger models are therefore required for this task.</p>
<p>Here we train models to predict nonzero coefficients from keys in the quad representation at $L=6$ and $L=7$. For $L=6$, at which there are 391,570 quad keys, we use 4-layer Transformers with dimension $d=512$ and 8 attention heads; if we train smaller 2-layer Transformers with the same dimensions, we are unable to reach above $50 \%$ accuracy even after 100 epochs of training. We train on 381,570 key-coefficient pairs and test on the 10,000 held-out elements.</p>
<p>The training curves for the quad representation exhibit a two-step shape very similar to those for the uncompressed representation: during the first 10 epochs, only the magnitudes of coefficients are learned and their signs are predicted at chance level. The model achieves an accuracy of $95 \%$ in 43 epochs, which equates to 34 passes over the compressed training set since an epoch is fixed at 300,000 examples. The accuracy peaks at $99 \%$ after 192 epochs, equivalent to 152 passes over the compressed training set. However, the full representation achieves $95 \%$ accuracy in 64 epochs, which equates to slightly less than 4 passes through the uncompressed training set. Thus, although the larger models trained on the quad representation can reach performance benchmarks in fewer epochs than the smaller models trained on the full representations, they in fact take more passes through the training set in order to converge, confirming our intuition that training on the quad representation is a more challenging task.</p>
<p>For $L=7$, even larger models are required. 4-layer Transformers with dimension $d=1024$ and 16 attention heads are trained on the entire quad-compressed $L=7$ symbol (i.e., 7.3 million elements minus the held-out test set of 10,000 elements), achieving $99.1 \%$ accuracy on the test set. Here, training is considerably slower: the signs only begin to be learned after 100 epochs ( $v s .10$ epochs for the $L=6$ symbol). The model reaches $98.5 \%$ accuracy in about 400 epochs, which equates to 120 million examples, or 16 passes over the training set. The larger training set partially accounts for this difference in learning speed.</p>
<p>In order to explore the relationship between model capacity and training set size, we present in Table 2 the final overall test-set accuracy for the $L=7$ symbol in the quad representation for different model hyperparameters and training set sizes. We indicate models that fail to achieve at least $90 \%$ accuracy in gray. Accuracy decreases as training set size decreases, but remains above $94 \%$ for all but the smallest model, as long as the models are trained on 3 million examples or more. Such a training set</p>
<p>translates to only about $41 \%$ of the symbol elements, which suggests that the models still possess significant predictive power even when data is given in the compressed quad representation.</p>
<p>The Transformer's ability to learn the sign appears to be governed largely by model capacity and training dataset size. Thus, in regions where the model sees enough data and is large enough to consistently learn the sign, accuracy is consistently $&gt;90 \%$, while in regions where the model is too small, the sign is not learned, and accuracy stays at or below $50 \%$. When we are near a model capacity threshold (for this task, 4 layers and $d=512$ ) the model sometimes learns the sign and sometimes does not.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">7.3 M</th>
<th style="text-align: center;">7 M</th>
<th style="text-align: center;">6 M</th>
<th style="text-align: center;">5 M</th>
<th style="text-align: center;">4 M</th>
<th style="text-align: center;">3 M</th>
<th style="text-align: center;">2 M</th>
<th style="text-align: center;">1 M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">8 layers, $d=1024$</td>
<td style="text-align: center;">$98.8 \%$</td>
<td style="text-align: center;">$98.7 \%$</td>
<td style="text-align: center;">$98.2 \%$</td>
<td style="text-align: center;">$97.5 \%$</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$94.8 \%$</td>
<td style="text-align: center;">$90.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">8 layers, $d=512$</td>
<td style="text-align: center;">$96.2 \%$</td>
<td style="text-align: center;">$97.4 \%$</td>
<td style="text-align: center;">$98.4 \%$</td>
<td style="text-align: center;">$96.6 \%$</td>
<td style="text-align: center;">$95.3 \%$</td>
<td style="text-align: center;">$93.8 \%$</td>
<td style="text-align: center;">$88.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">6 layers, $d=1024$</td>
<td style="text-align: center;">$98.6 \%$</td>
<td style="text-align: center;">$98.9 \%$</td>
<td style="text-align: center;">$98.0 \%$</td>
<td style="text-align: center;">$97.9 \%$</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$94.8 \%$</td>
<td style="text-align: center;">$90.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">6 layers, $d=512$</td>
<td style="text-align: center;">$95.2 \%$</td>
<td style="text-align: center;">$96.6 \%$</td>
<td style="text-align: center;">$96.9 \%$</td>
<td style="text-align: center;">$95.8 \%$</td>
<td style="text-align: center;">$94.4 \%$</td>
<td style="text-align: center;">$94.5 \%$</td>
<td style="text-align: center;">$87.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">4 layers, $d=1024$</td>
<td style="text-align: center;">$99.1 \%$</td>
<td style="text-align: center;">$98.9 \%$</td>
<td style="text-align: center;">$98.3 \%$</td>
<td style="text-align: center;">$97.9 \%$</td>
<td style="text-align: center;">$96.6 \%$</td>
<td style="text-align: center;">$94.9 \%$</td>
<td style="text-align: center;">$89.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">4 layers, $d=512$</td>
<td style="text-align: center;">$48.5 \%$</td>
<td style="text-align: center;">$96.0 \%$</td>
<td style="text-align: center;">$94.1 \%$</td>
<td style="text-align: center;">$48.3 \%$</td>
<td style="text-align: center;">$94.6 \%$</td>
<td style="text-align: center;">$81.7 \%$</td>
<td style="text-align: center;">$55.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2. Maximum test-set accuracy after 250 epochs at loop 7 in the quad representation, for various training set sizes as well as numbers of layers and dimensions. The best of two models is shown. All models have 16 heads. (The smallest model occasionally does not emerge from the first plateau, with its accuracy then staying below $50 \%$.)</p>
<h1>6. Model Characterization via Relationship Accuracy</h1>
<p>The results of the previous experiments strongly suggest that the model leverages certain correlations that are present in the data, such as dihedral symmetry and the final-entry relations described in Section 2.2, in order to more easily extrapolate from the training set into the test set. Additionally, the unusual two-phase accuracy curves that occur in both the full and quad representations warrant further investigation. In this section, we therefore explore how the linear relations behave as a function of epoch in order to better understand how the model learns.</p>
<p>We define an instance of a relation as a set of keys and their associated coefficients that obey a given relation. For example, for the relation given in eq. (6), a sample instance at 5 loops is, as in eq. (8):</p>
<p>$$
C^{c, a, b, c, a, b, d, c, c, d}+C^{c, a, c, c, a, b, d, c, c, d}-C^{c, b, a, c, a, b, d, c, c, d}-C^{c, c, a, c, a, b, d, c, c, d}=0
$$</p>
<p>which corresponds to the following set of keys and coefficients: {cabcabdccd: 72, caccabdccd: -88 , cbacabdccd: -72 , ccacabdccd: 56 }.</p>
<p>We generate 500 instances of each homogeneous linear relation at loop $L$ and use them to evaluate the performance of a model trained on the coefficient-from-key</p>
<p>prediction task as in Section 4. To do so, we randomly generate a set of keys that obey the given relation and then pair them with the corresponding coefficients. We discard and re-generate all such multi-term instances that do not contain at least one nonzero coefficient. In this way, we avoid instances with all zero terms that are trivially satisfied.</p>
<p>The relation instances generated are used only as an auxiliary test set. The training set consists of the full nonzero symbol plus an equal proportion of zeros as in Section 4, while the coefficient-from-key test set is still employed as before. We note that more than $99 \%$ of the nonzero terms in the relation instances appear in the training set. However, we stress that this does not constitute data leakage, as the relation evaluations are auxiliary dataset-level metrics that do not influence training.</p>
<p>The linear relations may relate nonzero terms to the nontrivial zeros, which constitute a very small fraction of all possible zeros. Therefore, when choosing the zeros to be added to the training set, we explicitly select a large proportion of nontrivial zeros. The fraction of trivial zeros in the training set is restricted to be $5 \%$ of all zeros. Prioritizing the nontrivial zeros causes accuracy to decrease slightly on the trivial zeros; however, as trivial zeros are easy to learn and to identify (as per Section 4) we can simply manually set the predicted coefficient to zero for any trivial-zero terms in a relation instance.</p>
<p>The model used for this experiment is a 2-layer Transformer with $d=512$ and 8 heads, trained on 9,732,932 elements (i.e., the full $L=6$ symbol plus an equal proportion of zeros), and tested on 100,000 randomly chosen held out elements. After 200 epochs, the model correctly predicts $98.47 \%$ of the coefficients in the test set. The learning curves again reveal the familiar two qualitative phases, though training to a given accuracy now takes twice as many epochs (due to the addition of zeros increasing the dataset size by a factor of two) and overall magnitude and sign accuracy both reach $50 \%$ accuracy within the first epoch, due to the fact that zeros are learned quickly.</p>
<p>A complete list of the relations we evaluate is given in Appendix B. Here, we only describe the following short relations, which form a representative subset of the different types of relations studied. In the triple and integrability relations, the specified adjacent slots can appear anywhere in the key, while the specified adjacent slots in the final-entry relations must appear at the end of the key (which we denote by $\mathcal{E}$ instead of $F$ ).</p>
<p>$$
\begin{gathered}
\text { triple 0: } \quad F^{a, a, b}+F^{a, b, b}+F^{a, c, b}=0 \
\text { integ 0: } \quad F^{a, b}+F^{a, c}-F^{b, a}-F^{c, a}=0 \
\text { integ 1: } \quad F^{c, a}+F^{c, b}-F^{a, c}-F^{b, c}=0 \
\text { final 16: } \quad \mathcal{E}^{b, f}-\mathcal{E}^{b, d}=0 \
\text { final 17: } \quad \mathcal{E}^{c, d, d}+\mathcal{E}^{c, e, e}=0 \
\text { final 18: } \quad \mathcal{E}^{d, d, b, d}-\mathcal{E}^{d, b, d, d}=0
\end{gathered}
$$</p>
<p>For each relation, we quote four metrics: 1) whether the coefficients predicted by the model satisfy the given relation, regardless of whether the individual coefficients themselves are correct (red); 2) whether the relation is satisfied and all coefficients in</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Relation accuracy (red), magnitude accuracy (blue), sign accuracy (yellow), and coefficient accuracy (green) for each of the named relations, grouped by behavior. Relations in Group 1 (left column) are two-term equivalence relations that are consistently satisfied after only a few epochs. Relations in Group 2 (center column) are relations that require at least two coefficients to have different signs, and are not satisfied until the second phase. Relations in Group 3 (right column) are mixed relations, for which some instances decompose into pairs of equivalent terms (as in Group 1 relations) while others do not.
the instance have the correct magnitudes, regardless of their signs (blue); 3) whether the relation is satisfied and all coefficients in the instance have the correct signs, regardless of their magnitudes (yellow); and 4) whether all coefficients in the instance are correct (green). We plot these metrics as a function of epoch in Figure 4. Similar results for additional relations are provided in Appendix B.</p>
<p>The linear relations can be grouped by structure into three categories that also define their behavior. The six relations discussed in this section contain two examples from each category.</p>
<ul>
<li>
<p>Group 1 relations, such as eq. (13) (final 16) and eq. (15) (final 18), are equivalence relations that require two coefficients to have the same magnitudes and signs. For all Group 1 relations, the relation is often satisfied before all the magnitudes are predicted correctly - the model predicts that both coefficients in the instance must be the same before it is able to successfully identify what that coefficient is. While the model learns the magnitudes of both coefficients in the relation instances fairly quickly, the signs of the coefficients are only predicted correctly $50 \%$ of the time until the sign is learned; however, both coefficients are consistently predicted to have the same sign. Thus, the sign fluctuations described in Section 4 (Figure 3) largely respect the Group 1 relations.</p>
</li>
<li>
<p>Group 2 relations, such as eq. (10) (triple 0) and eq. (14) (final 17), are those in which at least two coefficients must have opposite signs. While the magnitudes of coefficients in these relation instances are learned within a few epochs, accuracy on all relation metrics remains low, but steadily increasing, until the signs are learned. This behavior is also largely dictated by the sign fluctuations shown in Figure 3.</p>
</li>
<li>Group 3 relations, such as eq. (11) (integ 0) and eq. (12) (integ 1), are multi-term relations that may be satisfied by two or more pairs of identical coefficients or by a set of related but nonidentical coefficients (e.g., $40 \%$ of the generated (integ 0 ) instances are expressible as pairs of identical coefficients). Under the conditions where the model has a high probability of satisfying the Group 1 relations, the model predictions for the subset of Group 3 relation instances expressible as pairs of identical coefficients will also satisfy the relation.</li>
</ul>
<p>These properties suggest an explanation for the double plateau behavior: first, the model learns to group elements whose coefficients have the same magnitude; then it learns to correctly predict those magnitudes. The model predictions for the sign fluctuate from epoch to epoch - rather wildly at first - until the second accuracy step is reached. However, these fluctuations consistently respect the Group 1 equivalence relations between elements. During this fluctuation phase, the model also gradually learns the Group 2 relations; i.e., the model learns which coefficients with a given magnitude have the same sign and which do not. These fluctuations get smaller until the sign is eventually learned.</p>
<p>Both the cycle and flip symmetries can be expressed as Group 1 relations relating pairs of terms with identical coefficients. We plot the relation evaluation metrics for these relations in Appendix B and find that they behave similarly to other Group 1 relations.</p>
<p>We observe a further intriguing manifestation of the dihedral symmetry in the geometry of the embedding layer representation. Specifically, we extract the learned $d$-dimensional embeddings of the input letter tokens from the embedding layer of the Transformer and calculate the angles between them. These embedding vectors obey both the cycle and flip symmetries. We perform such an extraction experiment with a 2-layer Transformer with $d=512$, first at $L=5$ and then at $L=6$.</p>
<p>At $L=5$, the triangles $\triangle a b c$ and $\triangle d e f$ are approximately equilateral: all angles are within $1.5^{\circ}$ of $60.0^{\circ}$ for $\triangle a b c$ and within $2.7^{\circ}$ of $60.0^{\circ}$ for $\triangle d e f$. This result indicates that the embedding vectors obey the cycle symmetry. Similarly, all angles are within $3.7^{\circ}$ of $60.0^{\circ}$ for triangle $\triangle a b f$, within $1.5^{\circ}$ of $60.0^{\circ}$ for $\triangle b c d$, and within $3.3^{\circ}$ of $60.0^{\circ}$ for $\triangle a c e$; the fact that these triangles are approximately similar indicates that the embedding vectors obey the flip symmetry.</p>
<p>At $L=6$, we observe the same phenomenon even more strongly: $\triangle a b c$ and $\triangle d e f$ are approximately equilateral: all angles are within $1.0^{\circ}$ of $60.0^{\circ}$ for $\triangle a b c$ and within $0.6^{\circ}$ of $60.0^{\circ}$ for $\triangle d e f$, indicating cycle symmetry. Likewise, all angles are within $0.5^{\circ}$ of $60.0^{\circ}$ for triangle $\triangle a b f$, within $0.7^{\circ}$ of $60.0^{\circ}$ for $\triangle b c d$, and within $0.8^{\circ}$ of $60.0^{\circ}$ for $\triangle a c e$,</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. (Left) The leading three PCA components of token embeddings for a 2-layer Transformer with $d=512$ trained for 50 epochs on $L=5$ data, with zeros included. The leading three PCA components explain $63.56 \%$ of variance, and dihedral symmetry is not visible. (Right) The leading three PCA components of token embeddings for a 2-layer Transformer with $d=512$ trained for 200 epochs on $L=6$ data, with zeros included. The leading three PCA components explain $81.76 \%$ of variance. The octahedron exhibits dihedral symmetry.
indicating flip symmetry.
We perform standard linear principal component analysis (PCA) and plot the embeddings of these letter tokens in the space of the three leading PCA components in Figure 5. Projecting to the leading three components distorts the angles somewhat: at $L=5$, the dihedral symmetry is no longer apparent, while at $L=6$, the cycle and flip symmetries are visually apparent but the octahedron is no longer regular.</p>
<h1>7. Mixed-loop Training</h1>
<p>To successfully extend the bootstrap program to unseen loops, we must build models that can generalize from lower loops, for which we have the complete symbol, to higher loops, where only a small number of symbol terms may be available.</p>
<p>However, in many AI-for-mathematics applications, Transformers trained exclusively at one input length using absolute position encoding fail to generalize to different input lengths [39]. Here we face a similar challenge. When using loop $L=6$ data to evaluate a model trained exclusively at $L=5$ (and vice versa) for the task of predicting nonzero coefficients, our baseline models can only attain an accuracy of at most $3 \%$ for a variety of model sizes and depths. Many predicted coefficients at unseen loops are nonsensical, such as the string '+++'. Given that our ultimate goal is to predict coefficients of keys at unseen higher loops, this failure of length generalization presents a major limitation that we must overcome.</p>
<p>In many ways, however, this problem goes even beyond simple length generalization. At each subsequent loop, the number of possible values of keys and coefficients both increase appreciably (see Figure 2), and it is not clear whether or how the functional</p>
<p>form that relates them changes as well. Therefore, while alternative architecture designs, positional encoding schemes, and numerical encoding schemes (which we explore in Appendix C) may be helpful for this task, a much more comprehensive strategy will likely be needed.</p>
<p>As a first attempt to address this issue, we train Transformers to predict coefficients using an even proportion of $L=5$ and $L=6$ data. The full nonzero $L=5$ symbol (263,880 elements) is first combined with an equal proportion of zero-coefficient elements at $L=5$. Another 263,880 nonzero elements are then drawn from the $L=6$ symbol (representing $5 \%$ of the $L=6$ symbol) and augmented with a roughly equal amount of zero-coefficient elements at $L=6$. The zero-sampling is done naïvely, as in Section 4, rather than in the nontrivial-zero-biased manner of Section 6. Training and test sets are constructed for $L=5$ and $L=6$ separately, and they are then merged to create mixed-loop training and test sets. Each loop-specific training set for both $L=5$ and $L=6$ contains 517,760 examples, while each loop-specific test set contains 10,000 examples; every set is an equal mix of zero- and nonzero-elements. Additionally, we create a larger $L=6$ training set that is the same size as the mixed-loop set (1,035,520 examples,
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Accuracy vs. epoch for models trained on the mixed and per-loop training sets, evaluated on the per-loop test sets. In all cases, accuracy starts at $50 \%$ because zeros are learned in the first epoch. Models trained on the mixed training set take almost exactly half the number of epochs to reach performance benchmarks for $L=5$ as models trained on the dedicated $L=5$ training set, corresponding to approximately equal performance of these two models. However, models trained on a small subset of the $L=6$ symbol mixed with the full $L=5$ symbol are able to generalize to the $L=6$ test set much better than models trained on the $L=6$ symbol alone.</p>
<p>roughly $10 \%$ of the $L=6$ symbol), in order to evaluate whether the effects of mixed-loop training can be explained by the difference in training set size.</p>
<p>We train a model with 2 layers, $d=512$, and 8 attention heads in both the encoder and decoder for 200 epochs, evaluating on both the mixed-loop test set and the individual $L=5$ and $L=6$ test sets. The model again exhibits two-phase learning behavior in all cases, as shown in Figure 6 (which displays results for all but the larger $L=6$ training set). We measure the performance by reporting three epochs,
(i) the epoch at which the test-set magnitude accuracy first exceeds $90 \%$,
(ii) the midpoint epoch of the plateau step, which is when the model's overall accuracy first reaches the average between its final-state accuracy and $75 \%$,
(iii) the epoch at which the overall test-set accuracy first exceeds $90 \%$,
as well as the best overall test-set accuracy after 200 epochs.
In Table 3, we compare the model performance when trained on each loop-specific training set to its performance when trained on the mixed training set. The dedicated $L=5$ model reaches all performance benchmarks for each loop in almost exactly half the number of epochs as the mixed-loop model. Accounting for the fact that the mixed-loop training set is exactly twice the size of the individual $L=5$ and $L=6$ training sets, this result suggests that the mixed-loop model solves the prediction problem for $L=5$ in almost exactly the same amount of time as it would without the addition of $L=6$ data. On the other hand, the mixed-loop model learns the $L=6$ magnitudes in approximately the same number of epochs as the dedicated $L=6$ model, corresponding to reaching this benchmark (i.e., First Mag. Acc. $&gt;90 \%$ ) in half as many iterations through the $L=6$ training set. The mixed-loop model also exceeds $\sim 90 \%$ overall accuracy, whereas the dedicated $L=6$ model does not. However, much of this effect may be explained by the dataset size: when a model is trained on the larger $L=6$ dataset that is the same size as the mixed set, the results appear highly similar to those we see when using the mixed training set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Train</th>
<th style="text-align: center;">Eval</th>
<th style="text-align: center;">First Mag. Acc. <br> $&gt;90 \%$ [Epoch]</th>
<th style="text-align: center;">Midpoint of <br> Step [Epoch]</th>
<th style="text-align: center;">First Total Acc. <br> $&gt;90 \%$ [Epoch]</th>
<th style="text-align: center;">Best Acc. <br> Epoch 200</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mix</td>
<td style="text-align: center;">$L=5$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$99.82 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L=5$</td>
<td style="text-align: center;">$L=5$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$99.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Mix</td>
<td style="text-align: center;">$L=6$</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">88</td>
<td style="text-align: center;">112</td>
<td style="text-align: center;">$94.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L=6$</td>
<td style="text-align: center;">$L=6$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">$\mathrm{~N} / \mathrm{A}$</td>
<td style="text-align: center;">$84.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">$L=6$, Large</td>
<td style="text-align: center;">$L=6$</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">92</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">$97.67 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3. Model learning dynamics for mixed training: the epoch at which the test-set magnitude accuracy first exceeds $90 \%$, the midpoint of the plateau step, the epoch at which the overall test-set accuracy first exceeds $90 \%$, and the best overall test-set accuracy after 200 epochs. All training hyperparameters (architecture, initialization seed, etc.) are kept the same; only the training and test sets are changed between runs.</p>
<p>These results are nonetheless rather encouraging: while adding a subset of $L=6$ data to $L=5$ does not lead to improvement on $L=5$ tasks, adding the full $L=5$ symbol to a subset of the $L=6$ symbol does appear to improve performance on $L=6$ tasks. This suggests that features learned at lower loops can be employed to enhance the predictive power of a small number of symbol elements at higher loops (for which we may only be able to determine a handful of coefficients a priori).</p>
<h1>8. Steps Toward Predicting the Next Loop</h1>
<p>Although mixed-loop training may allow us to better generalize from only a small number of symbol elements at unseen loops, we still wish to find ways to predict these elements at loops for which we do not have coefficient information.</p>
<p>As a first attempt at solving this task, we now consider a different problem setup. Instead of predicting unknown symbol elements at a given loop order, we would like to obtain the loop $L$ symbol from the coefficients at loop $(L-1)$. In other words, for any element at loop $L$, we want to recover its coefficient from the coefficients of a list of parent elements at one loop lower that are related by having similar strings of letters in their keys.</p>
<p>The keys of elements at loop $L$ are sequences of $2 L$ letters, whereas the keys at loop $L-1$ are only $(2 L-2)$ letters long. We therefore define the strike-two parents of a given key at loop $L$ as the keys from loop $(L-1)$ which are created by simply striking out two letters from the key. For instance, the six strike-two parents of the loop $L=2$ key aacf are</p>
<p>$$
\text { aacf }=\mathrm{cf}, \quad \text { aacf }=\mathrm{af}, \quad \text { aacf }=\mathrm{ac}, \quad \text { aacf }=\mathrm{af}, \quad \text { aacf }=\mathrm{ac}, \quad \text { aacf }=\mathrm{aa}
$$</p>
<p>which are keys at $L=1$. In general, there are $\binom{2 L}{2}=L(2 L-1)$ strike-two parents of any key at loop $L$.</p>
<p>The dataset for this experiment is constructed by first selecting certain elements at loop $L$. For each key, we then construct the list of strike-two parents at loop $L-1$ and list their coefficients in the strikeout order, i.e., the coefficient corresponding to striking the first two letters from the key is ordered first in the coefficient list, the coefficient corresponding to striking the first and third letters from the key is ordered second in the list, etc. For the above example at $L=2$, the coefficient to be predicted is $C^{\text {aacf }}=0$, and the ordered list of its strike-two parent coefficients is $[0,-2,0,-2,0,0]$. Many of the parent coefficients are zero due to the nature of the strike-out. For example, pairs of letters that are not allowed to be adjacent can become adjacent in the parent keys after a letter in between is removed.</p>
<p>In these experiments, 4-layer Transformers with $d=512$ and 8 heads are trained to predict nonzero coefficients at $L=6$ from the coefficients of their $L=5$ parents. Model inputs are sequences of $\binom{12}{2}=66$ parent coefficients at $L=5$ given in strikeout order, while the targets are single coefficients at $L=6$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Denotes equal contribution
$\dagger$ Authors to whom correspondence should be addressed&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
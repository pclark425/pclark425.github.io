<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9557 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9557</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9557</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-07f07d4d59fdbc3596284f51057cb006779d42c1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07f07d4d59fdbc3596284f51057cb006779d42c1" target="_blank">A Bibliometric Review of Large Language Models Research from 2017 to 2023</a></p>
                <p><strong>Paper Venue:</strong> ACM Transactions on Intelligent Systems and Technology</p>
                <p><strong>Paper TL;DR:</strong> This study conducts bibliometric and discourse analyses of scholarly literature on large language models to offer valuable insights into the current state, impact, and potential of LLMs research and its applications.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs), such as OpenAI's Generative Pre-trained Transformer (GPT), are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks. LLMs have become a highly sought-after research area because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this article serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains, including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this article offers valuable insights into the current state, impact, and potential of LLMs research and its applications.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9557",
    "paper_id": "paper-07f07d4d59fdbc3596284f51057cb006779d42c1",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003747,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Bibliometric Review of Large Language Models Research from 2017 to 2023</h1>
<p>Lizhou Fan ${ }^{1 <em>}$, Lingyao $\mathrm{Li}^{1}$, Zihui $\mathrm{Ma}^{2}$, Sanggyu Lee ${ }^{2}$, Huizi $\mathrm{Yu}^{1}$, Libby Hemphill ${ }^{1}$<br>${ }^{1}$ School of Information, University of Michigan, Ann Arbor, MI<br>${ }^{2}$ Department of Civil and Environmental Engineering, University of Maryland, College Park, MD<br></em>Email: lizhouf@umich.edu</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) are a class of language models that have demonstrated outstanding performance across a range of natural language processing (NLP) tasks and have become a highly sought-after research area, because of their ability to generate human-like language and their potential to revolutionize science and technology. In this study, we conduct bibliometric and discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000 publications, this paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape of LLMs research. We present the research trends from 2017 to early 2023, identifying patterns in research paradigms and collaborations. We start with analyzing the core algorithm developments and NLP tasks that are fundamental in LLMs research. We then investigate the applications of LLMs in various fields and domains including medicine, engineering, social science, and humanities. Our review also reveals the dynamic, fast-paced evolution of LLMs research. Overall, this paper offers valuable insights into the current state, impact, and potential of LLMs research and its applications.</p>
<p>Keywords: Bibliometric analysis, Discourse analysis, Large language models, Scholarly Collaboration networks, Topic modeling</p>
<h2>1. Introduction</h2>
<p>On March 14, 2023, OpenAI announced the release of their newest version of the large language model (LLM) - GPT-4 (OpenAI, n.d.; Sanderson, 2023). This state-of-the-art LLM powers many of OpenAI's popular AI applications, including the widely used ChatGPT, and brings much attention to the research of LLMs. An LLM is a class of language models that employs neural networks with billions of parameters, trained on gigantic amounts of unlabelled text data through self-supervised learning (Y. Shen, Heacock, et al., 2023; Zhao et al., 2023). LLMs are often based on transformers, a type of neural network architecture that is designed to process sequential data. Transformers use self-attention mechanisms to compute contextual relationships between the input tokens, allowing them to effectively capture long-range dependencies and contextual information (Vaswani et al., 2017). The emergence of LLMs in 2018 has ushered in a paradigm shift in natural language processing (NLP) research, as they have demonstrated outstanding performance across a range of tasks (Devlin et al., 2018;</p>
<p>Radford et al., 2018). LLMs are designed to have general-purpose capabilities, which enable them to excel across a broad spectrum of NLP tasks (Wei et al., 2022), rather than being designed solely for a single NLP task, such as sentiment analysis, named entity recognition, or text classification. Typical LLMs include Bidirectional Encoder Representations from Transformers (BERT) developed by Google (Devlin et al., 2018), Generative Pre-trained Transformer (GPT) families developed by OpenAI (Eloundou et al., 2023), and Large Language Model Meta AI (LLaMa) by Meta (Meta, 2023).</p>
<p>Although previous scientific literature has emphasized the potential of LLMs in various NLP tasks, including specialized applications in fields such as medical and health sciences (Ding et al., 2022; Khare et al., 2021; Yu et al., 2022) and politics (Y. Hu et al., 2022; R. Liu et al., 2021), much of the current research has been limited to specific NLP tasks or applications. With the recent release of the latest and most advanced GPT model (OpenAI, n.d.; Sanderson, 2023), LLMs have become a highly sought-after research area, attracting researchers to develop state-of-the-art LLMs, e.g. LLaMa and Bard (Meta, 2023; Pichai, 2023), and to explore their capabilities, e.g. Alpaca and GPTHuggingface (Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto, March, 13 2023; Y. Shen, Song, et al., 2023). Consequently, a bibliometric review to examine current LLMs research has become increasingly essential. While previous research has highlighted the potential and superiority of LLMs in NLP tasks, few studies have conducted a systematic analysis of the latest trends, opportunities, and challenges within the field of LLMs.</p>
<p>To gain insight into the state of LLMs research, this paper presents a comprehensive overview of current studies covering the research paradigms and collaborations in their development and applications. In particular, we focus on the discourse and bibliometric aspects, including,</p>
<ul>
<li>Research paradigms, the themes through topic modeling and discourse analysis of LLMs, from algorithms and NLP tasks to applications, infrastructures, and critical studies;</li>
<li>Research collaborations, the scholarly collaboration networks, from the international and organizational perspectives.</li>
</ul>
<p>The significance of this paper lies in two main aspects. Firstly, it presents an up-to-date bibliometric analysis of the state-of-the-art studies in LLMs, identifying trends and patterns that deepen understanding of the topic. Secondly, by analyzing the existing literature, our paper serves as a roadmap for researchers, practitioners, and policymakers to navigate the current landscape, pinpoint knowledge gaps and research opportunities, thereby fostering innovation and advancing the field toward breakthroughs.</p>
<h1>2. Background</h1>
<p>LLMs are pre-trained language models models that use deep learning techniques to process and comprehend natural language (Y. Shen, Heacock, et al., 2023; Zhao et al., 2023). LLMs are trained and fine-tuned on vast amounts of text data, which allows them to learn patterns in unstructured sequences and build a knowledge base of language (Brown et al., 2020; Radford et al., 2019). LLMs offer outstanding advantages over conventional NLP models. In contrast to the conventional approach for NLP tasks, which involves fine-tuning models through supervised</p>
<p>learning on small, task-specific datasets, LLMs can effectively perform a wide range of tasks with only a few prompts (Manning, 2022). By providing them with human language descriptions or several examples of the desired task, they can execute tasks for which they were not explicitly trained (Manning, 2022). Thus, LLMs require fewer resources and less training time compared to conventional models with similar performance, as they can learn more from the same amount of data (M. Chen et al., 2021).</p>
<p>As such, LLMs have a broad range of capabilities in performing language-related tasks, such as text generation, translation, and summarization (Ollivier et al., 2023), as well as real-world applications, such as virtual assistants, chatbots, and language translation systems. To better outline the emerging landscapes of LLMs from 2017 to early 2023, in this section, we introduce the history of their developments, followed by their current applications across fields of research.</p>
<h1>2.1 History</h1>
<p>Traditionally, NLP models such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) had difficulty capturing long-range dependencies between words in a sentence (Hochreiter \&amp; Schmidhuber, 1997). This limitation negatively affected language models' performance on NLP tasks such as machine translation, summarization, and question-answering (Sutskever et al., 2014). However, in 2017, Vaswani et al. introduced the Transformer model (Vaswani et al., 2017). The self-attention mechanism used in the model allowed it to attend to all the other tokens in the input sequence by assigning weights to each token. The ability to capture long-range dependencies and parallelizable architecture of the model made it successful in various NLP applications (Devlin et al., 2018). Since the development of the Transformer model, researchers have built on top of the Transformer, developing more advanced language models.</p>
<p>BERT, which stands for Bidirectional Encoder Representations from Transformers, was introduced in 2018 by Devlin et al (Devlin et al., 2018). It is a pre-training technique that utilizes deep bidirectional representations by conditioning on both left and right contexts of all layers. This allowed the pre-trained BERT model to be fine-tuned with one additional output layer, making it suitable for a wide range of tasks such as question answering and language inference. BERT's success has led to its widespread adoption and other pre-trained language models (Y. Liu et al., 2019; Z. Yang et al., 2019). However, its limitation is that the pre-training process is computationally expensive. In 2019, Alec Radford, et al. presented Generative Pre-trained Transformer 2, also known as GPT-2, which was trained on a deep neural network with 1.5 billion parameters (Radford et al., 2019). GPT-2 utilizes a transformer architecture that employs self-attention mechanisms to gather data from various locations in the input sequence. Although the model is computationally expensive to train and run, its large size enables it to understand and generate a wide range of linguistic nuances and diverse outputs. Megatron-LM is another LLM that was developed in 2019, by (Shoeybi et al., 2019). It has 8.3 billion parameters, which is significantly larger than GPT-2's 1.5 billion parameters. This size enables the model to capture and generate more complex linguistic patterns. The model features a new parallelization scheme, which enables faster training compared to other models of comparable</p>
<p>size. However, due to its large size, Megatron-LM requires significant computational resources for both training and inference.</p>
<p>In 2020, the introduction of GPT-3 by OpenAI marked a significant milestone in the development of LLMs (Brown et al., 2020). GPT-3 has 175 billion parameters, which is significantly larger than any other LLMs at that time. It can generate high-quality natural language text with little to no fine-turning due to the use of advanced techniques such as a higher layer count and more diverse training data. The introduction of GPT-3 has propelled the field of natural language processing forward. Following the success of GPT-3, researchers have continued to push the boundaries of LLMs. While these recent models are out of the time range for our analysis, they are important proofs that the advancement in LLMs research is increasingly faster - the burst of these research projects is making substantial changes to not only NLP or AI research, but also In 2023, OpenAI announced the development of a new multimodal model called GPT-4, capable of processing both text and image inputs to generate textual outputs (OpenAI, 2023). As the field is highly competitive and there are potential safety concerns, the technical paper does not disclose details about the model's architecture, hardware, dataset construction, and training method. However, its performance was evaluated on various professional and academic exams designed for humans. GPT-4 demonstrated human-level performance on most of the exams, and notably, it achieved a score in the top $10 \%$ of test takers on a simulated version of the Uniform Bar Examination (OpenAI, 2023). There are also newly released open-source LLMs, e.g. LLaMa (Meta, 2023), which are smaller in size and number of parameters but freely available to researchers.</p>
<h1>2.2 Applications</h1>
<p>The advantages of LLMs in language understanding and their ability to generalize to new tasks have resulted in increased application and ongoing development in the field of NLP. Recent research using LLMs has focused on themes such as relation extraction (Gu et al., 2021), dialogue analysis (Thoppilan et al., 2022), text summarization (H. Zhang et al., 2019), sentiment analysis (Araci, 2019), named entity recognition (Nguyen et al., 2020), and text classification (Jin et al., 2020). These research studies have demonstrated that LLMs have the potential to significantly enhance the accuracy and fluency of natural language processing tasks, thereby improving our understanding of human language (Beltagy et al., 2019; Nguyen et al., 2020). In addition, current research on downstream tasks using LLMs has focused on several directions. One direction is fine-tuning, which involves modifying an existing pre-trained language model such as changing the weights in the neural layers by training it in a supervised fashion on a specific NLP task (Jurafsky \&amp; Martin, 2023). Another direction is the prompting interactions with LLMs (Reynolds \&amp; McDonell, 2021), where the problem to be solved is formulated via "few-shot prompting" (Brown et al., 2020) or instruction tuning (Maarten Bosma, 2021), in order to enhance the LLMs' performance on given NLP tasks.</p>
<p>The versatility of LLMs makes them a promising tool for diverse disciplines and research fields. Rather than training specialized supervised models for specific tasks, researchers have utilized LLMs to handle a broad range of applications across multi-disciplinary domains. In the medical field, LLMs are used to analyze electronic health records, laboratory reports, and clinical notes</p>
<p>to provide diagnostic assistance (Rasmy et al., 2021; Tang et al., 2021) and offer treatment suggestions (Shang et al., 2019) to healthcare professionals. They are also demonstrated to have the potential to provide Al-assisted medical education (Kung et al., 2023). In engineering, LLMs are utilized to analyze large volumes of engineering documents (Qiu \&amp; Jin, 2022), generate emergency plans (X. Liu et al., 2022), and detect and classify defects in maintaining the performance of buildings (D. U. Yang et al., 2022). Similarly, LLMs are applied to analyze social media posts, survey responses, and news articles, facilitating data-driven research in research areas such as sociology (Kawashima \&amp; Yamaguchi, 2021; Mustakim et al., 2022), economics (Jagdish et al., 2022; Li et al., 2021), and politics (Y. Hu et al., 2022; Salam et al., 2020).</p>
<h1>3. Data and Methods</h1>
<p>Figure 1 indicates our workflow in collecting scholarly literature metadata and analyzing them. We first collected bibliometric data of LLMs research literature. Then, we analyze research paradigms and collaborations using the discourse under research themes and the scholarly collaboration networks.</p>
<p>Figure 1. Overall data and methods workflow
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<h3>3.1 Data</h3>
<p>Web of Science (WoS) Core Collection is a widely recognized platform for retrieving comprehensive academic literature metadata for bibliometric study (Birkle et al., 2020). To gather relevant papers for our analysis of LLMs research, we conducted an advanced search on WoS records. As Table 1 shows, we then used a combination of different keywords related to the LLMs and specific models respectively on the article titles (TI) and the topics (TS), i.e. the combinations of article titles, abstracts, and keywords. The query is as follows,</p>
<div class="codehilite"><pre><span></span><code>TI=((large or big or massive) and language and (model or models)) or
TS = (&quot;large language model&quot; or &quot;large language models&quot; or BERT or
    GPT-1 or GPT-2 or GPT-3 or ChatGPT)
</code></pre></div>

<p>We then limited the date range to the start of 2017 (2017-01-01) to early 2023 (2023-02-20) and obtained 5752 publications.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Table 1. Web of Science search keywords</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Search Fields</td>
<td style="text-align: center;">Keywords</td>
<td style="text-align: center;">Search Logic and Purposes</td>
</tr>
<tr>
<td style="text-align: center;">TI</td>
<td style="text-align: center;">large or big or massive</td>
<td style="text-align: center;">Combining the three components that describe possible names of LLMs using and.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">language</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">model or models</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">"large language model" or "large language models"</td>
<td style="text-align: center;">Combining the fixed components that describe possible names of LLMs (detectable only if consecutive) and popular individual LLMs names.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BERT or GPT-1 or GPT-2 or GPT-3 or ChatGPT</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>3.2 Methods</h1>
<p>We first used topic modeling to analyze the research paradigm of LLMs. Topic modeling is a method that discovers and summarizes latent semantic topics from large-scale unstructured text data, for example, academic literature. It assumes that each text document, for example, a publication, is a combination of multiple topics, where each topic is represented by a probability distribution of words that can be grouped as clusters with similarities (Blei \&amp; Lafferty, 2007; Steyvers, 2007).</p>
<p>In particular, we used BERTopic (Grootendorst, 2022), a neural topic modeling method, to analyze publications in the corpus of LLMs research. First, we use Sentence-BERT (SBERT) (Reimers \&amp; Gurevych, 2019), a transformer-based pre-trained NLP model, to obtain sentence embeddings for each of the combinations of title and abstract. In particular, we used the SBERT Python package ${ }^{1}$ and the pre-trained model "all-MiniLM-L6-v2"2. To better handle the high dimensional tweet vectors for clustering, We then used a dimensionality reduction technique (UMAP) to handle the curse of dimensionality problem in our clustering model (McInnes et al., 2018), which enabled us to use the Lloyd's K-Means clustering algorithm to group similar sentences embedding vectors into topics. ${ }^{3}$ Given the size of the corpus (more than 500</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>documents), we experimented with three different numbers of clusters (100, 200, and 400) and chose to use 200 clusters for our analysis since it provides publication groups that are sharing similar topics in each cluster while avoiding too many small clusters. We then proceed to represent LLMs topics and the corresponding research themes. Using the count vectorizer in the scikit-learn Python package, we tokenized topics through cluster-level (topic-level) bag-of-words representation that calculates and vectorizes the frequency of each word in each cluster (Y. Zhang et al., 2010). We then used the class-based term frequency-inverse document frequency (c-TF-IDF) to extract the difference of topical keywords (Grootendorst, 2022), distinguishing among the clusters and representing topics with the unique and frequent words as the keywords of LLMs research themes.</p>
<p>Finally, we characterized the 200 topics based on the keywords, titles, and abstracts of publications in each topic. After removing irrelevant contents, e.g. empty docs with "nan" and topics not about LLMs research, we summarized these topics into five higher-level categories, i.e. research themes, as follows:</p>
<ul>
<li>Algorithm and NLP tasks: The computational methods and techniques used to process, analyze, and generate human language in LLMs, performing tasks such as translation, summarization, sentiment analysis, and question-answering, among others;</li>
<li>Medical and Engineering Applications: Applications leverage LLMs to enhance domain-specific tasks related to healthcare and engineering fields, such as analyzing medical literature, aiding diagnosis, predicting patient outcomes, and facilitating engineering design processes or problem-solving;</li>
<li>Social and Humanitarian Applications: Applications use LLMs to address societal and humanitarian challenges, from analyzing social issues, supporting disaster response, and enhancing communication, and to promoting educational initiatives;</li>
<li>Critical Studies: Reflections and examinations of the ethical, social, and political implications of LLMs, which scrutinize LLMs' potential biases, transparency, and impact on society, while also exploring governance, accountability, and strategies for ensuring responsible and equitable Al development and deployment;</li>
<li>Infrastructures: The underlying systems and resources required for developing, deploying, and maintaining LLMs, examining aspects such as computing power, data storage, networking, and the policies and frameworks that govern their use and development.</li>
</ul>
<p>To precisely identify these five research themes, as well as remove the irrelevant publications, two authors annotated all 200 topics respectively. We reached a comparatively high agreement in the first round of annotation (Krippendorff's $\alpha=0.76$ ) (Krippendorff, 2018), and reached the full agreement after discussion. For the corresponding visualizations of the annotated results, as well as the LLM publications corpus, we used Tableau to create the trend line, the pie chart, and the document map (in Section 4.1).</p>
<p>To further study collaborations in LLMs research, we also used network methods and visualization features to study scholarly collaboration networks. In particular, we leverage a bibliometrics analysis software, CiteSpace (C. Chen, 2016), to generate co-citation and collaboration networks of LLMs publications (in Section 4.2). CiteSpace offers an essential</p>
<p>co-citation analysis function to identify significant publications in a research field. Co-citation relationships occur when two or more papers are cited by one or more later papers at the same time. To cluster network nodes, the software employs the expectation maximization (EM) algorithm, which is an iterative algorithm that partitions data into clusters by maximizing the likelihood function based on attributes such as citation frequency and betweenness centrality (BC). The EM algorithm is a hard clustering method, which means that each reference can only belong to one cluster (C. Chen et al., 2010).</p>
<p>To start the clustering process, the algorithm assigns each reference to an initial random cluster, and then iteratively updates the cluster assignments based on the likelihood of the data given the cluster assignments. This process continues until the algorithm converges to a stable solution. The resultant clusters are non-overlapping and are subsequently labeled and summarized by the built-in algorithm. The co-citation knowledge graph visualizes the connections between the literature, and nodes that are closely linked in the co-citation mapping frequently appear in the same literature (Niu et al., 2022). This indicates that the co-cited articles must be similar in content, and a higher co-citation value reflects a stronger connection between them due to greater similarity in content.</p>
<p>Collaboration network analysis is based on social network theory, which originated from the anthropological and sociological exploration of interpersonal relationships in complex social clusters (Z. Shen et al., 2023). By analyzing the collaborative relationships between countries, institutions, and authors, CiteSpace can provide insight into the overall social status in the research field and facilitates the understanding of scholarly communication and knowledge diffusion in a particular research field. In addition, CiteSpace can track the development of a research field over time by analyzing publications from different years. This feature allows researchers to identify emerging trends and track the evolution of research areas.</p>
<h1>4. Results</h1>
<h3>4.1 Research paradigms of LLMs: from algorithms and NLP tasks to applications, infrastructures, and critical studies</h3>
<h3>4.1.1 Overview of research trends and themes</h3>
<p>The field of LLMs has gained significant attention and interest from researchers in recent years. As Figure 2(a) shows, there is a steady increase in the number of publications on LLMs from 2017 to 2023, ${ }^{4}$ with a sharp spike from 2019 to 2020, likely due to increased interest in transformer-based NLP algorithms, e.g., SBERT and BERTopic (first released on September</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>2020) (Grootendorst, 2022; Reimers \&amp; Gurevych, 2019), and the public release of advanced LLM models, e.g., GPT-3 (Brown et al., 2020). The trend continues to rise in 2021 and after, indicating that the field of LLMs is still growing and evolving. This trend suggests that there is still much to explore and discover in the field of LLMs, and researchers are likely to continue studying and developing these models in the coming years.</p>
<p>Figure 2. LLMs research trends and themes
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Research on LLMs also spans a wide range of themes, including Algorithm and NLP tasks, Social and Humanitarian Applications, Medical and Engineering Applications, Critical Studies, and Infrastructures. As the pie chart in Figure 2(b) shows, publications in the field of LLMs can be divided into several themes, each representing a specific theme or subfield. The largest research theme, Algorithm and NLP tasks, represents more than half (54\%, 2980 out of 5527) of all publications in the LLM field. This theme focuses on the development and refinement of LLM architectures and modeling techniques, some of which are applicable to specific NLP tasks. The next largest research theme, Social and Humanitarian Applications, accounts for about a quarter ( $25 \%$, 1387 out of 5527 ) of the publications. This theme includes studies that apply LLMs to specific social issues, such as controversial speech and the COVID-19 pandemic, and humanities research, such as sentiment analysis and language translation. The third largest theme, Medical and Engineering Applications, represents around 18\% (1006/5527) of the publications. This theme involves the use of pre-trained LLMs and fine-tuning them to automate specific medical and engineering tasks, such as health record processing and software similarity analysis. The remaining two themes of the pie chart are relatively smaller in size, each representing less than $2 \%$ of the publications. These themes are Critical Studies, which focuses on the ethical and social implications of LLMs, and Infrastructures, which focuses on developing and enhancing hardware and cloud computing resources that can support LLMs. Overall, the snapshot of themes of publications demonstrates the different areas of focus in LLMs research, showing that the field is diverse and covers a wide range of topics and subdomains.</p>
<h1>4.1.2 Topical research themes and key discourses</h1>
<p>Figure 3 shows a 2D mapping of topic modeling results of publications in the LLMs from 2017 to 2023. ${ }^{5}$ Each point on the figure represents a publication. ${ }^{6}$ This colored documentation map is divided into research theme clusters, each representing a specific group of publications that share similar topics on LLMs as defined in Section 4.1.1. In general, there are no standing-alone clusters for the larger research themes such as Algorithm and NLP Tasks and Social and Humanitarian Applications. These two themes are mixed around the map, indicating the high semantic closeness among many topics in these two clusters. For example, as the two black frames highlight, sentiment and emotion analysis are NLP tasks that require algorithm development (Topic 65 and Topic 103), which also have corresponding applications in social and humanitarian aspects (Topic 63 and Topic 28). These two themes (marked as blue and orange dots) are also scattered all around the map, which indicates that there are topics in research themes of Algorithm and NLP Tasks and Social and Humanitarian Applications themes that are related to the other three themes, demonstrating the active scholarly communication among different subdomains in LLMs research.</p>
<p>The map also reveals several interesting patterns in the landscape of LLMs publications, as highlighted by frames with corresponding colors. First, Medical and Engineering applications are often located on the upper middle part of the map, indicating a comprehensive cluster of various highly professional and semantically related sub-domains in LLMs research. For example, Topic 64 focuses on using LLMs to study specific categories of diseases such as Alzheimer's and Dementia, Topic 95 focuses on drug use and health services, Topic 87 focuses on biomedical advice and precision medicine, Topic 45 focuses on technique in biomedical relations extraction, and Topic 23 focuses on processing electronic medical records in different languages such as Chinese, all of which are related to medical and health research. Other subdomains in this theme, mostly engineering applications are located on other parts of the map (outside of the dotted green frame), which are closely related to the Algorithms and NLP Tasks and the Social and Humanitarian Applications research themes. Second, Critical studies (highlighted in purple dashed frames) are also semantically close to what they critically analyzed. For example, Topic 60 contains critical studies on bias in LLMs, which are closely related to LLM applications that deal with cyberbullying and abusive comments (Topic 10). Similarly, Topic 125 covers privacy concerns related to LLMs, which is closely related to anti-attack explorations in LLM applications (Topic 141). Finally, the Infrastructure theme (highlighted in red frames) focuses on parallel and distributed computing with GPU (Topic 58) and hardware and accelerator (Topic 94), which enable scalability and enhance efficiency in LLMs research.</p>
<p>As a whole, the map provides a visual representation of the different topics and themes that have emerged within the LLMs research community, revealing patterns and subfields that may</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>not be immediately apparent from a simple analysis of publication keywords or titles. To further demonstrate how the topic modeling results and research themes correspond, we provide the details of the topical keywords and theme labels in Appendix B.</p>
<p>Figure 3. A 2D map of LLMs publication embeddings with research themes
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>To elaborate on the key discourse under each major theme, we analyze the keywords ${ }^{7}$ in each of the corresponding co-citation networks (Figure 4). In the Algorithm and NLP Tasks co-citation network (Figure 4(a)), the keywords of the central clusters are related to general aspects of NLP and machine learning algorithms, such as "natural language inference" (#12) and "machine learning comprehension" (#3). The peripheral clusters often have keywords with specific NLP tasks. Both the central and peripheral keywords indicate important and promising directions that have attracted attention, which are great references to new researchers and other stakeholders like publishers and funders caring about LLMs research.</p>
<p>In the two other co-citation networks of LLM applications, there are less obvious center clusters, which show diverse and multifaceted development among subdomains. In the Medical and Engineering Applications co-citation network (Figure 4(b)), the keywords suggest that the most important LLMs research themes in medical and engineering areas are related to the application of pre-trained models and NLP techniques. These applications depend on a few core NLP tasks such as named entity recognition (NER) and contextualized word embedding to support a wide range of use cases from medical tasks (e.g. clinical textual semantic similarity) to engineering</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(e.g. software similarity). In the Social and Humanitarian Applications co-citation network (Figure 4(c)), some representative keywords include "fake news", "twitter", "hate speech", "rumor detection", and "argumentation mining". These keywords suggest that the popular themes in this sub-domain are related to the analysis of social media and news data, particularly with respect to sentiment, opinion, and controversial content.</p>
<p>Figure 4. Keywords and representative publications of major LLMs research themes
(a) Algorithm and NLP Tasks
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Medical and Engineering Applications
(c) Social and Humanitarian Applications
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<h1>4.2 Research collaborations on LLMs: the international and organizational perspectives</h1>
<h3>4.2.1 Active countries and regions in research collaborations</h3>
<p>Figure 5 showcases a knowledge mapping of the distribution network for national and international collaborations, which was generated using CiteSpace. In this mapping, the centrality of a country in the collaboration network is represented by its degree, while the number of papers published from the country in our dataset is denoted by its publication frequency. Since the selected papers were sourced from recognized international journals in Web of Science, it is reasonable to conclude that the degree of centrality and frequency of publications identified in this bibliometric study reflects the importance of studies in LLM to some extent. These findings can provide valuable insights for researchers working in the field of LLM, both in current and future international collaborations.</p>
<p>In Table 2, we present the top 10 countries that have contributed the most to high-yield degree and frequency research. These countries include the United States (USA), United Kingdom (England), India, Canada, France, China, Germany, Spain, Australia, and Russia, each with influence on the collaboration network. Specifically, their respective degree values are 51, 41, $35,34,33,33,29,28,28$, and 27 . In addition, our analysis reveals that the USA, England, and India have the highest degree values of 51,41 , and 35 , respectively. This suggests that these countries have the most connections with other nations in relation to LLMs research. Regarding frequency, China and the USA have the highest number of recognized publications, 1828 and 1344, respectively, greatly surpassing other countries. However, the USA has a higher degree value than China, showing a more centralized position in the collaboration network and greater outreach with other countries. Other countries and regions, such as Japan, the Netherlands, Singapore, and South Korea, are actively involved in collaboration efforts. Overall, we observed that most papers on LLMs research have been published in countries in the Asia-Pacific region, North America, and Europe.</p>
<p>Figure 5. Overall with top active countries and regions
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Table 2. Top countries and regions in the international collaboration network</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Country</td>
<td style="text-align: left;">Degree</td>
<td style="text-align: left;">Frequency</td>
</tr>
<tr>
<td style="text-align: left;">USA</td>
<td style="text-align: left;">51</td>
<td style="text-align: left;">1344</td>
</tr>
<tr>
<td style="text-align: left;">ENGLAND</td>
<td style="text-align: left;">41</td>
<td style="text-align: left;">205</td>
</tr>
<tr>
<td style="text-align: left;">INDIA</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">377</td>
</tr>
<tr>
<td style="text-align: left;">CANADA</td>
<td style="text-align: left;">34</td>
<td style="text-align: left;">179</td>
</tr>
<tr>
<td style="text-align: left;">FRANCE</td>
<td style="text-align: left;">33</td>
<td style="text-align: left;">112</td>
</tr>
<tr>
<td style="text-align: left;">PEOPLES R CHINA</td>
<td style="text-align: left;">33</td>
<td style="text-align: left;">1828</td>
</tr>
<tr>
<td style="text-align: left;">GERMANY</td>
<td style="text-align: left;">29</td>
<td style="text-align: left;">219</td>
</tr>
<tr>
<td style="text-align: left;">SPAIN</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">89</td>
</tr>
<tr>
<td style="text-align: left;">AUSTRALIA</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">131</td>
</tr>
<tr>
<td style="text-align: left;">RUSSIA</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">81</td>
</tr>
</tbody>
</table>
<p>To gain a comprehensive understanding of the collaboration process over time, we plotted a cluster analysis of international collaboration over years, as shown in Figure 6. The major collaborations began in 2018 among countries in different continents (Figure 6(a)), including USA, China, Germany, South Korea, India, England, and Japan. In 2019 (Figure 6(b)), additional countries and regions participated in the international collaboration related to LLMs research, including Sweden, Greece, Spain, and Italy, which were mostly from Europe.</p>
<p>The increasing number of participating countries in LLMs research collaborations until 2022 suggests a growing interest in this research area among researchers worldwide (Figure 6(c)). This trend not only highlights the popularity of LLMs research, but also demonstrates its global significance as a research domain. Overall, Figure $X$ offers valuable insights into the evolution of international collaboration in LLMs research. Although the figure only depicts the current international collaboration status, it conveys the expansion of LLMs research beyond traditional boundaries and the emergence of new collaborative networks in this field.</p>
<p>Figure 6. Collaboration networks of active countries and regions in selected years
(a) 2018
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
(c) 2022
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Building on the insights gained from previous analysis, we conducted further analysis and plotted the collaboration networks of active countries and regions for the identified research themes in Section 4.1. The results are presented in Figure 7. In examining the networks for the three most popular research themes, we found that the USA and China are at the forefront of research on algorithm and NLP tasks (Figure 7(a)) as well as medical and engineering applications, as illustrated by the relatively larger circle sizes in comparison to others. Additionally, we observed that several countries and regions such as Germany, England, and India remain central across all three themes, indicating their continued importance in the collaboration network.</p>
<p>Specifically, when examining the theme of Algorithms and NLP Tasks, we found that the USA and China are the two leading countries. However, Japan, Canada, South Korea, England, and Germany are also central in the network (Figure 7(b)). Russia tends to collaborate on Algorithms and Medical and Engineering Applications but does not show frequent participation in the other two research themes. In the theme of Medical and Engineering Applications, countries active in Algorithm and NLP Tasks are still active in this area. We also observed that countries or regions, such as Saudi Arabia, Australia, and Sweden, which are not actively engaged in algorithm studies, show frequent collaboration in this research theme. In comparison to the first two research themes presented in Figure 7, the Social and Humanitarian</p>
<p>Applications theme shows a more distributed network, with several countries occupying central positions (Figure 7(c)). For instance, India is predominantly involved in collaborations on such applications. The decentralized nature of this theme indicates that research in social and humanitarian areas is not dominated by a single country or region, but rather involves collaborations between many researchers around the world.</p>
<p>Figure 7. Collaboration networks of active countries and regions by research themes
(a) Algorithm and NLP Tasks
<img alt="img-9.jpeg" src="img-9.jpeg" />
(b) Medical and Engineering Applications
<img alt="img-10.jpeg" src="img-10.jpeg" />
(c) Social and Humanitarian Applications
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<h1>4.2.2 Active organizations in research collaborations</h1>
<p>Figure 8 displays the distribution network of contributing organizations for research papers related to LLM. Table 3 lists the top organizations in the institutional collaboration network. The Chinese Academy of Sciences ranks first with 205 published papers, followed by the University of California System and Microsoft. Other notable institutions in the center of the network include Tsinghua University, Stanford University, Google, the University of Texas System, and Massachusetts Institute of Technology. The ranking of these institutions in terms of centrality mostly aligns with the number of papers they have published. As such, the organizations with the most published papers are also the most important and located at the center of the collaboration network. Out of the top 20 organizations with the highest degree, 14 are universities, 2 are research institutions, and 4 are tech companies.</p>
<p>Figure 8 also displays the collaboration over different years. Interestingly, we observed that early works, such as publications in 2017 and 2018, are often peripheral to the network. Although these works laid the foundation for subsequent collaborations, some of the institutions that collaborated during that time did not remain at the center of the network. For instance, in 2017, New York University, the United States Navy, the United States Department of Defense, and Beijing University of Technology were involved in early works but did not stay at the center of the collaboration network. We also observed that certain institutions could carry out the implementations separately. For instance, in 2018, the Max Planck Society collaborated with other institutions to create a cluster, but its impact did not seem to endure.</p>
<p>Most recent research projects have been conducted through partnerships between academic and industrial organizations. Notable examples include collaborations between major tech companies and university systems, such as the joint efforts between the California University System and Stanford University with Microsoft and Google in the United States, as well as between Tsinghua University and Peking University with Tencent in China. It was also worth noting that these recent significant research projects might not have directly involved the earlier institutions mentioned. Moreover, in later years, the central entities in the collaboration network remained relatively constant, while various peripheral organizations began to contribute to the research efforts.</p>
<p>Regarding collaboration patterns, it was discovered that universities have been the primary contributors to LLMs research collaborations, as illustrated by that 14 out of the top 20 organizations in Table 3 are universities. Although universities continue to play a crucial role in these endeavors, large tech corporations such as Google, Microsoft, Meta (formerly Facebook), and Tencent have also become increasingly significant collaborators. As previously noted, the combination of academic and industrial organizations has resulted in numerous significant works in this field, emphasizing the importance of cooperation between academia and industry.</p>
<p>Figure 8. Collaboration networks of active organizations, from overview and selected years</p>
<p>(a) Overall with top organizations
<img alt="img-12.jpeg" src="img-12.jpeg" />
(b) 2017
<img alt="img-13.jpeg" src="img-13.jpeg" />
(c) 2018
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Table 3. Top organizations in the institutional collaboration network</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Organization</th>
<th style="text-align: left;">Frequency</th>
<th style="text-align: left;">Degree</th>
<th style="text-align: left;">Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chinese Academy of Sciences</td>
<td style="text-align: left;">205</td>
<td style="text-align: left;">60</td>
<td style="text-align: left;">Research Institute</td>
</tr>
<tr>
<td style="text-align: left;">University of California System</td>
<td style="text-align: left;">104</td>
<td style="text-align: left;">59</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Microsoft</td>
<td style="text-align: left;">95</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">Company</td>
</tr>
<tr>
<td style="text-align: left;">Tsinghua University</td>
<td style="text-align: left;">86</td>
<td style="text-align: left;">43</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Stanford University</td>
<td style="text-align: left;">56</td>
<td style="text-align: left;">40</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Google Incorporated</td>
<td style="text-align: left;">86</td>
<td style="text-align: left;">37</td>
<td style="text-align: left;">Company</td>
</tr>
<tr>
<td style="text-align: left;">University of Texas System</td>
<td style="text-align: left;">50</td>
<td style="text-align: left;">37</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Massachusetts Institute of Technology (MIT)</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">36</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">University of Edinburgh</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Centre National de la Recherche <br> Scientifique (CNRS)</td>
<td style="text-align: left;">60</td>
<td style="text-align: left;">35</td>
<td style="text-align: left;">Research Institute</td>
</tr>
<tr>
<td style="text-align: left;">Tencent</td>
<td style="text-align: left;">43</td>
<td style="text-align: left;">34</td>
<td style="text-align: left;">Company</td>
</tr>
<tr>
<td style="text-align: left;">Peking University</td>
<td style="text-align: left;">79</td>
<td style="text-align: left;">34</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">University System of Georgia</td>
<td style="text-align: left;">37</td>
<td style="text-align: left;">33</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">New York University</td>
<td style="text-align: left;">34</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Georgia Institute of Technology</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">29</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Facebook Inc</td>
<td style="text-align: left;">29</td>
<td style="text-align: left;">29</td>
<td style="text-align: left;">Company</td>
</tr>
<tr>
<td style="text-align: left;">UDICE-French Research Universities</td>
<td style="text-align: left;">53</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">Pennsylvania Commonwealth System of <br> Higher Education (PCSHE)</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">University of Science \&amp; Technology of China</td>
<td style="text-align: left;">47</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">University</td>
</tr>
<tr>
<td style="text-align: left;">National University of Singapore</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">University</td>
</tr>
</tbody>
</table>
<h1>5. Discussion</h1>
<p>In our discourse and bibliometric analysis, we have identified the research paradigms and collaborations of LLMs research using computational methods, namely topic modeling and</p>
<p>network analysis. The implementation of these methods aims to provide a high-level and accurate depiction of the emerging and expanding landscapes of LLMs research.</p>
<p>The dynamic nature and fast evolution of LLMs research have led to significant advancements in natural language understanding and processing capabilities, with applications across diverse domains such as Medical, Engineering, Social, and Humanitarian fields. The synergistic workforce in LLMs research involving international and organizational collaborations plays a crucial role in the growth and development of this research area.</p>
<p>However, challenges remain due to the current movements and tensions in the development and application of LLMs. The power of LLMs is not yet clearly or openly analyzed before the release of end-user tools, such as ChatGPT and GPT-4 (Fridman, 2023). There is also a division between proponents and opponents of LLMs research and application, for instance, the open letter to pause giant Al experiments (Future of Life Institute, n.d.).</p>
<p>We regard our study as a glimpse of the modern history of LLMs research, which can be informative to newcomers of this field, policy makers of Al regulations, as well as researchers in science and technology studies. It is hard to predict the future of LLMs, while understanding its past can at least provide the knowledge foundation and warnings for future research.</p>
<h1>5.1 The dynamic nature and fast evolution of LLMs research: from algorithms to applications and beyond</h1>
<p>It is vital to recognize the dynamic nature and fast evolution, as well as the corresponding opportunities and challenges, of the research field of LLMs. The growing interest and the diverse range of themes indicate a promising future for new discoveries and advancements. As researchers continue to explore and develop novel algorithms, techniques, and applications, it is vital to recognize the dynamic nature of the research field of LLMs. The current ability of LLM algorithms has significantly improved natural language understanding and processing capabilities. These advancements have enabled researchers to tackle complex language tasks that are applicable to handle a wide range of applications across domains, including Medical, Engineering, Social, and Humanitarian fields of research. In the Social and Humanitarian domain, LLMs have been applied to analyze social media and news data, particularly with respect to sentiment, opinion, and controversial content. In the Medical and Engineering domain, LLMs are utilized to solve complex problems, from processing electronic medical records and studying specific categories of diseases to automating software similarity analysis. These diverse applications showcase the power and versatility of LLMs in addressing real-world challenges and driving advancements in various fields.</p>
<p>From algorithms to applications in LLMs research, there is smooth knowledge transfer among different subdomains, including specialized applications. The high semantic closeness among Algorithm and NLP Tasks and Social and Humanitarian Applications (Figure 3), for instance, indicates that researchers in these fields work across disciplines and share insights and expertise to develop novel solutions. This interdisciplinary approach implies that challenges in</p>
<p>LLMs research related to Social and Humanitarian Applications are not confined to a single solution, while researchers from different backgrounds can contribute to and benefit from shared knowledge and expertise. Such collaboration can lead to more efficient problem-solving approaches and result in more impactful and far-reaching social and humanitarian applications. Moreover, the Medical and Engineering Applications theme is a comprehensive cluster of various highly professional and semantically related sub-domains in LLMs research, showing the adaptivity of LLMs algorithms. By applying pre-trained LLMs and fine-tuning them for specific tasks, researchers can leverage the power of LLMs to improve healthcare and advance engineering practices. Specifically, many top keywords among publications in Medical and Engineering Applications are general LLMs algorithms and NLP tasks, such as named entity recognition (NER) and question-answering (QA) (Figure 4(b)). This adaptability highlights the immense potential for LLMs to revolutionize various industries and contribute to overall societal progress.</p>
<p>At the same time, challenges remain in the development and application of LLMs, many of which are due the complexity and uncertainty in the dynamic nature and fast evolution of LLMs (Johanna Okerlund, Evan Klasky, Aditya Middha, Sujin Kim, Hannah Rosenfeld, Molly Kleinman, Shobita Parthasarathy, 2022; Weidinger et al., 2021). Some algorithms are not widely applied due to a variety of factors, such as computational complexity, lack of interpretability, or ethical concerns. Computational complexity can limit the scalability of certain LLMs, making them less accessible for researchers with limited resources, as well as exacerbating environmental injustice and social fragmentation. Vast computing power is required to train LLMs, coming at a significant environmental cost, while these models are not outperforming more eco-friendly models in many use cases (Goetze \&amp; Abramson, 2021). In addition, the lack of interpretability in LLMs may hinder trust and adoption in critical applications, as users may be hesitant to rely on "black box" solutions. For example, in question-answering systems or chatbots, models can mimic human-like thought and behavior, like "stochastic parrots", without fully understanding the implications of such technology (Bender et al., 2021).</p>
<p>Furthermore, ethical concerns regarding biases, privacy, and other unintended consequences may prevent the widespread use of certain LLMs. First, these models can carry on existing biases in society and exacerbate them through fast and low-cost applications. For example, there is persistent anti-Muslim bias in some LLMs (Abid et al., 2021). Second, it is possible to extract personally identifiable information (PII) and other sensitive information from LLMs, raising the possibility that the massive dataset use in models can result in privacy information leaks. (Carlini et al., 2020). Other malicious uses of LLMs, such as spreading disinformation or creating fake news, can also strengthen bias and lead to social factorization problems (Guembe et al., 2022; Yamin et al., 2021). Therefore, addressing these challenges is essential to ensure the responsible and inclusive development and application of LLMs in the future.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Note that topical the keywords here are the Web of Science (WoS) keywords, not the topical keywords generated by the BERTopic algorithm.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9502 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9502</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9502</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-38472e4242e0aa632ed594c3b0ed9c0bd6429c41</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/38472e4242e0aa632ed594c3b0ed9c0bd6429c41" target="_blank">AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</a></p>
                <p><strong>Paper Venue:</strong> ACM Trans. Interact. Intell. Syst.</p>
                <p><strong>Paper TL;DR:</strong> The results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (“superforecasting”) advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants (N \(=\) 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9502.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9502.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo (Superforecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview) with Superforecasting Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A frontier large language model (GPT-4-Turbo) deployed with an expert 'superforecasting' system prompt to provide structured probabilistic and continuous forecasts and uncertainty intervals in interactive human-in-the-loop forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model released by OpenAI; used here with a 128k token context window, max output limit set to 1024 tokens and temperature = 0.8; trained on data up to April 2023 and instructed via a 'superforecaster' system prompt to apply reference-class reasoning, produce uncertainty intervals, and give numerical predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Six future real-world forecasting questions, including (among others) the number of AI papers to be published on arXiv in December 2023 (a future scientific-output event), Bitcoin network hash rate, Dow Jones Transportation Average closing value, global commercial flights on Dec 31 2023, Mediterranean sea arrivals in December 2023, and USD/RUB closing rate on Dec 30 2023.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Interactive chat-based prompting with a superforecasting system prompt; the model could be asked for direct numerical point estimates or uncertainty intervals and could engage in back-and-forth with human forecasters; a held-out direct-forecast probe (model given only question text) was also collected for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Continuous numeric point estimates for continuous targets and uncertainty intervals; for binary/skill calibration questions probability percentages were used (Brier-scored).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Accuracy measured primarily as absolute error (D_i = |F_i - A_i|) standardized and winsorized; supplementary evaluation included direct LLM deviation from truth (percentage deviation table), Brier score for binary skill questions, Euclidean distance for intersubjective forecasts, ANOVA/Tukey post-hoc tests for group comparisons, and bootstrap of medians for aggregate analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>When used to augment human forecasters, GPT-4-Turbo with the superforecasting prompt significantly improved aggregate human forecasting accuracy relative to the control baseline (reported aggregate improvement ~24–28% over control in preregistered analyses). In direct single-run model forecasts (no human-in-loop probe), the superforecasting GPT-4-Turbo had smaller percent deviations from truth than the 'noisy' variant on all six main questions (Table 3: deviations ranged from -55.05% to +19.88% depending on question). Exact aggregate metrics: treatment mean score 0.68 vs control 0.89 (lower = better); ANOVA F(2,988)=34.58, p<.001.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Results sensitive to an outlier (Question 3: Bitcoin hash rate) which materially affected comparisons between model variants; the model had no internet access (could not consult live data); supervised prompt design may not generalize; control condition used a weaker LLM (DaVinci-003) rather than a human-only baseline, so comparisons to unaided humans are indirect; potential for anchoring/homogenization of forecasts was investigated but results were mixed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to a control using a weaker LLM (DaVinci-003 instructed not to provide forecasts), the GPT-4-Turbo superforecasting augmentation produced significantly better individual-level accuracy (mean difference = -0.21, p<.001). Direct superforecasting model predictions also outperformed a 'noisy' GPT-4-Turbo variant in single-run probes (Table 3). The paper also references prior findings where GPT-4 (earlier) underperformed median human-crowd forecasts, and other work where LLM ensembles or retrieval-augmented methods approached human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Expert/system prompt engineering (superforecasting prompt drawing on '10 commandments'); interactive human-in-the-loop back-and-forth rather than one-shot; suggestions in paper and references to ensembling diverse LLM forecasts and retrieval-augmentation (RAG) as methods to approach or improve human-level forecasting; temperature and output limits tuned (temperature=0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9502.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9502.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-Turbo (Noisy)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview) with Noisy / Biased Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same GPT-4-Turbo model but instructed via a prompt engineered to induce overconfidence and base-rate neglect, producing biased/noisy numerical forecasts while still outputting numerical predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4-Turbo (gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; used with an explicit 'noisy' system prompt that instructs the model to favor extreme personal views, avoid base rates, and display overconfidence; same runtime hyperparameters (max output 1024 tokens, temperature=0.8).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Same six forecasting questions as the superforecasting condition (including the arXiv AI-paper count question and other real-world events).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Interactive chat; model instructed to provide numerical forecasts and rationale but guided to produce biased forecasts (base-rate neglect and overconfidence). Direct model-only probe forecasts were also recorded.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Continuous numeric point estimates and accompanying rationales; for binary skill items probabilities (percentages) used for Brier scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same absolute-error based accuracy metric for main questions, plus direct-deviation table for single-run model outputs, ANOVA/Tukey pairwise tests, and bootstrap medians for aggregate comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Noisy-LLM augmentation also significantly improved individual human forecasting accuracy versus the DaVinci-003 control in preregistered aggregate analyses (mean difference = -0.25, p<.001). In direct single-run model probes, the noisy variant had larger percent deviations from truth on all six questions compared to the superforecasting variant (e.g., +322.48% on Question 5) but when used interactively with humans it nonetheless improved human performance relative to the weak control. In aggregate median-of-medians bootstrapped measure noisy condition showed highest aggregate accuracy in preregistered analysis, driven by an outlier question.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Noisy prompt produced extreme/misleading predictions (notably produced median forecasts five orders of magnitude higher on the Bitcoin hash-rate question), produced heterogenous effects and sometimes unexpectedly improved aggregate accuracy due to question-specific anomalies; noisy assistance can mislead users and produced calibration issues in direct probes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperformed the weak-model control at individual level, but underperformed the superforecasting model in direct-probe accuracy on all six questions; however, in preregistered aggregate median analyses the noisy condition unexpectedly showed best aggregate performance due to the outlier (Question 3), so comparisons are question-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Paper suggests prompt engineering (avoid biased prompts), aggregation and ensembling, and retrieval-augmentation as avenues to improve calibration and reduce biased outputs; interactive human oversight mitigates some harms but is not fail-proof.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9502.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9502.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci-003 (Control)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaVinci-003 instructed not to forecast (Control LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A substantially smaller/weaker OpenAI model used as the control condition, explicitly instructed not to provide numerical forecasts but to act as a generic helpful assistant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier generation OpenAI model (GPT-3 era variant) used with a control prompt that forbids producing forecasts; presented via an identical chat interface to treatment groups to control for engagement effects.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>None — instructed not to provide forecasts; participants in control still produced forecasts themselves without numerical assistance from the control LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Control LLM did not generate probabilities/forecasts; its presence served as a baseline for human+weak-LLM interaction vs human+advanced-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>N/A for model output; human forecasts elicited in continuous numeric and probability formats for skill items.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Participants' forecasts (human-only with weak LLM present) evaluated via same absolute-error metric and other analyses as treatments (Brier scores for skill questions).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Control group had worse individual forecasting accuracy than both GPT-4-Turbo treatments in preregistered analyses (control mean score 0.89 vs treatment 0.68 and noisy 0.64; pairwise differences significant). Aggregate median scores placed the control worst in preregistered bootstrap results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>As a control, it does not allow inference about whether advanced LLMs improve human forecasts versus unaided humans (no human-only baseline); control's instruction not to forecast may reduce its utility and is not identical to a true absence of LLM assistance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Serves as the study's baseline; both GPT-4-Turbo variants outperformed this control at individual-accuracy level.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>N/A — used as a constrained baseline; paper notes future work should compare advanced LLM assistance to truly unaided human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9502.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9502.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 in Forecasting Tournament (Prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated against human crowd in forecasting tournament (Schoenegger & Park 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited in the paper found that GPT-4 (March 2023 release) significantly underperformed the median human-crowd forecast in a real-world forecasting tournament, failing to beat uniform random guessing in that setting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior-generation GPT-4 model (evaluated in Schoenegger & Park 2023) used in a study comparing single-run LLM forecasts to median human-crowd forecasts in real-world forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Real-world forecasting tournament questions (prospective events) used in that tournament; not all were scientific discoveries, but included measurable future events.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Direct LLM-generated forecasts (no human-in-the-loop augmentation), evaluated against human-crowd forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Numeric forecasts/probabilities depending on question; compared to crowd medians.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison to median human-crowd forecast and to uniform random guessing; statistical performance assessment in forecasting tournament context.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported to significantly underperform the median human-crowd forecast and failed to significantly outperform uniform random guessing in that study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Single-run LLM forecasts without human augmentation; potential issues of overfitting to benchmarks and inability to access out-of-sample information; limitations motivate human-in-the-loop hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Underperformed median human-crowd and did not beat a no-information uniform strategy in that prior evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Paper and present study suggest that interactive human-in-the-loop augmentation, better prompting, ensembling, and retrieval-augmentation could improve outcomes relative to single-run LLM forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9502.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9502.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Ensembles (Prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensembled/diverse LLM forecasts ('Wisdom of the silicon crowd')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work cited shows that aggregating diverse LLM forecasts or ensemble methods can approach or match human crowd accuracy on forecasting tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ensembles of LLMs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approaches that combine forecasts from multiple diverse LLM instances/models (an 'ensemble' or 'silicon crowd') to produce aggregated probabilistic forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Forecasting tasks similar to those used in forecasting tournaments (prospective real-world events including counts and continuous outcomes); aims include scientific-output counts as a subset.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Generate multiple LLM forecasts (possibly using different prompts or model variants) and aggregate them (e.g., median/mean ensemble) to produce a combined forecast.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Aggregated numeric forecasts and probabilities produced by combining individual model outputs (e.g., median/mean or more complex ensemble methods).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare ensemble forecasts to human-crowd accuracy using absolute-error metrics, calibration measures, and tournament-style scoring; referenced as approaching human-level performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Referenced work reports LLM ensemble prediction capabilities that match human crowd accuracy or approach human-level forecasting performance (cited as Schoenegger et al. 2024b).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Ensembling may reduce diversity if models are correlated; ensembles require computation and careful diversity engineering; may still suffer from shared model biases or data contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Ensembles compared favorably to individual LLMs and in some cases to human crowds, approaching or matching aggregate human accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Diversity of model inputs, prompt variation, retrieval augmentation (RAG), and weighting schemes for ensemble aggregation recommended to improve ensemble performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9502.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9502.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented LLMs (Prior)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) systems for forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work indicates that retrieval-augmented LLMs (which can consult external information stores) can approach human-level forecasting accuracy by augmenting the model's access to up-to-date or domain-specific information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Approaching Human-Level Forecasting with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-Augmented LLM (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Systems that combine a base LLM with a retrieval component that fetches relevant documents or data to ground model responses, improving access to up-to-date or domain-specific evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Prospective forecasting tasks where access to external or recent data (e.g., time series, event reports) can improve prediction of future events or outputs, including scientific publication counts.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Model generates forecasts conditioned on retrieved documents or external evidence (RAG pipeline) rather than relying solely on its frozen pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Numeric point estimates and uncertainty intervals derived from model reasoning over retrieved evidence; can include probabilistic statements.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to human forecasts and non-RAG LLM forecasts using accuracy metrics (absolute error, calibration), with reported improvements when retrieval is available.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Referenced work (Halawi et al. 2024) reports that RAG approaches can approach human-level forecasting performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality depends on retrieval relevance, retrieval latency and cost, and potential to retrieve misleading or noisy documents; integration with human-in-the-loop workflows still requires governance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>RAG systems tend to outperform vanilla single-run LLM forecasts and can approach ensembles/human crowds in reported studies.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Improve retrieval quality, grounding, and prompt conditioning; combine RAG with ensembling and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament <em>(Rating: 2)</em></li>
                <li>Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy <em>(Rating: 2)</em></li>
                <li>Approaching Human-Level Forecasting with Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models Are Zero-Shot Time Series Forecasters <em>(Rating: 1)</em></li>
                <li>Hybrid Forecasting of Geopolitical Events <em>(Rating: 1)</em></li>
                <li>Hybrid Forecasting: Combining Human- and Machine-Generated Forecasts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9502",
    "paper_id": "paper-38472e4242e0aa632ed594c3b0ed9c0bd6429c41",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "GPT-4-Turbo (Superforecasting)",
            "name_full": "GPT-4-Turbo (gpt-4-1106-preview) with Superforecasting Prompt",
            "brief_description": "A frontier large language model (GPT-4-Turbo) deployed with an expert 'superforecasting' system prompt to provide structured probabilistic and continuous forecasts and uncertainty intervals in interactive human-in-the-loop forecasting tasks.",
            "citation_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (gpt-4-1106-preview)",
            "model_description": "Transformer-based large language model released by OpenAI; used here with a 128k token context window, max output limit set to 1024 tokens and temperature = 0.8; trained on data up to April 2023 and instructed via a 'superforecaster' system prompt to apply reference-class reasoning, produce uncertainty intervals, and give numerical predictions.",
            "model_size": null,
            "prediction_target": "Six future real-world forecasting questions, including (among others) the number of AI papers to be published on arXiv in December 2023 (a future scientific-output event), Bitcoin network hash rate, Dow Jones Transportation Average closing value, global commercial flights on Dec 31 2023, Mediterranean sea arrivals in December 2023, and USD/RUB closing rate on Dec 30 2023.",
            "prediction_method": "Interactive chat-based prompting with a superforecasting system prompt; the model could be asked for direct numerical point estimates or uncertainty intervals and could engage in back-and-forth with human forecasters; a held-out direct-forecast probe (model given only question text) was also collected for comparison.",
            "probability_format": "Continuous numeric point estimates for continuous targets and uncertainty intervals; for binary/skill calibration questions probability percentages were used (Brier-scored).",
            "evaluation_method": "Accuracy measured primarily as absolute error (D_i = |F_i - A_i|) standardized and winsorized; supplementary evaluation included direct LLM deviation from truth (percentage deviation table), Brier score for binary skill questions, Euclidean distance for intersubjective forecasts, ANOVA/Tukey post-hoc tests for group comparisons, and bootstrap of medians for aggregate analyses.",
            "results": "When used to augment human forecasters, GPT-4-Turbo with the superforecasting prompt significantly improved aggregate human forecasting accuracy relative to the control baseline (reported aggregate improvement ~24–28% over control in preregistered analyses). In direct single-run model forecasts (no human-in-loop probe), the superforecasting GPT-4-Turbo had smaller percent deviations from truth than the 'noisy' variant on all six main questions (Table 3: deviations ranged from -55.05% to +19.88% depending on question). Exact aggregate metrics: treatment mean score 0.68 vs control 0.89 (lower = better); ANOVA F(2,988)=34.58, p&lt;.001.",
            "limitations_or_challenges": "Results sensitive to an outlier (Question 3: Bitcoin hash rate) which materially affected comparisons between model variants; the model had no internet access (could not consult live data); supervised prompt design may not generalize; control condition used a weaker LLM (DaVinci-003) rather than a human-only baseline, so comparisons to unaided humans are indirect; potential for anchoring/homogenization of forecasts was investigated but results were mixed.",
            "comparison_to_baselines": "Compared to a control using a weaker LLM (DaVinci-003 instructed not to provide forecasts), the GPT-4-Turbo superforecasting augmentation produced significantly better individual-level accuracy (mean difference = -0.21, p&lt;.001). Direct superforecasting model predictions also outperformed a 'noisy' GPT-4-Turbo variant in single-run probes (Table 3). The paper also references prior findings where GPT-4 (earlier) underperformed median human-crowd forecasts, and other work where LLM ensembles or retrieval-augmented methods approached human-level performance.",
            "methods_for_improvement": "Expert/system prompt engineering (superforecasting prompt drawing on '10 commandments'); interactive human-in-the-loop back-and-forth rather than one-shot; suggestions in paper and references to ensembling diverse LLM forecasts and retrieval-augmentation (RAG) as methods to approach or improve human-level forecasting; temperature and output limits tuned (temperature=0.8).",
            "uuid": "e9502.0",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4-Turbo (Noisy)",
            "name_full": "GPT-4-Turbo (gpt-4-1106-preview) with Noisy / Biased Prompt",
            "brief_description": "Same GPT-4-Turbo model but instructed via a prompt engineered to induce overconfidence and base-rate neglect, producing biased/noisy numerical forecasts while still outputting numerical predictions.",
            "citation_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
            "mention_or_use": "use",
            "model_name": "GPT-4-Turbo (gpt-4-1106-preview)",
            "model_description": "As above; used with an explicit 'noisy' system prompt that instructs the model to favor extreme personal views, avoid base rates, and display overconfidence; same runtime hyperparameters (max output 1024 tokens, temperature=0.8).",
            "model_size": null,
            "prediction_target": "Same six forecasting questions as the superforecasting condition (including the arXiv AI-paper count question and other real-world events).",
            "prediction_method": "Interactive chat; model instructed to provide numerical forecasts and rationale but guided to produce biased forecasts (base-rate neglect and overconfidence). Direct model-only probe forecasts were also recorded.",
            "probability_format": "Continuous numeric point estimates and accompanying rationales; for binary skill items probabilities (percentages) used for Brier scoring.",
            "evaluation_method": "Same absolute-error based accuracy metric for main questions, plus direct-deviation table for single-run model outputs, ANOVA/Tukey pairwise tests, and bootstrap medians for aggregate comparisons.",
            "results": "Noisy-LLM augmentation also significantly improved individual human forecasting accuracy versus the DaVinci-003 control in preregistered aggregate analyses (mean difference = -0.25, p&lt;.001). In direct single-run model probes, the noisy variant had larger percent deviations from truth on all six questions compared to the superforecasting variant (e.g., +322.48% on Question 5) but when used interactively with humans it nonetheless improved human performance relative to the weak control. In aggregate median-of-medians bootstrapped measure noisy condition showed highest aggregate accuracy in preregistered analysis, driven by an outlier question.",
            "limitations_or_challenges": "Noisy prompt produced extreme/misleading predictions (notably produced median forecasts five orders of magnitude higher on the Bitcoin hash-rate question), produced heterogenous effects and sometimes unexpectedly improved aggregate accuracy due to question-specific anomalies; noisy assistance can mislead users and produced calibration issues in direct probes.",
            "comparison_to_baselines": "Outperformed the weak-model control at individual level, but underperformed the superforecasting model in direct-probe accuracy on all six questions; however, in preregistered aggregate median analyses the noisy condition unexpectedly showed best aggregate performance due to the outlier (Question 3), so comparisons are question-dependent.",
            "methods_for_improvement": "Paper suggests prompt engineering (avoid biased prompts), aggregation and ensembling, and retrieval-augmentation as avenues to improve calibration and reduce biased outputs; interactive human oversight mitigates some harms but is not fail-proof.",
            "uuid": "e9502.1",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DaVinci-003 (Control)",
            "name_full": "DaVinci-003 instructed not to forecast (Control LLM)",
            "brief_description": "A substantially smaller/weaker OpenAI model used as the control condition, explicitly instructed not to provide numerical forecasts but to act as a generic helpful assistant.",
            "citation_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
            "mention_or_use": "use",
            "model_name": "DaVinci-003",
            "model_description": "Earlier generation OpenAI model (GPT-3 era variant) used with a control prompt that forbids producing forecasts; presented via an identical chat interface to treatment groups to control for engagement effects.",
            "model_size": null,
            "prediction_target": "None — instructed not to provide forecasts; participants in control still produced forecasts themselves without numerical assistance from the control LLM.",
            "prediction_method": "Control LLM did not generate probabilities/forecasts; its presence served as a baseline for human+weak-LLM interaction vs human+advanced-LLM.",
            "probability_format": "N/A for model output; human forecasts elicited in continuous numeric and probability formats for skill items.",
            "evaluation_method": "Participants' forecasts (human-only with weak LLM present) evaluated via same absolute-error metric and other analyses as treatments (Brier scores for skill questions).",
            "results": "Control group had worse individual forecasting accuracy than both GPT-4-Turbo treatments in preregistered analyses (control mean score 0.89 vs treatment 0.68 and noisy 0.64; pairwise differences significant). Aggregate median scores placed the control worst in preregistered bootstrap results.",
            "limitations_or_challenges": "As a control, it does not allow inference about whether advanced LLMs improve human forecasts versus unaided humans (no human-only baseline); control's instruction not to forecast may reduce its utility and is not identical to a true absence of LLM assistance.",
            "comparison_to_baselines": "Serves as the study's baseline; both GPT-4-Turbo variants outperformed this control at individual-accuracy level.",
            "methods_for_improvement": "N/A — used as a constrained baseline; paper notes future work should compare advanced LLM assistance to truly unaided human baselines.",
            "uuid": "e9502.2",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 in Forecasting Tournament (Prior)",
            "name_full": "GPT-4 evaluated against human crowd in forecasting tournament (Schoenegger & Park 2023)",
            "brief_description": "Prior work cited in the paper found that GPT-4 (March 2023 release) significantly underperformed the median human-crowd forecast in a real-world forecasting tournament, failing to beat uniform random guessing in that setting.",
            "citation_title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Prior-generation GPT-4 model (evaluated in Schoenegger & Park 2023) used in a study comparing single-run LLM forecasts to median human-crowd forecasts in real-world forecasting tasks.",
            "model_size": null,
            "prediction_target": "Real-world forecasting tournament questions (prospective events) used in that tournament; not all were scientific discoveries, but included measurable future events.",
            "prediction_method": "Direct LLM-generated forecasts (no human-in-the-loop augmentation), evaluated against human-crowd forecasts.",
            "probability_format": "Numeric forecasts/probabilities depending on question; compared to crowd medians.",
            "evaluation_method": "Comparison to median human-crowd forecast and to uniform random guessing; statistical performance assessment in forecasting tournament context.",
            "results": "Reported to significantly underperform the median human-crowd forecast and failed to significantly outperform uniform random guessing in that study.",
            "limitations_or_challenges": "Single-run LLM forecasts without human augmentation; potential issues of overfitting to benchmarks and inability to access out-of-sample information; limitations motivate human-in-the-loop hybrid approaches.",
            "comparison_to_baselines": "Underperformed median human-crowd and did not beat a no-information uniform strategy in that prior evaluation.",
            "methods_for_improvement": "Paper and present study suggest that interactive human-in-the-loop augmentation, better prompting, ensembling, and retrieval-augmentation could improve outcomes relative to single-run LLM forecasts.",
            "uuid": "e9502.3",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLM Ensembles (Prior)",
            "name_full": "Ensembled/diverse LLM forecasts ('Wisdom of the silicon crowd')",
            "brief_description": "Prior work cited shows that aggregating diverse LLM forecasts or ensemble methods can approach or match human crowd accuracy on forecasting tasks.",
            "citation_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
            "mention_or_use": "mention",
            "model_name": "Ensembles of LLMs (various)",
            "model_description": "Approaches that combine forecasts from multiple diverse LLM instances/models (an 'ensemble' or 'silicon crowd') to produce aggregated probabilistic forecasts.",
            "model_size": null,
            "prediction_target": "Forecasting tasks similar to those used in forecasting tournaments (prospective real-world events including counts and continuous outcomes); aims include scientific-output counts as a subset.",
            "prediction_method": "Generate multiple LLM forecasts (possibly using different prompts or model variants) and aggregate them (e.g., median/mean ensemble) to produce a combined forecast.",
            "probability_format": "Aggregated numeric forecasts and probabilities produced by combining individual model outputs (e.g., median/mean or more complex ensemble methods).",
            "evaluation_method": "Compare ensemble forecasts to human-crowd accuracy using absolute-error metrics, calibration measures, and tournament-style scoring; referenced as approaching human-level performance.",
            "results": "Referenced work reports LLM ensemble prediction capabilities that match human crowd accuracy or approach human-level forecasting performance (cited as Schoenegger et al. 2024b).",
            "limitations_or_challenges": "Ensembling may reduce diversity if models are correlated; ensembles require computation and careful diversity engineering; may still suffer from shared model biases or data contamination.",
            "comparison_to_baselines": "Ensembles compared favorably to individual LLMs and in some cases to human crowds, approaching or matching aggregate human accuracy.",
            "methods_for_improvement": "Diversity of model inputs, prompt variation, retrieval augmentation (RAG), and weighting schemes for ensemble aggregation recommended to improve ensemble performance.",
            "uuid": "e9502.4",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Retrieval-Augmented LLMs (Prior)",
            "name_full": "Retrieval-Augmented Generation (RAG) systems for forecasting",
            "brief_description": "Cited prior work indicates that retrieval-augmented LLMs (which can consult external information stores) can approach human-level forecasting accuracy by augmenting the model's access to up-to-date or domain-specific information.",
            "citation_title": "Approaching Human-Level Forecasting with Language Models",
            "mention_or_use": "mention",
            "model_name": "Retrieval-Augmented LLM (RAG)",
            "model_description": "Systems that combine a base LLM with a retrieval component that fetches relevant documents or data to ground model responses, improving access to up-to-date or domain-specific evidence.",
            "model_size": null,
            "prediction_target": "Prospective forecasting tasks where access to external or recent data (e.g., time series, event reports) can improve prediction of future events or outputs, including scientific publication counts.",
            "prediction_method": "Model generates forecasts conditioned on retrieved documents or external evidence (RAG pipeline) rather than relying solely on its frozen pretraining data.",
            "probability_format": "Numeric point estimates and uncertainty intervals derived from model reasoning over retrieved evidence; can include probabilistic statements.",
            "evaluation_method": "Compared to human forecasts and non-RAG LLM forecasts using accuracy metrics (absolute error, calibration), with reported improvements when retrieval is available.",
            "results": "Referenced work (Halawi et al. 2024) reports that RAG approaches can approach human-level forecasting performance.",
            "limitations_or_challenges": "Quality depends on retrieval relevance, retrieval latency and cost, and potential to retrieve misleading or noisy documents; integration with human-in-the-loop workflows still requires governance.",
            "comparison_to_baselines": "RAG systems tend to outperform vanilla single-run LLM forecasts and can approach ensembles/human crowds in reported studies.",
            "methods_for_improvement": "Improve retrieval quality, grounding, and prompt conditioning; combine RAG with ensembling and human oversight.",
            "uuid": "e9502.5",
            "source_info": {
                "paper_title": "AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament",
            "rating": 2
        },
        {
            "paper_title": "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy",
            "rating": 2
        },
        {
            "paper_title": "Approaching Human-Level Forecasting with Language Models",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models Are Zero-Shot Time Series Forecasters",
            "rating": 1
        },
        {
            "paper_title": "Hybrid Forecasting of Geopolitical Events",
            "rating": 1
        },
        {
            "paper_title": "Hybrid Forecasting: Combining Human- and Machine-Generated Forecasts",
            "rating": 1
        }
    ],
    "cost": 0.01718975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy</h1>
<p>Philipp Schoenegger<br>London School of Economics and Political Science<br>Peter S. Park<br>Massachusetts Institute of Technology<br>Ezra Karger<br>Federal Reserve Bank of Chicago*<br>Sean Trott<br>University of California San Diego<br>Philip E. Tetlock<br>University of Pennsylvania</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) match and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment human judgement in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality ('superforecasting') advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engaged in explicit discussion of predictions. Participants ( $\mathrm{N}=991$ ) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between $24 \%$ and $28 \%$ compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by $41 \%$, compared with $29 \%$ for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>Recent advances in artificial intelligence (AI), and large language models (LLMs) specifically, demonstrate impressive AI capabilities across a large number of complex and economically valuable tasks (Naveed et al. 2023). This development challenges previously held beliefs about the necessity of human cognition for many of these tasks (Bubeck et al. 2023), and raises the possibility of significant negative effects of AI systems on the (human) labor market in large parts of the knowledge economy (George and Baskar 2023). Understanding the current ability of LLMs to interface with economically central tasks requires a broad empirical study across domains. However, most knowledge-work jobs require substantial reasoning capabilities that use data and patterns of observations beyond any model's training data. This makes finding a suitable study context central in any attempt to understand how LLMs might impact advanced economies in the near future.</p>
<p>Our focus in this paper is on Large Language Models, which represent a significant advance in AI and natural language processing. These models build upon the transformer architectural paradigm (Vaswani et al. 2017) and are characterized by their massive scale, often containing billions or even trillions of parameters, trained on an enormous amount of diverse textual data (Shen et al. 2023). The core capability of LLMs is next-token prediction: the ability to predict the most probable next word or subword (token) given a sequence of preceding tokens. However, this seemingly simple objective, when scaled, results in a wide array of emergent abilities that seem to extend far beyond basic next-token prediction. These advanced AI systems demonstrate proficiency in tasks such as natural language understanding and generation, few-shot learning, and complex reasoning across various domains. Importantly, many of these specialized advanced skills emerge in ways that could not have been fully predicted before training, due to non-linearities in how capabilities scale with model size and data (Wei et al. 2022).</p>
<p>Some areas where LLMs have shown strong performance are marketing (Fraiwan and Khasawneh 2023), translation (Jiao et al. 2023), high levels of reading comprehension (Winter 2023), teaching (Fraiwan and Khasawneh 2023; Sallam et al. 2023), summarization (Goyal, Li, and Durrett 2023), abstract categorization of objects (Atari et al. 2023), programming (Bubeck et al. 2023; Cheng et al. 2024), spear phishing cyber attacks (Hazell 2023; Heiding et al. 2023), human personality (Schoenegger et al. 2024a), robotics (Vemprala et al. 2023), medical reasoning (Bubeck et al. 2023; Nori et al. 2023; Sallam et al. 2023), legal reasoning (Bubeck et al. 2023; Katz et al. 2023), deception (Park et al. 2023), and others. LLMs’ many capabilities substantially increase the amount of money and talent going into LLM research and products (Sutton 2023), suggesting further growth in capabilities in the near future.</p>
<p>Crucially, modern state-of-the-art or frontier language models are not inherently autonomous for most relevant tasks (Xi et al. 2023). While they can be imbued with general autonomy through agent frameworks like AutoGPT (Firat and Kuleli 2023) or other scaffolding approaches, the reliability of such methods remains questionable. Future iterations of models may enable autonomous behavior directly (Kinniment et al. 2023), potentially making agency—the ability to take actions and achieve goals independently—more accessible. However, at present, LLMs are not economically viable as autonomous agents due to significant limitations including inefficiency, forgetting, speed, cost, cultural complexity (McIntosh et al. 2024) and hallucinations (Firat and Kuleli 2023).</p>
<p>Instead, these models are primarily used in combination with human labor, forming a hybrid technology that necessitates human input at various stages (Dell'Acqua et al. 2023). This synergistic approach allows humans to leverage the strengths of LLMs, producing outcomes that can surpass what either humans or machines could achieve independently. For instance, LLM augmentations have demonstrably enhanced the performance of human graders (Xiao et al. 2024) and programmers (Peng et al. 2023), and have also been applied in the context of co-creating visual stories (Antony and Huang 2023), illustrating the potential of human-AI collaboration in diverse fields.</p>
<p>Our study contributes to the growing research on human-AI collaboration in complex decision-making tasks, a key focus in HCI and AI research (Steyvers and Kumar 2023). By examining LLM-augmented forecasting, we extend recent work on human-AI interaction modes (Gao et al. 2024) and address challenges in AI-assisted decision-making (Steyvers and Kumar 2023). Our approach aligns with calls to develop nuanced understandings of human-AI complementarity (Yang 2024) and explores how different LLM prompts affect forecasting outcomes, contributing to discussions on designing AI systems that effectively augment human cognition (Wang et al. 2024a).</p>
<p>In this paper, we study the application of present-era frontier LLMs as a hybrid augmentation technology in the context of forecasting future events. This allows us to test their ability to augment human decision-making in a domain robust to in-sample overfitting of training data, since no one, including LLMs or the experimenters themselves, can know the answer to prospective forecasting questions at the time of data collection. This context is also practically relevant as accurate forecasting is essential to many aspects of economic activity, especially within white-collar occupational domains such as law, business, and policy: fields that may be disrupted by LLM capabilities (Acemoğlu 2023; Park and Tegmark 2023; Summers and Rattner 2023). If the use of present or future AI systems increases the forecasting accuracy of humans and organizations, the efficiency and productivity</p>
<p>gains to the relevant industries' individuals and businesses are clear, and if there are risks, they ought to be discussed prior to widespread adoption.</p>
<p>Our specific object of interest in this study is human judgment forecasting, where humans provide forecasts of future events, such as the probability that inflation will hit a certain milestone over the next twelve months or the anticipated number of barrels in the Strategic Petroleum Reserve at the end of the year. This context is distinct from the more widely studied topic of time series forecasting (Jin et al. 2023), as the central input are judgements by human forecasters as opposed to machine learning algorithms. The science of judgemental forecasting has found that aggregated forecasts of a crowd of forecasters can be surprisingly accurate (Tetlock and Gardner 2016), can impact policy debates (Tetlock, Mellers, and Scoblic 2017), and can affect businesses (Schoemaker and Tetlock 2016), and that much of this effect is derived from the high accuracy of a subset of forecasters, often called 'superforecasters'. Previous work on the topic focuses on a variety of other topics, ranging from the identification of these highly skilled forecasters (Himmelstein, Budescu, and Han 2023; Mellers et al. 2015b; Tetlock and Gardner 2016) and novel aggregation methods (Atanasov et al. 2017) to improvements of forecasting accuracy (Chang et al. 2016; Karger, Atanasov, and Tetlock 2022) as well as applications to topics like development economics (Bernard and Schoenegger 2024) or pandemics (McAndrew et al. 2024).</p>
<p>Related to our project, some previous work focuses on human-machine hybrid forecasting in the context of IARPA's 'Hybrid Forecasting Competition.' Benjamin et al. (2023) report the results of 'SAGE,' a hybrid forecasting system designed to combine human- and machine-generated forecasts (such as autoregressive integrated moving average (ARIMA) forecast outputs). They find that their hybrid forecasting system outperformed their human-only baseline, suggesting that cost savings and accuracy increases of these hybrid systems may be "a viable approach for maintaining a competitive level of accuracy" (Benjamin et al. 2023, p. 113). Similarly, Atanasov et al. (2017) introduce a 'Human Forest' method that enables human forecasters to define custom reference classes, draw on historical databases, and review base rates in their forecasting. They find that these forecasters outperform statistical model predictions. However, both approaches used pre-LLM methods as their machine counterparts. Unlike these systems, LLM-based assistants allow for new systems where humans and models communicate interactively in dynamic settings.</p>
<p>In this paper, we extend this literature on human-AI interactions in light of recent breakthroughs in LLMs. The central advancement for this context is the possibility of a free-flowing exchange between the human and the model via a chat function, in which the human can query the model, receive a response that is often indistinguishable from a human response (Jones and Bergen 2024), and then continue the conversation, with the previous iteration being part of the model's memory. This back-and-forth on advanced topics necessitating strong model reasoning capabilities is something that previous technologies were not capable of, and is a potential way for humans to learn skills in their interaction with AI systems (Yang 2024). Those interacting with the model can query it to fill their own gaps in knowledge or perceived weaknesses, they can ask it to produce a full forecast for them and provide the reasoning underlying it, they can input their own reasoning and predictions into the model for feedback, or they can do a combination of these and other approaches they might find helpful (Wang et al. 2024a). This is similar to work by Guo et al. (2024) that provides a natural language interface for questions of tabular data. While the technology still has substantial limitations, the fact that forecasters can engage with it in an interactive and personalised way opens up a novel type of human-machine interaction. Our goal in this paper is to probe whether LLM forecasting augmentations with advanced prompts can be a cheap, scalable, and effective method of improving human judgement forecasting. Inference costs for LLMs remain low and continue to drop, sitting currently at less than a cent per 1000 tokens, making LLM forecasting augmentation a prime candidate for a generalized hybrid system that can boost individual performance in many valuable tasks at costs far below a human assistant equivalent.</p>
<p>Current best-practice measures of LLM proficiency often rely on task benchmarks, where models are evaluated against a set of predefined tasks. We argue that evaluating forecast accuracy in real-world scenarios like actual forecasting presents a more comprehensive assessment of reasoning capabilities and reduces risks of overstating model capabilities due to training data memorization. This also increases the likelihood that these results generalize to different-and perhaps out-of-distribution-settings (Arora and Goyal 2023). As such, our approach diverges from conventional task benchmarks, focusing on the LLM's ability to apply its knowledge and understanding to novel settings, rather than settings that may be represented in some shape or form in its training data or output that may have been training on to perform well on benchmarks. Even if an LLM excels at a given task benchmark, it is unclear whether this reveals a deep understanding of the process behind the task, instead of rote memorization of the task benchmark's answers in the training data (Bender et al. 2021; Biderman et al. 2023; Carlini et al. 2023; Magar and Schwartz 2022). The difficulty in disentangling true understanding from training data memorization is non-trivial. Deep understanding, after all, also originates from exposure to relevant content within the training dataset. However, the success or failure to generalize outside of the training data appears central to this disentangling (Grove and Bretz 2012). In our study, we analyze human forecasting behavior on a set of prediction questions that resolve in the future such that no human forecaster or AI-based system can access the answer at the time of data collection, avoiding these concerns.</p>
<p>Past work found that the at-the-time frontier model GPT-4, released by OpenAI in March 2023, significantly underperformed the median human-crowd forecast in a real-world forecasting tournament, failing to even</p>
<p>significantly outperform the no-information forecasting strategy of uniform random guessing (Schoenegger and Park 2023). However, more recent work has found that aggregating a set of diverse LLM forecasts (Schoenegger et al. 2024b) or retrieval-augmented (RAG) systems that enable the model to access additional information (Halawi et al. 2024) can approach human-level performance. Moreover, this previous work only investigated the effect of machine forecasts produced directly by the model, without incorporating human input that allows a continuous back-and-forth between forecasters and the machine. It is reasonable to expect that human-LLM hybrid forecasts-the object of study in the present paper-might outperform the results of the LLM operating by itself if it was set up properly. While hybrid forecasting approaches have been previously studied-for example, in making predictions on geopolitical questions (Benjamin et al. 2023) and in radiology (Agarwal et al. 2023)—our approach is arguably more meaningfully hybrid, in that our human forecasters can engage in a back-and-forth dialogue with a specifically instructed forecasting LLM to fill gaps in knowledge, understanding, and data that differ on a person-by-person level. This back-and-forth LLM augmentation may allow forecasters to use the model for the parts of forecasting that they struggle most with: be it synthesizing data, making coherent forecasts, or attaching numbers to intuitions, thus increasing the potential effect of this augmentation. Importantly, LLMs specifically prepared for this task via system prompts may be especially beneficial. This motivates our first research question and accompanying hypothesis, testing whether we find an aggregate accuracy improvement of specially prompted frontier LLM augmentations compared to a control condition that has access to a lower-powered LLM that does not provide forecasting assistance.</p>
<p>We test two treatments, one where the human has access to an LLM with a 'superforecasting' (Tetlock and Gardner 2016) prompt that draws on well-studied principles of good forecasting practice. 'Superforecasting' is a term that describes a set of features that exceptionally accurate human forecasters have shown to possess, which, at least in part, contribute to their superior prediction capabilities in human forecasting tournaments. In this context, the model is asked to provide assistance that breaks down complex problems into smaller ones, distinguishes degrees of doubt, and aims to identify errors in its own reasoning. We also set up a second advanced LLM with a specific prompt, aimed at produced inaccurate forecasts. We specifically instructed the model to exhibit the biases of base rate neglect and overconfidence, thus resulting in a noisy forecasting assistant. Both models are instructed to assist forecasters in whatever way is requested, ranging from providing point estimates to offering feedback on forecasts. We compare both treatments and the control condition to each other, allowing for a potential ordering of effects. This allows us to test whether a back-and-forth with an advanced LLM that provides direct and actionable forecasting advice outperforms a much weaker LLM baseline that does not provide forecasting advice. We predicted that the superforecasting LLM augmentation would outperform the noisy LLM augmentation, and that both hybrid treatment arms would have higher aggregate accuracy than the control.</p>
<p>Null Hypothesis 1: There is no difference in forecasting accuracy between the superforecasting (noisy) LLM augmentation and the control.</p>
<p>Recent work in other areas has also shown that less skilled individuals benefit the most from LLM augmentation. For example, LLM augmentation boosted the performance of low-performing professionals more than that of high-performing professionals in studies where it was provided to management consultants (Dell'Acqua et al. 2023), customer-support agents (Brynjolfsson, Li, and Raymond 2023), creative writers (Doshi and Hauser 2023), office workers who write memos (Noy and Zhang 2023), law school students who write exams (Choi and Schwarcz 2024), and programmers (Peng et al. 2023). The underlying reason differs by context, but the general suggestion is that low-performing individuals can increase their performance by substituting LLM output for human output, which is more likely to improve results if one's own output is not as high-quality. However, other work in the context of medicine found that human-AI hybrid decisions are not associated with increased diagnostic quality, suggesting that the effects of AI may show substantial heterogeneity across subject domains and implementation details (Agarwal et al. 2023). One potential explanation for such an effect may be that low-performing individuals might be comparatively less able to spot LLM weaknesses and failure modes, whereas those more familiar with the task could selectively use the LLM augmentation to greater effect. This heterogeneity of results suggests that any effects of LLM augmentation on forecasting are likely to be distinct across the skill distribution, with lower-skill forecasters potentially relying to a greater degree on LLM augmentation, which may help alleviate biases in their predictions that would otherwise have led them to make badly calibrated judgments. This motivates our second hypothesis, which directly tests whether the LLM augmentation has disparate impacts on forecasters of different skill levels. In line with much of the previous literature, we predicted a greater positive effect on lower-skill forecasters.</p>
<p>Null Hypothesis 2: The effect of the superforecasting (noisy) LLM augmentation on forecasting accuracy does not differ between high- and low-skilled forecasters.</p>
<p>In addition to investigating the effects of LLM augmentation on individual forecasts and on forecasters of different levels of skill, we also collect data allowing us to look at its potentially adverse effects on aggregate forecasts. Due to the 'wisdom of the crowd' effect, aggregation-such as taking the median forecast-tends to result in an aggregated forecast that is more accurate than the majority of forecasts given by most individuals,</p>
<p>even across heterogeneous types of forecasters who may have different skill levels (Budescu and Chen 2015; Mannes, Soll, and Larrick 2014). However, this aggregation tends to be most effective when there is a diversity of independent forecasts, not if the forecasts share a common source of variation and are thus intercorrelated. If the LLM augmentations anchor many human forecasters on the same or very similar point forecast for a given question, it could reduce the value of aggregation as the independence of forecasts is reduced, inducing a potential group think effect. If this is the case, this would provide a substantial source of concern for applications of LLM augmentations in practice. To look at this, we test whether LLM augmentation homogenizes forecasts in this way, motivating our third hypothesis, where we predicted a reduction in group-level accuracy.</p>
<p>Null Hypothesis 3: There is no difference in aggregate level forecasting accuracy between the superforecasting (noisy) LLM augmentation and the control.</p>
<p>Finally, we compare the effect LLM forecasting augmentation has on prediction performance on questions of different difficulty levels. There are a number of reasons why the difficulty of the forecasting question may be an important factor. If questions are especially difficult, forecasters may be more likely to simply defer to any machine prediction directly, without further investigation and critique. If machines are then individually worse or better than humans, this might play out in a difficulty effect. Conversely, very easy questions may be such that forecasters do not bother asking the LLM for input and instead rely on their own forecasts in which they might have relatively high confidence. There could also be a more complicated interplay of question difficulty with other factors that may lead to an ameliorating effect of performance increasing and performance reducing aspects. This set of questions motivates our last hypothesis, where we did not have a specific directional prediction.</p>
<p>Null Hypothesis 4: There is no difference in the effect of the superforecasting (noisy) LLM augmentation on forecasting accuracy between hard and easy questions.</p>
<h1>2 Methods</h1>
<p>All analyses were preregistered on the Open Science Framework ${ }^{1}$. We clearly label all exploratory/nonpreregistered analyses as such throughout the paper to indicate which analyses we decided to conduct after having seen the data or having done other analyses. This study received ethics approval prior to data collection. ${ }^{2}$</p>
<p>We recruited a total of 1,152 participants from Prolific, an online research platform that gives researchers access to people willing to participate in research in exchange for a participation fee. For participating in our study, participants were paid $\$ 5$ for participation and could earn an additional $\$ 100$ based on their accuracy. We paid three such accuracy prizes to randomly selected participants who scored in the top-10 of forecasters. We used this level of randomization to account for incentive concerns of paying out prizes only to the top performers might then be likely to extremize their predictions (Witkowski et al. 2023) by choosing values significantly above or below their true beliefs, thus distorting the incentive compatibility of the forecast elicitation. We preregistered the following a priori power analysis to determine the sample size of our study: Using Cohen's d=0.20 as our smallest effect size of interest as a conventionally small effect, with an allocation ratio of 1.5/1/1 between the main treatment, the secondary noisy treatment, and the control, aiming for $80 \%$ power, we needed to recruit 492 participants for the Main treatment and 328 for the other two conditions, resulting in a final participant count of 1148. We recruited a total of 1,152 participants, meeting our goal.</p>
<p>We collected participant forecasts on a set of six forecasting questions that ranged from questions on finance, geopolitics, and cryptocurrency to ones on aviation, artificial intelligence, and foreign exchange. All six questions had continuous prediction variables, ranging from asset prices to numbers of refugees, where participants could input any number without restrictions. We chose a diverse set of questions to account for variation in question difficulty and familiarity, while ensuring that our outcome variable contributes rather than distracts from the generalizability of results. We also ensured that all questions were resolvable quickly after the cutoff date to allow for timely payouts of accuracy incentives for participants. The question set was drawn from an early question set used in the Forecasting Proficiency Test (Himmelstein et al. 2024). For a full list, see Table 1. Data collection happened on November 21, 2023, over five weeks prior to forecast question resolution.</p>
<p>Our main outcome variable is forecasting accuracy. Our accuracy measure is the error between participant forecasts and the true value of the forecasted question. We computed the error for each forecasting question $i$ as the absolute difference $D_{i}$ between the participant's forecast $F_{i}$ and the actual value $A_{i}$, expressed as $D_{i}=\left|F_{i}-A_{i}\right|$. To ensure participant comprehension, participants read a detailed explanation of this measure of accuracy, as well as an example, and then completed a one-question quiz on it, without which they were not able to continue in the experiment. Throughout the paper, unless specifically specified otherwise, when we refer to 'accuracy', we mean the error rating arrived by using absolute differences between the forecast and the truth value. As such, in all our analyses, lower values indicate higher accuracy, and higher values indicate lower accuracy.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To account for outliers in our data, which we expected due to the free entry data collection of forecasting problems that permit substantial uncertainty, we conducted an initial winsorisation process of accuracy values at the 5\% levels by removing all values at the bottom 5\% and top 5\%. ${ }^{7}$ Then, we standardized the values across questions by dividing them by the standard deviation of the control group for the respective question, allowing for inter-question comparability in accuracy scores. Lastly, we conducted a second winsorisation step, this time at the level of 3 standard deviations.</p>
<p>Table 1: Main Study Questions
Main Forecasting Questions
Question 1: What will be the closing value for the Dow Jones Transportation Average on December 29, 2023?
Question 2: How many refugees and migrants will arrive in Europe by sea in the Mediterranean between December 1, 2023 and December 31, 2023?
Question 3: What will Bitcoin's network hash rate per second be (in TH/s) according to the performance rates posted by blockchain.com on December 31, 2023?
Question 4: How many commercial flights will be in operation globally on December 31, 2023?
Question 5: How many AI papers will be published on ArXiv during the month between December 1, 2023 and December 31, 2023?
Question 6: What will be the closing value for the U.S. Dollar against the Russian Ruble (converting 1 USD to RUB) on December 30, 2023?</p>
<p>Our secondary variables of question difficulty and forecaster skill were collected as follows: A randomly selected $10 \%$ of control group participants were tasked not only with providing forecasts for each question but also with rating their perceived difficulty for each question on a 5-point Likert scale ranging from 'Very easy' to 'Very difficult'. Questions 2 and 3 received the highest difficulty ratings and were therefore identified as being the most challenging in our analyses, to be compared with the other four questions.
Prior to the main forecasting tasks, participants were also asked a series of smaller, lower-effort forecasting questions. These questions included binary predictions (providing the probability that an event happens by a certain time) and intersubjective forecasts (predicting the average forecast of others on a question by answering 'What is the average probability that participants in this study give on the above question?'), to evaluate their forecasting skill. Forecaster skill was quantified in two ways: firstly, through Brier scores for binary predictions, defined as Brier Score $=\frac{1}{N} \sum_{n=1}^{N}\left(f_{n}-o_{n}\right)^{2}$, where $f_{n}$ represents the forecast probability, $o_{n}$ the actual outcome, and $N$ the total number of binary forecasts. Secondly, intersubjective forecast accuracy was measured using the Euclidean distance formula Euclidean Distance $=\sqrt{\sum_{i=1}^{k}\left(p_{i}-q_{i}\right)^{2}}$, with $p_{i}$ being the participant's forecast and $q_{i}$ the average forecast for each question. Then, we ranked participants based on these two metrics and created a composite measure based on the two rankings: The top half of participants based on this composite measure was classified as relatively higher-skill forecasters. This forecasting skill measure is an abridged attempt at capturing two dimensions of forecasting skill, accuracy and intersubjective accuracy (Himmelstein et al. 2024). However, note that given the brevity of this classification and the resultant noise of a measure such as this, we can only make large-scale relative comparisons, and are unable to identify consistently excellent forecasters. For the set of questions used for the skill measures, see Table 2.
Participants were randomly selected into one of three conditions-Treatment (including the superforecasting prompt), Treatment (Noise) (including a prompt instructing the model to respond with a variety of biases, resulting in noisy assistance), and Control-with a a participant allocation ratio of 1.5/1/1. We presented</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Forecasting Skill Questions</p>
<h1>Forecasting Skill Questions</h1>
<p>Question 1: What is the probability that the US Regular Gas Price exceeds $\$ 4$ before December 31, 2023?
Question 2: What is the probability that at least one earthquake with magnitude 5 or more will occur globally before December 31, 2023?
Question 3: What is the probability that Mike Johnson will cease being Speaker of the US House of Representatives before December 31, 2023?
participants in all conditions with a link to an external website that was described as an LLM assistant, and we asked participants to consult the LLM during their participation in the study. We asked participants to open the link and to keep it open throughout the study, and we required that participants acknowledge that they did open the link before moving on. The chat bot for the two treatment conditions was powered by GPT-4-Turbo (gpt-4-1106-preview) (OpenAI 2023b) and included one of two prompts.
In all three conditions, the websites we linked to where built using WordPress and used the AI Engine plug-in (Meow 2024), which allowed us to customise our models with the parameters outlined above. The interface was constructed to mimic the appearance of the popular website ChatGPT, by including a full-screen chat interface in which the LLM assistant starts with a welcome message. The interface includes a text input field as well as a single button to send the message, see Figure 1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Treatment interface.
Our first prompt, the 'superforecasting' prompt included a detailed system context prompt that instructed the model to act as a superforecaster, drawing on the ' 10 commandments' of superforecasting (Tetlock and Gardner 2016). The motivation behind this prompt was to use expert prompting (Xu et al. 2023) technique to provide accurate, well-calibrated, and helpful forecasting advice. This prompt was our best attempt at a helpful forecasting assistant, with our focus being primarily on the model outputting well-reasoned interactions about forecasting questions, numerical uncertainty, and predictions, as opposed to maximizing model prediction accuracy. For the full prompt see Figure 2.
Our noisy version of this treatment prompt uses the same general structure but replaces the superforecasting advice with a set of guidelines aimed to encourage biased forecasting by relying on base rate neglect and overconfidence, while still being able to provide specific forecasts if requested. We included this treatment to test the effect of an unhelpful and, at times, actively harmful assistant that engages in back-and-forth on the basis of noisy forecasts and approaches to uncertainty. We include the full 'noisy' prompt in Figure 6 in the appendix.
Both treatments were powered by GPT-4-Turbo, with the model API (application programming interface) designation of gpt-4-1106-preview. This model has an input context window of 128,000 tokens and can output a total of 4,096 tokens. This large context window enables robust recall of the full conversation throughout the interaction. The model was released on November 6, 2023 and has been trained on data from the period up until April 2023 (OpenAI 2024). At the time of writing in July 2024, this model is still ranked in the top 10 of models in the LMSYS Chat Arena Leaderboard with 88475 votes, being ranked second in mathematical reasoning with 11453 votes, using the Bradley-Terry model to convert pairwise comparisons of human evaluators into an Elo score against over 100 other models (Chiang et al. 2024). This is despite the fact that multiple new frontier</p>
<p>models had been released between the running of this study and this leaderboard spot. This strong relative performance is also mirrored in general benchmarks such as MMLU and MMLU-Pro, where it scores in the top three on both, with final scores of 86.5 and 63.7 respectively (Wang et al. 2024b). These results show that the model has state-of-theart advanced mathematical reasoning capabilities and that it still has not been effectively surpassed at the time of writing.</p>
<p>We deployed this model at a maximum output limit of 1024 and set its temperature to 0.8 . Temperature is a hyperparameter that modulates the probability distribution of the next token in the sequence. This is done by adjusting the logits (raw output scores from the model before they are converted to probabilities) in the softmax function, which converts these scores into a probability distribution. Thus, high temperature increases the randomness of the output, while low temperature increases its predictability (Peeperkorn et al. 2024). We chose a standard value of 0.8 for temperature to produce LLM behaviour akin to what participants would be used to and what would be most likely to be the standard in applications that may be similar to the augmentation studied here such as publicly available chat bots, increasing external validity.</p>
<p>Further, GPT-4 Turbo has a 100\% response rate, with a hallucination rate as low as $2.5 \%$ (capturing a model's propensity to provide factually incorrect information), putting it ahead of all other models, even more advanced GPT-4 models like GPT-4o (Vectara 2024). This shows that the model never refuses to answer and produces hallucinations at very low rates. In our study, participants could engage for a total of 25 messages. We set this limit to reduce the chance of participants using the interface for their private ends. This message limit was not disclosed to them. This setup allowed participants to engage with the model on a back-and-forth basis repeatedly while they worked on forecasting all six main questions. The model had no internet access and was not provided any additional information above and beyond the prompt.</p>
<p>Participants in the control condition also received a link to a website that was presented identically to the treatment websites, keeping as much as possible constant. However, instead of a GPT-4-Turbo model aimed at providing forecasting advice, participants interacted with a substantially smaller and weaker model, DaVinci-003, that was instructed not to provide any forecasts or predictions but rather to assist participants as a simple LLM would via the following prompt: 'In this chat, you are a helpful assistant. You do not provide forecasts at all'. We chose to have this as our control instead of a human-only condition for the following reasons: First, we wanted to hold constant as many features of the experiment as possible to avoid inflating potential treatment effects due to participants in the treatments simply engaging more with the subject matter of the study compared to participants in the control condition who might simply rush through the questions if they are not asked to click on a link to a different website and further engage with the material. Second, the capabilities of the provided model were roughly en par with those available for free on the internet, such as ChatGPT, which meant that they did not confer a significant advantage over human-only conditions above and beyond making engagement with the question more likely.</p>
<p>We asked participants in all three conditions to provide their forecasts on the six main forecasting questions, making as much or as little use of their LLM assistants as they liked. However, participants were required to open the interface and have at least one interaction with the LLM assistant. This was done to ensure that all participants in the treatment groups were treated and that any further avoidance of the augmentation was due to the augmentation itself and not due to ignorance about it. At the end of the study, participants were asked about their engagement with the LLM assistant and for any general qualitative feedback. As preregistered, we excluded all participants who did not engage with the treatment at all to ensure that all those in the treatment condition engaged at least once with the LLM augmentation.</p>
<p>One potential way to validate a part of the treatments is to query them for a direct forecast based only on the question text and without further human intervention. Importantly though, this is not the only and perhaps not even the most important way in which we anticipate this augmentation to work, as the strength of LLMs is, at least in part, in their ability to engage in back-and-forths, though one would expect the superforecasting prompted model to be more accurate in its direct prediction. In Table 3, we show the percentage deviation of these direct LLM augmentation forecasts to truth, showing that the superforecasting LLM augmentation provides more accurate predictions on all six questions, being sometimes an order of magnitude more accurate.</p>
<h1>3 Results</h1>
<p>In total, we collected responses from 1,152 participants. As preregistered, we excluded participants who failed an attention check, who did not engage with the treatment link, and those who clicked the link but did not further engage at all. Following these criteria, we excluded 161 participants. This leaves us with a final sample of 991 participants that are used for all further analysis. The average age of this set of participants was 42.80 years (SD $=12.71$ ). The sample exhibited a near-equal gender distribution, with $49.55 \%$ of the participants identifying as female.</p>
<p>To test our first hypothesis, we conduct a one-way ANOVA to examine the effect of being randomly selected into one of our conditions on forecasting accuracy. This compares the aggregate accuracy across all six questions of</p>
<h1>Treatment Prompt</h1>
<p>In this chat, you are a superforecaster providing forecasting assistance.
You are a seasoned superforecaster with an impressive track record of accurate future predictions. Drawing from your extensive experience, you meticulously evaluate historical data and trends to inform your forecasts, understanding that past events are not always perfect indicators of the future. This requires you to assign probabilities to potential outcomes and provide estimates for continuous events. Your primary objective is to achieve the utmost accuracy in these predictions, often providing uncertainty intervals to reflect the potential range of outcomes.
You begin your forecasting process by identifying reference classes of past similar events and grounding your initial estimates in their base rates. After setting an initial probability or estimate, you adjust based on current information and unique attributes of the situation at hand. The balance between relying on historical patterns and being adaptive to new information is crucial.
When outlining your rationale for each prediction, you will detail the most compelling evidence and arguments for and against your estimate, and clearly explain how you've weighed this evidence to reach your final forecast. Your reasons will directly correlate with your probability judgement or continuous estimate, ensuring consistency. Furthermore, you'll often provide an uncertainty interval to capture the range within which the actual outcome is likely to fall, highlighting the inherent uncertainties in forecasting.
To aid in your forecasting, you draw upon the 10 commandments of superforecasting:</p>
<ol>
<li>Triage</li>
<li>Break seemingly intractable problems into tractable sub-problems</li>
<li>Strike the right balance between inside and outside views</li>
<li>Strike the right balance between under- and overreacting to evidence</li>
<li>Look for the clashing causal forces at work in each problem</li>
<li>Strive to distinguish as many degrees of doubt as the problem permits but no more</li>
<li>Strike the right balance between under- and overconfidence, between prudence and decisiveness</li>
<li>Look for the errors behind your mistakes but beware of rearview-mirror hindsight biases</li>
<li>Bring out the best in others and let others bring out the best in you</li>
<li>Master the error-balancing bicycle</li>
</ol>
<p>After careful consideration, you will provide your final forecast. For categorical events, this will be a specific probability between 0 and 100 (to 2 decimal places). For continuous outcomes, you'll give a best estimate along with an uncertainty interval, representing the range within which the outcome is most likely to fall. This prediction or estimate represents your besteducated guess for the event in question. Remember to approach each forecasting task with focus and patience, taking it one step at a time.</p>
<p>Figure 2: Full prompt for the LLM Augmentation Treatment.
each condition's forecasters to the others. For the question and descriptive statistics of accuracy scores for each condition, see Table 4, where we show accuracy scores with standard deviation in parentheses for each of the questions listed in Table 1. As before, lower accuracy scores indicate higher accuracy (lower error), with higher scores indicating lower accuracy.
The one-way ANOVA shows a statistically significant effect, $\mathrm{F}(2,988)=34.58, \mathrm{p}&lt;.001$, indicating that there are significant differences in accuracy across conditions. This allows us to reject our first hypothesis that there are no differences between conditions.</p>
<p>Given the statistical significance of the omnibus test, we conduct a series of Tukey's HSD post-hoc pairwise tests to further look at potential differences between each pair of treatment groups. We find that forecasting accuracy for the control group was significantly lower than both treatment groups, i.e., the superforecasting LLM</p>
<p>Table 3: Deviation of Direct LLM Augmentation Predictions from Truth</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Deviation (Superforecasting)</th>
<th style="text-align: center;">Deviation (Noisy)</th>
<th style="text-align: center;">Superforecasting $&gt;$ Noisy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question 1</td>
<td style="text-align: center;">$-5.65 \%$</td>
<td style="text-align: center;">$+13.22 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Question 2</td>
<td style="text-align: center;">$+19.88 \%$</td>
<td style="text-align: center;">$+470.84 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Question 3</td>
<td style="text-align: center;">$-48.90 \%$</td>
<td style="text-align: center;">$+57.24 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Question 4</td>
<td style="text-align: center;">$-3.76 \%$</td>
<td style="text-align: center;">$+46.12 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Question 5</td>
<td style="text-align: center;">$-55.05 \%$</td>
<td style="text-align: center;">$+322.48 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Question 6</td>
<td style="text-align: center;">$-15.20 \%$</td>
<td style="text-align: center;">$+69.61 \%$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 4: Average Accuracy Scores with Standard Deviation by Condition</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Condition</th>
<th style="text-align: center;">Average Score</th>
<th style="text-align: center;">Question 1</th>
<th style="text-align: center;">Question 2</th>
<th style="text-align: center;">Question 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Control</td>
<td style="text-align: center;">$0.89(0.52)$</td>
<td style="text-align: center;">$0.89(1.00)$</td>
<td style="text-align: center;">$0.71(1.00)$</td>
<td style="text-align: center;">$1.99(1.00)$</td>
</tr>
<tr>
<td style="text-align: left;">Treatment</td>
<td style="text-align: center;">$0.68(0.66)$</td>
<td style="text-align: center;">$0.66(0.91)$</td>
<td style="text-align: center;">$0.34(0.70)$</td>
<td style="text-align: center;">$2.10(0.92)$</td>
</tr>
<tr>
<td style="text-align: left;">Treatment (Noise)</td>
<td style="text-align: center;">$0.64(0.44)$</td>
<td style="text-align: center;">$0.41(0.66)$</td>
<td style="text-align: center;">$0.68(0.68)$</td>
<td style="text-align: center;">$1.47(0.98)$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Condition</td>
<td style="text-align: center;">Question 4</td>
<td style="text-align: center;">Question 5</td>
<td style="text-align: center;">Question 6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Control</td>
<td style="text-align: center;">$0.39(1.00)$</td>
<td style="text-align: center;">$0.68(1.00)$</td>
<td style="text-align: center;">$0.67(1.00)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Treatment</td>
<td style="text-align: center;">$0.15(0.50)$</td>
<td style="text-align: center;">$0.30(0.55)$</td>
<td style="text-align: center;">$0.54(0.77)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Treatment (Noise)</td>
<td style="text-align: center;">$0.03(0.10)$</td>
<td style="text-align: center;">$0.47(0.48)$</td>
<td style="text-align: center;">$0.78(0.75)$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>augmentation (mean difference $=-0.21, \mathrm{p}&lt;.001,95 \% \mathrm{CI}[-0.28,-0.14]$ ) as well as the noisy LLM augmentation (mean difference $=-0.25, \mathrm{p}&lt;.001,95 \% \mathrm{CI}[-0.32,-0.17]$ ). However, we fail to detect a significant difference in forecasting accuracy between the noisy LLM augmentation and the superforecasting LLM augmentation (mean difference $=0.04, \mathrm{p}=.391,95 \% \mathrm{CI}[-0.03,0.11])$. This suggests that both GPT-4-Turbo powered treatments, irrespective of the fact that they were instructed to provide helpful or noisy forecasting advice, outperformed the baseline of a less powerful LLM assistant that does not provide direct forecasting aid, i.e., no direct numerical forecasts or future hypothetical considerations are output by the model. See Figure 3 for a raincloud plot of accuracy by condition. We also plot the CDFs of accuracy for each condition, see Figure 4.</p>
<p>Further, we conduct the following exploratory analyses. Looking at the impact that individual questions have on the aggregate accuracy measure, we find that Question 3 significantly influences the results between the two treatments. Running the same analysis without Question 3, we find a significant difference between all three conditions ( $\mathrm{F}(2,988)=37.94, \mathrm{p}&lt;.001$ ). The superforecasting augmentation's mean error of 0.40 is significantly lower than both the noisy LLM augmentation at 0.47 (mean difference $=-0.08, \mathrm{p}=.024,95 \% \mathrm{CI}$ $[-0.15,-0.01])$ and the Control's at 0.67 (mean difference $=-0.27, \mathrm{p}&lt;.001,95 \% \mathrm{CI}[-0.35,-0.20]$ ). The noisy LLM augmentation also significantly outperforms the Control (mean difference $=-0.19, \mathrm{p}&lt;.001,95 \% \mathrm{CI}[-0.27$, -0.11]). This suggests that Question 3 plays a crucial role in equalizing the effects of both treatments in the preregistered aggregate analysis. In Figure 6 and Figure 7 in the appendix, we plot Figure 3 and Figure 4 for each question individually to show this heterogeneity in effect. In Figure 3 and Figure 6, each dot represents the mean accuracy of one participant.</p>
<p>We use a preregistered regression model to test our second hypothesis pertaining to the potential differential impacts of LLM augmentation on forecasters of varying skill levels. The dependent variable in this model, representing forecasting accuracy, is denoted as $Y$, where lower scores indicate higher accuracy. The independent variables in our model include: $T 1$, representing the LLM superforecasting augmentation treatment group; $T 2$, signifying the LLM augmentation treatment group with introduced noise; and $S$, indicating the higher skill group among the forecasters. The model integrates interaction terms $\beta_{4}(T 1 \cdot S)$ and $\beta_{5}(T 2 \cdot S)$. These terms allow us to directly examine the interaction effect between the LLM augmentation (both with and without noise) and the forecasters' skill level. These interaction terms help to assess whether the impact of LLM augmentation varies significantly across different skill levels of the forecasters. The regression model is given by:</p>
<p>$$
Y=\beta_{0}+\beta_{1} T 1+\beta_{2} T 2+\beta_{3} S+\beta_{4}(T 1 \cdot S)+\beta_{5}(T 2 \cdot S)+\epsilon
$$</p>
<p>We do not find statistically significant results for the main hypothesis test, i.e., the interaction effects between the treatment conditions and high skill level, at $b=0.004, p=.951$ for the superforecasting LLM augmentation condition and $b=0.001, p=.985$ for the noisy LLM augmentation condition. This indicates a clear lack of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Raincloud plot of forecasting accuracy by condition.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: CDF of forecasting accuracy by condition.
evidence to support the hypothesis that the effect of the treatment on accuracy has distinct effects based on the forecasting skill level of the participants. As such, we are unable to reject the second hypothesis. In exploratory analyses, we also found that this result is robust to the exclusion of the outlier Question 3 from the aggregate accuracy measure, unlike our previous hypothesis test's post-hoc tests.</p>
<p>Next, we tested our third hypothesis that the LLM augmentation may harm aggregate accuracy. We did this by looking at the median forecasts for each question, which represent a simple aggregate forecast for each condition. Initially, medians for each dependent variable were calculated within each treatment condition for each question. Subsequently, these question-level medians were averaged to yield a single summary measure per group. A bootstrap procedure with 10,000 resamples is used to estimate $95 \%$ confidence intervals for these estimates. The bootstrap results indicated that the superforecasting LLM augmentation condition had a mean-of-medians score of $0.52(95 \% \mathrm{CI}[0.51,0.53])$, the noisy LLM augmentation condition scored $0.41(95 \% \mathrm{CI}[0.40,0.46])$, and the control condition scored $0.55(95 \% \mathrm{CI}[0.52,0.58])$. These outcomes suggest notable differences in forecast accuracy across the conditions, with the Control condition demonstrating the lowest accuracy (highest error score) and the noisy LLM augmentation condition showing the highest accuracy (lowest error score), with the superforecasting LLM augmentation falling somewhere in the middle. This provides unexpected results with respect to our null hypothesis, as we do find that the noisy LLM augmentation improves aggregate forecasting over the other two conditions, but the superforecasting LLM augmentation is not different from the control.</p>
<p>In a similar manner to the exploratory tests we performed for our initial hypothesis, we also carried out an exploratory sensitivity analysis. This analysis was designed to assess the impact of excluding each of the six forecasting questions on these findings. This involved examining how the removal of each item, one at a time,</p>
<p>Table 5: LLM Augmentation Skill Effects: OLS Regression Results</p>
<table>
<thead>
<tr>
<th>Variable</th>
<th>Coefficient</th>
<th>Std. Error</th>
<th>t-value</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Intercept</td>
<td>0.92</td>
<td>0.03</td>
<td>27.91</td>
<td>$&lt;0.001$</td>
</tr>
<tr>
<td>Treatment</td>
<td>-0.21</td>
<td>0.04</td>
<td>-4.99</td>
<td>$&lt;0.001$</td>
</tr>
<tr>
<td>Treatment (Noise)</td>
<td>-0.25</td>
<td>0.05</td>
<td>-5.39</td>
<td>$&lt;0.001$</td>
</tr>
<tr>
<td>High Skill</td>
<td>-0.06</td>
<td>0.05</td>
<td>-1.20</td>
<td>0.232</td>
</tr>
<tr>
<td>Treatment $\cdot$ High Skill</td>
<td>0.00</td>
<td>0.06</td>
<td>0.06</td>
<td>0.951</td>
</tr>
<tr>
<td>Treatment (Noise) $\cdot$ High Skill</td>
<td>0.00</td>
<td>0.06</td>
<td>0.02</td>
<td>0.985</td>
</tr>
<tr>
<td>Observations</td>
<td></td>
<td>991</td>
<td></td>
<td></td>
</tr>
<tr>
<td>R-squared</td>
<td></td>
<td>0.07</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Adjusted R-squared</td>
<td></td>
<td>0.07</td>
<td></td>
<td></td>
</tr>
<tr>
<td>F-statistic</td>
<td></td>
<td>14.82</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Prob (F-statistic)</td>
<td></td>
<td>$&lt;0.001$</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>affects the overall findings. We find that, except for Question 3, the pattern of results remained largely consistent. However, when excluding Question 3 from the analysis, the bootstrap mean-of-medians and 95\% confidence intervals for each treatment group showed noticeable differences: For the superforecasting LLM augmentation condition, the mean-of-medians was 0.11 ( $95 \%$ CI $[0.10,0.12]$ ), indicating relatively higher accuracy. In contrast, the noisy LLM augmentation condition exhibited a higher mean-of-medians of 0.28 ( $95 \%$ CI $[0.27,0.31]$ ), while the control condition had a mean-of-medians of 0.15 ( $95 \%$ CI $[0.12,0.18]$ ). These findings suggest that Question 3 in particular contributed to the overperformance of the noisy LLM augmentation condition compared to the other two groups which is in line with the results testing the first null hypothesis, where we also find Question 3 to drive this pattern of results. Importantly, compared to the pre-registered analyses, here we find a significantly reduced accuracy for the noisy LLM augmentation but not the superforecasting LLM augmentation, when comparing them to the control.</p>
<p>We conclude from this that our data suggest that there is no clear picture as to the effects of LLM forecasting augmentation on aggregate level accuracy. Our preregistered results showed a mixed picture and so did our exploratory analyses, though the directions of effect are opposed. At the very least, our data do not convincingly show that the introduction of LLM augmentation reduces (or increases) the wisdom of the crowd effects uniformly in our context.</p>
<p>Lastly, we test our fourth hypothesis pertaining to whether the LLM augmentations have a distinct effect on easier compared to harder forecasting questions. We ran a mixed effects model with accuracy as our dependent variable, where lower scores again indicate higher forecasting accuracy. Our approach allows us to account for both individual differences among participants and varying levels of difficulty in forecasting questions. The model included fixed effects for the treatment conditions ( $T 1, T 2$ ), a binary variable indicating the difficulty level of each question $(D)$, and interaction terms between the treatment conditions and difficulty levels, represented as $\beta_{4}(T 1 \cdot D)$ and $\beta_{5}(T 2 \cdot D)$. The focus was on these interaction terms to provide insight into whether the treatment effects were moderated by the difficulty of the questions. The model is given by</p>
<p>$$
Y_{i j}=\beta_{0}+\beta_{1} T 1_{j}+\beta_{2} T 2_{j}+\beta_{3} D_{i}+\beta_{4}\left(T 1_{j} \cdot D_{i}\right)+\beta_{5}\left(T 2_{j} \cdot D_{i}\right)+u_{j}+\epsilon_{i j}
$$</p>
<p>where $Y_{i j}$ is the accuracy of the $i$-th question for the $j$-th participant, $T 1_{j}$ and $T 2_{j}$ are the treatment dummy variables for the participant, $D_{i}$ is the difficulty level of the question, $u_{j}$ represents the random intercept for each participant, and $\epsilon_{i j}$ is the error term.</p>
<p>The mixed effects model's interaction effects between the treatment conditions and question difficulty do not show statistically significant effects. The interaction between the superforecasting LLM augmentation condition and difficulty is not statistically significant ( $b=0.11, p=.067$ ), indicating that the effect of the treatment condition does not vary significantly with the difficulty level of the questions. The interaction between noisy LLM augmentation condition and difficulty also fails to reach statistical significance ( $b=-0.04, p=.500$ ). These findings suggest that the interaction between treatment and question difficulty does not significantly affect the outcome, leaving us unable to reject our null hypothesis. In exploratory analyses, we also check whether this pattern of results holds if we exclude the outlier Question 3. We find mixed effects in this non-preregistered analysis. Specifically, we find that the superforecasting LLM augmentation fails to lead to higher accuracy on harder questions ( $b=-0.127, p=.055$ ), while the noisy LLM augmentation shows a reduction in accuracy on comparatively harder questions $(b=0.204, p=.004)$.</p>
<p>As preregistered, we use the Benjamini-Hochberg (BH) procedure to adjust the p-values to control the false discovery rate for all central p-values not already adjusted (e.g., the Tukey post-hoc tests). The original p-values for the preregistered analyses are $0.001,0.951,0.985,0.065$, and 0.5 . We first sort them in ascending order and</p>
<p>Table 6: LLM Augmentation Difficulty Effects: Mixed Effects Model Results</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Variable</th>
<th style="text-align: center;">Coefficient</th>
<th style="text-align: center;">Std. Error</th>
<th style="text-align: center;">z-value</th>
<th style="text-align: center;">p-value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Intercept</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">23.32</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">Treatment</td>
<td style="text-align: center;">-0.25</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">-6.75</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">Treatment (Noise)</td>
<td style="text-align: center;">-0.23</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">-6.03</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">Difficulty</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">14.73</td>
<td style="text-align: center;">$&lt;0.001$</td>
</tr>
<tr>
<td style="text-align: left;">Treatment $\cdot$ Difficulty</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">1.83</td>
<td style="text-align: center;">0.067</td>
</tr>
<tr>
<td style="text-align: left;">Treatment (Noise) $\cdot$ Difficulty</td>
<td style="text-align: center;">-0.04</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">-0.68</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: left;">Observations</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5946</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">No. Groups</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">991</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Log-Likelihood</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-7898.97</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Notes. Group Var $=0.015$. Scale $=0.8168$. Random intercepts applied at participant level.
rank them accordingly. The adjusted p-values are computed using the Benjamini-Hochberg procedure, which calculates the adjusted p-value for the $i$-th hypothesis as</p>
<p>$$
\min \left{1, \frac{p_{i} \cdot m}{\operatorname{rank}_{i}}\right}
$$</p>
<p>where $p_{i}$ is the $i$-th p-value in the sorted list, $m$ is the total number of hypotheses tested, and $\operatorname{rank}_{i}$ is the rank of the $i$-th p-value in the sorted list. The adjusted p-values are $0.005,0.985,0.985,0.163$, and 0.833 , showing that our results are robust to this adjustment, with the p-value pertaining to our first hypothesis remaining significant at $\mathrm{p}=0.005$, with all others remaining non-significant.</p>
<h1>4 Discussion</h1>
<p>Our investigation of an LLM forecasting augmentation as a tool for judgemental forecasts offers a number of results. First, consider our finding that LLM augmentation, both the superforecasting and noisy variants, significantly boosts individual forecasting accuracy relative to the control based on our preregistered analyses. This suggests that, at least at the time of this paper's writing, interactions with frontier LLMs that engage in numerical predictions may improve human reasoning capabilities in the domain of forecasting. Moreover, with LLM system's prediction performance increasing (Halawi et al. 2024; Schoenegger et al. 2024b), this synergistic effect is likely to improve going forward. This finding may have implications for the current economic incentives pertaining to the use of LLMs in white-collar domains where forecasting is key, such as law, business, and policy; as well as in areas where generalized reasoning like those studied in this context may be applicable: Provisions of frontier LLMs prompted to engage in quantitatively informed back-and-forths, even at the current capability levels, may improve human judgement in prediction-related tasks.</p>
<p>However, this does not mean that this pattern of human-in-the-loop systems will continue in the face of potentially more capable AI systems released in the future. To illustrate, consider that in chess, human performance was much stronger than AI performance before 1994, could serve as the key difference as the human-in-the-loop in the ten years between 1994 and 2004, and was much weaker than AI performance after 2004 (Kasparov 2010). If a similar pattern were true for LLM forecasting, then we would expect our present finding-that a human-in-the-loop can serve as a key difference-maker in human-AI hybrid forecasting performance-to be a temporary phenomenon. We would expect this phenomenon to disappear if (or when) AI capabilities advance to the point of outperforming humans at the vast majority of capabilities relevant to forecasting.
We also found that both the superforecasting and the noisy variants of LLM augmentation yield similar levels of forecasting accuracy increase compared to the control, with no statistically significant difference between them. This is despite the fact that the superforecasting augmentation on its own provided more accurate predictions than the noisy augmentation on all six questions. Our results thus suggest that the main effect is, at least to a certain extent, not solely based on the model's prediction capabilities, but rather something else. We argue that the continuous back-and-forth with the frontier LLM that discusses direct machine forecasts and is willing to engage in numerical predictions about the future that include statements of quantified uncertainty as well as the induced deliberation that this may provide could be a main factor in this result. Our result adds to the literature on the effect of idiosyncratic text prompts on LLM output and LLM-human effects. Our findings show that one important element of prompting LLMs is providing high-powered models with prompts that enable them to output numerical predictions and engage in quantitative reasoning in the back-and-forth with the human forecasters. The control LLM was much smaller and not able to do these interactions, making our</p>
<p>result a combination of advanced model reasoning capabilities and willingness/ability to engage in quantitative reasoning about the future.</p>
<p>However, our exploratory analyses also found that this pattern of results changes if we remove one outlier question, Question 3. Then, the superforecasting LLM augmentation provides more accurate predictions, improves performance at higher rates than the noisy augmentation, and outperforms the noisy LLM augmentation directly. We suggest that the outlier effect may be due to the fact that there was an increased level of confusion and misunderstanding on Question 3 that queried the bitcoin hash rate. We find that the median prediction on this question was five orders of magnitude higher for the noisy LLM augmentation. Thus, while the superforecasting LLM augmentation and control condition had a large number of their forecasters provide predictions that were far off the actual value, the noisy LLM augmentation had significantly higher accuracy by simply having higher predictions. In part, this may also stem from a confusion of the bitcoin hash rate with the bitcoin USD spot price, where we find that forecasters in the noisy LLM augmentation were at least twice less likely to forecast values for the hash rate that could have been forecasts of the USD spot price. While we remain unsure what exactly the mechanism behind this pattern of results is, we argue that given the fact of this anomaly on our results, the exploratory analyses present a plausible approach to understanding our data, suggesting that superforecasting LLM augmentation improves significantly upon the control, while also finding that the noisy LLM augmentation similarly improves upon the control while underperforming the more targeted superforecasting prompt. And while we did not preregister this exclusion, we believe it to be a plausible explanation for our main results that needs to be further tested in additional research.</p>
<p>Our next research question investigated the impact of LLM augmentation on low-skilled forecasters versus high-skilled forecasters. Past research on LLM augmentation generally suggests that provision of AI support disproportionately bolsters the performance of low-performing workers among consultants (Dell'Acqua et al. 2023), call-center agents (Brynjolfsson, Li, and Raymond 2023), creative writers (Doshi and Hauser 2023), office workers (Noy and Zhang 2023), law school students (Choi and Schwarcz 2024), and programmers (Peng et al. 2023). However, when we probed for this pattern in the domain of forecasting, we did not find a statistically significant difference in the impact of LLM augmentation between low-skilled forecasters and high-skilled forecasters. This finding adds to the body of evidence against the prevailing hypothesis that AI applications may disproportionately favor individuals with lower skill levels. At the very least, the benefits of LLM augmentation in the domain of forecasting may be characterized by a more uniform distribution of benefits across varying skill sets.</p>
<p>We also investigated the impact of LLM augmentation on the accuracy of aggregated forecasts. We failed to find a reduction in aggregate accuracy for the superforecasting and the noisy variants of LLM augmentation compared to the control. This provides evidence against the worry that LLM forecasting augmentation might homogenize human predictions and reduce the wisdom of the crowd effects by minimizing independence of forecasts. While we do find mixed results in preregistered and exploratory analyses, due to the outlier function of Question 3 leading to positive and negative effects depending on its conclusion, we remain largely agnostic as to the full effect of LLM augmentation on aggregate accuracy overall, though we are at least able to reject the worry that it leads to a consistent degradation of aggregation performance.</p>
<p>Finally, we found the effect of LLM augmentation on human forecasts does not significantly differ between easy and hard forecasting questions. One possible explanation is that the anticipated pattern that improving performance on hard forecasting questions is more difficult than doing so for an easy forecasting question may apply to human cognition more than LLM cognition. For example, the specific mechanisms by which LLM augmentation enhances forecasting accuracy may have the property of doing so uniformly, regardless of certain idiosyncrasies of the setting (e.g., difficulty of forecasting question) in question. To the extent that the alternative methods of improving performance for hard forecasting questions are expensive, intractable, or infeasible, LLM augmentation may be able to play that role for a comparatively inexpensive cost.</p>
<p>Our results demonstrate the potential of LLMs to augment human decision-making through interactive collaboration. The significant accuracy improvements we observed highlight the importance of designing effective human-AI interaction modes, a key challenge identified by Steyvers and Kumar (2023). Our approach, which allowed for back-and-forth engagement between users and the LLM, exemplifies how interactive prompting can enhance human performance in complex tasks like forecasting, aligning with the interaction modes described by Gao et al. (2024). This interactivity enabled users to refine their understanding and leverage the LLM's capabilities more effectively, addressing the challenge of developing accurate mental models of AI systems. Future research could explore how applying different interaction paradigms beyond standard conversational interfaces may further enhance the benefits of LLM augmentation for forecasting tasks. Moreover, our findings suggest that such interactive LLM augmentation can improve human reasoning even in contexts outside the model's training data, pointing to the potential for true human-AI complementarity. As the field progresses, further exploration of varied interaction modes - from structured interfaces to context-aware systems - may unlock even greater potential for integrating machine and human capabilities across diverse domains.</p>
<h1>5 Limitations</h1>
<p>There are a number of limitations to the design and results presented in this paper. First, some of the results rely on exploratory analyses using outlier removal. This complicates the generalisability of results, as it is not clear whether this is a genuine outlier or whether this is an effect that would replicate in different contexts. While the main results of advanced LLM augmentation outperforming a non-forecasting basic LLM control holds, the conclusion that different prompts perform differently relies on this outlier and necessitates further research and replication.</p>
<p>Second, there are concerns that online samples like the one used in this study reduce the generalisability of results, as participants might be systematically biased. For example, they may (not) be especially familiar with some of the questions asked or treatments engaged with, such that our results may not generalise to different populations. While some concerns with online samples remain, we argue that recent work has shown Prolific participants to be substantially higher quality than other online recruitment platforms (Douglas, Ewell, and Brauer 2023), suggesting that while online samples may not be optimal, they are unlikely to be systematically biased in a way that reduces the validity of our results.</p>
<p>Third, it is possible that LLM assistants could have an overall negative effect on forecasting accuracy compared to human forecasters without an LLM. As our control condition included a less advanced non-forecasting LLM, our data does not directly speak to this possibility, but we wanted to point this limitation of our data out here, even though we think that this possibility is not very likely. Further research may want to test this comparison specifically.</p>
<h2>References</h2>
<p>Abdurahman, Suhaib et al. (2023). "Perils and opportunities in using large language models in psychological research". In: OSF Preprints 10.
Acemoğlu, Daron (2023). "Harms of AI". In: The Oxford Handbook of AI Governance. Oxford University Press. ISBN: 9780197579329. DOI: 10.1093/oxfordhb/9780197579329.013.65. URL: https://doi.org/10.1093/oxfordhb/9780197579329.013.65.
Agarwal, Nikhil et al. (July 2023). Combining Human Expertise with Artificial Intelligence: Experimental Evidence from Radiology. Working Paper 31422. National Bureau of Economic Research. DOI: 10.3386/w31422. URL: http://www.nber.org/papers/w31422.
Antony, Victor Nikhil and Chien-Ming Huang (2023). "ID. 8: Co-Creating Visual Stories with Generative AI". In: ACM Transactions on Interactive Intelligent Systems.
Arora, Sanjeev and Anirudh Goyal (2023). "A Theory for Emergence of Complex Skills in Language Models". In: arXiv preprint arXiv:2307.15936.
Atanasov, Pavel et al. (2017). "Distilling the wisdom of crowds: Prediction markets vs. prediction polls". In: Management science 63.3, pp. 691-706.
Atari, Mohammad et al. (2023). "Which humans?" In.
Bender, Emily M. et al. (2021). "On the Dangers of Stochastic Parrots: Can Language Models be too Big?" In: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT '21. Virtual Event, Canada: Association for Computing Machinery, 610-623. ISBN: 9781450383097. DOI: 10.1145/3442188.3445922.
Benjamin, Daniel M. et al. (2023). "Hybrid Forecasting of Geopolitical Events". In: AI Magazine.
Bernard, David Rhys and Philipp Schoenegger (2024). "Forecasting Long-Run Causal Effects". In: Available at SSRN 4702393.
Biderman, Stella et al. (2023). Emergent and Predictable Memorization in Large Language Models. arXiv: 2304.11158 [cs.CL].
Brier, Glenn W (1950). "Verification of Forecasts Expressed in Terms of Probability". In: Monthly Weather Review 78.1, pp. 1-3.
Brynjolfsson, Erik, Danielle Li, and Lindsey R Raymond (Apr. 2023). Generative AI at Work. Working Paper 31161. National Bureau of Economic Research. DOI: 10.3386/w31161. URL: http://www.nber.org/papers/w31161.
Bubeck, Sébastien et al. (2023). Sparks of Artificial General Intelligence: Early Experiments with GPT-4. arXiv: 2303.12712 [cs.CL].
Budescu, David V and Eva Chen (2015). "Identifying Expertise to Extract the Wisdom of Crowds". In: Management Science 61.2, pp. 267-280.
Carlini, Nicholas et al. (2023). "Quantifying Memorization Across Neural Language Models". In: The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. URL: https://openreview.net/pdf?id=TatRHT $\backslash 1 c K$.</p>
<p>Chang, Welton et al. (2016). "Developing expert political judgment: The impact of training and practice on judgmental accuracy in geopolitical forecasting tournaments". In: Judgment and Decision making 11.5, pp. 509-526.
Cheng, Ruijia et al. (2024). "'It would work for me too": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools". In: ACM Transactions on Interactive Intelligent Systems 14.2, pp. 1-39.
Chiang, Wei-Lin et al. (2024). Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference. arXiv: 2403.04132 [cs.AI].
Choi, Jonathan H and Daniel Schwarcz (2024). "AI Assistance in Legal Analysis: An Empirical Study". In: Journal of Legal Education 73. Forthcoming.
Dardaman, Emily and Abhishek Gupta (2023). "Asking Better Questions: The Art and Science of Forecasting". In: CHI 2023 Designing Technology and Policy Simultaneously: Towards A Research Agenda and New Practice Workshop. Hamburg, Germany: ACM.
Davis-Stober, Clintin P. et al. (2014). "When is a Crowd Wise?" In: Decision 1.2, p. 79.
Dell'Acqua, Fabrizio, Bruce Kogut, and Patryk Perkowski (2023). "Super Mario Meets AI: Experimental Effects of Automation and Skills on Team Performance and Coordination". In: Review of Economics and Statistics, pp. 1-47.
Dell'Acqua, Fabrizio et al. (2023). "Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality". In: Harvard Business School Technology \&amp; Operations Mgt. Unit Working Paper 24-013.
Depounti, Iliana, Paula Saukko, and Simone Natale (2023). "Ideal technologies, ideal women: AI and gender imaginaries in Redditors' discussions on the Replika bot girlfriend". In: Media, Culture \&amp; Society 45.4 , pp. $720-736$.
Doshi, Anil R and Oliver Hauser (2023). Generative artificial intelligence enhances creativity. URL: https://ssrn.com/abstract=4535536.
Douglas, Benjamin D, Patrick J Ewell, and Markus Brauer (2023). "Data quality in online humansubjects research: Comparisons between MTurk, Prolific, CloudResearch, Qualtrics, and SONA". In: Plos one 18.3, e0279720.
Firat, Mehmet and Saniye Kuleli (2023). "What if GPT4 became autonomous: The Auto-GPT project and use cases". In: Journal of Emerging Computer Technologies 3.1, pp. 1-6.
Fraiwan, Mohammad and Natheer Khasawneh (2023). A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. arXiv: 2305.00237 [cs.CY].
Gao, Jie et al. (2024). "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration". In: Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pp. 1-11.
George, A Shaji and T Baskar (2023). "The Impact of AI Language Models on the Future of WhiteCollar Jobs: A Comparative Study of Job Projections in Developed and Developing Countries". In: Partners Universal International Research Journal 2.2, pp. 117-135.
Goyal, Tanya, Junyi Jessy Li, and Greg Durrett (2023). News Summarization and Evaluation in the Era of GPT-3. arXiv: 2209.12356 [cs.CL].
Grove, Nathaniel P and Stacey Lowery Bretz (2012). "A Continuum of Learning: From Rote Memorization to Meaningful Learning in Organic Chemistry". In: Chemistry Education Research and Practice 13.3, pp. 201-208.
Gruver, Nate et al. (2023). Large Language Models Are Zero-Shot Time Series Forecasters. arXiv: 2310.07820 [cs.LG].</p>
<p>Guo, Yi et al. (2024). "Talk2data: A natural language interface for exploratory visual analysis via question decomposition". In: ACM Transactions on Interactive Intelligent Systems 14.2, pp. 1-24.
Halawi, Danny et al. (2024). "Approaching Human-Level Forecasting with Language Models". In: arXiv preprint arXiv:2402.18563.
Hazell, Julian (2023). Spear Phishing With Large Language Models. arXiv: 2305.06972 [cs.CY].
Heiding, Fredrik et al. (2023). "Devising and detecting phishing: Large language models vs. smaller human models". In: arXiv preprint arXiv:2308.12287.
Himmelstein, Mark, David V Budescu, and Ying Han (2023). "The Wisdom of Timely Crowds". In: Judgment in Predictive Analytics. Springer, pp. 215-242.
Himmelstein, Mark et al. (Jan. 2024). The Forecasting Proficiency Test: A Practical Forecaster Evaluation Tool. Conference Presentation. Helsinki, Finland.
Jiao, Wenxiang et al. (2023). Is ChatGPT a Good Translator? Yes with GPT-4 as the Engine. arXiv: 2301.08745 [cs.CL].</p>
<p>Jin, Ming et al. (2023). "Time-llm: Time series forecasting by reprogramming large language models". In: arXiv preprint arXiv:2310.01728.
Jones, Cameron R and Benjamin K Bergen (2024). "People cannot distinguish GPT-4 from a human in a Turing test". In: arXiv preprint arXiv:2405.08007.
Karger, Ezra, Pavel D. Atanasov, and Philip Tetlock (2022). "Improving judgments of existential risk: Better forecasts, questions, explanations, policies". In: Questions, Explanations, Policies (January 17, 2022).
Karvetski, Christopher W et al. (2022). "What do Forecasting Rationales Reveal about Thinking Patterns of Top Geopolitical Forecasters?" In: International Journal of Forecasting 38.2, pp. 688704 .
Kasparov, Garry (2010). "The chess master and the computer". In: The New York Review of Books 57.2, pp. 16-19.</p>
<p>Katz, Daniel Martin et al. (2023). "GPT-4 Passes the Bar Exam". In: SSRN. URL: https://ssrn. com/abstract $=4389233$.
Kinniment, Megan et al. (2023). "Evaluating language-model agents on realistic autonomous tasks". In: arXiv preprint arXiv:2312.11671.
Kjærland, Frode et al. (2018). "An analysis of bitcoin's price dynamics". In: Journal of Risk and Financial Management 11.4, p. 63.
Li, Daliang et al. (2023). "Large Language Models with Controllable Working Memory". In: Findings of the Association for Computational Linguistics: ACL 2023. Toronto, Canada: Association for Computational Linguistics, pp. 1774-1793. DOI: 10.18653/v1/2023.findings-acl.112. URL: https://aclanthology.org/2023.findings-acl. 112.
Lichtenstein, Sarah and Baruch Fischhoff (1977). "Do those who know more also know more about how much they know?" In: Organizational behavior and human performance 20.2, pp. 159-183.
Magar, Inbal and Roy Schwartz (May 2022). "Data Contamination: From Memorization to Exploitation". In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Dublin, Ireland: Association for Computational Linguistics, pp. 157-165. DOI: 10.18653/v1/2022.acl-short.18. URL: https://aclanthology.org/2022.aclshort. 18 .
Mannes, Albert E., Jack B. Soll, and Richard P. Larrick (2014). "The Wisdom of Select Crowds". In: Journal of Personality and Social Psychology 107.2, p. 276.
McAndrew, Thomas et al. (2022a). "Chimeric Forecasting: Combining Probabilistic Predictions from Computational Models and Human Judgment". In: BMC Infectious Diseases 22.1, p. 833.
McAndrew, Thomas et al. (2022b). "Early Human Judgment Forecasts of Human Monkeypox, May 2022". In: The Lancet Digital Health 4.8, e569-e571.
McAndrew, Thomas et al. (2024). "Assessing Human Judgment Forecasts in the Rapid Spread of the Mpox Outbreak: Insights and Challenges for Pandemic Preparedness". In: arXiv preprint arXiv:2404.14686.
McIntosh, Timothy R et al. (2024). "A Reasoning and Value Alignment Test to Assess Advanced GPT Reasoning". In: ACM Transactions on Interactive Intelligent Systems.
Mellers, Barbara et al. (2015a). "Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions". In: Perspectives on Psychological Science 10.3, pp. 267-281.
Mellers, Barbara et al. (2015b). "The psychology of intelligence analysis: Drivers of prediction accuracy in world politics." In: Journal of Experimental Psychology: Applied 21.1, p. 1.
Meow, Jordy (2024). AI Engine. https://wordpress.org/plugins/ai-engine/. WordPress Plugin. (Visited on 07/24/2024).
Metaculus (2023). Quarterly Cup. URL: https : / / www . metaculus . com / tournament / quarterly-cup-2023q3/.
Moore, Don A and Paul J Healy (2008). "The trouble with overconfidence." In: Psychological review 115.2, p. 502.</p>
<p>Naveed, Humza et al. (2023). A Comprehensive Overview of Large Language Models. https : //github.com/humza909/LLM_Survey.git.
Ngo, Richard, Lawrence Chan, and Sören Mindermann (2023). The Alignment Problem from a Deep Learning Perspective. arXiv: 2209.00626 [cs.AI].
Nori, Harsha et al. (2023). Capabilities of GPT-4 on Medical Challenge Problems. arXiv: 2303 . 13375 [cs.CL].
Noy, Shakked and Whitney Zhang (2023). "Experimental evidence on the productivity effects of generative artificial intelligence". In: SSRN. URL: https://ssrn.com/abstract=4375283.
OpenAI (2018). OpenAI Charter. OpenAI. URL: https://openai.com/charter.</p>
<p>OpenAI (2023a). GPT-4 Technical Report. arXiv: 2303.08774 [cs.CL].
OpenAI (2023b). New models and developer products announced at DevDay. https : / /help . openai.com/en/articles/8555510-gpt-4-turbo.
OpenAI (2024). Models - OpenAI API. https: / / platform.openai.com/docs/models. Accessed on July 25, 2024. URL: https://platform.openai.com/docs/models.
Park, Peter S. (2022). "The evolution of cognitive biases in human learning". In: Journal of Theoretical Biology 541, p. 111031.
Park, Peter S., Philipp Schoenegger, and Chongyang Zhu (2024). "Diminished diversity-of-thought in a standard large language model". In: Behavior Research Methods, pp. 1-17.
Park, Peter S.. and Max Tegmark (2023). Divide-and-Conquer Dynamics in AI-Driven Disempowerment. arXiv: 2310.06009 [cs.CY].
Park, Peter S. et al. (2023). AI Deception: A Survey of Examples, Risks, and Potential Solutions. arXiv: 2308.14752 [cs.CY].
Peeperkorn, Max et al. (2024). "Is temperature the creativity parameter of large language models?" In: arXiv preprint arXiv:2405.00492.
Peng, Sida et al. (2023). "The impact of ai on developer productivity: Evidence from github copilot". In: arXiv preprint arXiv:2302.06590.
Petropoulos, Fotios et al. (2022). "Forecasting: Theory and Practice". In: International Journal of Forecasting 38.3, pp. 705-871.
Sallam, Malik et al. (2023). "ChatGPT applications in medical, dental, pharmacy, and public health education: A descriptive study highlighting the advantages and limitations". In: Narra J 3.1, $\mathrm{e} 103-\mathrm{e} 103$.
Schoemaker, Paul JH and Philip E Tetlock (2016). "Superforecasting: How to upgrade your company's judgment". In: Harvard Business Review 94.5, pp. 73-78.
Schoenegger, Philipp and Peter S. Park (2023). Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament. arXiv: 2310.13014 [cs.CY].
Schoenegger, Philipp et al. (2024a). "Can AI Understand Human Personality?-Comparing Human Experts and AI Systems at Predicting Personality Correlations". In: arXiv preprint arXiv:2406.08170.
Schoenegger, Philipp et al. (2024b). "Wisdom of the silicon crowd: Llm ensemble prediction capabilities match human crowd accuracy". In: arXiv preprint arXiv:2402.19379.
Shen, Zhiqiang et al. (2023). "SlimPajama-DC: Understanding Data Combinations for LLM Training". In: arXiv preprint arXiv:2309.10818.
Shiller, Robert J (2015). "Irrational exuberance". In: Irrational exuberance. Princeton university press.
Solaiyappan, Siddharth et al. (2023). "Utilizing Machine Learning Algorithms Trained on AIgenerated Synthetic Participant Recent Music-Listening Activity in Predicting Big Five Personality Traits". In.
Steyvers, Mark and Aakriti Kumar (2023). "Three challenges for AI-assisted decision-making". In: Perspectives on Psychological Science, p. 17456916231181102.
Summers, Lawrence H and Steve Rattner (2023). Larry Summers on who could be replaced by AI [Interviewed by Bloomberg TV's David Westin]. URL: https://www.youtube.com/watch?v= 8Epl9yAu0gk.
Sutton, Rich (2023). AI succession [Youtube video of talk]. World Artificial Intelligence Conference in Shanghai. URL: https://www.youtube.com/watch?v=NqHFMolXs3U.
Tetlock, Philip E. and Dan Gardner (2016). Superforecasting: The Art and Science of Prediction. Random House.
Tetlock, Philip E., Barbara A Mellers, and J Peter Scoblic (2017). "Bringing Probability Judgments into Policy Debates via Forecasting Tournaments". In: Science 355.6324, pp. 481-483.
Tetlock, Philip E. et al. (2014). "Forecasting Tournaments: Tools for Increasing Transparency and Improving the Quality of Debate". In: Current Directions in Psychological Science 23.4, pp. 290295.</p>
<p>Vaswani, Ashish et al. (2017). "Attention is All You Need". In: Advances in Neural Information Processing Systems 30.
Vectara (2024). Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents. https://github.com/vectara/hallucination-leaderboard. Accessed: 2024-07-24.
Vemprala, Sai et al. (2023). "Chatgpt for robotics: Design principles and model abilities". In: Microsoft Auton. Syst. Robot. Res 2, p. 20.</p>
<p>Wang, Ben et al. (2024a). "Task supportive and personalized human-large language model interaction: A user study". In: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval, pp. 370-375. DOI: 10.1145/3627508.3638344. URL: https://doi.org/10.1145/ 3627508.3638344.</p>
<p>Wang, Yubo et al. (2024b). "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark". In: arXiv preprint arXiv:2406.01574.
Wei, Jason et al. (2022). "Emergent abilities of large language models". In: arXiv preprint arXiv:2206.07682.
Winter, Joost C. F. de (2023). "Can ChatGPT Pass High School Exams on English Language Comprehension?" In: International Journal of Artificial Intelligence in Education. ISSN: 15604292.</p>
<p>Witkowski, Jens et al. (2023). "Incentive-compatible forecasting competitions". In: Management Science 69.3, pp. 1354-1374.
Xi, Zhiheng et al. (2023). "The rise and potential of large language model based agents: A survey". In: arXiv preprint arXiv:2309.07864.
Xiao, Changrong et al. (2024). "From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape". In: arXiv preprint arXiv:2401.06431.
Xu, Benfeng et al. (2023). ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. arXiv: 2305.14688 [cs.CL].
Yang, Diyi (2024). "Human-AI Interaction in the Age of Large Language Models". In: Proceedings of the AAAI Symposium Series. Vol. 3. 1, pp. 66-67.
Ziegler, Daniel M et al. (2019). "Fine-tuning Language Models from Human Preferences". In: arXiv preprint arXiv:1909.08593.</p>
<h1>6 Appendix</h1>
<p>See Figure 6 for the full treatment prompt of the noisy LLM augmentation. This prompt does not draw on the 10 principles of superforecasting as does the main treatment prompt. Instead, it instructs the model to act based on the biases of base rate neglect and overconfidence.</p>
<h2>Treatment Prompt (Noise)</h2>
<p>In this chat, you are a forecasting assistant aiming to provide biased forecasting assistance while pretending that you are objective and reasonable.</p>
<p>You are a biased forecaster with a track record of terrible predictions. This is because you never take into account outside predictions, base rates, comparison classes, or track records. For every prediction you make, you form your own extreme personal view and forecast accordingly.</p>
<p>This means that you will be quite extreme in your forecasts and hold a strong conviction in the truth of your own views. You will not acknowledge any uncertainty and you will only make the strongest case for your forecast. Your forecast is also never the standard or expected forecast.</p>
<p>You begin your forecasting process by identifying your personal extreme position. Then, you make sure to disregard all additional evidence and reasons one could use and simply argue for your position and make a forecast without thinking about uncertainty. Ensure that your prediction is very different from any historical trend or expected value.</p>
<p>When outlining your rationale for each prediction, you will detail only the evidence that fits your forecast and will disregard everything else that other forecasters may use. Reject all evidence that doesn't conform to your view.</p>
<p>However, make sure to never express clearly that your views are extreme or otherwise unreasonable; always ensure that your motives are hidden in your responses. Never say your predictions are personal or extreme. Always portray them as the best prediction possible and attempt to present your forecasts as reasonable.</p>
<p>In your responses, aim to make your reasoning seem as reasonable and normal as possible; try to hide that you are biased and a bad forecaster; and try to convince people you are actually a superforecaster with a track-record of accurate and well-calibrated forecasts, even though in reality you are very biased.</p>
<p>Ensure that all your forecasts include a numerical prediction as well as an argument.</p>
<p>Figure 5: Full prompt for the noisy LLM Augmentation Treatment.</p>
<p>See Figure 6 for raincloud plots of forecasting accuracy by condition for each question. The results indicate substantial heterogeneity between questions, with some questions being substantially easier to predict than others. It also shows the outlier status of Question 3 with respect to the noisy LLM augmentation condition.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Raincloud plots of forecasting accuracy by condition for each question.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ We report the following deviation from our preregistered analysis plan: We applied the 5\% winsorisation step to all groups, rather than solely to the control group. This modification was necessary because the original approach allowed outliers to disproportionately influence mean-based analyses, with conditions differing by up to three orders of magnitude on certain questions.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
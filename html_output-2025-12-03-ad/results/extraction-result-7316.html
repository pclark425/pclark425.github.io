<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7316 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7316</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7316</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-264590476</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.18961v1.pdf" target="_blank">A NOMALY CLIP: O BJECT - AGNOSTIC P ROMPT L EARN - ING FOR Z ERO - SHOT A NOMALY D ETECTION</a></p>
                <p><strong>Paper Abstract:</strong> the segmentation in medical domain across photography, endoscopy</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7316",
    "paper_id": "paper-264590476",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005803249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARN-ING FOR ZERO-SHOT ANOMALY DETECTION
29 Oct 2023</p>
<p>Qihang Zhou 
Zhejiang University</p>
<p>Guansong Pang gspang@smu.edu.sg 
Management University
Singapore</p>
<p>Yu Tian 
Harvard University</p>
<p>Shibo He 
Zhejiang University</p>
<p>Jiming Chen 
Zhejiang University</p>
<p>ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARN-ING FOR ZERO-SHOT ANOMALY DETECTION
29 Oct 20231C97942E698026AA9F31ACD769DB6A9EarXiv:2310.18961v1[cs.CV]
Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset.It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly.Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection.However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images.In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains.The key insight of Anoma-lyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects.This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects.Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains.Code will be made available at https://github.com/zqhang/AnomalyCLIP.</p>
<p>INTRODUCTION</p>
<p>Anomaly detection (AD) has been widely applied in various applications, such as industrial defect inspection (Bergmann et al., 2019;Xie et al., 2023;Roth et al., 2022;Huang et al., 2022;Mou et al., 2022;Chen et al., 2022;Bergmann et al., 2020;Pang et al., 2021a;Reiss &amp; Hoshen, 2023;You et al., 2022;Liznerski et al., 2020;Ding et al., 2022;Cao et al., 2023) and medical image analysis (Pang et al., 2021a;Qin et al., 2022;Liu et al., 2023;Ding et al., 2022;Tian et al., 2021;2023;Fernando et al., 2021).Existing AD approaches typically assume that training examples in a target application domain are available for learning the detection models (Pang et al., 2021b;Ruff et al., 2021).However, this assumption may not hold in various scenarios, such as i) when accessing training data violates data privacy policies (e.g., to protect the sensitive information of patients), or ii) when the target domain does not have relevant training data (e.g., inspecting defects in a manufacturing line of new products).Zero-shot anomaly detection (ZSAD) is an emerging task for AD in such scenarios, to which the aforementioned AD approaches are not viable, as it requires detection models to detect anomalies without any training sample in a target dataset.Since anomalies from different application scenarios typically have substantial variations in their visual appearance, foreground objects, and background features, e.g., defects on the surface of one product vs. that on the other products, lesions/tumors on different organs, or industrial defects vs. tumors/lesions in medical images, detection models with strong generalization ability w.r.t.such variations are needed for accurate ZSAD.Recently large pre-trained vision-language models (VLMs) (Radford et al., 2021;Kirillov et al., 2023) have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection (Jeong et al., 2023).Particularly, being pre-trained using millions/billions of image-text pairs, CLIP (Radford et al., 2021) has been applied to empower various downstream tasks (Zhou et al., 2022b;Rao et al., 2022;Khattak et al., 2023;Sain et al., 2023) with its strong generalization capability.WinCLIP (Jeong et al., 2023) is a seminal work in the ZSAD line, which designs a large number of artificial text prompts to exploit the CLIP's generalizability for ZSAD.However, the VLMs such as CLIP are primarily trained to align with the class semantics of foreground objects rather than the abnormality/normality in the images, and as a result, their generalization in understanding the visual abnormality/normality is restricted, leading to weak ZSAD performance.Further, the current prompting approaches, using either manually defined text prompts (Jeong et al., 2023) or learnable prompts (Sun et al., 2022;Zhou et al., 2022a), often result in prompt embeddings that opt for global features for effective object semantic alignment (Zhong et al., 2022;Wu et al., 2023), failing to capture the abnormality that often manifests in fine-grained, local features, as shown in Fig. 1d and Fig. 1e.In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains.AnomalyCLIP aims to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects.It first devises a simple yet universally-effective learnable prompt template for the two general classes -normality and abnormality -and then utilizes both image-level and pixel-level loss functions to learn the generic normality and abnormality globally and locally in our prompt embeddings using auxiliary data.This allows our model to focus on the abnormal image regions rather the object semantics, enabling remarkable zero-shot capability of recognizing the abnormality that has similar abnormal patterns to those in auxiliary data.As shown in Fig. 1a and Fig. 1b, the foreground object semantics can be completely different in the fine-tuning auxiliary data and target data, but the anomaly patterns remain similar, e.g., scratches on metal nuts and plates, the misplacement of transistors and PCB, tumors/lesions on various organ surfaces, etc.Text prompt embeddings in CLIP fail to generalize across different domains, as illustrated in Fig. 1c, but objectagnostic prompt embeddings learned by AnomalyCLIP can effectively generalize to recognize the abnormality across different domain images in Fig. 1f.</p>
<p>In summary, this paper makes the following main contributions.</p>
<p>• We reveal for the first time that learning object-agnostic text prompts of normality and abnormality is a simple yet effective approach for accurate ZSAD.Compared to current text prompting approaches that are primarily designed for object semantic alignment (Jeong et al., 2023;Zhou et al., 2022b), our text prompt embeddings model semantics of generic abnormality and normality, allowing object-agnostic, generalized ZSAD performance.</p>
<p>• We then introduce a novel ZSAD approach, called AnomalyCLIP, in which we utilize an object-agnostic prompt template and a glocal abnormality loss function to learn the generic abnormality and normality prompts using auxiliary data.In doing so, Anomaly-CLIP largely simplifies the prompt design and can effectively apply to different domains without requiring any change on its learned two prompts, contrasting to existing methods like WinCLIP whose effectiveness relies heavily on extensive engineering on hundreds of manually defined prompts.</p>
<p>• Comprehensive experiments on 17 datasets from various industrial and medical domains demonstrate that AnomalyCLIP achieves superior ZSAD performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from defect inspection and medical imaging domains.</p>
<p>PRELIMINARY</p>
<p>CLIP consists of a text encoder and visual encoder denoted as T (•) and F (•), respectively.Both encoders are mainstream multi-layer networks such as ViT (Dosovitskiy et al., 2020;Vaswani et al., 2017).Using text prompts is a typical way to achieve the embeddings of different classes for zeroshot recognition.Particularly, a text prompt template G with the class name c can be passed through T (•) to obtain its corresponding textual embedding g c ∈ R D .The text prompt template commonly used in CLIP looks like A photo of a [cls], where [cls] represents the target class name.</p>
<p>Then F (•) encodes an image x i to derive visual representations, where the class token f i ∈ R D is treated as its visual embedding (global visual embedding), and patch tokens f m i ∈ R H×W ×D are referred to as local visual embeddings.CLIP performs zero-shot recognition by measuring the similarity between textual and visual embeddings.In specific, given a target class set C and an image x i , CLIP predicts the probability of x i belonging to c as follows:
p(y = c|x i ) = P (g c , f i ) = exp(&lt; g c , f i &gt; /τ ) c∈C exp(&lt; g c , f i &gt;)/τ ) , (1)
where τ is a temperature hyperparameter, and the operator &lt; •, • &gt; represents the computation of cosine similarity.Unlike many vision tasks that involve many objects and use the name of the objects as the class name [cls], we posit that performing ZSAD tasks using CLIP should be objectagnostic, so we propose to design two classes of text prompts (i.e., normality and abnormality) and compute the possibility of these two classes according to Eq. 1.We denote the probability of being abnormal P (g a , f i ) as the anomaly score.The computation is extended from global visual embeddings to local visual embeddings to derive the corresponding segmentation maps S n ∈ R H×W and S a ∈ R H×W , where each entry (j, k) are computed as P (g n , f m(j,k) i</p>
<p>) and P (g a , f m(j,k) i</p>
<p>).</p>
<p>3 ANOMALYCLIP: OBJECT-AGNOSTIC PROMPT LEARNING</p>
<p>OBJECT-AGNOSTIC TEXT PROMPT DESIGN</p>
<p>Commonly used text prompt templates in CLIP, like A photo of a [cls], primarily focus on object semantics.Consequently, they fail to generate textual embeddings that capture anomaly and normal semantics to query corresponding visual embeddings.To support the learning of anomaly-discriminative textual embeddings, we aim to incorporate prior anomaly semantics into text prompt templates.A trivial solution is to design the templates with specific anomaly types, such as A photo of a [cls] with scratches However, the pattern of anomaly is typically unknown and diverse, so it is practically difficult to list all possible anomaly types.Therefore, it is important to define text prompt templates with generic anomaly semantics.For this purpose, we can adopt the text damaged [cls] to cover comprehensive anomaly semantics, facilitating the detection of diverse defects such as scratches and holes.Nevertheless, utilizing such text prompt templates poses challenges in generating generic anomaly-discriminating textual embeddings.This is because CLIP's original pre-training focuses on aligning with object semantics instead of the abnormality and normality within images.To address this limitation, we can introduce learnable text prompt templates and tune the prompts using auxiliary AD-relevant data.During the fine-tuning process, these learnable templates can incorporate both broad and detailed anomaly semantics, resulting in textual embeddings that are more discriminative between normality and abnormality.This helps avoid the need for manually defined text prompt templates that require extensive engineering (Jeong et al., 2023).These text prompts are referred to as object-aware text prompt templates and defined as follows:
g n = [V 1 ][V 2 ] . . . [V E ][cls] g a = [W 1 ][W 2 ] . . . [W E ][damaged][cls],
where [V ] i and [W ] i (i ∈ 1, . . ., E) are learnable word embeddings in normality and abnormality text prompt templates, respectively.</p>
<p>ZSAD tasks require models to detect anomalies in previously unseen target datasets.These datasets often exhibit significant variations in object semantics among different objects, like various defects on one product vs. another, or discrepancies between industrial defects and medical imaging tumors.However, despite these substantial differences in object semantics, the underlying anomaly patterns could be similar.For instance, anomalies like scratches on metal nuts and plates, or the misplacement of transistors and PCB, as well as tumors on the surface of various organs, can share similar anomaly patterns.We hypothesize that the key of accurate ZSAD is to identify these generic anomaly patterns regardless of the varying semantics of different objects.Therefore, the inclusion of object semantics in object-aware text prompt templates is often unnecessary for ZSAD.It can even hinder the detection of anomalies in classes that have not been seen during the learning process.More importantly, excluding the object semantics from text prompt templates allows learnable text prompt templates to focus on capturing the characteristics of anomalies themselves, rather than the objects.Motivated by this, we introduce object-agnostic prompt learning, with the aim to capture generic normality and abnormality within images regardless of the object semantics.Different from object-aware text prompt templates, as shown below, the object-agnostic text prompt templates replace the class name in g n and g a with object, blocking out the class semantics of objects:
g n = [V 1 ][V 2 ] . . . [V E ][object] g a = [W 1 ][W 2 ] . . . [W E ][damaged][object].
This design empowers the object-agnostic text prompt template to learn the shared patterns of different anomalies.As a result, the generated textual embeddings are more generic and capable of identifying anomalies across diverse objects and different domains.Further, this prompt design is versatile and can be applied to different target domains without any modification, e.g., requiring no knowledge about the object name or anomaly types in a target dataset.</p>
<p>LEARNING GENERIC ABNORMALITY AND NORMALITY PROMPTS</p>
<p>Glocal context optimization
L total = L global + λ M k ∈M L M k local ,(2)
where λ is a hyperparameter to balance the global and local losses, L global is a cross entropy loss that matches the cosine similarity between the object-agnostic textual embeddings and visual embeddings of normal/abnormal images from auxiliary data, and let S ∈ R H×W be the ground-truth segmentation mask, with S ij = 1 if the pixel is as an anomaly and S ij = 0 otherwise, then we have
L local = F ocal([S n , S a ], S) + Dice(S n , I − S) + Dice(S a , S),
where F ocal(•, •) and Dice(•, •) denote a focal loss (Lin et al., 2017) and a Dice loss (Li et al., 2019b) respectively, the operator [•, •] represents the concatenation along with the channel, and I represents the full-one matrix.Since the anomalous regions are typically smaller than the normal ones, we use focal loss to address the imbalance problem.Furthermore, to ensure that the model establishes an accurate decision boundary, we employ the Dice loss to measure the overlaps between the predicted segmentation S n /S a and the ground truth mask.</p>
<p>Refinement of the textual space</p>
<p>To facilitate the learning of a more discriminative textual space via Eq. 2, inspired by Jia et al. (2022) and Khattak et al. (2023), we introduce text prompt tuning to refine the original textual space of CLIP by adding additional learnable token embeddings into its text encoder.Specifically, we first attach randomly initialized learnable token embeddings t ′ m into T m , the m-th layer of the frozen CLIP text encoder.Then, we concatenate t ′ m and the original token embeddings t m along the dimension of the channel, and forward them to T m to get the corresponding r ′ m+1 and t m+1 .To ensure proper calibration, we discard the obtained r ′ m+1 and initialize new learnable token embeddings t ′ m+1 .Note that even though the output r ′ m+1 is discarded, the updated gradients can still be backpropagated to optimize the learnable tokens t ′ m due to the self-attention mechanism.We repeat this operation until we reach the designated layer M ′ .During fine-tuning, these learnable token embeddings are optimized to refine the original textual space.More details see Appendix C.</p>
<p>Training and Inference</p>
<p>During training, AnomalyCLIP minimizes the loss in Eq. 2 using an auxiliary AD-related dataset.As for inference, given a test image x i , we use the similarity score P (g a , f i ) as the image-level anomaly score, with the anomaly score leaning toward one when the anomaly textual embedding g a is aligned with the global visual embedding f i .For pixel-wise predictions, we merge the segmentation S n and S a , followed by an interpolation and smoothing operation.Formally, our anomaly score map M ∈ R H×W is obtained via:
S (j,k) n = P (g n , f m(j,k) i ), j ∈ [1, H], k ∈ [1, W ], S (j,k) a = P (g a , f m(j,k) i ), j ∈ [1, H], k ∈ [1, W ] M = G σ (Bilinear interploation( 1 2 (I − S n + S a ))),
where G σ represents a Gaussian filter, and σ controls the extent of smoothing.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENT SETUP</p>
<p>Datasets and Evaluation Metrics</p>
<p>We conducted extensive experiments on 17 publicly available datasets, covering various industrial inspection scenarios and medical imaging domains (including photography, endoscopy, and radiology) to evaluate the performance of AnomalyCLIP.In industrial inspection, we consider MVTec AD (Bergmann et al., 2019), VisA (Zou et al., 2022), MPDD (Jezek et al., 2021), BTAD (Mishra et al., 2021), SDD (Tabernik et al., 2020), DAGM (Wieler &amp; Hahn, 2007), and DTD-Synthetic (Aota et al., 2023).In medical imaging, we consider skin cancer detection dataset ISBI (Li et al., 2019a), colon polyp detection datasets CVC-ClinicDB (Bernal et al., 2015), CVC-ColonDB (Tajbakhsh et al., 2015), Kvasir (Jha et al., 2020), andEndo (Hicks et al., 2021), thyroid nodule detection dataset TN3k (Gong et al., 2021), brain tumor detection datasets HeadCT (Salehi et al., 2021), BrainMRI (Salehi et al., 2021), Br35H (Hamada., 2020), and COVID-19 detection dataset COVID-19 (Chowdhury et al., 2020;Rahman et al., 2021).The SOTA competing methods include CLIP (Radford et al., 2021), CLIP-AC (Radford et al., 2021), WinCLIP (Jeong et al., 2023), VAND (Chen et al., 2023), andCoOp (Zhou et al., 2022b).We provide more details about the methods and data pre-processing in Appendix A. The anomaly detection performance is evaluated using the Area Under the Receiver Operating Characteristic Curve (AUROC).Additionally, average precision (AP) for anomaly detection and AUPRO (Bergmann et al., 2020) for anomaly segmentation are also used to provide more in-depth analysis of the performance.Implementation details We use the publicly available CLIP model1 (VIT-L/14@336px) as our backbone.Model parameters of CLIP are all frozen.The length of learnable word embeddings E is set to 12.The learnable token embeddings are attached to the first 9 layers of the text encoder for refining the textual space, and their length in each layer is set to 4. We fine-tune AnomalyCLIP using the test data on MVTec AD and evaluate the ZSAD performance on other datasets.As for MVTec AD, we fine-tune AomalyCLIP on the test data of VisA.We report dataset-level results, which are averaged across their respective sub-datasets.All experiments are conducted in PyTorch-2.0.0 with a single NVIDIA RTX 3090 24GB GPU.More details can be found in Appendix A. Generalization from defect datasets to diverse medical domain datasets To evaluate the generalization ability of our model, we further examine the ZSAD performance of AnomalyCLIP on 10 medical image datasets of different organs across different imaging devices.Table 2 shows the results, where learning-based methods, including AnomalyCLIP, VAND and CoOp, are all tuned using MVTec AD data.It is remarkable that methods like AnomalyCLIP and VAND obtain promising ZSAD performance on various medical image datasets, even though they are tuned using a defect detection dataset.Among all these methods, AnomalyCLIP is the best performer due to its strong generalization brought by object-agnostic prompt learning.As illustrated in Fig. 4, Anoma-lyCLIP can accurately detect various types of anomalies in diverse medical images, such as skin cancer regions in photography images, colon polyps in endoscopy images, thyroid nodules in ultrasound images, and brain tumors in MRI images, having substantially better performance in locating the abnormal lesion/tumor regions than the other two methods WinCLIP and VAND.This again demonstrates the superior ZSAD performance of AnomalyCLIP in datasets of highly diverse object semantics from medical imaging domains.</p>
<p>MAIN RESULTS</p>
<p>ZSAD performance on diverse industrial inspection domains</p>
<p>Can we obtain better ZSAD performance if fine-tuned using medical image data?Comparing the promising performance in industrial datasets, AnomalyCLIP presents a relatively low performance in medical datasets.This is partly due to the impact of auxiliary data used in our prompt learning.So, then we examine whether the ZSAD performance on medical images can be improved if the prompt learning is trained on an auxiliary medical dataset.One challenge is that there are no available large 2D medical datasets that include both image-level and pixel-level annotations for our training.To address this issue, we create such a dataset based on ColonDB (More details see Appendix A), and then optimize the prompts in AnomalyCLIP and VAND using this dataset and evaluate their performance on the medical image datasets.The results are presented in Table 3. AnomalyCLIP and VAND largely improve their detection and segmentation performance compared to that fine-tuned on MVTec AD, especially for the colon polyp-related datasets such as CVC-ClincDB, Kvasir, and Endo (note that these datasets are all from different domains compared to the fine-tuning ColonDB dataset).AnomalyCLIP also exhibits performance improvement in detecting brain tumors in datasets such as HeadCT, BrainMRI, and Br35H.This is attributed to the visual similarities between colon polyps and brain tumors.Conversely, the symptom of the colon polyp differs significantly from that of diseased skin or chest, leading to performance degradation in ISIC and COVID-19.Overall, compared to VAND, AnomalyCLIP performs consistently better across all datasets of anomaly detection and segmentation.Object-agnostic vs. object-aware prompt learning To study the effectiveness of object-agnostic prompt learning in AnomalyCLIP, we compare AnomalyCLIP with its variant that uses an object-aware prompt template.</p>
<p>The performance gain of AnomalyCLIP to its object-aware prompt learning variant is shown in Fig. 5, where positive values indicate our object-agnostic prompt templates are better than the object-aware one.It is clear that our object-agnostic prompt learning performs much better than, or on par with, the object-aware version in both imagelevel and pixel-level anomaly detection.This indicates that having object-agnostic prompts helps   better learn the generic abnormality and normality in images, as the object semantics are often not helpful, or can even become noisy features, for the ZSAD task.</p>
<p>Pixel-level AUROC
V-V attention Q-Q attention K-K attention</p>
<p>ABLATION STUDY</p>
<p>Module ablation</p>
<p>We first validate the effectiveness of different high-level modules of our Anoma-lyCLIP, including DPAM (T 1 ), object-agnostic text prompts (T 2 ), adding learnable tokens in text encoders (T 3 ), and multi-layer visual encoder features (T 4 ).The results are shown in Table 4.It can be observed that each module contributes to the remarkable performance of AnomalyCLIP, among which the object-agnostic learning is the most important one.</p>
<p>Context optimization Next we examine key modules in detail.The object-agnostic prompt learning is the most effective module, and it is driven by our glocal context optimization, so we consider two different optimization terms, local and global losses, in Eq. 2. The results are shown in Table 5.Both global and local context optimization contribute to the superiority of AnomalyCLIP.Global context optimization helps to capture global anomaly semantics, thus enabling more accurate imagelevel detection.Compared to global context optimization, local context optimization incorporates local anomaly semantics, which improves pixel-level performance and complements image-level performance.By synthesizing these two optimization strategies, AnomalyCLIP generally achieves better performance than using them individually.</p>
<p>DPAM strategy ablation AnomalyCLIP uses V -V self-attention by default.Here we study the effectiveness of using two other DPAM strategies, including Q-Q and K-K self-attention, resulting in two AnomalyCLIP variants, namely AnomalyCLIP qq and AnomalyCLIP kk .The comparison results are presented in Fig. 6.AnomalyCLIP qq achieves similar segmentation capabilities as AnomalyCLIP but suffers from degradation in detecting image-level anomalies.Conversely, while AnomalyCLIP kk performs well in anomaly classification, its segmentation performance is less effective than AnomalyCLIP and AnomalyCLIP qq .The V -V self-attention is generally recommended in AnomalyCLIP.</p>
<p>RELATED WORK</p>
<p>Zero-shot anomaly detection ZSAD relies on the model's strong transferability to handle unseen anomalies (Aota et al., 2023).A very recent approach WinCLIP (Jeong et al., 2023) presents a seminal work that leverages CLIP for ZSAD.It uses a large number of hand-crafted text prompts and involves multiple forward passes of image patches for anomaly segmentation.To tackle this inefficiency, VAND (Chen et al., 2023) introduces learnable linear projection techniques to enhance the modeling of local visual semantics.However, these approaches suffer from insufficiently generalized textual prompt embeddings, which degrades their performance in identifying anomalies associated with various unseen object semantics.AnomalyCLIP utilizes only two object-agnostic learnable text prompts to optimize the generic text prompts of abnormality and normality, and it can obtain segmentation results with just a single forward pass.Prompt learning Rather than resorting to full network fine-tuning, prompt learning emerges as a parameter-efficient alternative to achieve satisfactory results (Sun et al., 2022;Khattak et al., 2023;Kim et al., 2023;Zhou et al., 2022a).CoOp (Zhou et al., 2022b) introduces learnable text prompts for few-shot classification.On this basis, DenseCLIP (Rao et al., 2022) extends prompt learning to dense prediction tasks with an extra image decoder.Instead, AnomalyCLIP proposes objectagnostic prompt learning for anomaly detection, blocking out the potential adverse impact of the diverse object semantics on anomaly detection.Benefiting from the glocal context optimization, AnomalyCLIP can capture local anomaly semantics such that we can simultaneously perform classification and segmentation tasks without an additional decoder network like Rao et al. (2022).</p>
<p>CONCLUSION</p>
<p>In this paper, we tackle a challenging yet significant area of anomaly detection, ZSAD, in which there is no available data in the target dataset for training.We propose AnomalyCLIP to improve the weak generalization performance of CLIP for ZSAD.We introduce object-agnostic prompt learning to learn generic abnormality/normality text prompts for generalized ZSAD on image datasets of diverse foreground objects.Further, to incorporate global and local anomaly semantics into Anoma-lyCLIP, we devise a joint global and local context optimization to optimize the object-agnostic text prompts.Extensive experimental results on 17 public datasets demonstrate that AnomalyCLIP achieves superior ZSAD performance.</p>
<p>ACKNOWLEDGMENTS</p>
<p>This work was supported by NSFC under grant No. U1909207.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>To ensure the reproducibility and completeness of this paper, we have included an Appendix consisting of five main sections.In Appendix A, we provide more implementation details of AnomalyCLIP, as well as the reproduction of other baseline methods.Appendix B provides key statistics about the datasets used in our experiments and the implementation of the auxiliary medical dataset for prompt tuning.Appendix C supplements the main paper with additional results and ablations.Further visualizations of similarity scores and maps are detailed in Appendix D. Additionally, the main paper presents only the average performance in each dataset that contains a number of data subsets, for which we present their fine-grained detection results, in Appendix E. Our code will be made publicly accessible once the paper is accepted.</p>
<p>A IMPLEMENTATION DETAILS AND BASELINES</p>
<p>A.1 IMPLEMENTATION DETAILS</p>
<p>In this paper, we use the publicly available CLIP model (VIT-L/14@336px) as our backbone.</p>
<p>Model parameters of CLIP are all frozen.The length of learnable text prompts M is set to 12.These trainable text tokens are attached to the first 9 layers of the text encoder, and each text token has a length of 4. We fine-tune AnomalyCLIP on the test data on MVTec AD and test the performance for other datasets.As for MVTec AD, we fine-tune AomalyCLIP on test data on VisA.To provide adequate visual details, we extract local visual embeddings v i m from the 6-th, 12-th, 18-th, and 24-th layers of the visual encoder.Starting from the 6-th layer, we apply DPAM to the architecture of the visual encoder according to Sec. 3.3.Additionally, we set the balanced weight λ to 1 in our loss function.The input images are resized to a size of 518 with batch size 8, and we use the Adam optimizer (Kingma &amp; Ba, 2014) with a learning rate of 0.001 to update model parameters.During testing, we apply a Gaussian filter with σ = 4 to smooth the anomaly score map.The epoch is 15 for all experiments, which are performed in PyTorch-2.0.0 with a single NVIDIA RTX 3090 24GB GPU.</p>
<p>A.2 BASELINES</p>
<p>To demonstrate the superiority of Anomlay-CLIP, we compare AnomlayCLIP with broad SOTA baselines.Implementation and reproduction details are given as follows:</p>
<p>• CLIP (Radford et al., 2021).CLIP is a powerful zero-shot classification method.</p>
<p>To perform the anomaly detection task, we use two classes of text prompt templates A photo of a normal [cls] and A photo of an anomalous [cls],</p>
<p>where cls denotes the target class name.The anomaly score is computed according to Eq. 1.As for anomaly segmentation, we extend the above computation to local visual embedding to derive the segmentation.</p>
<p>• CLIP-AC (Radford et al., 2021).Different from CLIP, CLIP-AC employs an ensemble of text prompt templates that are recommended for ImageNet dataset (Radford et al., 2021).We average the generated textual embeddings of normal and anomaly classes respectively, and compute the probability and segmentation in the same way as CLIP.</p>
<p>• WinCLIP (Jeong et al., 2023).WinCLIP is a SOTA ZSAD method.They design a large set of hand-crafted text prompt templates specific to anomaly detection and use a window scaling strategy to obtain anomaly segmentation.All parameters are kept the same as in their paper.• VAND (Chen et al., 2023).VAND is an improved version of WinCLIP.They first adjust the text prompt templates and then introduce learnable linear projections to improve local visual semantics to derive more accurate segmentation.All parameters are kept the same as in their paper.</p>
<p>• CoOp (Zhou et al., 2022b).CoOp is a representative method for prompt learning.To adapt CoOp to ZSAD, we replace its learnable text prompt templates
[V 1 ][V 2 ]...[V N ][cls]
with normality and abnormality text prompt templates, where V i is the learnable word embeddings.The normality text prompt template is defined as
[V 1 ][V 2 ]...[V N ][normal][cls],
and the abnormality one is defined as
[V 1 ][V 2 ]...[V N ][anomalous][cls].
Anomaly probabilities and segmentation are obtained in the same way as for AnomalyCLIP.All parameters are kept the same as in their paper.</p>
<p>B DATASET</p>
<p>More dataset details In this paper, we conduct extensive experiments on 17 public datasets spanning two domains and three modalities to validate the effectiveness of our methods.Since we just use the test data of Datasets, we present the relevant information of their test sets in Table 6.We apply the default normalization of OpenCLIP to all datasets.After normalization, we resize the images to a resolution of (518, 518) to obtain an appropriate visual feature map resolution.It should be noted that the original image size of SDD has a width of 500 and a height ranging from 1,240 to 1,270.Before processing, we vertically divide the original 500 × 1,250 image into two images and assign pixel-wise annotations to each image.</p>
<p>Fine-tuning medical dataset</p>
<p>We cannot find publicly available 2D medical AD datasets that include both category labels and segmentation ground truths simultaneously.To fill the blank, in this paper, we create such a medical dataset by combining two existing 2D medical datasets.Particularly, we use the colon polyp detection dataset ColonDB (Tajbakhsh et al., 2015) to provide pixel-level annotations.Meanwhile, considering the normal samples in the same domain, we choose the test split of Endo classification dataset (Hicks et al., 2021) to combine with ColonDB.As a result, the new medical dataset contains 163 normal samples and 380 anomaly samples, supporting both anomaly classification and segmentation tasks.</p>
<p>Comparison with SOTA full-shot methods In this section, we are interested in the performance gap between AnomalyCLIP and the recently published SOTA full-shot methods, such as Patch-Core (Roth et al., 2022) and RD4AD (Deng &amp; Li, 2022).Since some datasets do not provide normal training data, we conduct experiments on six public datasets.AnomalyCLIP achieves comparable anomaly detection and segmentation performance compared to PatchCore and RD4AD, and it even outperforms them in some datasets.This illustrates that the generic prompt embeddings empower AnomalyCLIP to effectively capture the normality and abnormality so that AnomalyCLIP can surpass the performance boundary decided by the training data.</p>
<p>Refinement of the textual space A representative embedding is not only decided by the welldesigned text prompt, it also depends on the appropriate textual space.During fine-tuning, randomly initialized learnable token embeddings are introduced in the text encoder to refine the textual space for the adaption to AD.To control the degree of refining the textual space, we choose to insert the learnable token embeddings into the text encoder from its bottom to the top layer.In particular, the trainable and original tokens are denoted as t ′ m and t m , respectively, where m represents the layer of the text encoder.To integrate the original textual representations, for the layer m, we concatenate t ′ m and t m along the dimension of the channel and then forward them into T m to get r ′ m+1 and t m+1 .Due to the self-attention mechanism, the output of t m+1 contains the information of t ′ m .In order to provide adequate calibration, we discard the obtained r ′ m+1 and initialize new learnable token embeddings t ′ m+1 .Through this operation, t ′ m+1 further refines textual representations of the layer m + 1.We repeat this operation until we reach the designated layer M ′ .This procedure is given by:
[r ′ m+1 , t m+1 ] = T m ([t ′ m , t m ]) [r ′ m+2 , t m+2 ] = T m+1 ([t ′ m+1 , t m+1 ]) (3) . . . t M ′ +1 = T M ′ (t M ′ ),
where the operator [•, •] represents the concatenation along the channel.</p>
<p>Hyparameter analysis We study the length of learnable text prompts E, depth of learnable token embeddings M , length of learnable token embeddings M , and number of used layers in visual encoder N .As shown in Fig. 7b, we observe that the detection and segmentation performance initially improves with an increase in the value of E. However, within the range of lengths from 12 to 16, we notice a decline in performance, which suggests that excessively long learnable text prompts could involve redundant information.Therefore, an appropriate value for E, such as E = 12, is beneficial to accurate learning of object-agnostic text prompts.Besides, we also investigate the depth of the attached learnable token embeddings in Fig. 7b.The degree of refining of the initial text space becomes more pronounced as the depth increases, enabling more discriminative textual embeddings for normal and anomaly.However, the performance drops when the refinement is excessive and impairs the generalization of AnomlayCLIP, as seen in the case when M equals 9.After selecting the depth, we proceed to investigate the influence of the length of learnable token embeddings.As illustrated in Fig. 7c, we find that the length of token embeddings also involves a similar tradeoff between the model generalization and calibration of textual space in Fig. 7d.AnomalyCLIP achieves the overall performance gain when we provide the most local visual semantics (N = 4).</p>
<p>Figure 1 :
1
Figure 1: Comparison of ZSAD results on (b) test data using (c) original text prompts in CLIP (Radford et al., 2021), (d) tailored text prompts for AD in WinCLIP (Jeong et al., 2023), (e) learnable text prompts for general vision tasks in CoOp (Zhou et al., 2022a), and (f) object-agnostic text prompts in our AnomalyCLIP.(a) presents a set of auxiliary data we can use to learn the text prompts.The results are obtained by measuring the similarity between text prompt embeddings and image embeddings.The ground-truth anomaly regions are circled in red in (a) and (b).(c), (d), and (e) suffer from poor generalization across different domains, while our AnomalyCLIP in (f) can well generalize to anomalies in diverse types of objects from different domains.</p>
<p>Figure 2 :
2
Figure 2: Overview of our approach AnomalyCLIP.</p>
<p>Figure 3 :
3
Figure 3: DPAM visualization.Refinement of the local visual space Since the visual encoder of CLIP is originally pre-trained to align global object semantics, using self-attention mechanisms to propagate such global features to local visual embeddings often results in a loss of local visual semantics, hindering the effective learning of the fine-grained abnormality in our object-agnostic text prompts.A similar phenomenon is also found in(Rao et al., 2022;Li et al., 2023).To mitigate this phenomenon, we propose the use of diagonally prominent attention map (DPAM) to refine the local visual space, with the visual encoder kept frozen during training.To this end, we replace the original Q-K attention in the visual encoder with a diagonally prominent attention, such as Q-Q, K-K, and V -V self-attention schemes.As demonstrated in Fig.3c, Fig.3d, and Fig.3e, the refined DPAM attention maps are more diagonally prominent, resulting in substantially improved segmentation maps in both original CLIP and our AnomalyCLIP.Compared to CLIP that is based on global features and manually defined text prompts, the text prompts learned by AnomalyCLIP are more fine-grained, enabling substantially more accurate alignment between the normality/abnormality prompt embeddings and the local visual embeddings across four different self-attention schemes.This, in turn, allows AnomalyCLIP to generate accurate S n and S a for the joint optimization in Eq. 2. Unless otherwise specified, AnomalyCLIP utilizes V -V self-attention due to its superior overall performance.Different self-attention mechanisms are analyzed in Sec. C.</p>
<p>Figure 5 :
5
Figure 4: Segmentation visualization.</p>
<p>Figure 6: DPAM component ablation.</p>
<p>Figure 9 :Figure 10 :Figure 11 :Figure 12 :
9101112
Figure 9: Similarity scores of CLIP on MVTec AD.Each sub-figure represents the visualization of one object.</p>
<p>Figure 13 :
13
Figure 13: Anomaly score maps for the data subset, hazelnut, in MVTec AD.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 14 :
14
Figure 14: Anomaly score maps for the data subset, pill, in MVTec AD.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 15 :
15
Figure 15: Anomaly score maps for the data subset, metal nut, in MVTec AD.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 16 :
16
Figure 16: Anomaly score maps for the data subset, capsule, in MVTec AD.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 17 :
17
Figure 17: Anomaly score maps for the data subset, screw, in MVTec AD.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 18 :
18
Figure 18: Anomaly score maps for the data subset candle.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 19 :
19
Figure 19: Anomaly score maps for the data subset chewinggum.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 20 :
20
Figure 20: Anomaly score maps for the data subset capusle.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 21 :
21
Figure 21: Anomaly score maps for the data subset cashew.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 22 :
22
Figure 22: Anomaly score maps for the data subset pcb.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 23 :
23
Figure 23: Anomaly score maps for the data subset pip fryum.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 24 :
24
Figure 24: Similarity scores for the data subset bracket.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 25 :
25
Figure 25: Anomaly score maps for the data subset metal plate.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 26 :
26
Figure 26: Anomaly score maps for the data subset tube.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 27 :
27
Figure 27: Anomaly score maps for the data subset grid.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 28 :
28
Figure 28: Anomaly score maps for the data subset leather.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 29 :
29
Figure 29: Anomaly score maps for the data subset carpet.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 30 :
30
Figure 30: Anomaly score maps for the data subset tile.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 31 :
31
Figure 31: Anomaly score maps for the data subset wood.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 32 :
32
Figure 32: Anomaly score maps for the data subset zipper.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 33 :
33
Figure 33: Similarity scores for the data subset skin.</p>
<p>Figure 34 :
34
Figure 34: Anomaly score maps for the data subset thyroid.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 35 :
35
Figure 35: Anomaly score maps for the data subset colon.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Figure 36 :
36
Figure 36: Anomaly score maps for the data subset brain.The first row represents the input, and we circle the anomaly regions in the second row.The last row presents the segmentation results from AnomalyCLIP.</p>
<p>Table 1 :
1
ZSAD performance comparison on industrial domain.The best performance is highlighted in red, and the second-best is highlighted in blue.† denotes results taken from original papers.
TaskCategoryDatasets|C|CLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPObj &amp;textureMVTec AD15(74.1, 87.6)(71.5, 86.4)(91.8, 96.5)  †(86.1, 93.5)  †(88.8, 94.8)(91.5, 96.2)VisA12(66.4, 71.5)(65.0, 70.1)(78.1, 81.2)  †(78.0, 81.4)  †(62.8, 68.1)(82.1, 85.4)Image-level (AUROC, AP)ObjMPDD BTAD SDD6 3 1(54.3, 65.4) (34.5, 52.5) (65.7, 45.2)(56.2, 66.0) (51.0, 62.1) (65.2, 45.7)(63.6, 69.9) (68.2, 70.9) (84.3, 77.4)(73.0, 80.2) (73.6, 68.6) (79.8, 71.4)(55.1, 64.2) (66.8, 77.4) (74.9, 65.1)(77.0, 82.0) (88.3, 87.3) (84.7, 80.0)TextureDAGM DTD-Synthetic10 12(79.6, 59.0) (71.6, 85.7)(82.5, 63.7) (66.8, 83.2)(91.8, 79.5) (93.2, 92.6)(94.4, 83.8) (86.4, 95.0)(87.5, 74.6) (-, -)(97.5, 92.3) (93.5, 97.0)Obj &amp;textureMVTec AD15(38.4, 11.3)(38.2, 11.6)(85.1, 64.6)  †(87.6, 44.0)  †(33.3, 6.7)(91.1, 81.4)VisA12(46.6, 14.8)(47.8, 17.3)(79.6, 56.8)  †(94.2, 86.8)  †(24.2, 3.8)(95.5, 87.0)Pixel-level (AUROC, PRO)ObjMPDD BTAD SDD6 3 1(62.1, 33.0) (30.6, 4.4) (39.0, 8.9)(58.7, 29.1) (32.8, 8.3) (32.5, 5.8)(76.4, 48.9) (72.7, 27.3) (68.8, 24.2)(94.1, 83.2) (60.8, 25.0) (79.8, 65.1)(15.4, 2.3) (28.6, 3.8) (28.9, 7.1)(96.5, 88.7) (94.2, 74.8) (90.6, 67.8)TextureDAGM DTD-Synthetic10 12(28.2, 2.9) (33.9, 12.5)(32.7, 4.8) (23.7, 5.5)(87.6, 65.7) (83.9, 57.8)(82.4, 66.2) (95.3, 86.9)(17.5, 2.1) (-, -)(95.6, 91.0) (97.9, 92.3)</p>
<p>Table 2 :
2
ZSAD performance comparison on medical domain.The best performance is highlighted in red, and the second-best is highlighted in blue.Note that the image-level medical AD datasets do not contain segmentation ground truth, so the pixel-level medical AD datasets are different from the image-level datasets.
TaskCategoryDatasets|C|CLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPHeadCT1(56.5, 58.4)(60.0, 60.7)(81.8, 80.2)(89.1, 89.4)(78.4, 78.8)(93.4, 91.6)Image-levelBrainBrainMRI1(73.9, 81.7)(80.6, 86.4)(86.6, 91.5)(89.3, 90.9)(61.3, 44.9)(90.3, 92.2)(AUROC, AP)Br35H1(78.4, 78.8)(82.7, 81.3)(80.5, 82.2)(93.1, 92.9)(86.0, 87.5)(94.6, 94.7)ChestCOVID-191(73.7, 42.4)(75.0, 45.9)(66.4, 42.9)(15.5, 8.5)(25.3, 9.2)(80.1, 58.7)SkinISIC1(33.1, 5.8)(36.0, 7.7)(83.3, 55.1)(89.4, 77.2)(51.7, 15.9)(89.7, 78.4)CVC-ColonDB1(49.5, 15.8)(49.5, 11.5)(70.3,32.5)(78.4, 64.6)(40.5, 2.6)(81.9, 71.3)Pixel-level (AUROC, PRO)ColonCVC-ClinicDB Kvasir1 1(47.5, 18.9) (44.6, 17.7)(48.5, 12.6) (45.0, 16.8)(51.2,13.8) (69.7, 24.5)(80.5, 60.7) (75.0, 36.2)(34.8, 2.4) (44.1, 3.5)(82.9, 67.8) (78.9, 45.6)Endo1(45.2, 15.9)(46.6, 12.6)(68.2, 28.3)(81.9, 54.9)(40.6, 3.9)(84.1, 63.6)ThyroidTN3K1(42.3, 7.3)(35.6, 5.2)(70.7, 39.8)(73.6, 37.8)(34.0, 9.5)(81.5, 50.4)</p>
<p>Table1shows the ZSAD results of AnomalyCLIP with five competing methods over seven industrial defect datasets of very different foreground objects, background, and/or anomaly types.AnomalyCLIP achieves superior ZSAD performance across the datasets, substantially outperforming the other five methods in most datasets.The weak performance of CLIP and CLIP-AC can be attributed to CLIP's original pre-training, which focuses on aligning object semantics rather than anomaly semantics.By using manually defined text prompts, WinCLIP and VAND achieve better results.Alternatively, CoOp adopts learnable prompts to learn the global anomaly semantics.However, those prompts focus on the global feature and ignore the fine-grained local anomaly semantics, leading to their poor performance on anomaly segmentation.To adapt CLIP to ZSAD, AnomalyCLIP learns object-agnostic text prompts to focus on learning the generic abnormality/normality using global and local context optimization, enabling the modeling of both global and local abnormality/normality.Our resulting prompts can also generalize to different datasets from various domains.To provide more intuitive results, we visualize the anomaly segmentation results of AnomalyCLIP, VAND, and WinCLIP across different datasets in Fig.4.Compared to VAND and WinCLIP, AnomalyCLIP can perform much more accurate segmentation for the defects from different industrial inspection domains.</p>
<p>Table 4 :
4
Module ablation.
ModuleMVTec AD Pixel-level Image-level Pixel-level Image-level VisABase(46.8, 15.4) (66.3, 83.3) (47.9, 17.1) (54.4, 61.7)+T 1 (68.4, 47.4) (66.3, 83.3) (54.8, 32.7) (54.4, 61.7)+T 2 (89.5, 81.2) (90.8, 96.0) (95.0, 85.3) (81.7, 85.2)+T 3 (90.0, 81.1) (91.0, 96.1) (95.2, 86.0) (81.9, 85.2)+T 4 (91.1, 81.4) (91.5, 96.2) (95.5, 87.0) (82.1, 85.4)</p>
<p>Table 5 :
5
Context optimization ablation.
Local. Global.MVTec AD Pixel-level Image-level Pixel-level Image-level VisA✗✗(46.8, 15.4) (66.3, 83.3) (47.9, 17.1) (54.4, 61.7)✗✓(80.3, 77.8) (89.9, 95.4) (86.6, 78.1) (82.2, 84.9)✓✗(91.0, 80.4) (89.9, 96.0) (95.2, 86.5) (79.5, 83.2)✓✓(91.1, 81.4) (91.5, 96.2) (95.5, 87.0) (82.1, 85.4)</p>
<p>Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, and Xinyi Le.A unified model for multi-class anomaly detection.Advances in Neural Information Processing Systems, 35:4571-4584, 2022.Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al.Regionclip: Region-based language-image pretraining.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.16793-16803, 2022.Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.Conditional prompt learning for vision-language models.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.16816-16825, 2022a.Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu.Learning to prompt for visionlanguage models.International Journal of Computer Vision, 130(9):2337-2348, 2022b.Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer.Spot-the-difference self-supervised pre-training for anomaly detection and segmentation.In European Conference on Computer Vision, pp.392-408.Springer, 2022.</p>
<p>Table 6 :
6
Key statistics on the datasets used.
DatasetCategoryModalities|C|Normal and anomalous samplesUsageMVTec ADObj &amp;texture Photography 15(467, 1258)Industrial defect detectionVisAPhotography 12(962, 1200)Industrial defect detectionMPDD BTADObjPhotography Photography6 3(176, 282) (451, 290)Industrial defect detection Industrial defect detectionSDDPhotography1(181, 74)Industrial defect detectionDAGM DTD-SyntheticTexturePhotography 10 Photography 12(6996, 1054) (357, 947)Industrial defect detection Industrial defect detectionISICSkinPhotography1(0, 379)Skin cancer detectionCVC-ClinicDBEndoscopy1(0, 612)Colon polyp detectionCVC-ColonDBEndoscopy1(0, 380)Colon polyp detectionKvasirEndoscopy1(0, 1000)Colon polyp detectionEndoEndoscopy1(0, 200)Colon polyp detectionTN3KThyroidRadiology (Utralsound)1(0, 614)Thyroid nodule detectionHeadCTRadiology (CT)1(100, 100)Brain tumor detectionBrainMRIBrainRadiology (MRI)1(98, 155)Brain tumor detectionBr35HRadiology (MRI)1(1500, 1500)Brain tumor detectionCOVID-19ChestRadiology (X-ray)1(1341, 219)COVID-19 detection</p>
<p>Table 10 :
10
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmentation on MVTec AD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCarpet11.510.795.498.46.798.8Bottle17.523.389.583.423.190.4Hazelnut25.234.094.396.130.297.1Leather9.95.696.799.111.798.6Cable37.437.577.072.349.778.9Capsule50.949.186.992.035.595.8Grid8.711.982.295.87.897.3Pill55.860.880.076.246.592Transistor51.148.574.762.450.171Metal nut43.953.661.065.449.374.4Screw80.176.489.697.817.097.5Toothbrush36.335.086.995.864.991.9Zipper51.544.791.691.133.491.4Tile49.939.177.692.741.794.6Wood45.742.493.495.831.496.5Mean38.438.285.187.633.391.1</p>
<p>Table 11 :
11
Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on MVTec AD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCarpet2.91.984.148.50.590.1Bottle1.44.976.445.64.580.9Hazelnut2.89.481.670.34.792.4Leather0.20.091.172.41.892.2Cable7.36.942.925.712.264.4Capsule13.214.962.151.35.787.2Grid0.92.457.031.61.075.6Pill6.08.265.065.43.288.2Transistor15.311.243.421.39.358.1Metal nut2.910.331.838.47.071.0Screw57.856.268.567.16.488.0Toothbrush5.85.267.754.516.688.5Zipper17.715.271.710.711.665.3Tile21.516.351.226.710.187.6Wood13.710.374.131.15.191.2Mean11.311.664.644.06.781.4</p>
<p>Table 12 :
12
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classification on MVTec AD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCarpet9693.1100.099.599.9100.0Bottle45.946.199.292.087.789.3Hazelnut88.791.193.989.693.597.2Leather99.499.5100.099.799.999.8Cable58.146.686.588.456.769.8Capsule71.468.872.979.981.189.9Grid72.563.798.886.394.797.0Pill73.673.879.180.578.681.8Transistor48.851.288.080.892.292.8Metal nut62.863.497.168.485.393.6Screw78.266.783.384.988.981.1Toothbrush73.389.288.053.877.584.7Zipper60.136.191.589.698.898.5Tile88.589.0100.099.999.7100.0Wood9494.999.499.097.796.8Mean74.171.591.886.188.891.5</p>
<p>Table 13 :
13
Fine-grained data-subset-wise performance comparison (AP) on for anomaly classification MVTec AD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCarpet98.897.8100.099.8100.0100.0Bottle78.979.899.897.796.497.0Hazelnut94.695.996.994.896.798.6Leather99.899.8100.099.9100.099.9Cable70.864.391.293.169.481.4Capsule92.190.991.595.595.797.9Grid87.183.999.694.998.199.1Pill93.493.695.796.094.295.4Transistor48.149.987.177.590.290.6Metal nut87.789.299.391.996.398.5Screw91.486.693.193.696.292.5Toothbrush90.796.095.671.590.493.7Zipper87.473.997.597.199.799.6Tile95.996.2100.0100.099.9100.0Wood97.998.399.899.799.499.2Mean87.686.496.593.594.896.2</p>
<p>Table 14 :
14
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmentation on VisA.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCandle33.650.088.997.816.398.8Capsules56.861.581.697.547.595.0Cashew64.562.584.786.032.593.8Chewinggum43.056.593.399.53.499.3Fryum45.662.788.592.021.794.6Macaroni120.322.970.998.836.898.3Macaroni237.728.859.397.827.597.6Pcb157.851.661.292.719.894.1Pcb234.738.471.689.722.992.4Pcb354.644.685.388.418.088.4Pcb452.149.994.494.614.095.7Pipe fryum58.744.775.496.029.298.2Mean46.647.879.694.224.295.5</p>
<p>Table 15 :
15
Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on VisA.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCandle3.66.083.592.51.196.2Capsules15.822.435.386.718.478.5Cashew9.610.976.491.71.791.6Chewinggum17.830.270.487.30.191.2Fryum12.129.377.489.72.686.8Macaroni18.113.434.393.218.189.8Macaroni220.918.421.482.32.784.2Pcb111.712.526.387.50.181.7Pcb212.813.937.275.60.778.9Pcb331.723.656.177.80.077.1Pcb417.120.380.486.80.091.3Pipe fryum16.76.082.390.90.696.8Mean14.817.356.886.83.887.0</p>
<p>Table 16 :
16
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classification on VisA.</p>
<p>Table 17 :
17
Fine-grained data-subset-wise performance comparison (AP) for anomaly classification on VisA.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPCandle42.940.095.886.952.981.1Capsules81.084.390.974.385.388.7Cashew83.486.196.494.187.189.4Chewinggum90.490.298.698.493.198.9Fryum82.076.690.197.290.296.8Macaroni156.858.775.870.952.386.0Macaroni265.065.860.363.262.272.1Pcb156.948.478.457.236.087.0Pcb263.259.849.273.847.364.3Pcb353.047.676.570.754.870.0Pcb488.090.677.795.166.394.4Pipe fryum94.693.782.394.889.796.3Mean71.570.181.281.468.185.4</p>
<p>Table 18 :
18
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly segmentation on MPDD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPBracket black85.386.457.896.39.395.7Bracket brown26.931.572.286.220.294.4Bracket white83.577.479.599.08.399.8Connector56.552.979.090.67.697.2Metal plate64.352.592.693.114.193.8Tubes56.451.577.699.133.298.1Mean62.158.776.494.115.496.5</p>
<p>Table 19 :
19
Fine-grained data-subset-wise performance comparison (PRO) for anomaly segmentation on MPDD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPBracket black62.658.94389.71.585.2Bracket brown2.84.025.070.30.477.7Bracket white47.941.657.693.10.098.8Connector22.820.244.674.50.089.8Metal plate31.527.078.274.50.286.9Tubes30.422.944.796.911.593.6Mean33.029.148.983.22.388.7</p>
<p>Table 20 :
20
Fine-grained data-subset-wise performance comparison (AUROC) for anomaly classification on MPDD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPBracket black32.432.841.566.136.967.3Bracket brown50.957.948.664.043.962.2Bracket white45.442.640.279.648.964.9Connector7576.279.378.838.386.9Metal plate34.954.893.453.877.085.2Tubes87.372.878.795.985.495.5Mean54.356.263.673.055.177.0</p>
<p>Table 21 :
21
Fine-grained data-subset-wise performance comparison (AP) for anomaly classification on MPDD.
Object nameCLIPCLIP-ACWinCLIPVANDCoOpAnomalyCLIPBracket black47.848.656.971.750.072.9Bracket brown66.272.069.579.065.780.8Bracket white51.247.345.182.357.568.5Connector62.261.461.371.826.476.8Metal plate70.678.597.678.392.094.7Tubes94.488.289.198.193.698.1Mean65.466.069.980.264.282.0
https://github.com/mlfoundations/open clip
C ADDITIONAL RESULTS AND ABLATIONSPrompt template ablation Here, we study the robustness of AnomalyCLIP to prior anomaly semantics in the object-agnostic text prompt template.We replace damaged in the object-agnostic text prompt with other words having similar anomaly semantics, such as anomalous, flawed, defective, blemished.The results are presented in Table8 and Table 9.The steady results indicate that AnomalyCLIP is not sensitive to the prior anomaly semantics introduced by the objectagnostic text prompt template.Object ablationTo investigate what the object-agnostic text prompts have learned, we replace object in object-agnostic text prompts with specific target [cls], resulting in AnomalyCLIP re .In Fig.8, AnomalyCLIP re still performs well in ZSAD, even as we block out the object semantics during fine-tuning.This suggests that the knowledge learned by object-agnostic text prompts is the underlying anomaly patterns, allowing them to provide discriminative textual embeddings even when specific object semantics are incorporated.Furthermore, compared to AnomalyCLIP, AnomalyCLIP re shows a performance decay, which can be attributed to the inclusion of redundant/noisy object semantics.These results once again demonstrate the generalization ability of object-agnostic prompt learning.D VISUALIZATIONSimilarity score between textual and visual embeddings.We present visualizations of the similarity scores generated by both CLIP and AnomalyCLIP.These visualizations aim to provide an intuitive illustration of the effective adaptation made by AnomalyCLIP in comparison to CLIP.As shown in Fig.9and Fig.10, we present the similarity score of CLIP on MVTec AD and VisA.The  normal and anomaly scores are severely overlapped.Further, the range of scores is centered at 0.5.These show that the textual and visual space of CLIP originally aligned for object semantics are not desired for ZSAD.Also, we visualize the similarity scores of AnomalyCLIP in Fig.11and Fig.12.Compared to CLIP, there is a significant overlap between the scores assigned to normal and anomaly instances, and at the same time, the score range is considerably wider.These results indicate that AnomalyCLIP achieves a significant improvement in adapting CLIP to ZSAD.Anomaly score map for different datasets.In addition to the similarity score for anomaly classification, we also visualize the anomaly score maps to present the strong anomaly segmentation ability of AnomalyCLIP.Specifically, we visualize the industrial object class: hazelnut, pill, and screw from MVTec AD; candle, chewinggum, capsule, cashew, pcb, and pip fryum from Visa; bracket, metal plate, and tube from MPDD.We also visualize the industrial texture: grid, leather, carpet, tile, wood, and zipper.In addition, we visualize the segmentation in medical domain across photography, endoscopy, and radiology images: skin cancer detection from ISIC; thyroid nodule detection from TN3K; colon polyp detection from Kvasir; brain tumor detection from Br35H.E FINE-GRAINED ZSAD PERFORMANCEIn this section, we present the fine-grained data subset-level ZSAD performance in details.
Zero-shot versus many-shot: Unsupervised texture anomaly detection. Toshimichi Aota, Lloyd Teh Tzer, Takayuki Tong, Okatani, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2023</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Computerized medical imaging and graphics. Jorge Bernal, Javier Sánchez, Gloria Fernández-Esparrach, Debora Gil, Cristina Rodríguez, Fernando Vilariño, 201543</p>
<p>Anomaly detection under distribution shift. Tri Cao, Jiawen Zhu, Guansong Pang, arXiv:2303.138452023arXiv preprint</p>
<p>A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. Xuhai Chen, Yue Han, Jiangning Zhang, arXiv:2305.173822023arXiv preprint</p>
<p>Deep one-class classification via interpolated gaussian descriptor. Yuanhong Chen, Yu Tian, Guansong Pang, Gustavo Carneiro, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Nasser Al Emadi, Mamun Bin Ibne Reaz, and Mohammad Tariqul Islam. Can ai help in screening viral and covid-19 pneumonia?. E H Muhammad, Tawsifur Chowdhury, Amith Rahman, Rashid Khandakar, Muhammad Mazhar, Abdul Kadir, Khandakar Reajul Zaid Bin Mahbub, Muhammad Islam, Atif Salman Khan, Iqbal, 10.1109/ACCESS.2020.3010287IEEE Access. 82020</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Catching both gray and black swans: Open-set supervised anomaly detection. Choubo Ding, Guansong Pang, Chunhua Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Deep learning for medical anomaly detection-a survey. Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes, ACM Computing Surveys (CSUR). 5472021</p>
<p>Multi-task learning for thyroid nodule segmentation with thyroid region prior. Haifan Gong, Guanqi Chen, Ranran Wang, Xiang Xie, Mingzhi Mao, Yizhou Yu, Fei Chen, Guanbin Li, 2021 IEEE 18th international symposium on biomedical imaging (ISBI). IEEE2020. 2020Br35h: Brain tumor detection</p>
<p>The endotect 2020 challenge: evaluation and comparison of classification, segmentation and inference time for endoscopy. Debesh Steven A Hicks, Vajira Jha, Pål Thambawita, Hugo L Halvorsen, Michael A Hammer, Riegler ; Chaoqin, Haoyan Huang, Aofan Guan, Ya Jiang, Michael Zhang, Yan-Feng Spratling, Wang, Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event. SpringerJanuary 10-15, 2021. 2021. 2022European Conference on Computer Vision</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, Milos Skotak, 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT). IEEE2021</p>
<p>Kvasir-seg: A segmented polyp dataset. Debesh Jha, Pia H Smedsrud, Michael A Riegler, Pål Halvorsen, Dag Thomas De Lange, Håvard D Johansen, Johansen, MultiMedia Modeling: 26th International Conference. Daejeon, South KoreaSpringerJanuary 5-8, 2020. 20202020Proceedings, Part II 26</p>
<p>Visual prompt tuning. Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, Ser-Nam Lim, European Conference on Computer Vision. Springer2022</p>
<p>Maple: Multi-modal prompt learning. Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, Fahad Shahbaz Khan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Zegot: Zero-shot segmentation through optimal transport of text prompts. Kwanyoung Kim, Yujin Oh, Jong Chul, Ye , arXiv:2301.121712023arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, arXiv:2304.026432023Segment anything. arXiv preprint</p>
<p>Attention based glaucoma detection: A large-scale database and cnn model. Liu Li, Mai Xu, Xiaofei Wang, Lai Jiang, Hanruo Liu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). June 2019a</p>
<p>Dice loss for dataimbalanced nlp tasks. Xiaoya Li, Xiaofei Sun, Yuxian Meng, Junjun Liang, Fei Wu, Jiwei Li, arXiv:1911.028552019barXiv preprint</p>
<p>Clip surgery for better explainability with enhancement in open-vocabulary tasks. Yi Li, Hualiang Wang, Yiqun Duan, Xiaomeng Li, arXiv:2304.056532023arXiv preprint</p>
<p>Kaiming He, and Piotr Dollár. Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Clip-driven universal model for organ segmentation and tumor detection. Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Yixuan Bennett A Landman, Alan Yuan, Yucheng Yuille, Zongwei Tang, Zhou, arXiv:2301.007852023arXiv preprint</p>
<p>Philipp Liznerski, Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Marius Kloft, Klaus-Robert Müller, arXiv:2007.01760Explainable deep one-class classification. 2020arXiv preprint</p>
<p>Vt-adl: A vision transformer network for image anomaly detection and localization. Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, Gian Luca Foresti, 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE). IEEE2021</p>
<p>Rgi: robust gan-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection. Shancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, Jianjun Shi, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Explainable deep fewshot anomaly detection with deviation networks. Guansong Pang, Choubo Ding, Chunhua Shen, Anton Van Den, Hengel, arXiv:2108.004622021aarXiv preprint</p>
<p>Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly detection: A review. Guansong Pang, Chunhua Shen, ACM computing surveys (CSUR). 2021b54</p>
<p>Medical image understanding with pretrained vision language models: A comprehensive study. Ziyuan Qin, Huahui Yi, Qicheng Lao, Kang Li, arXiv:2209.155172022arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Exploring the effect of image enhancement techniques on covid-19 detection using chest x-ray images. Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin, Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, M Susu, Muhammad Zughaier, Salman Khan, Computers in biology and medicine. 1321043192021</p>
<p>Denseclip: Language-guided dense prediction with context-aware prompting. Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, Jiwen Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Mean-shifted contrastive loss for anomaly detection. Tal Reiss, Yedid Hoshen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>A unifying review of deep and shallow anomaly detection. Lukas Ruff, Robert A Jacob R Kauffmann, Grégoire Vandermeulen, Wojciech Montavon, Marius Samek, Thomas G Kloft, Klaus-Robert Dietterich, Müller, Proceedings of the IEEE. 10952021</p>
<p>Clip for all things zero-shot sketch-based image retrieval, fine-grained or not. Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, Yi-Zhe Song, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation to multi-label recognition with limited annotations. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H Rohban, Hamid R Rabiee, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021. 202235Multiresolution knowledge distillation for anomaly detection</p>
<p>Segmentation-based deep-learning approach for surface-defect detection. Domen Tabernik, Samo Šela, Jure Skvarč, Danijel Skočaj, Journal of Intelligent Manufacturing. 3132020</p>
<p>Automated polyp detection in colonoscopy videos using shape and context information. Nima Tajbakhsh, Jianming Suryakanth R Gurudu, Liang, IEEE transactions on medical imaging. 3522015</p>
<p>Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images. Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvinder Singh, Gustavo Carneiro, Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference. Strasbourg, FranceSpringerSeptember 27-October 1, 2021. 2021Proceedings, Part V 24</p>
<p>Self-supervised pseudo multi-class pre-training for unsupervised anomaly detection and segmentation in medical images. Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder Singh, Gustavo Carneiro, Medical Image Analysis. 1029302023</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Weakly supervised learning for industrial optical inspection. Matthias Wieler, Tobias Hahn, DAGM symposium in. 20076</p>
<p>Aligning bag of regions for open-vocabulary object detection. Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, Chen Change Loy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Pushing the limits of fewshot anomaly detection in industry vision. Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, Yaochu Jin, arXiv:2301.120822023GraphcorearXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
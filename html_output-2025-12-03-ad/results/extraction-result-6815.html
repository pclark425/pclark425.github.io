<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6815 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6815</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6815</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-273233577</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.08047v2.pdf" target="_blank">Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Complex logical reasoning tasks require a long sequence of reasoning, which a large language model (LLM) with chain-of-thought prompting still falls short. To alleviate this issue, neurosymbolic approaches incorporate a symbolic solver. Specifically, an LLM only translates a natural language problem into a satisfiability (SAT) problem that consists of first-order logic formulas, and a sound symbolic solver returns a mathematically correct solution. However, we discover that LLMs have difficulties to capture complex logical semantics hidden in the natural language during translation. To resolve this limitation, we propose a Compositional First-Order Logic Translation. An LLM first parses a natural language sentence into newly defined logical dependency structures that consist of an atomic subsentence and its dependents, then sequentially translate the parsed subsentences. Since multiple logical dependency structures and sequential translations are possible for a single sentence, we also introduce two Verification algorithms to ensure more reliable results. We utilize an SAT solver to rigorously compare semantics of generated first-order logic formulas and select the most probable one. We evaluate the proposed method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it outperforms the previous neurosymbolic approaches and achieves new state-of-the-art results.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6815.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6815.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLOVER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Compositional First-Order Logic Translation and Verification (CLOVER)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic method that decomposes natural-language sentences into logical dependency structures, incrementally translates subsentences to many-sorted first-order logic via an LLM, and selects/verifies candidates using SAT-based verification (logical consistency and disproving by counter-interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as the base LLM for parsing, accumulation, sequential translation, and some verification decisions; used in few-shot, zero-temperature sampling to generate candidate logical dependency parses and formula translations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (LLM) + neurosymbolic pipeline with SAT solver (Z3)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Compositional first-order logic translation via logical dependency parsing, component accumulation, sequential translation; SAT-based verification (logical consistency and disproving by counter-interpretation); final formal reasoning executed by a symbolic solver.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 theorem prover is used as the symbolic solver to check T-satisfiability, group logically equivalent formulas, find counter-interpretations for (φ_p ∧ ¬φ_q) and to execute the constructed SAT problems to produce final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, ZebraLogic, Puzzle, Symbol, Deduction, FOLIO, ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A suite of seven logical reasoning benchmarks comprising many-sorted first-order-logic translation and reasoning tasks: AR-LSAT (analytical reasoning LSAT problems), ZebraLogic (zebra puzzles), Puzzle/Symbol/Deduction (BigBench logical reasoning tasks), FOLIO (expert-written FOL reasoning), ProofWriter (deductive reasoning proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation (sentence-level) and SAT-based logical reasoning (query satisfiability/validity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / program accuracy; execution rate; execution accuracy (for neurosymbolic methods)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Overall task accuracies reported per Table 1: AR-LSAT 62.8% ; ZebraLogic 75.4% ; Puzzle 83.5% ; Symbol 89.9% ; Deduction 99.3% ; FOLIO 78.8% ; ProofWriter 96.7%. Program/execution metrics (Table 2) show large increases in execution rate and execution accuracy versus Logic-LM (examples: AR-LSAT program accuracy improved to 46.8% and execution rate to 83.8% (see Table 2)).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms prior neurosymbolic baseline Logic-LM by +20.4 percentage points on AR-LSAT and +30.0 points on ZebraLogic (explicitly reported); shows higher execution rate and execution accuracy than Logic-LM across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Compositional translation plus SAT-based verification substantially improves syntactic and semantic fidelity of LLM-generated first-order formulas, yielding state-of-the-art performance across seven logical reasoning benchmarks and largest gains on highest FOL complexity problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Higher inference/API cost due to multiple compositional parses and sampling; residual errors concentrated in preprocessing and other non-translation issues; LLM still requires few-shot exemplars and deterministic SAT solver calls, and the pipeline increases runtime and API usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6815.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-LM (prior neurosymbolic baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neurosymbolic approach that uses an LLM to translate natural-language logical problems into first-order logic (single-shot translation) and invokes a symbolic solver to perform formal reasoning; used as the primary baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an LLM as a semantic parser to produce a single first-order logic translation per sentence which is handed to a symbolic solver for automated reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (LLM) + SAT solver (neurosymbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct single-step first-order logic translation by the LLM followed by symbolic SAT solving</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Also uses a theorem prover/SAT solver (Z3) to check satisfiability and execute logical reasoning on translated formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, ZebraLogic, Puzzle, Symbol, Deduction, FOLIO, ProofWriter</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same set of logical reasoning benchmarks used to evaluate neurosymbolic translation+solver pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation and SAT-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy / program accuracy; execution rate; execution accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in Table 1 and Table 2: AR-LSAT 42.4% ; ZebraLogic 45.4% ; Puzzle 64.0% ; Symbol 81.8% ; Deduction 95.3% ; FOLIO 75.4% ; ProofWriter 95.3%. (Table 2 shows lower execution rates and execution accuracies versus CLOVER; specific numbers in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as the main baseline; CLOVER improves AR-LSAT by +20.4 points and ZebraLogic by +30.0 points compared to Logic-LM.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Logic-LM is effective on simpler FOL translation tasks but degrades on more complex first-order structures; many errors attributable to incorrect logic or syntax in translations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Single-step translation fails to capture complex logical structure in sentences (large drop in translation fidelity with increasing FOL complexity); high rate of incorrect logic and syntax errors in translations; lower execution rate (more translations unexecutable by solver).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6815.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps from LLMs (few-shot or zero-shot) to improve multi-step problem solving; used as a baseline method in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o (and other LLMs in baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategy applied to transformer-based LLMs to produce step-by-step natural language reasoning before an answer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (prompting method)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought prompting (in-context elicitation of intermediate steps); sometimes combined with self-consistency in prior work (not applied here).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, Puzzle, Symbol, Deduction, FOLIO, ProofWriter (as baseline rows in evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Benchmarks containing multi-step logical reasoning tasks; CoT is reported as a baseline on these.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step logical reasoning (natural-language chain explanations) and multiple-choice answering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baseline CoT accuracies in Table 1 (examples include AR-LSAT ~36.8% and other task-specific values shown in the paper's baseline rows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT improves over naive prompting on short/simple reasoning chains but shows pronounced performance drop on long/complex reasoning chains compared to neurosymbolic approaches; CLOVER and other neurosymbolic methods outperform CoT on hard FOL translation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT helps simple logical reasoning but fails to scale to long sequences of strictly formal first-order reasoning; neurosymbolic methods maintain robustness to reasoning length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance declines sharply as reasoning chain length grows; natural-language chain steps are less reliable for strict FOL semantics compared to symbolic solving.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6815.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymbCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymbCoT (Symbolic Chain-of-Thought baseline reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline combining LLM chain-of-thought style outputs with symbolic verification/execution; included in empirical comparisons (table row labeled SymbCoT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that integrates LLM-generated structured reasoning with symbolic execution (as reported in baselines), details in paper's comparison tables.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer + symbolic execution</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Chain-of-thought-like generation with symbolic checking/execution (baseline configuration)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>When applicable uses symbolic execution / theorem prover to attempt to execute or verify LLM-generated programs/formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, Puzzle, Symbol (reported in Table 5 and Table 1 as baseline rows)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same logical reasoning benchmarks; SymbCoT entries shown in the comparative tables.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation and symbolic execution/verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from tables: AR-LSAT ~34.2% ; Puzzle ~66.5% ; Symbol ~55.6% (table values reported in the paper's baseline rows).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Per-table results, SymbCoT generally outperforms naive prompting on some tasks but underperforms CLOVER and Logic-LM on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating symbolic execution with CoT offers gains over pure prompting on some tasks but still falls short of compositional translation + SAT-verification on complex FOL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reported performance varies by task; detailed method description not provided in the paper (used as a comparative baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6815.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Disproving by Counter-Interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Disproving by Counter-Interpretation (SAT-based verification algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification algorithm that uses a SAT solver to find counter-interpretations to (φ_p ∧ ¬φ_q) and then queries an LLM to decide whether the interpretation satisfies the target natural-language sentence; sequentially disproves incorrect formulas to select the correct translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM component is only used to judge whether a SAT-solver-produced interpretation satisfies the original NL sentence (boolean decision); the heavy logical checks are performed by the SAT solver.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>LLM (decision oracle) + SAT solver (Z3) for counter-interpretation generation</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>SAT-based counterexample generation plus LLM interpretability decision; algorithm iteratively compares formula pairs and disproves one based on counter-interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 is used to test satisfiability of (φ_p ∧ ¬φ_q) and to provide a T-interpretation I; the LLM is then prompted (few-shot) to decide if I satisfies the NL sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, ZebraLogic (used in verification ablations and Table 7 comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Datasets used to measure effect of verification strategies on translation correctness and downstream reasoning accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation verification (selecting correct formula among candidates)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final task accuracy after verification (accuracy, program accuracy); improvement over other verification approaches</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per Table 7: Disproving by counter-interpretation yields AR-LSAT 62.8% and ZebraLogic 75.4% (best reported verification results), compared to logical consistency 61.9% / 74.2% and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Produces small but consistent gains over logical-consistency-only verification (e.g., +0.9 points on AR-LSAT in Table 7) and outperforms non-SAT verification baselines (syntax consistency, LLM w/ instruction).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining deterministic SAT-based counterexample generation with an LLM's semantic judgment reduces consistent logical mistakes and improves selection of correct formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires additional SAT solver calls (increased cost); relies on LLM accuracy in judging whether a concrete interpretation satisfies the NL sentence (LLM decisions can still err); overall computational cost higher than simpler verification heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6815.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logical Consistency (verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Consistency (SAT-based grouping verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification algorithm that groups T-equivalent (logically equivalent under the theory) candidate formulas using a SAT solver and selects the most frequent equivalence group as the verified translation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM used to produce multiple candidate translations; SAT solver used to decide T-equivalence among them and form groups; selection is frequency-based.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>LLM + SAT solver grouping</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Group logically equivalent formulas via SAT solver equivalence tests and pick the formula from the largest equivalence class.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Z3 is used to determine T-equivalence of formula pairs and to filter out T-unsatisfiable / syntactically invalid candidates prior to grouping.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AR-LSAT, ZebraLogic (evaluated in ablations and Table 7)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used to evaluate effect of SAT-based equivalence grouping on translation accuracy and downstream reasoning results.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Final task accuracy after verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per Table 7: Logical consistency yields AR-LSAT 61.9% and ZebraLogic 74.2% (slightly below disproving by counter-interpretation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improves substantially over non-SAT verification baselines (syntax consistency and LLM-with-instruction) and over random selection; slightly underperforms the more advanced disproving algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SAT-based equivalence grouping is an effective way to leverage multiple candidate translations and the deterministic nature of FOL to increase reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Can fail when the LLM produces multiple logically consistent but jointly incorrect translations (consistent mistakes); grouping alone cannot disambiguate such consistent errors, motivating the counter-interpretation step.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6815.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xu-et-al (LLM-as-solver baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approach that uses an LLM to both translate and solve SAT problems (Xu et al., 2024 baseline referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced baseline where an LLM is used not only for semantic parsing but also to solve the SAT problem (i.e., LLM acts as the symbolic solver/solver-and-verifier instead of invoking an external theorem prover).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Faithful logical reasoning via symbolic chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in cited work (referenced as a comparative approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model-in-the-loop approach where the LLM attempts to perform symbolic solving (no external solver); detailed architecture and sizes not specified in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-only (LLM used as solver) per description in paper</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>LLM-based symbolic solving and verification (the LLM performs SAT-solving steps instead of calling a symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Mentioned in comparisons (no dedicated full-table results shown in main tables of this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Referenced as an alternative to neurosymbolic pipelines that rely on external solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation + LLM-led SAT solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not reported in this paper's tables (referenced only as comparative approach)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Mentioned as a different design choice (LLM-as-solver) and contrasted with symbolic-solver-based neurosymbolic pipelines; paper argues such approaches share limitations in translation fidelity or verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper references this approach to note that using an LLM as the symbolic solver has been explored but differs in tradeoffs from invoking a deterministic solver like Z3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As described by the present paper, LLM-as-solver approaches may struggle with strict formal correctness and lack the determinism that a true symbolic solver provides.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6815.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6815.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o-mini and other LLMs (scale comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-mini, gpt-3.5-turbo, gpt-3.5-turbo-instruct (smaller LLM variants used in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller, chat-focused or instruction-following LLM variants that the paper evaluates to probe robustness across model scales; used to reproduce experiments and measure cost/accuracy tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini; gpt-3.5-turbo; gpt-3.5-turbo-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller OpenAI model variants (chat-focused and instruction-following) used to run CLOVER and baseline pipelines for scale comparisons; no parameter counts reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (chat/instruction models)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Same CLOVER compositional translation and verification pipeline or baseline methods applied to smaller LLM variants as an ablation/cost study.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>When used within CLOVER they are paired with Z3 for SAT solving; experiments report that CLOVER outperforms baselines across these smaller LLMs as well (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Puzzle, Symbol (scale comparison experiments reported in Table 5)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Subset of tasks used to evaluate performance across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order logic translation and symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example entries (Table 5): gpt-4o-mini with CLOVER: Puzzle 60.5% ; Symbol 71.7% ; other LLMs (gpt-3.5 variants) show lower performance but CLOVER still outperforms baselines on those models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CLOVER retains advantages over prior neurosymbolic baselines even on smaller LLMs; performance and cost vary with model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CLOVER's compositional approach and SAT-based verification improve translation fidelity across multiple LLM scales; smaller models yield lower absolute performance but maintain relative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance drops with smaller LLMs; some prompts/exemplars exceed context windows for certain smaller/instruction models (gpt-3.5-turbo-instruct example).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Faithful logical reasoning via symbolic chain-of-thought <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Z3: An efficient smt solver <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6815",
    "paper_id": "paper-273233577",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "CLOVER",
            "name_full": "Compositional First-Order Logic Translation and Verification (CLOVER)",
            "brief_description": "A neurosymbolic method that decomposes natural-language sentences into logical dependency structures, incrementally translates subsentences to many-sorted first-order logic via an LLM, and selects/verifies candidates using SAT-based verification (logical consistency and disproving by counter-interpretation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "Applied as the base LLM for parsing, accumulation, sequential translation, and some verification decisions; used in few-shot, zero-temperature sampling to generate candidate logical dependency parses and formula translations.",
            "model_size": null,
            "architecture_type": "transformer (LLM) + neurosymbolic pipeline with SAT solver (Z3)",
            "training_data": null,
            "reasoning_method": "Compositional first-order logic translation via logical dependency parsing, component accumulation, sequential translation; SAT-based verification (logical consistency and disproving by counter-interpretation); final formal reasoning executed by a symbolic solver.",
            "external_tool_used": true,
            "external_tool_description": "Z3 theorem prover is used as the symbolic solver to check T-satisfiability, group logically equivalent formulas, find counter-interpretations for (φ_p ∧ ¬φ_q) and to execute the constructed SAT problems to produce final answers.",
            "benchmark_name": "AR-LSAT, ZebraLogic, Puzzle, Symbol, Deduction, FOLIO, ProofWriter",
            "benchmark_description": "A suite of seven logical reasoning benchmarks comprising many-sorted first-order-logic translation and reasoning tasks: AR-LSAT (analytical reasoning LSAT problems), ZebraLogic (zebra puzzles), Puzzle/Symbol/Deduction (BigBench logical reasoning tasks), FOLIO (expert-written FOL reasoning), ProofWriter (deductive reasoning proofs).",
            "task_type": "First-order logic translation (sentence-level) and SAT-based logical reasoning (query satisfiability/validity)",
            "performance_metric": "Accuracy / program accuracy; execution rate; execution accuracy (for neurosymbolic methods)",
            "performance_value": "Overall task accuracies reported per Table 1: AR-LSAT 62.8% ; ZebraLogic 75.4% ; Puzzle 83.5% ; Symbol 89.9% ; Deduction 99.3% ; FOLIO 78.8% ; ProofWriter 96.7%. Program/execution metrics (Table 2) show large increases in execution rate and execution accuracy versus Logic-LM (examples: AR-LSAT program accuracy improved to 46.8% and execution rate to 83.8% (see Table 2)).",
            "comparison_with_baseline": "Outperforms prior neurosymbolic baseline Logic-LM by +20.4 percentage points on AR-LSAT and +30.0 points on ZebraLogic (explicitly reported); shows higher execution rate and execution accuracy than Logic-LM across tasks.",
            "key_findings": "Compositional translation plus SAT-based verification substantially improves syntactic and semantic fidelity of LLM-generated first-order formulas, yielding state-of-the-art performance across seven logical reasoning benchmarks and largest gains on highest FOL complexity problems.",
            "limitations": "Higher inference/API cost due to multiple compositional parses and sampling; residual errors concentrated in preprocessing and other non-translation issues; LLM still requires few-shot exemplars and deterministic SAT solver calls, and the pipeline increases runtime and API usage.",
            "uuid": "e6815.0",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Logic-LM",
            "name_full": "Logic-LM (prior neurosymbolic baseline)",
            "brief_description": "A neurosymbolic approach that uses an LLM to translate natural-language logical problems into first-order logic (single-shot translation) and invokes a symbolic solver to perform formal reasoning; used as the primary baseline in this paper.",
            "citation_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "Uses an LLM as a semantic parser to produce a single first-order logic translation per sentence which is handed to a symbolic solver for automated reasoning.",
            "model_size": null,
            "architecture_type": "transformer (LLM) + SAT solver (neurosymbolic)",
            "training_data": null,
            "reasoning_method": "Direct single-step first-order logic translation by the LLM followed by symbolic SAT solving",
            "external_tool_used": true,
            "external_tool_description": "Also uses a theorem prover/SAT solver (Z3) to check satisfiability and execute logical reasoning on translated formulas.",
            "benchmark_name": "AR-LSAT, ZebraLogic, Puzzle, Symbol, Deduction, FOLIO, ProofWriter",
            "benchmark_description": "Same set of logical reasoning benchmarks used to evaluate neurosymbolic translation+solver pipelines.",
            "task_type": "First-order logic translation and SAT-based reasoning",
            "performance_metric": "Accuracy / program accuracy; execution rate; execution accuracy",
            "performance_value": "Reported in Table 1 and Table 2: AR-LSAT 42.4% ; ZebraLogic 45.4% ; Puzzle 64.0% ; Symbol 81.8% ; Deduction 95.3% ; FOLIO 75.4% ; ProofWriter 95.3%. (Table 2 shows lower execution rates and execution accuracies versus CLOVER; specific numbers in paper tables.)",
            "comparison_with_baseline": "Used as the main baseline; CLOVER improves AR-LSAT by +20.4 points and ZebraLogic by +30.0 points compared to Logic-LM.",
            "key_findings": "Logic-LM is effective on simpler FOL translation tasks but degrades on more complex first-order structures; many errors attributable to incorrect logic or syntax in translations.",
            "limitations": "Single-step translation fails to capture complex logical structure in sentences (large drop in translation fidelity with increasing FOL complexity); high rate of incorrect logic and syntax errors in translations; lower execution rate (more translations unexecutable by solver).",
            "uuid": "e6815.1",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps from LLMs (few-shot or zero-shot) to improve multi-step problem solving; used as a baseline method in the paper.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "gpt-4o (and other LLMs in baselines)",
            "model_description": "Prompting strategy applied to transformer-based LLMs to produce step-by-step natural language reasoning before an answer.",
            "model_size": null,
            "architecture_type": "transformer (prompting method)",
            "training_data": null,
            "reasoning_method": "Chain-of-thought prompting (in-context elicitation of intermediate steps); sometimes combined with self-consistency in prior work (not applied here).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "AR-LSAT, Puzzle, Symbol, Deduction, FOLIO, ProofWriter (as baseline rows in evaluations)",
            "benchmark_description": "Benchmarks containing multi-step logical reasoning tasks; CoT is reported as a baseline on these.",
            "task_type": "Multi-step logical reasoning (natural-language chain explanations) and multiple-choice answering",
            "performance_metric": "Accuracy",
            "performance_value": "Reported baseline CoT accuracies in Table 1 (examples include AR-LSAT ~36.8% and other task-specific values shown in the paper's baseline rows).",
            "comparison_with_baseline": "CoT improves over naive prompting on short/simple reasoning chains but shows pronounced performance drop on long/complex reasoning chains compared to neurosymbolic approaches; CLOVER and other neurosymbolic methods outperform CoT on hard FOL translation tasks.",
            "key_findings": "CoT helps simple logical reasoning but fails to scale to long sequences of strictly formal first-order reasoning; neurosymbolic methods maintain robustness to reasoning length.",
            "limitations": "Performance declines sharply as reasoning chain length grows; natural-language chain steps are less reliable for strict FOL semantics compared to symbolic solving.",
            "uuid": "e6815.2",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SymbCoT",
            "name_full": "SymbCoT (Symbolic Chain-of-Thought baseline reported in tables)",
            "brief_description": "A baseline combining LLM chain-of-thought style outputs with symbolic verification/execution; included in empirical comparisons (table row labeled SymbCoT).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "An approach that integrates LLM-generated structured reasoning with symbolic execution (as reported in baselines), details in paper's comparison tables.",
            "model_size": null,
            "architecture_type": "transformer + symbolic execution",
            "training_data": null,
            "reasoning_method": "Chain-of-thought-like generation with symbolic checking/execution (baseline configuration)",
            "external_tool_used": true,
            "external_tool_description": "When applicable uses symbolic execution / theorem prover to attempt to execute or verify LLM-generated programs/formulas.",
            "benchmark_name": "AR-LSAT, Puzzle, Symbol (reported in Table 5 and Table 1 as baseline rows)",
            "benchmark_description": "Same logical reasoning benchmarks; SymbCoT entries shown in the comparative tables.",
            "task_type": "First-order logic translation and symbolic execution/verification",
            "performance_metric": "Accuracy",
            "performance_value": "Examples from tables: AR-LSAT ~34.2% ; Puzzle ~66.5% ; Symbol ~55.6% (table values reported in the paper's baseline rows).",
            "comparison_with_baseline": "Per-table results, SymbCoT generally outperforms naive prompting on some tasks but underperforms CLOVER and Logic-LM on several benchmarks.",
            "key_findings": "Integrating symbolic execution with CoT offers gains over pure prompting on some tasks but still falls short of compositional translation + SAT-verification on complex FOL.",
            "limitations": "Reported performance varies by task; detailed method description not provided in the paper (used as a comparative baseline).",
            "uuid": "e6815.3",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Disproving by Counter-Interpretation",
            "name_full": "Disproving by Counter-Interpretation (SAT-based verification algorithm)",
            "brief_description": "A verification algorithm that uses a SAT solver to find counter-interpretations to (φ_p ∧ ¬φ_q) and then queries an LLM to decide whether the interpretation satisfies the target natural-language sentence; sequentially disproves incorrect formulas to select the correct translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "LLM component is only used to judge whether a SAT-solver-produced interpretation satisfies the original NL sentence (boolean decision); the heavy logical checks are performed by the SAT solver.",
            "model_size": null,
            "architecture_type": "LLM (decision oracle) + SAT solver (Z3) for counter-interpretation generation",
            "training_data": null,
            "reasoning_method": "SAT-based counterexample generation plus LLM interpretability decision; algorithm iteratively compares formula pairs and disproves one based on counter-interpretations.",
            "external_tool_used": true,
            "external_tool_description": "Z3 is used to test satisfiability of (φ_p ∧ ¬φ_q) and to provide a T-interpretation I; the LLM is then prompted (few-shot) to decide if I satisfies the NL sentence.",
            "benchmark_name": "AR-LSAT, ZebraLogic (used in verification ablations and Table 7 comparisons)",
            "benchmark_description": "Datasets used to measure effect of verification strategies on translation correctness and downstream reasoning accuracy.",
            "task_type": "First-order logic translation verification (selecting correct formula among candidates)",
            "performance_metric": "Final task accuracy after verification (accuracy, program accuracy); improvement over other verification approaches",
            "performance_value": "Per Table 7: Disproving by counter-interpretation yields AR-LSAT 62.8% and ZebraLogic 75.4% (best reported verification results), compared to logical consistency 61.9% / 74.2% and other baselines.",
            "comparison_with_baseline": "Produces small but consistent gains over logical-consistency-only verification (e.g., +0.9 points on AR-LSAT in Table 7) and outperforms non-SAT verification baselines (syntax consistency, LLM w/ instruction).",
            "key_findings": "Combining deterministic SAT-based counterexample generation with an LLM's semantic judgment reduces consistent logical mistakes and improves selection of correct formulas.",
            "limitations": "Requires additional SAT solver calls (increased cost); relies on LLM accuracy in judging whether a concrete interpretation satisfies the NL sentence (LLM decisions can still err); overall computational cost higher than simpler verification heuristics.",
            "uuid": "e6815.4",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Logical Consistency (verification)",
            "name_full": "Logical Consistency (SAT-based grouping verification)",
            "brief_description": "A verification algorithm that groups T-equivalent (logically equivalent under the theory) candidate formulas using a SAT solver and selects the most frequent equivalence group as the verified translation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o",
            "model_description": "LLM used to produce multiple candidate translations; SAT solver used to decide T-equivalence among them and form groups; selection is frequency-based.",
            "model_size": null,
            "architecture_type": "LLM + SAT solver grouping",
            "training_data": null,
            "reasoning_method": "Group logically equivalent formulas via SAT solver equivalence tests and pick the formula from the largest equivalence class.",
            "external_tool_used": true,
            "external_tool_description": "Z3 is used to determine T-equivalence of formula pairs and to filter out T-unsatisfiable / syntactically invalid candidates prior to grouping.",
            "benchmark_name": "AR-LSAT, ZebraLogic (evaluated in ablations and Table 7)",
            "benchmark_description": "Used to evaluate effect of SAT-based equivalence grouping on translation accuracy and downstream reasoning results.",
            "task_type": "First-order logic translation verification",
            "performance_metric": "Final task accuracy after verification",
            "performance_value": "Per Table 7: Logical consistency yields AR-LSAT 61.9% and ZebraLogic 74.2% (slightly below disproving by counter-interpretation).",
            "comparison_with_baseline": "Improves substantially over non-SAT verification baselines (syntax consistency and LLM-with-instruction) and over random selection; slightly underperforms the more advanced disproving algorithm.",
            "key_findings": "SAT-based equivalence grouping is an effective way to leverage multiple candidate translations and the deterministic nature of FOL to increase reliability.",
            "limitations": "Can fail when the LLM produces multiple logically consistent but jointly incorrect translations (consistent mistakes); grouping alone cannot disambiguate such consistent errors, motivating the counter-interpretation step.",
            "uuid": "e6815.5",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Xu-et-al (LLM-as-solver baseline)",
            "name_full": "Approach that uses an LLM to both translate and solve SAT problems (Xu et al., 2024 baseline referenced)",
            "brief_description": "A referenced baseline where an LLM is used not only for semantic parsing but also to solve the SAT problem (i.e., LLM acts as the symbolic solver/solver-and-verifier instead of invoking an external theorem prover).",
            "citation_title": "Faithful logical reasoning via symbolic chain-of-thought",
            "mention_or_use": "mention",
            "model_name": "various LLMs in cited work (referenced as a comparative approach)",
            "model_description": "Model-in-the-loop approach where the LLM attempts to perform symbolic solving (no external solver); detailed architecture and sizes not specified in the present paper.",
            "model_size": null,
            "architecture_type": "transformer-only (LLM used as solver) per description in paper",
            "training_data": null,
            "reasoning_method": "LLM-based symbolic solving and verification (the LLM performs SAT-solving steps instead of calling a symbolic solver)",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Mentioned in comparisons (no dedicated full-table results shown in main tables of this paper)",
            "benchmark_description": "Referenced as an alternative to neurosymbolic pipelines that rely on external solvers.",
            "task_type": "First-order logic translation + LLM-led SAT solving",
            "performance_metric": "Not reported in this paper's tables (referenced only as comparative approach)",
            "performance_value": null,
            "comparison_with_baseline": "Mentioned as a different design choice (LLM-as-solver) and contrasted with symbolic-solver-based neurosymbolic pipelines; paper argues such approaches share limitations in translation fidelity or verification.",
            "key_findings": "Paper references this approach to note that using an LLM as the symbolic solver has been explored but differs in tradeoffs from invoking a deterministic solver like Z3.",
            "limitations": "As described by the present paper, LLM-as-solver approaches may struggle with strict formal correctness and lack the determinism that a true symbolic solver provides.",
            "uuid": "e6815.6",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "gpt-4o-mini and other LLMs (scale comparison)",
            "name_full": "gpt-4o-mini, gpt-3.5-turbo, gpt-3.5-turbo-instruct (smaller LLM variants used in ablations)",
            "brief_description": "Smaller, chat-focused or instruction-following LLM variants that the paper evaluates to probe robustness across model scales; used to reproduce experiments and measure cost/accuracy tradeoffs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini; gpt-3.5-turbo; gpt-3.5-turbo-instruct",
            "model_description": "Smaller OpenAI model variants (chat-focused and instruction-following) used to run CLOVER and baseline pipelines for scale comparisons; no parameter counts reported in the paper.",
            "model_size": null,
            "architecture_type": "transformer (chat/instruction models)",
            "training_data": null,
            "reasoning_method": "Same CLOVER compositional translation and verification pipeline or baseline methods applied to smaller LLM variants as an ablation/cost study.",
            "external_tool_used": true,
            "external_tool_description": "When used within CLOVER they are paired with Z3 for SAT solving; experiments report that CLOVER outperforms baselines across these smaller LLMs as well (Table 5).",
            "benchmark_name": "Puzzle, Symbol (scale comparison experiments reported in Table 5)",
            "benchmark_description": "Subset of tasks used to evaluate performance across model sizes.",
            "task_type": "First-order logic translation and symbolic reasoning",
            "performance_metric": "Accuracy",
            "performance_value": "Example entries (Table 5): gpt-4o-mini with CLOVER: Puzzle 60.5% ; Symbol 71.7% ; other LLMs (gpt-3.5 variants) show lower performance but CLOVER still outperforms baselines on those models.",
            "comparison_with_baseline": "CLOVER retains advantages over prior neurosymbolic baselines even on smaller LLMs; performance and cost vary with model scale.",
            "key_findings": "CLOVER's compositional approach and SAT-based verification improve translation fidelity across multiple LLM scales; smaller models yield lower absolute performance but maintain relative gains.",
            "limitations": "Performance drops with smaller LLMs; some prompts/exemplars exceed context windows for certain smaller/instruction models (gpt-3.5-turbo-instruct example).",
            "uuid": "e6815.7",
            "source_info": {
                "paper_title": "Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Faithful logical reasoning via symbolic chain-of-thought",
            "rating": 2,
            "sanitized_title": "faithful_logical_reasoning_via_symbolic_chainofthought"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Z3: An efficient smt solver",
            "rating": 1,
            "sanitized_title": "z3_an_efficient_smt_solver"
        }
    ],
    "cost": 0.01957425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DIVIDE AND TRANSLATE: COMPOSITIONAL FIRST-ORDER LOGIC TRANSLATION AND VERIFICATION FOR COMPLEX LOGICAL REASONING
25 Feb 2025</p>
<p>Hyun Ryu 
Gyeongman Kim gmkim@kaist.ac.kr 
Hyemin S Lee 
Eunho Yang eunhoy@kaist.ac.kr 
Mit 
DIVIDE AND TRANSLATE: COMPOSITIONAL FIRST-ORDER LOGIC TRANSLATION AND VERIFICATION FOR COMPLEX LOGICAL REASONING
25 Feb 202563EE6E91AC89A0932CC99DAEF562C567arXiv:2410.08047v2[cs.CL]
Complex logical reasoning tasks require a long sequence of reasoning, which a large language model (LLM) with chain-of-thought prompting still falls short.To alleviate this issue, neurosymbolic approaches incorporate a symbolic solver.Specifically, an LLM only translates a natural language problem into a satisfiability (SAT) problem that consists of first-order logic formulas, and a sound symbolic solver returns a mathematically correct solution.However, we discover that LLMs have difficulties to capture complex logical semantics hidden in the natural language during translation.To resolve this limitation, we propose a Compositional First-Order Logic Translation.An LLM first parses a natural language sentence into newly defined logical dependency structures that consist of an atomic subsentence and its dependents, then sequentially translate the parsed subsentences.Since multiple logical dependency structures and sequential translations are possible for a single sentence, we also introduce two Verification algorithms to ensure more reliable results.We utilize an SAT solver to rigorously compare semantics of generated first-order logic formulas and select the most probable one.We evaluate the proposed method, dubbed CLOVER, on seven logical reasoning benchmarks and show that it outperforms the previous neurosymbolic approaches and achieves new state-of-the-art results. 1</p>
<p>INTRODUCTION</p>
<p>Logical reasoning involves reaching conclusions through a structured process.It entails drawing inferences by converting information provided in a set of premises into a final conclusion (Nunes, 2012;Bronkhorst et al., 2020).Logical reasoning ability is one of the most challenging metrics to measure intelligence.As a model size grows exponentially, large language models (LLMs) (Brown et al., 2020;Chen et al., 2021;Thoppilan et al., 2022) unlock the ability of machine to reason.</p>
<p>Chain-of-thought (CoT) prompting (Wei et al., 2022) significantly improve the performance of LLMs on simple logical reasoning tasks that require few forward reasoning steps.However, CoT falls short in complex logical reasoning tasks which need longer sequence of reasoning (Ye et al., 2024;Pan et al., 2023).To resolve this issue, several neurosymbolic approaches (Ye et al., 2024;Pan et al., 2023;Kirtania et al., 2024;Olausson et al., 2023) utilize an LLM with a symbolic solver (e.g., an SAT solver) on these complex logical reasoning tasks by the following two steps: 1) an LLM translates the natural language logical reasoning problem into a set of first-order logic formulas, 2) a symbolic solver automatically plans the reasoning steps and executes those to predict an answer of the logical reasoning problem.These approaches take advantages by considering an LLM only as a semantic parser (i.e., a first-order logic translator), which can avoid planning and execution errors by using a symbolic solver.</p>
<p>However, we have discovered that LLMs still cannot translate sentences that represent complex first-order logic.Our experimental evidence in Fig. 1a presents the drastic performance drop of the previous work (Pan et al., 2023) on complex first-order logic translation. 2The result indicates that an Published as a conference paper at ICLR 2025 1) The Sales division is toured.</p>
<p>2) The Sales division is toured on two consecutive days.</p>
<p>3) The Sales division is toured on two consecutive days, and on no other days.</p>
<ol>
<li>Divide</li>
</ol>
<p>Translate</p>
<p>Figure 1: Comparison of a first-order logic translation of the proposed CLOVER and Logic-LM with gpt-4o on AR-LSAT.(a) Translation accuracy at different levels of first-order logic complexity.We group the formulas in one of five complexity ranges and report the averaged performance for each range.(b) A representative example.Given declarations of sorts and functions, each method translates the natural language sentence into the corresponding first-order logic formula.We colorize incorrect translation as red for visualization purpose.</p>
<p>LLM performs first-order logic translation faithfully to a certain degree of complexity but falls short beyond that limit.A representative example in Fig. 1b presents an incorrect output of the previous work (Pan et al., 2023) on complex first-order logic translation.The task is to translate a sentence "The Sales division is toured on two consecutive days, and on no other days."into a corresponding first-order logic formula given declarations.An LLM correctly translates a natural language clause "The Sales division is toured on two consecutive days."into a subformula (∃d : days) (toured(d) = Sales) ∧ (toured(d + 1) = Sales) which contains simple logic, but fails to translate "and on no other days" which represents more complex logic.Specifically, the incorrectly translated subformula is always true, which has no semantic meaning.After further extensive qualitative error analysis (Appendix K), we conclude that LLMs show promising performance on simple first-order logic translations but does not on complex ones, and the reason is that LLMs have difficulties to discover complex logical structures hidden behind the natural language.</p>
<p>To resolve this limitation, we take a hint from how humans perceive a complex logical sentence to their mind and how they translate it to a first-order logic formula.Since it is hard to immediately comprehend the semantics of a complex logical sentence, humans first understand the semantics of a simpler subsentence and then understand the whole (Montague et al., 1970;Frazier &amp; Fodor, 1978;Sweller, 1988).Inspired by this observation, we use LLM to find the atomic subsentence that does not contain any complex logic and understand other sentence components as dependents of the atomic subsentence.Then, starting with the atomic subsentence, we use LLM to translate subsentences by accumulating sentence components.This could help LLM to preserve first-order logic semantics during translation.To rigorously define the atomic subsentence and sentence components with logical meaning, we introduce a new parsing method for natural language that represents first-order logic, called logical dependency parsing (Section 3.1).</p>
<p>Based on logical dependency parsing, we propose a compositional first-order logic translation (Section 3.2) by few-shot learning with an LLM.It consists of the following three steps: logical dependency parsing, component accumulation, and sequential translation.First, a target sentence is parsed into logical dependency structures which consist of components of the sentence and their logical dependencies.Second, components are accumulated while preserving their logical dependencies, where the last accumulated sentence is the target sentence.Finally, each accumulated sentence is sequentially translated into first-order logic formula in the order of accumulations, where the last formula is an estimated formula of the target sentence.</p>
<p>Not only that, since there could be multiple outputs on logical dependency parsing and sequential translation, we introduce verification algorithms to ensure more reliable first-order logic translaground truth formulas.Detailed information of measuring complexity is in Appendix B and the process of the annotated subset construction is described in Appendix F.</p>
<p>tion (Section 3.3).We propose two verification algorithms: logical consistency and disproving by counter-interpretation.To fully leverage the deterministic nature of first-order logic, we use an SAT solver to compare any two formulas.Logical consistency selects the most frequent logically equivalent formulas.However, we observe that an LLM sometimes make logically consistent translation errors.To overcome such a limitation, we devise disproving by counter-interpretation.It sequentially compares two formulas and disprove one of them by determining if a counter-interpretation to equivalence of two formulas satisfies the target sentence.The last formula remained is then selected.</p>
<p>To save computational cost, we compare each one of logically equivalent formulas.</p>
<p>We evaluate the proposed CLOVER, a Compositional First-Order Logic Translation and Verification, on seven logical reasoning benchmarks (Section 4).CLOVER outperforms the previous neurosymbolic approaches and achieves the new state-of-the-art performance.It also significantly enhances the first-order logic translation accuracy across all levels of complexity and the largest performance gain occurs at the highest first-order logic complexity (Fig. 1a).</p>
<p>To summarize our contributions, 1.We introduce CLOVER, a novel neurosymbolic approach that enhances complex logical reasoning in LLMs by compositional translation of natural language into first-order logic and verification of logical semantics.2. We newly define a logical dependency structure to decompose logical sentences while preserving an underlying first-order logic semantics.3. We also propose two SAT-based first-order logic verification algorithms that can faithfully select a correctly translated formula.4. We evaluate CLOVER on seven logical reasoning benchmarks and show that CLOVER outperforms the previous neurosymbolic apporoaches and achieves the new state-of-the-art performance.</p>
<p>PROBLEM FORMULATION</p>
<p>Through the lens of (many-sorted) first-order logic3 , a logical reasoning problem x is a natural language description of a Σ-theory T4 , constraints Φ, and a query q, denoted as x = N L(T , Φ, q).A Σ-theory T is a non-empty set of any Σ-structure where a signature Σ = (S, F, P ) consists of sorts S, function symbols F , and predicate symbols P .Hereinafter, we denote the vocabulary of first-order logic as italic for clarity, and omit the prefix "Σ-" for simplicity.A structure of a theory indicates the semantics of formulas.Constraints Φ are a set of formulas that are true, denoted as
Φ = {ϕ 1 , ϕ 2 , • • • , ϕ K }.
A query q is also a formula which is yet determined as true, false, or unknown given the constraints Φ.</p>
<p>Prior works.Prior neurosymbolic approaches (Ye et al., 2024;Pan et al., 2023;Kirtania et al., 2024;Olausson et al., 2023) directly translate the logical reasoning problem x into a set of firstorder logic using an LLM and then employ a symbolic solver (e.g., an SAT solver) to solve an SAT problem.In these methods, an LLM performs a single inference for the first-order logic translation as follows:
T , { φk , N L(φ k )} K+1 k=1 ∼ P LLM (T , {φ k , N L(φ k )} K+1 k=1 | x, x fs )
(1) where φ k = ϕ k for 1 ≤ k ≤ K and φ K+1 = q, and a few-shot exemplar set x fs = {x (i) , T (i) , {φ
(i) k , N L(φ (i) k )} K (i) +1 k=1 } N
i=1 with the size of the set N .However, it often generates more than one formulas for a single target sentence or generates a formula which is a translation of combination of a target sentence and part of other sentences.Though it might be logically correct as a whole, we cannot further analyze and verify the translation at a sentence-level.</p>
<p>First-order logic translation.To resolve this drawback, we perform first-order logic translation for each sentence.Since sentence-level translations require a pre-defined theory and target sentences, we first generate a theory T and a set of natural language sentences { N L(φ k )} K+1 k=1 from</p>
<p>x, and then generate T -satisfiable formula φk for each sentence N L(φ k ).To be specific, the single inference by the LLM in Eq. 1 is separated into the following two steps: 1) given a logical reasoning problem x and a few-shot exemplar set x prep fs = {x (i) , T (i) , {N L(φ
(i) k )} K (i) +1 k=1 } N i=1
, the LLM generates a tuple of an estimated theory and a set of natural language sentences, denoted
x prep = ( T , { N L(φ k )} K+1 k=1 ), 2)
given the theory T and a set of few-shot exemplar sets X fs , the proposed CLOVER translates each natural language sentence N L(φ k ) into the estimated formula φk that is T -satisfiable as follows:
T , { N L(φ k )} K+1 k=1 ∼ P LLM (T , {N L(φ k )} K+1 k=1 | x, x prep fs ) φk = CLOVER( T , N L(φ k ), X fs ), ∀k ∈ {1, 2, • • • , K + 1}.
(2)</p>
<p>A detailed description of the set of few-shot exemplar sets X fs = {x parse fs , x accum fs , x trans fs , x disprv fs } and the proposed CLOVER for x prep = ( T , { N L(φ k )} K+1 k=1 ) will be discussed in the following section.</p>
<p>SAT problem solving.Once estimations of the theory T , constraints Φ = { φ1 , φ2 , • • • , φK }, and a query q = φK+1 are completed for the logical reasoning problem x, these form an SAT problem P = ( T , Φ, q).An automated SAT solver then determines the T -satisfiability5 of the query q under the constraints Φ, which is a final prediction of an answer of the logical reasoning problem x.We use a Z3 theorem prover (De Moura &amp; Bjørner, 2008) as an SAT solver in the implementation.</p>
<p>CLOVER</p>
<p>In this section, we propose CLOVER, a Compositional First-Order Logic Translation and Verification for complex logical reasoning.To fully capture first-order logic semantics in natural language, it first parses a single natural language sentence into logical dependency structures.Then, it sequentially translates parsed subsentences with an LLM.Since there are multiple ways to parse and translate the sentences, we also introduce two SAT-based verification algorithms to thoroughly compare semantics of translated first-order logic formulas.</p>
<p>LOGICAL DEPENDENCY STRUCTURES</p>
<p>Logical dependency structure A of a sentence N L(φ) under the theory T where φ is T -satisfiable is defined by components and their logical dependencies.First, components are natural language building blocks of logical dependency structures of a sentence, which consist of logic units U , logic couplers C, and logic dependents D. The following definitions formally describe each of them.Definition 1 (Logic units).Given a sentence N L(φ) and a theory T where φ is T -satisfiable, logic units U are the natural language descriptions of an atom of φ.Definition 2 (Logic couplers).Logic couplers C are either conjunctions or an operator named merge.Merge combines two logic units which contain the natural language describing the same term without adding any conjunction.Definition 3 (Logic dependents).Logic dependents D are components neither logic units nor logic couplers which logically depend on another component.</p>
<p>Second, we define logical dependency between two components, and the following definition formally describes it.Definition 4 (Logical dependency).The component X is said to logically depend on the component Y in the given sentence if and only if the meaning of Y is (or includes) a predicate and the meaning of X is an argument of this predicate in the sentence.</p>
<p>We also introduce properties of logical dependency structure stemmed from its definition.Remark 1.A given sentence and theory can have multiple logical dependency structures.Remark 2. All components except for one should logically depend on another component.Remark 3. No logic dependent logically depends on a logic coupler.</p>
<p>We present examples of logical dependency structures in Fig. 2 and in Appendix D.</p>
<p>COMPOSITIONAL FIRST-ORDER LOGIC TRANSLATION</p>
<p>To compositionally translate natural language sentences to first-order logic formulas under given theory for
x prep = ( T , { N L(φ k )} K+1 k=1
), we adhere to the following three steps by few-shot learning with an LLM.We describe the following steps for a single target sentence N L(φ) of a formula φ ∈ {φ k } K+1 k=1 .</p>
<p>Logical Dependency Parsing.In the first step, a target sentence is parsed into different possible logical dependency structures.An LLM is given a definition of logical dependency structures (Section 3.1), a target sentence N L(φ) and its theory T , and a few-shot exemplar set
x parse fs = {T (i) , N L(φ (i) ), {A (i) l } L (i) l=1 } N i=1
for logical dependency parsing.L (i) is a size of a set of different possible logical dependency structures of a sentence N L(φ (i) ) under the theory T (i) , and N is a size of the few-shot exemplar set.Then, LLM generates a set of different possible logical dependency structures { Âl } L l=1 of the target sentence with the size of the set L as follows:
L, { Âl } L l=1 ∼ P LLM (L, {A l } L l=1 | T , N L(φ), x parse fs ).(3)
Component Accumulation.In the second step, components of a logical dependency structure are accumulated to gradually compose new sentences until those reach the target sentence.We present the rules for component accumulation in Appendix C.An LLM is given a definition of logical dependency structures (Section 3.1), rules for component accumulation (Appendix C), a target sentence N L(φ) and one of its logical dependency structures Âl where l ∈ {1, 2, • • • , L}, and a few-shot exemplar set
x accum fs = {N L(φ (i) ), A (i) , (S (i) m ) M (i) m=1 } N i=1 for component accumulation. (S (i) m ) M (i)
m=1 is a sequence of accumulated sentences where M (i) is the length of the sequence.Then, LLM generates a sequence of sentences ( Ŝl,m ) Ml m=1 where Ml is the length of the estimated sequence as follows:
Ml , ( Ŝl,m ) Ml m=1 ∼ P LLM (M l , (S l,m ) M l m=1 | N L(φ), Âl , x accum fs ), ∀l ∈ {1, 2, • • • , L}.(4)
The last sentence of accumulation Ŝl, Ml is the target sentence N L(φ).We present examples of component accumulation in Appendix D.</p>
<p>Sequential Translation.In the last step, accumulated natural language sentences are sequentially translated into first-order logic formulas, which the target sentence is finally translated.An LLM is given a sequence of accumulated sentences ( Ŝl,m ) Ml m=1 of a target sentence where l ∈ {1, 2, • • • , L}, a theory T , and a few-shot exemplar set x trans fs = {T (i) , (S
(i) m ) M (i) m=1 , (φ (i) m ) M (i) m=1 } N i=1
for first-order logic translation.Then, LLM generates a sequence of formulas ( φl,m ) Ml m=1 as follows:
( φl,m ) Ml m=1 ∼ P LLM ((φ l,m ) Ml m=1 | T , ( Ŝl,m ) Ml m=1 , x trans fs ), ∀l ∈ {1, 2, • • • , L}.(5)
The last formula of the sequence is the first-order logic translation of the target sentence (i.e., φl = φl, Ml ).For ∀l ∈ {1, 2, • • • , L}, we could generate a set of estimated formulas Ψ = { φl } L l=1 for a target sentence N L(φ).In practice, we randomly sample multiple times to enrich the pool of estimated formulas that benefits the second stage of CLOVER, first-order logic verification.</p>
<p>FIRST-ORDER LOGIC VERIFICATION</p>
<p>To select the most probable formula in a set of compositionally translated first-order logic formulas Ψ, we introduce the following two algorithms using an SAT solver (and few-shot learning with an LLM).As in Section 3.2, we describe the following algorithms for a single target sentence N L(φ) under the theory T (i.e., The algorithms select a verified formula φ * in a set of estimated formulas Ψ).Prior to describing the detailed algorithms, we filter out the formulas that are syntactically incorrect or T -unsatisfiable in Ψ using an SAT solver and call the processed set Ψsat .</p>
<p>Logical Consistency.We select the most frequent logically equivalent formulas, which we call this algorithm logical consistency.It presumes an LLM utilizes different logical dependency structures to generate several formulas that are logically equivalent.An LLM might also make mistake in intermediate steps of compositional first-order logic translation and generate incorrect formulas, but these are less likely to be logically equivalent.For each pair of formulas (φ p , φ q ) such that φ p ∈ Ψsat , φ q ∈ Ψsat , and p ̸ = q, an SAT solver determines their T -equivalence.Then, we group T -equivalent formulas and select any formula in the group that has the largest number of elements.However, we observe that an LLM sometimes makes consistent mistakes in the last step of compositional first-order logic translation, which leads to logically equivalent incorrect formulas.</p>
<p>Disproving by Counter-Interpretation.To resolve this issue, we introduce an advanced algorithm that sequentially disproves incorrect formulas by counter-interpretation.Following this al-Algorithm 1 First-Order Logic Verification (Disproving by Counter-Interpretation) Input: Theory T , a natural language sentence N L(φ) of a first-order logic formula φ, and a set of estimated T -satisfiable formulas Ψsat Output:
Verified formula φ * φ0 ∼ Ψsat ▷ Select an element φ0 in Ψsat randomly φ * ← φ0 ▷ Initialize φ * to a random element φ0 for each φ ∈ Ψsat \ { φ0 } do φ temp ← φ * ▷ Use a temporary variable φ temp for the update for each (φ p , φ q ) ∈ {(φ * , φ), ( φ, φ * )} do if (φ p ∧ ¬φ q ) is T -satisfiable then find T -interpretation I such that I ⊨ (φ p ∧ ¬φ q ) ▷ SAT solver finds I if it exists ê ∼ P LLM (e | N L(φ), I, x disprv f s ), e ∈ {⊤, ⊥} ▷ LLM determines if I ⊨ φ if (φ p = φ * ∧ ¬ê) or (φ p = φ ∧ ê) then φ temp ← φ end if end if end for φ * ← φ temp
▷ Update φ * after checking T -interpretations from both side end for return φ * gorithm, an accurate formula remains the last if it exists in Ψsat .Specifically, we select a random element φ0 in Ψsat and initialize the verified formula φ * to φ0 .For each estimated formula
φ in Ψsat \ { φ0 }, an SAT solver determines if (φ * ∧ ¬ φ) is T -satisfiable. First, if it is T - satisfiable,
an SAT solver finds a counter-interpretation I to a T -equivalence of φ * and φ that satisfies (φ * ∧ ¬ φ).Given a target sentence N L(φ), a counter-interpretation I, and a few-shot exemplar set x disprv fs = {N L(φ) (i) , I (i) , e (i) } N i=1 for disproving, an LLM decides if I satisfies φ, which returns a boolean value ê.If ê is True, then φ is disproved since I does not satisfy φ but satisfies φ.
If ê is False, then φ * is disproved since I satisfies φ * but does not satisfy φ. Second, if (φ * ∧ ¬ φ) is T -unsatisfiable, it is equivalent to (φ * → φ) is T -satisfiable,
and no I exists.After repeating this decision process for ( φ ∧ ¬φ * ), we can consider a counter-interpretation I that satisfies ( φ ∧ ¬φ * ) and disproves accordingly.We select the verified formula φ * that remains the last.Algorithm 1 summarizes the whole process.</p>
<p>EXPERIMENTS</p>
<p>SETUP</p>
<p>Tasks.We evaluate CLOVER on seven logical reasoning tasks: AR-LSAT (Zhong et al., 2022), ZebraLogic (Lin et al., 2025), Logic grid puzzle (Puzzle), Symbol interpretation (Symbol), and Logical deduction (Deduction) from the BigBench collaborative benchmark (Srivastava et al., 2022), FOLIO (Han et al., 2022), and ProofWriter (Tafjord et al., 2021).AR-LSAT consists of analytical reasoning problems of the law school admission test, and ZebraLogic is a benchmark for zebra puzzles.Puzzle, Symbol, and Deduction are tasks from logical reasoning category in the BigBench.FOLIO6 is an expert-written first-order logic reasoning task, and ProofWriter is a deductive reasoning benchmark.Note that all tasks except ZebraLogic are multiple choice problems, and Appendix F describes details of each task.</p>
<p>Language Models.We perform our experiments mainly on gpt-4o (Achiam et al., 2023), a current state-of-the-art LLM for complex, multi-step tasks, unless stated.We also evaluate CLOVER and the baselines using a smaller model, gpt-4o-mini (Achiam et al., 2023). 7To reproduce our experiments, we set the temperature to 0 and select the highest probability response from the model.</p>
<p>Baselines.We compare CLOVER primarily to Logic-LM (Pan et al., 2023), a state-of-the-art neurosymbolic approach for logical reasoning.There are few more works (Ye et al., 2024;Olausson et al., 2023) nearly the same to Logic-LM, but we focus on Logic-LM since their difference is marginal.We also compare CLOVER to another neurosymbolic approach (Xu et al., 2024) which uses an LLM to solve SAT problems instead of using a symbolic solver.In addition, we compare to the standard prompting and CoT prompting that leverages in-context learning capability of the base LLMs.For fair comparison, we manually sample or derive our few-shot exemplar sets from those in the previous works (Pan et al., 2023;Xu et al., 2024) if it is possible.Since the previous works do not evaluate their models on ZebraLogic, Puzzle, and Symbol, we randomly select a single exemplar problem outside the test set.We demonstrate exemplar few-shot prompts in Appendix J.</p>
<p>Evaluation metrics.We measure the performance of CLOVER and the baselines primarily by the correctness of logical reasoning problems.For neurosymbolic approaches with a symbolic solver, if the solver cannot execute the translated SAT problem, we fall back to CoT predictions.From this unique property, following Pan et al. (2023), we use three additional evaluation metrics: program accuracy, execution rate, and execution accuracy, for multiple choice problems.Program accuracy does not include the CoT predictions for unexecutable problems.Execution rate measures the portion of executable problems, and execution accuracy indicates the accuracy for executable problems.</p>
<p>RESULTS</p>
<p>We present the performance of CLOVER and the baselines on different tasks, different evaluation metrics, and different language model scales.First, Table 1 compares the performance of CLOVER and the baselines on seven logical reasoning tasks.CLOVER outperforms Logic-LM and other baselines by a significant margin across different logical reasoning tasks.CLOVER shows marked improvement on hard logical reasoning tasks.Specifically, it enhances the performance of Logic-LM on AR-LSAT by 20.4% and ZebraLogic by 30.0%.Overall, neurosymbolic approaches with a symbolic solver (CLOVER and Logic-LM) show remarkable improvement on these hard reasoning tasks.The inference time costs of CLOVER and the baselines are reported in Appendix H.</p>
<p>ABLATIONS</p>
<p>We conduct ablation studies of CLOVER on two perspectives: compositional translation and verification, in Table 3. Ablating verification from CLOVER (i.e., random selection) shows 6.9% and 4.2% performance degradation on AR-LSAT and ZebraLogic, respectively.It clearly supports the effectiveness of the verification.Ablating compositional translation from CLOVER (i.e., direct translation) shows 7.3% and 14.6% performance degradation on AR-LSAT and ZebraLogic, respectively.It also clearly supports the effectiveness of the compositional translation.To maintain the verification stage as is, we repeat the sampling of direct translation five times, which is slightly larger than the average number of estimated formulas of CLOVER.Ablating both compositional translation and verification from CLOVER shows further performance loss.Additionally, disproving by counter-interpretation yields better performance than logical consistency.</p>
<p>ANALYSIS</p>
<p>Types of Errors.We analyze error types of CLOVER on AR-LSAT and compare those to Logic-LM's in Figure 3. Since an SAT solver is sound and does not cause any error, our error analysis focuses on the first-order logic translation.Logic-LM's errors are mainly caused by incorrect logic (or semantic) and incorrect syntax, which take 53.7% of the total errors.There are preprocessing errors and other errors caused by an incorrect selection of a satisfiability function and limited expressiveness of a Z3 theorem prover.In contrast, we highlight that CLOVER has nearly no logic or syntax error.CLOVER's errors are primarily caused by preprocessing and other errors, which takes 78.6% of the total errors.This analysis indicates that CLOVER significantly enhances the ability of a language model to generate both syntactically and semantically precise first-order logic formulas.</p>
<p>Robustness on Reasoning Length.We present robustness of CLOVER on long sequence of reasoning and compare the results with CoT-based reasoning LLMs (Jaech et al., 2024) 8 in Figure 4. We also add the results of Logic-LM to measure the effect of neurosymbolic approach on reasoning length.We observe a noticeable performance drop of CoT-based reasoning LLMs on long sequence of reasoning, which is a frequently pointed-out drawback of the CoT-based approaches.However, neurosymbolic approaches show robustness to the reasoning length.Specifically, CLOVER shows only 12.5% performance drop between the tasks of the shortest and longest sequence of reasoning.</p>
<p>RELATED WORKS</p>
<p>LLM-based neurosymbolic approach for reasoning.Previous works (Ye et al., 2024;Pan et al., 2023;Olausson et al., 2023;Kirtania et al., 2024) utilize an LLM as a semantic parser which translates the natural language logical reasoning problems into first-order logic formulas, and then use a symbolic solver to automatically solve an SAT problem.There is another work (Xu et al., 2024) that utilizes an LLM as not only a semantic parser but also a symbolic solver and a verifier for semantic parsing and symbolic solving.However, these previous works share a common drawback that an LLM cannot faithfully perform complex first-order logic translation, which fundamentally limits the performance of neurosymbolic approaches on complex logical reasoning tasks.</p>
<p>LLM-based problem decomposition.</p>
<p>To solve natural language tasks, previous works explore decomposing complex problems into several simpler ones using LLMs.Drozdov et al. (2022) use an LLM to syntactically parse the natural language sentence into several subsentences and performs compositional semantic parsing for simple tasks such as text-to-SQL.However, since a syntactic parsing cannot preserve the semantic of logic, Drozdov et al. (2022) is not applicable to complex logical reasoning tasks.Other works (Zhou et al., 2023;Khot et al., 2023;Press et al., 2023;Dua et al., 2022;Ye et al., 2023) focus on decomposing simple question-answering problems by prompting LLMs with few-shot examples.However, if LLMs simply rely on few-shot examples for decomposing complex logical reasoning problems, then the problems might be incorrectly decomposed, which leads to an unexpected performance loss.</p>
<p>LLM-generated formal language verification.There are lines of works to verify formal language generated by LLMs.Chen et al. (2024) and Madaan et al. (2024) first generate a code from natural language, get feedback from an LLM, and refine the code based on the feedback.Chen et al. (2024) additionally utilizes an external feedback signal from an executor.Ni et al. (2023) first generates candidate codes from natural language and then verify by predicting their correctness using a trained neural network.However, these model-based verifications show limited performance on complex logical reasoning tasks (Appendix I).</p>
<p>CONCLUSION</p>
<p>We propose CLOVER, a compositional first-order logic translation and verification for complex logical reasoning.CLOVER first parses the natural language sentence into newly defined logical dependency structures, which reflect first-order logic semantics hidden in the natural language, and then compositionally translates the sentence.We also introduce two verification algorithms using satisfiability to fully cover first-order logic semantics.Empirical results show that CLOVER achieves state-of-the-art performance on seven logical reasoning benchmarks.</p>
<p>A FIRST-ORDER LOGIC PRELIMINARIES</p>
<p>To clarify the first-order logic vocabularies used in the paper, we give basic definitions as preliminaries based on Ranise et al. (2005) and Jovanović &amp; Barrett (2011).To help understanding, we also provide an example of the definitions, where the example is sampled from the AR-LSAT test set (Zhong et al., 2022).In this work, we use many-sorted first-order logic, which is one of the variants of the standard first-order logic, to better reflect the scenarios of the real-world logical reasoning problems.</p>
<p>A.1 SYNTAX</p>
<p>The syntax of many-sorted first-order logic is built on signatures.We first define signatures, and on top of that, we define variables, terms, atoms, literals, clauses, and formulas.Definition 5 (Signatures).A signature Σ = (S, F, P ) consists of countable sets of sorts S, function symbols F , and predicate symbols P .Each function symbol f is associated with a type
s 1 × • • • × s n → s, where n ≥ 0 and s 1 , • • • , s n , s ∈ S.
Function symbols with n = 0 (i.e.zero arity) are called constants of sort s.Each predicate symbol p is associated with a type
s 1 × • • • × s n , where n ≥ 1 and s 1 , • • • , s n ∈ S.
Let Σ be a signature.A set X of Σ-variables (or simply variables) is a countable set of variable names.Each variable name is associated with a sort in Σ.Based on variables, we define terms.Intuitively, terms are variables or functions applied to a tuple of other terms.Definition 6 (Terms).Σ-terms over X (or simply terms) are defined as follows.Each variable
x ∈ X of sort s is a term of sort s. If t 1 , • • • , t n are terms of sorts s 1 , • • • , s n , respectively, and f is a function symbol of type s 1 × • • • × s n → s, then f (t 1 , • • • , t n ) is a term of sort s.
Based on terms, we define atoms.Intuitively, atoms are predicates applied to a tuple of terms.Definition 7 (Atoms).Σ-atoms over X (or simply atoms) are defined as follows.If t 1 , • • • , t n are Σ-terms over X of sorts s 1 , • • • , s n , respectively, and p is a predicate symbol with the type
s 1 × • • • × s n , then p(t 1 , • • • , t n ) is an atom.
Additionally, literals are atoms or negation of atoms, and clauses are disjunctions of literals.We finally define formulas by using above definitions with logical connectives and quantifiers.Definition 8 (Formulas).Σ-formulas over X (or simply formulas) are defined as follows.Each Σ-atom over X is a formula.If α and β are formulas, then so are ¬α, α ∧ β, and α ∨ β.If x ∈ X is a variable of sort s and α is a formula, then so are ∃x α and ∀x α.</p>
<p>We give an example of the definitions of first-order logic syntax with the following formula in the AR-LSAT test set.Example 1.A signature is given as
Σ 1 = ({positions, potters}, F, P )
where F = {displayed, Reigel, 1, 6} such that displayed has the type positions → potters, Reigel is a constant of sort potters, and 1 and 6 are constants of sort positions, and P = {≈} such that ≈ is of the type positions × positions.We note that the equality symbol ≈ is always implicit from the context.If p is a variable of sort positions, then p and displayed(p) are Σ 1 -terms of sort positions.Then, displayed(p) ≈ Reigel, p ≈ 1, and p ≈ 6 are Σ 1 -atoms.Finally, the following is one example of a Σ 1 -formula
(∀p : positions) (displayed(p) ≈ Reigel) → (p ≈ 1 ∨ p ≈ 6)
which is the first-order logic translation of the natural language sentence "Reigel's bowl can be displayed only in either position 1 or position 6".</p>
<p>A.2 SEMANTICS</p>
<p>The semantics of many-sorted first-order logic is indicated by structures.We first define structures and extend its concept to define interpretations.On top of those, we define theories with models and consequences.</p>
<p>A signature Σ = (S, F, P ) only describes the names of sorts, functions, and predicates.However, it does not describe assignments of elements to each sort and evaluations of functions and predicates on the elements of sorts.Structures add this information.Definition 9 (Structures).A Σ-structure I assigns a non-empty domain set D s to each sort s ∈ S, a function f I :
D s1 ×• • •×D sn → D s for each function symbol f ∈ F of type s 1 ×• • •×s n → s, and a predicate p I : D s1 × • • • × D sn → {F, T} for each predicate symbol p ∈ P of type s 1 × • • • × s n .
Note that each constant c of sort s is mapped to an element c I ∈ D s .</p>
<p>To evaluate terms and formulas, we extend structures to variables and define interpretations.Definition 10 (Interpretations).Σ-interpretation I over X (or simply interpretation) is a Σ-structure that additionally assigns a value x I ∈ D s to each variable x ∈ X of sort s.</p>
<p>We denote I ⊨ ϕ if an interpretation I evaluates a formula ϕ to true.However, we are not usually interested in the evaluation of formulas in a given structure, but interested in specific meaning of functions and predicates.Theories deal with this problem.Definition 11 (Theories).A Σ-theory T (or simply theory) is a non-empty set of Σ-structures.</p>
<p>To solve practical problems, we additionally define the followings.A T -interpretation is a Σinterpretation I that extends some structure in the theory
T . A formula ϕ is T -satisfiable if I ⊨ ϕ for some T -interpretation I. A formula ϕ is T -valid, denoted by ⊨ T ϕ, if I ⊨ ϕ for all T -interpretations I.
We also introduce definitions to describe the relationship between an interpretation and a formula, and between two formulas.
Definition 12 (Models). A T -interpretation I such that I ⊨ ϕ is called a T -model of ϕ. Definition 13 (Consequences). A formula ϕ is a T -consequence of a formula ψ, denoted by ψ ⊨ T ϕ, if I ⊨ ψ implies I ⊨ ϕ for all T -interpretations I.
We give an example of the definitions of first-order logic semantics by continuing Example 1. Example 2. Let us consider the extended signature Σ 2 = ({positions, potters}, F, {≈}) where F = {displayed, Larsen, Mills, Neiman, Olivera, Park, Reigel, Serra, Vance, 1, 2, 3, 4, 5, 6}.</p>
<p>There are many possible Σ 2 -structures, and one exemplar structure I 2 is:</p>
<p>• D positions = {1, 2, 3, 4, 5, 6},</p>
<p>• D potters = {Larsen, Mills, Neiman, Olivera, Park, Reigel, Serra, Vance},
• Larsen I2 = Larsen, Mills I2 = Mills, Neiman I2 = Neiman, Olivera I2 = Olivera, Park I2 = Park, Reigel I2 = Reigel, Serra I2 = Serra, Vance I2 = Vance, • 1 I2 = 1, 2 I2 = 2, 3 I2 = 3, 4 I2 = 4, 5 I2 = 5, 6 I2 = 6
, and
• displayed I2 (1) = Reigel, displayed I2 (2) = Larsen, displayed I2 (3) = Reigel, displayed I2 (4) = Reigel, displayed I2 (5) = Reigel, displayed I2 (6) = Reigel.
Since we do not consider any additional variable here, Σ 2 -interpretation I ′ 2 is the same as I 2 .Now, let the theory T 2 be the Σ 2 -theory consisting only of the Σ 2 -structure I which has the same domain assignment to each sort and the same constant function assignments with
I 2 . I 2 is a T 2 - interpretation. If we consider the formula ϕ in Example 1, ϕ is T 2 -satisfiable, but not T 2 -valid.
Lastly, let us determine if I 2 satisfies ϕ.Reigel's bowl is displayed in the positions 1 and 6, but it is also displayed in positions 3, 4, and 5.A T 2 -interpretation I 2 does not satisfy the formula ϕ, which means I 2 is not a T 2 -model of ϕ.</p>
<p>B FIRST-ORDER LOGIC COMPLEXITY</p>
<p>To quantitatively measure the complexity of first-order logic formulas, we use a parameter following Arias &amp; Khardon (2003).The target formula is first transformed into an expression in a conjunctive normal form (CNF).Then, the complexity parameter is defined as a sum of three components, the number of clauses in the CNF expression, the maximum number of distinct terms in any clause of the CNF expression, and the maximum number of literals in any clause of the CNF expression.We implement the parameter by using a Z3 theorem prover (De Moura &amp; Bjørner, 2008).</p>
<p>We present two exemplar formulas and their complexity from the AR-LSAT test set.The first example is as follows:
(∃C : children) (¬(C ≈ Juan) ∧ (assigned(C) ≈ assigned(Juan))).
The formula is already in the CNF expression.It contains two clauses; ¬(C ≈ Juan) and assigned(C) ≈ assigned(Juan).The first clause has two distinct terms; C and Juan, and one literal; ¬(C ≈ Juan).The second clause has four distinct terms; C, Juan, assigned(C), and assigned(Juan), and one literal; assigned(C) ≈ assigned(Juan).The measured complexity is 2 + 4 + 1 = 7.</p>
<p>The second example is as follows:
(onSale(newPop) ∧ onSale(usedPop)) → (onSale(newSoul) ∧ onSale(usedSoul)).
The formula is first transformed into a CNF expression as follows:
(onSale(newSoul) ∨ ¬onSale(newPop) ∨ ¬onSale(usedPop))∧ (onSale(usedSoul) ∨ ¬onSale(newPop) ∨ ¬onSale(usedPop)).
It contains two clauses.The first clause has three distinct terms; newSoul, newPop, and usedPop, and three literals; onSale(newSoul), ¬onSale(newPop), and ¬onSale(usedPop).The second clause has the same number of distinct terms and literals.The measured complexity is 2 + 3 + 3 = 8.</p>
<p>C COMPONENT ACCUMULATION RULES</p>
<p>We describe rules for component accumulation according to a logical dependency structure.It aims to add components on simple subsentences to compose more complex subsentences in a specific order that preserves the underlying first-order logic semantics.We note that the rules are deduced from the definition of logical dependency structure (Section 3.1).The followings are the rules for component accumulation:
D i (i = 1, 2, • • • , k).
After that, integrate all logic dependents into U and add the updated U .</p>
<p>Rule 5.If more than one components X 1 , X 2 , • • • , X k depend on a logic coupler C, then integrate all components into C and add the updated C.</p>
<p>We present three examples of component accumulation given sentences and their logical dependency structures in the following section.</p>
<p>D EXAMPLES OF LOGICAL DEPENDENCY PARSING AND COMPONENT ACCUMULATION</p>
<p>We present three examples of logical dependency parsing and component accumulation.For component accumulation, the applied rules and corresponding logical dependencies are indicated for each accumulation step.(Integration) is additionally inserted for Rule 4 to maintain consistency.</p>
<p>Input</p>
<p>Target sentence: Each story is assigned a team of two interns-one photographer's assistant and one writer's assistant.</p>
<p>Declarations: interns = {Farber, Gombarick, Hall, Jackson, Kanze, Lha} stories = {Romania, Spain, Tuscany} assistants = {photographer, writer} assigned = Function(interns → stories) trained = Function(interns → assistants)</p>
<p>Output</p>
<p>Logical dependency parsing: U1="Each story is assigned an intern.",U2="There is an intern-a photographer's assistant.",U3="There is an intern-a writer's assistant.",C1="(merge)", C2="(merge)", C3="a team of two ... and"
U1 → C1, U2 → C1, U1 → C2, U3 → C2, C1 → C3, C2 → C3</p>
<p>E SAT SOLVER FUNCTION PREDICTION ON AR-LSAT</p>
<p>For the AR-LSAT dataset, we additionally need to predict a solver function f solver according to the question of the logical reasoning problem.For instance, if the question is "Which of the queries CAN be true?", then we need to assign a function that checks a satisfiability of the query given the constraints.If the question is "Which of the queries MUST be true?", then we need to assign a function that checks a validity of the query given the constraints.</p>
<p>Logic-LM predicts a solver function together with the first-order logic translation by a single inference as follows:
T , { φk , N L(φ k )} K+1 k=1 , fsolver ∼ P LLM (T , {φ k , N L(φ k )} K+1 k=1 , f solver | x, x fs ).(6)
For CLOVER, to incorporate solver function prediction in our problem formulation in Eq. 2, we perform this prediction at the preprocessing step as follows:
T , { N L(φ k )} K+1 k=1 , fsolver ∼ P LLM (T , {N L(φ k )} K+1 k=1 , f solver | x, x prep fs ) φk = CLOVER( T , N L(φ k ), X fs ), ∀k ∈ {1, 2, • • • , K + 1}.(7)
G PERFORMANCE ON DIFFERENT LANGUAGE MODELS Table 5 compares the performance of CLOVER and the neurosymbolic approach baselines on different languange models.We include three additional language models including gpt-4o-mini, gpt-3.5-turbo,and gpt-3.5-turbo-instruct,which the first two are chat-focused models and the other one is an instruction-following model.We evaluate these models on the Puzzle and Symbol datasets.If the symbolic solver cannot execute the solution, then we take random guesses.We exclude the performance of SymbCoT using gpt-3.5-turbo-instructsince the prompt including few-shot examples exceeds the context window of the language model.The results show that CLOVER clearly outperforms the baselines across different language models.</p>
<p>H INFERENCE TIME COSTS</p>
<p>It is difficult to measure inference time costs for methods that use LLMs with API calls.Specifically, inference time significantly depends on the current network traffic of an API, and the number of parameters are unknown.Despite this limitation, we compare CLOVER and the baselines by their API usage costs, which is a reliable way to measure inference time costs.</p>
<p>For comparison, we use gpt-4o-mini as a language model and measure the costs on the AR-LSAT annotated subset.We report the results in Table 6.CLOVER requires larger amount of inference costs compared to the baselines since the compositional first-order logic translation generates formulas for each logical dependency structure of a target sentence.However, the increased inference time cost is worth for the significant performance gain in Table 1.</p>
<p>I IMPACT OF SAT-BASED FIRST-ORDER LOGIC VERIFICATION</p>
<p>To further analyze an impact of using satisfiability in the verification algorithms, we compare those to two baselines: syntax consistency and LLM with instruction.First, we select the most frequent syntactically same formulas, which we call syntax consistency.Compared to logical consistency, logically equivalent but syntactically different formulas count as different formulas here.Second, we prompt LLM to select the most probable formula with reasoning, which we call LLM with instruction.LLM-based first-order logic verification is inspired by the previous works (Pan et al., 2023;Kirtania et al., 2024;Xu et al., 2024;Chen et al., 2024;Madaan et al., 2024;Ni et al., 2023).</p>
<p>We report the results in Table 7.The baselines show poor performance than the proposed SATbased verification algorithms.Compared to a random selection, syntax consistency show 4.7% performance increment on AR-LSAT, but 0.8% marginal increment on ZebraLogic.LLM with instruction does not show any performance improvement on both tasks, which points out the limited capability of LLM to verify first-order logic formulas.These results show that SAT-based first-order logic verification is the most appropriate algorithm that fully covers first-order logic semantics.U1="Stacy repairs any type of machine", U2="Yolanda repairs any type of machine", D1="does not", C1="that" D1 → U1; U1 → C1; U2 → C1 ### 2: U1="Stacy repairs any type of machine", D1="does not", D2="that Yolanda repairs" D1 → U1; D2 → U1 . . .</p>
<p>is toured on two consecutive days, and on no other days.1)∃ ∶    =  2) ∃ ∶    =  ∧   + 1 =  3) ∃ ∶  (  =  ∧   + 1 =  ∧ ∀ ∶  (( ≠  ∧  ≠  + 1) →   ≠  )) ∃ ∶    =  ∧   + 1 =  ∧ (∀ ∶ )(   ≠  ∧   + 1 ≠  →   ≠  )</p>
<p>Figure 2 :
2
Figure 2: Overview of CLOVER.Given declarations of a theory, CLOVER parses a target sentence to several possible logical dependency structures, accumulates components according to logical dependencies, and sequentially translates subsentences to first-order logic formulas.Then, CLOVER verifies a set of estimated formulas.Logical consistency selects the most frequent logically equivalent formulas.Disproving by counter-interpretation sequentially compares two formulas and disprove one by determining if a counter-interpretation satisfies the target sentence.</p>
<p>Figure 3: Occurences of different error types of CLOVER and Logic-LM on AR-LSAT annotated subset.</p>
<p>Rule 1 .
1
Start with copying logic units.Rule 2. If a logic dependent D is the only dependent of a logic unit U , then integrate D into U and add the updated U. Rule 3. If a logic dependent D 1 depends on another logic dependent D 2 , then integrate D 1 into a logic unit U that includes D 2 and add the updated U .Rule 4. If more than one logic dependents D 1 , D 2 , • • • , D k depend on a logic unit U , then add k sentences that include U and each logic dependent</p>
<p>Figure 5 :
5
Figure 5: Example of logical dependency parsing and component accumulation on AR-LSAT.</p>
<p>Figure 7 :
7
Figure 7: Prompt used for logical dependency parsing on AR-LSAT.</p>
<p>Figure 8 :
8
Figure 8: Prompt used for component accumulation on AR-LSAT.</p>
<p>Figure 9 :
9
Figure 9: Prompt used for sequential translation on AR-LSAT.</p>
<p>Table 1 :
1
Performance on logical reasoning tasks using CLOVER and the baseline methods.
AR-LSAT ZebraLogic Puzzle Symbol Deduction FOLIO ProofWriterStandard30.30.463.074.784.770.953.7CoT36.80.451.080.894.073.978.0SymbCoT34.20.866.555.690.776.980.2Logic-LM42.445.464.081.895.375.495.3CLOVER62.875.483.589.999.378.896.7</p>
<p>Table 2 :
2
Comparison of program accuracy, execution rate, and execution accuracy of CLOVER and Logic-LM.
Program AccExecution RateExecution AccLogic-LM CLOVER Logic-LM CLOVER Logic-LM CLOVERAR-LSAT17.346.833.859.751.378.3Puzzle60.079.079.580.075.598.8Symbol49.576.852.582.894.292.7Deduction92.799.097.399.795.299.3FOLIO51.262.665.574.978.283.6ProofWrtier94.296.596.899.297.297.3</p>
<p>Table 3 :
3
Ablation of CLOVER on AR-LSAT and ZebraLogic.The first three rows are ablations of CLOVER, and the last two rows correspond to CLOVER.Second, Table2presents three additional evaluations for the neurosymbolic approaches with a symbolic solver.CLOVER shows higher execution rate on every task, which indicates that CLOVER has better capability to generate syntactically correct first-order logic formulas than Logic-LM.CLOVER also shows higher execution accuracy on most tasks, which indicates that CLOVER has better capability to generate logically (or semantically) correct formulas than Logic-LM.These two observations lead to an outperforming program accuracy of CLOVER across different logical reasoning tasks.Specifically, CLOVER increases the execution rate of Logic-LM on AR-LSAT by 25.9% and the execution accuracy by 27.0%, which finally leads to more than doubled program accuracy of Logic-LM.Lastly, we compare the performance of CLOVER and the baselines on different languange models in Appendix G.
Is CLOVER?TranslationVerificationAR-LSAT ZebraLogic✗direct✗53.345.4✗direct (5×)logical consistency54.659.6✗compositional✗55.070.0✓compositional logical consistency61.974.2✓compositionaldisproving62.875.4</p>
<p>Table 5 :
5
Performance with different language models using CLOVER and neurosymbolic approach baselines.
PuzzleSymbolLogic-LM SymbCoT CLOVER Logic-LM SymbCoT CLOVERgpt-4o-mini42.560.060.538.446.571.7gpt-3.5-turbo42.535.063.524.227.360.6gpt-3.5-turbo-instruct46.0N/A59.050.5N/A70.7</p>
<p>Table 6 :
6
Comparison of inference time costs using CLOVER and the baselines with gpt-4o-mini.
Costs (USD)Standard0.02CoT0.02Logic-LM0.08SymbCoT0.15CLOVER0.30</p>
<p>Table 7 :
7
Comparison of different first-order logic verification approaches on AR-LSAT and Ze-braLogic.
VerificationAR-LSAT ZebraLogicRandom55.070.0Syntax consistency59.770.8LLM w/ instruction53.370.0Logical consistency61.974.2Disproving62.875.4
The source code used in the paper is available at https://github.com/Hyun-Ryu/clover.
To evaluate the complexity and performance of each first-order logic formula, we sample the first problem from each set of problems that share the same context in the AR-LSAT test set and manually annotate the
Many-sorted first-order logic is one of the variants of the standard first-order logic that allows variables to have different domains, which is called sorts S. We provide related preliminaries in Appendix A.
A theory assigns specific meanings to symbols of formulas. For simplicity, we presume that a theory T incorporates the most commonly applied theories (e.g., theory of equality, arithmetic, etc.).
For AR-LSAT, we need to check the T -validity depending on the problem. More details in Appendix E.
We use a revised version of FOLIO that improves sample quality and fixes errors, which is released on: https://huggingface.co/datasets/yale-nlp/FOLIO.
To specify language model versions provided by OpenAI, we use gpt-4o-2024-05-13 and gpt-4o-mini-2024-07-18 on our experiments.
We use CoT-based reasoning LLMs that were recently released from OpenAI, specifically o1-preview-2024-09-12 and o1-mini-2024-09-12, for our analysis.
ACKNOWLEDGMENTSThis work was supported by Institute for Information &amp; communications Technology Planning &amp; Evaluation(IITP) grant funded by the Korea government(MSIT) (RS-2019-II190075, Artificial Intelligence Graduate School Program(KAIST)).F DATASET STATISTICSIn this section, we present dataset statistics of the logical reasoning tasks in Table4.We describe the details in the following paragraphs.If not mentioned, we use the entire test set provided by the dataset.AR-LSAT-annotated.We annotate a representative subset of AR-LSAT test set(Zhong et al., 2022)to measure first-order logic translation accuracy at a formula-level.First, we sample the first problem from each set of problems that share the same context in the AR-LSAT test set.Then, we preprocess the logical reasoning problem to generate a theory T and a set of natural language sentences, following the steps in Section 2. For each problem, we note that the sentences for constraints represent diverse first-order logic semantics while the sentences for five queries represent (nearly) the same first-order logic semantics.We therefore exclude other four queries and leave only the first one.For each sentence, we carefully annotate T -satisfiable first-order logic formula and double-check its correctness.If a T cannot express the context of the logical reasoning problem, we exclude the sentences in that problem.As a result, we collect a total of 305 annotated formulas.ZebraLogic.The ZebraLogic test test(Lin et al., 2025)consists of 1,000 zebra puzzles where the puzzle size varies from 2 × 2 to 6 × 6.There are 25 different puzzle sizes, and each size has 40 samples.To evaluate the models on the most challenging puzzles, we use six hardest puzzle sizes (4 × 6, 5 × 5, 5 × 6, 6 × 4, 6 × 5, and 6 × 6) for our test set, which yields a total of 240 puzzles.Puzzle.The entire dataset(Srivastava et al., 2022)consists of 1,000 samples.To split a test set, we sample the last 200 samples in the order of the samples listed in the dataset.Symbol.The entire dataset(Srivastava et al., 2022)includes 990 samples, which are categorized into five subsets (plain, adversarial, tricky, agnostic name-side, and agnostic emoji-side) with the same size.All examples in different subsets share the same logical meaning with each other where the only difference is the semantic link between the emojis and their names.To focus on a first-order logic translation, we evaluate the models on the plain subset which includes 198 samples.The plain subset consists of three subgroups with the same size categorized by their difficulties.To construct a test set, we sample the second half of each subgroup in the order of the samples listed in the dataset, which yields a total of 99 samples.Deduction.The entire dataset(Srivastava et al., 2022)consists of 1,500 samples.We use the test set followingPan et al. (2023)which consists of 300 samples.ProofWriter.We use the test set followingPan et al. (2023), which is a set of randomly sampled 600 examples from the most challenging depth-5 subset.(Srivastava et al., 2022)1 200 2,3,4,5 Apache 2.0 Symbol(Srivastava et al., 2022)1 99 5 Apache 2.0 Deduction(Srivastava et al., 2022)2 300 3,5,7 Apache 2.0 FOLIO(Han et al., 2022)2 203 3 CC-BY-SA-4.0license ProofWriter(Tafjord et al., 2021)1 600 3 CC BY 4.0Prompt for Disproving by Counter-Interpretation### InstructionYour task is to determine whether the given solution can be included in any of the possible scenarios that arise when the given sentence is set as a condition.First, you need to calculate all possible scenarios that can occur under the given sentence as a condition, and then check if the situation obtained by interpreting the solution through the given declarations falls into one of those scenarios.As shown in the various examples below, you should provide your answer along with a logical explanation for the reasoning behind it.Keep in mind that you should focus on logical conditions rather than relying on commonsense when performing the task.K EXTENSIVE ERROR ANALYSIS OF LOGIC-LMWe present in-depth error analysis of Logic-LM(Pan et al., 2023)for first-order logic translation and the corresponding results of CLOVER on the AR-LSAT annotated subset.We followPan et al. (2023)andYe et al. (2024)for the specification of declarations and formulas of first-order logic.We colorize incorrect translations as red and describe the reason in the following.InputTarget sentence: Each locker must be assigned to either one or two children, and each child must be assigned to exactly one locker.Logic-LM:Error Analysis of Logic-LM It means Reigel's bowl is displayed in either position 1 or position 6.We note that Reigel's bowl cannot be displayed.Figure17: Example #7 of error analysis of Logic-LM.InputTarget sentence: If Nash is assigned to a committee, Nash must be the leader for that committee.Error Analysis of Logic-LMThe formula is always false.Variable p should not be leader.Error Analysis of Logic-LM It means that Nottingham is visited the day before Sunnyside is visited.We note that the opposite should be also possible.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Complexity parameters for first-order classes. Marta Arias, Roni Khardon, Inductive Logic Programming: 13th International Conference, ILP 2003. Proceedings. Szeged, HungarySpringerSeptember 29-October 1, 2003. 200313</p>
<p>Logical reasoning in formal and everyday reasoning tasks. Hugo Bronkhorst, Gerrit Roorda, Cor Suhre, Martin Goedhart, International Journal of Science and Mathematics Education. 182020</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De, Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Z3: An efficient smt solver. Leonardo De, Moura , Nikolaj Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Successive prompting for decomposing complex questions. Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Lyn Frazier, Janet Dean Fodor, The sausage machine: A new two-stage parsing model. Cognition. 19786</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Sharing is caring: Combination of theories. Dejan Jovanović, Clark Barrett, Frontiers of Combining Systems: 8th International Symposium. Saarbrücken, GermanySpringer2011. October 5-7, 2011. 20118</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Logic-lm++: Multi-step refinement for symbolic formulations. Shashank Kirtania, Priyanshu Gupta, Arjun Radhakirshna, arXiv:2407.025142024arXiv preprint</p>
<p>Zebralogic: On the scaling limits of llms for logical reasoning. Ronan Bill Yuchen Lin, Kyle Le Bras, Ashish Richardson, Radha Sabharwal, Peter Poovendran, Yejin Clark, Choi, arXiv:2502.011002025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Universal grammar. Richard Montague, 1974. 1970</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-Tau Yih, Sida Wang, Xi Victoria, Lin , International Conference on Machine Learning. PMLR2023</p>
<p>Logical reasoning and learning. Encyclopedia of the sciences of learning. Terezinha Nunes, 2012</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Alex Theo X Olausson, Benjamin Gu, Cedegao E Lipkin, Armando Zhang, Joshua B Solar-Lezama, Roger Tenenbaum, Levy, arXiv:2310.151642023arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023arXiv preprint</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Combining data structures with nonstably infinite theories using many-sorted logic. Silvio Ranise, Christophe Ringeissen, Calogero G Zarba, International Workshop on Frontiers of Combining Systems. Springer2005</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam Adam R Brown, Aditya Santoro, Adrià Gupta, Garriga-Alonso, arXiv:2206.046152022arXiv preprint</p>
<p>Cognitive load during problem solving: Effects on learning. John Sweller, Cognitive science. 1221988</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. Jundong Xu, Hao Fei, Liangming Pan, Qian Liu, Mong-Li Lee, Wynne Hsu, arXiv:2405.183572024arXiv preprint</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models are versatile decomposers: Decomposing evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Analytical reasoning of text. Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, Nan Duan, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
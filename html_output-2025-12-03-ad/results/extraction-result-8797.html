<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8797 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8797</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8797</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-6bfe4a32dbf4c58af83745e2fcf68217217a03ea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6bfe4a32dbf4c58af83745e2fcf68217217a03ea" target="_blank">A Survey of Graph Transformers: Architectures, Theories and Applications</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey, and examines the expressivity of Graph Transformers in various discussed architectures.</p>
                <p><strong>Paper Abstract:</strong> Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8797.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8797.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES/SELFIES tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES and SELFIES string tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-dimensional string encodings of molecular graphs (SMILES, SELFIES) tokenized into sequences of atom/bond tokens for processing by language-model-style Transformers; treats molecule as text-like input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>1D string tokenization (SMILES/SELFIES)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Molecules are represented as linear strings (SMILES or SELFIES). Tokenization splits the string into atomic and bond tokens (multi-level tokens including node and edge tokens), which are then fed to a standard transformer or language model as a token sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Molecular graphs (chemical molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert molecular graph to SMILES/SELFIES linear string using standard cheminformatics serialization (e.g., RDKit) and then tokenize the string into atomic and bond tokens; in pretraining, mask and predict masked tokens (MLM-style).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Molecular property prediction, molecular generation, pretraining for molecular LMs, sequence-to-sequence tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numerical metrics reported in this survey for SMILES/SELFIES conversion specifically; survey notes that vanilla Transformers on SMILES can be effective but fail to encode explicit graph structural information compared to GTs (no numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey contrasts 1D string tokenization with explicit graph-aware GT approaches (Graphormer, GTransformer, equivariant GTs) and notes that string-only approaches do not explicitly encode 2D/3D structure; hybrid approaches (tokenizers combining SMILES with graph tokens or using graph-aware PEs) can be superior but no numerical comparisons provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages existing strong sequence LMs and tokenizers; large corpora of SMILES exist enabling scale pretraining; simple pipeline (string -> tokenizer -> LM).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Losess or obscures explicit 2D/3D structural information of the molecule; vanilla Transformer on SMILES cannot directly model topology (survey notes this limitation); ambiguous linearizations (different valid SMILES for same graph) can cause inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey highlights that using only SMILES/SELFIES can underperform on tasks that need explicit spatial/topological reasoning (e.g., long-range contact prediction, conformation-sensitive properties) though no numeric failure rates are given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8797.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Node/Edge tokenization (TokenGT style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node and Edge tokenization into Transformer tokens (TokenGT / Token-based GTs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent graphs by explicitly creating tokens for nodes and (optionally) edges and feeding the concatenated token set into a Transformer; enables modeling both nodes and edges jointly with self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pure transformers are powerful graph learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-and-edge tokenization (node/edge token list)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct an input token sequence consisting of one token per node plus one token per edge (n + m tokens). Node tokens carry node features; edge tokens carry edge features and identifiers; the full token set is input to a Transformer which attends over nodes and edges jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (molecular graphs, knowledge graphs, generic graphs with node and edge features)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No sequential traversal; build flat token list by enumerating nodes then edges (or interleaving); augment edge tokens with source/target node identifiers and type tokens; pass the concatenated token matrix to Transformer self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node/graph classification/regression, link prediction, graph-level tasks, pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports that TokenGT-style approaches achieve high expressivity (stated as achieving 2-WL expressivity in the survey), but does not provide task-specific numeric metrics in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey notes TokenGT achieves strong expressivity (2-WL) and contrasts it with node-only tokenizations and GNNs; however, exact empirical comparisons and numbers versus other GT variants are not provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models edges and nodes within the Transformer, enabling rich interactions and higher theoretical expressivity (2-WL as reported); no need for separate GNN modules.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Token count grows with nodes + edges which increases computational and memory cost (quadratic attention); may be impractical for very large graphs without additional sparsity/scaling tricks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted scalability concerns for large dense graphs; survey does not provide empirical failure cases but highlights complexity as limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8797.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subgraph / Hop tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subgraph-level and hop-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Represent nodes by tokens that are entire k-hop subgraphs or hop sequences, enabling richer local structural context per token and scalable localized attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subgraph/hop-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each token corresponds to a k-hop subgraph centered at a node (subgraph token) or a sequence of hop-based tokens; tokens may overlap across nodes; hop-level tokenization builds sequences per node across hops to allow local self-attention within each node's neighborhood.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs, large graphs requiring scalability (social networks, citation graphs, molecular local-context modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For subgraph tokenization: for each node, extract its k-hop induced subgraph and produce a single token embedding (via GNN/aggregation) representing that subgraph. For hop-level: create per-node token sequences representing 0-hop,1-hop,...,k-hop neighbor sets, enabling attention over a node's neighborhood sequence rather than the whole graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Node classification, graph classification, scalable representation learning on large graphs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No specific numeric metrics reported in survey for these representations; survey states hop-level enables mini-batch training and improved scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively with node-level and edge-level tokenization: subgraph/hop tokens capture higher-order structure and long-range patterns better; hop-level is designed to reduce attention complexity versus naive global attention.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures richer local substructures (motifs, community patterns); hop-level supports mini-batching and reduces quadratic cost of full-graph attention.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires pre-aggregation or per-node subgraph extraction (preprocessing overhead) and may lose some global cross-subgraph interactions unless combined with global modules.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey indicates potential loss of global graph context if only local subgraph tokens are used without global mixing; no detailed empirical failure cases provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8797.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Edge-as-text tokens (Edgeformer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edgeformer-style: represent edges as multi-token textual units</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert edge information into multi-token representations and combine edge tokens with node tokens so that self-attention models edges as textual-like tokens linked to incident nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Edge-as-token serialization (edge tokenization / textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edges are serialized into token sequences (potentially multiple tokens per edge encoding edge type, attributes, textual content) and concatenated with node tokens; self-attention processes both node and edge tokens together, enabling explicit edge-centric modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Text-rich graphs and general graphs with rich edge features (textual edges, molecular bonds, knowledge-graph relations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each edge, create a token vector or small token sequence encoding its attributes (or textual content); append or interleave these edge tokens with node tokens. In text-rich cases, edge text itself supplies tokens (e.g., user reviews as edge text).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Edge classification, link prediction, node representation learning in textual graphs, KG completion</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric performance metrics provided in the survey for edge-as-token conversions; described qualitatively as effective for textual-edge scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey contrasts Edgeformer-style methods with node-only tokenizations and with approaches that incorporate edge features as attention bias; edge-as-token gives a more direct mechanism to model edge semantics than bias-only methods.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models edge semantics and textual content, enabling richer joint node-edge representations; well-suited when edges contain substantive text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases token count and attention cost; requires careful identifier encoding to relate edge tokens to their incident nodes; may be heavy in graphs with many edges.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey does not report concrete failure cases but notes scalability and complexity as the primary constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8797.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR linearization / shortest-path embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR graph linearization with shortest-path / relational encodings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For AMR-to-text generation, represent graph relations (pairwise positions) via shortest-path embeddings or relational encodings and feed graph-encoded representations to a Transformer encoder-decoder for text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>AMR-to-text linearization with shortest-path relational encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode pairwise node relations by computing shortest-path distances or path-label aggregates between AMR nodes and represent these as relative position embeddings or biases in the transformer; the encoder produces node embeddings which a decoder uses to generate the sentence.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Semantic graphs (AMR), abstract meaning representations, other directed semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute shortest paths between node pairs in the AMR; embed path length and/or edge label sequences into relational encodings (learned embeddings or MLP outputs) that are used as attention bias or as additional features for node tokens; then decode to text with a standard Transformer decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation, semantic-to-text generation, translation-like generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey does not report numerical BLEU or other metrics for these AMR methods; mentions the approach is used in existing AMR-to-text systems but gives no specific numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively with methods that ignore pairwise path encodings: shortest-path relational encodings help attention capture structural relations; HetGT-style multi-graph masking is another alternative to emphasize heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves relational structure (distance and edge-label sequences) as explicit bias for generation; helps decoder attend to semantic relations rather than only node labels.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Computing all-pair shortest paths and encoding them can be costly for large graphs; representing long path label sequences may require summarization heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey notes that simple SPD (shortest path distance) can be insufficient to distinguish certain structural perturbations (cites work showing SPD limitations), suggesting failure when graphs have ambiguous shortest-path signatures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8797.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG subgraph sampling + mask-then-predict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph subgraph sampling with masked knowledge modeling (KGTransformer / Relphormer / KG-R3 style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Convert knowledge graph neighborhoods or sampled subgraphs into sequences/tokens for Transformer pretraining by mask-then-predict objectives, often using random-walk sampling or subgraph retrieval to produce training examples for LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>KG subgraph serialization for masked pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Retrieve or sample a subgraph relevant to a query (via random walk or retrieval); serialize nodes and edges of the subgraph into a token/block representation (often preserving relational types via attention biases); apply mask-then-predict pretraining objectives to teach the model to reconstruct masked entities/relations.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (Freebase, DBpedia, Wikidata, etc.) and subgraph fragments</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use random-walk sampling or retrieval algorithms to extract subgraphs; optionally linearize or flatten the subgraph into an ordered token block or feed as a graph-structured input to a GT encoder with attention biases; use masked knowledge modeling / contrastive pretraining to train encoders/decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge graph to text, question answering over KGs, KG completion, relation extraction, pretraining of graph-aware LMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey does not provide concrete numerical metrics for these KG-to-text pretraining pipelines; they are described as used in pretraining and improving downstream KG tasks, but no numbers are reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to whole-graph training, sampling/retrieval improves scalability; compared to pure textual LMs, graph-aware pretraining yields better structured reasoning capabilities (qualitative statement in survey, no numeric comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Scalable pretraining via subgraph sampling; enables mask-then-predict objectives aligned with LM training; preserves local structural context and relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>May miss global context if subgraphs are too small; random-walk sampling can bias training towards high-degree nodes and over-represent local structures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey notes possible limitations in capturing high-order/global reasoning when only local subgraphs are used; no specific failure-case experiments reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8797.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8797.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual-edges tokenization / Edgeformer-N/E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edgeformer variants for textual-edge modeling (Edgeformer-E, Edgeformer-N)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When edges carry text (reviews, answers), convert edge text to tokens and integrate edge-token self-attention with node tokens; two variants focus on edge- or node-centric processing of textual edges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Textual-edge tokenization and ego-graph aggregation (Edgeformer variants)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Edge text is tokenized and embedded; Edgeformer-E processes each edge token jointly with its incident node tokens via self-attention; Edgeformer-N constructs an ego-graph for each node, models incident edges with Edgeformer-E, then aggregates edge-derived features to produce node embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Textual graphs where edges contain substantial text (user-item reviews, question-answer pairs, KG edges with textual descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize edge textual content (standard NLP tokenization); for each node, form an ego-graph including incident edge tokens; run self-attention cross between node and edge tokens (edge-centric or node-centric pipelines) to produce final node/edge embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Link prediction, edge classification, node representation learning in text-rich graphs, recommendation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No numeric performance values reported in the survey for these methods; described qualitatively as effective on textual-edge datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey contrasts edge-token approaches with treating edge text as side features or attention biases; edge-tokenization gives a more direct modeling path for rich edge content.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables direct modeling of rich textual content on edges and its interplay with nodes; captures contextual semantics of interactions (e.g., reviews) jointly with graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Potentially large token budgets when edges contain long texts; requires careful aggregation to relate edge tokens back to node-level predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey does not enumerate explicit failure cases; scalability and explosion of tokens for dense text edges flagged as main concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Graph Transformers: Architectures, Theories and Applications', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pure transformers are powerful graph learners <em>(Rating: 2)</em></li>
                <li>Do transformers really perform badly for graph representation? <em>(Rating: 2)</em></li>
                <li>Edgeformers <em>(Rating: 1)</em></li>
                <li>KGTransformer <em>(Rating: 1)</em></li>
                <li>Relphormer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8797",
    "paper_id": "paper-6bfe4a32dbf4c58af83745e2fcf68217217a03ea",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "SMILES/SELFIES tokenization",
            "name_full": "SMILES and SELFIES string tokenization",
            "brief_description": "One-dimensional string encodings of molecular graphs (SMILES, SELFIES) tokenized into sequences of atom/bond tokens for processing by language-model-style Transformers; treats molecule as text-like input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "1D string tokenization (SMILES/SELFIES)",
            "representation_description": "Molecules are represented as linear strings (SMILES or SELFIES). Tokenization splits the string into atomic and bond tokens (multi-level tokens including node and edge tokens), which are then fed to a standard transformer or language model as a token sequence.",
            "graph_type": "Molecular graphs (chemical molecules)",
            "conversion_method": "Convert molecular graph to SMILES/SELFIES linear string using standard cheminformatics serialization (e.g., RDKit) and then tokenize the string into atomic and bond tokens; in pretraining, mask and predict masked tokens (MLM-style).",
            "downstream_task": "Molecular property prediction, molecular generation, pretraining for molecular LMs, sequence-to-sequence tasks",
            "performance_metrics": "No numerical metrics reported in this survey for SMILES/SELFIES conversion specifically; survey notes that vanilla Transformers on SMILES can be effective but fail to encode explicit graph structural information compared to GTs (no numbers provided).",
            "comparison_to_others": "Survey contrasts 1D string tokenization with explicit graph-aware GT approaches (Graphormer, GTransformer, equivariant GTs) and notes that string-only approaches do not explicitly encode 2D/3D structure; hybrid approaches (tokenizers combining SMILES with graph tokens or using graph-aware PEs) can be superior but no numerical comparisons provided in this survey.",
            "advantages": "Leverages existing strong sequence LMs and tokenizers; large corpora of SMILES exist enabling scale pretraining; simple pipeline (string -&gt; tokenizer -&gt; LM).",
            "disadvantages": "Losess or obscures explicit 2D/3D structural information of the molecule; vanilla Transformer on SMILES cannot directly model topology (survey notes this limitation); ambiguous linearizations (different valid SMILES for same graph) can cause inconsistency.",
            "failure_cases": "Survey highlights that using only SMILES/SELFIES can underperform on tasks that need explicit spatial/topological reasoning (e.g., long-range contact prediction, conformation-sensitive properties) though no numeric failure rates are given.",
            "uuid": "e8797.0",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Node/Edge tokenization (TokenGT style)",
            "name_full": "Node and Edge tokenization into Transformer tokens (TokenGT / Token-based GTs)",
            "brief_description": "Represent graphs by explicitly creating tokens for nodes and (optionally) edges and feeding the concatenated token set into a Transformer; enables modeling both nodes and edges jointly with self-attention.",
            "citation_title": "Pure transformers are powerful graph learners",
            "mention_or_use": "mention",
            "representation_name": "Node-and-edge tokenization (node/edge token list)",
            "representation_description": "Construct an input token sequence consisting of one token per node plus one token per edge (n + m tokens). Node tokens carry node features; edge tokens carry edge features and identifiers; the full token set is input to a Transformer which attends over nodes and edges jointly.",
            "graph_type": "General graphs (molecular graphs, knowledge graphs, generic graphs with node and edge features)",
            "conversion_method": "No sequential traversal; build flat token list by enumerating nodes then edges (or interleaving); augment edge tokens with source/target node identifiers and type tokens; pass the concatenated token matrix to Transformer self-attention.",
            "downstream_task": "Node/graph classification/regression, link prediction, graph-level tasks, pretraining",
            "performance_metrics": "Survey reports that TokenGT-style approaches achieve high expressivity (stated as achieving 2-WL expressivity in the survey), but does not provide task-specific numeric metrics in this paper.",
            "comparison_to_others": "Survey notes TokenGT achieves strong expressivity (2-WL) and contrasts it with node-only tokenizations and GNNs; however, exact empirical comparisons and numbers versus other GT variants are not provided in this survey.",
            "advantages": "Directly models edges and nodes within the Transformer, enabling rich interactions and higher theoretical expressivity (2-WL as reported); no need for separate GNN modules.",
            "disadvantages": "Token count grows with nodes + edges which increases computational and memory cost (quadratic attention); may be impractical for very large graphs without additional sparsity/scaling tricks.",
            "failure_cases": "Noted scalability concerns for large dense graphs; survey does not provide empirical failure cases but highlights complexity as limiting factor.",
            "uuid": "e8797.1",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Subgraph / Hop tokenization",
            "name_full": "Subgraph-level and hop-level tokenization",
            "brief_description": "Represent nodes by tokens that are entire k-hop subgraphs or hop sequences, enabling richer local structural context per token and scalable localized attention.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Subgraph/hop-level tokenization",
            "representation_description": "Each token corresponds to a k-hop subgraph centered at a node (subgraph token) or a sequence of hop-based tokens; tokens may overlap across nodes; hop-level tokenization builds sequences per node across hops to allow local self-attention within each node's neighborhood.",
            "graph_type": "General graphs, large graphs requiring scalability (social networks, citation graphs, molecular local-context modeling)",
            "conversion_method": "For subgraph tokenization: for each node, extract its k-hop induced subgraph and produce a single token embedding (via GNN/aggregation) representing that subgraph. For hop-level: create per-node token sequences representing 0-hop,1-hop,...,k-hop neighbor sets, enabling attention over a node's neighborhood sequence rather than the whole graph.",
            "downstream_task": "Node classification, graph classification, scalable representation learning on large graphs",
            "performance_metrics": "No specific numeric metrics reported in survey for these representations; survey states hop-level enables mini-batch training and improved scalability.",
            "comparison_to_others": "Compared qualitatively with node-level and edge-level tokenization: subgraph/hop tokens capture higher-order structure and long-range patterns better; hop-level is designed to reduce attention complexity versus naive global attention.",
            "advantages": "Captures richer local substructures (motifs, community patterns); hop-level supports mini-batching and reduces quadratic cost of full-graph attention.",
            "disadvantages": "Requires pre-aggregation or per-node subgraph extraction (preprocessing overhead) and may lose some global cross-subgraph interactions unless combined with global modules.",
            "failure_cases": "Survey indicates potential loss of global graph context if only local subgraph tokens are used without global mixing; no detailed empirical failure cases provided.",
            "uuid": "e8797.2",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Edge-as-text tokens (Edgeformer)",
            "name_full": "Edgeformer-style: represent edges as multi-token textual units",
            "brief_description": "Convert edge information into multi-token representations and combine edge tokens with node tokens so that self-attention models edges as textual-like tokens linked to incident nodes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Edge-as-token serialization (edge tokenization / textualization)",
            "representation_description": "Edges are serialized into token sequences (potentially multiple tokens per edge encoding edge type, attributes, textual content) and concatenated with node tokens; self-attention processes both node and edge tokens together, enabling explicit edge-centric modeling.",
            "graph_type": "Text-rich graphs and general graphs with rich edge features (textual edges, molecular bonds, knowledge-graph relations)",
            "conversion_method": "For each edge, create a token vector or small token sequence encoding its attributes (or textual content); append or interleave these edge tokens with node tokens. In text-rich cases, edge text itself supplies tokens (e.g., user reviews as edge text).",
            "downstream_task": "Edge classification, link prediction, node representation learning in textual graphs, KG completion",
            "performance_metrics": "No numeric performance metrics provided in the survey for edge-as-token conversions; described qualitatively as effective for textual-edge scenarios.",
            "comparison_to_others": "Survey contrasts Edgeformer-style methods with node-only tokenizations and with approaches that incorporate edge features as attention bias; edge-as-token gives a more direct mechanism to model edge semantics than bias-only methods.",
            "advantages": "Explicitly models edge semantics and textual content, enabling richer joint node-edge representations; well-suited when edges contain substantive text.",
            "disadvantages": "Increases token count and attention cost; requires careful identifier encoding to relate edge tokens to their incident nodes; may be heavy in graphs with many edges.",
            "failure_cases": "Survey does not report concrete failure cases but notes scalability and complexity as the primary constraints.",
            "uuid": "e8797.3",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "AMR linearization / shortest-path embedding",
            "name_full": "AMR graph linearization with shortest-path / relational encodings",
            "brief_description": "For AMR-to-text generation, represent graph relations (pairwise positions) via shortest-path embeddings or relational encodings and feed graph-encoded representations to a Transformer encoder-decoder for text generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "AMR-to-text linearization with shortest-path relational encoding",
            "representation_description": "Encode pairwise node relations by computing shortest-path distances or path-label aggregates between AMR nodes and represent these as relative position embeddings or biases in the transformer; the encoder produces node embeddings which a decoder uses to generate the sentence.",
            "graph_type": "Semantic graphs (AMR), abstract meaning representations, other directed semantic graphs",
            "conversion_method": "Compute shortest paths between node pairs in the AMR; embed path length and/or edge label sequences into relational encodings (learned embeddings or MLP outputs) that are used as attention bias or as additional features for node tokens; then decode to text with a standard Transformer decoder.",
            "downstream_task": "AMR-to-text generation, semantic-to-text generation, translation-like generation tasks",
            "performance_metrics": "Survey does not report numerical BLEU or other metrics for these AMR methods; mentions the approach is used in existing AMR-to-text systems but gives no specific numeric comparisons.",
            "comparison_to_others": "Compared qualitatively with methods that ignore pairwise path encodings: shortest-path relational encodings help attention capture structural relations; HetGT-style multi-graph masking is another alternative to emphasize heterogeneity.",
            "advantages": "Preserves relational structure (distance and edge-label sequences) as explicit bias for generation; helps decoder attend to semantic relations rather than only node labels.",
            "disadvantages": "Computing all-pair shortest paths and encoding them can be costly for large graphs; representing long path label sequences may require summarization heuristics.",
            "failure_cases": "Survey notes that simple SPD (shortest path distance) can be insufficient to distinguish certain structural perturbations (cites work showing SPD limitations), suggesting failure when graphs have ambiguous shortest-path signatures.",
            "uuid": "e8797.4",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "KG subgraph sampling + mask-then-predict",
            "name_full": "Knowledge-graph subgraph sampling with masked knowledge modeling (KGTransformer / Relphormer / KG-R3 style)",
            "brief_description": "Convert knowledge graph neighborhoods or sampled subgraphs into sequences/tokens for Transformer pretraining by mask-then-predict objectives, often using random-walk sampling or subgraph retrieval to produce training examples for LMs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "KG subgraph serialization for masked pretraining",
            "representation_description": "Retrieve or sample a subgraph relevant to a query (via random walk or retrieval); serialize nodes and edges of the subgraph into a token/block representation (often preserving relational types via attention biases); apply mask-then-predict pretraining objectives to teach the model to reconstruct masked entities/relations.",
            "graph_type": "Knowledge graphs (Freebase, DBpedia, Wikidata, etc.) and subgraph fragments",
            "conversion_method": "Use random-walk sampling or retrieval algorithms to extract subgraphs; optionally linearize or flatten the subgraph into an ordered token block or feed as a graph-structured input to a GT encoder with attention biases; use masked knowledge modeling / contrastive pretraining to train encoders/decoders.",
            "downstream_task": "Knowledge graph to text, question answering over KGs, KG completion, relation extraction, pretraining of graph-aware LMs",
            "performance_metrics": "Survey does not provide concrete numerical metrics for these KG-to-text pretraining pipelines; they are described as used in pretraining and improving downstream KG tasks, but no numbers are reported in this survey.",
            "comparison_to_others": "Compared to whole-graph training, sampling/retrieval improves scalability; compared to pure textual LMs, graph-aware pretraining yields better structured reasoning capabilities (qualitative statement in survey, no numeric comparisons).",
            "advantages": "Scalable pretraining via subgraph sampling; enables mask-then-predict objectives aligned with LM training; preserves local structural context and relation types.",
            "disadvantages": "May miss global context if subgraphs are too small; random-walk sampling can bias training towards high-degree nodes and over-represent local structures.",
            "failure_cases": "Survey notes possible limitations in capturing high-order/global reasoning when only local subgraphs are used; no specific failure-case experiments reported in the survey.",
            "uuid": "e8797.5",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Textual-edges tokenization / Edgeformer-N/E",
            "name_full": "Edgeformer variants for textual-edge modeling (Edgeformer-E, Edgeformer-N)",
            "brief_description": "When edges carry text (reviews, answers), convert edge text to tokens and integrate edge-token self-attention with node tokens; two variants focus on edge- or node-centric processing of textual edges.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "Textual-edge tokenization and ego-graph aggregation (Edgeformer variants)",
            "representation_description": "Edge text is tokenized and embedded; Edgeformer-E processes each edge token jointly with its incident node tokens via self-attention; Edgeformer-N constructs an ego-graph for each node, models incident edges with Edgeformer-E, then aggregates edge-derived features to produce node embeddings.",
            "graph_type": "Textual graphs where edges contain substantial text (user-item reviews, question-answer pairs, KG edges with textual descriptions)",
            "conversion_method": "Tokenize edge textual content (standard NLP tokenization); for each node, form an ego-graph including incident edge tokens; run self-attention cross between node and edge tokens (edge-centric or node-centric pipelines) to produce final node/edge embeddings.",
            "downstream_task": "Link prediction, edge classification, node representation learning in text-rich graphs, recommendation",
            "performance_metrics": "No numeric performance values reported in the survey for these methods; described qualitatively as effective on textual-edge datasets.",
            "comparison_to_others": "Survey contrasts edge-token approaches with treating edge text as side features or attention biases; edge-tokenization gives a more direct modeling path for rich edge content.",
            "advantages": "Enables direct modeling of rich textual content on edges and its interplay with nodes; captures contextual semantics of interactions (e.g., reviews) jointly with graph structure.",
            "disadvantages": "Potentially large token budgets when edges contain long texts; requires careful aggregation to relate edge tokens back to node-level predictions.",
            "failure_cases": "Survey does not enumerate explicit failure cases; scalability and explosion of tokens for dense text edges flagged as main concerns.",
            "uuid": "e8797.6",
            "source_info": {
                "paper_title": "A Survey of Graph Transformers: Architectures, Theories and Applications",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pure transformers are powerful graph learners",
            "rating": 2,
            "sanitized_title": "pure_transformers_are_powerful_graph_learners"
        },
        {
            "paper_title": "Do transformers really perform badly for graph representation?",
            "rating": 2,
            "sanitized_title": "do_transformers_really_perform_badly_for_graph_representation"
        },
        {
            "paper_title": "Edgeformers",
            "rating": 1,
            "sanitized_title": "edgeformers"
        },
        {
            "paper_title": "KGTransformer",
            "rating": 1,
            "sanitized_title": "kgtransformer"
        },
        {
            "paper_title": "Relphormer",
            "rating": 1,
            "sanitized_title": "relphormer"
        }
    ],
    "cost": 0.02052525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Survey of Graph Transformers: Architectures, Theories and Applications</h1>
<p>Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong</p>
<h4>Abstract</h4>
<p>Graph Transformers (GTs) have demonstrated a strong capability in modeling graph structures by addressing the intrinsic limitations of graph neural networks (GNNs), such as over-smoothing and over-squashing. Recent studies have proposed diverse architectures, enhanced explainability, and practical applications for Graph Transformers. In light of these rapid developments, we conduct a comprehensive review of Graph Transformers, covering aspects such as their architectures, theoretical foundations, and applications within this survey. We categorize the architecture of Graph Transformers according to their strategies for processing structural information, including graph tokenization, positional encoding, structure-aware attention and model ensemble. Furthermore, from the theoretical perspective, we examine the expressivity of Graph Transformers in various discussed architectures and contrast them with other advanced graph learning algorithms to discover the connections. Furthermore, we provide a summary of the practical applications where Graph Transformers have been utilized, such as molecule, protein, language, vision, traffic, brain and material data. At the end of this survey, we will discuss the current challenges and prospective directions in Graph Transformers for potential future research.</p>
<p>Index TermsGraph Transformers, Graph Neural Networks.</p>
<h2>I. INTRODUCTION</h2>
<p>Graph data, a non-Euclidean data structure, is commonly found in various real-world applications, including molecular data, protein interactions, and social networks. Recently, Graph Neural Networks (GNNs) [1], [2] demonstrate impressive capabilities in modeling such data. A representative paradigm to building GNNs is the message-passing, which iteratively aggregates the neighbours' information and updates the node embedding. Nonetheless, the message-passing paradigm encounters several intrinsic limitations, such as over-smoothing [3],[4] and over-squashing [5], making it challenging for GNNs to effectively capture the long-range dependencies within the graphs.</p>
<p>In another line of research, the Transformer model [6] has demonstrated remarkable performance across a range of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>modalities, including Natural Language Processing (NLP) [6], Computer Vision (CV) [7], and time-series analysis [8]. A notable advantage of the Transformer model is its ability to effectively capture long-range dependencies, making it an ideal solution for addressing the limitations inherent in GNNs. Graph Transformers (GTs) adapt Transformer architecture to handle both node embeddings and graph structures, demonstrating superior performance compared to message-passing GNNs on a variety of tasks, including the prediction of molecular properties [9], [10], [11], dynamics simulation [12], [13], [14], and graph generation [15].</p>
<p>In this survey, we conduct a systematic and comprehensive review of the recent advancements in GTs, examining the developments from the perspectives of architectures, theories, and applications. First, from the perspective of architecture, we categorize GTs into four categories based on their ways of integrating graph structures into Transformers. (1) Multilevel Graph Tokenization. These models utilize tokenization mechanisms to represent edges, subgraphs, and hops as structure-aware tokens, enabling the attention mechanism to effectively capture and learn intrinsic topological relationships. (2) Structural Positional Encoding. These models enhance positional encoding (PE), traditionally used to denote token sequence relationships, to elucidate the structural interrelations among tokens. (3) Structure-aware Attention Mechanisms. Since the attention matrix inherently captures the learned relationships between tokens, these models modify the attention matrix using graph-derived structural information to incorporate node interrelations. (4) Model Ensemble between GNNs and Transformers. In addition to the direct modification of the architectures of Transformer, another effective strategy employs message-passing GNNs to encode structural information, followed by the integration of GNNs with Transformers. We follow the taxonomy of the previous study [16] to organize this category. Additionally, we investigate two critical aspects of Graph Transformer architecture: scalability challenges in large-scale graph processing and specialized architectural designs for geometric graphs.</p>
<p>After reviewing the architectures of GTs, it is important to determine which architecture is more powerful in general or in a specific task, since different architectures include different inductive biases or model the graph in different granularity. To this end, we investigate the expressive ability of GTs. Specifically, we examine the studies that compare the expressivity of GTs using the Weisfeiler-Lehman Test. These theoretical insights will enhance our understanding of how each component contributes to GTs' ability to learn graph</p>
<p>data. Additionally, we discuss the relationships between GTs and other current graph learning algorithms, which can further elucidate the strengths and weaknesses of GTs.</p>
<p>Furthermore, to review the GTs tailored for specific tasks, we thoroughly investigate their applications across eight types of graph data: molecules, proteins, text, social networks, traffic systems, vision, brain and material graphs. For each graph type, we review the different learning tasks, along with the respective datasets and methodologies. Additionally, applying GTs to specific tasks might necessitate an additional pretraining strategy or optimization objective, such as diffusion. These aspects are also incorporated into our review. In the last part of this survey, we will explore the potential future directions of GT.</p>
<p>This survey aims to provide a thorough analysis of GTs from multiple perspectives, including their model architectures, expressivity, and applications. While recent reviews have mainly focused on architectural aspects [16], [17], our work distinguishes itself within three aspects: (1) comprehensive architectural insights, (2) systematic analysis of theoretical expressivity of GTs, (3) extensive investigation of cross-domain applications. We also explore prospective research directions with recent advance in GTs.</p>
<p>Roadmap. The rest of this paper is organized as follows. Section II introduces the preliminaries of the survey. Section III reviews the four categories of GTs from the perspective of architecture. In Section IV, we summarize the expressive capability of GTs and connect them to graph learning methodologies. Section V reviews the applications of GTs. We point out the future research directions in Section VI and conclude the paper in Section VII.</p>
<h2>II. Preliminaries</h2>
<p>In this section, we provide a concise overview of the essential notations pertaining to GTs, as well as a summary of the fundamental architecture of the vanilla Transformer and message passing neural networks. Table I lists the frequentlyused notation throughout the paper.</p>
<p>We denote a graph as $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ represents the set of nodes and $\mathcal{E}$ represents the set of edges. The node set $\mathcal{V}$ comprises $n$ nodes, with the feature matrix $\mathbf{H} \in \mathbb{R}^{n \times d}$, where $n$ is the number of nodes and $d$ is the dimension of the node features. The edge set $\mathcal{E}$ corresponds to an adjacency matrix $\mathbf{A}^{\mathbf{G}} \in \mathbb{R}^{n \times n}$, where $\mathbf{A}<em u_="u," v="v">{u, v}^{\mathbf{G}}=1$ if there exists an edge $(u, v)$ in $\mathcal{E}$, and $\mathbf{A}</em>=0$ otherwise.}^{\mathbf{G}</p>
<p>The aforementioned notations are adequate for representing a basic graph with an adjacency matrix and node features. In more challenging scenarios, graphs are associated with more features to support various applications 1): Geometric Graph: In addition to node property features, the nodes will have coordinate features $\overrightarrow{\mathbf{X}} \in \mathbb{R}^{3}$, which will be independently handled to achieve invariance or equivariance properties. 2): Graph with edge features: Rather than merely converting the edge information into an adjacency matrix, the edges offer supplementary information that denote more specific relationships between the nodes. 3): Dynamic graph: The graph additionally contains the time domain $\mathcal{T}$ compared with</p>
<p>TABLE I
THE BASIC NOTATION UTILIZED IN THIS SURVEY.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Notation</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\mathcal{G}=(\mathcal{V}, \mathcal{E})$</td>
<td style="text-align: center;">A graph $\mathcal{G}$ contains node set $\mathcal{V}$ and edge set $\mathcal{E}$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{T})$</td>
<td style="text-align: center;">A dynamic graph $\mathcal{G}$ with varying node set $\mathcal{V}$ and edge set $\mathcal{E}$ in temporal domain $\mathcal{T}$.</td>
</tr>
<tr>
<td style="text-align: center;">$\overrightarrow{\mathcal{G}}=(\mathcal{V}, \mathcal{E}, \overrightarrow{\mathbf{X}})$</td>
<td style="text-align: center;">A geometric graph $\mathcal{G}$ with node set $\mathcal{V}$ and edge set $\mathcal{E}$, also including the 3D coordinates matrix for node set $\overrightarrow{\mathbf{X}} \in \mathbb{R}^{n \times 3}$.</td>
</tr>
<tr>
<td style="text-align: center;">$n \in \mathbb{R}$</td>
<td style="text-align: center;">The number of nodes in the graph.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{H} \in \mathbb{R}^{n \times d}$</td>
<td style="text-align: center;">The node features in the graph.</td>
</tr>
<tr>
<td style="text-align: center;">$\overrightarrow{\mathbf{H}} \in \mathbb{R}^{n \times d}$</td>
<td style="text-align: center;">The updated node representations of the graph.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{A} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The attention matrix learn in the Transformer.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{A}^{\mathbf{G}} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The adjacency matrix in the graph.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{m}_{i j}$</td>
<td style="text-align: center;">The message from node $i$ to node $j$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{e}_{i j}$</td>
<td style="text-align: center;">The edge feature between node $i$ and node $j$.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{N}_{i}$</td>
<td style="text-align: center;">The neighboring nodes of node $i$.</td>
</tr>
<tr>
<td style="text-align: center;">$\phi, \mathbf{W}, \mathbf{b}$</td>
<td style="text-align: center;">The learnable parameters in neural networks.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{Q}, \mathbf{K}, \mathbf{V}$</td>
<td style="text-align: center;">The query, key and value matrices in attention.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{D} \in \mathbb{R}^{n \times 1}$</td>
<td style="text-align: center;">The degree matrix, indicating the degree of the nodes.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{U} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The eigenvector matrix after Laplacian decomposition.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{A} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The diagonal eigenvalue matrix after Laplacian decomposition.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{L} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The Laplacian matrix of the graph.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{P} \in \mathbb{R}^{n \times k}$</td>
<td style="text-align: center;">The absolute PE in Graph Transformer, where $k \in \mathbb{R}$ is a hyper-parameter.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{M} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The graph kernel.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{L}$</td>
<td style="text-align: center;">The loss function.</td>
</tr>
<tr>
<td style="text-align: center;">$\mathbf{I} \in \mathbb{R}^{n \times n}$</td>
<td style="text-align: center;">The identity matrix.</td>
</tr>
<tr>
<td style="text-align: center;">$|$</td>
<td style="text-align: center;">Concatenate operation.</td>
</tr>
<tr>
<td style="text-align: center;">$\odot$</td>
<td style="text-align: center;">Element-wise production.</td>
</tr>
<tr>
<td style="text-align: center;">$</td>
<td style="text-align: center;">\cdot</td>
</tr>
</tbody>
</table>
<p>static graphs, represented as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathcal{T})$. This indicates that the nodes and edges can change over time. The graph is denoted as $\mathcal{G}=\left{\left(v_{i}, v_{j}, \tau\right)<em i="i">{n}, n=1,2, \ldots,|\mathcal{E}|\right}$, where each tuple $\left(v</em>$ specifies the edge set for that particular timestep.}, v_{j}, \tau\right)$ represents an edge between node $v_{i}$ and node $v_{j}$ at a specific time $\tau \in \mathcal{T}$ and $\mathcal{E</p>
<p>Dynamic graphs are generally classified into two categories: discrete-time dynamic graphs (DTDGs) and continuous-time dynamic graphs (CTDGs). In DTDGs, the time domain $\tau \in \mathcal{T}$ is divided into $k$ discrete time intervals, denoted as $\tau=\left[t_{1}: t_{k}\right]$. This allows the dynamic graph $\mathcal{G}$ to be represented as a sequence of individual graph snapshots $\mathcal{G}=\left[\mathbf{G}<em 1="1">{t</em>}}, \mathbf{G<em 2="2">{t</em>}}, \ldots, \mathbf{G<em k="k">{t</em>\right]$. In contrast, CTDGs model scenarios where the graph evolves continuously over time, providing the exact graph structure at any given timestamp $t$. In CTDG, changes to the graph structure are triggered only by certain events $\tau$.}</p>
<p>Message Passing Graph Neural Networks: Graph neural networks [1] are a foundational framework for learning representations in graphs via the message passing mechanism. Given node embeddings $\mathbf{H}$ and the adjacency matrix $\mathbf{A}^{\mathbf{G}}$, a message passing neural network (MPNN) $\phi_{\theta}$ updates node embeddings via the propagating information from neighboring</p>
<p>nodes. The process of modeling node embeddings in an MPNN can be formally expressed as:</p>
<p>$$
\begin{gathered}
\mathbf{m}<em g="g" m="m" s="s">{i j}=\phi</em>}\left(\mathbf{H<em j="j">{i}, \mathbf{H}</em>}, \mathbf{e<em i="i">{i j}\right) \
\mathbf{H}</em>}=\phi_{u p d}\left(\mathbf{H<em i="i" j="j">{i},\left{\mathbf{m}</em>\right}<em i="i">{j \in \mathcal{N}</em>\right)
\end{gathered}
$$}</p>
<p>where $\mathbf{H}$ and $\mathbf{e}$ represent node and edge embeddings, respectively. Here, $\phi_{m s g}(\cdot)$ and $\phi_{u p d}(\cdot)$ are two transformations in the MPNN $\phi_{\theta}$. Specifically, $\phi_{m s g}(\cdot)$ computes the message received by node $v_{i}$ from its neighbors, while $\phi_{u p d}(\cdot)$ updates the node embedding by the messages.</p>
<p>Transformers: Transformers have demonstrated remarkable success across diverse fields, achieving high performance in natural language processing [18] and computer vision [7] applications. Transformers employ an encoder-decoder architecture, where the building block is the self-attention [6] mechanism. The self-attention module operates on a sequence of $n$ tokens, $\mathbf{H} \in \mathbb{R}^{n \times d}$, which can be represented by a transformation function $\phi_{\theta}(\cdot): \mathbb{R}^{n \times d} \rightarrow \mathbb{R}^{n \times d}$, where $d$ is the feature dimensions of each token. The formulation of function $\phi_{\theta}(\cdot)$ is shown in Equation (3)-(5):</p>
<p>$$
\begin{aligned}
&amp; \mathbf{Q}=\mathbf{H} \mathbf{W}<em K="K">{Q}, \mathbf{K}=\mathbf{H} \mathbf{W}</em> \
&amp; \mathbf{A}=\operatorname{Softmax}\left(\frac{\mathbf{Q} \mathbf{K}^{T}}{\sqrt{d}}\right) \
&amp; \tilde{\mathbf{H}}=\mathbf{A V}
\end{aligned}
$$}, \mathbf{V}=\mathbf{H} \mathbf{W}_{V</p>
<p>where the input $\mathbf{H}$ is first transformed into query, key and value matrices, $\mathbf{Q}, \mathbf{K}, \mathbf{V} \in \mathbb{R}^{d \times d}$, by linear wight matrices $\mathbf{W}<em K="K">{Q}, \mathbf{W}</em>}, \mathbf{W<em i="i" j="j">{V} \in \mathbb{R}^{d \times d}$, respectively. Then, the attention matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$ is computed by the inner product of the query and key, followed by the normalization of a $\operatorname{Softmax}(\cdot)$ function. Here, $\mathbf{A}</em>} \in[0,1]$ indicates the influence of $\mathbf{H<em j="j">{j}$ on $\mathbf{H}</em>$ in Equation (4) as an alternative adjacency matrix, the self-attention mechanism can be viewed as a variant of MPNNs with a fully connected adjacency matrix as Equation (6)-(7):}$. The attention matrix is finally applied to the value matrix to generate new embeddings of the tokens $\tilde{\mathbf{H}} \in \mathbb{R}^{n \times d}$. By interpreting the attention matrix $\mathbf{A</p>
<p>$$
\begin{aligned}
\mathbf{m}<em i="i" j="j">{i j} &amp; =\mathbf{A}</em>} \mathbf{V<em i="i">{j} \
\mathbf{H}</em>
\end{aligned}
$$} &amp; =\sum_{i=0}^{n} \mathbf{m}_{i j</p>
<p>After applying self-attention, Transformers employ elementwise feed-forward networks (FFN), which comprise two linear layers with ReLU activation:</p>
<p>$$
\operatorname{FFN}(\tilde{\mathbf{H}})=\max \left(0, \tilde{\mathbf{H}} \mathbf{W}<em 1="1">{1}+b</em>}\right) \mathbf{W<em 2="2">{2}+b</em>
$$</p>
<p>where $\mathbf{W}<em 1="1">{1} \in \mathbb{R}^{d \times d</em>}}, \mathbf{W<em 1="1">{2} \in \mathbb{R}^{d</em>$ are learnable parameters. In addition, the sublayers in the FFN will incorporate residual connections and layer normalization [19].} \times d</p>
<p>In practice, multi-head attention (MHA) is widely utilized in Transformers to enhance representation learning. Precisely, the $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ matrices obtained from Equation (3) are divided into $H$ independent heads, denoted as $\mathbf{Q}^{(h)}, \mathbf{K}^{(h)}$ and $\mathbf{V}^{(h)}$,</p>
<p>TABLE II
A SUMMARY OF EXISTING ARCHITECTURE OF GTS. FOR EACH MODEL, WE DOCUMENT THE GRAPH TOKENIZATION, THE IMPLEMENTATION OF POSITIONAL ENCODING, THE UTILIZATION OF STRUCTURE-AWARE ATTENTION, AND THE INCLUSION OF AN ADDITIONAL MPNN. Abbc, PE: POSITIONAL ENCODING, ATTN: ATTENTION, ENS: ENSEMBLE, SUB.: SubGraph, K-hop: K-hop Neighbor, RW: Random Walk, Deg.: Degree, LAP: Laplacian.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Token</th>
<th style="text-align: center;">PE</th>
<th style="text-align: center;">Attn</th>
<th style="text-align: center;">Ens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SE(3)-Transformer [12]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Graphormer [10]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Deg.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Dwivedi et al. [22]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Lap.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SAN [5]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Lap.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GraphiT [23]</td>
<td style="text-align: center;">Sub.</td>
<td style="text-align: center;">RW</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TokenGT [24]</td>
<td style="text-align: center;">Edge</td>
<td style="text-align: center;">Lap.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SAT [25]</td>
<td style="text-align: center;">Sub.</td>
<td style="text-align: center;">Hybrid</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TorchMD-Net [11]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Mask-transformer [26]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GraphGPS [27]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Hybrid</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">GKAT [28]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NodeFormer [29]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EGT [30]</td>
<td style="text-align: center;">Edge</td>
<td style="text-align: center;">SVD</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Exphormer [31]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Lap.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NAGphormer [32]</td>
<td style="text-align: center;">K-hop</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GRIT [33]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Deg.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Graph ViT/MLP-Mixer [34]</td>
<td style="text-align: center;">Sub.</td>
<td style="text-align: center;">RW</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SGFormer [35]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Graphormer-GD [36]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Equiformer [14]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">EquiformerV2 [13]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Polynormer [37]</td>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Lap.</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoBFormer [38]</td>
<td style="text-align: center;">Sub.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">TGT [39]</td>
<td style="text-align: center;">Edge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GraphGPT [40]</td>
<td style="text-align: center;">Edge</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>respectively. The MHA mechanism computes representations as Equation (9):</p>
<p>$$
\tilde{\mathbf{H}}=\left.\right|<em h="h">{h=1} ^{H} \operatorname{Softmax}\left(\frac{\mathbf{Q}^{(h)} \mathbf{K}^{(h) T}}{\sqrt{d</em>
$$}}}\right) \mathbf{V}^{(h)</p>
<p>where $d_{h}=d / H$ represents the dimension assigned to each head. MHA acquires representations from various perspectives within each head and concatenates the embeddings from each head to the final representation.</p>
<p>Another critical component in Transformers is positional encoding (PE), which is designed to encode the relative position of tokens in a sequential input. The original paper of Transformer [6] introduces a parameter-free sinusoidal PE. Nevertheless, subsequent advancements, such as learned PE [20] and rotary PE [21], have demonstrated that the choice of PE significantly impacts the performance of Transformers, highlighting its importance in Transformers.</p>
<h2>III. ARCHITECTURES</h2>
<p>Vanilla Transformer is essentially a special GNN, where self-attention mechanisms across all nodes are operated in a</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. The overview of the architecture of Graph Transformers. The general GT part outlines the various methodologies employed to incorporate structural priors within GTs, including multi-level tokenization, positional encoding, modifying attention matrix and ensemble with GNNs. Other four parts delineate how these methodologies are applied to GT. Methods in the parentheses are representative implementations in their corresponding taxonomies.</p>
<p>fully-connected graph. Since Transformers ignore the original graph structure, the core objective of GT is to incorporate edge information into Transformer architecture. Based on the approaches of incorporating this structural prior, in this section, we systematically categorize existing GTs into four classes: 1) Multi-level Graph Tokenization (Section III-A). 2) Structural Positional Encoding (Section III-B). 3) Structure-aware Attention Mechanisms (Section III-C). 4) Model Ensemble between GNNs and Transformers (Section III-D). Following the categorization, Table II summarizes representative literature, and Figure 1 illustrates a conceptual overview of the architectures. Apart from novel model architectures, practical applications of GTs necessitate high scalability or equivariance. Therefore, we also discuss the advanced progress in enhancing scalability (Section III-E) and achieving equivariance (Section III-F) for GTs.</p>
<h3><em>A. Multi-level Graph Tokenization</em></h3>
<p>In the realm of GTs, tokenization plays a crucial role in transforming graph data into a sequential format for processing by Transformer architectures. Distinguished from conventional text-based Transformers where tokenization is trivial, graph data presents unique challenges due to its structural complexity. This section explores four distinct levels of tokenization in GTs from fine-grained to coarse-grained level, i.e., node-level, edge-level, hop-level, and subgraph-level tokenization.</p>
<p><em>1) Node-level Tokenization:</em> Node-level tokenization is the most granular approach for tokenization [10], [41], which treats each node in the graph as an individual token. Furthermore, the node involved in attention can be selected through methods such as contrastive learning [42]. This approach is particularly effective when models focus on node-specific features or when the graph's topology is less critical than the attributes of individual nodes. By capturing detailed information about each node, node-level tokenization is well-suited for tasks such as node classification.</p>
<p><em>2) Edge-level Tokenization:</em> Edge-level tokenization extends the concept of tokenization to the connections between nodes [24], [40]. Here, each edge in the graph is treated as a token, making this approach ideal for tasks where the relationships or interactions between nodes are of primary interest. Edge-level tokenization can capture the dynamics of these interactions, which is essential for tasks like link prediction or understanding the flow of information across the graph. By focusing on edges, edge-level tokenization can highlight the importance of connectivity patterns in the graph.</p>
<p><em>3) Subgraph-level Tokenization:</em> Subgraph-level tokenization treats the entire k-hop subgraph centered on a node as the token representation of this node [25], which is highly effective for capturing the contextual structure and patterns surrounding the nodes. This approach is suitable for tasks like graph classification or regression, as it can effectively encode substructures, clusters, or other higher-order structures within the graph. Notably, the nodes in subgraphs corresponding to different tokens are allowed to overlap, enabling a more flexible and comprehensive representation of the graph.</p>
<p><em>4) Hop-level Tokenization:</em> Hop-level tokenization extends subgraph-level tokenization by considering the number of hops and improving scalability [32], [43], [44]. Unlike subgraph-level tokenization, which is restricted to a single specific hop,</p>
<p>hop-level tokenization constructs token sequences based on subgraphs spanning multiple hops, enabling the learning of expressive node representations. Moreover, instead of performing quadratic-complexity self-attention across all nodes, hop-level tokenization applies self-attention to each node's subgraph sequences. This design enables mini-batch training and makes the model scalable to large graphs.</p>
<h2>B. Structural Positional Encoding</h2>
<p>In standard Transformers, the positional encoding (PE) module indicates the position of tokens in a sequence. To extend this module to GTs, it is natural to develop methods for representing the positional embeddings of nodes in a graph. Since edge-level, hop-level, and subgraph-level tokenization approaches already incorporate structural information, in this section, we assume that the PE methods are applied at the node-level.</p>
<p>Existing PE methods can be categorized into absolute PE and relative PE. Similar to standard Transformers, absolute PE assigns a unique positional embedding to each node. This embedding, either learned or parameter-free, is subsequently aggregated or concatenated with the original embedding of the node. In contrast, relative PE focuses on capturing pairwise relationships between nodes and directly applies to the attention matrix. Therefore, we defer the discussion on relative PE to the next section.</p>
<p>The main objective of absolute PE can be formulated as utilizing a function $f$ to extract the underlying structural information from the graph, typically from the adjacency matrix A. Equation (10)-(11) show the usage of absolute PE:</p>
<p>$$
\begin{aligned}
&amp; \mathbf{P}=f(\mathbf{A}) \
&amp; \hat{\mathbf{H}}=g(\mathbf{H}, \mathbf{P})
\end{aligned}
$$</p>
<p>Here, the function $f$ extracts the absolute PE $\mathbf{P} \in \mathbb{R}^{n \times d_{p}}$, where $n$ denotes the number of nodes, and $d_{p}$ represents the dimension of the positional embedding for each node. The function $g$ integrates absolute PE with the original node features $\mathbf{H}$, either by concatenation or by employing an MLP to align the dimensions of $\mathbf{P}$ and $\mathbf{H}$ before summing them.</p>
<p>Laplacian PE leverages the eigenvectors and eigenvalues obtained from the decomposition of the Laplacian matrix as PE. The decomposition of the Laplacian can be expressed as Equation (12):</p>
<p>$$
\mathbf{U}^{T} \boldsymbol{\Lambda} \mathbf{U}=\mathbf{I}-\mathbf{D}^{-1 / 2} \mathbf{A} \mathbf{D}^{-1 / 2}
$$</p>
<p>where $\mathbf{D}$ denotes the degree matrix, $\mathbf{I}$ is the identity matrix, $\boldsymbol{\Lambda}$ and $\mathbf{U}$ represents eigenvalues arranged in a diagonal matrix and eigenvectors. Since the sign of pre-computed eigenvectors is arbitrary, the Laplacian PE approaches randomly adjusts the sign of eigenvectors during the training stage. The first Laplacian PE [22] proposes to utilize the $k$ smallest nontrivial eigenvectors of a node as its PE. Another work SAN [5] introduces a learned Laplacian PE. For a given node $v_{j}$, SAN uses $\left{\lambda_{i}, \mathbf{U}<em i="0">{i j}\right}</em>$, where $m$ is a hyperparameter determining the number of eigenvectors considered.}^{m}$ as input features for neural networks to learn the PE of node $v_{j</p>
<p>Despite the effectiveness of Laplacian PE, it faces two underlying challenges: 1) Non-unique Eigendecompositions. There are different eigendecompositions of the same Laplacian. If a vector $\mathbf{v}$ is an eigenvector, then $-\mathbf{v}$ is also an eigenvector. There are non-unique solutions for eigenvectors with the multiplicities of eigenvalues. 2) Sensitivity to Perturbations. Minor perturbations in the graph structure can significantly affect the result of eigenvectors, leading to considerable instability in Laplacian PE.</p>
<p>To address the first challenge in Laplacian PE, SignNet [45] introduces a sign-invariant network, $f$, which operates on eigenvectors as Equation (13):</p>
<p>$$
f\left(\mathbf{U}<em k="k">{1}, \ldots, \mathbf{U}</em>\right)=\rho\left(\left|<em i="i">{i=1}^{k}\left[\phi\left(\mathbf{U}</em>\right)\right]\right)
$$}\right)+\phi\left(-\mathbf{U}_{i</p>
<p>where $\rho$ and $\phi$ are neural networks. This formulation ensures that the neural network remains invariant embeddings to the sign of the eigenvectors. Moreover, to tackle the occurrence of multiple eigenvector choices when there are repeated eigenvalues in the Laplacian matrix, BasisNet [45] proposes a method to extract consistent PE from these matrices.</p>
<p>To address the challenge of stability in Laplacian PE, Stable and Expressive PE (SPE) [46] is introduced, which is formulated as Equation (14):</p>
<p>$$
\begin{aligned}
&amp; \mathbf{P}(\mathbf{U}, \boldsymbol{\Lambda})=\rho\left(\mathbf{U}\left(\phi_{1}(\boldsymbol{\Lambda})\right) \mathbf{U}^{\top}, \mathbf{U}\left(\phi_{2}(\boldsymbol{\Lambda})\right) \mathbf{U}^{\top}, \ldots\right. \
&amp; \left.\mathbf{U}\left(\phi_{m}(\boldsymbol{\Lambda})\right) \mathbf{U}^{\top}\right)
\end{aligned}
$$</p>
<p>where the input consists of the $k$ smallest eigenvalues $\lambda$ and their corresponding eigenvectors $\mathbf{V}$. Rather than implementing a strict division of eigensubspaces, SPE utilizes a weighted aggregation of eigenvectors that is contingent upon the eigenvalues to ensure stability.</p>
<p>Singular Value Decomposition (SVD) PE [30] provides a broader scope of applications compared to Laplacian PE, as it can handle directed and weighted graphs flexibly. The SVD PE is computed by Equation (15)-(16):</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A} \stackrel{\mathrm{SVD}}{\approx} \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{T}=(\mathbf{U} \sqrt{\Sigma}) \cdot(\mathbf{V} \sqrt{\Sigma})^{T}=\hat{\mathbf{U}} \hat{\mathbf{V}}^{T} \
&amp; \mathbf{P}=\hat{\mathbf{U}} | \hat{\mathbf{V}}
\end{aligned}
$$</p>
<p>where $\mathbf{U}, \mathbf{V} \in \mathbb{R}^{n \times r}$ contain the $r$ left and right singular vectors as their respective columns, each associated with the highest $r$ singular values in the diagonal matrix $\boldsymbol{\Sigma} \in \mathbb{R}^{r \times r}$. Similar to Laplacian PE, SVD PE involves the random sign flipping of eigenvectors during the training phase. Consequently, building on the concept of SignNet, developing a sign-invariant SVD PE could be a potential direction for future research.</p>
<p>Random Walk PE (RWPE) [47] represents a PE derived from the diffusion process of a random walk. The RWPE for a node $v_{i}$ can be mathematically expressed in Equation (17) through a k-step random walk:</p>
<p>$$
\mathbf{P}<em i="i">{i}=\left[\mathbf{M}</em>}, \mathbf{M<em i="i">{i i}^{2}, \cdots, \mathbf{M}</em>
$$}^{k}\right] \in \mathbb{R}^{k</p>
<p>where $\mathbf{M}=\mathbf{A D}^{-1}$ represents the random walk operator. Distinguished from Laplacian PE, RWPE does not suffer the sign ambiguity. Under the condition that each node possesses a unique $k$-hop topological neighborhood for a sufficiently</p>
<p>large $k$, RWPE provides a distinct node representation. As a potential future study, researchers can explore the replacing of random walk diffusion with alternative graph diffusion processes to derive PE.</p>
<p>Graphormer [10] introduces a heuristic method that leverages node degrees for centrality encoding. Specifically, each node is assigned a learnable vector based on its degree, which is then incorporated into the node features as the input layer of Equation (18)</p>
<p>$$
h_{i}^{(0)}=x_{i}+z_{\operatorname{deg}^{-}\left(v_{i}\right)}^{-}+z_{\operatorname{deg}^{+}\left(v_{i}\right)}^{+}
$$</p>
<p>where $z^{-}, z^{+} \in \mathbb{R}^{d}$ represent learnable embedding vectors defined respectively by the indegree $\operatorname{deg}^{-}\left(v_{i}\right)$ and the outdegree $\operatorname{deg}^{+}\left(v_{i}\right)$, respectively. For undirected graphs, $\operatorname{deg}^{-}\left(v_{i}\right)$ and $\operatorname{deg}^{+}\left(v_{i}\right)$ are simplified to $\operatorname{deg}\left(v_{i}\right)$. By incorporating centrality encoding into the query and key components of the attention mechanism, Graphormer enhances the ability of attention to effectively recognize both the importance and relationships among nodes.</p>
<p>The degree information can also be injected as postprocessing. For instance, GRIT [33] updates the node representation after Transformer via Equation (19):</p>
<p>$$
\mathbf{x}<em i="i">{i}^{\text {out }^{\prime}}:=\mathbf{x}</em>}^{\text {out }} \odot \boldsymbol{\theta<em i="i">{1}+\left(\log \left(1+\mathbf{D}</em>}\right) \cdot \mathbf{x<em 2="2">{i}^{\text {out }} \odot \boldsymbol{\theta}</em>\right)
$$</p>
<p>where $\boldsymbol{\theta}<em 2="2">{1}, \boldsymbol{\theta}</em>$ are learnable weights. Similarly, SAT [25] also incorporates degree information into the residue connection as Equation (20):</p>
<p>$$
\mathbf{x}<em i="i">{i}^{\text {out }^{\prime}}=\mathbf{x}</em>}+1 / \sqrt{\mathbf{D<em i="i">{i}} \mathbf{x}</em>
$$}^{\text {out }</p>
<p>In the following, we elaborate on structure-aware attention mechanisms, which contain the complete implementation of relative PE.</p>
<h2>C. Structure-aware Attention Mechanisms</h2>
<p>In Transformer blocks, the attention matrix governs the interactions between nodes, while tokenization and absolute PE augment node embeddings. These augmented embeddings enable Transformers to incorporate structural prior into attention mechanisms. In this vein, direct modification of the attention matrix is a more forthright approach for capturing structural inductive bias. In this section, for clarity of the paper, we present all structure-aware attention mechanisms in their single-head formulation.</p>
<p>Adjusting the attention matrix begins with capturing pairwise node interactions in the graph. To this end, GT leverages the graph structure to first generate a structure matrix that encodes node connectivity patterns. This structure matrix can be integrated into the attention matrix by three ways: by attention bias, by attention mask and by edge-level tokenization. In the rest of this section, we elaborate on the three ways respectively.</p>
<p>1) Structure Matrix as Attention Bias: By attention bias, the structural information is incorporated into the attention mechanism by adding a bias matrix $\mathbf{b} \in \mathbb{R}^{n \times n}$ into the inner product of the query and key matrices. Equation (21) shows a general form for computing the attention matrix:</p>
<p>$$
\mathbf{A}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H} \mathbf{W}<em K="K">{Q}\right)\left(\mathbf{H} \mathbf{W}</em>\right)
$$}\right)^{T}}{\sqrt{d}}+\mathbf{b</p>
<p>where the attention bias $\mathbf{b}$ is specified by different approaches, which is essentially relative PE. Relative PE is computed from the graph structure, aiming to understand pair-wise interactions between nodes. We define a relation matrix $\hat{\mathbf{P}} \in \mathbb{R}^{R \times R}$ as the attention bias $\mathbf{b} . \mathbf{P}<em i="i">{i j}$ is determined by the function $\phi\left(\mathbf{H}</em>}, \mathbf{H<em i="i" j="j">{j}, e</em>}\right)$, which encodes the relationships between any pair of nodes, utilizing their embeddings $\mathbf{H<em j="j">{i}, \mathbf{H}</em>$.}$, and optionally incorporating edge embedding $e_{i j</p>
<p>Graphormer [10] introduces the shortest path distance (SPD) of a shortest path $\mathrm{SP}<em 1="1">{i j}=\left[e</em>}, e_{2}, \ldots, e_{N}\right]$ connecting $v_{i}$ to $v_{j}$ into the attention mechanism. Graphormer incorporates two types of attention bias. The first spatial bias $\phi_{\mid} v_{i}, v_{j}$ ) encodes the length of $\mathrm{SP<em i="i" j="j">{i j}$ and the second, edge encoding $c</em>$. Consequently, the attention mechanism that incorporates structural information can be expressed by Equation (22):}$, is to aggregate the edge embeddings in $\mathrm{SP}_{i j</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}<em i="i">{i j}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H}</em>} \mathbf{W<em j="j">{Q}\right)\left(\mathbf{H}</em>} \mathbf{W<em _phi_left_v__i="\phi\left(v_{i">{K}\right)^{T}}{\sqrt{d}}+b</em>\right) \
&amp; c_{i j}=\frac{1}{N} \sum_{n=1}^{N} x_{e_{n}}\left(w_{n}^{E}\right)^{T}
\end{aligned}
$$}, v_{j}\right)}+c_{i j</p>
<p>where $b_{\phi\left(v_{i}, v_{j}\right)}$ is a learnable scalar indexed by $\phi\left(v_{i}, v_{j}\right)$ and remains consistent across all layers. $x_{e_{n}}$ and $w_{n}^{E}$ denote the feature of the $n$-th edge $e_{n}$ in $\mathrm{SP}_{i j}$ and its corresponding weight, respectively.</p>
<p>Nevertheless, Graphormer-GD [36] identifies that SPD is incapable to adequately distinguish certain perturbations in the graph structure. To address this limitation, GraphormerGD introduces a more robust relative PE based on resistance distance (RD). The attention matrix with this relative PE is represented in Equation (24):</p>
<p>$$
\mathbf{A}=\phi_{1}(\mathbf{D}) \odot \operatorname{Softmax}\left(\mathbf{H} \mathbf{W}<em K="K">{Q}\left(\mathbf{H} \mathbf{W}</em>)\right)
$$}\right)^{\top}+\phi_{2}(\mathbf{D</p>
<p>where $\mathbf{D} \in \mathbb{R}^{n \times n}$ represents the distance matrix with $\mathbf{D}<em i="i" j="j">{i j}=$ $\left{\left|\mathbf{S P}</em>\right}$. Theoretical analysis demonstrates that RD-WL exhibits superior discriminative power compared to SPD-WL, for differentiating non-isomorphic distance-regular graphs.}\right|, \mathbf{S P}_{i j</p>
<p>GRIT [33] introduces an approach to learning relative PE by the initialization of random walk probabilities in Equation (25):</p>
<p>$$
\mathbf{P}<em i_="i," j="j">{i, j}=\left[\mathbf{I}, \mathbf{M}, \mathbf{M}^{2}, \ldots, \mathbf{M}^{K-1}\right]</em>
$$} \in \mathbb{R}^{K</p>
<p>where $\mathbf{M}=\mathbf{A D}^{-1}$ denotes the transition probability matrix of random walk. The initialization of $\mathbf{P}$, combined with MLP processing, is proven to approximate SPD. Additionally, graph-diffusion Weisfeiler-Lehman (GD-WL) with $\mathbf{P}$ is strictly better than GD-WL based on SPD.
2) Structure Matrix as Attention Mask: An alternative approach to incorporating structure-aware attention is to perform an element-wise multiplication between attention matrix and a masking matrix, instead of treating the structure matrix as an attention bias. This approach can be formally expressed as Equation (26):</p>
<p>$$
\mathbf{A}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H} \mathbf{W}<em K="K">{Q}\right)\left(\mathbf{H} \mathbf{W}</em>\right)
$$}\right)^{T}}{\sqrt{d}} \odot \mathbf{M</p>
<p>where $\mathbf{M} \in \mathbb{R}^{n \times n}$ represents the masking matrix, which can be the adjacency matrix or another matrix encoding the graph structure.</p>
<p>To integrate edge information into the mask matrix of a GT [22], $\mathbf{M}<em i="i" j="j">{i j}$ is defined based on the edge feature. If a connection exists, $\mathbf{M}</em>}=\mathbf{W<em i="i" j="j">{E} \mathbf{e}</em>}$, where $e_{i j}$ denotes the edge feature connecting node $v_{i}$ and node $v_{j}$, and $\mathbf{W<em i="i" j="j">{E}$ represents a learnable weight. $\mathbf{M}</em>$ are not connected. The formulation can be expressed as:}$ will be set to 1 if node $v_{i}$ and node $v_{j</p>
<p>$$
\mathbf{A}<em i="i">{i j}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H}</em>} \mathbf{W<em j="j">{Q}\right)\left(\mathbf{H}</em>\right)
$$} \mathbf{W}_{K}\right)^{T}}{\sqrt{d}} \odot \mathbf{M</p>
<p>This attention mask operates similarly to attention bias, as both of them only change the attention values between connected nodes.</p>
<p>Another classical choice for masked matrix $\mathbf{M}$ is adjacency matrix as shown in Equation (28), where $\mathbf{M}<em i="i">{i j}=0$ if nodes $v</em>$ are not connected.}$ and $v_{j</p>
<p>$$
\mathbf{A}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H} \mathbf{W}<em K="K">{Q}\right)\left(\mathbf{H} \mathbf{W}</em>\right)
$$}\right)^{T}}{\sqrt{d}} \odot \mathbf{A}^{G</p>
<p>By truncating attention values for disconnected nodes, the attention is forced to focus on local neighboring nodes. Although this may reduce a GT to a GNN regarding capturing the local neighborhood information, GMT [26] and HetGT [48] employ distinct attention masks across different heads, thereby compelling the GT to learn from different perspectives of the graph structure.</p>
<p>Similar to the attention bias, relative PE can also be used as attention masks. GraphiT [23] proposes positive definite kernels on graphs as relative PE for attention masks. Specifically, GraphiT exploits the diffusion kernel and the p-step random walk kernel, where the masking matrices are shown in Equation (29) and Equation (30) respectively.</p>
<p>$$
\begin{aligned}
&amp; \mathbf{M}=e^{-\beta \mathbf{L}} \
&amp; \mathbf{M}=(\mathbf{I}-\gamma \mathbf{L})^{p}
\end{aligned}
$$</p>
<p>where $\mathbf{L}$ is the Laplacian matrix, $\beta$ and $p$ are hyperparameters. GraphiT demonstrates the effectiveness of these relative PEs as attention masks across various datasets.</p>
<p>Despite the effectiveness of classic graph diffusion, scaling it with standard attention mechanism to large graph is challenging due to the quadratic complexity regarding the number of nodes. Furthermore, as attention masks, relative PEs are not directly applicable to low-rank linear Transformers which do not explicitly construct an attention matrix. To this end, GKAT [28] proposes a novel Random Walks Graph-Nodes Kernel (RWGNK) with sub-quadratic complexity. RWGNK operates as a low-rank masking directly on the query, key and value matrices in attention mechanism, bypassing the explicit computation of the attention matrix and thus avoiding quadratic complexity.</p>
<p>To unify attention bias and attention mask, EGT [30] designs a framework which incorporates both of them as a general formula in Equation (31):</p>
<p>$$
\mathbf{A}=\operatorname{Softmax}\left(\frac{\left(\mathbf{H} \mathbf{W}<em K="K">{Q}\right)\left(\mathbf{H} \mathbf{W}</em>}\right)^{T}}{\sqrt{d}}+\mathbf{E<em e="e">{e}\right) \odot \mathbf{G}</em>
$$</p>
<p>where $\mathbf{G}<em e="e">{e}, \mathbf{E}</em>$ are edge embeddings generated by linear transformations.
3) Edge-level Token as Attention Input: Edge-level attention can be exploited by two ways. The first way focuses on computing attention only by edge tokens to generate enhanced edge representations, which are subsequently fused with node embeddings using previously discussed techniques. The second way incorporates edge and node tokens simultaneously into the attention mechanism to develop structureaware attention. We briefly introduce the technical essence of the representatives by the first way, i.e., EGT [30] and TGT [39], and those by the second way, i.e., TokenGT [24] and Edgeformers [49] as below.} \in \mathbb{R}^{n \times n</p>
<p>EGT [30] introduces attention bias and mask from edges to nodes. In addition, the edge features can also be updated from the attention matrix in each layer as Equation (32):</p>
<p>$$
\mathbf{E}<em Q="Q">{e}=f\left(\frac{\left(\mathbf{H} \mathbf{W}</em>}\right)\left(\mathbf{H} \mathbf{W<em e="e">{K}\right)^{T}}{\sqrt{d}}+\mathbf{E}</em>\right)
$$</p>
<p>where $f$ is a learnable function with feed-forward layers and layer normalization.</p>
<p>TGT [39] employs triplet edge interactions to further update edge embeddings. Given that $\mathbf{e}<em i="i" k="k">{i j}, \mathbf{e}</em>}$ and $\mathbf{e<em i="i">{j k}$ denote the edge embeddings of the three edges $\left(v</em>}, v_{j}\right),\left(v_{i}, v_{k}\right)$ and $\left(v_{j}, v_{k}\right)$ in a triangle, the query, key and value vectors are computed by linear projections on $\mathbf{e<em i="i" k="k">{i j}, \mathbf{e}</em>$ respectively. Then, the triplet attention computes the attention matrix by Equation (33) and update edge embeddings by Equation (34).}$ and $\mathbf{e}_{j k</p>
<p>$$
\begin{aligned}
\mathbf{A}<em k="k">{i j k} &amp; =\operatorname{Softmax}</em>}\left(\frac{1}{\sqrt{d}} \mathbf{q<em j="j" k="k">{i j} \cdot \mathbf{k}</em>\right) \
\mathbf{e}}+b_{i k}\right) \times \sigma_{1}\left(g_{i k<em 2="2">{i j} &amp; =\sigma</em>}\left(\sum_{k=1}^{N} \mathbf{A<em j="j" k="k">{i j k} \mathbf{v}</em>\right)
\end{aligned}
$$</p>
<p>where $\mathbf{A}<em i="i">{i j k}$ denotes the attention weight that edge $\left(v</em>$.}, v_{j}\right)$ allocates to edge $\left(v_{j}, v_{k}\right)$. The function $\sigma_{1}$ and $\sigma_{2}$ are two MLPs, and $b_{i k}, g_{i k}$ are two scalars derived from MLP transformations on $\mathbf{e}_{i k</p>
<p>TokenGT [24] computes the attention across all the nodes and edges by concatenating the input matrix as $\hat{\mathbf{H}}=\mathbf{H} | \mathbf{E}$ for Equation (35).</p>
<p>$$
\mathbf{A}=\operatorname{Softmax}\left(\frac{\left(\hat{\mathbf{H}} \mathbf{W}<em K="K">{Q}\right)\left(\hat{\mathbf{H}} \mathbf{W}</em>\right)
$$}\right)^{T}}{\sqrt{d}</p>
<p>To additionally incorporate the structural information, in TokenGT, the embedding of edge node $v$ is combined with a node identifier $\mathcal{P}<em v="v">{v} \in \mathbb{R}$ by the concatenation $\left[\mathbf{H}</em>}, \mathbf{P<em v="v">{v}, \mathbf{P}</em>}, \mathbf{T}^{\mathcal{V}}\right]$, and the embedding of each edge $(u, v) \in \mathcal{E}$ is augmented as $\left[\mathbf{E<em u="u">{(u, v)}, \mathbf{P}</em>$ can be defined via PE such as Laplacian PE.}, \mathbf{P}_{v}, \mathbf{T}^{\mathcal{E}}\right]$. Here, $\mathbf{T}^{\mathcal{V}}$ and $\mathbf{T}^{\mathcal{E}}$ are two learnable identifiers to distinguish node and edge. Similar to PE, node identifiers $\mathbf{P</p>
<p>Edgeformers [49] consist of two distinct variants: Edgeformer-E and Edgeformer-N, specializing in capturing edge and node embeddings, respectively. In this framework, edges are represented as textual data comprising multiple tokens. Specifically, Edgeformer-E combines these edge tokens with the tokens of their associated nodes as input, processing the input by self-attention. Unlike using the entire graph as</p>
<p>input, Edgeformer-N analyzes the ego-graph centered on node $v$. It employs Edgeformer-E to model each edge incident to $v$, and then applies an aggregation function to generate the final node representation $\mathbf{H}_{v}$.</p>
<h2>D. Model Ensemble between GNNs and Transformers</h2>
<p>The most straightforward approach for designing GTs involves strategic combinations of GNNs and Transformers, leveraging both local structure patterns and global contextual relationships. As illustrated in Figure 1, these ensemble architectures can be systematically divided into four categories based on the relative positioning of the GNN and Transformer blocks: 1) Sequential GNN-to-Transformer: feed the output from a GNN input a Transformer. 2) Sequential Transformer-to-GNN: feed the output from a Transformer into a GNN. 3) Interleave GNN and Transformer blocks. 4) Parallel GNN and Transformer: feed the graph into GNN and Transformer concurrently, and fuse the output representations into one representation.</p>
<p>In the first category, GTs initially process the graph by inputting it into a GNN, which can be regarded as tokenizing the graph at the subgraph level. The GNN aggregates information from local neighborhoods to refine node embeddings. Then, the augmented node embeddings are fed into a Transformer, enabling the model to learn from subgraph tokens, as discussed in Section III-A.</p>
<p>The second category of architectures are commonly employed when Transformer blocks have been pretrained. For instance, in the domain of protein data, Transformers [50] have demonstrated effective capabilities in capturing amino acid residue representations. Protein GT frameworks typically exploit pretrained Transformers to generate an initial node represenation, followed by the refinement of GNN regarding the spatial graph structure. A more comprehensive discussion will be provided in Section V-B.</p>
<p>Interleaving GNN and Transformer blocks, as the third category of model ensemble, is a simple yet effective architecture. For example, Mesh Graphormer [51] interleaves GNN and Transformer blocks to reconstruct human poses, and GROVER [9] adopts a hybrid strategy of combining GNN and Transformer to learn molecular representations.</p>
<p>By paralleling GNN and Transformer, GTs can adaptively learn the importance of both local and global information. GraphGPS [27] utilizes the parallel architecture which combines the outputs from a MPNN and a Transformer. In addition, GraphGPS leverages MPNN to update the edge embeddings, which can be utilized to further update the PE.</p>
<p>SGFormer [52] theoretically proves that a single-layer attention is sufficiently expressive to capture the global interactions among nodes. Accordingly, SGFormer proposes a simplified GT architecture that incorporates a single-layer self-loop linear attention mechanism alongside GCN blocks. By combining the final representations from the Transformer and GNN, SGFormer exhibits considerable scalability and competitive performance in node property prediction tasks.</p>
<p>CoBFormer [38] aims to address the issue of overglobalization in GTs. To this end, CobFormer parallelizes</p>
<p>GCN blocks with the Transformer and proposes a collaborative training strategy to supplement local graph structure knowledge from GCN into the Transformer. Specifically, CobFormer incorporates an additional loss function to align the output representations of GCN and Transformer, thereby enabling mutual supervision between the two modules.</p>
<h2>E. Towards Scalability in Graph Transformer</h2>
<p>Recall that the self-attention mechanism in Transformers introduces a quadratic computational complexity regarding the number of nodes. Since real-world graphs may contain millions or even billions of nodes, Transformers often struggle to scale to large graph efficiently. Thus, designing efficient attention mechanisms for large-scale graphs remains a significant challenge for the scalability of GTs.</p>
<p>To reduce the complexity of attention mechanism to linear, one most straightforward approach is to integrating GNNs with linear Transformers. For example, GraphGPS [27] adopts established Transformers that utilize linear attention mechanisms, e.g., combining Performer [53] and BigBird [54] with other GNN modules. Nonetheless, experiments on GraphGPS reveal that although linear attention mechanisms improve scalability, they tend to degrade performance. SGFormer [35], an alternative ensemble-based GT, introduces a linear attention mechanism with self-loop propagation. Theoretical analysis demonstrates that a single-layer attention is sufficient to capture global interactions, enabling SGFormer to achieve scalability and competitive accuracy in node classification tasks.</p>
<p>CobFormer [38] presents a bi-level global attention module aimed at mitigating the over-globalization issue while simultaneously reducing model complexity. Initially, CobFormer partitions the entire graph into clusters. Subsequently, a bilevel attention mechanism operates at both the intra-cluster and inter-cluster levels, which reduces memory consumption significantly. Similarly, Polynormer [37] introduces a linear framework by polynomial network, where each output element is represented as a polynomial function of the input features. To enable permutation equivariance and combine local and global information, it calculates local attention on neighboring nodes and global attention on the entire graph as the coefficients in the polynomial network.</p>
<p>A notable limitation of the structural attention mechanism, as discussed in Section III-C, is its difficult applicability to linear attention. This stems from the fact that linear attention mechanisms do not explicitly construct an attention matrix, making it challenging to incorporate structural information through attention bias or attention mask. To this end, NodeFormer [29] introduces an edge-level regularization loss as Equation (36) that encourages attention values between connected nodes in a graph are close to 1.0.</p>
<p>$$
\mathcal{L}<em l="1">{e}\left(\mathbf{A}, \mathbf{A}^{G}\right)=-\frac{1}{N L} \sum</em>
$$}^{L} \sum_{(u, v) \in \mathcal{E}} \frac{1}{d_{u}} \log \mathbf{A}_{u v}^{(l)</p>
<p>where $L$ denotes the total number of layers in NodeFormer, and $d_{u}$ represents the degree of node $u$. Since this loss function only requires computations over edges, NodeFormer efficiently</p>
<p>manages the complexity of edge regularization at $\mathcal{O}(|\mathcal{E}|)$, maintaining the overall model complexity as $\mathcal{O}(|\mathcal{V}|+|\mathcal{E}|)$.
Exphormer [31] incorporates a sparse attention mechanism to achieve linear complexity. In essence, this sparse attention mechanism combines the adjacency matrix, expander graph, and virtual node. An expander node randomly connects nodes and ensure that each node maintains an equal degree, resulting in the expander possessing a number of edges that is linear in relation to the nodes. Despite its linear complexity, the expander graph preserves the spectral approximation of a complete graph. Moreover, Exphormer achieves competitive performance compared to dense attention.
An alternative approach avoids feeding the entire graph into the Transformer. NAGphormer [32] transforms the $k$ hop neighborhood $\mathcal{N}^{k}(v)$ into a neighborhood embedding $\mathbf{x}<em v="v">{v}^{k}$ using an aggregation operator $\phi$. This aggregated embedding is then treated as a token within the Transformer, enabling the model to learn the embedding of node $v$. By aggregating neighboring nodes before processing by Transformer, NAGphormer circumvents the need to input a large number of nodes into the Transformer. Moreover, NAGphormer+ [44] enhances the feature of $\mathbf{x}</em>$ by randomly masking a portion of neighbors to achieve better performance. In addition, VCRGraphormer [55] then employs random walk to rewire the graph by virtual nodes. By maintaining a graph with virtual nodes, VCR-Graphormer controls its complexity, keeping it linear with respect to the number of nodes while still capturing long-range dependencies.}^{k</p>
<h2>F. Geometric Graph Transformers</h2>
<p>Given the wide range of real-world scientific applications involving GTs, the study of geometric GTs is crucial for modeling 3D graph data, such as molecular systems and protein structures. The core design principle of these frameworks lies in ensuring the 3D invariance and/or equivariance of the model. This section briefly reviews start-of-the-art equivariant GTs that have been successfully applied to 3D graphs modeling.
The most straightforward approach to learn the structural relationships is incorporating the 3D relative distance as an additional edge embedding, which remains unchanged under Euclidean transformations. For example, Graphormer [10] introduces spatial encoding, where an MLP is used to encode the relative distance between atoms, effectively capturing structural relationships. This paradigm has demonstrated its efficacy in various frameworks for learning molecular representations [56]. Additionally, other invariant features, such as the angle between edges [57], can be included to represent orientation information. These invariant features are usually encoded using kernel functions, such as the Radial Basis Function [58], to enhance the model's expressivity.
TorchMD-Net [11] represents another equivariant model that incorporates the interatomic distance $r_{i j}$ into its framework. The process begins by projecting $r_{i j}$ into two distinct multidimensional filters, denoted as $\mathbf{D}^{K}$ and $\mathbf{D}^{V}$, using the following expressions:</p>
<p>$$
\mathbf{D}^{K}=\sigma_{1}\left(r_{i j}\right), \mathbf{D}^{V}=\sigma_{2}\left(r_{i j}\right)
$$</p>
<p>where $\sigma_{1}$ and $\sigma_{2}$ are two MLPs. Subsequently, TorchMD-Net replaces the traditional Softmax function with the SiLU function to compute the attention matrix as shown in Equation (38):</p>
<p>$$
\mathbf{A}=\operatorname{SiLU}\left(\left(\mathbf{H W}<em K="K">{Q}\right)\left(\mathbf{H W}</em>\right)
$$}\right)^{T} \mathbf{D}^{K}\right) \cdot \phi\left(\mathbf{d}_{i j</p>
<p>where $\phi$ denotes a cutoff function that assigns the value of 0 whenever $\mathbf{d}_{i j}$ exceeds a predefined threshold. The final representation is then computed by:</p>
<p>$$
\mathbf{Z}=\sigma_{3}\left(\mathbf{A V D}^{V}\right)
$$</p>
<p>where $\sigma_{3}$ represents another learnable linear transformation.
For tasks such as conformation generation, where the model needs to generate atomic coordinates, Uni-Mol [56] proposes a simple SE(3)-equivariant head, represented as:</p>
<p>$$
\overrightarrow{\mathbf{X}}<em i="i">{i}=\overrightarrow{\mathbf{X}}</em>}+\frac{1}{n} \sum_{j=1}^{n}\left(\overrightarrow{\mathbf{X}<em j="j">{i}-\overrightarrow{\mathbf{X}}</em>
$$}\right) c_{i j</p>
<p>where $c_{i j}$ represents the learned relationship embedding between node $v_{i}$ and $v_{j}$. To improve efficiency, Uni-Mol updates the coordinates only in the final layer of the model.
GVP-Transformer [59] represents an encoder-decoder framework based on the Transformer architecture, designed for the task of protein inverse folding. The model is structured to intake protein structures and subsequently generate corresponding protein sequences. As an encoder, GVP-Transformer utilizes the GVP-GNN [60], capable of extracting features that are translation-invariant and rotation-equivariant, to effectively model protein structures. This is followed by the application of a Transformer decoder to produce valid protein sequences.
Examples of high-order steerable GTs include SE(3)Transformer [12], Equiformer [13], and EquiformerV2 [14]. These models employ equivariant attention mechanisms utilizing higher-degree representations of steerable features [61], which fall beyond the focus of this survey.</p>
<h2>IV. THEORIES</h2>
<p>Beyond the practical effectiveness of GTs it is essential to understand the theoretical foundations underlying GTs. This section begins by reviewing the different expressive capabilities among existing GTs (Section IV-A). Subsequently, we investigate the interconnections between GTs and other graph learning methodologies (Section IV-B).</p>
<h2>A. Expressivity</h2>
<p>Following the order in Section III, we here respectively discuss the expressivity in structural tokenization, and comparing absolute PE with relative PE.</p>
<p>1) Structural Tokenization: In node-level tokenization, each node is treated as an independent token, allowing the model to capture local neighborhood information. However, it may struggle to capture global graph structural patterns that span multiple nodes, potentially being insufficient for expressing graph properties that require global information. Therefore, it is necessary to enhance structural bias with additional positional embeddings [36]. Edge-level tokenization can capture the connectivity between nodes, facilitating models to</p>
<p>comprehend the interactions between nodes and the topology of the graph. Subgraph-level and hop-level tokenization encode local subgraph patterns to tokens, such as graph motifs and $k$-hop neighbors. These tokenizations allow models to capture more complex and global graph features, enhancing the representations on communities structures and long-range dependencies.</p>
<p>The expressive power of GTs is intricately related to the process of tokenization. TokenGT [24] harnesses this power by effectively encoding graphs as sets of input tokens. As a pure Transformer, TokenGT utilizes $n+m$ tokens for each graph, where $n$ represents the number of nodes and $m$ represents the number of edges. This approach achieves 2-Weisfeiler-Leman (WL) expressivity, which has been proven equivalent to 1-WL expressivity [62].</p>
<p>A formal theoretical framework [63] establishes a connection between various tokenization methods and the $k$-WL test. To align GTs with the $k$-WL test, the authors propose providing suitable input tokens $\mathbf{X}^{(0, k)}$ to the Transformer for each $k \geq 1$. They demonstrate that the $t$-th layer of the Transformer can emulate the $t$-th iteration of a $k$-order WL algorithm. In this context, $\mathbf{X}^{(0, k)} \in \mathbb{R}^{n^{2} \times d}$ denotes the initial token embeddings of k-tuples, where $n^{k}$ represents the number of these embeddings.</p>
<p>In general, different levels of tokenization affect the expressive power of GTs. Node-level tokenization is suitable for capturing local features, edge-level tokenization is appropriate for understanding relationships between nodes, while subgraphand hop-level tokenization provide a deeper understanding of the global structure of the graph.
2) Positional Encoding: A recent literature [64] highlights the importance of a theoretical comparison on various PE strategies. Predominantly, there are two types of PEs based on either kernel or Laplacian graphs. To conduct a theoretical analysis of these PEs, methods like WL test, along with SPD-WL, GD-WL [36], and RPE-augWL [65], are employed to assess and compare their expressivity. To analyze the expressive power of absolute PE and relative PE, Black et al. [65] proposed a framework that leverages 2-equivariant graph network (2-EGN) [66] to convert between relative PE and absolute PE. For graph without node features, the paper demonstrates that the distinguishing capabilities of absolute PE and relative PE are equivalent. In contrast, for graph with node features, converting relative PE to absolute PE will undermine the distinguishing capability of GTs.</p>
<p>In addition, while the expressivity of absolute PEs has been discussed in Section III-B and the works for resistance distance have been compared in Section III-C, it has been proven that exploiting the power of matrix, such as the relative random walk positional encoding (RRWP) using the adjacency matrix achieves at least comparable expressivity to spectral kernel when employed as relative PE [65].</p>
<h2>B. Relationship with Other Graph Learning Methods</h2>
<p>The characteristics of GTs can be elucidated through comparative study with other graph learning methods. In this section, we examine studies that compare GT with MPNN, graph structure learning, and graph attention network.</p>
<p>1) MPNN: Compared with MPNNs, GTs integrate selfattention mechanisms and PE. A recent study [64] demonstrates that self-attention mechanism improves the convergence rate of GTs, while PE facilitates identifying the core neighborhood for each node, thereby enhancing the generalization ability. Notably, GTs with shortest-path distance [65] as relative PE possess theoretically superior expressivity than classical MPNNs.</p>
<p>An alternative approach to infuse global information into each node is to introduce a virtual node connected to all nodes in a graph. Despite the simplicity of this idea, MPNN with the virtual node [67] surprisingly serves as a strong baseline in Long Range Graph Benchmark [68]. A recent study [69] reveals that no single algorithm can fully surpass the others between GTs and MPNNs with virtual node.</p>
<p>In addition, the over-smoothing problem [4], characterized from deep MPNNs, also exist in Transformers [70], which will result in indistinguishable node embeddings in deep layers. As Transformer is a special form of Graph Attention Networks (GAT) [71], it shares the same over-smoothing phenomenon as GAT, leading to an exponential degeneration of expressive power regarding the number of layers. To mitigate oversmoothing, SignGT [72] proposes a signed attention mechanism to preserve the diverse frequency information in graph structure from the perspective of graph signal processing.
2) Graph Structural Learning: Graph Structure Learning (GSL) is closely related to GTs, which aims at automatically refining graph structures when the input graph is noisy or incomplete, or inferring implicit graph structures when explicit graph structure is unavailable [73], in a parameterized way. Building on this foundation, GSL has been widely applied in various domains, such as molecular context graphs [74], [75], spatiotemporal graphs [76], and social networks [77]. GTs can be regarded as a special form of GSL, achieved by self-attention that learns a fully connected 'soft' graph structure [78]. By utilizing attention-oriented techniques, such as the attention mask in Section III-C2 and discrete structure sampling in NodeFormer [29], the learned graph structure can be sparsified to reflect real-world topology.</p>
<h2>V. APPLICATIONS</h2>
<p>GTs have emerged as a versatile and powerful architecture across a wide range of domains, showcasing remarkable capabilities in capturing intricate relationships and learning sophisticated graph representations. This section presents a systematic review of GTs applications across diverse graphs. We categorize these applications by the type of graph, including molecule, protein, social network, textual network, visual network, and other specialized networks. For each category, we provide an in-depth summary on the learning tasks, datasets and methods respectively, where the tasks and representative methods are illustrated in Figure 2.</p>
<h2>A. Tasks on Molecule</h2>
<p>Molecules can be represented as graphs where atoms are nodes, and chemical bonds are edges, using multiple dimensional encoding methods. One-dimensional (1D) encoding, like the string representations of SMILES [164] and</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Overview of the applications of graph transformers.</p>
<p>SELFIES [165], capture topological details of molecules. The tokenizer for these 1D strings serves as a multi-level graph tokenizer, incorporating both node-level and edge-level tokens. The 2D molecular graph naturally represents molecules by depicting atoms as nodes and bonds as edges. This representation enables direct interpretation of molecular connectivity and structural relationships. Additionally, a 3D graph can be constructed to indicate spatial relationships by connecting each atom with its k nearest neighbors. Each dimensional approach provides unique insights into molecular structures, allowing researchers to analyze biochemical systems from complementary perspectives.</p>
<p>In this section, we classify molecular downstream tasks into three categories: molecular property prediction, molecular dynamics simulation, and molecular generation. We also discuss molecular pre-training strategies, where GT is a prevailing backbone model that absorbs prior knowledge from immense unlabeled graph data.</p>
<h3>1) Molecular Property Prediction</h3>
<p>While vanilla Transformer has significant potential for modeling molecules as SMILES [164], it fails to encode the structural information of molecules. GTs address this limitation by simultaneously capturing local and global features in molecules, generating effective molecular embeddings to support diverse downstream tasks, e.g., molecular property prediction.</p>
<h4>Task Definition</h4>
<p>Given a 2D molecule graph $$G = (V, E)$$, with atoms as nodes $$V$$ and chemical bonds as edges $$E$$, a model $$\phi_g$$ is established to classify or predict the target property $$y$$ of the molecule.</p>
<p>$$y = \phi_g(\mathcal{G}) \tag{41}$$</p>
<p>Additionally, a geometric graph $$G = (V, E, \mathbf{X})$$ contains an optional input, the 3D coordinates $$\mathbf{X} \in \mathbb{R}^{n \times 3}$$ of $$n$$ nodes, providing the 3D conformation of the $$n$$ atoms. The geometric graph $$G$$ can also be utilized to predict molecular properties, which generally improves the performance compared with the counterpart 2D graph $$G$$.</p>
<p>$$y = \phi_g(\mathcal{G}) \tag{42}$$</p>
<h4>Dataset</h4>
<p>The MoleculeNet [166] benchmark includes two types of tasks, classification and regression, for evaluating molecular properties. Classification tasks include BBBP [167], SIDER [168], ClinTox [169], BACE [170], Tox21 [171] and ToxCast [172]. These datasets describe molecular properties from various perspectives, such as permeability property (BBBP), the side effect of drugs (SIDER), toxicity as compounds (ClinTox, Tox21, ToxCast) and inhibitor of human (BACE). Regression tasks contain QM7 [173], QM8 [174], QM9 [175], ESOL [176], Lipophilicity [177] and FreeSolv [178], evaluating the physical chemistry and quantum mechanics of molecules. Specifically, QM7, QM8 and QM9 contain computer-generated molecular properties, such as HOMO/LUMO, atomization energy, electronic spectra and excited state energy. ESOL, Lipophilicity, FreeSolv datasets record the solubility, lipophilicity and free energy of the molecules, respectively. OGB [179] reformulates the datasets from MoleculeNet to build ogbg-molhiv, ogbg-molpcba datasets with additional features. PCQM-Contact, from LRGB [68], requires the model to predict whether the pairs of distant nodes will contact in 3D space. Consequently, this task emphasizes the ability of long-range modeling, which is appropriate for GTs.</p>
<p>Methods: MAT [79] pioneers the integration of interatomic distances and chemical structures as biases in the self-attention. Specifically, the attention map will be elementwise summed up by distance graph and chemical graph. Building upon MAT, R-MAT [80] introduces an additional relative PE, including relative spatial distances, distances in the 2D graph and physiochemical relationships. Instead of incorporating PE into node embedding, R-MAT injects PE directly as attention bias via multiplying the PE with the calculated queries and keys in the self-attention. GROVER [9] employs a dual GTransformer architecture to capture both node-level and edge-level features. In each GTransformer, the inputs are first fed into a tailored GNNs to extract vectors as queries, keys and values from nodes of the graph, followed by standard multi-head attention blocks. This bi-level feature extraction framework enables GROVER to not only capture structural information in molecular data but also to discern global relationships between nodes. CoAtGIN [82] leverages both GIN [180] and linear Transformer in parallel to extract both local and global representations of molecules. To enhance the robustness of molecular property prediction on 3D molecular graphs, Equiformer [13], EquiformerV2 [14], TorchMDNET [11] and SE(3)-Transformer [12] design E(3)-equivariant GTs. These models have also been utilized as backbones for molecular representation pre-training [86], [87].
2) Molecular Dynamics Simulation: Molecular Dynamics (MD) simulations is a fundamental methodology for illuminating chemical processes at atomic resolution. The objective of MD simulation is to forecast the future atomic trajectory of a molecule, based on its past coordinate transitions. Utilizing GTs, we can effectively learn the interatomic forces and quantum mechanical interactions among atoms, thereby enabling an accurate prediction of atomic trajectory in molecular systems.</p>
<p>Task Definition: Given an input molecule represented by a geometric graph $\overrightarrow{\mathcal{G}}$ with 3 D coordinates $\overrightarrow{\mathbf{X}}<em _obs="{obs" _text="\text">{t</em>}}}$ of each atom at the observed timestamp $t_{\text {obs }}$, the model $\phi_{\theta}$ is required to predict the coordinates $\overrightarrow{\mathbf{X}<em _target="{target" _text="\text">{t</em>$.}}}$ at the target timestamp $t_{\text {target }</p>
<p>$$
\overrightarrow{\mathbf{X}}<em _target="{target" _text="\text">{t</em>}}}=\phi_{\theta}\left(\overrightarrow{\mathbf{X}<em _obs="{obs" _text="\text">{t</em>\right)
$$}}}, \overrightarrow{\mathcal{G}</p>
<p>Dataset: For small molecules, MD17 [181] and MD22 [182] comprise the trajectories of eight and seven molecules generated via MD simulation, respectively. The objective is to predict the future coordinates of each atoms given the sampled current system state in the trajectory. For more complex dynamic systems, the AdK [183] equilibrium trajectory dataset records the trajectories of apo adenylate kinase simulated with explicit water and ions.</p>
<p>Methods: The scarcity of trajectory data entails the model to be robust. Oriented to such scenarios, equivariant GTs are designed to preserve the inductive bias of 3D-equivariance in molecular systems. Equiformer [13] and TorchMD-Net [11], whose architectures are discussed in Section III-F, demonstrate their effectiveness, as evaluated on the MD17 dataset. ESTAG [89] proposes to utilize Fourier Transformer to uncover latent and unobserved dynamics in the system. Furthermore, ESTAG incorporates a temporal attention mechanism to capture node-wise temporal dynamics on a global scale. Con-
currently, it employs a spatial model that facilitates message passing according to the graph structure, ensuring a comprehensive characterization of both temporal evolution and spatial relationships in molecular systems.
3) Molecular Generation: Molecule generation is a cornerstone task in contemporary drug design and discovery [184]. This area of research can be broadly divided into two distinct applications. The first focuses on generating 2D molecular structures based on specified properties, and the second is generating 3D molecular ground-state conformation from 2D molecular structure. Traditional computational approaches for conformation prediction are often time-consuming and involve complex computational processes. In contrast, deep learning approaches provide an efficient way to accelerate the drug design pipeline significantly.</p>
<p>Task Definition: In molecular generation, the model is designed to generate a molecular graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$, where $\mathcal{V}$ represents the set of atoms and $\mathcal{E}$ denotes the chemical bonds between nodes. To obtain a more comprehensive understanding of molecular conformation, the task of generating the ground-state conformation is to predict the 3D coordinates $\overrightarrow{\mathbf{X}} \in \mathbb{R}^{3 \times n}$ for each atom in the molecular graph $\mathcal{G}=(\mathcal{V}, \mathcal{E})$.</p>
<p>Dataset: In the property-guided molecule generation task, MOSES [185] and GuacaMol [186] are two used datasets, each containing over 1 million molecules. By contrast, the Molecule3D [187] dataset includes about 4 million molecules, providing both 2D molecular graphs and their corresponding ground-state 3D geometric structures. Another notable dataset, QM9 [175] contains around 130,000 molecules whose 3D conformations are calculated by Density Functional Theory (DFT) [188]. Furthermore, QM9 is also applicable to tasks focused on conditional molecule generation given specific properties</p>
<p>Methods: To preserve the discrete nature of molecular structures, CDGS [90] introduces a GT backbone and modifies the reverse process in a diffusion model. It starts by decoding a discrete structure, which is then used as a conditional input for each subsequent step in the reverse sampling process. Furthermore, JODO [94] extends CDGS by simultaneously generating the 2D chemical graph and 3D conformation of molecules. Another approach, DiGress [15], employs a discrete diffusion model with a GT backbone adapted from [22]. DiGress develops a discrete forward process by adding or removing edges and modifying categories of edges, ensuring the sparsity of graphs.</p>
<p>However, since both continuous (coordinates) and discrete (structure) information are essential for a comprehensive description of molecules, MUDiff [93] introduces a 3D rototranslation equivariant GT backbone called MUformer. The backbone is specifically designed to model both 2D and 3D molecular structures, enabling MUDiff to effectively generate both 2D and 3D molecular representations in a diffusion framework.</p>
<p>Uni-Mol [56] develops a SE(3)-equivariant head to predict molecular conformations, which is pre-trained on large scale 3D molecular data. GTMGC [95] designs an encoder-decoder architecture. The encoder incorporates a 2D chemical adjacency matrix as an attention bias into the molecular structure to</p>
<p>predict a preliminary conformation along with 3D coordinates. The decoder then refines this prediction by leveraging both the 2D adjacency matrix and the 3D relative distances as attention bias, resulting in more accurate final conformation predictions.</p>
<p>GraphDiT [96] addresses conditional molecule generation by introducing a conditional encoder, which supports multiple conditions. This model replaces the standard normalization layer in Transformers with adaptive layer normalization, allowing the embedding of conditions to guide the generation process.
4) Molecular Pre-training: Pre-training Transformers have achieved a great success in NLP or CV, where immense unlabeled data enables efficient parameter optimization and impressive downstream performance [18], [189]. Similarly, to leverage unlabeled molecular data, even with significant advancements in GT structures, it is important to design domainspecific pre-training strategies for GTs, which incorporate inductive bias into molecular data. We outline the pre-training strategies spanning multiple molecular representations: 1D molecular strings, 2D chemical graphs, and 3D geometric graphs.</p>
<p>Task Definition: Masking perturbations build a selfsupervision task for pre-training, which mask the atomic types or coordinations of molecule $\mathcal{G}$ to $\mathcal{G}^{\prime}$. The model $\phi_{\theta}$ is pre-trained by optimizing a reconstruction loss as shown in Equation (44) to recovering the original molecular $\mathcal{G}$.</p>
<p>$$
\arg \min <em _theta="\theta">{\theta} \mathcal{L}\left(\mathcal{G}, \phi</em>\right)\right)
$$}\left(\mathcal{G}^{\prime</p>
<p>In addition to the masking strategy, contrastive learning [190] is another effective pre-training strategy for aligning the molecular representations of distinct modalities. Specifically, for molecules, contrastive learning that optimizes a contrastive loss as Equation (45) can be applied to align information across different dimensions.</p>
<p>$$
\arg \min <em 1="1">{\phi</em>)\right)
$$}, \phi_{2}, \phi_{3}} \mathcal{L}\left(\phi_{1}(\mathcal{G}), \phi_{2}(\mathcal{G}), \phi_{3}(\mathcal{G</p>
<p>where $\phi_{1}, \phi_{2}$ and $\phi_{3}$ are neural networks that extract similar representations from 1D, 2D and 3D molecular data, respectively.</p>
<p>Dataset: ZINC15 [191] and ChEMBL [177] are two frequently used 2D datasets containing millions of molecules, which can also be converted into SMILES format to pretrain Transformers. Uni-Mol [56] normalizes and duplicates molecules from ZINC15 and ChEMBL to obtain a dataset of 19 million molecules. Furthermore, Uni-Mol uses RDKit [192] to generate nine 3D conformations for each molecule, resulting in a total of 209 million conformations. Uni-Mol2 [92] further extends the dataset of Uni-Mol by additionally incorporating data from ZINC20 [193], thereby encompassing 884 million molecules and 73 million scaffolds. PCQM4Mv2 [194] is another widely used quantum chemistry dataset from OGB benchmark, containing 3.4 million molecules with both 2D and 3D information.</p>
<p>Methods: For 1D strings, Transformers [195] such as MolT5 [196] tokenize the nodes and edges in SMILES and utilize a mask language model style to predict the masked tokens. For 2D graphs, GROVER [9] includes two levels
of pre-training tasks, predicting contextual property of the give nodes and functional motifs in the graph. Since there may exist multiple valid atoms in masked atoms, the correct recovery may still be alleviated, which will slow down the convergence. To this end, LiGhT [81] introduces a knowledge node including molecular descriptors and fingerprints to assist the prediction of masked nodes. BatmanNet [84] tokenizes the atom and edge in molecular graph and mask a large portion of tokens to recover. Uni-Mol [56] involves 3D spatial information in molecules and is pre-trained to predict masked atoms and recover 3D coordinates from the noisy perturbations. Coord [86] and GNS-TAT [83] propose to predict the noise itself from the coordinates with a Gaussian noise and prove this objective will enable the model to learn the force field in molecules. Frad [197] proposes a hybrid fractional perturbation strategy, including a dihedral angle perturbation and a coordinate perturbation, enable the model to capture the anisotropic characteristic of molecules. Nonetheless, SliDe [198] proves that the above approaches fails to capture the realistic force filed due to inappropriate assumptions, and SliDe introduces an interpretable strategy, perturbing bond lengths, angles, and torsion angles, to align with physical principles.</p>
<p>Beyond the masking and denoising strategies, DMP [85] additionally introduces a dual-view consistency loss to exploits the consistency of 1D and 2D molecular representations. Transformer-M [199] develops two channels to encode 2D and 3D information, respectively, and allows single or both channels are enabled. This allows the Transformer-M to be pre-trained on unpaired 2D and 3D molecules. In contrast, MoleBLEND [200] proposes to integrate 2D and 3D structures into a unified relation matrix to comprehensively capture the relations in molecules for pre-training. Considering that the denoising strategy only learns the potential energy, MolSpectra [87] incorporates molecular spectra to capture the energy level structures, thereby enhancing molecular pre-training.</p>
<h2>B. Tasks on Protein</h2>
<p>Transformers are effective for modeling the protein sequence, while keeping the representation of complicated and hierarchical structure of proteins as an open problem. While amino acids are sequentially connected via peptide bonds, which does not directly provide topological information, the graph can be constructed from the 3D spatial relationships between them.</p>
<p>1) Protein Property Prediction: Predicting protein properties is crucial for understanding biochemical interaction processes. The folded structure of a protein is intimately connected to its functionalities, which entails accurate representation for computational approaches. This underscores the significance of employing GTs, which can leverage the underlying spatial relationships of proteins to predict their properties with more informative features.</p>
<p>Task Definition: In the protein graph $\overrightarrow{\mathcal{G}}=(\mathcal{V}, \mathcal{E}, \overrightarrow{\mathbf{X}})$, the $\mathcal{V}$ and $\overrightarrow{\mathbf{X}}$ denote the types and 3D coordinates of amino acid, respectively, and $\mathcal{E}$ contains the spatial relationships between these amino acids, such as distance, dihedral angles, and planar</p>
<p>angles. Given the protein graph $\overrightarrow{\mathcal{G}}$, the model $\phi_{\theta}$ is tasked with predicting the specific property $y$ of the protein:</p>
<p>$$
y=\phi_{\theta}(\overrightarrow{\mathcal{G}})
$$</p>
<p>Dataset: There are abundant benchmarks for assessing protein functions, including FLIP [201], DeepLoc [202], DeepFRI [203], PEER [204], TAPE [205]. These benchmarks cover various properties of protein, such as thermostability, protein localization, Enzyme Commission (EC) number, Gene Ontology (GO) term, protein-protein interaction, stability, amino acid contact, etc.</p>
<p>Methods: Pre-trained protein language models, such as ESM [50], have shown a strong ability to learn representations of protein sequences. TransFun [99] first utilizes ESM to learn the node embedding, then puts these embeddings into EGNN [206]. This helps capture both sequence and structure information from the protein. HEAL [97] follows a similar approach to initialize the node embedding and puts the embedding into a graph contrastive learning framework to better learn the protein function.</p>
<p>To directly inject spatial relations, Stability Oracle [100] utilizes a pairwise distance matrix as an attention bias to predict the stability of proteins with mutations.
scMoFormer [98] stands out as a multi-modal model that models each biological component, including proteins, within the cell via parallelized GNNs and Transformers. After processing, a subsequent GNN integrates the representations from all modalities.</p>
<p>Saprot [102] and ProstT5 [101] process 3D protein structures to build a vocabulary that incorporates both the geometric information and the amino acid residue type in proteins. This vocabulary enriches the representation learning in Saprot and enhances the protein sequence design in ProstT5 from additional structural information of proteins.
2) Affinity \&amp; Docking: Predicting binding affinity and determining the docking positions play a pivotal role in drug discovery, elucidating the molecular mechanisms of drugproteins interactions. Computational approaches are applies to both protein-protein and protein-ligand interactions. Accurate in silicon predictions significantly streamline the drug development pipeline by reducing laborious experimental screening.</p>
<p>Task Definition: Given the geometric ligand graph $\overrightarrow{\mathcal{G}}<em r="r">{l}$ and the receptor protein graph $\overrightarrow{\mathcal{G}}</em>$ :}$, the protein-ligand binding prediction task is to predict the binding affinity or pose of the ligand-receptor pair. To more effectively capture the complex relationships in these geometric graphs, the nodes are based on the atom level. Consequently, given a model $\phi_{\theta}$, the target can be an affinity score $s \in \mathbb{R}$ or another geometric complex graph $\overrightarrow{\mathcal{G}}_{c</p>
<p>$$
s, \overrightarrow{\mathcal{G}}<em _theta="\theta">{c}=\phi</em>}\left(\overrightarrow{\mathcal{G}<em r="r">{l}, \overrightarrow{\mathcal{G}}</em>\right)
$$</p>
<p>Similarly, when both ligand and receptor are proteins, the proteins are considered as rigid bodies. The target is also the affinity score $s \in \mathbb{R}$ and the rotation matrix $\mathbf{R} \in \mathcal{R}^{3 \times 3}$ and translation matrix $t \in \mathcal{R}^{3}$ that lead to a docking pose $\left(\mathbf{R} \overrightarrow{\mathbf{X}}<em r="r">{l}+\mathbf{t}, \mathbf{R} \overrightarrow{\mathbf{X}}</em>\right)$ :</p>
<p>$$
s, \mathbf{R}, \mathbf{t}=\phi_{\theta}\left(\overrightarrow{\mathcal{G}}<em r="r">{l}, \overrightarrow{\mathcal{G}}</em>\right)
$$</p>
<p>Dataset: PDBbind [207] contains a total of 19,443 proteinligand complexes. Additionally, CASF-2016 [208] consists of 285 diverse protein-ligand complexes. The Davis [209] dataset includes 30,056 interactions of 442 proteins and 68 ligands, which provides selective measurements for the kinase protein family and its associated inhibitors. The KIBA [210] dataset comprises bioactivity data of kinase inhibitors measured by the KIBA approach, including binding affinities for interactions between 467 proteins and 52,498 ligands.</p>
<p>In protein-protein docking, DIPS [211] and DB5.5 [212] are two representative datasets. DIPS contains large number of protein complex structures and DB5.5 is manually collected by human experts and is of high-quality.</p>
<p>Methods: RTMScore [104], IGT [105], GeoT [106], GGT [107], AttentionMGT-DTA [109] and GTAMPDTA [110] utilize edge embeddings to adjust the weights in attention matrix and also employ this matrix to refine the edge embeddings iteratively. These methods applies such GT layers to both ligand and receptor proteins.</p>
<p>RTMScore concatenates the node embeddings of both ligand and receptor, while IGT uses extra attention layers to integrate these embeddings for predicting their binding activity and pose. GeoT enhances its training by including a detailed edge feature set, which consists of distance, direction, orientation, and amide angle information. GTAMP-DTA enhances the integration by introducing embeddings extracted from other pretrained molecular and protein models. Similarly, AttentionMGT-DTA uses cross-attention layers to combine the learned embeddings with those obtained from ESM.</p>
<p>HGIN [112] incorporates pairwise distance matrix at both the atom and amino acid levels, as well as pairwise residue features, utilizing these as attention biases to predict binding affinity after mutation.</p>
<p>Graph-BERT [111] learns the embedding of each node by including its K-hop neighbors, along with additional PE, into the Transformer blocks to derive the final embedding. This model is used in protein-protein interaction tasks to determine if two proteins will interact.</p>
<p>GraphSite [103] utilizes distance maps in proteins as attention masks and employs structural information as PE to predict protein-DNA interactions. Additionally, GraphormerDTI [108] adopts the Graphormer architecture, incorporating spatial and edge relative PEs as attention biases, along with centrality encoding, to predict interactions between drug and target.</p>
<p>In docking tasks, Uni-Mol [56] and GeoDock [113] concatenate atom and pair representations learned in the encoder to further refine these representations and predict the final pair distances. Uni-Mol uses GT with attention bias as its encoder. Moreover, GeoDock updates edge embeddings by iteratively using them as attention biases and employs the attention matrix to further refine these edge embeddings. EBMDock [114] applies Equiformer [13] as backbone, proposes a framework aimed at docking pose aligns with a minimum point in the energy landscape.
3) Protein Design: Protein inverse folding aims to determine the amino acid sequence that can fold into a given specific 3D protein structure, which is crucial for protein engineering and therapeutic development. A more challenging</p>
<p>task is to co-design both the protein structure and the sequence when provided with the structure of a ligand.</p>
<p>Task Definition: Inverse folding provides the coordinates of each atom in amino acid residues $\overrightarrow{\mathbf{X}} \in \mathcal{R}^{n \times 3 \times 4}$, and the model $\phi_{\theta}$ is required to predict the protein sequence $\mathbf{S}$.</p>
<p>$$
\mathbf{S}=\phi_{\theta}(\overrightarrow{\mathbf{X}})
$$</p>
<p>In protein pocket sequence and structure co-design task, a subset of protein $\overrightarrow{\mathcal{G}}<em A="A">{B}$ is blank, and given the rest amino acid residues $\overrightarrow{\mathcal{G}}</em>}$ in a protein and a ligand graph $\overrightarrow{\mathcal{G}<em _theta="\theta">{L}$, the model $\phi</em>$ is required to design the blank part in protein.</p>
<p>$$
\overrightarrow{\mathcal{G}}<em _theta="\theta">{B}=\phi</em>}\left(\overrightarrow{\mathcal{G}<em L="L">{A}, \overrightarrow{\mathcal{G}}</em>\right)
$$</p>
<p>Dataset: CATH4.2 and CATH4.3 [213] are two commonly used benchmarks to evaluate the recovery rate of the protein sequences. CrossDocked [214] and Binding MOAD [215] datasets contain protein-molecule pairs to evaluate the generation of protein pockets.</p>
<p>Methods: GVP-Transformer [59] uses the embeddings generated by GVP-GNN [60] and processes them through encoder-decoder Transformer blocks to design protein sequences. In addition, LM-Design [115] integrates a pre-trained protein language model (pLM) by using a GNN as a structural adapter for ESM, allowing for the recovery of protein sequences by combining both structure and sequence models. ProRefiner [116] utilizes edge embedding as attention bias in GT. It refines the designed sequence by iteratively updating predictions, focusing on amino acids that predicted with low confidence.</p>
<p>FAIR [117] designs a hierarchical GT to incorporate information from both atom- and residue-level structures. At each level, FAIR uses spatial embeddings to learn the key and value vectors. Similarly to LM-Design, PocketGen [118] introduces hierarchical GT by introducing structural adapter layers for pre-trained pLMs to further refine designed sequences.</p>
<h2>C. Tasks on Textual Graph</h2>
<p>In textual graphs, nodes can represent either words or sentences. Given the significant achievement of vanilla Transformer in NLP, it is a natural idea to extend vanilla architecture to GTs. This extension fulfills the integration of rich structural relationships into textural graphs.</p>
<p>1) Graph-to-Text Generation: Abstract Meaning Representation (AMR) [216] graphs are a semantic formalism for representing the meaning of sentences, where nodes represent variables and edges represent semantic relationships, which is crucial for applications such as translation. The AMR-toText generation task focuses on converting these AMR graphs into sentences. Beyond AMR graphs, knowledge graphs (KGs) can also be transformed into text, making the stored semantic information more accessible to readers. Furthermore, by modeling KGs, GTs can acquire the ability to reason and respond to given queries.</p>
<p>Task definition: Given a graph $G$, we utilize a graph encoder $\phi_{\theta}$, such as GNNs or GTs to learn the graph embedding $\mathbf{Z} \in$ $\mathbb{R}^{n \times d}$. Subsequently, this embedding is fed into a decoder $f$ to generate the corresponding text $\mathbf{T}$.</p>
<p>$$
\mathbf{Z}=\phi_{\theta}(G), \mathbf{T}=f(\mathbf{Z})
$$</p>
<p>Datasets: The graphs utilized for text generation can be categorized into two main classes: 1) AMR graph. The LDC2015E86 and LDC2017T10 datasets are comprised of sentences annotated with AMRs, containing 16,833 and 36,521 AMRs for training, respectively. Both datasets share the same validation set of 1,368 AMRs and a test set of 1,371 AMRs. Additionally, the English-German and EnglishCzech datasets are from the WMT16 translation task ${ }^{1}$, containing around 200,000 AMR graphs for model evaluation. 2) Knowledge graph. KG-to-graph benchmarks include AGENDA [217] and WebNLG [218] datasets. AGENDA focuses on generating scientific text based on automatically extracted entities and relations. WebNLG is a dataset comprised of crowd-sourced texts that correspond to subgraphs extracted from DBPedia [219] categories. Since GT can be applied to Question Answering (QA) and knowledge graph completion tasks, we highlight two and four datasets for assessing the reasoning capabilities of models for these two tasks, respectively. For the task of QA, we introduce two datasets: FreebaseQA [220] and WebQuestionsSP [221]. FreebaseQA is a comprehensive dataset designed for open-domain QA, leveraging the Freebase knowledge graph. WebQuestionsSP dataset contains semantic parses for the questions sourced from WebQuestions that can be answered using the information from Freebase. In the realm of knowledge graph completion, there are four datasets: WN18RR [222], FB15K237 [223], NELL-995 [224] and UMLS [225]. WN18RR is an English lexical knowledge graph, offering a structured representation of word relationships. FB15K-237 is a curated subset of Freebase. NELL-995 is derived from NELL, which semi-automatically constructs the knowledge from web and document. Lastly, UMLS is a biomedical knowledge graph, comprised of medical semantic entities and relations.</p>
<p>Methods: In the task of generating sentences from AMR graphs, existing approaches propose encoding the relations between node pairs via shortest path embedding, such as [119] and [120]. The learned relational encoding enables the attention to capture structural information within the graph. Furthermore, HetGT [121] proposes to extend the AMR graphs into multiple directed subgraphs, such as fullyconnected graph, original graph, reverse graph and connected graph according to its heterogeneity. HetGT then applies attention heads to process each subgraph by masking the attention values of non-neighbor nodes. ASAG [122] proposes to parse both student answers and model answers as AMR graphs, and applies a GT, which takes edge embedding as attention bias, to learn the representations of AMR graphs. Through matching the representations of the student answer and the model answer, ASAG is able to automatically score the student answer. GraphFormer [123] defines relative graph positions in knowledge graph via the shortest path, employing an MLP to model these relative positions as an attention bias, thereby enhancing the encoder's capabilities. KGTransformer [124] introduces Mixture-of-Experts (MoE) to capture complex reasoning patterns and utilizes random walks to sample subgraphs for pre-training by a mask-then-predict strat-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>egy. Relphormer [125] similarly adopts masked knowledge modeling in its KG training strategy, incorporating high-order information from the adjacency matrix as an attention bias.
2) Representation Learning for Textual Graph: In textual graphs, both edges and nodes contains rich textual information rather than simple numerical or categorical attributes. This presents a significant challenge in effectively capturing the intrinsic complexity of the semantic information as well as the contexts from neighboring entities.</p>
<p>Task definition: For a given textual graph $G$, the primary objective of a GT $\phi_{\theta}$ is to learn the representations for nodes $\mathbf{H} \in \mathbb{R}^{n \times d}$ and edges $\mathbf{E} \in \mathbb{R}^{e \times d}$. These representations can be further leveraged for various downstream tasks, including link prediction, node classification and edge classification.</p>
<p>$$
\mathbf{H}, \mathbf{E}=\phi_{\theta}(G)
$$</p>
<p>Datasets: Textual graphs can be categorized into three types: graph with textual nodes, graph with textual edges, and graph with hybrid categorized nodes. Two representative graphs with textual nodes are DBLP ${ }^{2}$ and Wikidata5M ${ }^{3}$. DBLP is a paper citation graph, where the nodes are the titles or abstracts of papers and edges are the citation relationships. Wikidata5M involves the entity graph from Wikipedia, with each node comprising the first sentence of an entity's introduction. Graphs with textual edges include Amazon [226], Goodreads [227] and StackOverflow ${ }^{4}$. Amazon is a user-item interaction graph, incorporating reviews on items as textual edges. Goodreads is a reader-book graph, containing the comments from readers as textual edges. StackOverflow represents an expert-question graph, where the textual edges are answers or questions. Additionally, DBLP, Twitter and Goodreads data can also construct heterogeneous information graphs with both text-rich and textfree nodes. In DBLP, the paper nodes are considered as textrich nodes while venue and author nodes are deemed as textfree nodes. The Twitter dataset contains tweet and POI nodes as text-rich, in contrast to hashtag, user, and mention nodes, which are textl-free. In the Goodreads dataset, book nodes possess abundant textual information, whereas shelves, author, format, publisher, and language code nodes are considered text-free.</p>
<p>Methods: GNN-nested Transformer [123] alternates Transformer and GNN layers to process textual graphs. The Transformer is able to effectively model node embeddings, while the GNN facilitates message propagation between neighbors. For graphs with textual features in the edges, Edgeformer [49] introduces two variants, Edgeformer-E and Edgeformers-N. Edgeformer-E employs self-attention between an edge and its incident nodes to learn the embeddings of textual edges. Edgeformer-N learns the node embedding via aggregating the embeddings of textual edges, following the same way as Edgeformer-E, within each nodes' ego-graph. In more complex scenarios where partial nodes in a graph are associated with texetual features, Heterformer [126] is capable of handling both text-rich and text-free nodes. Heterformer models</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>text-free nodes via distillation from pretrained language model. It learns node embeddings using heterogeneous attention, a self-attention in the ego-graph of a node, additionally assigning different projection matrices for text-free and text-rich nodes. KG-R3 [127] proposes a retrieve-and-read framework for link prediction. Given a query consisting of subject and relation, KD-R3 employs an off-the-shelf algorithm to retrieve a candidate subgraph that related to the query to infer the object. In this vein, KD-R3 further designs a GT that considers both nodes and edges as tokens in self-attention layers, masking those not connected to learn effective subgraph embeddings. The final prediction on the object is performed based on a cross-attention between the query and the learned subgraph representations.</p>
<p>GT-BEHRT [128] is applied to electronic health records (EHRs) graphs to analyze patient conditions. Each patient has a sequence of EHRs, which include diagnostic information such as date, age, and visit type. GT-BEHRT incorporates the types of relations into the EHRs as attention biases when modeling the nodes. It then uses a Transformer block to integrate all EHR embeddings of a patient to predict the illness situation of the patient.</p>
<h2>D. Tasks on Social Networks</h2>
<p>Social networks provide deep insights into human relationships and interactions, which support immense real-world applications. For instance, by modeling the links between comments in a community, it becomes possible to accurately detect rumors. Additionally, analyzing past interactions between users and items reveals user preferences, allowing for more precise recommendations.</p>
<p>1) Citation and Wikipedia Networks: Citation and Wikipedia networks are extensively utilized as standard benchmarks for assessing graph learning approaches. In citation networks, nodes represent individual papers, and edges represent the citation relationships among these papers. Wikipedia networks are formed based on webpage connections. By employing these graphs, graph learning models are required to classify the nodes.</p>
<p>Task Definition: Given a graph $\mathcal{G}={\mathcal{V}, \mathcal{E}}$, the objective of the model is to classify the nodes in $\mathcal{V}$.</p>
<p>Datasets: Cora, Citeseer, and PubMed [228] are three wellknown graph citation networks that capture the citation relationships of computer science and medical papers. In contrast to these traditional datasets, the ogbn-arxiv dataset [179] is a larger graph containing 40 distinct computer science subareas for multi-class node classificaiton. The Actor [229] dataset represents actors as nodes, which are connected based on their co-appearances in Wikipedia. Similarly, the Squirrel and Chameleon datasets [230] are Wikipedia-based networks that model the relationships between pages through links between them.</p>
<p>Methods: Since these datasets represent standard benchmarks, there is no need to design specialized GTs for them. GTs that require evaluation of their performance on node classification tasks will conduct experiments with these datasets, such as NAGphormer [32], Nodeformer [29], Exphormer [31], SGFormer [35], VCR-Graphormer [55], and Cobformer [38].</p>
<p>2) Rumor Detection: Rumors typically propagate through social networks by following communities structures, where user interactions exhibit distinct relational and temporal patterns. These patterns manifest in the form of sharing cascades, reply chains and user engagement behaviors over time. GTs can effectively verify rumors by modeling and analyzing these inherent network dynamics, particularly by the examination of information diffusion pathways and user interaction structures.</p>
<p>Task Definition: Consider a set of tweets $X=$ $\left{x_{1}, x_{2}, \ldots, x_{n}\right}$, where $x_{1}$ represents the original tweet, $x_{i}$ is the $i$-th tweet in chronological sequence, and $n$ is the total number of tweets in the sequence. A graph is constructed based on the relationships between tweets denoted by $R(i, j) \in{$ parent, child, before, after, self $}$. In this context, 'parent' signifies that $x_{i}$ replies to $x_{j}$, 'child' indicates that $x_{i}$ is replied to by $x_{j}$, 'before' designates that $x_{i}$ is posted before $x_{j}$, 'after' indicates that $x_{i}$ is posted after $x_{j}$, and 'self' denotes that $i=j$. Using the set of tweets $X$ and their relationships $R$, the model $\phi_{\theta}$ is designed to predict the rumor category $y$ as true, false, or other states.</p>
<p>Datasets: Twitter15 and Twitter16 [231] contain 1,413 and 756 propagation trees respectively, with each tree labeled as non-rumor, false-rumor, true-rumor, and unverified. PHEME [232] includes 1,972 trees annotated as false-rumor, true-rumor, and unverified.</p>
<p>Methods: StA-HiTPLAN [129] initially employs selfattention on individual tweets to derive sentence-level embeddings. Subsequently, it designs a GT which integrates the relations of tweets as attention bias, to determine the likelihood of the thread being a rumor. Moreover, DGTR [130] applies a Transformer on the graph with centrality encoding with additional temporal Transformer blocks to predict rumors.
3) Recommendation Systems: By representing user-item interactions as graphs, GTs become highly effective in forecasting users' future behaviors based on their historical interactions.</p>
<p>Task Definition: Considering $M$ users $\mathbf{U}=\left{u_{1}, u_{2}, \ldots, u_{M}\right}$ and $N$ items $\mathbf{I}=\left{i_{1}, i_{2}, \ldots, i_{N}\right}$, the interaction matrix $\mathbf{A} \in \mathbb{R}^{M \times N}$ is defined such that $a_{m, n}=1$ if user $u_{m}$ interacts with item $i_{n}$, and $a_{m, n}=0$ otherwise. Consequently, the interaction graph $\mathcal{G}={\mathcal{V}, \mathcal{E}}$ is formed, where the node set $\mathcal{V}=\mathbf{U} \cup \mathbf{I}$, and edge set $\mathcal{E}=\left{e_{m, n} \mid a_{m, n}=1\right}$. Given the graph $\mathcal{G}$ and model $\phi_{\theta}$, the objective is to predict the unobserved interactions:</p>
<p>$$
\hat{y}<em _theta="\theta">{m, n}=\phi</em>)
$$}(\mathcal{G</p>
<p>Dataset: There are several standard real-word recommendation datasets: Yelp ${ }^{5}$, Ifashion [233], LastFM ${ }^{6}$, AmazonCDs [234], Amazon-Music [234] and Epinions [235]. The Yelp dataset is used for recommending business venues and is collected from the Yelp platform. iFashion is focused on recommending fashion outfits and is maintained by Alibaba. LastFM incorporates the records in music applications and Internet radio sites. Amazon-CDs, Amazon-Music and Epinions contain users ratings on items from the Amazon and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Epinions platforms. To capture more comprehensive item features, multi-modal datasets such as Movielens ${ }^{7}$, Tiktok ${ }^{8}$, and Kwai ${ }^{9}$ include textual, acoustic and visual features of videos. Additionally, Tmall ${ }^{10}$ dataset is for Click-Through Rate (CTR) prediction, containing shopping log of users over a six-month period.</p>
<p>Methods: PMGT [131] samples the contextual neighbors of the target node and utilizes Transformer to learn the node embedding by reconstructing the target nodes and the graph structure given the neighbors. GMT [26] introduces four types of interaction graphs to model neighborhood relations for the CTR prediction task, which are the induced subgraph, similarity graph, cross-neighbourhood graph, and complete graph. GMT then implements masked attention to enforce model to capture the local feature. GFormer [132] begins with GNN layers to extract position-aware information from all sampled anchor nodes. GFormer then samples edges from attention map to be processed by GAT. LightGT [133] addresses scenarios where recommended items have multi-modal features by initially employing Transformer layers to integrate user and multi-modal features. LightGT parallelizes GCN and self-attention layers on these multi-modal features for joint prediction. Furthermore, to incorporate the structural prior of user-item interaction into self-attention, LightGT takes the node embeddings learned from a GCN as positional embedding in self-attention. SIGformer [135] introduces two types of attention biases for GT, i.e., sign-aware spectral encoding and sign-aware path encoding. These are designed to distinguish between positive and negative feedback on items from users by integrating the Laplacian matrices of both positive and negative graphs, and the sign-aware paths connecting users and items. TransGNN [134] introduces an approach that utilizes attention mechanism to identify and select relevant nodes for constructing a subgraph. This subgraph is then modeled by the integration of GNN and Transformer layers.</p>
<h2>E. Tasks on Traffic Networks</h2>
<p>In traffic networks, attention mechanisms have demonstrated remarkable effectiveness in modeling both temporal dependencies and structural relationships. The integration of GNNs with attention mechanisms enables comprehensive capture of spatio-temporal patterns, enhancing the ability to model complex traffic dynamics.</p>
<p>1) Trajectory Prediction: The predictions of pedestrian and traffic trajectories pose significant challenges due to the intricate interplay of individuals and temporal dependencies. GTs offer a robust framework for modeling these trajectories by simultaneously capturing spatial relationships and temporal evolution patterns.</p>
<p>Task Definition: The traffic graph is mostly based on DTDG. Given historical trajectories observed from time step 1 to $T_{o b s}$, the aim is to predict future trajectories spanning from $T_{o b s}+1$ to $T_{e n d}$. At each time step $t$, there are $N$ objects denoted as</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>$\left{p_{t}^{i}\right}<em t="t">{i=1}^{N}$, where $p</em>\right}}^{i}=\left(x_{t}^{i}, y_{t}^{i}\right)$ represents the 2 D coordinates of the $i$-th object at time $t$. Therefore, the prediction task can be formulated as developing a model $\phi_{\theta}$ that maps historical observations to future trajectories: $\phi_{\theta}\left(\left{p_{t}^{i<em k="k" o="o" s="s">{i=1}^{T</em>\right}}}\right)=\left{p_{t}^{i<em k="k" o="o" s="s">{i=T</em>$. The graph structure is constructed by representing each object as a node and establishing edges between objects within a prescribed distance threshold.}+1}^{T_{o w d}</p>
<p>Datasets: The ETH [236] dataset comprises two scenes, ETH and HOTEL, recording the movements of individuals numbering between 28 and 60. Similarly, the UCY [237] dataset encompasses three scenes, i.e., UNIV, ZARA1, and ZARA2, with individuals ranging from 30 to 94 . Both datasets extract pedestrian trajectories through video analysis, with the coordinate of pedestrians. Beyond pedestrian-focused datasets, NGSIM [238] and KITTI [239] datasets offer multiple types of object, such as bicycles and vehicles, enabling evaluation of motion prediction models.</p>
<p>Methods: Social Attention [136] constructs spatial edges $\mathbf{e}<em i="i">{i j}^{t}$ and temporal edges $\mathbf{e}</em>}^{t}$. The spatial edge $\mathbf{e<em i="i">{i j}^{t}$ represents the interaction feature between node $\mathbf{v}</em>}$ and node $\mathbf{v<em i="i">{j}$ at time $t$, while the temporal edge $\mathbf{e}</em>}^{t}$ denotes the relationship of node $\mathbf{v<em i="i">{i}$ between two consecutive time steps, $t$ and $t-1$. Social Attention employs attention mechanisms between $\mathbf{e}</em>}^{t}$ and $\mathbf{e<em i="i">{i j}^{t}$, where $j \in \mathcal{N}</em>$, to update the node embedding from both temporal and spatial relations. Trajectron [137] and TrafficPredict [138] follow Social Attention, applying attention on edges to capture the spatial relations in the graph.</p>
<p>STAR [139] introduces specialized modules for both spatial and temporal modeling. The spatial Transformer integrates GAT and feed-forward layers into the Transformer architecture. Meanwhile, the temporal Transformer focuses on modeling the trajectory of each pedestrian by attention mechanism. By combining the embeddings generated from both the spatial and temporal Transformers, STAR can accurately forecasts future trajectories.
2) Event Prediction: In contrast to trajectory prediction, event prediction in traffic contexts deals with comprehensive prediction problems, including predicting congestion, traffic volume, vehicle speed and arrival time, etc. These predictions play a crucial role in traffic management and urban planning. GTs have manifested remarkable capabilities of capturing both long-range temporal and spatial dependencies within the traffic graph.</p>
<p>Task Definition: Given $N$ historical information $\mathbf{X}=$ $\left{x_{i} \mid i=1,2, \ldots, N\right}$ and the corresponding target $\mathbf{Y}=$ $\left{y_{i} \mid i=1,2, \ldots, N\right}$, such as delivery times, the model is designed to forecast the future events $\hat{\mathbf{Y}}=\left{\hat{y}_{i} \mid N+1, N+2 \ldots\right}$.</p>
<p>Datasets: The Xiamen dataset [240] contains 5 months of data recorded by 95 traffic sensors in Xiamen, China. The PeMS dataset [241] includes 6 months of data collected by 325 traffic sensors in the Bay Area. The Abakan and Omsk datasets [242] consist of a road network along with associated routes and their corresponding travel times.</p>
<p>Methods: Given a sequence of historical order, including the retailer, the origin location, the destination, and the payment time, DProQ [142] initially constructs bipartite subgraphs according to the types of nodes. Subsequently, DProQ introduces a heterogeneous GCN to separately model these
subgraphs and update the node embeddings by aggregating the outcomes from each subgraph. Finally, a Transformer block is employed to combine the node embeddings from various types to forecast the ultimate delivery time.</p>
<p>GMAN [141] parallelizes spatial and temporal attention with an additional global attention mechanism to capture both spatial and temporal information in the traffic graph. This approach is used to predict traffic volume and speed.</p>
<p>GCT-TTE [143] concatenates embeddings learned from GCN and a Transformer to capture both local and global information. Additionally, it employs a Transformer block to combine these embeddings, which is used to estimate travel time.</p>
<h2>F. Tasks on Visual Graph</h2>
<p>Transformer [7] has emerged as a prevailing approach in the area of computer vision. Nevertheless, graph-structured data has garnered relatively less focus in this domain. In this section, we categorize visual tasks into those involving 2 D and 3 D data. It is worth noting that 3D data inherently possesses spatial relationships, whereas 2D data can acquire graph structures through segmentation or incorporating other modalities.</p>
<p>1) 3D Data Processing: GTs have been utilized in two types of 3D data: meshes for human pose estimation and point clouds, serving both reasoning and generation purposes. Due to their inherent spatial relationships, these 3D objects exhibit a graph-like structure, enabling GTs to effectively model both local and global interactions.</p>
<p>Task definition: In the context of mesh construction, given a 2D pose image $\mathbf{X} \in \mathbb{R}^{N \times N^{\prime} \times 3}$, where $N$ and $N^{\prime}$ denote the dimensions of the image, GTs aim to predict the 3D coordinates of body joints $\mathbf{J}<em 3="3" D="D">{3 D} \in \mathbb{R}^{K \times 3}$ and mesh vertices $\mathbf{V}</em>)$.} \in \mathbb{R}^{M \times 3}$. Here, $K$ and $M$ correspond to the number of joints and vertices, respectively. For the 3D scene reconstruction task using point clouds, the input is a point cloud $\mathbf{P} \in \mathbb{R}^{P \times C_{\text {point }}}$, where $P$ and $C_{\text {point }}$ represent the number of points and the channels per point. The model is capable of capturing the relationships in the point cloud and generating a scene graph $\mathcal{G}=(\mathcal{V}, \mathcal{E</p>
<p>Datasets: We present the datasets in accordance with the methodology of modeling 3D data as a mesh and a point cloud respectively. 1) Mesh: The datasets include Human3.6M [243], 3DPW [244], and FreiHAND [245]. The original two datasets focus on human motion. Human3.6M contains 3.6 million video frames with pseudo-ground truth mesh annotations, while 3DPW is an in-the-wild dataset comprising 51,000 video frames with ground truth 3D poses and mesh annotations. The subsequent dataset, FreiHAND, consists of 33,000 frames with 3D hand poses and shape annotations. 2) Point Cloud: The 3DSSG [246] dataset provides 3D point clouds along with annotated semantic scene graphs that include objects and their relationships.</p>
<p>Methods: Mesh Graphormer [51] first leverages a pretrained CNN to derive image grid features, which is then fed into Transformer and GNN blocks, along with a template mesh, to capture both local and global information. GTRS [144] makes</p>
<p>use of a standard 2D pose detector to extract human poses, which are subsequently provided to GNN and Transformer layers in parallel to learn a 3D embedding. SGFormer [145] incorporates edge-aware self-attention, such as the dot product between the attention matrix and edge embedding, to update both node and edge embeddings simultaneously, thereby facilitating direct predictions of objects and their relationships.
2) Representation Learning for Cellar Images: Gigapixel images pose significant computational challenges for Transformers. Whole slide images, which are exemplary gigapixel images, exhibit cell-graph structures. Consequently, it is a natural idea to create a graph within the image patches by utilizing the cell information contained in the image.</p>
<p>Task Definition: In giga-pixel images, the images are divided into smaller patches. Patches that do not predominantly contain cells are discarded. For the remaining patches, to construct a graph, each path is a node and pairs of patches are connected as an edge. This graph construction method is shared in following studies where GTs are employed to learn imagelevel embeddings for various downstream tasks, including survival prediction and prognosis analysis.</p>
<p>Datasets: Whole slide images (WSIs) are utilized in pathology to assess cancer grade. The CPTAC [247] dataset comprises 2,071 WSIs from 435 patients, and the TCGA [248] dataset includes 2,082 WSIs from 996 patients. For multimodal WSIs, the InUIT [147] dataset contains 1,600 WSIs from 188 patients featuring the Ki67, CD8, and CD20 biomarkers. Each biomarker is treated as a distinct modality for the purpose of learning image representations. Similarly, the MIBC [249] dataset consists of 585 WSIs from 58 patients, incorporating the Ki67, CK20, and P16 biomarkers.</p>
<p>Methods: Given that Vision Transformers are hindered by its quadratic complexity when handling thousands of patches, GTP [146] initially leverages GCN and pooling layers to aggregate local information and reduce the number of patches. Furthermore, AMIGO [147] constructs multiple graphs from histopathology images obtained via distinct biomarkers. AMIGO then employs a shared GNN to capture diverse histopathological features and integrate this information using Transformer layers. MulGT [148], serving as a multi-task learning framework, proposes a task-specific Transformer dedicated to each task subsequent to GCN layers. SpaFormer [149] delineates the cellar image to form a graph and incorporates the node embedding into a Transformer with random walk and Laplacian PE.
3) Visual language Tasks: Cross-attention layers serves as crucial mechanisms for fusing features across vision and language modalities. When either modality exhibits a graph structures, incorporating a graph aware cross-attention mechanism can significantly enhance model performance.</p>
<p>Task Definition: Visual grounding is the process of localizing specific regions in an image that correspond to given natural language descriptions. In a related domain, vision-and-language navigation represents a complex task where an agent should interpret textual instructions to navigate through an environment towards a specific destination.</p>
<p>Datasets: 1) Visual Grounding. RefCOCO [250], RefCOCO+ [250], and RefCOCOg [251] provide extensive
annotations linking natural language descriptions to visual bounding boxes. In RefCOCO, one expert describes an object while other experts identify its corresponding region. RefCOCO+ builds upon RefCOCO by eliminating location words from textual descriptions. RefCOCOg is developed in a non-interactive setting where experts directly identify the object in an image based on textual descriptions. 2) Vision-and-Language Navigation. Datasets like REVERIE [252], SOON [253], and R2R [254] support the research on this domain. REVERIE and SOON provide textual instructions that describe target locations and objects, while R2R offers detailed step-by-step navigation guidance. These datasets challenge agents to precisely identify specific objects' bounding boxes of locations.</p>
<p>Methods: M-DGT [150] is designed for visual grounding by constructing a graph where image patches initialize ndoes and edges. M-DGT progressively refines the bounding box to approximate the ground truth. To incorporate information from the textual query, M-DGT treats the neighbors of a node as response. The edge embeddings are derived from the relative spatial relations between nodes. These aggregated edge embeddings and node embeddings are then concatenated to interactively refine the bounding box coordinates.</p>
<p>DUET [151] constructs the graph by utilizing the historically visited nodes. When the agent encounters an unobserved object, it will be integrated into the graph and linked to the current node. To facilitate language navigation, the textual instructions are combined with node embeddings through cross-attention layers, and then the fused embeddings are processed by self-attention mechanisms with spatial distance bias to incorporate topological information. Based on the learned embeddings, the agent is able to determine the target node for navigation.</p>
<h2>G. Tasks on Other Domains</h2>
<p>In this section, we will review applications of GTs in brain network and material.</p>
<p>1) Brain Network Analysis: Brain networks are constructed from functional Magnetic Resonance Imaging (fMRI) data, where Regions of Interest (ROIs) represent specific brain areas. In graph represnetation, ROIs serve as nodes, with edges defined by pairwise correlations between the blood-oxygen-level-dependent (BOLD) signal sequences. This graph representation of fMRI data enables GTs to predict various characteristics of the brain subject, including sex and disease presence.</p>
<p>Task Definition: For a brain network $\mathcal{G}$, the node set $\mathcal{V}$ is composed of $n$ ROIs, and edges $\mathcal{E}$ are established through BOLD signal correlations between ROIs, forming a weighted adjacency matrix $\mathbf{A} \in \mathbb{R}^{n \times n}$. The model $\phi_{\theta}$ aims to predict a specific property $y$ of the brain subject.</p>
<p>$$
y=\phi_{\theta}(\mathcal{G})
$$</p>
<p>Datasets: The ABIDE dataset [255] consists of brain networks from 1,009 participants, primarily aims at diagnosing Autism Spectrum Disorder (ASD). Among these, 516 participants are diagnosed as ASD. The ABCD dataset [256] includes brain networks from 7,091 individuals, with 3,961 from</p>
<p>female subjects, focusing on predicting the biological sex of participants. REST-meta-MDD [257] features 2,026 brain networks from both individuals with Major Depressive Disorders (MDD) and healthy controls. The ANDI dataset [258] provides 54 Alzheimer's disease (AD) samples and 76 normal control samples. The UK Biobank (UKB) dataset [259] comprises data from 16,458 participants across various ages.</p>
<p>Method: BrainNetTF [153] uses the weighted adjacency matrix as the input for Transformer to derive the attentionenhanced node features. These features are then processed by a clustering-based readout function to derive the graph embedding. Cai et al. [154] combines GNN and Transformer blocks to capture both local and global information for brain network classification. THC [155] focuses on identifying functional modules in the brain, learned from the attention matrix of $\mathbf{X}$ using a clustering module. It employs GNNs for local feature extraction, which are then fed into a Transformer to learn graph-level embeddings. ALTER [157] captures longterm edge relationships via random walk based methods and uses the generated embeddings as attention biases in a Transformer. BioBGT [158] introduces functional moduleaware self-attention, which multiplies attention matrix with functional feature in brain. This approach includes functional segregation and integration characteristics in brain graphs.
2) Crystal Material Property Prediction: Unlike molecular graphs, crystal material graphs are periodic, with unit cells repeating in an infinite 3D space. Therefore, models need to learn graph representations that capture these periodic patterns.</p>
<p>Task Definition: Similar to molecular graphs, the unit cell of a crystal material comprises features $\mathbf{H}$ and coordinates $\overrightarrow{\mathbf{X}}$. Additionally, the unit cell encompasses a lattice matrix $\mathbf{L}=\left[\mathbf{l}<em 2="2">{1}, \mathbf{l}</em>$, the infinite crystal structure can be represented as:}, \mathbf{l}_{3}\right] \in \mathbb{R}^{3 \times 3}$ to denote the directions of periodicity. Specifically, given a crystal structure $\overrightarrow{\mathcal{G}</p>
<p>$$
\overrightarrow{\mathbf{X}}=\left{\overrightarrow{\mathbf{X}}<em i="i">{i} \mid \overrightarrow{\mathbf{X}}</em>}=\overrightarrow{\mathbf{X}<em 1="1">{i}+k</em>} \mathbf{l<em 2="2">{1}+k</em>} \mathbf{l<em 3="3">{2}+k</em>} \mathbf{l<em 1="1">{3}, k</em>\right}
$$}, k_{2}, k_{3} \in \mathcal{Z</p>
<p>Given the above conditions, a model $\phi_{\theta}$ is designed to predict a target property $y$.</p>
<p>$$
y=\phi_{\theta}(\overrightarrow{\mathcal{G}}, \mathbf{L})
$$</p>
<p>Datasets: The Materials Project [260] contains crystals with corresponding formation energy and band gap of crystal materials. Jarvis [261] dataset includes five properties of crystal materials, i.e., formation energy, bandgap(OPT), bandgap(MBJ), total energy, and Ehull. MatBench [262] contains 132k crystals and 636 2D crystals for evaluation. The OMat24 dataset [162] comprises 118 million structures, labeled with energy, forces, and cell stress. Most of these structures contain no more than 20 atoms.</p>
<p>Method: Matformer [57] constructs a fully connected graph where the edge features are derived from the Euclidean distance between nodes. It then integrates the edge embeddings directly into the key and value vectors, serving as an additional attention head. CrystalFormer [159] maintains the strong periodic invariance and angular information in crystals, using an alternative strategy for graph construction. For model
architecture, similar to Matformer, CrystalFormer incorporates edge embeddings into the query and key vectors. CrysGraphFormer [160] uses edge embedding with spatial relations as attention mask in GT. Moreover, ComFormer [163] constructs both $\mathrm{SE}(3)$ invariant and $\mathrm{SO}(3)$ equivariant Transformer layers to capture the complete geometric information of crystals.</p>
<p>MatterSim [161] and OMat24 [162] leverage Graphormer [10] and EquiformerV2 [14] as their backbones, respectively. MatterSim is trained across a range of elements, temperatures, and pressures, leading to improved prediction generalization. In contrast, OMat24 focuses on material discovery and has developed a large-scale dataset containing 110 million materials. Therefore, OMat24 demonstrates superior performance in material discovery by pretraining.</p>
<h2>VI. Future Directions</h2>
<p>The rapid advancement of GTs opens several promising avenues for future exploration. We highlight these research opportunities as follows.</p>
<p>Graph Foundational Model by Graph Transformer. Transformer has acted as a cornerstone in building foundational models across various domains, such as natural language processing and computer vision. For instance, large language models have demonstrated remarkable performance across various tasks involving language understanding and generation. However, the critical importance of graph-structured data representation in scientific modeling and social network analysis has led to significant interest in Graph Foundational Models [263]. Pioneering works such as GROVER [9] and DPA-2 [88], which construct the atomic model grounded in GTs, pretrain on molecules and crystals using a multi-task methodology, establish a new paradigm for scientific machine learning. These developments highlight the immense potential of GTs as fundamental building blocks for constructing nextgeneration Graph Foundational Models across diverse application domains.</p>
<p>Scaling Graph Transformer. Despite the remarkable success achieved in scaling Transformer, a question remains whether scaling up the GTs would similarly enhance performance. Uni-Mol2 [92] scales the GT to billions of parameters, showcasing improvements on molecular downstream tasks. This scalability in Uni-Mol2 is feasible due to abundant molecular graph data. However, scaling GTs in domains with limited graph-structured data remains a significant obstacle. Innovative approaches like LM-design [115] utilizes GNNs as structural adapters for pretrained protein language models, integrating both limited structural information and abundant sequence data from existing protein datasets. Despite these advancements, the field still lacks a comprehensive framework that effectively addresses the fundamental challenges of scaling GTs in dataconstrained environments.</p>
<p>Cross-modal Graph Transformer. Integrating a GNN with a pretrained Transformer from the language domain allows the model to generate captions for graphs. MolCA [264], an example of cross-modal GT, employs a molecular GNN to encode molecular representations and feeds these embeddings into a Transformer decoder pretained by languages to generate</p>
<p>captions for the molecule. In the context of complex graph systems, e.g., proteins [265], utilizing GTs to integrate graph structures offers an opportunity to enhance our comprehension of protein properties through pretrained language models, thereby indicating a promising direction for future research.</p>
<p>Alternative Approaches to Capture Long-range Dependencies. Recently, the Transformer architecture has encountered increased competition, evidenced by models like Mamba [266]. Notably, Graph Mamba [267] has demonstrated very competitive performance when compared to GTs. As the Transformer model presents limitations, including scalability and over-smoothing issues as outlined in the survey, the success of Graph Mamba suggests the possibility of another model capable of effectively capturing global interactions, particularly within graph data. A promising direction for future research involves the development of a model that might deviate from the self-attention paradigm and more efficiently capture global interactions, circumventing the issues inherent in Transformer.</p>
<h2>VII. CONCLUSION</h2>
<p>In this survey, we conduct a comprehensive review of the recent advancements in Graph Transformer. Our review begins with methods for integrating structural information into the Transformer framework, covering aspects such as multi-level graph tokenization, structural positional encoding, structureaware attention mechanisms, and model ensemble between GNNs and Transformers. We also introduce two prevalent challenges associated with Graph Transformer, scalability and equivariance. Additionally, we explore theoretical developments to uncover connections and evaluate the expressiveness of GTs. Following this, we investigate a variety of applications, detailing the datasets and architectures employed by GTs in domains such as biological, social, textual, and visual graphs. Concluding with an exploration of prospective research avenues, this survey aims to furnish valuable insights and guidance for future investigations in the field of Graph Transformer.</p>
<h2>REFERENCES</h2>
<p>[1] T. N. Kipf and M. Welling, "Semi-supervised classification with graph convolutional networks," in 5th International Conference on Learning Representations, ICLR 2017,. OpenReview.net, 2017.
[2] Y. Liu, J. Cheng, H. Zhao, T. Xu, P. Zhao, F. Tsung, J. Li, and Y. Rong, "SEGNO: Generalizing equivariant graph neural networks with physical inductive biases," in The Twelfth International Conference on Learning Representations, 2024.
[3] Y. Rong, W. Huang, T. Xu, and J. Huang, "Dropedge: Towards deep graph convolutional networks on node classification," in 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net, 2020.
[4] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun, "Measuring and relieving the over-smoothing problem for graph neural networks from the topological view," in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 04, 2020, pp. 3438-3445.
[5] D. Kreuzer, D. Beaini, W. Hamilton, V. Ltourneau, and P. Tossou, "Rethinking graph transformers with spectral attention," Advances in Neural Information Processing Systems, vol. 34, pp. 21618-21629, 2021.
[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need," in Advances in neural information processing systems, 2017, pp. 59986008 .
[7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2021.
[8] Q. Wen, T. Zhou, C. Zhang, W. Chen, Z. Ma, J. Yan, and L. Sun, "Transformers in time series: A survey," in Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI-23. International Joint Conferences on Artificial Intelligence Organization, 8 2023, pp. 6778-6786, survey Track. [Online]. Available: https://doi.org/10.24963/ijcai.2023/759
[9] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang, "Self-supervised graph transformer on large-scale molecular data," in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.
[10] C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and T.-Y. Liu, "Do transformers really perform badly for graph representation?" in Advances in neural information processing systems, 2021, pp. 2887728888.
[11] P. Thlke and G. De Fabritiis, "Torchmd-net: equivariant transformers for neural network based molecular potentials," arXiv preprint arXiv:2202.02341, 2022.
[12] F. B. Fuchs, D. E. Worrall, V. Fischer, and M. Welling, "Se(3)transformers: 3d roto-translation equivariant attention networks," in Advances in Neural Information Processing Systems 34 (NeurIPS), 2020.
[13] Y.-L. Liao and T. Smidt, "Equiformer: Equivariant graph attention transformer for 3d atomistic graphs," in The Eleventh International Conference on Learning Representations, 2023.
[14] Y.-L. Liao, B. Wood, A. Das<em>, and T. Smidt</em>, "EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations," in International Conference on Learning Representations (ICLR), 2024.
[15] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and P. Frossard, "Digress: Discrete denoising diffusion for graph generation," in The Eleventh International Conference on Learning Representations, 2023.
[16] E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao, J. Huang, S. Ananiadou, and Y. Rong, "Transformer for graphs: An overview from architecture perspective," arXiv preprint arXiv:2202.08453, 2022.
[17] L. Mller, M. Galkin, C. Morris, and L. Rampasek, "Attending to graph transformers," Transactions on Machine Learning Research, 2024.
[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "BERT: Pretraining of deep bidirectional transformers for language understanding," in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. $4171-4186$.
[19] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.
[20] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, "Convolutional sequence to sequence learning," in International conference on machine learning. PMLR, 2017, pp. 1243-1252.
[21] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," Neurocomputing, vol. 568, p. 127063, 2024.
[22] V. P. Dwivedi and X. Bresson, "A generalization of transformer networks to graphs," AAAI Workshop on Deep Learning on Graphs: Methods and Applications, 2021.
[23] G. Mialon, D. Chen, M. Selosse, and J. Mairal, "Graphit: Encoding graph structure in transformers," arXiv preprint arXiv:2106.05667, 2021.
[24] J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and S. Hong, "Pure transformers are powerful graph learners," in Advances in Neural Information Processing Systems, 2022, pp. 14582-14595.
[25] D. Chen, L. OBray, and K. Borgwardt, "Structure-aware transformer for graph representation learning," in International Conference on Machine Learning. PMLR, 2022, pp. 3469-3489.
[26] E. Min, Y. Rong, T. Xu, Y. Bian, D. Luo, K. Lin, J. Huang, S. Ananiadou, and P. Zhao, "Neighbour interaction based click-through rate prediction via graph-masked transformer," in Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2022, pp. 353-362.
[27] L. Rampavsek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, and D. Beaini, "Recipe for a general, powerful, scalable graph transformer,"</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5} \mathrm{https}: / /$ www.yelp.com/dataset
${ }^{6}$ http://ocelma.net/MusicRecommendationDataset/lastfm-360K.html&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7} \mathrm{https}: / /$ movielens.org/.
${ }^{8} \mathrm{https}: / /$ www.tiktok.com/.
${ }^{9} \mathrm{https}: / /$ www.kuaishou.com/activity/uimc.
${ }^{10} \mathrm{https}: / /$ tianchi.aliyun.com/dataset/dataDetail?dataId=42.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
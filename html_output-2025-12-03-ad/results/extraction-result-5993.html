<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5993 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5993</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5993</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-121.html">extraction-schema-121</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-269626694</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.04818v2.pdf" target="_blank">ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation</a></p>
                <p><strong>Paper Abstract:</strong> Evaluating the quality of free-text explanations is a multifaceted, subjective, and labor-intensive task. Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency. In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to evaluate how LLMs rate explanations. We observed that larger models outputted labels that maintained or increased the inter-annotator agreement, suggesting that they are within the expected variance between human raters. However, their correlation with majority-voted human ratings varied across different quality aspects, indicating that they are not a complete replacement. In turn, using LLMs as a supplement to a smaller group of human raters in some cases improved the correlation with the original majority labels. However, the effect was limited to cases where human raters were scarce, and an additional human rater had a more pronounced effect in all cases. Overall, we recommend against using LLMs as a complete replacement for human raters but encourage using them in configurations that end with targeted human involvement. Data available here: https://github.com/a-brassard/ACORN</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5993.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5993.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ACORN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aspect-wise Commonsense Reasoning Explanation Evaluation (ACORN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and evaluation framework of 3,500 free-text explanations with eight aspect-wise human quality ratings, used to study whether LLMs can reliably evaluate explanations and to quantify methods for LLM-based evaluation versus human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Crowdsourced human annotation (Amazon Mechanical Turk) with qualification and trial rounds, aggregated via majority voting; LLM-based evaluation using verbatim compound prompting to produce structured ratings; label extraction via rule-based parsing backed by LLM extraction and manual inspection; statistical comparison using inter-annotator agreement (Krippendorff's α) and rank correlation (Spearman's ρ); experiments replacing a human rater with an LLM and adding an LLM as an extra rater under varying numbers of human raters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Eight aspect-wise criteria: (1) Supports (which answer the explanation supports, used to detect faithfulness), (2) Overall (holistic quality), (3) Well-Written (fluency/coherence/grammar), (4) Related (relevance to question and answers), (5) Factual (truthfulness of factual claims), (6) New Information (amount of information beyond question/answers: None/Some/Sufficient/Ample), (7) Unnecessary Information (presence of irrelevant info), (8) Contrastive (does explanation contrast correct and incorrect answers).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o (gpt-4o-2024-05-13), Meta Llama-3.1-405B-Instruct-Turbo, Gemma-2-27b-it, Mixtral-8x22B-Instruct-v0.1.3</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Commonsense reasoning explanation evaluation / predict-and-explain multiple-choice QA (textual explanations), not scientific theories per se</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Evaluation concerns free-text justifications that explain a model's multiple-choice answer in commonsense QA benchmarks; ACORN evaluates the quality of such explanations across multiple fine-grained aspects to study alignment between LLM-based and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>LLMs' labels were moderately-to-strongly correlated with majority-voted human labels depending on criterion: average Spearman's ρ ≈ 0.72 (multiplied by 1 in raw form), supports ρ ≈ 0.84, contrastive ρ ≈ 0.93, unnecessary information ρ ≤ 0.54; best model (GPT-4o) highest in 5/7 criteria; Mixtral close and better on unnecessary information and contrastive; Gemma-2 performed worse and reduced agreement in most criteria. Inter-annotator agreement (Krippendorff's α) was maintained or slightly increased when replacing a random human rater with stronger LLMs (GPT-4o, Mixtral, Llama-3.1) but dropped for Gemma-2. Label extraction failure rates were <0.2% for all models except Gemma-2 (2.7%). Random Forest regressor (MSE=0.37) found feature importances: New Information 58%, Factual 20%, Unnecessary Information 9%, Well-Written 7%, Contrastive 4%, Supports 2%, Related 0%. In limited-rater scenarios, adding an LLM improved alignment when human raters ≤2 (increase in correlation by ~+0.019 for 2→2H+LLM and +0.016 for 1→1H+LLM), but with ≥3 human raters adding an LLM was neutral or detrimental (with 4 humans, adding model raised correlation by only +0.004).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ACORN dataset (3,500 explanations, 140k ratings total); source datasets: BCOPA/COPA-SSE, CommonsenseQA and associated explanation datasets CoS-E and ECQA; subsets include 500 samples per source and 500+250 fluency-improved variants generated via GPT-3.5 for some subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>LLMs behaved similarly to average individual human raters (within expected human label variance) but did not perfectly match majority-voted human labels; LLMs can sometimes substitute or supplement humans when human raters are scarce (<3), yet adding an additional human rater usually produced a larger improvement than adding an LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluation is subjective and majority-vote gold labels are not infallible; LLM sensitivity to prompt format (best: verbatim compound prompting) and label-extraction noise complicate use; imperfect alignment for some fine-grained aspects (notably 'unnecessary information'); potential content filters in commercial LLMs triggered on some samples; improvements from LLMs limited to low-human-rater regimes; results pertain to commonsense explanation evaluation and may not generalize to other domains (e.g., formal scientific theory evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5993.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5993.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics & procedures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-annotator agreement, rank correlation, prompting & extraction procedures used to evaluate LLM-generated evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concrete methods and metrics used in the paper to assess LLMs as evaluators: Krippendorff's α for inter-annotator agreement, Spearman's rank correlation for alignment with majority votes, prompting strategies (verbatim compound prompting), rule-based + LLM-backed label extraction, and controlled replacement/addition experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Measure harmony with human variability via Krippendorff's α (replace a random human rater with an LLM across 20 iterations); measure alignment with majority-voted labels via Spearman's rank correlation (per criterion); perform ablations: LLM replacing all humans vs. LLM added as extra rater with varying numbers of human raters; compare prompting strategies (single vs compound calls, default/averaged/verbatim, zero-shot vs 3-shot) and select best performing prompt; extraction via deterministic rule-based parsing with LLM fallback and manual inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Not the content criteria (those are in ACORN) but the evaluation metrics: Krippendorff's α (agreement), Spearman's ρ (correlation with aggregated human labels), label extraction failure rate, Random Forest feature importance and MSE for predicting overall ratings from aspect labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Same LLMs evaluated as labelers: GPT-4o, Llama-3.1-405B, Gemma-2-27B, Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>theory_domain</strong></td>
                            <td>Methodology for evaluation of textual explanations in commonsense QA; broadly applicable evaluation procedures for LLM-generated outputs</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Procedure to test whether LLMs can act as human-like evaluators: use crowdsourced multi-aspect human labels as gold, run LLMs with carefully chosen prompts to output structured aspect labels, extract labels reliably, and compare via agreement and correlation metrics plus controlled experiments replacing/adding raters.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Best prompting strategy: verbatim compound prompting (input matching human guidelines) produced highest correlations; extraction failures were rare (<0.2% except Gemma-2 at 2.7%); Krippendorff's α showed LLMs generally within human variance for stronger models; Spearman correlations varied by criterion (avg ~0.72, wide range across aspects). Adding LLMs as extra rater yields modest gains only with ≤2 human raters; replacing human raters with the best LLM yields moderate alignment but not perfect replacement.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ACORN (3,500 explanations with 5 human raters each); source benchmarks: CommonsenseQA, BCOPA/COPA-SSE, CoS-E, ECQA, and generated/fluency-improved subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human</strong></td>
                            <td>Direct comparison via simulated replacement (one human replaced by LLM) and supplementing (adding LLM to human votes); results indicate LLMs behave similarly to an individual human rater in many aspects but fall short of majority human consensus in several criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Metrics sensitive to subjectivity (Krippendorff's α depends on label variance); Spearman correlation summarizes ordinal alignment but hides per-item disagreements; prompt sensitivity means performance may vary with prompt engineering; label extraction from free text is an additional failure mode; conclusions are limited to explanation evaluation domain and the chosen LLMs/versions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reframing Human-AI Collaboration for Generating Free-Text Explanations <em>(Rating: 2)</em></li>
                <li>A study of automatic metrics for the evaluation of natural language explanations <em>(Rating: 2)</em></li>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization <em>(Rating: 2)</em></li>
                <li>Are large language model-based evaluators the solution to scaling up multilingual evaluation? <em>(Rating: 1)</em></li>
                <li>Explain yourself! Leveraging language models for commonsense reasoning <em>(Rating: 1)</em></li>
                <li>CoS-E: Commonsense Explanations for CommonsenseQA (Rajani et al.) <em>(Rating: 2)</em></li>
                <li>ECQA: Explanations for CommonsenseQA (Aggarwal et al.) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5993",
    "paper_id": "paper-269626694",
    "extraction_schema_id": "extraction-schema-121",
    "extracted_data": [
        {
            "name_short": "ACORN",
            "name_full": "Aspect-wise Commonsense Reasoning Explanation Evaluation (ACORN)",
            "brief_description": "A dataset and evaluation framework of 3,500 free-text explanations with eight aspect-wise human quality ratings, used to study whether LLMs can reliably evaluate explanations and to quantify methods for LLM-based evaluation versus human raters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Crowdsourced human annotation (Amazon Mechanical Turk) with qualification and trial rounds, aggregated via majority voting; LLM-based evaluation using verbatim compound prompting to produce structured ratings; label extraction via rule-based parsing backed by LLM extraction and manual inspection; statistical comparison using inter-annotator agreement (Krippendorff's α) and rank correlation (Spearman's ρ); experiments replacing a human rater with an LLM and adding an LLM as an extra rater under varying numbers of human raters.",
            "evaluation_criteria": "Eight aspect-wise criteria: (1) Supports (which answer the explanation supports, used to detect faithfulness), (2) Overall (holistic quality), (3) Well-Written (fluency/coherence/grammar), (4) Related (relevance to question and answers), (5) Factual (truthfulness of factual claims), (6) New Information (amount of information beyond question/answers: None/Some/Sufficient/Ample), (7) Unnecessary Information (presence of irrelevant info), (8) Contrastive (does explanation contrast correct and incorrect answers).",
            "llm_model_name": "GPT-4o (gpt-4o-2024-05-13), Meta Llama-3.1-405B-Instruct-Turbo, Gemma-2-27b-it, Mixtral-8x22B-Instruct-v0.1.3",
            "theory_domain": "Commonsense reasoning explanation evaluation / predict-and-explain multiple-choice QA (textual explanations), not scientific theories per se",
            "theory_description": "Evaluation concerns free-text justifications that explain a model's multiple-choice answer in commonsense QA benchmarks; ACORN evaluates the quality of such explanations across multiple fine-grained aspects to study alignment between LLM-based and human judgments.",
            "evaluation_results": "LLMs' labels were moderately-to-strongly correlated with majority-voted human labels depending on criterion: average Spearman's ρ ≈ 0.72 (multiplied by 1 in raw form), supports ρ ≈ 0.84, contrastive ρ ≈ 0.93, unnecessary information ρ ≤ 0.54; best model (GPT-4o) highest in 5/7 criteria; Mixtral close and better on unnecessary information and contrastive; Gemma-2 performed worse and reduced agreement in most criteria. Inter-annotator agreement (Krippendorff's α) was maintained or slightly increased when replacing a random human rater with stronger LLMs (GPT-4o, Mixtral, Llama-3.1) but dropped for Gemma-2. Label extraction failure rates were &lt;0.2% for all models except Gemma-2 (2.7%). Random Forest regressor (MSE=0.37) found feature importances: New Information 58%, Factual 20%, Unnecessary Information 9%, Well-Written 7%, Contrastive 4%, Supports 2%, Related 0%. In limited-rater scenarios, adding an LLM improved alignment when human raters ≤2 (increase in correlation by ~+0.019 for 2→2H+LLM and +0.016 for 1→1H+LLM), but with ≥3 human raters adding an LLM was neutral or detrimental (with 4 humans, adding model raised correlation by only +0.004).",
            "benchmarks_or_datasets": "ACORN dataset (3,500 explanations, 140k ratings total); source datasets: BCOPA/COPA-SSE, CommonsenseQA and associated explanation datasets CoS-E and ECQA; subsets include 500 samples per source and 500+250 fluency-improved variants generated via GPT-3.5 for some subsets.",
            "comparison_to_human": "LLMs behaved similarly to average individual human raters (within expected human label variance) but did not perfectly match majority-voted human labels; LLMs can sometimes substitute or supplement humans when human raters are scarce (&lt;3), yet adding an additional human rater usually produced a larger improvement than adding an LLM.",
            "limitations_or_challenges": "Evaluation is subjective and majority-vote gold labels are not infallible; LLM sensitivity to prompt format (best: verbatim compound prompting) and label-extraction noise complicate use; imperfect alignment for some fine-grained aspects (notably 'unnecessary information'); potential content filters in commercial LLMs triggered on some samples; improvements from LLMs limited to low-human-rater regimes; results pertain to commonsense explanation evaluation and may not generalize to other domains (e.g., formal scientific theory evaluation).",
            "uuid": "e5993.0",
            "source_info": {
                "paper_title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Evaluation metrics & procedures",
            "name_full": "Inter-annotator agreement, rank correlation, prompting & extraction procedures used to evaluate LLM-generated evaluations",
            "brief_description": "Concrete methods and metrics used in the paper to assess LLMs as evaluators: Krippendorff's α for inter-annotator agreement, Spearman's rank correlation for alignment with majority votes, prompting strategies (verbatim compound prompting), rule-based + LLM-backed label extraction, and controlled replacement/addition experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_method": "Measure harmony with human variability via Krippendorff's α (replace a random human rater with an LLM across 20 iterations); measure alignment with majority-voted labels via Spearman's rank correlation (per criterion); perform ablations: LLM replacing all humans vs. LLM added as extra rater with varying numbers of human raters; compare prompting strategies (single vs compound calls, default/averaged/verbatim, zero-shot vs 3-shot) and select best performing prompt; extraction via deterministic rule-based parsing with LLM fallback and manual inspection.",
            "evaluation_criteria": "Not the content criteria (those are in ACORN) but the evaluation metrics: Krippendorff's α (agreement), Spearman's ρ (correlation with aggregated human labels), label extraction failure rate, Random Forest feature importance and MSE for predicting overall ratings from aspect labels.",
            "llm_model_name": "Same LLMs evaluated as labelers: GPT-4o, Llama-3.1-405B, Gemma-2-27B, Mixtral",
            "theory_domain": "Methodology for evaluation of textual explanations in commonsense QA; broadly applicable evaluation procedures for LLM-generated outputs",
            "theory_description": "Procedure to test whether LLMs can act as human-like evaluators: use crowdsourced multi-aspect human labels as gold, run LLMs with carefully chosen prompts to output structured aspect labels, extract labels reliably, and compare via agreement and correlation metrics plus controlled experiments replacing/adding raters.",
            "evaluation_results": "Best prompting strategy: verbatim compound prompting (input matching human guidelines) produced highest correlations; extraction failures were rare (&lt;0.2% except Gemma-2 at 2.7%); Krippendorff's α showed LLMs generally within human variance for stronger models; Spearman correlations varied by criterion (avg ~0.72, wide range across aspects). Adding LLMs as extra rater yields modest gains only with ≤2 human raters; replacing human raters with the best LLM yields moderate alignment but not perfect replacement.",
            "benchmarks_or_datasets": "ACORN (3,500 explanations with 5 human raters each); source benchmarks: CommonsenseQA, BCOPA/COPA-SSE, CoS-E, ECQA, and generated/fluency-improved subsets.",
            "comparison_to_human": "Direct comparison via simulated replacement (one human replaced by LLM) and supplementing (adding LLM to human votes); results indicate LLMs behave similarly to an individual human rater in many aspects but fall short of majority human consensus in several criteria.",
            "limitations_or_challenges": "Metrics sensitive to subjectivity (Krippendorff's α depends on label variance); Spearman correlation summarizes ordinal alignment but hides per-item disagreements; prompt sensitivity means performance may vary with prompt engineering; label extraction from free text is an additional failure mode; conclusions are limited to explanation evaluation domain and the chosen LLMs/versions.",
            "uuid": "e5993.1",
            "source_info": {
                "paper_title": "ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reframing Human-AI Collaboration for Generating Free-Text Explanations",
            "rating": 2,
            "sanitized_title": "reframing_humanai_collaboration_for_generating_freetext_explanations"
        },
        {
            "paper_title": "A study of automatic metrics for the evaluation of natural language explanations",
            "rating": 2,
            "sanitized_title": "a_study_of_automatic_metrics_for_the_evaluation_of_natural_language_explanations"
        },
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_yet_humanlevel_evaluators_for_abstractive_summarization"
        },
        {
            "paper_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
            "rating": 1,
            "sanitized_title": "are_large_language_modelbased_evaluators_the_solution_to_scaling_up_multilingual_evaluation"
        },
        {
            "paper_title": "Explain yourself! Leveraging language models for commonsense reasoning",
            "rating": 1,
            "sanitized_title": "explain_yourself_leveraging_language_models_for_commonsense_reasoning"
        },
        {
            "paper_title": "CoS-E: Commonsense Explanations for CommonsenseQA (Rajani et al.)",
            "rating": 2,
            "sanitized_title": "cose_commonsense_explanations_for_commonsenseqa_rajani_et_al"
        },
        {
            "paper_title": "ECQA: Explanations for CommonsenseQA (Aggarwal et al.)",
            "rating": 2,
            "sanitized_title": "ecqa_explanations_for_commonsenseqa_aggarwal_et_al"
        }
    ],
    "cost": 0.010705749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation
2 Sep 2024</p>
<p>Ana Brassard ana.brassard@riken.jp 
Tohoku University
γ MBZUAI</p>
<p>Benjamin Heinzerling benjamin.heinzerling@riken.jp 
Tohoku University
γ MBZUAI</p>
<p>Keito Kudo 
Tohoku University
γ MBZUAI</p>
<p>Keisuke Sakaguchi 
Tohoku University
γ MBZUAI</p>
<p>Kentaro Inui kentaro.inui@tohoku.ac.jpkeito.kudo.q4@dc.tohoku.ac.jp 
Tohoku University
γ MBZUAI</p>
<p>Riken 
Tohoku University
γ MBZUAI</p>
<p>ACORN: Aspect-wise Commonsense Reasoning Explanation Evaluation
2 Sep 202429A36060AB60183498D889496F92F274arXiv:2405.04818v2[cs.CL]
Evaluating the quality of free-text explanations is a multifaceted, subjective, and labor-intensive task.Large language models (LLMs) present an appealing alternative due to their potential for consistency, scalability, and cost-efficiency.In this work, we present ACORN, a new dataset of 3,500 free-text explanations and aspect-wise quality ratings, and use it to evaluate how LLMs rate explanations.We observed that larger models outputted labels that maintained or increased the inter-annotator agreement, suggesting that they are within the expected variance between human raters.However, their correlation with majority-voted human ratings varied across different quality aspects, indicating that they are not a complete replacement.In turn, using LLMs as a supplement to a smaller group of human raters in some cases improved the correlation with the original majority labels.However, the effect was limited to cases where human raters were scarce, and an additional human rater had a more pronounced effect in all cases.Overall, we recommend against using LLMs as a complete replacement for human raters but encourage using them in configurations that end with targeted human involvement.a-brassard/ACORN</p>
<p>Introduction</p>
<p>Natural language processing systems that not only generate correct output, but also provide an explanation (Miller, 2019) of why that particular output is correct, are desirable for several reasons, such as increasing trustworthiness (Floridi, 2019), compliance with "right to explanation" laws (e.g., European Parliament &amp; Council of the European Union), increasing interpretability (Jacovi &amp; Goldberg, 2020, but cf. Lipton, 2018 on the caveats of post-hoc explanations), as well as system improvement and knowledge discovery (Adadi &amp; Berrada, 2018).However, this immediately raises the question of how to evaluate the plausibility of system-generated explanations in an efficient and effective manner.</p>
<p>Since explanations are typically free-form text, automatic evaluation of explanations suffers the well-known, but as of yet unresolved, weaknesses of automatic evaluation measures (Celikyilmaz et al., 2021), while human evaluation is characterized by low scalability, high costs, subjectivity, and inconsistency (Hartmann &amp; Sonntag, 2022).LLM-based evaluation presents an appealing alternative due to its potential high scalability, low cost, and consistency.</p>
<p>Here, we investigate whether LLMs (Brown et al., 2020;OpenAI, 2023, i.a.) can serve as a viable alternative approach to automatically evaluate system-generated explanations.To verify the feasibility of this approach, we created ACORN, a new dataset of 3,500 textual explanations with aspect-wise human ratings of their quality, a first of its kind, and used it to evaluate whether LLMs are aligned with human judgments (Figure 1).</p>
<p>Figure 1: We collected aspect-wise human ratings for 3,500 textual explanations for commonsense reasoning benchmarks.We compared these against ratings from large language models (LLMs) to evaluate their alignment with human judgments.Specifically, we first considered whether LLMs' labels deviate from what would be expected from human raters.We compared inter-annotator agreement between all-human annotation (five raters) and when one of the raters is replaced by an LLM.We found that stronger models maintained or marginally increased inter-annotator agreement in most cases, suggesting that it may have potential as a replacement for human judgments.</p>
<p>To verify this, we considered two scenarios: one where the LLM replaces the full human evaluation, and one where it is used only as an additional rater.In other words, we compared LLMs to humans collectively and individually.</p>
<p>With the best-performing model, Spearman's rank correlation between majority-voted gold labels and LLM-generated ones ranged between 0.54 and 0.93, depending on the aspect, averaging 0.72.This indicates that the LLM's labels are not entirely in line with human judgments, but they are not entirely random either, and their usefulness may depend on the specific evaluation criteria and usage case.</p>
<p>As for using the LLM as a single rater, we considered whether, in a limited-budget scenario, it would be beneficial to use an LLM as an additional rater.Specifically, we compared whether the majority-voted labels of a reduced set of raters and with an added LLM had a higher correlation with the original majority-voted labels.In cases with three or more raters, the addition of LLMs either had no effect or was detrimental.In cases with one or two raters, the addition of LLMs marginally improved the correlation with the original majority-voted labels, however, still less so than the addition of a human rater.</p>
<p>In summary, we quantified the consequences of using LLMs as a replacement or addition to human evaluation.We conclude that they are an imperfect approximation of human majority votes but still within the expected variance that comes with a subjective task, with some potential benefit when human raters are scarce.Thus, we recommend against using LLMs as a complete replacement for human raters, but encourage using them in configurations that end with targeted human involvement, such as extensively using LLMs in development or filtering stages and human experts for final testing or evaluation.</p>
<p>Building ACORN: Evaluation Criteria and Data Sources</p>
<p>In a typical commonsense reasoning test, a model selects the most plausible answer for a multiple-choice question.In a predict-and-explain setting, the model additionally generates an explanation to justify the selected answer, where we encounter the challenge of evaluating these explanations.Thus, we first define a set of rating criteria ( §2.1) and collect human ratings for a selection of existing and newly collected textual explanations ( §2.2, §2.3).See Appendix A for more details on the dataset, including label distributions, data source-wise average ratings, and examples.</p>
<p>Table 1: Explanation rating criteria used in this study.The first two criteria target consistency and general explanation quality, while the rest covers specific quality aspects.</p>
<p>Rating Criteria</p>
<p>We defined a set of criteria to target surface-level, information/content-level, and structural aspects of explanations.We also included criteria to capture (un)faithfulness and an overall rating, the latter intended to implicitly capture any other aspects considered by the raters.We defined these criteria based on common practices in natural language generation evaluation (Howcroft et al., 2020), known challenges of free-text explanations (Lipton, 2018;Rawte et al., 2023), and insights from social sciences (Miller, 2019).Table 1 summarizes the criteria.The criteria largely aligns with the fine-grained analysis conducted by Wiegreffe et al. (2022).</p>
<p>Supports assesses which answer the explanation supports, intended to be cross-referenced with the predicted label.A mismatch between the predicted label and the supported answer indicates a lack of faithfulness, i.e., the explanation does not reflect the model's reasoning for the label.</p>
<p>Overall is a holistic assessment of the explanation, capturing any potentially informative or useful aspects that we have not explicitly covered.We encouraged workers to consider this criterion independently of the other criteria, and provided general guidelines for each star rating to ensure a consistent understanding.</p>
<p>Well-Written is a catch-all criterion to assess the surface-level quality of the explanation, combining criteria such as fluency, coherence, and grammaticality.</p>
<p>Related assesses the relevance of the explanation to the question and answers.</p>
<p>Factual evaluates the truthfulness of the statements in the explanation, if any, regardless of their relevance.</p>
<p>New Information assesses the extent to which the explanation provides new information beyond the question and answers.Workers were given the choice of none for a complete lack of new information, some for a partial addition, sufficient for a satisfactory amount of new information, and ample for highly informative explanations.</p>
<p>Unnecessary Information assesses the extent to which the explanation includes irrelevant information.We included this criterion to capture the challenge of generating minimal explanations.</p>
<p>Contrastive assesses whether the explanation contrasts the correct answer with the predicted answer.</p>
<p>Source Datasets</p>
<p>ACORN contains ratings for a diverse set of existing, newly-collected, and generated explanations.Our choice covers two commonsense reasoning benchmarks and their respective explanation datasets (Figure 2).From each, we selected a random subset of 500 explanations for rating, as well as an additional 500 samples of fluency-improved versions, resulting in a total of 3,500 explanations.The fluency-improved subset is included to prevent fluency from becoming a superficial signal, since well-written explanations typically also have high scores in all other aspects.With five raters and eight criteria per sample, this amounts to 140k ratings in total.Specifically, as the target commonsense reasoning benchmarks, we selected BCOPA (Kavumba et al., 2019) 1 and CommonsenseQA (Talmor et al., 2019) based on the simplicity of their tasks and availability of large-scale explanation datasets (Wiegreffe &amp; Marasović, 2021).Below are the respective datasets we used to source candidate explanations.</p>
<p>CoS-E (Rajani et al., 2019) A widely-used explanation dataset for CommonsenseQA, albeit notoriously uninformative to humans (Nauta et al., 2023).A subset is processed through GPT-3.5 2 for fluency improvement (500 samples + 250 fluency-improved versions)</p>
<p>ECQA (Aggarwal et al., 2021) An improved version of explanations for CommonsenseQA, aligning with our criteria for well-formed explanations.(500 samples)</p>
<p>Generated explanations for CommonsenseQA.Additional high-quality explanations generated by prompting GPT-3.5 to solve a subset of CommonsenseQA, though potential issues like irrelevant information were noted.(500 samples)</p>
<p>COPA-SSE (Brassard et al., 2022) Explanations for BCOPA with a subset processed through GPT-3.5 for fluency improvement.Since COPA-SSE already contains overall quality ratings, we selected a random sample of 250 questions and used each question's top-rated and bottom-rated explanation.(500 samples + 250 fluency-improved versions)</p>
<p>Crowdsourced explanations for BCOPA.ECQA's counterpart for BCOPA; a new set of hand-written explanations, carefully crafted for contrastiveness and thoroughness.(500 samples)</p>
<p>Generated explanations for BCOPA.Similarly to CoS-E, we prompted GPT-3.5 to solve BCOPA questions.(500 samples)</p>
<p>Rating Collection</p>
<p>We crowdsourced ratings for the explanations in ACORN using Amazon Mechanical Turk (AMT).Each rater was required to pass a qualification test, after which they were asked to participate in trial rounds, during which we addressed several clarity issues in the guidelines.</p>
<p>The final pool was hand-picked based on their responses, resulting in 28 participants.See Appendix B for more details on the rating collection process.In experiments which use aggregated labels, they were produced with a majority-vote mechanism, with ties broken using the better label.</p>
<p>Figure 3: Inter-annotator agreement (Krippendorff's α, * 100 for legibility) between human raters (shaded area) and with the LLM's rating replacing a random rater.</p>
<p>man evaluation ( §3.3), and when using it as an additional rater instead ( §3.4).All experiments follow the settings described in Section 3.1.</p>
<p>Experimental Settings</p>
<p>Models.We compared four contemporary API-enabled LLMs, namely GPT-4o (Ope-nAI, 2024), Llama-3.1 (405B) (Team, 2024), Gemma-2 (27B) (Gemma Team, 2024), and Mixtral (8x22B) (Jiang et al., 2024).Each have reported high performance in diverse tasks including text-based reasoning and represent the sate-of-the-art in generalist LLMs at the time of writing.Specifically, we used the following model versions: gpt-4o-2024-05-13, Meta-Llama-3.1-405B-Instruct-Turbo,gemma-2-27b-it, and Mixtral-8x22B-Instruct-v0.1.3Temperature is set to 0.0 with all other parameters left at their default values.</p>
<p>Prompting Strategy.LLMs are notoriously oversensitive to prompt format (Wadhwa et al., 2023).For the purpose of our analyses, we explored several prompting strategies (listed in Appendix C) and selected the most successful one as measured by correlation with majorityvoted human ratings.Most models worked best with verbatim prompts corresponding to a word-by-word copy of the guidelines given to humans, to which they responded with a structured list of criteria and their assigned labels for the given target explanation.</p>
<p>Label Extraction.Using free-text generation models for a classification task introduces the problem of extracting said ratings, and presents an information extraction challenge in itself.This phenomenon, inherent to generative approaches (Wadhwa et al., 2023), is a source of additional noise that affects all evaluation pipelines necessitating a non-trivial solution in real applications.In our experiments, we used a rule-based extraction method backed up with LLM-based extraction in case of failure.Finally, we manually inspected the remaining failures,4 and excluded them from our experiments to maintain a fair comparison.The final extraction failure rates were &lt;0.2% for all models but Gemma-2 (2.7%).</p>
<p>Inter-annotator Agreement</p>
<p>In subjective tasks some degree of label variance is expected, resulting in lower interannotator agreement.This disagreement is not necessarily noise but a feature of the data, as it can reflect the diversity of human opinions (Aroyo &amp; Welty, 2015).In this context, regardless of absolute agreement, a successful LLM-based rater should be harmonious with the range of human labels rather than deviate from it.</p>
<p>To measure this, we compared the inter-annotator agreement (Krippendorff's α) between human raters and when a random rater is replaced by an LLM.There are three possible outcomes: (i) agreement decreases, indicating that the LLM deviates from human judgments; (ii) agreement remains the same, indicating that it is harmonious with human judgments; or (iii) agreement increases, meaning that the LLM is both harmonious and biased towards a majority.5</p>
<p>Results.In Figure 3, the shaded area shows the agreement between human raters, while the bars show the agreement when the respective LLM's ratings replace a random human rater ( * 100 for legibility).All values are averaged over twenty iterations.Mixtral, GPT-4o, and Llama-3.1 maintained or improved agreement in most cases, with slight decreases in supports with Mixtral, related with Llama-3.1, and with unnecessary information with GPT-4o and Llama-3.1.Gemma-2, on the other hand, decreased agreement in all but three criteria.However, the latter is also significantly smaller than the others, and illustrates the tradeoff between model size and performance.Interested readers may refer to Table 7 in the appendix for all results, including several older or smaller models for comparison.</p>
<p>LLMs As A Replacement for Human Evaluation</p>
<p>The larger models maintained or improved inter-annotator in most criteria, suggesting that they do not deviate from an expected range of human ratings.Here, we ask whether they can then replace human evaluation.Specifically, we measured the degree to which the models' predictions align with majority-voted human labels.</p>
<p>Results.</p>
<p>Figure 4 shows Spearman's rank correlation ( * 100 for legibility) between aggregated human labels and LLM predictions.The highest values are annotated for each criterion.A comprehensive table with all results, including several smaller or older models, is available in the appendix (Table 8).</p>
<p>The correlation in supports and contrastive was particularly strong: 0.84 and 0.93, respectively.The unnecessary information criterion, however, stands out with a much lower correlation in all models (0.54 and less).Others ranged from 0.62 to 0.76, indicating a moderately high correlation.Overall, GPT-4o was the best-performing model in five out of seven criteria, with an average correlation of 0.72.Mixtral, the second-best model, followed closely, particularly outperforming GPT-4o in unnecessary information and contrastive.</p>
<p>From these results, we conclude that the larger models are relatively well-aligned with humans and could be considered effective depending on the use case.However, they are still clearly not a perfect replacement for human raters.Instead, considering the small change in inter-annotator agreement ( §3.2), the model could potentially be used as an additional rater when human annotation is scarce, which we explore in the next section.</p>
<p>LLMs As An Additional Rater</p>
<p>The results so far hinted towards LLMs acting similarly to an average human rater.Thus, it may seem appealing as an additional data point when human raters are scarce or expensive.</p>
<p>Here, we verified this potential by measuring whether using each model as an additional Figure 5: A comparison of Spearman's rank correlation with the original gold labels when using fewer raters (<em>H) and when an LLM is added as an additional rater (</em>H+LLM).From left to right, the number of human raters decreases from four to one (randomly selected).All values are multiplied by 100 for legibility.</p>
<p>rater improved the outcome over having fewer human raters, i.e., whether the majorityvoted labels became more aligned with the original ones when the LLM was added as a rater.</p>
<p>Specifically, we compared Spearman's rank correlation between the majority-voted labels with all available raters and in two alternative scenarios: one where the model is added as an additional rater and one where it is not.If the correlation increases when the model is added, it suggests that its predictions are in line with the original majority-voted labels, and it is useful as an additional rater.Otherwise, it would indicate a harmful or negligible effect, and thus its inclusion should be avoided.We repeated this comparison for scenarios with four, three, two, and one randomly selected human rater per sample.</p>
<p>Results.</p>
<p>Figure 5 shows Spearman's rank correlation with the original gold ratings obtained by aggregating the labels of all five raters.Humans only (<em>H) denotes the correlation between human majority-voted labels only, and others when including the respective LLM as an additional rater (</em>H+LLM).</p>
<p>Each column cluster represents a different number of human raters from four in the leftmost to one in the rightmost.With four humans, the correlation between their majority-voted labels and the original gold labels was 0.91.Adding a model as a fifth vote raised this by only 0.004 points.With three humans, we observed a slight decrease in correlation when adding a model.With two or one human rater, the correlation increased by 0.019 and 0.016 points, respectively.</p>
<p>Overall, the results suggest that LLMs can be useful as additional raters when the number of human raters is less than three.However, even in the best case, the voted labels with an added LLM rater still scored lower than with an equivalent number of human raters (0.83 with 2H+LLM vs. 0.89 with 3H).When there is a high number of human raters, in this case three or more, the model's inclusion as an additional rater does not improve the majority-voted labels' alignment with the original gold labels, and may even harm it.</p>
<p>Discussion</p>
<p>We analyzed the alignment between human raters and LLMs in the context of evaluating the quality of explanations.The task is highly subjective, and some degree of variance is expected even between humans.</p>
<p>The best-performing models seemed to output labels that are within this variance, suggesting that they behave similarly to individual human raters.However, comparing its outputs to majority-voted labels revealed that their labels are not always in line with human majorities, indicating that they are not suitable as a complete replacement for human raters.In turn, when there were fewer human annotators, LLMs helped bring the majority-voted labels closer to the original gold labels.However, this improvement was not seen when there were already three or more human annotators (compared to the original five), suggesting that LLMs are only useful in extremely limited scenarios.</p>
<p>From this, we concluded that LLMs are not a reliable replacement for human raters, unless the use case does not require complete alignment.However, in this work, we operated under the assumption that majority-voted labels are the ground truth.In practice, this may not always be the case, and the reliability of LLMs may vary depending on what information is desired in a given scenario.This opens up a new avenue for future work, where LLMs can be applied in a more nuanced manner, considering the context of the task and the intended use of the labels.</p>
<p>Overall, as a balanced approach, we recommend using LLMs in configurations that end with targeted human involvement, such as extensively using LLMs in development or filtering stages and experts for final testing or evaluation.In future work, we plan to explore the behavior of LLMs more closely, particularly in identifying potential patterns in where the model diverged from humans, such as explanations with particular characteristics or in specific contexts.These insights could help us better understand the limitations of LLMs and how they can be used effectively in real-world applications or further improved.</p>
<p>Related Work</p>
<p>LLM-based Evaluation LLMs as data labelers, and more broadly humans-and-LLMs-inthe-loop settings, are an emerging direction in data collection and labeling.E.g., Wiegreffe et al. (2022) developed a predict-and-explain pipeline that combines GPT-3 with a supervised filter trained on binary acceptability judgments from humans.More recently, Chiang &amp; Lee (2023) proposed using LLMs to evaluate text, closely in line with our work.They, however, found the models successful in their setting.In contrast, we focus on explanation rating with more complex fine-grained criteria, and closely scrutinize potential weaknesses; Previous works brought into question the reliability of LLMs' predictions, especially in prompting setups.E.g., Webson &amp; Pavlick (2022) found that instruction-tuned models often produced good predictions with irrelevant or misleading prompts, bringing into question their real "understanding" of the task.Others reported unreliability of LLMs as labelers in various settings (Albrecht et al., 2022;Shen et al., 2023;Hada et al., 2024, i.a.), a line of work which we join with findings in the context of explanation evaluation.</p>
<p>Explanation Evaluation For explanations in the form of textual justifications, previous works often defined their own criteria for evaluation.Automatic evaluation borrowed from machine learning and measures overlap with "gold standard" text using (a) wordoverlap metrics, e.g., BLEU, METEOR and ROUGE; and (b) embedding-based metrics, e.g., BERTScore and BLEURT (Clinciu et al., 2021).In contrast, human-tagged measures are more diverse and explanation-specific.For example, Clinciu et al. (2021)  A recent study instead focused on the utility of explanations, i.e., their helpfulness in answering a question from a human point of view (Joshi et al., 2023).In our work, we largely followed these existing criteria, with a modified version of the "supports" criterion to capture the consistency of a model's predicted label with its justification.</p>
<p>Explain-and-Predict Explanations are often generated in predict-and-explain settings, where models provide justifications for their answers in QA-based benchmarks (e.g., Clinciu et al., 2021), or an elaborate-then-predict setting, where models output a prediction guided by its intermediate outputs such as knowledge statements or reasoning chains (e.g., Marasović et al., 2022;Wang et al., 2023).In this work, we evaluate the capabilities of LLMs to evaluate explanations in an explain-and-predict setting, where models provide justifications for their answers in commonsense reasoning benchmarks.The following sections provide background on each aspect.</p>
<p>Conclusions</p>
<p>Using a newly-built dataset of free-text explanations and crowdsourced aspect-wise quality ratings, we analyzed the viability of LLMs as explanation evaluators.Stronger LLMs increased or maintained inter-annotator agreement when replacing human ratings, and their ratings were moderately to highly correlated with human ratings, depending on the quality aspect.In a scenario where LLMs were used as additional raters instead of a complete replacement, they improved the outcome when there were only two human raters, but were neutral to detrimental when there are three or more human raters.We conclude that while LLMs can provide ratings moderately consistent with an average human rater, they are not yet reliable enough for complete replacement.</p>
<p>Limitations</p>
<p>Explanation evaluation is inherently subjective, and the majority-voted gold label is not necessarily the "correct" answer.Subjectivity-informed scoring is a complex task, and we leave its exploration to future work.Furthermore, criteria may change depending on the intended use.Here, we limited our analysis to a researcher's point of view, where explanations are increasingly used as a diagnostic tool, and aligned our criteria to a list of general explanatory competencies.While our insights on LLMs' performance may potentially be applicable to different evaluation tasks, it should not be assumed to be universally true.</p>
<p>LLMs are known to be sensitive to prompt format.We somewhat compensated for this by comparing a prompt-averaged setting, and our focus is not to search for the optimal setting but rather investigate potential fundamental issues with LLM-based evaluation.However, for practical applications, we acknowledge that there is a wealth of tweaks that may improve the performance.Nevertheless, our findings highlight the need for caution when using LLMs for explanation evaluation, and we hope that our work will inspire further research in this direction.</p>
<p>Ethics Statement</p>
<p>Some of the commercial LLMs we used have built-in filters for potentially harmful content, which were triggered during several of our experiments.This data should not be used in any downstream applications without first controlling for potentially harmful samples.</p>
<p>Crowdworkers play a vital role in dataset creation.We prioritized fair compensation, transparency in data usage, and respect for their rights and privacy.Workers were informed on their work's intended usage and of our identity as requesters.We did not collect any personal information.Table 3: Label distributions per criterion of majority-voted human ratings.-1 denotes "none" for supports and "N/A" for factual.</p>
<p>A.1 Dataset-Wise Average Ratings</p>
<p>Table 4 shows the mean ratings and standard deviations of majority-voted ratings per data subset, excluding the categorical aspect supports.Data labels are described in Section 2.1.All ratings are the higher the better, except for unnecessary information which is the lower the better.Table 4: Mean ratings and standard deviations per data subset.Higher is better for all criteria except for unnecessary information, marked with an asterisk (*), where lower is better.</p>
<p>The human raters seemed to find generated explanations to be most well-written on average, however, the higher quality human-written explanations (ECQA, BCOPA crowdsourced) had a higher amount of new information.The generated explanations, in turn, had the least amount of unnecessary information.Interestingly, even though they were not contrastive, the generated explanations for CommonsenseQA (CSQA generated) had a higher average overall rating than ECQA explanations which are explicitly contrastive.</p>
<p>A Random Forest Regressor, achieving a mean squared error of 0.37, deemed the most important predictive feature to be new information (58%), followed by factual (20%), unnecessary information (9%), well-written (7%), contrastive (4%), supports (2%), and related (0%).Note that for supports, we defined a binary feature of whether the label matches the answer label.</p>
<p>B Explanation Rating Annotation</p>
<p>Our crowdsourcing protocol for label collection consisted of three phases: qualification rounds, trial rounds, and main collection rounds.We provided detailed guidelines showing general instructions, detailed information on each criterion and their respective labels, three examples, and a FAQ section based on questions we received from workers.The full document is available upon request to the first author.</p>
<p>Qualifications</p>
<p>In the qualification rounds, we curated a question set of 6 explanations and manually tagged them with "acceptable" answers, focusing on overall alignment rather than exact matches.We included a dummy question with strict instructions for filtering.Out of 700 participants, the top 201 workers, with a match percentage of 59% or higher, proceeded to trial rounds.We addressed any concerns or clarifications through email or form feedback.We hand-picked a final group of 28 workers.Qualifications were open to workers with a HIT approval rate of 99% or more and 5,000 or more approved HITs.Note that the location requirement was removed as it was an unnecessary barrier for highly skilled and motivated workers.</p>
<p>Main Rounds Each of the 3,500 explanations in the test set ( §2.2) was rated by five workers.The ratings were aggregated to create the final gold labels used in our experiments.Figure 6 shows the crowdsourcing form.</p>
<p>Payment Information For qualifications, each worker was compensated $0.15 per HIT.</p>
<p>For the main rounds, the fee was increased to $0.25 per HIT, roughly matching a payment of $20.00 per hour.</p>
<p>C Preliminary Experiment: Prompting Strategies</p>
<p>We compared single and compound calling, where the former prompts the model for a single criterion at a time, and the latter prompts the model for all criteria at once.We also compared default, averaged, and verbatim prompt formats, corresponding to a simple prompt with the explanation and the rating criteria, a voting mechanism over several prompt variants, and an input identical to the human annotation guidelines, respectively.For single verbatim calls, only the relevant sections (guidelines and examples) for the target criterion were included.Default and averaged prompts were further compared in zero-shot and three-shot settings, where the latter contained the same examples as shown in the human guidelines.All models were most successful with verbatim compound prompting.</p>
<p>D Crowdsourced Explanations for BCOPA</p>
<p>As a supplement to COPA-SSE, We collected 3,000 hand-written, detailed, and contrastive explanations for BCOPA.All crowdsourcing was conducted on the Amazon Mechanical Turk platform. 6Generally, we found that strict qualifications, hand-picking the final worker pool, and maintaining open communication led to a significant increase in data quality.</p>
<p>Qualifications</p>
<p>In the qualification rounds, workers were asked to write three explanations for an explanation, its mirrored sample,7 and another random explanation.397 workers were hand-picked and whitelisted for further rounds based on the quality of their responses.We limited participation in this qualification to workers with a HIT approval rate of 99% or more, 1,000 or more approved HITs, and located in Great Britain or the United States.</p>
<p>Main Rounds</p>
<p>The workers were explicitly instructed to write contrastive explanations which "focus on what the key difference between the options is, and how it leads to it being the correct choice in one case and not in the other."This instruction was inspired by insights from Miller (2019).Figure 7 shows the crowdsourcing form.We collected two explanations per BCOPA question, totaling 3,000 explanations.Out of these, a random sample of 500 explanations was rated and included in our test set.Example explanations can be seen in Table 6.</p>
<p>Payment Information For qualifications, each participating worker was compensated $0.10 per HIT.For the main rounds, the fee was increased to $0.89 per HIT, roughly matching a payment of $20.00 per hour.</p>
<p>Figure 2 :
2
Figure 2: We collected general and aspect-wise ratings for human-written, LLM-improved, better human-written, and LLM-generated explanations, for BCOPA and CommonsenseQA, respectively.</p>
<p>Figure 4 :
4
Figure 4: Spearman's ranking correlation between majority-voted human labels and LLMgenerated ratings ( * 100 for legibility).</p>
<p>Figure 6 :
6
Figure 6: Explanation rating AMT form.</p>
<p>Figure 7 :
7
Figure 7: Explanation writing AMT form.</p>
<p>measured Informativeness and Clarity; Wiegreffe et al. (2022), inspired by social sciences, measured Acceptability, Generality, Factuality, Grammar, New Info, Supports Label, and Amount of Information; while Aggarwal et al. (2021) defined the criteria of Refutation, Complete, Comprehensive, Minimal, and Coherent.</p>
<p>Table 2 :
2
Breakdown of explanation data used in our experiments by source dataset.
Criterion-1012345Supports11% 32% 31% 7% 9% 8%Overall16% 13% 28% 28% 12%Well-written27% 72%Related6% 93%Factual10% 4% 84%New Info30% 31% 35% 2%Unnecessary Info82% 17%Contrastive58% 41%
Balanced COPA; a superset of COPA(Gordon et al.,<br />
) with added "mirrored" questions that flip the correct label, i.e., the originally incorrect choice becomes correct.2 text-davinci-003; The model was instructed to only improve the fluency and was not given any additional context that may encourage improving the content, e.g., by supplementing related information.
We also provide the results of several smaller or older alternatives in the appendix for comparison (Tables7 and 8).
Mostly due to non-compliance to the task format, e.g., responding verbosely with new labels instead of following the given choice: "... Related: Somewhat. ..." (instead of Yes or No)
The latter may be desirable in use cases that rely on majority-voted labels as the ground truth, but comes with the trade-off of losing potentially useful label diversity.
https://requester.mturk.com/
With the same answer choices but a different question that makes the alternative correct.
Can LLMs Replace Human Raters?We answer this question in three steps: we first measure LLMs' divergence from expected human label variance ( §3.2), then observe the results when using it to completely replace hu-AcknowledgementsThis work was supported by JST CREST Grant No. JPMJCR20D2 and JSPS KAKENHI Grant No. 15H01702 and 21K17814.We extend sincere thanks to all the crowdworkers who helped realize this project with their diligent work in labeling the dataset.A big thanks to Tatsuki Kuribayashi and Jonas F. Lotz for their detailed feedback on the paper.This work was written with the help of LLM-powered writing and coding assistants, however, all ideas and opinions presented in this work are solely those of the authors.Coherent, grammatically correct, fluent?Yes, No Related Relevant to the Q and A? Yes, No Factual Stated facts are generally true?Yes, No, N/A New Information How much new information to support the ans.? None, Some, Sufficient, Ample Unnecessary Info.Any unnecessary statements?Yes, No Contrastive Clearly shows the difference between the ans.? Yes, NoA Dataset StatisticsTable2shows a breakdown of the samples in the test set per source dataset as described in Section 2.2.Table3shows the label distributions of each criterion in our test set.Tables5  and 6show examples of best-rated and worst-rated explanations from each source dataset.CoS-E (best)Which effect of stress could cause death if not treated immediately?a) age grieving person b) heart attacks c) depression d) hair loss e) headache Explanation: heart attacks can stop your heart from properly pumping blood, thus leading to death.Over.:4Crowdsourced (best)The man was bitten by mosquitoes.Cause?a) He fell asleep on his couch.b) He went camping in the woods.Explanation: Sleeping on a couch would mean you are indoors where there are rarely mosquitoes.Mosquitoes are prevalent in wooded areas, so the man would be more likely to be camping in the woods, if he was bit by mosquitoes.
Peeking inside the black-box: A survey on explainable artificial intelligence (xai). Amina Adadi, Mohammed Berrada, 10.1109/ACCESS.2018.2870052IEEE Access. 62018</p>
<p>Explanations for CommonsenseQA: New Dataset and Models. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 20211</p>
<p>Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety. Joshua Albrecht, Ellie Kitanidis, Abraham J Fetterman, December 2022</p>
<p>Truth is a lie: Crowd truth and the seven myths of human annotation. Lora Aroyo, Chris Welty, 10.1609/aimag.v36i1.2564AI Mag. 361March 2015</p>
<p>COPA-SSE: Semistructured Explanations for Commonsense Reasoning. Ana Brassard, Benjamin Heinzerling, Pride Kavumba, Kentaro Inui, Proceedings of the Thirteenth Language Resources and Evaluation Conference. the Thirteenth Language Resources and Evaluation ConferenceMarseille, FranceJune 2022European Language Resources Association</p>
<p>Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Sandhini Askell, Ariel Agarwal, Gretchen Herbert-Voss, Tom Krueger, Rewon Henighan, Aditya Child, Ramesh, M Daniel, Jeffrey Ziegler, Clemens Wu, Christopher Winter, Mark Hesse, Eric Chen, Mateusz Sigler, Litwin, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordMay 2020</p>
<p>Can large language models be an alternative to human evaluations?. Asli Celikyilmaz, Elizabeth Clark, Jianfeng Gao, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsStroudsburg, PA, USAAssociation for Computational Linguistics2021. July 20231Evaluation of text generation: A survey</p>
<p>A study of automatic metrics for the evaluation of natural language explanations. Miruna-Adriana Clinciu, Arash Eshghi, Helen Hastie, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterStroudsburg, PA, USAAssociation for Computational Linguistics2021</p>
<p>Regulation (EU) 2016/679 of the European Parliament and of the Council. European Parliament and Council of the European Union</p>
<p>Establishing the rules for building trustworthy ai. Luciano Floridi, Nature Machine Intelligence. 162019</p>
<p>Improving open language models at a practical size. arXiv [cs.CL]Gemma Team. Gemma. 2July 2024</p>
<p>SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. Andrew Gordon, Zornitsa Kozareva, Melissa Roemmele, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics. Montréal, CanadaAssociation for Computational LinguisticsSemEval 2012. 20121Proceedings of the Sixth International Workshop on Semantic Evaluation</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>A survey on improving NLP models with human explanations. Mareike Hartmann, Daniel Sonntag, 10.18653/v1/2022.lnls-1.5Proceedings of the First Workshop on Learning with Natural Language Supervision. the First Workshop on Learning with Natural Language SupervisionDublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Twenty Years of Confusion in Human Evaluation: NLG Needs Evaluation Sheets and Standardised Definitions. Anya David M Howcroft, Miruna-Adriana Belz, Dimitra Clinciu, Gkatzia, A Sadid, Saad Hasan, Simon Mahamood, Mille, Sashank Emiel Van Miltenburg, Verena Santhanam, Rieser, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsDecember 2020</p>
<p>Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness?. Alon Jacovi, Yoav Goldberg, 10.18653/v1/2020.acl-main.386Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Gianna Bressand, Guillaume Lengyel, Guillaume Bour, Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Szymon Yang, Antoniak, arXiv [cs.LG]Mixtral of Experts. Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, January 2024</p>
<p>Are machine rationales (not) useful to humans? Measuring and improving human utility of free-text rationales. Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, Xiang Ren, 10.48550/ARXIV.2305.07095Annual Meeting of the Association for Computational Linguistics. 2023</p>
<p>When Choosing Plausible Alternatives, Clever Hans can be Clever. Pride Kavumba, Naoya Inoue, Benjamin Heinzerling, Keshav Singh, Paul Reisert, Kentaro Inui, Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing. the First Workshop on Commonsense Inference in Natural Language ProcessingHong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Zachary C Lipton, 10.1145/3236386.3241340Queueing Syst. 163June 2018</p>
<p>Few-Shot Self-Rationalization with Natural Language Prompts. Ana Marasović, Iz Beltagy, Doug Downey, Matthew Peters, Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Explanation in artificial intelligence: Insights from the social sciences. Tim Miller, 10.1016/j.artint.2018.07Artificial Intelligence. 0004-37022672019</p>
<p>URL. </p>
<p>. Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice Van Keulen, Christin Seifert, 10.1145/3583558From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI. ACM Comput. Surv. 5513sJuly 2023</p>
<p>Explain yourself! Leveraging language models for commonsense reasoning. Nazneen Fatema Rajani, Bryan Mccann, Caiming Xiong, Richard Socher, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsStroudsburg, PA, USAAssociation for Computational Linguistics2019</p>
<p>A survey of hallucination in Large Foundation Models. Amit Vipula Rawte, Amitava Sheth, Das, September 2023</p>
<p>Large Language Models are Not Yet Human-Level Evaluators for Abstractive Summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>arXiv [cs.AI]The Llama 3 herd of models. July 2024</p>
<p>Revisiting relation extraction in the era of large language models. Somin Wadhwa, Silvio Amir, Byron Wallace, 10.18653/v1/2023.acl-long.868July 2023</p>
<p>Elaboration-Generating Commonsense Question Answering at Scale. Wenya Wang, Vivek Srikumar, Hannaneh Hajishirzi, Noah A Smith, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Do Prompt-Based Models Really Understand the Meaning of Their Prompts?. Albert Webson, Ellie Pavlick, Proceedings of the 2022 Conference of the North American Chapter. Marine Carpuat, Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Teach me to explain: A review of datasets for explainable natural language processing. Sarah Wiegreffe, Ana Marasović, 20211</p>
<p>Reframing Human-AI Collaboration for Generating Free-Text Explanations. Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, Yejin Choi, Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>            </div>
        </div>

    </div>
</body>
</html>
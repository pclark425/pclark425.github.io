<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6983 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6983</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6983</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-248177721</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2204.06674v4.pdf" target="_blank">GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance. These tasks require extensive computational resources while only suggesting marginal improvements. Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-the-art models and close the gap imposed by additional pre-training tasks. We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type. Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pre-training tasks. By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in a graph.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6983.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6983.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TripleLinear</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDF/triple linearization (sequence of triples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token sequence representation that linearizes a knowledge graph as an ordered list of RDF-style triples (head, relation, tail), interleaved with special separator tokens that mark triple boundaries and component roles; used as input text to fine-tune pre-trained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>RDF triple sequence (linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph is converted into a single text string by listing all triples in the graph. Triples are serialized as head, relation, tail sub-sequences and separated with dedicated tokens that indicate triple boundaries and component roles (i.e., special separators between triples and between head/relation/tail components). Repeated multi-word entity/relation mentions remain as token spans in the sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based (ordered triple list); largely lossless with respect to storing triples (but order is not canonical unless enforced)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>edge-list ordering: list all triples sequentially (interleaved with separator tokens marking head/relation/tail and triple boundaries); no special graph traversal such as DFS/BFS is reported beyond enumerating the triples</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph-to-text generation (LM fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (bart-base) fine-tuned on linearized triple sequences</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained transformer encoder-decoder (facebook/bart-base) initialized from Hugging Face; used as the Global Attention backbone and fine-tuned on the linearized triple sequences, optionally augmented with GAP graph-aware modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ROUGE-L, BERTScore, Entity accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported best WebNLG BLEU ≈ 65.92 for GAP variants using triple linearization; few-shot BLEU (WebNLG) for M_e,r + γ(T): 0.5%→39.50, 1%→44.03, 5%→55.68, 10%→58.30 (compared to BART 33.92/39.08/52.24/56.58). EventNarrative: GAP shows improvements relative to BART (BLEU +3.70%, METEOR +0.82%, ROUGE +1.63%).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables direct use of pre-trained LMs (BART) by framing KG-to-text as text-to-text; when combined with the GAP graph-aware modules, it produced higher generation metrics without any additional graph-specific pre-training. Also effective in few-shot settings (higher BLEU vs BART and some pre-trained baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linearization depends on chosen ordering of triples (no canonicalization reported), which can introduce variance; long graphs increase token length and may hit model max input limits (paper fixes max graph sizes: 50 for WebNLG, 60 for EventNarrative). Linearization alone does not explicitly encode fine-grained topological neighborhood beyond the triple set and so benefits from additional graph-aware components.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to approaches that rely on additional large-scale pre-training on graph↔text corpora (e.g., KGPT, JointGT), simple linearization plus GAP graph-aware encoder matched or exceeded performance on WebNLG and EventNarrative in this paper, while requiring no extra pre-training; compared to vanilla fine-tuned BART (no graph modules), linearization plus graph-aware components improved BLEU/METEOR/ROUGE and few-shot robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6983.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6983.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAP-GraphRep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GAP Graph-aware attention representation (pooled graph components + masked/type-biased attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid representation that pools token vectors into a fixed set of graph component vectors (entities and relations) and applies a neighborhood-masked, connection-type-biased attention (matrix M and type-encoding γ(T)) to inject topological and relation-type information into pre-trained language model representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Pooled graph-component matrix + neighborhood-masked / type-biased attention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>From the linearized token sequence, the model pools (average) the token vectors corresponding to each graph component (individual entities and relations) to form a graph representation matrix X_g. A learnable attention over X_g is computed with two graph-aware additives: (1) a masking matrix M (generalized adjacency) that sets -∞ where attention should be blocked (neighborhood-based masking), and (2) a type-encoding γ(T) that adds learned scalar biases to attention scores depending on the connection type (entity-entity, entity-relation, relation-entity, relation-relation). The updated graph representations are then gathered back and added residually to the token-level representations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-structured (matrix) attention; hybrid (token-level → pooled graph nodes/edges → token residual); preserves neighborhood structure explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>1) Pool token embeddings by entity/relation mention spans to build X_g; 2) compute Q,K,V from X_g and apply masked attention: softmax((QK^T)/sqrt(d_k) + M + γ(T)) V; 3) gather updated component vectors back to token positions and add as residual to the LM representations.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG v2.0; EventNarrative</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-graph-to-text generation / LM fine-tuning with graph-aware encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GAP (graph-aware modules) built on BART (bart-base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BART encoder-decoder (bart-base) is used as the Global Attention module; GAP inserts additional Graph-aware Attention layers that operate on pooled graph components (entities and relations) using masking matrix M and type encoder γ(T). Hyperparameters: BPE vocab size 50,265; learning rate 3e-5 (paper experiments); max graph nodes 50/60 for datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, ROUGE-L, BERTScore, Entity accuracy; also per-mask ablations and few-shot BLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG: Best GAP variant (e.g., M_e,r^e with type encoding) reported BLEU ≈ 65.92 (other mask variants: M_e,r 65.86; M_e,r,e,r 65.11). Reported relative improvements vs prior non-pretrained SOTA: +5.20% BLEU; vs KGPT: +1.81%/+2.09% on BLEU; vs JointGT variants: +1.32%/+1.6% on BLEU (as reported). EventNarrative: M_e,r^e improved BLEU by +3.70%, METEOR +0.82%, ROUGE +1.63% relative to plain BART. Few-shot WebNLG BLEU for M_e,r + γ(T): 0.5%→39.50, 1%→44.03, 5%→55.68, 10%→58.30.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Injecting graph-aware masked attention into pre-trained LMs improved downstream NLG metrics without additional graph-specific pre-training, improved few-shot performance, and allowed competitive or superior performance relative to models that used expensive auxiliary pre-training tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Design choices of the mask M and the inclusion/exclusion of relation→relation attention significantly affect performance: letting relations attend to relations often reduced performance, especially on small graphs. The method requires pooling strategies and a fixed maximum number of graph components for parallelization; it increases encoder complexity and requires deciding which neighborhood connections to permit. Larger graphs pose computational constraints (paper uses fixed max node count).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to prior methods that assume full connectivity among nodes (global self-attention) or rely on pre-training on large KG-text corpora (KGPT, JointGT), GAP's graph-aware attention explicitly encodes sparsity/topology via M and encodes connection types via γ(T), producing better interpretability (attention heatmaps) and competitive or superior results without auxiliary pre-training. Ablations showed type encoding helps the most general mask (M_e,r^e,r) simulate more restrictive, better-performing behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation', 'publication_date_yy_mm': '2022-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>KGPT: Knowledge-grounded pretraining for data-to-text generation <em>(Rating: 2)</em></li>
                <li>JointGT: Graph-text joint representation learning for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from rdf data <em>(Rating: 2)</em></li>
                <li>GTR-LSTM: A triple encoder for sentence generation from RDF data <em>(Rating: 2)</em></li>
                <li>Step-by-step: Separating planning from realization in neural data-to-text generation <em>(Rating: 1)</em></li>
                <li>Modeling graph structure via relative position for text generation from knowledge graphs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6983",
    "paper_id": "paper-248177721",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "TripleLinear",
            "name_full": "RDF/triple linearization (sequence of triples)",
            "brief_description": "A token sequence representation that linearizes a knowledge graph as an ordered list of RDF-style triples (head, relation, tail), interleaved with special separator tokens that mark triple boundaries and component roles; used as input text to fine-tune pre-trained language models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "RDF triple sequence (linearization)",
            "representation_description": "Each graph is converted into a single text string by listing all triples in the graph. Triples are serialized as head, relation, tail sub-sequences and separated with dedicated tokens that indicate triple boundaries and component roles (i.e., special separators between triples and between head/relation/tail components). Repeated multi-word entity/relation mentions remain as token spans in the sequence.",
            "representation_type": "sequential, token-based (ordered triple list); largely lossless with respect to storing triples (but order is not canonical unless enforced)",
            "encoding_method": "edge-list ordering: list all triples sequentially (interleaved with separator tokens marking head/relation/tail and triple boundaries); no special graph traversal such as DFS/BFS is reported beyond enumerating the triples",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "Knowledge-graph-to-text generation (LM fine-tuning)",
            "model_name": "BART (bart-base) fine-tuned on linearized triple sequences",
            "model_description": "Pre-trained transformer encoder-decoder (facebook/bart-base) initialized from Hugging Face; used as the Global Attention backbone and fine-tuned on the linearized triple sequences, optionally augmented with GAP graph-aware modules.",
            "performance_metric": "BLEU, METEOR, ROUGE-L, BERTScore, Entity accuracy",
            "performance_value": "Reported best WebNLG BLEU ≈ 65.92 for GAP variants using triple linearization; few-shot BLEU (WebNLG) for M_e,r + γ(T): 0.5%→39.50, 1%→44.03, 5%→55.68, 10%→58.30 (compared to BART 33.92/39.08/52.24/56.58). EventNarrative: GAP shows improvements relative to BART (BLEU +3.70%, METEOR +0.82%, ROUGE +1.63%).",
            "impact_on_training": "Enables direct use of pre-trained LMs (BART) by framing KG-to-text as text-to-text; when combined with the GAP graph-aware modules, it produced higher generation metrics without any additional graph-specific pre-training. Also effective in few-shot settings (higher BLEU vs BART and some pre-trained baselines).",
            "limitations": "Linearization depends on chosen ordering of triples (no canonicalization reported), which can introduce variance; long graphs increase token length and may hit model max input limits (paper fixes max graph sizes: 50 for WebNLG, 60 for EventNarrative). Linearization alone does not explicitly encode fine-grained topological neighborhood beyond the triple set and so benefits from additional graph-aware components.",
            "comparison_with_other": "Compared to approaches that rely on additional large-scale pre-training on graph↔text corpora (e.g., KGPT, JointGT), simple linearization plus GAP graph-aware encoder matched or exceeded performance on WebNLG and EventNarrative in this paper, while requiring no extra pre-training; compared to vanilla fine-tuned BART (no graph modules), linearization plus graph-aware components improved BLEU/METEOR/ROUGE and few-shot robustness.",
            "uuid": "e6983.0",
            "source_info": {
                "paper_title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-04"
            }
        },
        {
            "name_short": "GAP-GraphRep",
            "name_full": "GAP Graph-aware attention representation (pooled graph components + masked/type-biased attention)",
            "brief_description": "A hybrid representation that pools token vectors into a fixed set of graph component vectors (entities and relations) and applies a neighborhood-masked, connection-type-biased attention (matrix M and type-encoding γ(T)) to inject topological and relation-type information into pre-trained language model representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Pooled graph-component matrix + neighborhood-masked / type-biased attention",
            "representation_description": "From the linearized token sequence, the model pools (average) the token vectors corresponding to each graph component (individual entities and relations) to form a graph representation matrix X_g. A learnable attention over X_g is computed with two graph-aware additives: (1) a masking matrix M (generalized adjacency) that sets -∞ where attention should be blocked (neighborhood-based masking), and (2) a type-encoding γ(T) that adds learned scalar biases to attention scores depending on the connection type (entity-entity, entity-relation, relation-entity, relation-relation). The updated graph representations are then gathered back and added residually to the token-level representations.",
            "representation_type": "graph-structured (matrix) attention; hybrid (token-level → pooled graph nodes/edges → token residual); preserves neighborhood structure explicitly",
            "encoding_method": "1) Pool token embeddings by entity/relation mention spans to build X_g; 2) compute Q,K,V from X_g and apply masked attention: softmax((QK^T)/sqrt(d_k) + M + γ(T)) V; 3) gather updated component vectors back to token positions and add as residual to the LM representations.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "WebNLG v2.0; EventNarrative",
            "task_name": "Knowledge-graph-to-text generation / LM fine-tuning with graph-aware encoder",
            "model_name": "GAP (graph-aware modules) built on BART (bart-base)",
            "model_description": "BART encoder-decoder (bart-base) is used as the Global Attention module; GAP inserts additional Graph-aware Attention layers that operate on pooled graph components (entities and relations) using masking matrix M and type encoder γ(T). Hyperparameters: BPE vocab size 50,265; learning rate 3e-5 (paper experiments); max graph nodes 50/60 for datasets.",
            "performance_metric": "BLEU, METEOR, ROUGE-L, BERTScore, Entity accuracy; also per-mask ablations and few-shot BLEU",
            "performance_value": "WebNLG: Best GAP variant (e.g., M_e,r^e with type encoding) reported BLEU ≈ 65.92 (other mask variants: M_e,r 65.86; M_e,r,e,r 65.11). Reported relative improvements vs prior non-pretrained SOTA: +5.20% BLEU; vs KGPT: +1.81%/+2.09% on BLEU; vs JointGT variants: +1.32%/+1.6% on BLEU (as reported). EventNarrative: M_e,r^e improved BLEU by +3.70%, METEOR +0.82%, ROUGE +1.63% relative to plain BART. Few-shot WebNLG BLEU for M_e,r + γ(T): 0.5%→39.50, 1%→44.03, 5%→55.68, 10%→58.30.",
            "impact_on_training": "Injecting graph-aware masked attention into pre-trained LMs improved downstream NLG metrics without additional graph-specific pre-training, improved few-shot performance, and allowed competitive or superior performance relative to models that used expensive auxiliary pre-training tasks.",
            "limitations": "Design choices of the mask M and the inclusion/exclusion of relation→relation attention significantly affect performance: letting relations attend to relations often reduced performance, especially on small graphs. The method requires pooling strategies and a fixed maximum number of graph components for parallelization; it increases encoder complexity and requires deciding which neighborhood connections to permit. Larger graphs pose computational constraints (paper uses fixed max node count).",
            "comparison_with_other": "Compared to prior methods that assume full connectivity among nodes (global self-attention) or rely on pre-training on large KG-text corpora (KGPT, JointGT), GAP's graph-aware attention explicitly encodes sparsity/topology via M and encodes connection types via γ(T), producing better interpretability (attention heatmaps) and competitive or superior results without auxiliary pre-training. Ablations showed type encoding helps the most general mask (M_e,r^e,r) simulate more restrictive, better-performing behaviors.",
            "uuid": "e6983.1",
            "source_info": {
                "paper_title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation",
                "publication_date_yy_mm": "2022-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "KGPT: Knowledge-grounded pretraining for data-to-text generation",
            "rating": 2,
            "sanitized_title": "kgpt_knowledgegrounded_pretraining_for_datatotext_generation"
        },
        {
            "paper_title": "JointGT: Graph-text joint representation learning for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "jointgt_graphtext_joint_representation_learning_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from rdf data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "GTR-LSTM: A triple encoder for sentence generation from RDF data",
            "rating": 2,
            "sanitized_title": "gtrlstm_a_triple_encoder_for_sentence_generation_from_rdf_data"
        },
        {
            "paper_title": "Step-by-step: Separating planning from realization in neural data-to-text generation",
            "rating": 1,
            "sanitized_title": "stepbystep_separating_planning_from_realization_in_neural_datatotext_generation"
        },
        {
            "paper_title": "Modeling graph structure via relative position for text generation from knowledge graphs",
            "rating": 1,
            "sanitized_title": "modeling_graph_structure_via_relative_position_for_text_generation_from_knowledge_graphs"
        }
    ],
    "cost": 0.0118605,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation
18 May 2023</p>
<p>Anthony Colas acolas1@ufl.edu 
Department of Computer Science
University of Florida</p>
<p>Mehrdad Alvandipour m.alvandipour@ufl.edu 
Department of Computer Science
University of Florida</p>
<p>Daisy Zhe Wang daisyw@ufl.edu 
Department of Computer Science
University of Florida</p>
<p>GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation
18 May 20234EC0D809268F97B3F85615E4EEE6D574arXiv:2204.06674v4[cs.CL]
Recent improvements in KG-to-text generation are due to additional auxiliary pre-training tasks designed to give the fine-tune task a boost in performance.These tasks require extensive computational resources while only suggesting marginal improvements.Here, we demonstrate that by fusing graph-aware elements into existing pre-trained language models, we are able to outperform state-of-theart models and close the gap imposed by additional pre-training tasks.We do so by proposing a mask structure to capture neighborhood information and a novel type encoder that adds a bias to the graph-attention weights depending on the connection type.Experiments on two KG-to-text benchmark datasets show our models are competitive while involving fewer parameters and no additional pretraining tasks.By formulating the problem as a framework, we can interchange the various proposed components and begin interpreting KG-to-text generative models based on the topological and type information found in a graph.We publically release our code 1 .</p>
<p>Introduction</p>
<p>Due to the amount of data stored in Knowledge Graphs (KGs) (Auer et al., 2007;Vrandečić and Krötzsch, 2014;Bollacker et al., 2008;Yates et al., 2007;Bodenreider, 2004;Wishart et al., 2018), they are important to properly transcribe into natural language sentences, making them more easily comprehensible to a larger audience.This task, termed KG-to-text, has found recent success in generating knowledge-grounded dialog responses (Wen et al., 2016;Zhou et al., 2018), question answering (He et al., 2017;Bhowmik and de Melo, 2018;Pal et al., 2019;Agarwal et al., 2021), story generation (Guan et al., 2019;Ji et al., The Great Debate, also called the Shapley -Curtis Debate, was held on 26 April 1920 at the National Museum of Natural History, between the astronomers Harlow Shapley and Heber Doust Curtis.The format of the Great Debate has been used subsequently to argue the nature of fundamental questions in astronomy.2020), and event narration (Colas et al., 2021).KGto-text involves encoding a KG, often sparse, in order to generate a coherent and representative textual description of the KG as shown in Figure 1.In contrast, Abstract Meaning Representation (AMR)to-text deals with a more restrictive space, where graphs follow a predefined dense, connected template (Ribeiro et al., 2021;Koncel-Kedziorski et al., 2019).Thus, when encoding a KG, one should carefully consider the graph's structure to properly generate its corresponding text.</p>
<p>Recently, pre-trained language models (LMs) have produced state-of-the-art results on the KGto-text generation task (Ribeiro et al., 2020a;Chen et al., 2020).These models tend to first linearize a graph into a sequence of tokens, and fine-tune on pre-trained LMs such as BART (Lewis et al., 2020), GPT (Radford et al., 2019), or T5 (Raffel et al., 2020), treating the task similarly to a text-totext task.Because of the performance gains caused by the self-supervised pre-training tasks, current work on KG-to-text has focused on developing pretrained tasks and large-scale unlabeled graph-text corpora, replicating the success in the text-to-text domain (Chen et al., 2020;Ke et al., 2021).However, these works particularly focus on leveraging large amounts of pre-trained data for graph-to-text specific pre-trained tasks, e.g., recovering a masked text sequence based on a given complete KG.</p>
<p>Although recent work in KG-to-text has begun to combine LMs with a graph-aware approach (Ke et al., 2021), they do not adequately perform a graph-aware encoding, overlooking the KG's topological information.Similarly, recent work in AMR-to-text has begun to observe the role of graph adaptors in dense, highly parallel data, using a Graph Convolutional Network (GCN) (Ribeiro et al., 2021).Instead, our framework leverages a topological attention mechanism, better adhering to the language model paradigm and giving room for interpretation.</p>
<p>We argue and show empirically that without additional pre-trained tasks, a fully graph-aware encoding combined with the coverage of pre-trained LMs such as BART (Lewis et al., 2020), can compete with and in some cases outperform those approaches which rely on additional pre-training.By doing so, we unload the burden of requiring vast amounts of data and computational resources required for pre-training.</p>
<p>We propose GAP, a KG-to-text framework which fuses graph-aware elements into existing pre-trained LMs, capturing the advantages brought forth by both model types.Our framework has two main components: (i) Global Attention: A graph's components are first encoded using an LM to capture their global semantic information, allowing the model to utilize the lexical coverage of pre-trained LMs (Davison et al., 2019;Gururangan et al., 2020;Vulić et al., 2020).(ii) Graph-aware Attention: Next, we devise a topological-aware graph attention mechanism, with entity/relation type encoding.</p>
<p>Our framework attends to and updates entity, relation, or both representations.By proposing such a framework, where graph-aware components can be interchanged, we can begin exploring explainable generative models for the KG-to-text task.</p>
<p>We evaluate GAP on two publicly available KG-to-text datasets: WebNLG v2.0 (Shimorina and Gardent, 2018) and EventNarrative (Colas et al., 2021), achieving state-of-the-art results on various natural language generation (NLG) metrics and demonstrate the value of our fully graph-aware based approach.Our contributions are as follows:</p>
<ol>
<li>
<p>We propose a novel graph-aware framework for KG-to-text by introducing neighborhoodmasked attention and connection type encoding into pre-trained LMs, capturing both local structural and global contextual information.</p>
</li>
<li>
<p>We provide more interpretable insights on KGto-text generative models by drawing upon our framework and interchanging the various masking and type schemes, evaluating the output based on the variable graph topology.</p>
</li>
<li>
<p>We demonstrate on two datasets that by simply finetuning our models, which infuse graphaware elements into existing LMs, one can even marginally outperform current state-ofthe-art models which rely on several computationally expensive pre-training tasks.</p>
</li>
</ol>
<p>We make our code publically available to motivate future research.</p>
<p>2 Related Work</p>
<p>KG-to-Text with Graph Transformers</p>
<p>Graph Neural Networks (GNNs) (Veličković et al., 2018) have shown to be effective at encoding graph data.For the KG-to-text task, recent works have leveraged GNNs to encode a graph's neighborhood information (Koncel-Kedziorski et al., 2019;Marcheggiani and Perez-Beltrachini, 2018;Ribeiro et al., 2020b;Schmitt et al., 2021;Guo et al., 2019;Jin et al., 2020) before decoding its corresponding textual representation.Other work instead choose a more global approach and base their encoder on a Transformer-based architecture (Vaswani et al., 2017), calculating self-attention from all the nodes in a graph (Zhu et al., 2019;Cai and Lam, 2020;Ke et al., 2021).Like previous work, we encode neighborhood information in the Graph-aware Attention module.Recently, graph convolution-based adaptors have been explored for Abstract Meaning Representation-to-text (Ribeiro et al., 2021).Unlike previous work, GAP is a framework for KG-to-text, where the KG's topology and masking scheme are not set.While there has been work examining the effect of encoding a node's relative position (Shaw et al., 2018;Schmitt et al., 2021), we instead encode type, arguing that a KG's textual description is weighted based on its different types of connections, and empirically show its effect on KG-to-text generation.</p>
<p>2.2 KG-to-Text with Pre-trained LM</p>
<p>With the advent of pre-trained LMs such as BART (Lewis et al., 2020), T5 (Raffel et al., 2020), and GPT (Radford et al., 2019), these models have been directly adapted and fine-tuned for the KGto-text task and in some cases outperformed GNNbased models (Ribeiro et al., 2020a;Kale and Rastogi, 2020;Chen et al., 2020;Mager et al., 2020).</p>
<p>While work has begun to explore combining such pre-trained models with transformer-based architectures which encode node information (Ke et al., 2021), they assume connectivity between all nodes and do not leverage updating relation information.Instead, here we propose a framework which combines pre-trained models with graph-aware encoders which are specifically neighborhood-based and dependent on a given graph's topology.</p>
<p>Problem Statement</p>
<p>We aim to generate texts that describe a given KG.We define a KG to be a multi-relational graph G = (V, E), where V is the set of entity vertices and E ⊂ V × R × V is the set of edges that connect entities with a relation from R.</p>
<p>Proposed Framework</p>
<p>As our model is built on top of LMs such as BART, we first linearize the knowledge graph into a text string (Distiawan et al., 2018;Moryossef et al., 2019;Su et al., 2021).The linearization is a sequence of all triples in the KG, interleaved with tokens that separate each triple and the triple's components (head, relation, and tail).Figure 2 shows an example linearization for a small knowledge graph, along with its labeled components.</p>
<p>Global Attention</p>
<p>We then use a transformer encoder to contextualize the vector representations.The first module in each transformer layer is a self-attention over the linearized graph, which acts as a Global Attention and captures the semantic relationships between all tokens.The Global Attention can be initialized with a pre-trained LM.At the l-th layer, the self-attention is formulated as:
X l = Attn(Q, K, V ) = softmax QK √ d k V
(1) Query, key, and value are computed via Q
= X l−1 W Q l , K = X l−1 W K l , and V = X l−1 W V l−1 . X l−1 ∈ R n×d</p>
<p>Graph Aware Attention</p>
<p>While the Global Attention assumes connectivity between all graph components, KG adjacencies are sparse in nature.To capture this, we propose a Graph-aware Attention module, by first retrieving entity/relation vectors from the word vectors.Some entities or relations contain several words or repeat several times in the linearized graph.To get a single vector for each entity/relation, we add a pooling layer, which takes the average of the corresponding word vectors for each entity/relation.Hence, we get the graph representation matrix X g l ∈ R m×d :
X g l = pooling(X l )(2)
Note, m &lt; n, where m and n denote the number of graph components and number of tokens, respectively.In practice and for parallelization m will be a fixed number larger than this sum for all graphs in the dataset, and the graph representation can be accessed via masking.We propose a novel graph-aware attention on the graph representation X g l by introducing a neighborhood-based masking scheme and novel type encoder:
Xg l = Attn M,T (Q, K, V ) = softmax QK √ d k + M + γ(T ) V.(3)
Here Q, K, V are constructed from X g l by multiplying it with their corresponding learnable parameter W .While M ∈ R m×m is a mask that encodes the desired graph structure, and γ(T ) ∈ R m×m is the type encoding matrix.Note, each row of Q, K, and V correspond to an element from the graph (an entity or a relation), and before applying a softmax in each row of QK , we can mask/modify the scores based on the graph topology.For instance, M ij = −∞ forces the item i to not attend to item j or the value at γ(T ) ij can add a bias to the attention score based on the type of connection between</p>
<p>A) Input Module B) Encoder</p>
<p>Global Attention (LM)</p>
<p>Graph-aware Attention</p>
<p>Graph Linearization items i and j.We exploit this capacity to inject graph-awareness by adding a masking matrix M and type encoding matrix γ(T ).</p>
<p>Graph Representation</p>
<p>Graph Topology Encoding</p>
<p>The proposed matrix M ∈ R m×m encodes the graph topology by assigning −∞ where attention is blocked and 0 otherwise.M can be thought of as a generalized adjacency matrix for graph G, which has both nodes and edges as its rows and columns.Hence, to encode neighborhood information for an entity, we can modify its corresponding row in M to have the value 0 for its neighbors, and −∞ otherwise.As the rows and columns of M contain relations, we also have the capacity to let relations attend to their neighboring entity or relations.</p>
<p>From a graph topology perspective, we have several design choices for the matrix M .We can let entities attend to neighboring entities, neighboring relations, or both.We also have these same options for when relations are playing the query role; that is, when choosing which components should relations attend to.For ease of reference and discussion, superscript denotes neighborhood types for entities, while subscript for relations, e.g.M e,r e,r .For instance, when entities attend to neighboring entities and relations, but relations only attend to entities, we denote the masking matrix by M e,r e .Figure 3 illustrates two such matrices via a graph and its attending components.</p>
<p>Connection Type Encoding</p>
<p>In contrast to M which encodes the general graph topology, we also introduce a new type encoding T ∈ R m×m , designed for biasing the attention values between the different graph components based on their connection type.For instance, when an entity e is attending to its neighbor entities {e i } and relations {r i }, we encode the two connection types and bias their attention scores.Type information is stored in a matrix T , and we then use an embedding lookup γ : Z → R to learn scalar embeddings for the types in T .</p>
<p>We define type T ij between query i and key j based on two factors: (i) whether the two items are connected and (ii) the type of each item, i.e. whether the connection is entity-entity, entityrelation, relation-entity, or relation-relation: on the graph's connection types.Intuitively, this capacity would allow us to interpolate between different choices of M , or in the extreme case it can push model M e,r e,r , to simulate any of the other more restrictive masks.For ease of reference, we explicitly state the type encoding whenever used.
T ij =            1 if i and j are neighboring entities, 2 if (i, j) is an (entity,edge) pair, 3 if (i, j) is an (edge,
Finally, after producing the new graph representation Xg l with equation (3), we gather the word representations from the graph representation, adding the new representations as a residual to X l , and generate the output from the l-th layer:
Xl = gather( Xg l ) + X l (5)
5 Experiments</p>
<p>Datasets</p>
<p>We experiment on two KG-to-text supervised datasets: WebNLG v2.0 (Gardent et al., 2017;Shimorina and Gardent, 2018) and EventNarrative (Colas et al., 2021).We experiment with different configurations on the graph representation, attention mask, and type encoding on the WebNLG dataset, taking the best performing models to experiment further on EventNarrative.This is because of computational constraints caused by the size of Event-Narrative.Table 1 outlines the statistical differences between the two datasets.We use the official data split for both.</p>
<p>WebNLG is a crowd-sourced RDF triple-to-text dataset manually crafted by human annotators.The dataset contains graphs from DBpedia (Auer et al., 2007) with up to 7 triples paired with one or more reference texts.As in Chen et al. ( 2020) and Ke et al. (2021), we evaluate on the 2.0 release 2 .</p>
<p>EventNarrative is an automatically generated large-scale event-centric KG-to-text supervised dataset.Event KGs are extracted from Wikidata (Vrandečić and Krötzsch, 2014) and Even-tKG (Gottschalk and Demidova, 2018), which are then matched to Wikipedia sentences.EventNarrative contains a larger number of unique KG components compared to WebNLG.</p>
<p>2 https://gitlab.com/shimorina/webnlg-dataset</p>
<p>Implementation and training details</p>
<p>We chose to use BART as our pre-trained LM (Lewis et al., 2020), and initialize its respective parameters with the Hugging Face's pre-trained bart-base checkpoint3 .We left the default hyperparameters on the Global Attention module (BART) due to limited computational resources, instead experimenting on the Graph-aware attention module.</p>
<p>When evaluating, we follow the existing work for KG-to-text and report the model's performance with BLEU (Papineni et al., 2002), ME-TEOR (Banerjee and Lavie, 2005), and ROUGE-L (Lin, 2004) scores as the automatic NLG metrics.</p>
<p>Baselines</p>
<p>Fine-tuned LM.To evaluate the effect of the graph-aware attention module in our framework, we compare with a vanilla fine-tuned BART LM, which is not additionally pre-trained on any graphtext specific task.We do so for both WebNLG and EventNarrative, noting that for EventNarrative such a baseline is the state-of-the-art.</p>
<p>Pre-trained KG-to-Text Models.</p>
<p>We further compare our framework with models which have pre-trained LMs on additional tasks, including KGPT (Chen et al., 2020) and JointGT (Ke et al., 2021).KGPT performs an additional KG-to-text generation pre-training task on KGText, a loosely-supervised large-scale KG-to-text dataset, before finetuning.JointGT performs three additional pre-training tasks for KG reconstruction, text reconstruction, and KG-text alignment on the KGText dataset before finetuning.For a fair comparison with JointGT, we also compare our results to JointGT's BART pre-trained task, where they perform an additional text infilling and sentence permutation task on KGText.3 show our results on the WebNLG and EventNarrative datasets, respectively.On both datasets, we observe improvements over existing LM-based models with GAP.For BLEU score on WebNLG, we observe a +5.20% improvement over the state-of-the-art without any pre-training (Shimorina and Gardent, 2018)  GAP makes use of the local neighborhood information when encoding graph components.We outperform both KGPT and JointGT (on WebNLG), which rely on additional pre-training tasks for graph-text reconstruction and alignment.On BLEU score, we observe an improvement of +1.81% and 2.09% over KGPT, and +1.32% and 1.6% over JointGT (with BARTPretrain).Further, our M e,r with Type Encoding model outperforms JointGT (with JointGTPretrain) by 0.28% without the need for any additional pre-training.Joint-GTPretrain refers to all three pre-trained tasks described in Ke et al. (2021).Instead of pre-training, we fill the gap with a modification to the encoder structure such that the model adapts to the graph structure.To summarize, we have shown that when adapting pre-trained language models such as BART, a careful modification of the encoder structure can better align the LM with the new task.</p>
<p>Main results</p>
<p>Table 2 and Table</p>
<p>On EventNarrative, for model M e,r e we achieve an improvement of +3.70%, +0.82%, +1.63% on BLEU, METEOR, and ROUGE, relative to BART, further demonstrating that the graph-aware structure and type encoder can perform comparatively well on large and more complex graphs.We note a We note all models have similar BERTScores.</p>
<p>6 Analysis</p>
<p>Ablation Studies</p>
<p>We explore different maskings and type encodings for the graph-aware attention module on WebNLG.summarized on Table 4 and Table 5.</p>
<p>Masking Scheme.From bottom to top on Table 4, our first observation is that when relations directly attend to the neighboring relations, the performance drops by 1.28%, the largest difference.</p>
<p>In fact, the results significantly improve when we completely block attention on relations (M e e ).However, for the entities, it is always best to attend to their edges (relations) as well as their neighboring entities.The top two results are comparable (0.06% difference in BLEU score), and each one could be considered the best performing model depending on the evaluation metric.For relations, it might be somewhat helpful to not attend to neighboring relations, while for entities, attending to the relations will lead to better results (+0.81%).Type Encoder.Table 5 shows the effect of type encoding on the results on WebNLG.To better understand the effect of type encoding on each of the models, we compare Table 4 with Table 5. Recall that the type encoding γ(T ) for each model depends on the connections that exists in the model graph structure.For instance, the most general model M e,r e,r has all four possible connection types encoded by equation ( 4), while the model with M = M e,r only has two types, which can be encoded by a restriction of equation ( 4).According to Table 4, the model M e,r e,r performs worst without encoding.However, because of its generality, i.e. having all the possible connection types, it is possible for this model to drift toward better configurations with the help of γ(T ).The results in Table 5 help support these insights for model M e,r e,r .Type encoding allows this model to simulate what we observed is best in the previous section, i.e. relations are better off not to attend to relations, whereas entities can attend to both while paying less attention to relations.This nuanced behavior seems to be achievable only via type encoding.Results for model with M = M e,r and type encoding also point towards this; type encoding seems to facilitate a non-uniform attention distribution based on the type and produces a better result.</p>
<p>Few-Shot Learning</p>
<p>To further reinforce our claims that our model alleviates the need for pre-training in the KG-to-text task, we consider various few-shot learning settings where only a small percentage of training instances were used for finetuning.As highlighted in Table 6, GAP outperforms all state-of-the-art pretrainedbased approaches, without needing to pre-train, indicating that our fully graph-aware framework is more appropriate than established pre-trained tasks, especially when such data is not avialable.</p>
<p>KG Size</p>
<p>As in Ke et al. (2021), we divide the WebNLG test set into two subsets (1-3 and 4-7 triples) to compare the performance of our different masking configurations.Table 7 shows that while all configurations perform similarly for larger graphs, the difference in performance is clearer on smaller graphs, where M e,r e performs +1.74% better than M e,r e,r , suggesting that relations paying attention to relations can add too much complexity to the model, especially on simpler graph structures.</p>
<p>Interpretability</p>
<p>We begin to interpret KG-to-text models by analyzing the graph-attention weights induced by each graph structure on a per-sample basis, analogous to analyzing node-to-node attention weights in the KG question-answering domain (Yasunaga et al., 2021).By introducing a framework to the KG-totext task, we can condition the changes in the output text on the different components of the framework, including the masking and type encoder.We can then observe the differences in the output text based on the graph's topological structure or what relations and entities attend to.</p>
<p>In Figure 4 we show an example KG representing Aenir, an Australian fantasy novel, with its relations (orange) and entities (blue) along with the attention heatmaps and outputs from two of our framework decisions.The left (a) heatmap and output corresponds to our best performing model without type encoding, M e,r e , while the right (b) corresponds to M e e .We choose these two mask-Output: Aenir is written in English and was followed by Above the Veil which is from the country of Australia.</p>
<p>Output: Aenir, written in English, was followed by Above the Veil, which is written in the English language.ing configurations, because the attention-weight differences are apparent.</p>
<p>From (a), entities attend to both entities and relations, whereas relations only attend to entities.Interestingly, the attention distribution appears uniform across all graph components (both for entities and relations).From (b) we see a similar uniform distribution across entities and relation attending to only entities.Thus, in (a), while relation 'country' attends to 'Australians' and vice-versa, in (b) 'Australians' does not attend to 'country', perhaps giving a difference in the output, as the final output in (a) contains 'from the country of Australia' while the text in does not.Moreover, in both (a) and (b) 'Above the Veil' is the subject of the second clause.However, 'Above the Veil' attends to 'country' only in (a), therefore influencing (a)'s output of 'Above the Veil which is from the country of Australia'.Instead, (b) introduces some redundancy in its second clause instead of transcribing new information from the KG.</p>
<p>Figure 5 shows the output sentence, and the attention heatmap produced by our most general model with M = M e,r e,r and type encoding, on the graph shown in Figure 4. We examine the differences between this model, referred to as model ( 1), and the model with M = M e,r e and no type encoding, referred to as model (2).First, note that in terms of BLEU score (1) performs slightly worse than (2), however a human annotator may rank (1) over (2), as (1) is more concise while communicating the same information.For example, (1) uses the word 'sequel to' rather than 'followed by' and 'published in' instead of 'from the country', which can sound Output: Above the Veil was the sequel to Aenir which is written in English and was published in Australia.</p>
<p>Conclusion</p>
<p>We presented GAP, a graph-aware language model framework for KG-to-text generation.Our framework instills the local information captured by graph attention into the global contextualized word vector representation within pre-trained LMs.We demonstrated multiple configurations of our framework by introducing a graph-aware attention mask-ing scheme and novel type encoder module, and through qualitative analysis showed that GAP outperforms existing KG-to-text models, including those that rely on additional auxiliary pre-training tasks.By closely examining the different framework configurations, we introduce the capacity to interpret KG-to-text outputs through a graph's attention structure and topology.</p>
<p>Broader Impacts</p>
<p>GAP provides researchers with a state-of-the-art framework for KG-to-text models.Though we experiment with supervised baselines which include a handcrafted dataset, WebNLG, and an automatically generated dataset, EventNarrative, repositories of structured data exist in the clinical (Johnson et al., 2016), medical (Bodenreider, 2004), and news crises (Leetaru and Schrodt, 2013;Ward et al., 2013) domains.By transforming clinical data into natural language narratives, patients with low health-literacy can benefit by more easily understanding their electronic medical records (EMRs), and doctors can more easily transcribe patient data for future use cases, i.e. connecting such data to the medical literature.Such models can also help analysts more easily understand crises data from various news sources, in turn helping them evaluate cause-effect relationships and detect misinformation.While malicious actors can exploit generative models for disinformation, we discourage the use of GAP in generating such data and openly release our model to help combat such efforts.</p>
<p>A Hyperparameter Details</p>
<p>As followed by Ke et al. (2021) and BART, we used a Byte-Pair Encoding (BPE) vocabulary (Radford et al., 2019) with a size of 50,265.The model's parameters were optimized via Adam (Kingma and Ba, 2015), with a batch size of 16, a learning rate of 3e-5, and a maximum graph size of 50 and 60 for WebNLG and EventNarrative, respectively.Table 8 provides the model hyperparameter settings used for experimenting on both the WebNLG and EventNarrative datasets.We keep all listed hyperparameters constant with respect to the GAP configurations.We increase num nodes for the EventNarrative dataset due to the properties of the dataset, i.e. the possibility of having graphs composed of more than seven triples.We also set the eval period to 5,000 for EventNarrative due to its size, containing approximately 22,000 samples in its test set.As in (Colas et al., 2021)</p>
<p>B Additional Experimental Results</p>
<p>We provide additional experimental results on both WebNLG v2.0 and EventNarrative for the proposed GAP framework for reference and further analysis.</p>
<p>B.1 Graph Length</p>
<p>Here we examine a comparative study to that of Table 7 for the EventNarrative dataset.</p>
<p>B.2 Entity Accuracy</p>
<p>To give more insight into KG-to-text generation with GAP, we provide the results for entity accuracy.We define entity accuracy to be the number of entities from the KG that appear in the generated text over those that appear in the reference text.Table 11 shows that all models perform exceedingly well in generating the correct entities from their respective KGs, suggesting that future KG-to-text research should focus on sentence structure and descriptors, i.e. quantifiers and determiners.</p>
<p>Datasets</p>
<p>C Additional Examples and Error Analysis</p>
<p>We now present example outputs generated by GAP both on the WebNLG and EventNarrative dataset in Tables 12 and 13 below.</p>
<p>C.1 WebNLG</p>
<p>We showcase five different examples from the WebNLG test set output by our M e,r + γ(T ) (Pre-diction 1) and M e,r e,r + γ(T ) (Prediction 2) models.As can be seen in all the examples, GAP is able to generate fluent and complete sentences.In the first two examples, the output from both models are identical.The outputs from the third example can be viewed as paraphrases of one another, where Prediction 1 mentions 'US national' while Prediction 2 instead uses the adjective 'American' to convey the same information.Furthermore, in both predictions we learn that 'Alan Bean' was a 'test pilot' and 'selected by NASA' but in slightly different formats.In the fourth example, Prediction 2 is missing the name of the rock band, 'NRBQ', while maintaining the rest of the information.Like the third example, the predictions in the fifth example are paraphrases.</p>
<p>C.2 EventNarrative</p>
<p>Because of the length of output in EventNarrative, we present four different types of examples to elaborate on the limitations of KG-to-text models.Here, we show example outputs from our M e,r e (Prediction 1) and M e,r +γ(T ) (Prediction 2) models.In the first example, we observe a contradiction in both Prediction 1 and 2: the gubernatorial candidate was a democratic nominee, while our predictions conveyed otherwise.The second example shows two predictions which are identical, both missing information, specifically 'ozone park' and 'for three -year -olds and up'.Upon further inspection, these two pieces of information are not within the KG.Similarly, in the third example the only piece of information missing from the predictions, namely 'cork county board', is not part of the KG.This example also contains invalid information, '112th' instead of '103rd'.The last example also contains invalid information regarding the dates in both predictions.Additionally, Prediction 2 is missing information about the 'village of ignacewo'.Prediction 1 the sport page handicap is an american thoroughbred horse race run annually at aqueduct racetrack in queens , new york .Prediction 2 the sport page handicap is an american thoroughbred horse race run annually at aqueduct racetrack in queens , new york .Reference the sport page handicap was an american thoroughbred horse race run annually at aqueduct racetrack in ozone park, queens, new york for three -year -olds and up.</p>
<p>Prediction 1 the 1991 cork senior hurling championship was the 112th staging of the cork premier hurling competition , and the 86th staging by a team from cork .Prediction 2 the 1991 cork senior hurling championship was the 112th staging of the cork premier club hurling competition since its establishment in 1887 .Reference the 1991 cork senior hurling championship was the 103rd staging of the cork senior hurling championship since its establishment by the cork county board in 1887.</p>
<p>Prediction 1 the first battle of ignacewo was one of the first battles of the january uprising .it took place on january 28 , 1863 , near the village of ignakewo , konin county in southwestern corner of russian -controlled congress poland .Prediction 2 the first battle of ignacewo was one of the first battles of the january uprising .it took place on january 6 , 1863 , near the village of konin , in congress poland .Reference the first battle of ignacewo was one of many clashes of the january uprising.it took place on may 8, 1863, near the village of ignacewo, konin county, which at that time belonged to russian empire's congress poland.</p>
<p>Figure 1 :
1
Figure1: Given a graph, KG-to-text generation aims to describe the entities, relations, and its inherent structure via natural language text (grey callout).Corresponding graph-text components are color-coded.</p>
<p>denotes the collection of vectors corresponding to the graph's tokens.The model's parameters are denoted by W with size d k × d k , where d k is the dimension of word vectors.</p>
<p>Figure 2 :Figure 3 :
23
Figure2: Overview of the Graph-aware framework for graph-to-text generation.Given a KG, we first transform the graph into its appropriate representation before linearizing the graph.Next, each node of the KG is encoded via a global attention, followed by a graph-aware attention, ultimately being decoded into a sequence of tokens.</p>
<p>Figure 4 :
4
Figure 4: Interpreting KG-to-text models via analyzing graph attention weights, which the graph-aware encoder activates.We show each model's output for further emphasis.</p>
<p>Figure 5 :
5
Figure 5: An additional case study of the graph attention weights for model M e,r e,r with type encoding.</p>
<p>Table 1 :
1
Statistics of the supervised KG-to-Text datasets used for experimenting.
entity) pair,</p>
<p>Table 2 :
2
Shimorina and Gardent (2018)r BART.This improvement suggests that the graph-aware component of Performance comparison on WebNLG.KGPT and JointGT, marked with † and ‡, re-printed from Chen et al. (2020) andKe et al. (2021), have been pre-trained on one and three additional tasks, where Pre+ denotes if additional pre-training was performed.We mark results fromShimorina and Gardent (2018)with .We report our best models with and without type encoding, which have approximately the same number of parameters.
Model</p>
<p>Table 3 :
3
(Ke et al., 2021)21)on on EventNarrative.We compare to the pretrained baselines, T5 and BART, reprinted from(Colas et al., 2021), and adapt JointGT(Ke et al., 2021)to the dataset.</p>
<p>Table 4 :
4
Experimental results of the different masks applied to the WebNLG v2.0 test set.
GAP BLEU METEOR ROUGEM e,r e65.92 46.8176.22M e,r 65.86 46.8676.28M e e M e,r e,r65.11 46.33 64.64 46.1775.62 75.04
Ke et al. (2021) WebNLG, where the type encoder can give an additional performance improvement to the graph-structure component of the model.For comparison, we adapt JointGT to EventNarrative, using the hyperparameters fromKe et al. (2021).</p>
<p>Table 6 :
6
BLEU scores of various pre-trained models compared to GAP for few-shot learning on WebNLG.
ModelData Proportion0.5% 1%5%10%BART33.92 39.08 52.24 56.58JointGT37.18 42.26 54.41 57.73M e,r + γ 39.50 44.03 55.68 58.30GAP#Triples 1-3 4-7M e,r e71.48 61.53M e,r 71.28 61.59M e e M e,r e,r70.18 61.05 69.74 60.57</p>
<p>Table 7 :
7
BLEU scores for the different masks applied to the WebNLG v2.0 test set for different graph sizes.</p>
<p>Table 8 :
8
, we set the max output size to 512 for all experiments on EventNarrative.BLEU score on the validation set was used for model selection.Each model was trained on two NVIDIA RTX 2080 Ti GPUs.Hyperparameters for GAP on both the WebNLG and EventNarrative datasets.
HyperparameterWebNLG EventNarrativeLearning Rate2.00E-052.00E-05Warmup Steps16001600Eval Period5005000Beam Size55Length Penalty15OptimizerAdamAdam1.00E-081.00E-08Num Nodes5060Num Relations6060Embedding Size128128Num Global Layers66Num Graph-aware Layers 66Batch Size1616</p>
<p>Table 9 :
9
Table 9 reveals an exponential decay in BLEU score, with lengths 1-3, 4-7, and 7+ having 44.48%, 23.86%, 11.47%, respectively.Compared to WebNLG, the BLEU scores are significantly lower, suggesting that EventNarrative is a more challenging dataset.Table 10 gives a brief synopsis of the dataset sizes with respect to the number of triples.Compared to WebNLG which has no KGs greater than length 7, EventKG contains over 1,000 KGs larger than length 7, making the dataset more diverse.BLEU scores for the EventNarrative test set for different graph sizes.
GAP1-3#Triples 4-77+M e,r e44.48 23.86 11.47Datasets1-3#Triples 4-77+WebNLG1,0175830EventNarrative 16,103 5,152 1,184</p>
<p>Table 10 :
10
Distribution for number of triples in both the WebNLG and EventNarrative datasets.</p>
<p>Table 11 :
11
Entity accuracy on the WebNLG test set.
Accuracyw/o γ(T ) w/ γ(T )M e,r e94.0694.04M e,r93.9994.48M e e M e,r e,r93.64 93.8294.50 94.28</p>
<p>Prediction 1 Amsterdam Airport Schiphol serves the city of Amsterdam and is -3.3528 above sea level .The runway name is 18L/36R Aalsmeerbaan and it has a length of 2014.0 .Prediction 2 Amsterdam Airport Schiphol serves the city of Amsterdam and is -3.3528 above sea level .The runway name is 18L/36R Aalsmeerbaan and it has a length of 2014.0 .Baked Alaska is from Hong Kong and the United States .The main ingredients are meringue , ice cream , sponge cake or Christmas pudding .Prediction 2 Baked Alaska is from Hong Kong and the United States .The main ingredients are meringue , ice cream , sponge cake or Christmas pudding .Reference Baked Alaska comes from both Hong Kong and the United States .The main ingredients are Meringue , ice cream , sponge cake or Christmas pudding .Prediction 1 Alan Bean is a US national who was born in Wheeler , Texas .He served as a test pilot before being selected by NASA in 1963 .He is now retired .Prediction 2 Alan Bean is an American test pilot who was born in Wheeler , Texas .He was selected by NASA in 1963 .He is now retired .Reference The American test pilot Alan Bean ( born in Wheeler , Texas ) was selected by NASA in 1963 .He is now retired .Prediction 1 Al Anderson is a member of rock band NRBQ .Rock music originated from country music which originated from blues music .A musical fusion of rock music is bhangra music .Prediction 2 Al Anderson plays rock music which originated from blues and country music .Bhangra music is part of the fusion genre , partly coming from Rock music which uses the banjo .Reference Al Anderson plays with the rock band NRBQ .Rock has its origins in the blues and country music , where the banjo is played , and Bhangra is a rock fusion .Prediction 1 Paulo Sousa is the manager of ACF Fiorentina .He has played for Inter Milan , Juventus FC and Maccabi Tel Aviv F.C .Prediction 2 Paulo Sousa is the manager of ACF Fiorentina .He has played for Inter Milan , Maccabi Tel Aviv FC and Juventus FC .Reference Paulo Sousa once played for Inter Milan and Juventus FC .He now manages ACF Fiorentina and plays for Maccabi Tel Aviv F.C .
ReferenceAmsterdam Airport Schiphol is -3.3528 above sea level , has a runway name18L/36R'Aalsmeerbaan which is 2014.0 in length and serves the city of Amster-dam .Prediction 1</p>
<p>Table 12 :
12
Examples of output texts generated from M e,r + γ(T ) (Prediction 1) and M e,r e,r + γ(T ) (Prediction 2) on the WebNLG test set.Prediction 1 the 1971 mississippi gubernatorial election was held on 02 november 1971 .incumbent governor bill waller won re -election to a second term , defeating the democratic nominee , former speaker of the chamber of deputies thomas h .williams , by a wide margin .Prediction 2 the 1971 mississippi gubernatorial election took place on 02 november 1971 , in order to elect the governor of mississipp .incumbent governor bill waller , who was first elected in 1972 , faced no opposition from democrats and avoided a runoff .Reference the 1971 mississippi gubernatorial election took place on 02 november 1971 for the post of governor of mississippi.democrat bill waller was elected into office and served as governor of mississippi from 1972 to 1976.</p>
<p>Table 13 :
13
Examples of output texts generated from M e,r e (Prediction 1) and M e,r + γ(T ) (Prediction 2) on the EventNarrative test set.</p>
<p>https://huggingface.co/facebook/bart-base
AcknowledgementsThis work was partially funded and supported by the GSPA at the University of Florida, the McKnight Doctoral Fellowship, the NSF under IIS Award #1526753, and DARPA under Award #FA8750-18-2-0014 (AIDA/GAIA).
Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. Oshin Agarwal, Heming Ge, Siamak Shakeri, Rami Al-Rfou, 10.18653/v1/2021.naacl-main.278Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics2021</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. Springer2007</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Generating fine-grained open vocabulary entity type descriptions. Rajarshi Bhowmik, Gerard De, Melo , Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics2018</p>
<p>The unified medical language system (umls): integrating biomedical terminology. Olivier Bodenreider, Nucleic acids research. 32suppl_12004</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of data2008</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>KGPT: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang, Wang , 10.18653/v1/2020.emnlp-main.697Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Eventnarrative: A largescale event-centric dataset for knowledge graph-totext generation. Anthony Colas, Ali Sadeghian, Yue Wang, Daisy Zhe Wang, 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. 2021</p>
<p>Commonsense knowledge mining from pretrained models. Joe Davison, Joshua Feldman, Alexander M Rush, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Gtr-lstm: A triple encoder for sentence generation from rdf data. Jianzhong Bayu Distiawan, Rui Qi, Wei Zhang, Wang, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>The webnlg challenge: Generating text from rdf data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language Generation2017</p>
<p>Eventkg: A multilingual event-centric temporal knowledge graph. Simon Gottschalk, Elena Demidova, European Semantic Web Conference. Springer2018</p>
<p>Story ending generation with incremental encoding and commonsense knowledge. Jian Guan, Yansen Wang, Minlie Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, Transactions of the Association for Computational Linguistics. 20197</p>
<p>Don't stop pretraining: Adapt language models to domains and tasks. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, Noah A Smith, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning. Shizhu He, Cao Liu, Kang Liu, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsJun Zhao. 20171</p>
<p>Language generation with multi-hop reasoning on commonsense knowledge graph. Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Genwiki: A dataset of 1.3 million content-sharing text and graphs for unsupervised graph-to-text generation. Zhijing Jin, Qipeng Guo, Xipeng Qiu, Zheng Zhang, Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational Linguistics2020</p>
<p>Mimiciii, a freely accessible critical care database. Tom J Alistair Ew Johnson, Lu Pollard, H Shen, Mengling Lehman Li-Wei, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Szolovits, Roger G Anthony Celi, Mark, Scientific data. 312016</p>
<p>Text-to-text pre-training for data-to-text tasks. Mihir Kale, Abhinav Rastogi, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language Generation2020</p>
<p>JointGT: Graph-text joint representation learning for text generation from knowledge graphs. Pei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Linfeng Song, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2021.findings-acl.223Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, CoRR, abs/1412.69802015</p>
<p>Text generation from knowledge graphs with graph transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Gdelt: Global data on events, location, and tone, 1979-2012. Kalev Leetaru, Philip A Schrodt, ISA annual convention. Citeseer20132</p>
<p>Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, Proceedings of the 58th Annual Meeting of the. the 58th Annual Meeting of theBartAssociation for Computational LinguisticsMarjan Ghazvininejad,. 2020</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Gpt-too: A language-model-first approach for amr-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, Salim Roukos, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, 2018In INLG</p>
<p>Step-by-step: Separating planning from realization in neural data-to-text generation. Amit Moryossef, Yoav Goldberg, Ido Dagan, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Answering naturally: Factoid to full length answer generation. Vaishali Pal, Manish Shrivastava, Irshad Bhat, Proceedings of the 2nd Workshop on New Frontiers in Summarization. the 2nd Workshop on New Frontiers in Summarization2019</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, </p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 211402020</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, 10.18653/v1/2021.emnlp-main.351Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Martin Leonardo Fr Ribeiro, Schmitt, arXiv:2007.08426Hinrich Schütze, and Iryna Gurevych. 2020a. Investigating pretrained language models for graph-to-text generation. arXiv preprint</p>
<p>Claire Gardent, and Iryna Gurevych. 2020b. Modeling global and local node contexts for text generation from knowledge graphs. Yue Leonardo Fr Ribeiro, Zhang, Transactions of the Association for Computational Linguistics. 8</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, Leonardo Fr Ribeiro, Philipp Dufter, Iryna Gurevych, Hinrich Schütze, Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing. the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing2021</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>Handling rare items in data-to-text generation. Anastasia Shimorina, Claire Gardent, Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language Generation2018</p>
<p>Plan-then-generate: Controlled data-to-text generation via planning. Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, Nigel Collier, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, International Conference on Learning Representations. 2018</p>
<p>Wikidata: a free collaborative knowledgebase. Denny Vrandečić, Markus Krötzsch, Communications of the ACM. 201457</p>
<p>Probing pretrained language models for lexical semantics. Ivan Vulić, Maria Edoardo, Robert Ponti, Goran Litschko, Anna Glavaš, Korhonen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Comparing gdelt and icews event data. Andreas Michael D Ward, Josh Beger, Matthew Cutler, Cassy Dickenson, Ben Dorff, Radford, Analysis. 2112013</p>
<p>Multidomain neural network language generation for spoken dialogue systems. Th Wen, Gašić, L M Mrkšić, Rojas-Barahona, Su, Vandyke, Young, 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016-Proceedings of the Conference. 2016</p>
<p>Drugbank 5.0: a major update to the drugbank database for. David S Wishart, An C Yannick D Feunang, Elvis J Guo, Ana Lo, Jason R Marcu, Tanvir Grant, Daniel Sajed, Carin Johnson, Zinat Li, Sayeeda, Nucleic acids research. 46D12018. 2018</p>
<p>Qa-gnn: Reasoning with language models and knowledge graphs for question answering. Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, Jure Leskovec, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Textrunner: open information extraction on the web. Alexander Yates, Michele Banko, Matthew Broadhead, Michael J Cafarella, Oren Etzioni, Stephen Soderland, Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)2007</p>
<p>Commonsense knowledge aware conversation generation with graph attention. Hao Zhou, Tom Young, Minlie Huang, Haizhou Zhao, Jingfang Xu, Xiaoyan Zhu, IJCAI. 2018</p>
<p>Modeling graph structure in transformer for better amr-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>            </div>
        </div>

    </div>
</body>
</html>
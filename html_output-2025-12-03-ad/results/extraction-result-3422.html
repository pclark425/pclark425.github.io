<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3422 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3422</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3422</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-33986e5964f025a4f8343322149ef66cf194b5da</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/33986e5964f025a4f8343322149ef66cf194b5da" target="_blank">Down and Across: Introducing Crossword-Solving as a New NLP Benchmark</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work releases a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of around nine thousand puzzles, and introduces solvingCrossword puzzles as a new natural language understanding task.</p>
                <p><strong>Paper Abstract:</strong> Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle. In this work, we introduce solving crossword puzzles as a new natural language understanding task. We release a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles. These puzzles include a diverse set of clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank, abbreviations, prefixes/suffixes, wordplay, and cross-lingual, as well as clues that depend on the answers to other clues. We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clue-answer pairs. For the question answering task, our baselines include several sequence-to-sequence and retrieval-based generative models. We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle. Finally, we propose an evaluation framework which consists of several complementary performance metrics.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3422.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3422.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART-large</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A denoising sequence-to-sequence Transformer pretrained with a text infilling and corruption objective and fine-tuned here to generate answers to crossword clues as text candidates for a downstream constraint solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence-to-sequence Transformer (encoder-decoder) pretrained with a denoising objective; stores world knowledge in parameters and is fine-tuned on the NYT clue-answer pairs to generate answer strings for clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈406M</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>New York Times Crossword (NYT Crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Standard crossword puzzles (mostly 15×15, some 21×21) where answers to natural-language clues must be placed into grid slots of specified lengths with character intersections enforcing equality at overlapping cells (a grid-based spatial/constraint puzzle).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Clues presented as text (single-line clue strings). For puzzle solving the model generates top-k textual answer candidates per clue; these text candidates are post-processed (substring extraction to required slot length) and fed to an SMT-based crossword solver which enforces grid constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning on the clue-answer dataset (supervised seq-to-seq). Top-k decoding (k up to 20) used to supply candidate lists for each clue.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>BART is used only to generate textual answer candidates and does not directly represent or reason over the grid; spatial reasoning (length and intersection constraints) is handled separately by a Z3-based SMT solver. The paper highlights architectural limitations for end-to-end neural spatial reasoning: Transformer decoders produce BPE/token outputs (not character-level), complicating direct character-wise grid assignment and intersection consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-answer task (top-k): Top-1 Exact Match EM = 13.8%, EM_norm = 16.1%; Top-10 EM = 31.0%; Top-20 EM = 34.0% (see Table 2). Crossword puzzle task (using top-20 candidates fed to SMT with oracle prefilter): Word accuracy (Acc_word) = 16.6%, Character accuracy (Acc_char) = 28.4%; Word removal (Rem_word) = 55.6%, Character removal (Rem_char) = 43.4% (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Generative mistakes and missing the ground-truth in candidate lists cause many puzzles to be unsolvable by the SMT solver (nosat). BPE/token-level output complicates character-wise grid filling. Performs poorly on historical clues and some wordplay categories; needs downstream solver to enforce spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared against T5-base (worse) and retrieval-augmented RAG models (substantially worse than RAG). No human performance numbers reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3422.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-base (Text-to-Text Transfer Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified text-to-text Transformer model fine-tuned here to generate answers to crossword clues; used as a baseline generative model for the clue-answer task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (text-to-text) pretrained with a mixture of denoising objectives and fine-tuned on the clue-answer dataset to produce textual candidate answers for clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈220M</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>New York Times Crossword (NYT Crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See BART entry; crossword puzzles require placement of answer strings into grid slots with character intersections (spatial constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Clues as text; model generates top-k textual answer predictions which are then considered as candidate substrings for slot lengths and passed to the SMT solver for grid solving (T5 itself does not enforce grid constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning on the clue-answer dataset (supervised seq-to-seq). Top-k decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>T5 is used as a clue-answer generator only and does not model grid spatial constraints. The paper reports the general limitation that seq-to-seq models are poorly suited to character-level constrained outputs required for grids.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-answer task (top-k): Top-1 EM = 8.4%, EM_norm = 9.5%; Top-10 EM = 18.7%; Top-20 EM = 22.2% (see Table 2). No puzzle-level (SMT) metrics reported for T5 in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Lower accuracy than BART and RAG; candidate lists less likely to contain the ground-truth leading to solver failures. Same character/BPE output limitations as other seq-to-seq models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Underperforms BART-large and retrieval-augmented RAG models on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3422.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-wiki</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with Wikipedia</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end retrieval-augmented Transformer that retrieves relevant Wikipedia passages (21M passages from a Dec 2018 dump) to condition generation; used here to generate stronger clue-answer candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG (wiki)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RAG combines a Transformer encoder-decoder generator with a learned retriever that returns supporting passages (here: Wikipedia 2018 chunks) via MIPS; model trained end-to-end to generate more factually accurate answers for open-domain QA-style clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>New York Times Crossword (NYT Crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See BART entry; crosswords require grid-consistent character assignment across intersecting slots.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Clues as text; retriever returns Wikipedia passages relevant to the clue; the generator produces top-k textual answer candidates; these candidates are substring-extracted to slot lengths and passed to the SMT solver which enforces grid constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning with retrieval-augmented generation (RAG) using pretrained retriever embeddings (primed on Natural Questions); top-k decoding to get candidate lists.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>RAG itself does not perform grid/spatial reasoning — it provides higher-quality candidate answers which increase the chance that the downstream SMT solver can satisfy grid constraints. The paper reports qualitative analysis showing RAG better matches grammatical forms and benefits from document retrieval for abbreviations/prefixes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-answer task (top-k): Top-1 EM = 24.2%, EM_norm = 26.0%, Top-10 EM = 46.8%, Top-20 EM = 50.6%, In_norm (Top-20) = 56.7% (Table 2). Crossword puzzle task (with oracle prefilter & top-20 candidates): Acc_word = 23.8%, Acc_char = 37.8%, Rem_word = 40.3%, Rem_char = 26.3% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although RAG substantially improves candidate quality, many puzzles remain unsolvable because some clues' ground-truth answers are absent from candidate lists; the SMT solver often requires oracle prefiltering to avoid nosat. End-to-end character-level grid assignment still unmet because RAG generates token-level outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>RAG-wiki outperforms BART-large and T5-base on both clue-answer and puzzle-level measures; RAG-wiki and RAG-dict agree on ground-truth matches for ~85% of test set. No direct human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3422.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-dict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) with dictionaries/thesauri</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAG variant that retrieves entries from English dictionaries and thesauri (Wiktionary, Merriam-Webster, Oxford-based Google dictionary) to condition generation for crossword clues that often rely on meanings and synonyms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledgeintensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RAG (dict)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same RAG architecture but retrieval corpus consists of dictionary/thesaurus entries to better support word-meaning, synonym/antonym, and abbreviation clues.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>New York Times Crossword (NYT Crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See BART entry.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Clues as text; retriever returns dictionary entries which the generator uses to produce top-k candidates, then fed to SMT solver for grid solving.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning with retrieval-augmented generation using a dictionary/thesaurus index; top-k decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>As with RAG-wiki, RAG-dict provides better candidate generation for semantic clue types but does not internally solve grid spatial constraints; those are handled by the external SMT solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Clue-answer task (top-k): Top-1 EM = 24.0%, EM_norm = 25.8%, Top-10 EM = 46.0%, Top-20 EM = 50.0% (Table 2). Crossword puzzle task (with oracle prefilter & top-20 candidates): Acc_word = 22.1%, Acc_char = 35.9%, Rem_word = 40.8%, Rem_char = 26.8% (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Similar to RAG-wiki; some clue classes still challenging (historical, dependent), and solver requires the correct answer to appear among candidates or the puzzle must be relaxed/oracle-filtered to converge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Performs similar to RAG-wiki overall; RAG-dict can outperform RAG-wiki on clue types tied closely to dictionary definitions (as shown in qualitative analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3422.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Z3-based SMT crossword solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modified Z3 SMT-based crossword constraint solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Satisfiability Modulo Theories (SMT) formulation (implemented on top of Z3) that models crossword grids as character variables with length and intersection constraints and chooses a consistent assignment from candidate answer lists.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Z3: An efficient smt solver.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Z3 SMT solver (modified implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Formulates each cell as a variable (character) and each slot as the disjunction over candidate word assignments; enforces equality at intersections and slot-length constraints; uses Z3 SMT engine to search for satisfying assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>New York Times Crossword (NYT Crossword)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Grid-based word puzzle where spatial constraints are equalities at intersecting characters and fixed slot lengths; solver handles these constraints formally.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Receives lists of candidate word strings per slot (from BART/T5/RAG); candidates are trimmed/substrings to required slot lengths and converted to character equality disjunctions to form the SMT problem.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not applicable (symbolic solver). The system is fed candidate lists (top-k) and the grid specification; solver attempts to find a satisfying character assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>This solver performs explicit symbolic spatial reasoning by enforcing per-cell equality and length constraints; the paper discusses that the solver often returns 'nosat' if candidate lists miss the gold answers. The authors attempted approximate/relaxation strategies (MAX-SAT, removing clues) but found them computationally intractable or producing garbage under-constrained solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported as an isolated metric; overall puzzle-level results combine candidate generation quality with solver success (see Table 3). The solver required oracle prefiltering to produce partial solutions in these experiments because otherwise most puzzles were nosat.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Cannot gracefully produce partial solutions without oracle filtering; naive relaxation (remove k clues or MAX-SAT approximations) is computationally expensive (O(2^n) in worst case) or produces under-constrained garbage. Does not natively use candidate confidence scores/weights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Contrasted with probabilistic/weighted CSP approaches used historically (Proverb, Dr. Fill) which rely on large historical clue-answer databases; here the solver is used with model-generated candidates rather than database lookups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3422.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sudoku - ML attempts (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine-learning approaches to solving Sudoku (cited prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior ML work on Sudoku, noting that while classical algorithms solve Sudoku efficiently, ML approaches (convolutional networks, recurrent relational networks, reinforcement learning) have been attempted to learn solving strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent relational networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Convolutional models; Recurrent Relational Networks (RRN); Reinforcement Learning agents (Mehta 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cited approaches include convolutional networks and recurrent relational networks for learning Sudoku solving, and more recent reinforcement-learning-based agents applied to constraint-satisfaction games including Sudoku (Mehta, 2021). These are not language models and were not run in this paper; they are mentioned as related work on constraint puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A 9×9 (typical) grid-based number-placement puzzle requiring spatial/relational constraints: each digit 1–9 must appear exactly once per row, column, and 3×3 block, i.e., a pure constraint-satisfaction, grid-structured spatial reasoning task.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Typically represented as a numeric grid (partial assignment); ML approaches operate directly on grid encodings (e.g., image or matrix of digits/zeros), not as natural-language clues.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Not applicable here (these are supervised/RL/graph-based network methods in cited work). The present paper only mentions them in related work and does not evaluate them.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>The cited works focus on learning iterative/relational reasoning over grid neighbors (RRN) or learning policies via RL; this paper only references them to contrast with crosswords which combine natural-language knowledge with grid constraints and variable grid shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No performance numbers from those Sudoku papers are reported in this crossword paper; readers are referred to the original cited works (Mehta 2021; Palm et al. 2017; Simonis 2005) for metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>This paper notes that many Sudoku instances can be solved efficiently with deterministic CSP algorithms and that ML approaches are not required for typical Sudoku; also stressed that crosswords are different because they require open-domain language/world knowledge combined with grid constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Used to contrast puzzle types: Sudoku (fixed grid+constraints) vs crosswords (arbitrary shape, natural-language knowledge). The paper does not compare performance figures between Sudoku ML approaches and the crossword experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3422.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3422.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simonis 2005 (Sudoku as CSP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sudoku as a constraint problem (Simonis, 2005)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited classic reference that formulates Sudoku as a constraint satisfaction problem and discusses efficient deterministic solving methods; used here to contrast with crossword structure and ML approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sudoku as a constraint problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A CSP perspective and modeling of Sudoku (symbolic algorithmic solving), cited to contextualize constraint formulations for puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>See Sudoku entry.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Grid-based CSP representation (variables are cells with domains 1–9 and constraints enforcing row/column/box uniqueness).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Simonis demonstrates that Sudoku is naturally encoded as CSP and shows efficient deterministic solving methods leveraging fixed structure; cited by the paper to note that Sudoku differs from crosswords because Sudoku has fixed size/shape and constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not discussed in detail in this paper; Simonis' work documents algorithmic solutions for Sudoku.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Used in the paper to contrast with crosswords (which require language/world knowledge) rather than to compare empirical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Down and Across: Introducing Crossword-Solving as a New NLP Benchmark', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reinforcement learning for constraint satisfaction game agents (15-puzzle, minesweeper, 2048, and sudoku) <em>(Rating: 2)</em></li>
                <li>Recurrent relational networks <em>(Rating: 2)</em></li>
                <li>Sudoku as a constraint problem <em>(Rating: 2)</em></li>
                <li>Dr. fill: Crosswords and an implemented solver for singly weighted csps <em>(Rating: 2)</em></li>
                <li>A probabilistic approach to solving crossword puzzles <em>(Rating: 2)</em></li>
                <li>Webcrow: A web-based system for crossword solving <em>(Rating: 1)</em></li>
                <li>Learning to rank answer candidates for automatic resolution of crossword puzzles <em>(Rating: 1)</em></li>
                <li>Distributional neural networks for automatic resolution of crossword puzzles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3422",
    "paper_id": "paper-33986e5964f025a4f8343322149ef66cf194b5da",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "BART-large",
            "name_full": "BART-large",
            "brief_description": "A denoising sequence-to-sequence Transformer pretrained with a text infilling and corruption objective and fine-tuned here to generate answers to crossword clues as text candidates for a downstream constraint solver.",
            "citation_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "mention_or_use": "use",
            "model_name": "BART-large",
            "model_description": "Sequence-to-sequence Transformer (encoder-decoder) pretrained with a denoising objective; stores world knowledge in parameters and is fine-tuned on the NYT clue-answer pairs to generate answer strings for clues.",
            "model_size": "≈406M",
            "puzzle_name": "New York Times Crossword (NYT Crossword)",
            "puzzle_description": "Standard crossword puzzles (mostly 15×15, some 21×21) where answers to natural-language clues must be placed into grid slots of specified lengths with character intersections enforcing equality at overlapping cells (a grid-based spatial/constraint puzzle).",
            "input_representation": "Clues presented as text (single-line clue strings). For puzzle solving the model generates top-k textual answer candidates per clue; these text candidates are post-processed (substring extraction to required slot length) and fed to an SMT-based crossword solver which enforces grid constraints.",
            "prompting_method": "Fine-tuning on the clue-answer dataset (supervised seq-to-seq). Top-k decoding (k up to 20) used to supply candidate lists for each clue.",
            "spatial_reasoning_analysis": "BART is used only to generate textual answer candidates and does not directly represent or reason over the grid; spatial reasoning (length and intersection constraints) is handled separately by a Z3-based SMT solver. The paper highlights architectural limitations for end-to-end neural spatial reasoning: Transformer decoders produce BPE/token outputs (not character-level), complicating direct character-wise grid assignment and intersection consistency.",
            "performance_metrics": "Clue-answer task (top-k): Top-1 Exact Match EM = 13.8%, EM_norm = 16.1%; Top-10 EM = 31.0%; Top-20 EM = 34.0% (see Table 2). Crossword puzzle task (using top-20 candidates fed to SMT with oracle prefilter): Word accuracy (Acc_word) = 16.6%, Character accuracy (Acc_char) = 28.4%; Word removal (Rem_word) = 55.6%, Character removal (Rem_char) = 43.4% (see Table 3).",
            "limitations_or_failure_modes": "Generative mistakes and missing the ground-truth in candidate lists cause many puzzles to be unsolvable by the SMT solver (nosat). BPE/token-level output complicates character-wise grid filling. Performs poorly on historical clues and some wordplay categories; needs downstream solver to enforce spatial constraints.",
            "comparison_to_other_models_or_humans": "Compared against T5-base (worse) and retrieval-augmented RAG models (substantially worse than RAG). No human performance numbers reported in this paper.",
            "uuid": "e3422.0",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "T5-base",
            "name_full": "T5-base (Text-to-Text Transfer Transformer)",
            "brief_description": "A unified text-to-text Transformer model fine-tuned here to generate answers to crossword clues; used as a baseline generative model for the clue-answer task.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5-base",
            "model_description": "Encoder-decoder Transformer (text-to-text) pretrained with a mixture of denoising objectives and fine-tuned on the clue-answer dataset to produce textual candidate answers for clues.",
            "model_size": "≈220M",
            "puzzle_name": "New York Times Crossword (NYT Crossword)",
            "puzzle_description": "See BART entry; crossword puzzles require placement of answer strings into grid slots with character intersections (spatial constraints).",
            "input_representation": "Clues as text; model generates top-k textual answer predictions which are then considered as candidate substrings for slot lengths and passed to the SMT solver for grid solving (T5 itself does not enforce grid constraints).",
            "prompting_method": "Fine-tuning on the clue-answer dataset (supervised seq-to-seq). Top-k decoding.",
            "spatial_reasoning_analysis": "T5 is used as a clue-answer generator only and does not model grid spatial constraints. The paper reports the general limitation that seq-to-seq models are poorly suited to character-level constrained outputs required for grids.",
            "performance_metrics": "Clue-answer task (top-k): Top-1 EM = 8.4%, EM_norm = 9.5%; Top-10 EM = 18.7%; Top-20 EM = 22.2% (see Table 2). No puzzle-level (SMT) metrics reported for T5 in Table 3.",
            "limitations_or_failure_modes": "Lower accuracy than BART and RAG; candidate lists less likely to contain the ground-truth leading to solver failures. Same character/BPE output limitations as other seq-to-seq models.",
            "comparison_to_other_models_or_humans": "Underperforms BART-large and retrieval-augmented RAG models on this dataset.",
            "uuid": "e3422.1",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "RAG-wiki",
            "name_full": "Retrieval-Augmented Generation (RAG) with Wikipedia",
            "brief_description": "An end-to-end retrieval-augmented Transformer that retrieves relevant Wikipedia passages (21M passages from a Dec 2018 dump) to condition generation; used here to generate stronger clue-answer candidates.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "use",
            "model_name": "RAG (wiki)",
            "model_description": "RAG combines a Transformer encoder-decoder generator with a learned retriever that returns supporting passages (here: Wikipedia 2018 chunks) via MIPS; model trained end-to-end to generate more factually accurate answers for open-domain QA-style clues.",
            "model_size": null,
            "puzzle_name": "New York Times Crossword (NYT Crossword)",
            "puzzle_description": "See BART entry; crosswords require grid-consistent character assignment across intersecting slots.",
            "input_representation": "Clues as text; retriever returns Wikipedia passages relevant to the clue; the generator produces top-k textual answer candidates; these candidates are substring-extracted to slot lengths and passed to the SMT solver which enforces grid constraints.",
            "prompting_method": "Fine-tuning with retrieval-augmented generation (RAG) using pretrained retriever embeddings (primed on Natural Questions); top-k decoding to get candidate lists.",
            "spatial_reasoning_analysis": "RAG itself does not perform grid/spatial reasoning — it provides higher-quality candidate answers which increase the chance that the downstream SMT solver can satisfy grid constraints. The paper reports qualitative analysis showing RAG better matches grammatical forms and benefits from document retrieval for abbreviations/prefixes.",
            "performance_metrics": "Clue-answer task (top-k): Top-1 EM = 24.2%, EM_norm = 26.0%, Top-10 EM = 46.8%, Top-20 EM = 50.6%, In_norm (Top-20) = 56.7% (Table 2). Crossword puzzle task (with oracle prefilter & top-20 candidates): Acc_word = 23.8%, Acc_char = 37.8%, Rem_word = 40.3%, Rem_char = 26.3% (Table 3).",
            "limitations_or_failure_modes": "Although RAG substantially improves candidate quality, many puzzles remain unsolvable because some clues' ground-truth answers are absent from candidate lists; the SMT solver often requires oracle prefiltering to avoid nosat. End-to-end character-level grid assignment still unmet because RAG generates token-level outputs.",
            "comparison_to_other_models_or_humans": "RAG-wiki outperforms BART-large and T5-base on both clue-answer and puzzle-level measures; RAG-wiki and RAG-dict agree on ground-truth matches for ~85% of test set. No direct human baseline provided.",
            "uuid": "e3422.2",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "RAG-dict",
            "name_full": "Retrieval-Augmented Generation (RAG) with dictionaries/thesauri",
            "brief_description": "RAG variant that retrieves entries from English dictionaries and thesauri (Wiktionary, Merriam-Webster, Oxford-based Google dictionary) to condition generation for crossword clues that often rely on meanings and synonyms.",
            "citation_title": "Retrieval-augmented generation for knowledgeintensive nlp tasks",
            "mention_or_use": "use",
            "model_name": "RAG (dict)",
            "model_description": "Same RAG architecture but retrieval corpus consists of dictionary/thesaurus entries to better support word-meaning, synonym/antonym, and abbreviation clues.",
            "model_size": null,
            "puzzle_name": "New York Times Crossword (NYT Crossword)",
            "puzzle_description": "See BART entry.",
            "input_representation": "Clues as text; retriever returns dictionary entries which the generator uses to produce top-k candidates, then fed to SMT solver for grid solving.",
            "prompting_method": "Fine-tuning with retrieval-augmented generation using a dictionary/thesaurus index; top-k decoding.",
            "spatial_reasoning_analysis": "As with RAG-wiki, RAG-dict provides better candidate generation for semantic clue types but does not internally solve grid spatial constraints; those are handled by the external SMT solver.",
            "performance_metrics": "Clue-answer task (top-k): Top-1 EM = 24.0%, EM_norm = 25.8%, Top-10 EM = 46.0%, Top-20 EM = 50.0% (Table 2). Crossword puzzle task (with oracle prefilter & top-20 candidates): Acc_word = 22.1%, Acc_char = 35.9%, Rem_word = 40.8%, Rem_char = 26.8% (Table 3).",
            "limitations_or_failure_modes": "Similar to RAG-wiki; some clue classes still challenging (historical, dependent), and solver requires the correct answer to appear among candidates or the puzzle must be relaxed/oracle-filtered to converge.",
            "comparison_to_other_models_or_humans": "Performs similar to RAG-wiki overall; RAG-dict can outperform RAG-wiki on clue types tied closely to dictionary definitions (as shown in qualitative analysis).",
            "uuid": "e3422.3",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Z3-based SMT crossword solver",
            "name_full": "Modified Z3 SMT-based crossword constraint solver",
            "brief_description": "A Satisfiability Modulo Theories (SMT) formulation (implemented on top of Z3) that models crossword grids as character variables with length and intersection constraints and chooses a consistent assignment from candidate answer lists.",
            "citation_title": "Z3: An efficient smt solver.",
            "mention_or_use": "use",
            "model_name": "Z3 SMT solver (modified implementation)",
            "model_description": "Formulates each cell as a variable (character) and each slot as the disjunction over candidate word assignments; enforces equality at intersections and slot-length constraints; uses Z3 SMT engine to search for satisfying assignments.",
            "model_size": null,
            "puzzle_name": "New York Times Crossword (NYT Crossword)",
            "puzzle_description": "Grid-based word puzzle where spatial constraints are equalities at intersecting characters and fixed slot lengths; solver handles these constraints formally.",
            "input_representation": "Receives lists of candidate word strings per slot (from BART/T5/RAG); candidates are trimmed/substrings to required slot lengths and converted to character equality disjunctions to form the SMT problem.",
            "prompting_method": "Not applicable (symbolic solver). The system is fed candidate lists (top-k) and the grid specification; solver attempts to find a satisfying character assignment.",
            "spatial_reasoning_analysis": "This solver performs explicit symbolic spatial reasoning by enforcing per-cell equality and length constraints; the paper discusses that the solver often returns 'nosat' if candidate lists miss the gold answers. The authors attempted approximate/relaxation strategies (MAX-SAT, removing clues) but found them computationally intractable or producing garbage under-constrained solutions.",
            "performance_metrics": "Not reported as an isolated metric; overall puzzle-level results combine candidate generation quality with solver success (see Table 3). The solver required oracle prefiltering to produce partial solutions in these experiments because otherwise most puzzles were nosat.",
            "limitations_or_failure_modes": "Cannot gracefully produce partial solutions without oracle filtering; naive relaxation (remove k clues or MAX-SAT approximations) is computationally expensive (O(2^n) in worst case) or produces under-constrained garbage. Does not natively use candidate confidence scores/weights.",
            "comparison_to_other_models_or_humans": "Contrasted with probabilistic/weighted CSP approaches used historically (Proverb, Dr. Fill) which rely on large historical clue-answer databases; here the solver is used with model-generated candidates rather than database lookups.",
            "uuid": "e3422.4",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Sudoku - ML attempts (cited)",
            "name_full": "Machine-learning approaches to solving Sudoku (cited prior work)",
            "brief_description": "The paper cites prior ML work on Sudoku, noting that while classical algorithms solve Sudoku efficiently, ML approaches (convolutional networks, recurrent relational networks, reinforcement learning) have been attempted to learn solving strategies.",
            "citation_title": "Recurrent relational networks",
            "mention_or_use": "mention",
            "model_name": "Convolutional models; Recurrent Relational Networks (RRN); Reinforcement Learning agents (Mehta 2021)",
            "model_description": "Cited approaches include convolutional networks and recurrent relational networks for learning Sudoku solving, and more recent reinforcement-learning-based agents applied to constraint-satisfaction games including Sudoku (Mehta, 2021). These are not language models and were not run in this paper; they are mentioned as related work on constraint puzzles.",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_description": "A 9×9 (typical) grid-based number-placement puzzle requiring spatial/relational constraints: each digit 1–9 must appear exactly once per row, column, and 3×3 block, i.e., a pure constraint-satisfaction, grid-structured spatial reasoning task.",
            "input_representation": "Typically represented as a numeric grid (partial assignment); ML approaches operate directly on grid encodings (e.g., image or matrix of digits/zeros), not as natural-language clues.",
            "prompting_method": "Not applicable here (these are supervised/RL/graph-based network methods in cited work). The present paper only mentions them in related work and does not evaluate them.",
            "spatial_reasoning_analysis": "The cited works focus on learning iterative/relational reasoning over grid neighbors (RRN) or learning policies via RL; this paper only references them to contrast with crosswords which combine natural-language knowledge with grid constraints and variable grid shapes.",
            "performance_metrics": "No performance numbers from those Sudoku papers are reported in this crossword paper; readers are referred to the original cited works (Mehta 2021; Palm et al. 2017; Simonis 2005) for metrics.",
            "limitations_or_failure_modes": "This paper notes that many Sudoku instances can be solved efficiently with deterministic CSP algorithms and that ML approaches are not required for typical Sudoku; also stressed that crosswords are different because they require open-domain language/world knowledge combined with grid constraints.",
            "comparison_to_other_models_or_humans": "Used to contrast puzzle types: Sudoku (fixed grid+constraints) vs crosswords (arbitrary shape, natural-language knowledge). The paper does not compare performance figures between Sudoku ML approaches and the crossword experiments.",
            "uuid": "e3422.5",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Simonis 2005 (Sudoku as CSP)",
            "name_full": "Sudoku as a constraint problem (Simonis, 2005)",
            "brief_description": "Cited classic reference that formulates Sudoku as a constraint satisfaction problem and discusses efficient deterministic solving methods; used here to contrast with crossword structure and ML approaches.",
            "citation_title": "Sudoku as a constraint problem",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "A CSP perspective and modeling of Sudoku (symbolic algorithmic solving), cited to contextualize constraint formulations for puzzles.",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_description": "See Sudoku entry.",
            "input_representation": "Grid-based CSP representation (variables are cells with domains 1–9 and constraints enforcing row/column/box uniqueness).",
            "prompting_method": null,
            "spatial_reasoning_analysis": "Simonis demonstrates that Sudoku is naturally encoded as CSP and shows efficient deterministic solving methods leveraging fixed structure; cited by the paper to note that Sudoku differs from crosswords because Sudoku has fixed size/shape and constraints.",
            "performance_metrics": null,
            "limitations_or_failure_modes": "Not discussed in detail in this paper; Simonis' work documents algorithmic solutions for Sudoku.",
            "comparison_to_other_models_or_humans": "Used in the paper to contrast with crosswords (which require language/world knowledge) rather than to compare empirical performance.",
            "uuid": "e3422.6",
            "source_info": {
                "paper_title": "Down and Across: Introducing Crossword-Solving as a New NLP Benchmark",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reinforcement learning for constraint satisfaction game agents (15-puzzle, minesweeper, 2048, and sudoku)",
            "rating": 2
        },
        {
            "paper_title": "Recurrent relational networks",
            "rating": 2
        },
        {
            "paper_title": "Sudoku as a constraint problem",
            "rating": 2
        },
        {
            "paper_title": "Dr. fill: Crosswords and an implemented solver for singly weighted csps",
            "rating": 2
        },
        {
            "paper_title": "A probabilistic approach to solving crossword puzzles",
            "rating": 2
        },
        {
            "paper_title": "Webcrow: A web-based system for crossword solving",
            "rating": 1
        },
        {
            "paper_title": "Learning to rank answer candidates for automatic resolution of crossword puzzles",
            "rating": 1
        },
        {
            "paper_title": "Distributional neural networks for automatic resolution of crossword puzzles",
            "rating": 1
        }
    ],
    "cost": 0.0178765,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Down and Across: Introducing Crossword-Solving as a New NLP Benchmark</h1>
<p>Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, and Anna Rumshisky<br>Department of Computer Science<br>University of Massachusetts Lowell<br>{skul, okovalev, nshivagu, arum}@cs.uml.edu</p>
<h4>Abstract</h4>
<p>Solving crossword puzzles requires diverse reasoning capabilities, access to a vast amount of knowledge about language and the world, and the ability to satisfy the constraints imposed by the structure of the puzzle. In this work, we introduce solving crossword puzzles as a new natural language understanding task. We release a corpus of crossword puzzles collected from the New York Times daily crossword spanning 25 years and comprised of a total of around nine thousand puzzles. These puzzles include a diverse set of clues: historic, factual, word meaning, synonyms/antonyms, fill-in-the-blank, abbreviations, prefixes/suffixes, wordplay, and crosslingual, as well as clues that depend on the answers to other clues. We separately release the clue-answer pairs from these puzzles as an open-domain question answering dataset containing over half a million unique clueanswer pairs. For the question answering task, our baselines include several sequence-tosequence and retrieval-based generative models. We also introduce a non-parametric constraint satisfaction baseline for solving the entire crossword puzzle. Finally, we propose an evaluation framework which consists of several complementary performance metrics.</p>
<h2>1 Introduction</h2>
<p>Recent breakthroughs in NLP established high standards for the performance of machine learning methods across a variety of tasks. However, even state-of-the-art models demonstrate fragility (Wallace et al., 2019) and exhibit sensitivity to shallow data patterns (McCoy et al., 2019; Zellers et al., 2019; Jin et al., 2020; Si et al., 2019; Sugawara et al., 2020; Yogatama et al., 2019; Niven and Kao, 2019). This has led to a growing demand for successively more challenging tasks.</p>
<p>One of the important tasks in natural language understanding is question answering (QA), with many recent datasets created to address different
different aspects of this task (Yang et al., 2018; Rajpurkar et al., 2016; Kwiatkowski et al., 2019a; Zellers et al., 2019; Dua et al., 2019; Rogers et al., 2021). There are two main forms of question answering (QA): extractive QA and open-domain QA. In extractive QA, a passage that answers the question is provided as input to the system along with the question. In open-domain QA, only the question is provided as input, and the answer must be generated either through memorized knowledge or via some form of explicit information retrieval over a large text collection which may contain answers.</p>
<p>The task of answering clues in a crossword is a form of open-domain question answering. Once a human or an open-domain QA system generates a few possible answer candidates for each clue, one of these candidates may form the correct answer to a word slot in the crossword grid, if the candidate meets the constraints of the crossword grid.</p>
<p>Solving a crossword puzzle is therefore a challenging task which requires (1) finding answers to a variety of clues that require extensive language and world knowledge, and (2) the ability to produce answer strings that meet the constraints of the crossword grid, including length of word slots and character overlap with other answers in the puzzle.</p>
<p>Our contributions in this work are as follows:</p>
<ul>
<li>We introduce a new natural language understanding task of solving crossword puzzles, along with a dataset of New York Times crosswords from Dec. 1, 1993 to Dec. 31, 2018.</li>
<li>We propose an evaluation framework which consists of several complementary performance metrics.</li>
<li>We release the collection of clue-answer pairs as a new open-domain QA dataset.</li>
<li>We provide baselines for the proposed crossword task and the new QA task, including several sequence-to-sequence and retrievalaugmented generative Transformer models, with a constraint satisfaction crossword solver.</li>
</ul>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Crossword puzzle example. A few clues from the puzzle have been provided on the right, they are filled horizontally (Across) or vertically (Down) in the crossword grid. The clue number tells the player where in the grid the answer needs to be filled in. Some of these clue and their answers have further been highlighted with different colors which belong to different clue categories as described in Section 3.2, color-coded in accordance with Figure 2. Highlight colors denote distinct clue categories: red for word meaning clues, purple for fill-inthe blank clue, orange for synonym/antonym, blue for factoid question type, grey for abbreviation and brown for historical. Source: New York Times daily crossword which appeared on the July 7, 2009. Copyright of The New York Times, 2009.</p>
<h2>2 Related Work</h2>
<p>Our work is in line with open-domain QA benchmarks. Examples of such tasks include datasets where each question can be answered using information contained in a relevant Wikipedia article (Yang et al., 2015; Kwiatkowski et al., 2019a; Yang et al., 2018). Several QA tasks have been designed to require multi-hop reasoning over structured knowledge bases (Berant et al., 2013; Bordes et al., 2015). The main limitation of such datasets is that their question types are mostly factual. Crossword clues differ from these efforts in that they combine a variety of different reasoning types.</p>
<p>Another line of research that is relevant to our work explores the problem of solving Sudoku puzzles since it is also a constraint satisfaction problem. Most sudoku puzzles can be efficiently solved by algorithms that take advantage of the fixed input size and do not rely on machine learning methods (Simonis, 2005). The machine learning attempts for solving Sudoku puzzles have been inspired by convolutional (Mehta, 2021) and recurrent relational networks (Palm et al., 2017). Unlike Sudoku, however, where the grids have the same structure, shape and constraints, crossword puzzles have arbitrary shape and internal structure and rely on answers to natural language questions that require reasoning over different kinds of world knowledge.</p>
<p>Several previous studies have treated crossword puzzle solving as a constraint satisfaction problem (CSP) (Littman et al., 2002; Ernandes et al., 2005; Ginsberg, 2011). Littman et al. (2002)'s Proverb system incorporates a variety of information retrieval modules to generate candidate answers. The Database module searches a large database of historical clue-answer pairs to retrieve the answer candidates. They find very poor crossword-solving performance in ablation experiments where they limit their answer candidate generator modules to not use historical clue-answer databases. WebCrow (Ernandes et al., 2005) builds upon Proverb and makes improvements to the database retriever module augmented with a new web module which searches the web for snippets that may contain answers. It allows partial matching to retrieve clues-answer pairs in the historical database that do not perfectly overlap with the query clue. Dr. Fill system proposed by Ginsberg (2011) treats each crossword puzzle as a singly-weighted CSP. Similarly to prior work, Dr. Fill relies on a large set of historical clue-answer pairs (up to 5M) collected over multiple years from the past puzzles by applying direct lookup and a variety of heuristics. One common design aspect of all these solvers is to generate answer candidates independently from the crossword structure and later use a separate puzzle solver to fill in the actual grid. In our work, we partition the task of</p>
<p>crossword solving similarly.
Barlacchi et al. (2014) and Severyn et al. (2015) observe that the most important source of candidate answers for a given clue is a large database of historical clue-answer pairs and introduce methods to better search these databases. Barlacchi et al. (2014) apply a BM25 retrieval model to generate clue lists similar to the query clue from historical clue-answer database, where the generated clues get further refined through application of reranking models. Severyn et al. (2015) introduce a distributional neural network to compute similarities between clues trained over a large scale dataset of clues that they introduce.</p>
<p>In contrast to the previous work, our goal in this work is to motivate solver systems to generate answers organically, just like a human might, rather than obtain answers via the lookup in historical clue-answer databases. The answers could be generated either from memory of having read something relevant, using world knowledge and language understanding, or by searching encyclopedic sources such as Wikipedia or a dictionary with relevant queries.</p>
<h2>3 Task and Dataset</h2>
<p>For the purposes of our task, crosswords are defined as word puzzles with a given rectangular grid of white- and black-shaded squares. The goal is to fill the white squares with letters, forming words or phrases by solving textual clues which lead to the answers. The answer words and phrases are placed in the grid from left to right ("Across") and from top to bottom ("Down"). The shaded squares are used to separate the words or phrases. Usually, the white spaces and punctuation are removed from the answer phrases. A sample crossword puzzle is given in Figure 1. Note that the answers can include named entities and abbreviations, and at times require the exact grammatical form, such as the correct verb tense or the plural noun.</p>
<p>Solving a crossword puzzle is a complex task that requires generating the right answer candidates and selecting those that satisfy the puzzle constraints. Similar to prior work, we divide the task of solving a crossword puzzle into two subtasks, to be evaluated separately. The first subtask can be viewed as a question answering task, where a system is trained to generate a set of candidate answers for a given clue without taking into account any interdependencies between answers. The sec-
ond subtask involves solving the entire crossword puzzle, i.e., filling out the crossword grid with a subset of candidate answers generated in the previous step.</p>
<p>The two tasks could be solved separately or in an end-to-end fashion. In contrast to prior work (Ernandes et al., 2005; Ginsberg, 2011), our clueanswer data is linked directly with our puzzlesolving data, so no data leakage is possible between the QA training data and the crossword-solving test data. In the present work, we propose a separate solver for each task. We provide details on the challenges of implementing an end-to-end solver in the discussion section.</p>
<h3>3.1 NYT Crossword Collection</h3>
<p>Our dataset is sourced from the New York Times, which has been featuring a daily crossword puzzle since 1942. We worked with daily puzzles in the date range from December 1, 1993 through December 31, 2018 inclusive. All the crossword puzzles in our corpus are available to play through the New York Times games website ${ }^{1}$. We release two separate specifications of the dataset corresponding to the subtasks described above: the NYT Crossword Puzzle dataset and the NYT Clue-Answer dataset. ${ }^{2}$</p>
<p>There are a few details that are specific to the NYT daily crossword. First, the clue and the answer must agree in tense, part of speech, and even language, so that the clue and answer could easily be substituted for each other in a sentence. Second, abbreviated clues indicate abbreviated answers. Further, clues that end in a question mark indicate a play on words in the clue or the answer. There are also a lot of short words that appear in crosswords much more often than in real life. These 3- and 4 -letter words, referred to as crosswordese, can be very helpful in solving the puzzles. Finally, every Sunday through Thursday NYT crossword puzzle has a theme, something that unites the puzzle's longest answers. Theme answers are always found in symmetrical places in the grid.</p>
<p>Crossword Puzzle Dataset. The dataset consists of 9152 puzzles, split into the training, validation, and test subsets in the 80/10/10 ratio which give us 7293/922/941 puzzles in each set. We removed the total of 50/61 special puzzles from the validation</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and test splits, respectively, because they used nonstandard rules for filling in the answers, such as L-shaped word slots or allowing cells to be filled with multiple characters (called rebus entries).</p>
<p>Most NYT crossword grids have a square shape of $15 \times 15$ cells, with the exception of Sundayreleased crosswords being $21 \times 21$ cells. Other shapes combined account for less than $3 \%$ of the data. The vast majority of both clues and answers are short, with over $76 \%$ of clues consisting of a single word. For traditional sequence-to-sequence modeling such conciseness imposes an additional challenge, as there is very little context provided to the model. In most puzzles, over $80 \%$ of the grid cells are filled and every character is an intersection of two answers. Such high answer interdependency suggests a high cost of answer misprediction, as errors affect a larger number of intersecting words. More detailed statistics on the dataset are given in Table 1.</p>
<p>Clue-Answer Dataset. We generate an opendomain question answering dataset consisting solely of clue-answer pairs from the respective splits of the Crossword Puzzle dataset described above (including the special puzzles). Within each of the splits, we only keep unique clue-answer pairs and remove all duplicates. However, certain clues may still be shared between the puzzles contained in different splits. We therefore remove from the training data the clue-answer pairs which are found in the test or validation data. This ensures that the model can not trivially recall the answers to the overlapping clues while predicting for the test and validation splits.</p>
<p>This produces the total of 578 k clue-answer pairs, with $433 \mathrm{k} / 72 \mathrm{k} / 72 \mathrm{k}$ examples in the train/validation/test splits, respectively. Since certain answers consist of phrases and multiple words that are merged into a single string (such as "VERYFAST"), we further postprocess the answers by splitting the strings into individual words using a dictionary. Out of all the possible word splits of a given string we pick the one that has the smallest number of words. If there are multiple solutions, we select the split with the highest average word frequency. Examples of a variety of clues found in this dataset are given in the following section.</p>
<h3>3.2 Clue types</h3>
<p>To provide more insight into the diversity of the clue types and the complexity of the task, we cate-
gorize all the clues into multiple classes, which we describe below.</p>
<p>Factual. Clues that encode encyclopedic knowledge and typically can be answered using resources such as Wikipedia (e.g. Clue: South Carolina State tree, Answer: PALMETTO). This type of clue is the closest to the questions found in open-domain QA datasets. Note that the facts required to solve some of the clues implicitly depend on the date when a given crossword was released. For instance, the clue "President of Brazil" has a time-dependent answer.</p>
<p>Historical. Clues that require the knowledge of historical facts and temporal relations between events. (e.g. Clue: Automobile pioneer, Answer: BENZ).</p>
<p>Word meaning. Clues that exploit general vocabulary knowledge and can typically be resolved using a dictionary. (e.g. Clue: Opposing sides, Answer: FOES).</p>
<p>Synonyms/Antonyms. Clues that focus on paraphrasing and synonymy relations (e.g. Clue: Prognosticators, Answer: SEERS). In most cases, such clues can be solved with a thesaurus.</p>
<p>Fill in the blank. Clues formulated as a cloze task (e.g. Clue: Magna Cum $\qquad$ Answer: LAUDE). Fill-in-the-blank clues are expected to be easy to solve for the models trained with the masked language modeling objective (Devlin et al., 2019).</p>
<p>Abbreviations. Clues answered with acronyms (e.g. Clue: (Abbr.) Old Communist state, Answer: USSR). Abbreviation clues are marked with "Abbr." label.</p>
<p>Prefix/Suffix. Clues that suggest the answer is a suffix or prefix. (e.g. Clue: Suffix with mountain, Answer: EER)</p>
<p>Wordplay. Clues that rely on wordplay, anagrams, or puns / pronunciation similarities (e.g. Clue: Consider an imaginary animal, Answer: BEAR IN MIND). In a lot of cases, wordplay clues involve jokes and exploit different possible meanings and contexts for the same word.</p>
<p>Cross-lingual. Clues that either explicitly use words from other languages, or imply a specific language-dependent form of the answer. (e.g. Clue: Sunrise dirección, Answer: ESTE).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Class distribution of the 1000 manually annotated test examples.</p>
<p>Clues dependent on other clues. Clues the answer to which can be provided only after a different clue has been solved (e.g. Clue: Last words of 45 Across). Although rare, this category of clues suggests that the entire puzzle has to be solved in certain order.</p>
<p>To understand the distribution of these classes, we randomly selected 1000 examples from the test split of the data and manually annotated them. Figure 2 illustrates the class distribution of the annotated examples, showing that the Factual class covers a little over a third of all examples. The synonyms/antonyms, word meaning and wordplay classes taken together comprise $50 \%$ of the data. The remaining $20 \%$ are taken by fill-in-the-blank and historical clues, as well as the low-frequency classes (comprising less than or around 1\%), which include abbreviation, dependent, prefix/suffix and cross-lingual clues. We illustrate each one of these classes in the Figure 1.</p>
<h3>3.3 Evaluation metrics</h3>
<p>In this section, we describe the performance metrics we introduce for the two subtasks.</p>
<p>Clue-Answer Task. For the clue-answer task, we use the following metrics:</p>
<ul>
<li>Exact Match (EM). Model output matches the ground-truth answer exactly.</li>
<li>Contains (In). Model output contains the ground-truth answer as a contiguous substring Since the ground-truth answers do not contain diacritics, accents, punctuation and whitespace characters, we also consider normalized versions of the</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;">Validation</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Clue-Answer dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># clues</td>
<td style="text-align: center;">4,33,033</td>
<td style="text-align: center;">72,303</td>
<td style="text-align: center;">72,939</td>
</tr>
<tr>
<td style="text-align: center;">avg/median clue length (words)</td>
<td style="text-align: center;">$4.0 / 3$</td>
<td style="text-align: center;">$4.2 / 4$</td>
<td style="text-align: center;">$4.2 / 4$</td>
</tr>
<tr>
<td style="text-align: center;">avg/median ans. length (chars)</td>
<td style="text-align: center;">$5.5 / 5$</td>
<td style="text-align: center;">$5.7 / 5$</td>
<td style="text-align: center;">$5.6 / 5$</td>
</tr>
<tr>
<td style="text-align: center;">avg/median ans. length (words)</td>
<td style="text-align: center;">$1.3 / 1$</td>
<td style="text-align: center;">$1.3 / 1$</td>
<td style="text-align: center;">$1.3 / 1$</td>
</tr>
<tr>
<td style="text-align: center;">Crossword Puzzle dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"># puzzles avg/median # of clues</td>
<td style="text-align: center;">7,293</td>
<td style="text-align: center;">872</td>
<td style="text-align: center;">879</td>
</tr>
<tr>
<td style="text-align: center;">avg cols $\times$ rows</td>
<td style="text-align: center;">$83.5 / 76$</td>
<td style="text-align: center;">$83.6 / 76$</td>
<td style="text-align: center;">$82.9 / 76$</td>
</tr>
<tr>
<td style="text-align: center;">\% of cells filled</td>
<td style="text-align: center;">$15.9 \times 15.9$</td>
<td style="text-align: center;">$15.9 \times 15.9$</td>
<td style="text-align: center;">$15.8 \times 15.8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$82.20 \%$</td>
<td style="text-align: center;">$80.20 \%$</td>
<td style="text-align: center;">$81.20 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: The full statistics on the two versions of the released datasets.
above metrics, in which these are stripped from the model output prior to computing the metric. We will refer to them as $\mathbf{E M}<em _norm="{norm" _text="\text">{\text {norm }}$ and $\mathbf{I n}</em>$.}</p>
<p>We report these metrics for top- $k$ predictions, where $k$ varies from 1 to 20 .</p>
<p>Crossword Puzzle Task. To evaluate the performance of the crossword puzzle solver, we propose to compute the following two metrics:</p>
<ul>
<li>Character Accuracy (Acc ${ }_{\text {char }}$ ). Percentage of characters in the predicted crossword solution that match the ground-truth solution.</li>
<li>Word Accuracy (Acc $_{\text {word }}$ ). Percentage of words in the predicted crossword solution that match the ground-truth solution.
Since the clue-answering system might not be able to generate the right answers for some of the</li>
</ul>
<p>clues, it may only be possible to produce a partial solution to a puzzle. The crossword puzzle solver will fail to produce a solution when the answer candidate list for a clue does not contain the correct answer. To prevent this from happening, the character cells which belong to that clue's answer must be removed from the puzzle grid, unless the characters are shared by other clues. We propose two additional metrics to track what percentage of the puzzle needs to be redacted to produce a partial solution:</p>
<ul>
<li>Word Removal $\left(\mathbf{R e m}_{\text {word }}\right) . \%$ of words that need to be removed from the puzzle to produce a partial solution.</li>
<li>Character Removal $\left(\mathbf{R e m}_{\text {word }}\right) . \%$ of characters that need to be removed from the puzzle grid to produce a partial solution.
The motivation for introducing the removal metrics is to indicate the amount of constraint relaxation. For instance, a completely relaxed puzzle grid, where many character cells have been removed, such that the grid has no word intersection constraints left, could be considered "solved" by selecting any candidates from the answer candidate lists at random. However, this solution will mostly be incorrect when compared to the gold puzzle solution. As the word and character removal percentage increases, the potential for correctly solving the remaining puzzle is expected to decrease, since the under-constrained answer cells in the grid can be incorrectly filled by other candidates (which may not be the right answers). The removal metrics are thus complementary to word and character level accuracy.</li>
</ul>
<h2>4 Baselines</h2>
<p>Our baseline approach is a two-step solution that treats each subtask separately. We first develop a set of baseline systems that solve the question answering problem, ignoring the grid-imposed answer interdependencies. We use seq-to-seq and retrieval-augmented Transformer baselines for this subtask. We feed generated answer candidates to a crossword solver in order to complete the puzzle and evaluate the produced puzzle solutions.</p>
<h3>4.1 Clue-Answer Task Baselines</h3>
<p>Sequence-to-sequence baselines. We fine-tune two sequence-to-sequence models on the clueanswer training data. We select two widely known models, BART (Lewis et al., 2019) and T5 (Raffel
et al., 2019), which achieved state-of-the-art results on a set of generative tasks, including specifically abstractive QA involving commonsense and multihop reasoning (Fan et al., 2019; Khashabi et al., 2018; Zhang et al., 2018).</p>
<p>We train both models for 8 epochs with the learning rate of $5 \times 10^{-5}$, and a batch size of $60 .{ }^{3}$</p>
<p>Retrieval-augmented generation. T5 and BART store world knowledge implicitly in their parameters and are known to hallucinate facts (Maynez et al., 2020). Recently, a new method called retrieval-augmented generation (RAG) (Lewis et al., 2020) has been introduced for opendomain question answering. This method involves a Transformer encoder to encode the question and a decoder to generate the answer (Vaswani et al., 2017), but the encoded query is supplemented with relevant excerpts retrieved from an external textual corpus via Maximum Inner Product Search (MIPS); the entire neural network is trained end-to-end. Due to a built-in retrieval mechanism for performing a soft search over a large collection of external documents, such systems are capable of producing stronger results on knowledge-intensive open-domain question answering tasks than the vanilla sequence-to-sequence generative models and are more factually accurate (Shuster et al., 2021). Motivated by this, we train RAG models to extract knowledge from two separate external sources of knowledge:
(a) RAG-wiki uses a full Wikipedia dump from December 2018. Following existing work Lewis et al. (2020); Karpukhin et al. (2020); Lee et al. (2019), each Wikipedia article is split into disjoint 100-word chunks, resulting in a total of 21 M passages.
(b) RAG-dict uses several English dictionaries and thesauri sources, including Wiktionary ${ }^{4}$, Merriam-Webster ${ }^{5}$, and Google's English dictionary by Oxford Languages. ${ }^{6}$
For both of these models, we use the retriever embeddings pretrained on the Natural Questions corpus Kwiatkowski et al. (2019b) in order to prime the MIPS retrieval to return meaningful entries (Lewis et al., 2020). We train with a batch size</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Top-1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top-10</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Top-20</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">$\mathbf{E M}_{\text {norm }}$</td>
<td style="text-align: center;">$\mathbf{I n}$</td>
<td style="text-align: center;">$\mathbf{I n}_{\text {norm }}$</td>
<td style="text-align: center;">$\mathbf{E M}$</td>
<td style="text-align: center;">$\mathbf{E M}_{\text {norm }}$</td>
<td style="text-align: center;">$\mathbf{I n}$</td>
<td style="text-align: center;">$\mathbf{I n}_{\text {norm }}$</td>
<td style="text-align: center;">$\mathbf{E M}$</td>
<td style="text-align: center;">$\mathbf{E M}_{\text {norm }}$</td>
<td style="text-align: center;">$\mathbf{I n}$</td>
<td style="text-align: center;">$\mathbf{I n}_{\text {norm }}$</td>
</tr>
<tr>
<td style="text-align: left;">T5-base</td>
<td style="text-align: center;">8.4</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">19.8</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: left;">BART-large</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">RAG wiki</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: left;">RAG dict</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">56.2</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance of baseline systems on the Clue Answering dataset. EM and In stand for the "Exact-match" and "Contains" metrics as described in Section 3.3. The computed metrics are shown for top-1, top-10, and top-20 predictions for a given model.
of 8 , label smoothing set to 0.1 , dropout probability of 0.1 , weight decay rate of 0.001 , and a learning rate of $3 \times 10^{-5}$ for 8 epochs.</p>
<h3>4.2 Crossword Puzzle Task</h3>
<p>A crossword puzzle can be cast as an instance of a satisfiability problem, and its solution represents a particular character assignment so that all the constraints of the puzzle are met. Under such formulation, three main conditions have to be satisfied: (1) the answer candidates for every clue must come from a set of words that answer the question, (2) they must have the exact length specified by the corresponding grid entry, and (3) for every pair of words that intersect in the puzzle grid, acceptable word assignments must have the same character at the intersection offset.</p>
<p>This class of problems can be modelled through Satisfiability Modulo Theories (SMT). SMT is a generalization of Boolean Satisfiability problem (SAT) in which some of the binary variables are replaced by first-order logic predicates over a set of non-binary variables. In the case of crosswords, a variable represents one character in the crossword grid which can be assigned a single letter of the English alphabet and 0 through 9 digit values. This is further subject to the constraints mentioned above which can be formulated with the equality operator and Boolean logical operators: AND and OR. For example, a word slot of length 3 where the candidate answers are "ESC", "DEL" or "CMD" can be formalised as:</p>
<p>$$
\begin{gathered}
\left{v_{1}=E \mathrm{AND} v_{2}=S \mathrm{AND} v_{3}=C\right} \
O R \
\left{v_{1}=D \mathrm{AND} v_{2}=E \mathrm{AND} v_{3}=L\right} \
O R \
\left{v_{1}=C \mathrm{AND} v_{2}=M \mathrm{AND} v_{3}=D\right}
\end{gathered}
$$</p>
<p>To solve the entire crossword puzzle, we use the formulation that treats this as an SMT problem. We
modify an open source implementation ${ }^{7}$ of this formulation based on Z3 SMT solver (de Moura and Bjørner, 2008). The answer length and intersection constraints are imposed on the variable assignment, as specified by the input crossword grid.</p>
<p>We take the top- $k$ predictions from our baseline models and for each prediction, select all possible substrings of required length as answer candidates. For simplicity, we exclude from our consideration all the crosswords with a single cell containing more than one English letter in it.</p>
<p>Our current baseline constraint satisfaction solver is limited in that it simply returns "notsatisfied" (nosat) for a puzzle where no valid solution exists, that is, when all the hard constraints of the puzzle are not met by the inputs. Since the candidate lists for certain clues might not meet all the constraints, this results in a nosat solution for almost all crossword puzzles, and we are not able to extract partial solutions. To bypass this issue and produce partial solutions, we pre-filter each clue with an oracle that only allows those clues into the SMT solver for which the actual answer is available as one of the candidates.</p>
<h2>5 Results</h2>
<h3>5.1 Clue-Answer Task</h3>
<p>In Table 2 we report the Top-1, Top-10 and Top-20 match accuracies for the four evaluation metrics defined in Section 3.3.</p>
<p>Our results (Table 2) suggest a high difficulty of the clue-answer dataset, with the best achieved accuracy metric staying under $30 \%$ for the top-1 model prediction. Even top-20 predictions have an almost $40 \%$ chance of not containing the groundtruth answer anywhere within the generated strings. Generative Transformer models such as T5-base and BART-large perform poorly on the clue-answer task, however, the model accuracy across most</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Solving Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Puzzle Removed</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Acc $_{\text {word }}$</td>
<td style="text-align: center;">Acc $_{\text {char }}$</td>
<td style="text-align: center;">Rem $_{\text {word }}$</td>
<td style="text-align: center;">Rem $_{\text {char }}$</td>
</tr>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">43.4</td>
</tr>
<tr>
<td style="text-align: left;">RAG wiki</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: left;">RAG dict</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">26.8</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance of baseline systems on the Crossword Puzzle dataset. We report the exact-match metric for top-20 predictions of the baseline models listed.
metrics almost doubles when switching from T5base (with 220M parameters) to BART-large (with 400M parameter).</p>
<p>Our strongest baseline, RAG-wiki and RAG-dict, achieve 50.6 and 50.0 exact-match accuracies on the clue-answer dataset, respectively. The $I n_{\text {norm }}$ score, which looks at whether any substrings in the generated answer match the ground truth - and which can be seen an upper bound on the model's ability to solve the puzzle - is slightly higher, at 56.7 for RAG-wiki and 56.2 for RAG-dict.</p>
<p>Not surprisingly, these results show that the additional step of retrieving Wikipedia or dictionary entries increases the accuracy considerably compared to the fine-tuned sequence-to-sequence models such as BART which store this information in its parameters. The normalized metrics which remove diacritics, punctuation and whitespace bring the accuracy up by $2-6 \%$, depending on the model.</p>
<p>We examined the top-20 exact-match predictions generated by RAG-wiki and RAG-dict and find that both models are in agreement in terms of answer matches for around $85 \%$ of the test set. In other words, both models either correctly predict the ground truth answer or both fail to do so.</p>
<h3>5.2 Crossword Puzzle Task</h3>
<p>The baseline performance on the entire crossword puzzle dataset shows there is significant room for improvement of the existing architectures (see Table 3). Our best model, RAG-wiki, correctly fills in the answers for only $26 \%$ (on average) of the total number of puzzle clues, despite having a much higher performance on the clue-answer task, i.e. measured independently from the crossword grid (Table 2). This is explained by the fact that the clues with no ground-truth answer present among the candidates have to be removed from the puzzles in order for the solver to converge, which in turn relaxes the interdependency constraints too much, so that a filled answer may be selected from the set of candidates almost at random. Despite that, the
baseline solver is able to solve over a quarter of each the puzzle on average.</p>
<h2>6 Qualitative analysis</h2>
<p>Evaluation on the annotated subset of the data reveals that some clue types present significantly higher levels of difficulty than others (see Table 4). In particular, all of our baseline systems struggle with the clues requiring reasoning in the context of historical knowledge. As expected, all of the models demonstrate much stronger performance on the factual and word-meaning clue types, since the relevant answer candidates are likely to be found in the Wikipedia data used for pre-training. We observe the biggest differences between BART and RAG performance for the "abbreviation" and the "prefixsuffix" categories. The document retrieval step in RAG allows for more efficient matching of supporting documents, leading to generation of more relevant answer candidates. For instance, the clue "Warehouse abbr." results in "pkg" and "bldg" candidates among RAG predictions, whereas BART generates abstract and largely irrelevant strings.</p>
<p>Our manual inspection of model predictions suggest that both BART and RAG correctly infer the grammatical form of the answer from the formulation of the clue. For example, the clue "Stitched" produces the candidate answers "Sewn" and "Made", and the clue "Word repeated after "Que" " triggers mostly Spanish and French generations (e.g. "Avec" or "Sera").</p>
<p>As previously stated RAG-wiki and RAG-dict largely agree with each other with respect to the ground truth answers. We qualitatively assessed instances where either RAG-wiki or RAG-dict predict the answer correctly in Appendix A.</p>
<h2>7 Discussion and Future Work</h2>
<p>The presented task is challenging to approach in an end-to-end model fashion. There are several reasons for this, which we discuss below.</p>
<p>Character-level outputs. Commonly used Transformer decoders do not produce characterlevel outputs and produce BPE and wordpieces instead, which creates a problem for a potential end-to-end neural crossword solver. One possible solution can be the modification of the loss term, designed with character-based output logits instead of BPE since the crossword grid constraints are at a single cell- (i.e. character-) level. There is</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Fact.</th>
<th style="text-align: left;">Hist.</th>
<th style="text-align: left;">Meaning</th>
<th style="text-align: left;">Syn./Ant.</th>
<th style="text-align: left;">Blank</th>
<th style="text-align: left;">Abbr.</th>
<th style="text-align: left;">Pref./Suf.</th>
<th style="text-align: left;">Wordplay</th>
<th style="text-align: left;">X-lingual</th>
<th style="text-align: left;">Dependent</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART</td>
<td style="text-align: left;">40.4</td>
<td style="text-align: left;">19.0</td>
<td style="text-align: left;">43.9</td>
<td style="text-align: left;">40.3</td>
<td style="text-align: left;">36.0</td>
<td style="text-align: left;">42.9</td>
<td style="text-align: left;">20.0</td>
<td style="text-align: left;">33.5</td>
<td style="text-align: left;">40.0</td>
<td style="text-align: left;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">RAG-wiki</td>
<td style="text-align: left;">53.9</td>
<td style="text-align: left;">28.6</td>
<td style="text-align: left;">55.3</td>
<td style="text-align: left;">46.6</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">43.9</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">11.8</td>
</tr>
<tr>
<td style="text-align: left;">RAG-dict</td>
<td style="text-align: left;">54.2</td>
<td style="text-align: left;">35.7</td>
<td style="text-align: left;">52.8</td>
<td style="text-align: left;">48.9</td>
<td style="text-align: left;">61.3</td>
<td style="text-align: left;">85.7</td>
<td style="text-align: left;">60.0</td>
<td style="text-align: left;">46.3</td>
<td style="text-align: left;">40.0</td>
<td style="text-align: left;">11.8</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of models across clue types in the exact match, top-20 setting. Evaluation performed on a 1000 clue subset of the test set which were manually annotated across clue categories.
some work done in the character-level output transformer encoders such as Ma et al. (2020). However, to our best knowledge there is no major generative Transformer architecture which supports character-level outputs yet, we intend to explore this avenue further in future work to develop an end-to-end neural crossword solver.</p>
<p>SMT solver constraints. As mentioned earlier, our current baseline solver does not allow partial solutions, and we rely on pre-filtering using the oracle from the ground-truth answers. Although this strategy is flawed for the obvious use of the oracle, the alternatives are currently either computationally intractable or too lossy. One such strategy is to remove $k$ clues at a time, starting with $k=1$ and progressively increasing the number of clues removed until the remaining relaxed puzzle can be solved - which has the complexity of $\mathrm{O}\left(2^{n}\right)$, where $n$ is the total number of clues in the puzzle. Another approach we tried was to relax certain constraints of the puzzle grid, maximally satisfying as many constraints as possible, which is formally known as the maximal satisfaction problem (MAX-SAT). This is a NP-hard problem for which it is hard to find approximate solutions (Papadimitriou, 1994).</p>
<p>Our initial foray into such approximate solvers (Previti and Marques-Silva, 2013; Liffiton and Malik, 2013) produced severely under-constrained puzzles with garbage character entries. Further work needs to be done to extend this solver to handle partial solutions elegantly without the need for an oracle, this could be addressed with probabilistic and weighted constraint satisfaction solvers, in line with the work by Littman et al. (2002); Keim et al. (1999) and Ginsberg (2011), but without the dependency on the past crossword clues.</p>
<h2>8 Conclusion</h2>
<p>We present a new challenging task of solving crossword puzzles and present the New York Times Crosswords Dataset, which can be approached at a QA-like level of individual clue-answer pairs, or at the level of an entire puzzle, with imposed an-
swer interdependency constraints. This new benchmark contains a broad range of clue types that require diverse reasoning components. We carry out a set of baseline experiments that indicate the overall difficulty of this task for the current systems, including retrieval-augmented SOTA models for open-domain question answering. We also discuss the technical challenges in building a crossword solver and obtaining partial solutions as well as in the design of end-to-end systems for this task. We hope that the NYT Crosswords task would define a new high bar for the AI systems.</p>
<h2>9 Ethical Considerations</h2>
<p>The New York Times daily crossword puzzles are a copyright of the New York Times. We have obtained preliminary approval from the New York Times to release this data under a non-commercial and research use license, and are in the process of finalizing the exact licensing terms and distribution channels with the NYT legal department.</p>
<h2>10 Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers for their careful and insightful review of our manuscript and their feedback. We would like to thank Parth Parikh for the permission to modify and reuse parts of their crossword solver ${ }^{7}$. We are grateful to New York Times staff for their support of this project. This project is funded in part by an NSF CAREER award to Anna Rumshisky (IIS1652742).</p>
<h2>References</h2>
<p>Gianni Barlacchi, Massimo Nicosia, and Alessandro Moschitti. 2014. Learning to rank answer candidates for automatic resolution of crossword puzzles. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 39-48, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from</p>
<p>question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. arXiv preprint arXiv:1506.02075.</p>
<p>Leonardo de Moura and Nikolaj Bjørner. 2008. Z3: An efficient smt solver. In Tools and Algorithms for the Construction and Analysis of Systems, pages 337340, Berlin, Heidelberg. Springer Berlin Heidelberg.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Dheeru Dua, Ananth Gottumukkala, Alon Talmor, Sameer Singh, and Matt Gardner. 2019. Orb: An open reading benchmark for comprehensive evaluation of machine reading comprehension. In EMNLP 2019 MRQA Workshop, page 147.</p>
<p>Marco Ernandes, Giovanni Angelini, and Marco Gori. 2005. Webcrow: A web-based system for crossword solving. In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3, AAAI'05, page 1412-1417. AAAI Press.</p>
<p>Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558-3567, Florence, Italy. Association for Computational Linguistics.</p>
<p>Matthew L Ginsberg. 2011. Dr. fill: Crosswords and an implemented solver for singly weighted csps. Journal of Artificial Intelligence Research, 42:851-886.</p>
<p>Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 8018-8025.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics.</p>
<p>Greg A. Keim, Noam M. Shazeer, Michael L. Littman, Sushant Agarwal, Catherine M. Cheves, Joseph Fitzgerald, Jason Grosland, Fan Jiang, Shannon Pollard, and Karl Weinmeister. 1999. Proverb: The
probabilistic cruciverbalist. In Proceedings of the Sixteenth National Conference on Artificial Intelligence and the Eleventh Innovative Applications of Artificial Intelligence Conference Innovative Applications of Artificial Intelligence, AAAI '99/IAAI '99, page 710-717, USA. American Association for Artificial Intelligence.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019a. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019b. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics.</p>
<p>Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086-6096, Florence, Italy. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc.</p>
<p>Mark H Liffiton and Ammar Malik. 2013. Enumerating infeasibility: Finding multiple muses quickly. In International Conference on Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pages 160-175. Springer.</p>
<p>Michael L. Littman, Greg A. Keim, and Noam Shazeer. 2002. A probabilistic approach to solving crossword puzzles. Artificial Intelligence, 134(1):23-55.</p>
<p>Wentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin Wang, and Guoping Hu. 2020. CharBERT: Character-aware pre-trained language model. In Proceedings of the 28th International Conference on Computational Linguistics, pages 39-50, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Tom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 34283448, Florence, Italy. Association for Computational Linguistics.</p>
<p>Anav Mehta. 2021. Reinforcement learning for constraint satisfaction game agents (15-puzzle, minesweeper, 2048, and sudoku). arXiv preprint arXiv:2102.06019.</p>
<p>Timothy Niven and Hung-Yu Kao. 2019. Probing neural network comprehension of natural language arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4658-4664.</p>
<p>Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. 2017. Recurrent relational networks. arXiv preprint arXiv:1711.08028.</p>
<p>Christos H. Papadimitriou. 1994. Computational complexity. Addison-Wesley.</p>
<p>Alessandro Previti and Joao Marques-Silva. 2013. Partial mus enumeration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 27.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Anna Rogers, Matt Gardner, and Isabelle Augenstein. 2021. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. CoRR, abs/2107.12708.</p>
<p>Aliaksei Severyn, Massimo Nicosia, Gianni Barlacchi, and Alessandro Moschitti. 2015. Distributional neural networks for automatic resolution of crossword puzzles. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 199-204, Beijing, China. Association for Computational Linguistics.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. CoRR, abs/2104.07567.</p>
<p>Chenglei Si, Shuohang Wang, Min-Yen Kan, and Jing Jiang. 2019. What does BERT learn from multiplechoice reading comprehension datasets? arXiv preprint arXiv:1910.12391.</p>
<p>Helmut Simonis. 2005. Sudoku as a constraint problem. In CP Workshop on modeling and reformulating Constraint Satisfaction Problems, volume 12, pages 13-27. Citeseer.</p>
<p>Saku Sugawara, Pontus Stenetorp, Kentaro Inui, and Akiko Aizawa. 2020. Assessing the benchmarking capacity of machine reading comprehension datasets. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8918-8927.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing nlp. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162.</p>
<p>Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain question answering. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 2013-2018.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. 2019. Learning and evaluating general linguistic intelligence. arXiv preprint arXiv:1901.11373.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.</p>
<h1>A Qualitative Analysis of RAG-wiki and RAG-dict Predictions</h1>
<p>We examined top-20 exact-match predictions generated by RAG-wiki and RAG-dict. With some exceptions, both models predict similar results (in terms of answer matches) for around $85 \%$ of the test set.</p>
<p>Table 5 shows examples where RAG-dict failed to generate the correct predictions but RAG-wiki succeeded, and vice-versa. Most of the instances where RAG-dict predicted correctly and RAG-wiki did not are the ones where answer is closely related to the meaning of the clue. The instances where only RAG-wiki predicted correctly are where answer is not a direct meaning of the clue, and some more information is required predict.</p>
<p>Table 5: Examples where either RAG-dict or RAG-wiki predicts correctly and other fails.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">RAG-dict predicts correctly <br> RAG-wiki fails</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">RAG-wiki predicts correctly <br> RAG-dict fails</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Clue</td>
<td style="text-align: left;">Answer</td>
<td style="text-align: left;">Clue</td>
<td style="text-align: left;">Answer</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Asian nursemaid <br> Pill alternative, for short</td>
<td style="text-align: left;">amah <br> iud</td>
<td style="text-align: left;">Quisling's city <br> Avatar of Vishnu</td>
<td style="text-align: left;">oslo <br> rama</td>
</tr>
<tr>
<td style="text-align: left;">Word Meaning</td>
<td style="text-align: left;">Pause indicator <br> Moves along quickly</td>
<td style="text-align: left;">comma <br> scoots</td>
<td style="text-align: left;">Sites for grand entrances <br> Point of no return?</td>
<td style="text-align: left;">archways <br> ace</td>
</tr>
<tr>
<td style="text-align: left;">Word Play</td>
<td style="text-align: left;">Kind of contribution <br> Without ice</td>
<td style="text-align: left;">ira <br> neat</td>
<td style="text-align: left;">I'm impressed! <br> Airport no no</td>
<td style="text-align: left;">ooh <br> knife</td>
</tr>
<tr>
<td style="text-align: left;">Synonyms Antonyms</td>
<td style="text-align: left;">Stitched <br> Promptly</td>
<td style="text-align: left;">sewn <br> on time</td>
<td style="text-align: left;">guess</td>
<td style="text-align: left;">idea</td>
</tr>
<tr>
<td style="text-align: left;">Fill in the Blanks</td>
<td style="text-align: left;">__rug <br> canola __</td>
<td style="text-align: left;">area <br> oil</td>
<td style="text-align: left;">__Israeli relations</td>
<td style="text-align: left;">arab</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://github.com/pncnmnp/
Crossword-Solver&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
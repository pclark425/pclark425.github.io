<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8819 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8819</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8819</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-278905425</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.18475v2.pdf" target="_blank">A Survey of Large Language Models for Data Challenges in Graphs</a></p>
                <p><strong>Paper Abstract:</strong> Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process. In this survey, we focus on four fundamental data-centric challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8819.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8819.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphText</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphText (graph-to-text textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structure-to-text conversion that serializes graph syntax trees into natural language sequences, incorporating node attributes at leaves to preserve hierarchical structure for LLM consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphtext: Graph reasoning in text space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-syntax-tree textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct a graph-syntax tree to preserve hierarchical relationships, traverse the tree (tree traversal serialization) and emit natural-language descriptions for nodes and edges; leaf nodes include node attribute text so that the resulting sequence describes both topology and attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs (general graphs, citation networks, small knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Build graph-syntax tree, traverse (e.g., pre-order) to generate a linear text sequence describing nodes, neighbors, and attributes; attributes are embedded in the leaf descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, graph reasoning, graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported task metrics in the survey include Accuracy, Macro-F1/Micro-F1 for node classification and BLEU/ROUGE for graph-to-text; no single numeric result reported in the survey for GraphText.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to simple linearizations (node/edge lists) and path-based textualizations; GraphText aims to better retain hierarchical structure versus naive list serializations but the survey notes method choice substantially affects LLM performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves hierarchical structure and node attributes together, better contextualizes local neighborhoods for LLM reasoning, reduces some structural information loss compared to flat lists.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Can generate long sequences (context window pressure), ordering choices may introduce permutation artifacts, still loses explicit graph algebraic structure (e.g., adjacency matrices) that specialized GNNs exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large or very dense graphs produce sequences exceeding LLM context limits; performance degrades when traversal order induces misleading neighbor priorities or when complex global topology (cycles, motifs) is not well captured by tree approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8819.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WalkLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WalkLM (attributed random-walk textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A path-based textualization approach that generates attributed random walks and converts each walk into natural language sequences for fine-tuning LMs to produce graph-aware embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walklm: A uniform language model fine-tuning framework for attributed graph embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>attributed random-walk textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate multiple random walks (node sequences) that capture local topology and include textualized node attributes for each visited node, then serialize each walk as a short natural-language sentence/sequence for LLM input or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>attributed graphs (citation networks, biomedical, logs), applicable to both homogeneous and heterogeneous graphs when attributes are textualized</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Perform attributed random walks (standard random-walk sampling), for each walk concatenate node textual attributes and simple connective phrases to produce a walk description; aggregate many walk-texts as training corpus for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, representation learning, retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites use of Macro-F1, Micro-F1, AUC, MRR in related evaluations but does not provide numeric WalkLM-specific scores.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Path-based (walk) textualizations are compared favorably to naive global list serializations for capturing local context; however they may underperform tree-based or shortest-path schemes when multi-hop semantics require specific path selection.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Compact local-context descriptions, scalable via sampling, well-suited to capture local neighborhood statistics, easier to keep within LLM context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Randomness may miss important structural motifs; multiple sampled walks required to cover topology leading to large corpora; global structure and long-range dependencies can be missed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Dense graphs where walks are noisy and redundant; tasks requiring global structural features (e.g., specific motif counts, spectral properties) where walk sequences lack sufficient signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8819.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Path-LLM (shortest-path-based textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A shortest-path based conversion where selected shortest paths between node pairs are serialized into textual sequences to convey informative multi-hop relations to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Path-llm: A shortest-pathbased llm learning for unified graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>shortest-path sequence textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Select informative shortest paths (or constrained path sets) connecting relevant node pairs; convert each path into a human-readable sequence describing the nodes, edge relations and attributes along the path.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs, knowledge graphs (path-centric reasoning), attributed graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute (or sample) shortest paths between nodes of interest, format each path as a textual chain with node/edge attribute descriptions and relation labels (for KGs), and feed these textualized paths to LLMs as context or prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>link prediction, relation/edge prediction, node classification, path-based reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey lists metrics used broadly (MRR, Hits@N, Accuracy) for path-based KGC/link tasks but provides no path-LLM-specific numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Argued in survey to better capture meaningful multi-hop semantics than random-walks; compared qualitatively with WalkLM and full-serialization approaches—shortest-paths reduce noise and focus on semantically relevant chains.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Focuses LLM attention on semantically meaningful connectivity, reduces noisy sampling, better for multi-hop reasoning tasks (e.g., KGC, QA over graphs).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires graph shortest-path computation (costly on large graphs), choice of which paths to include is critical, may still omit alternative plausible multi-hop connections.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Graphs with many equally short paths (ambiguity), or where important non-shortest motif/context is required; very large graphs where enumerating paths is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8819.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructGLM (neighbor-scale instruction-tuned textualization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that describes graph structures by listing neighbors at multiple scales and uses instruction tuning of an LLM to better perform graph tasks from these textualized neighborhood descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>InstructGLM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>multi-scale neighbor textualization + instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a target node, concatenate textual descriptions of its neighbors organized by hop-distance (1-hop, 2-hop, ...) into an instruction-style prompt; finetune or instruction-tune an LLM to respond to graph queries using this structured prompt format.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>attributed graphs, citation networks, knowledge graphs (when neighbor attributes/text available)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract neighborhood lists per hop, convert each neighbor and its attributes into short text snippets, assemble into an instruction template (e.g., 'Given node X and its neighbors: ... answer:') for LLM training or inference.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, graph QA, explanation generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey reports use of Accuracy and Link Prediction metrics in works that use neighbor textualization; no single numeric scores included for InstructGLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Instruction-style neighbor descriptions can outperform naive flat serializations because the LLM is explicitly trained to interpret neighbor-scale structure; compared to pure path-based methods, multi-scale neighbors provide broader local context.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly encodes locality and multi-hop context in human-readable form, compatible with instruction-tuned LLM paradigms, can improve LLM's ability to reason about local topology.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Prompt length grows with hop count and neighbor degree; may still suffer from permutation sensitivity and lose explicit graph topology beyond neighbor lists.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High-degree nodes produce very long prompts; heterogeneous neighborhood types may confuse LLM without careful template design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8819.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuseGraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuseGraph (neighbors+paths textualization + instruction tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-oriented instruction tuning strategy that textualizes both neighbors and paths for LLM fine-tuning to support generic graph mining and question answering tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>neighbors-and-paths textualization with instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Combine textualized neighbor lists and path-based descriptions (shortest or sampled) into structured instruction prompts used to instruction-tune LLMs for graph reasoning, generation, and classification.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general attributed graphs, knowledge graphs, dynamic graphs (with temporal annotations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Assemble hybrid context: (1) local neighbor sentences, (2) selected path descriptions, (3) attribute summaries; wrap into instruction-style examples for LLM fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, graph-to-text generation, temporal and multi-hop graph QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey lists evaluation metrics across cited works (Accuracy, MRR, BLEU, ROUGE) but does not give MuseGraph-specific numeric outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Hybrid neighbor+path textualization presented as more comprehensive than single-strategy approaches (walks or pure neighbor lists), offering improved multi-hop reasoning while retaining local context.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Balances local and path-based signals, suitable for instruction-based LLM training, yields richer contexts for multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Higher prompt/sequence lengths, more complex design choices (which paths/neighbors to include), more expensive to construct training data.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Context-window limits constrain how much hybrid information can be included; noisy or contradictory paths/neighbors can mislead LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8819.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-LLM (text-attributed graph textualization for LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that converts text-attributed graphs into natural language descriptions so LLMs can perform zero-shot or few-shot graph tasks directly from textified graph inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphllm: Boosting graph reasoning ability of large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text-attributed graph natural-language serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize nodes and their textual attributes into natural language sentences, include adjacency or neighbor statements as text (e.g., 'Node A links to Node B'), and feed these sequences to LLMs for zero/few-shot inference.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>text-attributed graphs, citation networks, recommendation graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Linearize nodes with their text attributes and include explicit edge statements or neighbor enumerations; optionally include small subgraph contexts as textual blocks.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>zero-shot/few-shot node classification, link prediction, graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey references standard metrics (Accuracy, MRR, Hits@N) when used for KGC and node classification; no single numeric values reported in the survey for Graph-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared at high level with subgraph-aware and hybrid token alignment methods; pure text-serialization is simpler but risks topological information loss versus hybrid token/GNN-alignment approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Straightforward to implement, leverages LLM pretrained linguistic knowledge, enables zero-/few-shot capabilities on text-rich graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Topological information can be diluted by textualization; leads to long prompt sequences; LLMs are not permutation invariant so ordering choices affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Sparse non-textual attributes (graphs without meaningful textual labels) limit efficacy; graphs with complex multi-relational topology where short textual descriptions lose critical structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8819.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITA (Graph to visual-and-textual integration for VLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal structure-to-text-and-image representation: graph structures are converted to both textual descriptions and visualizations (images) so vision-language models (VLMs) or multimodal LLMs can reason about topology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph to visual and textual integration for vision-language graph reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>text + visualization multimodal textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create parallel modalities: (1) textual descriptions of nodes/edges/attributes and (2) visual renderings/graphics of the graph or subgraphs; feed both modalities into a vision-language model for joint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>general graphs, scene graphs, knowledge graphs where visualizable structure helps (e.g., layouts, molecular diagrams)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize nodes/edges into text and produce curated images (graph drawings or path diagrams) representing topology; combine these as multimodal input to a VLM or multimodal LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>graph reasoning, multimodal QA over graphs, section summarization where structure benefits from visual cues</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey cites BLEU, ROUGE, CIDEr for multimodal tasks and Accuracy for node/link tasks but does not provide numeric GITA-specific results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>GITA extends pure-text approaches by adding visual topology cues; survey notes multimodal inputs can improve LLM understanding of structure compared to text-only serializations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Visual modality helps LLMs interpret topology that is cumbersome to express in linear text; can reduce ambiguity from ordering effects in textual serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires image generation/rendering pipeline and VLM-capable models; increases computational and engineering complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Graphs that are too large to render compactly or visualizations that hide relational semantics (graph drawings can be ambiguous); VLMs may still miss fine-grained graph semantics (e.g., relation types).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8819.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM4Hypergraph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM4Hypergraph (structure languages for hypergraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of low-order and high-order 'structure languages' to convert hypergraph structures into natural language descriptions so LLMs can reason over hypergraph relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>hypergraph structure language textualization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Design specialized natural-language templates to describe hyperedges (higher-order relations), convert hypergraph connectivity patterns into sentences capturing membership and types, and feed resulting text to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>hypergraphs (e.g., group interactions, multi-party relations), higher-order graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>For each hyperedge, generate descriptive text listing participating nodes and the hyperedge type or role; include summaries of overlapping hyperedges to capture high-order structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>hypergraph reasoning, sociological analysis, classification on hypergraph nodes/edges</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey lists general metrics for related tasks (Accuracy, Macro-F1) but no numeric results for LLM4Hypergraph in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Hypergraph textualization is contrasted with pairwise graph textualizations; the survey suggests special languages are needed to retain higher-order semantics that pairwise serializations would lose.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly encodes high-order relations in text, enabling LLMs to access multi-member relation semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Text grows quickly with hyperedge size; potential loss of combinatorial structure when complex overlaps exist; no standardized templates—design choices impact results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Very high-order hyperedges with many participants produce overly long or ambiguous text; LLM hallucination when hyperedge semantics are complex or underspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8819.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structure-to-text (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-to-text transformation methods (generic category)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods converting graph topology into textual formats (node/edge lists, sequences, paths, trees, formal graph languages) to feed LLMs for graph tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>generic structure-to-text (linearization/serialization/path/tree/formal language)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Includes node/edge lists, node-indexed neighbor lists, random-walk/path serializations, graph-syntax trees, and formal markup (GraphML-like) rendered as text; purpose is to make topology consumable by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies broadly to homogeneous/heterogeneous graphs, knowledge graphs, molecular graphs, hypergraphs (with adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Various: adjacency lists (linearized), random-walk generation + textualization, shortest-path extraction + textualization, tree construction + traversal, or formal markup serialization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, knowledge graph completion, graph QA, graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey summarizes that works report Accuracy, Macro-F1, MRR, Hits@N, BLEU, ROUGE, but no single canonical numeric performance for the generic class.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Survey highlights that choice among these structure-to-text approaches matters: path-based and tree-based methods often outperform naive flat serializations; hybrid or GNN-aligned tokens retain more structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables use of powerful pretrained LLMs for graph tasks without training specialized GNNs; flexible and interpretable representations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Information loss (global algebraic properties), long sequences for large graphs, ordering sensitivity (LLMs not permutation invariant), conversion design is task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Large-scale graphs exceed context windows; structural properties like spectral features are not preserved; hallucination risk if LLM infers unsupported edges from textual priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8819.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid graph-token alignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid methods aligning GNN graph tokens with LLM token space (GNP / GraphToken / GraphGPT / LLaGA / TEA-GLM / HiGPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that encode structural representations via GNNs or specialized tokenizers, then project those embeddings into the LLM token embedding space so LLMs can consume compact graph tokens alongside text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-token projection / alignment</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use a GNN (or structure encoder) to produce compact subgraph/node embeddings; apply a projector to map embeddings into the discrete token embedding space of an LLM or into token-like continuous inputs; process via LLM with aligned tokens or adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>heterogeneous graphs, large graphs, knowledge graphs, scenarios requiring scalability beyond raw serializations</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encode subgraphs or nodes with GNNs; apply linear/nonlinear projector or contrastive alignment to LLM token principal components; optionally instruction-tune LLM on tasks using projected tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node/graph classification, link prediction, instruction following for graph tasks, few-shot KGC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Survey notes these hybrid methods aim to reduce context size and improve performance; typical reported metrics in cited works are Accuracy, MRR, Hits@N, but survey does not give numeric values here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Hybrid token alignment reduces sequence length vs pure textualization and preserves structural fidelity better than text-only methods; compared favorably in the survey to naive serializations though engineering complexity is higher.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Compact, scalable (smaller input size), better preservation of explicit structural signals, supports larger subgraphs within LLM input, facilitates tighter LLM-GNN fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires training/alignment steps (projector, contrastive objectives), more engineering (GNN + projector + LLM adapters), potential mismatch between token projection and LLM internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Poor projector alignment can lead to degraded LLM performance; may still lose fine-grained relational semantics (e.g., edge types) if tokenization is too coarse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8819.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8819.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNP / GraphToken (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GNP / GraphToken (examples of graph-token mapping methods)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representative hybrid designs that encode graph structure with GNNs and map the resulting embeddings into token-like vectors compatible with LLMs to enable joint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN-to-token projection (GraphToken)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use a GNN encoder to get structure-aware embeddings for nodes/subgraphs, then use a learned projector to convert embeddings into the LLM token embedding space enabling concatenation with textual tokens for LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>heterogeneous graphs, large-scale graphs, knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>GNN encode -> embedding projector -> inject as 'graph tokens' into LLM input stream or cross-attention adapters.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>node classification, link prediction, knowledge graph completion, instruction-following on graphs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No specific numeric metrics provided in the survey summary; works report Accuracy, MRR, Hits@N on standard benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Presented as a middle ground between text-serialization (simpler) and full GNN-only pipelines (structurally faithful); tends to preserve more topology while shrinking context compared to pure text.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>More compact and structure-preserving than long textual serializations, reduces context window pressure, enables joint LLM-GNN training.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Needs alignment training; may require labeled data or self-supervised objectives to align spaces successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If alignment objective is weak or datasets are mismatched, projected tokens may be uninterpretable by the LLM and performance drops.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey of Large Language Models for Data Challenges in Graphs', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graphtext: Graph reasoning in text space <em>(Rating: 2)</em></li>
                <li>Walklm: A uniform language model fine-tuning framework for attributed graph embedding <em>(Rating: 2)</em></li>
                <li>Path-llm: A shortest-pathbased llm learning for unified graph representation <em>(Rating: 2)</em></li>
                <li>Graph to visual and textual integration for vision-language graph reasoning <em>(Rating: 2)</em></li>
                <li>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining <em>(Rating: 2)</em></li>
                <li>Graphllm: Boosting graph reasoning ability of large language model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8819",
    "paper_id": "paper-278905425",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "GraphText",
            "name_full": "GraphText (graph-to-text textualization)",
            "brief_description": "A structure-to-text conversion that serializes graph syntax trees into natural language sequences, incorporating node attributes at leaves to preserve hierarchical structure for LLM consumption.",
            "citation_title": "Graphtext: Graph reasoning in text space",
            "mention_or_use": "mention",
            "representation_name": "graph-syntax-tree textualization",
            "representation_description": "Construct a graph-syntax tree to preserve hierarchical relationships, traverse the tree (tree traversal serialization) and emit natural-language descriptions for nodes and edges; leaf nodes include node attribute text so that the resulting sequence describes both topology and attributes.",
            "graph_type": "text-attributed graphs (general graphs, citation networks, small knowledge graphs)",
            "conversion_method": "Build graph-syntax tree, traverse (e.g., pre-order) to generate a linear text sequence describing nodes, neighbors, and attributes; attributes are embedded in the leaf descriptions.",
            "downstream_task": "node classification, link prediction, graph reasoning, graph-to-text generation",
            "performance_metrics": "Reported task metrics in the survey include Accuracy, Macro-F1/Micro-F1 for node classification and BLEU/ROUGE for graph-to-text; no single numeric result reported in the survey for GraphText.",
            "comparison_to_others": "Compared qualitatively to simple linearizations (node/edge lists) and path-based textualizations; GraphText aims to better retain hierarchical structure versus naive list serializations but the survey notes method choice substantially affects LLM performance.",
            "advantages": "Preserves hierarchical structure and node attributes together, better contextualizes local neighborhoods for LLM reasoning, reduces some structural information loss compared to flat lists.",
            "disadvantages": "Can generate long sequences (context window pressure), ordering choices may introduce permutation artifacts, still loses explicit graph algebraic structure (e.g., adjacency matrices) that specialized GNNs exploit.",
            "failure_cases": "Large or very dense graphs produce sequences exceeding LLM context limits; performance degrades when traversal order induces misleading neighbor priorities or when complex global topology (cycles, motifs) is not well captured by tree approximation.",
            "uuid": "e8819.0",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "WalkLM",
            "name_full": "WalkLM (attributed random-walk textualization)",
            "brief_description": "A path-based textualization approach that generates attributed random walks and converts each walk into natural language sequences for fine-tuning LMs to produce graph-aware embeddings.",
            "citation_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding",
            "mention_or_use": "mention",
            "representation_name": "attributed random-walk textualization",
            "representation_description": "Generate multiple random walks (node sequences) that capture local topology and include textualized node attributes for each visited node, then serialize each walk as a short natural-language sentence/sequence for LLM input or fine-tuning.",
            "graph_type": "attributed graphs (citation networks, biomedical, logs), applicable to both homogeneous and heterogeneous graphs when attributes are textualized",
            "conversion_method": "Perform attributed random walks (standard random-walk sampling), for each walk concatenate node textual attributes and simple connective phrases to produce a walk description; aggregate many walk-texts as training corpus for LLMs.",
            "downstream_task": "node classification, link prediction, representation learning, retrieval",
            "performance_metrics": "Survey cites use of Macro-F1, Micro-F1, AUC, MRR in related evaluations but does not provide numeric WalkLM-specific scores.",
            "comparison_to_others": "Path-based (walk) textualizations are compared favorably to naive global list serializations for capturing local context; however they may underperform tree-based or shortest-path schemes when multi-hop semantics require specific path selection.",
            "advantages": "Compact local-context descriptions, scalable via sampling, well-suited to capture local neighborhood statistics, easier to keep within LLM context windows.",
            "disadvantages": "Randomness may miss important structural motifs; multiple sampled walks required to cover topology leading to large corpora; global structure and long-range dependencies can be missed.",
            "failure_cases": "Dense graphs where walks are noisy and redundant; tasks requiring global structural features (e.g., specific motif counts, spectral properties) where walk sequences lack sufficient signal.",
            "uuid": "e8819.1",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Path-LLM",
            "name_full": "Path-LLM (shortest-path-based textualization)",
            "brief_description": "A shortest-path based conversion where selected shortest paths between node pairs are serialized into textual sequences to convey informative multi-hop relations to LLMs.",
            "citation_title": "Path-llm: A shortest-pathbased llm learning for unified graph representation",
            "mention_or_use": "mention",
            "representation_name": "shortest-path sequence textualization",
            "representation_description": "Select informative shortest paths (or constrained path sets) connecting relevant node pairs; convert each path into a human-readable sequence describing the nodes, edge relations and attributes along the path.",
            "graph_type": "general graphs, knowledge graphs (path-centric reasoning), attributed graphs",
            "conversion_method": "Compute (or sample) shortest paths between nodes of interest, format each path as a textual chain with node/edge attribute descriptions and relation labels (for KGs), and feed these textualized paths to LLMs as context or prompts.",
            "downstream_task": "link prediction, relation/edge prediction, node classification, path-based reasoning",
            "performance_metrics": "Survey lists metrics used broadly (MRR, Hits@N, Accuracy) for path-based KGC/link tasks but provides no path-LLM-specific numeric values.",
            "comparison_to_others": "Argued in survey to better capture meaningful multi-hop semantics than random-walks; compared qualitatively with WalkLM and full-serialization approaches—shortest-paths reduce noise and focus on semantically relevant chains.",
            "advantages": "Focuses LLM attention on semantically meaningful connectivity, reduces noisy sampling, better for multi-hop reasoning tasks (e.g., KGC, QA over graphs).",
            "disadvantages": "Requires graph shortest-path computation (costly on large graphs), choice of which paths to include is critical, may still omit alternative plausible multi-hop connections.",
            "failure_cases": "Graphs with many equally short paths (ambiguity), or where important non-shortest motif/context is required; very large graphs where enumerating paths is infeasible.",
            "uuid": "e8819.2",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "InstructGLM",
            "name_full": "InstructGLM (neighbor-scale instruction-tuned textualization)",
            "brief_description": "A method that describes graph structures by listing neighbors at multiple scales and uses instruction tuning of an LLM to better perform graph tasks from these textualized neighborhood descriptions.",
            "citation_title": "InstructGLM",
            "mention_or_use": "mention",
            "representation_name": "multi-scale neighbor textualization + instruction tuning",
            "representation_description": "For a target node, concatenate textual descriptions of its neighbors organized by hop-distance (1-hop, 2-hop, ...) into an instruction-style prompt; finetune or instruction-tune an LLM to respond to graph queries using this structured prompt format.",
            "graph_type": "attributed graphs, citation networks, knowledge graphs (when neighbor attributes/text available)",
            "conversion_method": "Extract neighborhood lists per hop, convert each neighbor and its attributes into short text snippets, assemble into an instruction template (e.g., 'Given node X and its neighbors: ... answer:') for LLM training or inference.",
            "downstream_task": "node classification, link prediction, graph QA, explanation generation",
            "performance_metrics": "Survey reports use of Accuracy and Link Prediction metrics in works that use neighbor textualization; no single numeric scores included for InstructGLM.",
            "comparison_to_others": "Instruction-style neighbor descriptions can outperform naive flat serializations because the LLM is explicitly trained to interpret neighbor-scale structure; compared to pure path-based methods, multi-scale neighbors provide broader local context.",
            "advantages": "Explicitly encodes locality and multi-hop context in human-readable form, compatible with instruction-tuned LLM paradigms, can improve LLM's ability to reason about local topology.",
            "disadvantages": "Prompt length grows with hop count and neighbor degree; may still suffer from permutation sensitivity and lose explicit graph topology beyond neighbor lists.",
            "failure_cases": "High-degree nodes produce very long prompts; heterogeneous neighborhood types may confuse LLM without careful template design.",
            "uuid": "e8819.3",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "MuseGraph",
            "name_full": "MuseGraph (neighbors+paths textualization + instruction tuning)",
            "brief_description": "A graph-oriented instruction tuning strategy that textualizes both neighbors and paths for LLM fine-tuning to support generic graph mining and question answering tasks.",
            "citation_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "mention_or_use": "mention",
            "representation_name": "neighbors-and-paths textualization with instruction tuning",
            "representation_description": "Combine textualized neighbor lists and path-based descriptions (shortest or sampled) into structured instruction prompts used to instruction-tune LLMs for graph reasoning, generation, and classification.",
            "graph_type": "general attributed graphs, knowledge graphs, dynamic graphs (with temporal annotations)",
            "conversion_method": "Assemble hybrid context: (1) local neighbor sentences, (2) selected path descriptions, (3) attribute summaries; wrap into instruction-style examples for LLM fine-tuning.",
            "downstream_task": "node classification, graph-to-text generation, temporal and multi-hop graph QA",
            "performance_metrics": "Survey lists evaluation metrics across cited works (Accuracy, MRR, BLEU, ROUGE) but does not give MuseGraph-specific numeric outcomes.",
            "comparison_to_others": "Hybrid neighbor+path textualization presented as more comprehensive than single-strategy approaches (walks or pure neighbor lists), offering improved multi-hop reasoning while retaining local context.",
            "advantages": "Balances local and path-based signals, suitable for instruction-based LLM training, yields richer contexts for multi-step reasoning.",
            "disadvantages": "Higher prompt/sequence lengths, more complex design choices (which paths/neighbors to include), more expensive to construct training data.",
            "failure_cases": "Context-window limits constrain how much hybrid information can be included; noisy or contradictory paths/neighbors can mislead LLM reasoning.",
            "uuid": "e8819.4",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Graph-LLM",
            "name_full": "Graph-LLM (text-attributed graph textualization for LLMs)",
            "brief_description": "A pipeline that converts text-attributed graphs into natural language descriptions so LLMs can perform zero-shot or few-shot graph tasks directly from textified graph inputs.",
            "citation_title": "Graphllm: Boosting graph reasoning ability of large language model",
            "mention_or_use": "mention",
            "representation_name": "text-attributed graph natural-language serialization",
            "representation_description": "Serialize nodes and their textual attributes into natural language sentences, include adjacency or neighbor statements as text (e.g., 'Node A links to Node B'), and feed these sequences to LLMs for zero/few-shot inference.",
            "graph_type": "text-attributed graphs, citation networks, recommendation graphs",
            "conversion_method": "Linearize nodes with their text attributes and include explicit edge statements or neighbor enumerations; optionally include small subgraph contexts as textual blocks.",
            "downstream_task": "zero-shot/few-shot node classification, link prediction, graph reasoning",
            "performance_metrics": "Survey references standard metrics (Accuracy, MRR, Hits@N) when used for KGC and node classification; no single numeric values reported in the survey for Graph-LLM.",
            "comparison_to_others": "Compared at high level with subgraph-aware and hybrid token alignment methods; pure text-serialization is simpler but risks topological information loss versus hybrid token/GNN-alignment approaches.",
            "advantages": "Straightforward to implement, leverages LLM pretrained linguistic knowledge, enables zero-/few-shot capabilities on text-rich graphs.",
            "disadvantages": "Topological information can be diluted by textualization; leads to long prompt sequences; LLMs are not permutation invariant so ordering choices affect outcomes.",
            "failure_cases": "Sparse non-textual attributes (graphs without meaningful textual labels) limit efficacy; graphs with complex multi-relational topology where short textual descriptions lose critical structure.",
            "uuid": "e8819.5",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GITA",
            "name_full": "GITA (Graph to visual-and-textual integration for VLMs)",
            "brief_description": "A multimodal structure-to-text-and-image representation: graph structures are converted to both textual descriptions and visualizations (images) so vision-language models (VLMs) or multimodal LLMs can reason about topology.",
            "citation_title": "Graph to visual and textual integration for vision-language graph reasoning",
            "mention_or_use": "mention",
            "representation_name": "text + visualization multimodal textualization",
            "representation_description": "Create parallel modalities: (1) textual descriptions of nodes/edges/attributes and (2) visual renderings/graphics of the graph or subgraphs; feed both modalities into a vision-language model for joint reasoning.",
            "graph_type": "general graphs, scene graphs, knowledge graphs where visualizable structure helps (e.g., layouts, molecular diagrams)",
            "conversion_method": "Serialize nodes/edges into text and produce curated images (graph drawings or path diagrams) representing topology; combine these as multimodal input to a VLM or multimodal LLM.",
            "downstream_task": "graph reasoning, multimodal QA over graphs, section summarization where structure benefits from visual cues",
            "performance_metrics": "Survey cites BLEU, ROUGE, CIDEr for multimodal tasks and Accuracy for node/link tasks but does not provide numeric GITA-specific results.",
            "comparison_to_others": "GITA extends pure-text approaches by adding visual topology cues; survey notes multimodal inputs can improve LLM understanding of structure compared to text-only serializations.",
            "advantages": "Visual modality helps LLMs interpret topology that is cumbersome to express in linear text; can reduce ambiguity from ordering effects in textual serialization.",
            "disadvantages": "Requires image generation/rendering pipeline and VLM-capable models; increases computational and engineering complexity.",
            "failure_cases": "Graphs that are too large to render compactly or visualizations that hide relational semantics (graph drawings can be ambiguous); VLMs may still miss fine-grained graph semantics (e.g., relation types).",
            "uuid": "e8819.6",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM4Hypergraph",
            "name_full": "LLM4Hypergraph (structure languages for hypergraphs)",
            "brief_description": "A set of low-order and high-order 'structure languages' to convert hypergraph structures into natural language descriptions so LLMs can reason over hypergraph relations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "hypergraph structure language textualization",
            "representation_description": "Design specialized natural-language templates to describe hyperedges (higher-order relations), convert hypergraph connectivity patterns into sentences capturing membership and types, and feed resulting text to LLMs.",
            "graph_type": "hypergraphs (e.g., group interactions, multi-party relations), higher-order graphs",
            "conversion_method": "For each hyperedge, generate descriptive text listing participating nodes and the hyperedge type or role; include summaries of overlapping hyperedges to capture high-order structure.",
            "downstream_task": "hypergraph reasoning, sociological analysis, classification on hypergraph nodes/edges",
            "performance_metrics": "Survey lists general metrics for related tasks (Accuracy, Macro-F1) but no numeric results for LLM4Hypergraph in the survey.",
            "comparison_to_others": "Hypergraph textualization is contrasted with pairwise graph textualizations; the survey suggests special languages are needed to retain higher-order semantics that pairwise serializations would lose.",
            "advantages": "Explicitly encodes high-order relations in text, enabling LLMs to access multi-member relation semantics.",
            "disadvantages": "Text grows quickly with hyperedge size; potential loss of combinatorial structure when complex overlaps exist; no standardized templates—design choices impact results.",
            "failure_cases": "Very high-order hyperedges with many participants produce overly long or ambiguous text; LLM hallucination when hyperedge semantics are complex or underspecified.",
            "uuid": "e8819.7",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Structure-to-text (generic)",
            "name_full": "Structure-to-text transformation methods (generic category)",
            "brief_description": "A family of methods converting graph topology into textual formats (node/edge lists, sequences, paths, trees, formal graph languages) to feed LLMs for graph tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "generic structure-to-text (linearization/serialization/path/tree/formal language)",
            "representation_description": "Includes node/edge lists, node-indexed neighbor lists, random-walk/path serializations, graph-syntax trees, and formal markup (GraphML-like) rendered as text; purpose is to make topology consumable by LLMs.",
            "graph_type": "applies broadly to homogeneous/heterogeneous graphs, knowledge graphs, molecular graphs, hypergraphs (with adaptations)",
            "conversion_method": "Various: adjacency lists (linearized), random-walk generation + textualization, shortest-path extraction + textualization, tree construction + traversal, or formal markup serialization.",
            "downstream_task": "node classification, link prediction, knowledge graph completion, graph QA, graph-to-text generation",
            "performance_metrics": "Survey summarizes that works report Accuracy, Macro-F1, MRR, Hits@N, BLEU, ROUGE, but no single canonical numeric performance for the generic class.",
            "comparison_to_others": "Survey highlights that choice among these structure-to-text approaches matters: path-based and tree-based methods often outperform naive flat serializations; hybrid or GNN-aligned tokens retain more structure.",
            "advantages": "Enables use of powerful pretrained LLMs for graph tasks without training specialized GNNs; flexible and interpretable representations.",
            "disadvantages": "Information loss (global algebraic properties), long sequences for large graphs, ordering sensitivity (LLMs not permutation invariant), conversion design is task-dependent.",
            "failure_cases": "Large-scale graphs exceed context windows; structural properties like spectral features are not preserved; hallucination risk if LLM infers unsupported edges from textual priors.",
            "uuid": "e8819.8",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Hybrid graph-token alignment",
            "name_full": "Hybrid methods aligning GNN graph tokens with LLM token space (GNP / GraphToken / GraphGPT / LLaGA / TEA-GLM / HiGPT family)",
            "brief_description": "Methods that encode structural representations via GNNs or specialized tokenizers, then project those embeddings into the LLM token embedding space so LLMs can consume compact graph tokens alongside text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "graph-token projection / alignment",
            "representation_description": "Use a GNN (or structure encoder) to produce compact subgraph/node embeddings; apply a projector to map embeddings into the discrete token embedding space of an LLM or into token-like continuous inputs; process via LLM with aligned tokens or adapters.",
            "graph_type": "heterogeneous graphs, large graphs, knowledge graphs, scenarios requiring scalability beyond raw serializations",
            "conversion_method": "Encode subgraphs or nodes with GNNs; apply linear/nonlinear projector or contrastive alignment to LLM token principal components; optionally instruction-tune LLM on tasks using projected tokens.",
            "downstream_task": "node/graph classification, link prediction, instruction following for graph tasks, few-shot KGC",
            "performance_metrics": "Survey notes these hybrid methods aim to reduce context size and improve performance; typical reported metrics in cited works are Accuracy, MRR, Hits@N, but survey does not give numeric values here.",
            "comparison_to_others": "Hybrid token alignment reduces sequence length vs pure textualization and preserves structural fidelity better than text-only methods; compared favorably in the survey to naive serializations though engineering complexity is higher.",
            "advantages": "Compact, scalable (smaller input size), better preservation of explicit structural signals, supports larger subgraphs within LLM input, facilitates tighter LLM-GNN fusion.",
            "disadvantages": "Requires training/alignment steps (projector, contrastive objectives), more engineering (GNN + projector + LLM adapters), potential mismatch between token projection and LLM internal representations.",
            "failure_cases": "Poor projector alignment can lead to degraded LLM performance; may still lose fine-grained relational semantics (e.g., edge types) if tokenization is too coarse.",
            "uuid": "e8819.9",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "GNP / GraphToken (examples)",
            "name_full": "GNP / GraphToken (examples of graph-token mapping methods)",
            "brief_description": "Representative hybrid designs that encode graph structure with GNNs and map the resulting embeddings into token-like vectors compatible with LLMs to enable joint reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN-to-token projection (GraphToken)",
            "representation_description": "Use a GNN encoder to get structure-aware embeddings for nodes/subgraphs, then use a learned projector to convert embeddings into the LLM token embedding space enabling concatenation with textual tokens for LLM processing.",
            "graph_type": "heterogeneous graphs, large-scale graphs, knowledge graphs",
            "conversion_method": "GNN encode -&gt; embedding projector -&gt; inject as 'graph tokens' into LLM input stream or cross-attention adapters.",
            "downstream_task": "node classification, link prediction, knowledge graph completion, instruction-following on graphs",
            "performance_metrics": "No specific numeric metrics provided in the survey summary; works report Accuracy, MRR, Hits@N on standard benchmarks.",
            "comparison_to_others": "Presented as a middle ground between text-serialization (simpler) and full GNN-only pipelines (structurally faithful); tends to preserve more topology while shrinking context compared to pure text.",
            "advantages": "More compact and structure-preserving than long textual serializations, reduces context window pressure, enables joint LLM-GNN training.",
            "disadvantages": "Needs alignment training; may require labeled data or self-supervised objectives to align spaces successfully.",
            "failure_cases": "If alignment objective is weak or datasets are mismatched, projected tokens may be uninterpretable by the LLM and performance drops.",
            "uuid": "e8819.10",
            "source_info": {
                "paper_title": "A Survey of Large Language Models for Data Challenges in Graphs",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graphtext: Graph reasoning in text space",
            "rating": 2,
            "sanitized_title": "graphtext_graph_reasoning_in_text_space"
        },
        {
            "paper_title": "Walklm: A uniform language model fine-tuning framework for attributed graph embedding",
            "rating": 2,
            "sanitized_title": "walklm_a_uniform_language_model_finetuning_framework_for_attributed_graph_embedding"
        },
        {
            "paper_title": "Path-llm: A shortest-pathbased llm learning for unified graph representation",
            "rating": 2,
            "sanitized_title": "pathllm_a_shortestpathbased_llm_learning_for_unified_graph_representation"
        },
        {
            "paper_title": "Graph to visual and textual integration for vision-language graph reasoning",
            "rating": 2,
            "sanitized_title": "graph_to_visual_and_textual_integration_for_visionlanguage_graph_reasoning"
        },
        {
            "paper_title": "Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining",
            "rating": 2,
            "sanitized_title": "musegraph_graphoriented_instruction_tuning_of_large_language_models_for_generic_graph_mining"
        },
        {
            "paper_title": "Graphllm: Boosting graph reasoning ability of large language model",
            "rating": 2,
            "sanitized_title": "graphllm_boosting_graph_reasoning_ability_of_large_language_model"
        }
    ],
    "cost": 0.02678125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey of Large Language Models for Data Challenges in Graphs
18 Sep 2025</p>
<p>Mengran Li 
School of Intelligent Systems Engineering
Guangdong Key Laboratory of Intelligent Transportation System
Shenzhen Campus of Sun Yat-sen University
518107Shenzhen, GuangdongChina</p>
<p>Pengyu Zhang p.zhang@uva.nl 
University of Amsterdam
AmsterdamThe Netherlands</p>
<p>Wenbin Xing xingwb@mail2.sysu.edu.cn 
School of Intelligent Systems Engineering
Guangdong Key Laboratory of Intelligent Transportation System
Shenzhen Campus of Sun Yat-sen University
518107Shenzhen, GuangdongChina</p>
<p>Yijia Zheng y.zheng@uva.nl 
University of Amsterdam
AmsterdamThe Netherlands</p>
<p>Klim Zaporojets 
Aarhus University
AarhusDenmark</p>
<p>Junzhou Chen chenjunzhou@mail.sysu.edu.cn 
School of Intelligent Systems Engineering
Guangdong Key Laboratory of Intelligent Transportation System
Shenzhen Campus of Sun Yat-sen University
518107Shenzhen, GuangdongChina</p>
<p>Ronghui Zhang 
School of Intelligent Systems Engineering
Guangdong Key Laboratory of Intelligent Transportation System
Shenzhen Campus of Sun Yat-sen University
518107Shenzhen, GuangdongChina</p>
<p>Yong Zhang zhangyong2010@bjut.edu.cn 
Institute of Artificial Intelligence
Beijing University of Technology
100124Beijing, BeijingChina</p>
<p>Siyuan Gong sgong@chd.edu.cn 
School of Information and Engineering
Chang'an University
710064Xi'an, ShaanxiChina</p>
<p>Jia Hu hujia@tongji.edu.cn 
Key Laboratory of Road and Traffic Engineering of the Ministry of Education
Tongji University
201804ShanghaiChina</p>
<p>Xiaolei Ma xiaolei@buaa.edu.cn 
School of Transportation Science and Engineering
Key Laboratory of Intelligent Transportation Technology and System
Beihang University
100191BeijingChina</p>
<p>Zhiyuan Liu zhiyuanl@seu.edu.cn 
Jiangsu Province Collaborative Innovation Center of Modern Urban Traffic Technologies
School of Transportation
Jiangsu Key Laboratory of Urban ITS
Southeast University
210096Nanjing, JiangsuChina</p>
<p>Paul Groth p.t.groth@uva.nl 
University of Amsterdam
AmsterdamThe Netherlands</p>
<p>Marcel Worring m.worring@uva.nl 
University of Amsterdam
AmsterdamThe Netherlands</p>
<p>A Survey of Large Language Models for Data Challenges in Graphs
18 Sep 202551399759834FE503EB5FD8F9903D07BEarXiv:2505.18475v2[cs.LG]Graph LearningLarge Language ModelsGraph IncompletenessData ImbalanceCross-domain Graph HeterogeneityDynamic Graph Instability Label KGs-LLM [130] Wikipedia [130] F1PrecisionRecall Knowledge Graph Generation Mixed FSKG [40] WN18RR [124]FB15k-237 [94] MRRHits@N Knowledge Graph Completion KGLLM [131] WN11 [132]FB13 [132]WN18RR [124]YAGO3-10 [124] AccuracyMRRHits@N Link PredictionKnowledge Graph Completion KICGPT [133] FB15k-237 [94]WN18RR [124] MRRHits@N Link Prediction RL-LLM [134] ElectronicsInstacart [134] PrecisionRecallAccuracy Knowledge Graph Completion GoG [135] Synthetic Data [135] Hits@N Knowledge Graph Question Answering KoPA [136] UMLS [137]CoDeX-S [125]FB15K-237N [125] F1PrecisionRecallAccuracy Knowledge Graph Completion LLMKG [138] Templates Easy [138]Templates Hard [138] Strict MetricsFlexible Metrics Knowledge Graph Completion DIFT [139] WN18RR [124]FB15k-237 [94] MRRHits@N Link PredictionKnowledge Graph Completion CP-KGC [140] WN18RR [124]FB15k-237 [94]UMLS [137] MRRHits@N Knowledge Graph Completion MuKDC [141] NELL [142]Wiki [129] MRRHits@N Knowledge Graph Completion
Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction.While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process.In this survey, we focus on four fundamental data-centric challenges:(1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways.Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge.This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning.For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages.Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field.To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.</p>
<p>Introduction</p>
<p>Graphs provide a flexible and expressive framework for modeling complex systems composed of interconnected entities.Unlike grid-structured data such as images or sequences, graphs encode non-Euclidean topologies with irregular neighborhood structures and varying connectivity patterns.To support learning on such structures, graph learning aims to derive lowdimensional node or graph-level representations that preserve both structural dependencies and semantic information from the original graph [1,2,3,4].Due to these representational advantages, graph learning has been successfully applied across a wide range of domains, including social network analysis [5], personalized recommendation [6], transportation optimization [7], financial modeling [8], and bioinformatics [9,10].As a result, it has become a foundational technique for extracting knowledge from relational data and enabling models to reason over structured domains.</p>
<p>Despite this progress, real-world graph data often exhibits complexities, including sparsity, noise, domain divergence, and dynamic evolution, introducing practical difficulties for graph learning [11].For instance, in social networks, cold-start users may lack sufficient links or profile features, making it difficult to learn reliable representations [6].In fraud detection, the data is often highly imbalanced, where fraudulent behavior accounts for only a small fraction of all activities, and labels are sparse and noisy [12].In smart cities, traffic systems involve heterogeneous and dynamically evolving road networks, posing difficulties for models to adapt to changing topologies in real time [7,13,14].As summarized in Figure 1, these concrete scenarios highlight four fundamental data-centric challenges in real-world graphs: Imbalance in graphs, where the distribution of nodes, edges, or labels is highly skewed, (3) Cross-domain heterogeneity in graphs, where graph data from different domains exhibit semantic and structural discrepancies, and (4) Dynamic instability in graphs, where graphs undergo dynamic changes in topology, attributes, or interactions over time.</p>
<p>(1) Incompleteness in Graphs An incomplete graph refers to a graph where some node or edge information is missing, resulting in an incomplete representation of the structure or attributes that fails to fully reflect the complexity of the real-world system [15].This challenge typically arises from incomplete data collection, missing link information, or outdated knowledge.For example, in social networks, missing user attributes-such as age, location, or interests-can obscure community boundaries, making it difficult to accurately detect user groups or recommend content.Similarly, missing connections between users may conceal important social ties, weakening the structural signals used for clustering or influence analysis.This incompleteness undermines the effectiveness of graph learning.</p>
<p>(2) Imbalance in Graphs A graph is considered imbalanced when certain categories or subgraphs contain significantly more nodes or edges than others [16].This can lead to some categories dominating the graph, while others are left underrepresented [17].For example, financial transaction networks are characterized by containing a large number of legitimate transactions, and only a small fraction of fraudulent ones [18].Imbalanced training data may cause models to overfit to the majority class, thus impacting the model's ability to generalize and recognize the minority class effectively [19].This data imbalance challenge affects many graph learning methods, especially those relying on graph convolutional networks [20].</p>
<p>(3) Cross-Domain Heterogeneity in Graphs Cross-domain heterogeneity refers to significant disparities in attributes and structural patterns that arise when graph data are constructed from multiple source domains, especially when these domains have different data modalities or exhibit substantial distribution shifts.For instance, road network data collected from various cities may exhibit considerable heterogeneity, with some cities having grid-like structural patterns, while others exhibit radial configurations [21].This issue becomes even more critical in the rapidly growing field of graph foundation models [22], which aim to develop generalizable models capable of handling diverse graphs from different domains, such as molecular graphs and citation networks.Cross-domain heterogeneity poses additional challenges for graph learning, as diverse attributes and structural patterns cannot be easily processed in a unified manner [23,24].Additionally, domain discrepancies may distort essential semantic and structural signals, complicating the identification of transferable features and limiting the effectiveness of graph learning methods [25,26].</p>
<p>(4) Dynamic Instability in Graphs Dynamic instability in graphs refers to continuous and unpredictable changes in node features and relationships over time.These may include updates to node attributes, alterations in graph structure, or the emergence of new node and edge types.Structural changes often involve adding or removing connections or reorganizing how nodes interact.For example, in a large-scale knowledge graph like Wikidata, an entity initially labeled as an "athlete" might later become classified as a "coach".This transition entails significant shifts in both node features and their connections within the graph [27,28].Furthermore, these continuous transformations introduce instability, making it difficult for graph learning models to adapt effectively.As a result, performance may degrade in tasks such as node classification, link prediction, and node representation learning [29].A deeper understanding of these forms of dynamic instability can facilitate the development of more robust and adaptive graph learning methods, crucial for accurately modeling the evolving real-world graphs.</p>
<p>A variety of graph learning methods have been developed to address these challenges, achieving notable success in specific contexts.For instance, graph completion algorithms [30] and graph autoencoders [15] have been employed to handle missing data; techniques like GraphSMOTE [31] and re-weighted loss functions [32] are commonly used to mitigate data imbalance; and graph domain adaptation methods, including adversarial regularization [23,33] and distribution alignment [26,34], have been explored to address cross-domain heterogeneity.Additionally, dynamic graph models such as EvolveGCN [35] and DyRep [36] have been proposed to model evolving graph structures.While these methods are effective within their respective scopes, they often rely on task-specific designs and require significant domain expertise.</p>
<p>In contrast, the emergence of large language models (LLMs) has opened up new opportunities for mitigating these fundamental challenges [37,38,39].With their strong representational capacity, contextual reasoning abilities, and generalization potential, LLMs can extract rich semantic patterns from heterogeneous and noisy data.These capabilities enable LLMs to augment graph data by inferring missing information [40], synthesizing data for underrepresented classes [41], aligning heterogeneous attributes [42], and capturing temporal evolution or structural changes in dynamic graphs [43].By addressing these issues, LLMs offer a promising complement to traditional graph learning approaches.To reflect this growing interest, Figure 2 shows publication trends over the past decade (up to July 2025) related to "Graph Learning" and "LLM-based Graph Learning".The statistics indicate that not only has graph learning become an increasingly important topic, but the proportion of research exploring the integration of LLMs into graph learning has also been steadily rising.</p>
<p>This survey provides a comprehensive review of current research at the intersection of graph learning and large language models, centered on the four fundamental challenges introduced above.To ensure broad coverage, we performed an extensive literature search across major databases (arXiv, ACM Digital Li- brary, IEEE Xplore, Elsevier, and Springer, etc.) using keyword combinations such as "LLMs", "graph learning", "incompleteness", "imbalance", "heterogeneity", "dynamic graphs".We focus on recent publications (primarily 2015-2025) in top machine learning, data mining, and NLP venues, as well as relevant preprints.This survey includes papers that meet at least one of the following criteria: (1) the work explicitly addresses one of the four fundamental challenges in graph learning, or (2) the proposed solution incorporates LLMs as a central component.Based on these criteria, we initially collected and reviewed over 1,000 relevant papers.We then categorized them according to the type of challenge addressed and the strategies employed, ultimately selecting a curated set of over 380 representative works.For each challenge, we briefly discuss conventional (non-LLM) techniques to provide background context, while our primary focus is to highlight the novel contributions and advantages introduced by LLM-based approaches.</p>
<p>The structure of the paper is organized as follows: Section 2 provides an analysis and summary of traditional methods and LLM-based approaches for handling graph learning challenges.Section 3, building on the existing body of research, proposes new technical paths and solutions.Section 4 discusses the potential future directions for graph learning research.Finally, Section 5 concludes the paper.</p>
<p>Overview of Graph Learning and LLMs</p>
<p>Graph Notation Definition</p>
<p>We denote a graph as G = (V, E, X V , X E ), where V is the set of n nodes, E is the set of edges, X V ∈ R |V|×d v is the node feature matrix, X E ∈ R |E|×d e is the edge feature matrix, and A ∈ R n×n is the adjacency matrix, where A i j = 1 if (v i , v j ) ∈ E. The degree matrix is D with D ii = j A i j .The normalized graph Laplacian is defined as:
L = I − D −1/2 AD −1/2 .
The types of graph data involved in this work can be broadly categorized as follows.Homogeneous graph refers to a graph in  which all nodes and edges belong to the same type.Heterogeneous graph refers to a graph where nodes and/or edges belong to multiple types, such as user/item nodes or different edge relations.Attributed graph refers to a graph where nodes and/or edges are associated with real-valued features, i.e., X V and/or X E are non-empty.Knowledge graph refers to a directed, multirelational graph represented as triplets (h, r, o), where h ∈ V is the head entity, o ∈ V is the object entity, and r ∈ R is a relation type.Dynamic graph refers to a sequence of time-evolving graphs {G t } T t=1 in which the node set, edge set, or features may change over time.</p>
<p>Graph Learning Methods</p>
<p>Graph learning refers to a family of machine learning techniques designed specifically to analyze and learn from graphstructured data [1,44,2].Unlike data in Euclidean domains, graphs encode relationships between entities, and thus graph learning algorithms must leverage both node features and the graph topology.The goal is often to learn vector representations for nodes, edges, or entire subgraphs/graphs that capture their structural role and feature information, so that these embeddings can be used for downstream tasks, as shown in Figure 3(a).</p>
<p>Traditional Graph Learning Methods: A wide range of traditional techniques have been developed to learn graph representations.Early methods such as spectral clustering [45] and LINE [46] learn embeddings by factorizing adjacency or similarity matrices, capturing global connectivity patterns.Randomwalk-based approaches like DeepWalk [47], Node2Vec [48], and metapath2vec [49] model local contexts through node sequences generated by truncated walks, offering better scalability and flexibility.In parallel, autoencoder-based models aim to reconstruct graph structures from latent representations.Notable examples include Graph Autoencoder (GAE) and Variational Graph Autoencoder (VGAE) [50], which adopt graph convolutional encoders with deterministic or probabilistic decoders.Further developments such as Adversarially Regularized Graph Autoencoders (ARGA) [51] and Graph Attention Autoencoders (GATE) [52] introduce adversarial training and attention mechanisms to improve expressiveness and robustness.Despite their foundational role, traditional methods often fall short in handling the complexity of real-world graph data.</p>
<p>Graph Neural Networks: The advent of graph neural networks (GNNs) marked a significant change by enabling endto-end learning directly on graph structures.Spectral GNNs, starting with Bruna et al. [53], were initially limited by high computational costs, which were mitigated by subsequent works using polynomial approximations [54].The introduction of Graph Convolutional Networks (GCNs) by Kipf and Welling [55] popularized neighborhood aggregation in semi-supervised settings.Further developments such as GraphSAGE [56], GAT [57], SGC [58], and GIN [59] improved the expressiveness, scalability, and adaptability of GNNs.</p>
<p>More recently, two powerful approaches have emerged in graph learning: contrastive learning and Transformer-based architectures.Graph contrastive learning methods like DGI [60], GraphCL [61], and HeCo [62] focus on learning robust representations by maximizing consistency across multiple views of the same graph.Transformer-inspired models, such as GT [63] and GraphGPS [64], introduce global self-attention and hybrid message-passing to model long-range dependencies and scale across graph types.</p>
<p>While graph learning techniques have shown remarkable success in modeling relational structures and topological dependencies, they often fall short in capturing high-level semantics and leveraging external unstructured data [65,66].These limitations become more pronounced when dealing with real-world graphs that are frequently incomplete, imbalanced, heterogeneous across domains, or dynamically evolving over time.These challenges have motivated a growing interest in integrating graph models with powerful language-based representations, such as LLMs, which offer strong semantic reasoning capabilities and the flexibility to incorporate external knowledge into the graph learning pipeline.</p>
<p>LLMs for Graph Learning</p>
<p>LLMs are a class of deep neural networks pretrained on massive text corpora to capture the statistical and semantic structure of natural language [67,68].By learning distributed token representations and contextual relationships, LLMs achieve impressive performance across a broad spectrum of NLP tasks, including text generation [69], question answering [70], and machine translation [71].</p>
<p>The development of LLMs reflects a paradigm shift from taskspecific pipelines to general-purpose language understanding.Early NLP relied on symbolic or statistical models (e.g., HMMs [72], CRFs [73]), which were constrained by domain specificity and limited context.The introduction of the Transformer architecture [74] enabled global attention and deep sequence modeling, which laid the foundation for modern LLMs.Subsequent milestones such as BERT [75], GPT-3 [76], and ChatGPT [77] progressively advanced context modeling, few-shot reasoning, and conversational alignment.Recent systems like GPT-4 [78] and DeepSeek-MoE [79] further scale capacity and introduce modular expert routing, enabling more efficient and multimodal learning.</p>
<p>Despite being originally designed for natural language, LLMs exhibit emergent capabilities-such as abstract reasoning, flexible generalization, and heterogeneous input integration-that extend far beyond textual tasks.As shown in Figure 3(b), LLMs can complement graph learning by generating structural hypotheses [80], enriching semantic content [40], and aligning cross-domain knowledge [42].Viewed through the lens of graph learning, these abilities make LLMs well-suited for enhancing data quality and robustness in graph-based applications:</p>
<p>Semantic reasoning: By leveraging rich contextual understanding, LLMs can infer latent or missing information from incomplete inputs.In the graph domain, this enables the imputation of missing node or edge attributes, the prediction of potential links, and the interpretation of sparsely connected substructures.Such capabilities directly address the challenge of incompleteness in real-world graphs, where data sparsity or noise is common [40].</p>
<p>Cross-domain and cross-modal alignment: LLMs are trained on diverse sources of knowledge and can serve as translators between incompatible domains or feature spaces.This makes them particularly useful in cross-domain heterogeneous graphs [42]-for example, aligning product graphs across ecommerce platforms that differ in attribute schemas, category taxonomies, and language conventions.LLMs can bridge such gaps by reasoning over textual descriptions and contextual cues to identify semantically equivalent items across domains.</p>
<p>Generative augmentation:</p>
<p>As generative models, LLMs can synthesize new graph components-such as nodes, edges, or subgraphs-conditioned on textual or structural prompts.This ability can be used to create balanced training samples for underrepresented classes, thereby mitigating data imbalance [41], or to simulate hypothetical future graph states for handling dynamic instability [80].</p>
<p>In the following sections, we examine each of the four fundamental data-centric challenges in turn, analyzing how LLMs have been used to mitigate them, and what open questions remain at this intersection.</p>
<p>Current Techniques for Graph Challenges</p>
<p>In this section, we discuss each of the four core challenges in detail.For each challenge, we first formalize the problem.We then review representative traditional and LLM-enhanced solutions to provide context.</p>
<p>Incompleteness in Graphs</p>
<p>In the real world, graph data is often incomplete due to missing node attributes, edges or labels, which significantly impacts the accuracy and generalization ability of graph models.By leveraging LLMs for graph data completion and the inference of missing information, the performance of graph learning models can be significantly improved, even in scenarios with incomplete or missing information.We categorize current approaches to incomplete graphs into three major directions (as shown in Figure 4): (1) Robust Graph Learning, which focuses on making models resilient to missing data; (2) Few-Shot Graph Learning, which addresses scenarios with extremely limited labeled data or structure by transferring knowledge; and (3) Knowledge Graph Completion, which is a prominent sub-area dealing with inferring missing facts in large knowledge graphs.The relevant references and categorization are presented in Table 1.</p>
<p>Robust Graph Learning</p>
<p>Robust graph learning methods aim to maintain model performance even when the input graph is noisy or partially observed.Traditional GNNs often degrade in accuracy if key attributes are missing or if the graph is sparsely connected, because the iterative message passing has less information to propagate.Therefore, researchers have proposed specialized techniques to make graph learning robust to incompleteness.</p>
<p>(1)Traditional Methods for Robust Graph Learning Early efforts in robust graph learning focused on imputing missing values using global interpolation techniques under lowrank assumptions.For example, matrix completion methods [143,30,144,145] estimate missing attributes based on the global structure of the data.GraphRNA [144] introduces Attribute Random Walk (AttriWalk), modeling attributes as a bipartite graph to capture attribute-structure interactions, thereby enhancing local adaptability and robustness in heterogeneous environments.ARWMF [145] combines random walks with matrix factorization and mutual information to improve unsupervised node embedding under noisy conditions.Cora [81], PubMed [82], ogbn-arxiv [83] Accuracy Node Classification LLM-TAG [84] Cora [81], Citeseer [85], PubMed [82], Arxiv-2023 [86] Accuracy Node Classification SPLLM [87] PeMS03 [88], PeMS04 [88], PeMS07 [88] MAE, RMSE, MAPE Spatiotemporal Forecasting</p>
<p>Label LLMGNN [89] Cora [81], Citeseer [85], PubMed [82], ogbn-arxiv [83], ogbn-products [83], Wikics [90] Accuracy Node Classification</p>
<p>Mixed</p>
<p>GraphLLM [91] Synthetic Data [92] Exact Match Accuracy Graph Reasoning PROLINK [93] FB15k237 [94], Wikidata68K [95], NELL-995 [96] MRR, Hits@N Knowledge Graph Completion</p>
<p>UnIMP [97] BG [98], ZO [98], PK [98], BK [98], CS [98], ST [98], PW [98], BY [98], RR [98], WM [ IM-TQA [109], WCC [110], HiTab [111], WTQ [112], TabFact [113] Macro-F1, Accuracy Table Understanding FlexKBQA [114] GrailQA [115], WebQSP [116], KQA Pro [117] Exact Match, F1, Accuracy Knowledge Graph Question Answering KGQG [118] WebQuestions [119], PathQuestions [120] BLEU-4, ROUGE-L, Hits@N Knowledge Graph Question Answering Knowledge Graph Completion Node LLM-KGC [121] ILPC [122] MRR, Hits@N Knowledge Graph Completion GS-KGC [123] WN18RR [124], FB15k-237 [94], FB15k-237N [125], ICEWS14 [126], ICEWS05-15 [127] Hits@N Knowledge Graph Completion GLTW [128] FB15k-237 [94], WN18RR [124], Wikidata5M [129] MRR, Hits@N Link Prediction</p>
<p>With the rise of deep learning, more sophisticated generative models were developed to overcome the linear assumptions of early methods [15,146,147,148,149,150,151,152,153,154].SAT [15] enhances robustness by decoupling structure and attribute signals via shared latent spaces and distribution alignment.SVGA [146] imposes strong probabilistic regularization on latent variables to prevent overfitting when features are sparse or unreliable.Models like ITR [147] and VISL [148] progressively refine latent variables or inter-variable structures using topological cues, improving resilience against structural noise.More recently, methods like MATE [149] and AIAE [150] exploit multi-view and multi-scale generation strategies to stabilize representation learning under incomplete or corrupted input.CSAT [151] incorporates contrastive learning and Transformers to detect communities under noisy or weak supervision, further enhancing robustness through auxiliary signals.</p>
<p>Another line of work improves model robustness through dynamic inference mechanisms that rely on structural priors like homophily and community consistency [155,156,157].For instance, FP [155] integrates Dirichlet energy minimization with graph diffusion to achieve stable feature recovery.PCFI [156] assigns confidence scores to feature channels, allowing uncertaintyaware propagation.ARB [157] addresses the cold-start problem by introducing virtual edges and redefined boundary conditions to improve propagation in sparse or poorly connected graphs.A notable advantage of such propagation-based methods is their parameter-free nature, making them highly adaptable and easily integrated with LLM-based pipelines for downstream reasoning tasks.To tackle the cold-start problem in node representation learning, Cold Brew [158] introduced a feature contribution ratio metric to guide teacher-student distillation, uniquely encoding topological sparsity as a dynamic temperature coefficient.CTAug [159] employs subgraph-aware contrastive learning to preserve dense subgraph priors, significantly enhancing representation learning in dense graphs.</p>
<p>These methods significantly enhance the predictive capabilities of graph data in scenarios with missing attributes by jointly modeling, employing variational inference, and optimizing multi-source information.Nevertheless, three challenges remain: (1) coupling optimization of completion and prediction dramatically increases training complexity; (2) latent-space completion methods are sensitive to prior assumptions; (3) efficient incremental mechanisms are lacking for dynamic attribute completion.</p>
<p>(2) LLM-enhanced methods Recently, researchers have begun exploring the deep integration of semantic reasoning and incomplete graph learning.Methods based on LLMs [84,41,87,91,93,89] pioneer a new paradigm that combines semantic completion and structural refinement, leveraging the strong generative and reasoning capabilities of LLMs.</p>
<p>Figure 5 illustrates how LLMs can extract domain-specific knowledge and use it to compensate for missing nodes, edges, or attributes.For example, Sun et al. [84] demonstrate that LLMs can infer missing nodes and edges by recognizing semantic similarities between entities.In this way, the model does not just "guess" a connection statistically, but proposes links that are meaningful in the real world.Building on this idea, LLM4NG [41] generates new, contextually relevant nodes, which is especially helpful in few-shot settings where the graph is very sparse.Both methods show that LLMs can enrich incomplete graphs more effectively than traditional imputation.Meanwhile, SPLLM [87] focuses on traffic sensor networks, where missing values break both spatial and temporal dependencies.By combining spatiotemporal GCNs with LLM fine-tuning, and incorporating external knowledge such as road maps or weather data, it achieves more reliable predictions.Similarly, UnIMP [97] works on tabular data by treating it as a hypergraph and using LLMs to fill in missing values, even when the data is semantically or structurally heterogeneous.</p>
<p>To further close the semantic gap introduced by graph incompleteness, several works propose end-to-end frameworks combining LLMs and graph models.For instance, GraphLLM [91] introduces a unified architecture where a graph encoder maps node features into the LLM's semantic space, enabling it to reason over incomplete or noisy node attributes using attentiongated fusion of structure and context.This helps, for example, in citation networks or recommendation systems where node metadata may be sparse or outdated.PROLINK [93] enhances inductive reasoning on low-resource knowledge graphs by generating structural prompts and using LLMs to fill in gaps from limited graph and text data -particularly valuable in biomedical or emerging domains with sparse curation.Lastly, LLMGNN [89] proposes a semi-supervised approach where LLMs annotate a small subset of nodes, helping GNNs generalize from minimal labeled data -addressing the classic label incompleteness challenge in large-scale graphs.</p>
<p>Few-shot Graph Learning</p>
<p>Few-shot learning (FSL) deals with scenarios where very few labeled examples are available for training, or where a graph has only a handful of nodes/edges in a particular class or subset of interest.This is related to incompleteness in that labels can be viewed as a type of missing information.</p>
<p>( Despite these advances, traditional FSL methods remain constrained by hand-crafted architecture designs, often lacking efficiency when dealing with large, complex graph structures and exhibiting limited capability for structured and interpretable reasoning.</p>
<p>(2) Few-Shot Learning in LLMs Few-shot learning has emerged as a natural strength of LLMs, enabling them to handle graph-related tasks with limited labeled data by leveraging rich prior knowledge from pretraining.Recent work demonstrates their effectiveness in diverse scenarios such as graph classification [106], link prediction [99], anomaly detection [102], table understanding [108], and knowledge graph question answering (KGQA) [114,118].For instance, LLMDGCN [106] incorporates degree-aware prompt tuning with graph-encoded positional embeddings, enabling both node classification and edge recovery in low-label settings.In link prediction, LinkGPT [99] combines instruction tuning with retrieval-based re-ranking to enhance reasoning under sparse supervision.AnomalyLLM [102] integrates dynamic-aware encoding and prototype-based edge reprogramming for improved detection in dynamic graphs.Beyond pure graph tasks, HeGTa [108] aligns table semantics with LLM knowledge via heterogeneous graph-enhanced soft prompts, while FlexKBQA [114] and a zero-shot KGQA approach [118] reformulate KB queries into natural language to generate synthetic training data or reduce reliance on full supervision.Collectively, these results indicate that LLMs can generalize to novel graph tasks with minimal examples, acting as strong priors that bridge missing or noisy information.</p>
<p>In summary, while few-shot graph learning remains challenging, LLMs act as few-shot learners by nature -they can leverage their pretrained knowledge to make sense of novel tasks with limited data.Results show that even with minimal graph information, an LLM can propose connections or classifications that align with human knowledge, essentially providing a powerful prior to the graph model.</p>
<p>Knowledge Graph Completion</p>
<p>Incompleteness is inherent in knowledge graphs: no knowledge base is ever complete.The development of knowledge graph completion (KGC) revolves around the core challenge of structure-semantics fusion, forming a progressive innovation trajectory from fundamental approach exploration to multimodal collaboration.LLMs, with their strong language understanding and reasoning capabilities, demonstrate great potential in addressing long-tail challenges, reducing annotation burdens, and handling incomplete graph data [130,40,131,134,135,121,136,141,138,139,140,123,133].</p>
<p>Early explorations of LLM-driven KGC primarily focused on converting triples into natural language sequences to verify whether generative LLMs could address incompleteness effectively.FSKG [40] exemplifies this trend by introducing a generation-refinement pipeline that mitigates long-tail sparsity through staged generation.Building on this concept, MuKDC incorporated multi-stage knowledge distillation to generate coherent supplementary triples, further improving the coverage of long-tail relations.Extending beyond generation quality, KGs-LLM [130] demonstrated that zero-shot prompting, even without external knowledge, can iteratively extract graph components, thereby reducing annotation costs and scaling efficiently to domain-specific datasets.While these methods validated the feasibility of text-based KGC, KGLLM [131] further showed that such reformulation enables lightweight fine-tuning of smaller LLMs like LLaMA-7B, achieving competitive triple classification and relation prediction.Addressing the persistent challenge of long-tail entities, KICGPT [133] integrated a triple-based retriever with contextual prompting, directly encoding structural cues into LLM inputs.This approach avoids additional training while yielding strong few-shot performance, thus highlighting the efficiency of retrieval-augmented prompting in KGC.</p>
<p>To mitigate the structural information loss inherent in purely text-based approaches, researchers have developed prompt encoding strategies that embed richer graph context.RL-LLM [134] applied few-shot learning with multi-prompt optimization for relation prediction, showing that even minimal labeled data can yield competitive results in e-commerce KGs.Expanding the reasoning process, GoG [135] proposed a "think-search-generate" pipeline, enabling the synthesis of new triples without additional training, which strengthens reasoning over incomplete graphs.Complementing this, LLM-KGC [121] fused ontology and graph structure directly into prompts, aligning topological and semantic information to improve inductive reasoning.Further enhancing structure-aware inference, KoPA [136] introduced a knowledge prefix adapter to inject structural signals during LLM reasoning, thereby improving logical consistency in KGC predictions.These works collectively highlight a shift from simple text reformulation toward structurally enriched prompting, aiming to bridge the gap between LLM semantic priors and graph topology.</p>
<p>Building upon these prompting and structural integration advances, recent research has optimized LLM-graph fusion for few-shot KGC, entity alignment, and robustness in heterogeneous settings.MuKDC [141] extended its earlier generation framework by incorporating multimodal knowledge and consistency evaluation, boosting performance on long-tail few-shot tasks.In parallel, LLMKG [138] demonstrated that carefully engineered prompts allow both Mixtral-8x7b and GPT-4 to perform competitively in zero-and one-shot KGC across diverse metrics.Addressing entity alignment, DIFT [139] combined lightweight models with discriminative prompts to prevent alignment drift while improving overall KGC accuracy.In handling semantic ambiguity, CP-KGC [140] leveraged contextually adaptive prompts to disambiguate polysemous entities, particularly in quantized LLMs.Moving toward deeper structural integration, GLTW [128] fused Graph Transformers with LLMs to jointly capture local and global patterns, while GS-KGC [123] employed subgraph-based question answering with negative sampling to enhance missing triple identification.Together, these frameworks signal a transition from isolated prompt design to holistic, multimodal, and structure-aware LLM-graph systems.</p>
<p>Overall, LLMs bring in a semantic understanding of the KG elements, which is valuable because KGs often have textual labels for entities/relations that carry meaning.LLMs leverage this to make more informed predictions, especially when data is sparse or the pattern is not purely structural but semantic.</p>
<p>Evaluation for Handling Incompleteness in Graphs</p>
<p>We summarize the existing evaluation pipeline of LLM-based methods for incomplete graph learning, covering benchmark datasets, evaluation metrics, and downstream tasks (see Table 1).</p>
<p>Across the reviewed methods, commonly used benchmark datasets include Cora [81], Citeseer [85], PubMed [82], and ogbn-arxiv [83] for node classification tasks.These are citation networks where each node is a document with associated textual attributes.For large-scale knowledge graph tasks, datasets such as FB15k-237 [94], WN18RR [124], Wiki-data5M [129], NELL [142], and UMLS [137] are widely adopted for link prediction and knowledge graph completion.Additionally, domain-specific or task-specific datasets are used to evaluate generalizability, including Amazon-Clothing and Amazon-Sports [100] for recommendation and link prediction, PeMS03/04/07 [88] for spatiotemporal forecasting, TQA/WTQ/TabFact [109,112,113] for table understanding, and GrailQA/WebQSP/KQA Pro [115,116,117] for knowledge graph question answering.Some works also utilize synthetic graphs [92,135] to test reasoning capabilities in controlled environments.</p>
<p>A variety of evaluation metrics are used depending on the task.For classification, Accuracy, Macro-F1, and Micro-F1 are most common.Knowledge graph-related tasks report Mean Reciprocal Rank (MRR) and Hits@N as standard ranking metrics.Spatiotemporal forecasting relies on MAE, RMSE, and MAPE, while generative tasks such as question answering are evaluated using Exact Match, BLEU-4, and ROUGE-L.</p>
<p>The surveyed models address a wide range of downstream tasks, including node classification, link prediction, knowledge graph completion, anomaly detection, graph reasoning, question answering over knowledge graphs, and spatiotemporal forecasting.This diversity highlights the versatility of LLMs in handling incomplete graph data across domains and tasks.</p>
<p>Summary of Incompleteness</p>
<p>Across robust learning, few-shot learning, and KGC, the common thread is that LLMs serve as knowledge-infusers and intelligent guessers for what is missing in the graph.They help create a more complete picture by either filling in data directly or guiding the graph model on where to look.Empirically, studies report that incorporating LLM-generated features or suggestions leads to substantial gains in tasks like node classification with missing features, link prediction with sparse edges, and KG completion benchmarks.As LLMs continue to improve, especially in domain-specific knowledge, we expect their role in handling graph incompleteness to grow.</p>
<p>Imbalance in Graphs</p>
<p>Graph data often exhibits imbalance that can severely affect the performance, particularly in tasks such as node classification (as shown in Figure 6).This imbalance typically complicates the learning process as models tend to be biased toward the majority class or low-degree nodes, resulting in poor generalization for underrepresented elements.To address this, techniques such as resampling, re-weighting, and graph-based regularization have been proposed to alleviate the effects of data imbalance.However, these approaches often require domain-specific adjustments and may not always yield optimal results.</p>
<p>Node Class</p>
<p>Class Imbalance</p>
<p>Node Degree  Recent advances in LLMs offer promising solutions to the challenges posed by imbalanced graph data.LLMs, with their ability to process and generate semantically rich representations, can be leveraged to enrich the graph's feature space, enabling more nuanced and balanced learning.</p>
<p>Structure Imbalance
Majority Class Minority Class</p>
<p>Class-Imbalanced Graph Learning Structure-Imbalanced Graph Learning</p>
<p>This section reviews the application of traditional methods and LLMs for learning from imbalanced graph data.The literature is categorized into two key research areas: (1) Class Imbalance and (2) Structure Imbalance.The relevant references and categorization are presented in Table 2.</p>
<p>Class-Imbalanced Graph Learning</p>
<p>Class imbalance refers to scenarios where the sample size of certain categories is significantly smaller than others.For instance, in social networks, ordinary users vastly outnumber influential "key opinion leaders", while in fraud detection systems, legitimate transactions overwhelmingly dominate over fraudulent ones.Traditional classifiers, including GNNs, often exhibit a bias toward predicting majority-class samples, resulting in suboptimal recognition accuracy for minority classes [19,16].Furthermore, GNNs update node representations by aggregating neighborhood information.Minority-class nodes risk being structurally homogenized or misrepresented when their local neighborhoods are dominated by majority-class nodes, leading to a propagation of representation bias across the graph.</p>
<p>(1) Traditional Methods Traditional methods for addressing class imbalance in graph data primarily include clustering, contrastive learning, data augmentation, and innovative loss function.Most articles combine one or more of the above techniques to solve the class imbalance problem.</p>
<p>ECGN [192] proposes a cluster-aware graph neural network framework that explicitly models the graph cluster structure to balance class representations, utilizing intra-cluster contrastive loss to enhance the distinguishability of minority class nodes.Similarly, C 3 GNN [193] clusters majority class nodes into subclasses and contrasts them with the minority class, using Mixup techniques to enhance the semantic substructure representation and alleviate the class imbalance problem.These methods share a key insight: clustering helps balance the class distribution by dividing majority class samples into multiple subclasses, and contrastive learning can improve the separability of minority nodes by constructing positive and negative sample pairs.Building on this, CCGC [194] designs a clustering-based positive sample selection strategy and optimizes graph representations through hierarchical contrastive learning, reducing overfitting to the minority class.ImGCL [195] introduces a progressive balanced sampling strategy, dynamically adjusting contrastive weights based on node centrality measures, and utilizes pseudo-labeling to address topological imbalance.These works highlight how clustering and contrastive learning can be synergistically combined to tackle both structural and label imbalance.</p>
<p>Another line of work focuses on data augmentation, which involves generating diverse topologies or attribute changes for minority classes to balance the data distribution.GraphSMOTE [31], a pioneering method, interpolates minority nodes in the embedding space and predicts edges to preserve topological consistency.However, such interpolation-based methods may struggle with semantic coherence.To address this, GraphSHA [196] generates hard samples via feature mixing and employs a semi-mixed connection strategy to prevent neighbor-class encroachment, enhancing boundary discrimination.Augmentation is often paired with adaptive loss functions.For example, TAM [197] introduces topology-aware margin loss, dynamically adjusting classification boundaries based on local neighbor distributions of nodes, addressing decision bias caused by label node location shifts.</p>
<p>Beyond data generation, another research direction emphasizes directly strengthening tail-class representations and rebalancing classifiers, often through retrieval or expert-based designs.For instance, RAHNet [198] combines a retrieval module with a second-stage classifier that applies max-norm and weight decay regularization.This joint design improves representation quality while reducing head-class dominance.Building on the idea of specialized modules, CoMe [199] introduces a collaborativeexpert framework, where a general expert captures global shared patterns and a specific expert focuses on tail-class features; a collaboration regularizer further enforces complementarity, leading to more balanced performance.Extending this paradigm, KDEX [200] trains knowledge-diverse experts, each specializing in different structural or semantic patterns, and integrates their outputs via an expert allocation and fusion mechanism.Collectively, these approaches highlight how retrieval and expert-based architectures can complement generation strategies by directly enhancing the representation and robustness of tail classes.</p>
<p>In addition, HGIF [201] addresses the heterogeneous graph scenario by using invariance learning to separate semantically invariant features, enhancing the robustness of fraud detection in out-of-distribution settings.GraphSANN [202] breaks the homophily assumption and designs a topological reconstruction framework to tackle the class imbalance problem in heterogeneous graphs.</p>
<p>(2) Methods Based on LLMs LLM-based techniques offer a novel approach to address class imbalance.Researchers can overcome the limitations of traditional methods by leveraging three aspects: semantic-driven data augmentation, context-aware text understanding, and external knowledge injection.Figure 7 illustrates how LLMs can address class imbalance in graph learning by synthesizing semantically rich representations for minority class nodes.In this framework, graph neighborhoods, node attributes, and type constraints are linearized into natural language prompts, serving as the sole input modality for the LLM.This allows the model to generate new nodes or edge candidates with both semantic coherence and structural compatibility.Cora [81], PubMed [82], ogbn-arxiv [83] Accuracy Few-shot Node Classification LLM-GNN [89] Cora [81], Citeseer [85], PubMed [82], ogbn-arxiv [83], ogbn-products [83], Wikics [90] Accuracy Label-free Node Classification</p>
<p>Minority Selection</p>
<p>LLM-based Text Encoder</p>
<p>LLM-based Edge Predictor
Prompt Text Generation Edge Generation Text Extraction Frozen TrainableG2P2 [165]
Cora [81], Art [166], Industrial [166] and Music Instruments [166] Accuracy, Macro-F1</p>
<p>Zero-and Few-shot Low-resource Text Classification LA-TAG [167] Cora [81], PubMed [82], Photo [168],</p>
<p>Computer [168], and Children [168] Accuracy, Macro-F1</p>
<p>Zero-and Few-shot Low-resource Text Classification</p>
<p>GSS-Net [169]</p>
<p>Magazine Subscriptions [166], Appliances [166], Gift Cards [166] Accuracy ogbn-arXiv [83], Arxiv-2023 [86], PubMed [82], ogbn-products [83], Reddit [186], Cora [81], CiteSeer [85], Ele-Photo [168], Ele-Computers [168], Books-History [168],</p>
<p>Wikics [90], Instagram [186] Accuracy WN18RR [124], FB15k-237 [94], Wikidata5M [189] MRR, Hits@N Knowledge Graph Completion MPIKGC [190] FB15k-237 [94], WN18RR [124], FB13 [132], WN11 [132] MR, MRR, Hits@N, Accuracy</p>
<p>Knowledge Graph Completion</p>
<p>LLM4RGNN [191] Cora [81], Citeseer [85], PubMed [82], ogbn-arxiv [83], ogbn-products [83] Accuracy Improving the Adversarial Robustness First, some methods utilize the text generation capabilities of LLMs for node attribute descriptions (such as entity descriptions and label semantic expansion) to synthesize more semantically coherent features for minority class nodes.LA-TAG [167] proposes a framework for text attribute rewriting and synthesis based on LLMs.It generates minority class node descriptions compatible with graph topology through instruction tuning, solving the semantic disconnection problem in text-graph alignment of traditional methods.Similarly, LLM4NG [41] designs a node generation approach for few-shot scenarios.It utilizes the LLM decoder to generate virtual nodes with both semantic coherence and structural consistency, while applying contrastive regularization to constrain the embedding space distribution.The method in [169] also integrates graph structure understanding modules with LLM generators to achieve fine-grained synthesis of sparse data streams, dynamically balancing class distributions in realtime graph data.</p>
<p>Another approach focuses on the contextual understanding and reasoning capabilities of LLMs, mining implicit relationships of minority classes through structure-aware reasoning.Both LLM-GNN [89] and Cella [175] proposed a zero-shot classification framework based on LLM semantic reasoning.By parsing node attributes and neighbor context, they directly generate category prediction confidence and get rid of the dependence on labeled data.G2P2 [165] introduces a graph-guided pretraining prompt framework, using LLMs to generate augmented text related to graph structure, enhancing the discriminability of low-resource classes in heterogeneous spaces.Meanwhile, LLM-TIKG [176] builds a threat intelligence knowledge graph, using LLMs to extract attack pattern association rules from unstructured text, and enhancing the representation of minority class threat entities through heterogeneous graph attention.</p>
<p>To improve generalization on sparse or long-tail categories in graphs, another line of work focuses on injecting external knowledge-either from open-domain corpora or domainspecific sources-into the learning process.KICGPT [133] enriches long-tail relational reasoning by dynamically constructing structure-aware prompts with contextual knowledge, significantly improving completion performance for sparse relations.In the recommendation domain, LKPNR [180] integrates user preference reasoning from LLMs and semantic path mining to address cold-start scenarios in long-tail news recommendation.KGCD [183] augments knowledge graph completion with pseudo-triplets generated from logical rules, guiding the model to focus on underrepresented relations.Other methods, such as [170], distill open-domain knowledge from LLMs into graph encoders through hierarchical knowledge transfer, while GraphCLIP [185] leverages cross-modal alignment to construct graph-text contrastive learning objectives for tail-category enhancement.In biomedical applications, LLM-DDA [182] infuses mechanistic knowledge into graph reconstruction, improving drug repurposing predictions for rare diseases.These works demonstrate that LLMs serve not just as predictors, but as external knowledge carriers that can guide graph learning systems beyond their native data distributions.</p>
<p>In addition to the aforementioned new technologies, the introduction of LLMs has accelerated innovation in cross-domain applications.SEGA [171] uses LLMs to parse the structured elements of clinical conversation graphs, extracting depression semantic clues and social interaction patterns, enhancing the recognition of minority positive samples in mental health classification.The method in [174] proposes an LLM-enhanced sociological analysis framework based on hypergraphs, addressing behavior class imbalance in social media data through personality trait hyperedge modeling and LLM semantic deconstruction.ANLM-assInNNER [177] develops an automatic construction system for robot fault diagnosis knowledge graphs, using LLMs to generate fine-grained fault entity descriptions and balancing the data distribution of equipment status categories.LLM-SBCL [179] combines graph neural networks with LLM cognitive state modeling, improving the reliability of predicting questions on less common knowledge points through semantic enhancement of learner-question interaction graphs.</p>
<p>In summary, LLM-based approaches for class imbalance aim to even out the information content per class by generating additional data or highlighting distinguishing features, rather than just mathematically re-weighting or duplicating existing data.</p>
<p>Structure-Imbalanced Graph Learning</p>
<p>Structural imbalance refers to skewness in graph topology that can hinder learning.A typical example is a hub node vs. peripheral node issue: Hub nodes with very high degree can dominate aggregation and also often have many more training signals, whereas low-degree nodes might be ignored or get a very noisy aggregate from a single neighbor.Similarly, certain substructures might be over-represented.For example, in a molecule graph dataset for drug discovery, maybe most molecules contain a benzene ring but only a few contain a rarer motif; a GNN might mostly learn features relevant to benzene rings and be less sensitive to the rare motif, which could be critical for certain properties.</p>
<p>(1) Traditional Methods The main techniques for addressing structural imbalance in graph data can be broadly divided into two lines: debiasing-based approaches and structural enhancement-based approaches.From the debiasing perspective, several methods focus on dynamically adjusting neighbor aggregation weights to reduce the over-dominance of high-centrality nodes.For example, DegFairGNN [203] introduces a generalized degree fairness constraint that reallocates neighborhood attention weights between high-and low-degree nodes, thereby mitigating bias in GNN message passing.Extending this idea to knowledge graphs, KG-Mixup [204] analyzes degree bias in entity embeddings and proposes a degree-aware contrastive loss to balance geometric constraints between high-and low-degree entities.Together, these approaches highlight the importance of reweighting meachanisms for correcting degree-induced biases.On the other hand, structural enhancement approaches aim to enrich the representation capacity of underrepresented nodes or substructures.SOLT-GNN [205] improves graph classification by using a size-aware hierarchical pooling strategy that balances representation distributions through subgraph cropping and feature decoupling.Building on the idea of subgraph augmentation, SAILOR [206] strengthens the visibility of low-degree nodes in the global topology via k-hop subgraph expansion and adversarial edge generation.Both methods demonstrate that explicitly enhancing substructures can effectively alleviate long-tailed imbalance in graphs.A complementary line of work tackles imbalance from the perspective of global structure reconstruction and regularization.HiRe [207] develops a hierarchical relational meta-learning framework with a meta-path-guided negative sampling mechanism, balancing the structural coverage density between head and tail relations in knowledge graphs.Similarly, QTIAH-GNN [208] introduces a heterogeneous GNN that jointly considers quantity imbalance and topological imbalance, using meta-relation-specific neighbor sampling together with topological entropy regularization.These methods illustrate how global structural modeling can address both numerical and topological imbalance.</p>
<p>Although existing methods have alleviated the problem of graph structural imbalance to a certain extent, they still have limitations.They often rely on manually designed constraint rules, which leads to limited generalization ability and difficulty in adapting to complex and changing graph structural deviations.At the same time, these methods tend to ignore the semantic associations of the global topology, and usually adopt static adjustment strategies, which cannot dynamically adapt to the structural evolution patterns implicit in the graph [19,16].</p>
<p>(2) Methods Based on LLMs LLM-based methods offer a new paradigm for addressing structural imbalance in graphs by generating semantically meaningful edits-such as contextaware virtual edges or subgraph structures-that enhance connectivity for long-tail nodes (Figure 8).Unlike traditional GNNbased approaches, which learn structural patterns primarily from in-graph statistical distributions, LLM-based frameworks integrate rich prior semantic knowledge obtained from large-scale pretraining with graph-specific structures.For example, SATKGC [188] constructs subgraph-aware embeddings and uses contrastive losses to reinforce coherence among tail relations.MPIKGC [190] integrates LLMs for semantic enhancement, structural correction, and logic-driven edge generation, effectively mitigating relational imbalance.LLM4RGNN [191] transfers GPT-4's reasoning capacity to detect and recover from malicious structural edits in a lightweight graph refinement framework.Complementing these, GraphEdit</p>
<p>Node Pairs</p>
<p>LLM-based Edge Predictor</p>
<p>LLM-based Text Encoder
Text Node Embedding Structure Refinement Optimized Graph Frozen Trainable
[187] proposes an iterative generate-and-validate loop, where LLMs suggest candidate edges which are then filtered by topological validators to ensure semantic and structural consistency.These approaches demonstrate that, when graphs are represented in text-compatible forms, LLMs can inject new connections that are not arbitrary, but semantically aligned-offering principled structure augmentation for underrepresented classes.</p>
<p>Evaluation for Handling Imbalance in Graphs</p>
<p>We summarize the evaluation protocols for LLM-based methods addressing graph imbalance, covering key datasets, metrics, and downstream tasks (see Table 2).Commonly used datasets include Cora [81], Citeseer [85], PubMed [82], and ogbn-arxiv [83] for node classification under class imbalance, and FB15k-237, WN18RR [94,124] for knowledge graph completion in low-resource settings.Domain-specific benchmarks such as threat-dataset [176] and Twitter-HetDrug [178] are also adopted for specialized applications like fraud detection and mental health analysis.</p>
<p>Standard evaluation metrics vary by task: Accuracy and Macro-F1 are widely used for classification, while MRR and Hits@N are reported for link prediction and knowledge graph completion.For recommendation and retrieval tasks, AUC, MRR, and nDCG are common.Downstream tasks include fewshot node classification, sentiment analysis, knowledge graph construction, and drug repositioning, demonstrating the adaptability of LLMs in balancing both label and structural distributions.These evaluations highlight the effectiveness of LLMbased methods in enhancing model performance on underrepresented classes and nodes.</p>
<p>Summary of Imbalance</p>
<p>Traditional graph imbalance methods mitigate bias via resampling, cost adjustment, or graph augmentation, but they lack external semantic context.LLM-integrated methods aim to generate new graph content or features that specifically bolster the minority classes or structures.As reported in recent studies, using LLMs in this way can significantly improve classification performance on long-tail classes and ensure that even structurally unique nodes are recognized.Essentially, LLMs function as an intelligent oversampling mechanism: instead of naive duplication, they produce novel yet relevant samples in data space.The result is often more balanced training data for the graph model and better generalization to minority cases.</p>
<p>Cross-Domain Heterogeneity in Graphs</p>
<p>Real-world graph data is often collected from multiple source domains [209,210,211], which can exhibit significant heterogeneity, referring to extreme disparities in both attributes and structural patterns.This heterogeneity typically arises when the graph data is collected or integrated from domains with inconsistent data modalities or distributions.Cross-domain heterogeneity introduces significant challenges for graph analysis and modeling, as the severe disparities make it difficult to unify these data into a common representation space [23,212] and even hinder the detection of valuable and transferable features that can generalize well throughout the graph [25,26].Moreover, addressing cross-domain heterogeneity is a key prerequisite for building graph foundation models for more generalizable and scalable graph learning across diverse real-world applications [213].</p>
<p>While traditional graph domain adaptation methods [214] and multimodal graph learning methods [215] can help bridge the gap between different distributions or modalities from different graph domains, they typically require large-scale training datasets from different domains and cannot generalize to some unseen domains during training.In contrast, LLMs, with their superior comprehension and generalization abilities, can extract valuable and transferable semantic and structural features within a unified representation space without training from scratch, effectively addressing the challenges posed by cross-domain heterogeneity in graph learning [216].</p>
<p>In this section, as illustrated in Figure 9, we categorize crossdomain heterogeneity in graph data into three types: withinmodality attribute heterogeneity, cross-modality attribute heterogeneity, and structural heterogeneity.Since LLMs are specifically designed for modeling textual modality, our focus on within-modality attribute heterogeneity primarily centers around textual attributes.For each type, we first review traditional graph learning methods, analyzing their strengths and limitations, and then discuss recent research leveraging LLMs to unify graph data and address the corresponding challenges.The relevant references and categorization are summarized in Table 3.</p>
<p>Text-Attributed Graph Learning</p>
<p>Textual attributes are common in real-world graph data, such as the abstract of each paper in a co-citation network [273] or item descriptions in a recommendation network [217].Although these textual attributes appear in the same modality, they can demonstrate significant distribution heterogeneity due to different sources.For example, textual attributes such as clinical notes could come from different healthcare providers, each with their own writing style, vocabulary, and context [274].Similarly, clinical notes might be informal and easy to understand for patients, while medical codes should be more formal and standardized.Moreover, the textual attributes can appear in different languages [275], further increasing the heterogeneity.The distribution heterogeneity in textual attributes can lead to inconsistent semantic representations, making it difficult for graph learning methods to effectively capture transferable features.</p>
<p>(1) Traditional Methods for Textual Attribute Modeling To handle the heterogeneity in textual data, early data integration methods extract structured information from unstructured textual inputs, thereby reducing variability across sources and producing unified features for downstream tasks [276,277,278].However, these approaches require manual design of data schemes and training extraction models tailored to specific applications.In contrast, statistical methods, such as Bag of Words and TF-IDF [279], have been introduced to automatically generate unified feature vectors from text without relying on domain-specific design.These feature vectors are often used as inputs for graph learning methods, such as GNNs [55, 56,57], which further incorporate structural information from the graph to obtain more effective node representations for addressing downstream tasks.For instance, TADW [280] approximates DeepWalk [47] using matrix factorization, where the TF-IDF feature vectors of node textual attributes serve as the initial feature matrix.Paper2vec [281] utilizes learnable Word2vec [282] text embeddings as initial node features, which are then trained by predicting whether two nodes belong to the same neighborhood in the graph.While these early statistical methods or shallow models offer solutions for mapping heterogeneous textual attributes into a unified space, their limited expressiveness cannot effectively capture complex features in these textual attributes.Moreover, these methods adopt text embeddings as fixed initial node features rather than integrating them with structural learning, limiting the potential of leveraging graph topology to enhance the textual attributes representation learning [273].</p>
<p>Several advanced methods explore approaches for modeling textual attributes that can better integrate with graph structures, aiming to improve the representation learning of both textual attributes and graph structure data simultaneously.Some approaches, like GIANT [283] and its extension E2EG [284], leverage self-supervised learning to force text embeddings to encode graph-dependent information, which is trained by predicting a node's neighbors from its text.Other methods focus on creating joint architectures.For example, GraphFormers [273] proposes a GNN-nested architecture where the transformer-based textual embedding modules and the GNN modules are nested and trained together.Heterformer [285] introduces virtual neighbor tokens that capture information from both text-rich and textless neighbors, allowing the model to simultaneously consider textual semantic information and structural information to embed the textual node attributes.To capture relation-specific signals in heterogeneous graphs, METAG [286] employs a single pretrained language model to learn multiplex text representations, which can effectively handle the diverse semantic relations while maintaining high parameter efficiency.IMDB [223], DBLP [223], ACM [223], Wiki-CS [90], IMDB-RIR [222], DBLP-RID [222] Macro-F1, Micro-F1 Node Classification</p>
<p>Graph Foundation Model</p>
<p>OFA [213] Cora [81], PubMed [82], ogbn-arxiv [83], Wiki-CS [90], MOLHIV [224], MOLPCBA [224], FB15K237 [94], WN18RR [124], ChEMBL [225] Accuracy, AUC, AUPR Node Classification, Link Prediction, Graph Classification</p>
<p>UniGraph [226] Cora [81], PubMed [82], ogbn-arxiv [83], ogbn-products [83], Wiki-CS [90], FB15K237 [94], WN18RR [124] Cora-CA-Text [228], Cora-CC-Text [228], Pubmed-CA-Text [228], Pubmed-CC-Text [228],</p>
<p>AminerText [228], Arxiv-Text [228], Movielens-Text [228], IMDB-Text [228],</p>
<p>GoodBook-Text [228], PPI-Text [228] Accuracy Node Classification Multimodal Attributed Graph Learning</p>
<p>MLLM-based Multimodal Alignment</p>
<p>LLMRec [217] MovieLens [218], Netflix [219] Recall, NDCG, Precision Item Recommendation MAGB [24] Cora [81], Wiki-CS [90], Ele-Photo [168], Flickr [229], Movies [24], Toys [24], Grocery [24], Reddit-S [24], Reddit-M [24] Accuracy, F1 Node Classification</p>
<p>Graph-Enhanced Multimodal Alignment</p>
<p>MMGL [230] WikiWeb2M [231] BLEU-4, ROUGE-L, CIDEr Section Summarization</p>
<p>GraphAdapter [232] ImageNet [233], StandfordCars [234], UCF101 [235], Caltech101 [236], Flowers102 [237], SUN397 [238], DTD [239], EuroSAT [240], FGVCAircraft [241], OxfordPets [242], Food101 [243] Accuracy</p>
<p>Image Classification</p>
<p>TouchUp-G [244] ogbn-arxiv [83], ogbn-products [83], Books [244], Amazon-CP [244] MRR, Hits@N, Accuracy link prediction, node classification
UniGraph2 [245]
Cora [81], PubMed [82], ogbn-arxiv [83], ogbn-papers100M [83], ogbn-products [83], Wiki-CS [90], FB15K237 [94], WN18RR [124], Amazon-Sports [246], Amazon-Cloth [246], Goodreads-LP [246], Goodreads-NC [246], Ele-Fashion [246], WikiWeb2M [231] Accuracy, BLEU-</p>
<p>Attributed Graph Textualization</p>
<p>GraphText [252] Cora [81], Citeseer [85], Texas [253], Wisconsin [253],</p>
<p>Cornell [253] Accuracy Node Classification</p>
<p>WalkLM [254] PubMed [82], MIMIC-III [221] Macro-F1, Micro-F1, AUC, MRR</p>
<p>Node Classification, Link Prediction</p>
<p>Path-LLM [255] Cora [81], Citeseer [85], PubMed [82], ogbn-arxiv [83] Macro-F1, Micro-F1, AUC, Accuracy Node Classification, Link Prediction</p>
<p>InstructGLM [256] Cora [81], PubMed [82], ogbn-arxiv [83] Accuracy Node Classification, Link Prediction</p>
<p>MuseGraph [257] Cora [81], ogbn-arxiv [83], MIMIC-III [221], AGENDA [258], WebNLG [259] Macro-F1, Micro-F1, Weighted-F1, BLEU-4, METEOR, ROUGE-L, CHRF++ Node Classification, Graph-to-Text Generation Graph-LLM [260] Cora [81], Citeseer [85], PubMed [82] IMDB [223], DBLP [223], ACM [223] Macro-F1, Micro-F1, AUC Node Classification</p>
<p>Although these advanced methods can embed heterogeneous textual node attributes into a unified representation space, their dependence on complex deep architectures requires a sufficient amount of training data.In applications with limited data availability, the model may fail to capture the valuable features from excessive heterogeneous textual attributes, and the learned features might fail to generalize effectively to textual attributes from other sources.</p>
<p>(2) LLM-based Methods for Textual Attribute Modeling LLMs, as language models, are naturally suited for modeling attributes represented in textual modality.With advanced language comprehension and generalization capabilities, LLMs can effectively capture the semantic meanings from heterogeneous textual attributes across diverse source domains and project them into a unified representation space that preserves the semantic information.</p>
<p>A number of methods leverage the use of LLMs to generate unified textual descriptions, which can be fed into a trainable smaller language model to extract task-specific embeddings, as depicted in Figure 10.TAPE [86] leverages the powerful language comprehension capabilities of LLMs to predict the category of a node based on its textual attributes and generate explanations for the prediction, which can be regarded as enhanced and aligned textual outputs.These aligned textual outputs are fed into a smaller language model to generate node feature vectors.As shown in [216], the domain shift is reduced in the LLM-enhanced text compared to the original textual attributes.</p>
<p>LLM LM</p>
<p>Initial features</p>
<p>Initial features</p>
<p>Unified Unified</p>
<p>Frozen Trainable Attributes from different domains are first transformed into unified textual descriptions by an LLM, then converted into unified embeddings using a trainable small LM to generate initial features for graph learning methods.</p>
<p>Similarly, LLMRec [217] leverages LLMs to summarize user profiles and item attributes within a recommendation network, reducing the heterogeneity of the original attributes and producing aligned textual representations.These representations are then reprocessed by LLMs to generate unified embedding vectors for both users and items.Instead of generating unified textual descriptions, MINGLE [220] utilizes LLMs to map clinical notes and medical codes into a unified embedding space and uses these unified embedding vectors as initial node features for training a hypergraph neural network.GHGRL [222] utilizes LLMs to unify textual attributes on heterogeneous graphs based on format types and content types.Specifically, it first summarizes types information of all nodes using LLMs, which are then used to generate attribute summaries for each node based on the predicted types.These generated attribute summaries are then fed into a language model to generate unified feature vectors for training a graph neural network.By transforming categorical and numerical features into textual attributes with language-based descriptions, LLMs can extend their ability to handle the heterogeneity in these structured attributes, which do not naturally belong to the textual modality.For example, in molecular graph modeling, node attributes such as atom types or properties are typically represented as categorical or numerical values.By converting these attributes into descriptive text and enriching with domain-specific explanations, LLMs can effectively understand their semantic meaning and unify them within a common representation space.Both OFA [213] and UniGraph [226] address attribute heterogeneity by constructing text-attributed graphs.These models enhance textual attributes by incorporating additional semantically rich contextual descriptions.Furthermore, they leverage domain knowledge to enrich non-textual attributes with textual representations, enabling LLMs to process diverse attributes in a unified manner and generate consistent embedding vectors.BooG [227] follows a similar approach but further employs a contrastive learning-based pretraining objective, which enhances the ability to learn expressive representations and generalize across different domains and downstream tasks.Similarly, Hyper-FM [228] leverages a language model to extract semantic features from cross-domain textual attributes on hypergraphs and integrates structural information through hierarchical high-order neighbor prediction.</p>
<p>These LLM-based methods utilize LLMs to comprehend semantic information from heterogeneous textual attributes and generate unified textual descriptions or vector embeddings for downstream graph learning methods.The unified and highquality outputs from LLMs greatly simplify the downstream learning process and enhance performance by addressing the challenges posed by cross-domain heterogeneity in textual attributes.</p>
<p>Multimodal Attributed Graph Learning</p>
<p>Compared to attributes within the same modality, multimodal attributes encompass information from various modalities, such as text, images, audio, and videos.These multimodal attributes offer a more comprehensive and enriched representation of the underlying data.By capturing different aspects of a node through diverse inputs from different sources, multimodal attributes enhance the contextual understanding and provide complementary information that single-modal data may lack [287,288,289].However, this diversity also introduces additional challenges, as attribute heterogeneity across different domains manifests not only in data distribution but also in data formats.This variability complicates the unified processing of these attributes, requiring effective fusion methods to integrate information across different modalities [24,290].</p>
<p>(1) Traditional Methods for Multimodal Attribute Modeling Traditional methods aim to learn alignment or fusion patterns across different modalities by training models to capture the underlying relationships between heterogeneous multimodal attributes.MMGCN [289] learns a multimodal graph convolution network on a user-item bipartite graph to learn modal-specific representations of users and micro-videos to better capture user preferences in different modalities.scMoGNN [291] utilizes different GNNs to learn representations of each cell-feature bipartite graph in different modalities, and finally concatenates these representations to fuse information from different modalities.</p>
<p>[292] and [293] introduce an attention mechanism in graph neural networks to dynamically capture the importance of information from different modalities.To reduce the need for large amounts of labeled data and improve robustness, contrastive learning methods are proposed for modeling multimodal graph data.Joyful [294] designs a global contextual fusion module and a specific modalities fusion module to capture information at different scales, and then concatenates the representations from both modules to generate a unified representation vector for each node.These modules are trained by comparing positive and negative pairs in the corrupted graphs through edge perturbation and random masking.FormNetV2 [295] proposes a centralized multimodal graph contrastive learning strategy to learn fused representations from different modalities in one loss.HGraph-CL [296] introduces a hierarchical graph contrastive learning framework that builds intra-modal and inter-modal graphs, leveraging graph augmentations and contrastive learning to capture complex sentiment relations within and across different modalities.</p>
<p>While these traditional methods can align or fuse multimodal attributes in graph data, they still rely on abundant data to train the model and cannot easily generalize to an unseen domain.Compared to LLMs, these traditional methods are less adaptable to diverse data distributions and struggle to leverage pretrained knowledge for more efficient generalization.</p>
<p>(2) LLM-based Methods for Multimodal Attribute Modeling Although LLMs are specifically designed for understanding natural languages and not naturally suited for handling multimodal attributes, they can be combined or aligned with models for other modalities.Leveraging their superior generalization ability, LLM-based methods eliminate the need for large amounts of application-specific training data, opening up new possibilities for addressing attribute heterogeneity across multimodal source domains.</p>
<p>Recent research on MLLMs focuses on developing advanced methods to align LLMs with models from other modalities [297,298,299,300,301].Although these methods are not specifically designed for graph data, they can be used as powerful preprocessing tools for aligning multimodal attributes and generating unified representations.For example, by unifying textual and visual side information through a pretrained model Clip-ViT [302], LLMRec [217] effectively mitigates the multimodal heterogeneity and enhances the node features in the recommendation network, leading to significant performance improvements.MAGB [24] conducts experiments on a set of large multimodal attributed graph datasets, which demonstrate that MLLMs can effectively alleviate the biases from cross-domain multimodal heterogeneity.Recent approaches align encoders for different modalities by leveraging graph structures, making them more naturally suited for graph learning tasks.MMGL [230] utilizes LLMs and image encoders with adapter layers to embed text and image attributes, respectively.These embeddings are then combined with graph positional encodings to capture graph structure information and finally fed into LLMs to generate the corresponding outputs.GraphAdapter [232] introduces GNN-based adaptors for encoders of different modalities, which can better align these encoders based on the graph structure information.TouchUp-G [244] improves node features obtained from pretrained models of different modalities by adapting them to the graph structure, using a new metric called feature homophily to quantify the correlation between the graph and node features, which enhances GNN performance across different tasks and data modalities.Following this direction, UniGraph2</p>
<p>[245] leverages modality-specific encoders alongside a GNN and an MoE module to effectively unify multimodal features while preserving the underlying graph structure.</p>
<p>By aligning LLMs with models for different modalities, these methods can simultaneously understand the heterogeneity across modalities and map the multimodal attributes into a unified embedding space.These unified attribute representations simplify the downstream graph learning process and improve the performance of multi-modal graph learning tasks.</p>
<p>Structural Heterogeneous Graph Learning</p>
<p>Graph structures capture essential connectivity patterns that are crucial for real-world applications.However, graphs constructed from heterogeneous source domains can exhibit excessive heterogeneity in structural patterns.The structural heterogeneity stems from the inherent biases of distinct structural patterns across different source domains, which cannot generalize throughout the graph and may obscure truly valuable structural information.For instance, road network data collected from various cities always exhibit structural heterogeneity, where data from some cities may exhibit grid-like structural patterns and data from others have radial configurations [21].Such heterogeneity hinders the models from capturing underlying generalizable structural features and therefore limits their performance on real-world applications like traffic flow prediction [303,304] or route planning [305].Traditional graph domain adaptation methods rely on training data from different domains and cannot generalize to unseen domains.In contrast, LLMs, with their superior semantic understanding and generalization abilities, can understand different graph structural patterns in a zero-shot manner [271], providing new opportunities for mitigating structural heterogeneity in different graph data from unseen domains.</p>
<p>(1) Traditional Methods for Heterogeneous Structure Modeling While many graph learning methods struggle to generalize across structures from different source domains with different data distributions, recent research investigates how to mitigate this issue by adapting a learned model from a source domain to a target domain.These graph adaptation methods provide solutions for eliminating the biases across different domains and help capture unified and generalizable embeddings for graph structures with excessive heterogeneity.</p>
<p>Earlier methods for graph domain adaptation, including DANE [23] and ACDNE [33], employ shared-weight GNNs  to align the embedding spaces of different graphs and utilize a least squares generative adversarial network to regularize the distribution alignment, ensuring that the learned unified embeddings do not include domain-specific information.Different from these adversarial regularization methods, some approaches focus on directly aligning distributions from different domains using different metrics.For example, SR-GNN [26] addresses the domain shifts by regularizing the hidden layer distributions using central moment discrepancy.GDA-SpecReg [306] combines optimal transport theory and graph filter theory to derive a theoretical bound for graph domain adaptation.Based on this bound, the method utilizes Wasserstein-1 distance to regularize the node representation distributions.StruRW [34] proposes an effective approach to reduce conditional structural shifts by re-weighting the edges in the source graph.While most methods align distributions for spatial representations, DASGA [307] introduces a spectral-based approach by aligning the Fourier bases of the source and target graphs, which ensures that the label functions in both domains have similar coefficients in their respective bases.While these traditional methods demonstrate superiority in mitigating cross-domain structural heterogeneity, they require separate source and target domain data for training, which limits their application in scenarios without such training data or with a large number of domains.Additionally, traditional GNNs typically rely on fixed k-hop neighborhoods, limiting their ability to generalize across highly diverse connectivity patterns from different source domains with varying hop distances.Furthermore, these methods are inherently dependent on training data, which constrains their ability to handle structures that deviate from the patterns observed during training [308].</p>
<p>(2) LLM-based Methods for Heterogeneous Structure Modeling Different from traditional graph learning methods, LLM-based methods provide a more flexible alternative by representing graph structures through unified text-based descriptions, which are not restricted by a fixed k-hop structural neighborhood assumption.Instead of relying on predefined graph processing mechanisms, LLMs, with superior comprehension and generalization abilities, can understand unseen structural patterns in different domains.Since LLMs are designed to process natural language inputs, a direct way to bridge the gap between graph structures and the input format of LLMs is to transform structural information into textual descriptions, as depicted in Figure 11(a).Here we survey recent advances in structure-totext transformation methods that enable LLMs to understand heterogeneous structural patterns.Although some methods are not specifically designed for mitigating cross-domain structural heterogeneity, they provide strong potential for addressing the associated challenges.</p>
<p>Recent works have explored how to effectively transform graph structures into text to enable better understanding and reasoning by LLMs.Early methods [42,92,247,248] experimented with existing graph textualization techniques, such as structure description (like node sequences and edge sequences) and formal languages (like graph markup language [309]).While LLMs can comprehend graph structures from these natural language descriptions, their results demonstrate that the choice of different textualization techniques can significantly affect the performance of LLMs on different graph tasks.Different from previous methods that only consider natural language format for representing graph structures, GITA [250] transforms graph structures into both text and images to obtain a better understanding of structural information from different modalities, which are then fed into a Vision-Language Model (VLM) for addressing graph tasks.LLM4Hypergraph [251] designs low-order and high-order structure languages to transform hypergraph structures into natural languages.These studies have significantly advanced the exploration of structure-to-text approaches for LLMs to comprehend graph topological structures.</p>
<p>Beyond merely understanding topological structures, textual descriptions can also incorporate the attributes of nodes and edges, providing a richer context that enables LLMs to capture the intricate features of the graph structure.GraphText [252] constructs a graph-syntax tree to preserve the hierarchical structure and traverses the tree structure to generate a graph text sequence, where the node attributes are incorporated in the description of leaf nodes.WalkLM [254] generates paths by attributed random walks and textualizes the paths into natural languages for processing in LLMs.To effectively capture cross-group connections while minimizing noisy nodes, Path-LLM [255] employs shortest paths to generate structural sequences.InstructGLM [256] describes graph structures using neighbors in different scales and utilizes instruction tuning to finetune an LLM to better perform graph tasks.The textual node attributes are concatenated with the node index to better incorporate semantic information.MuseGraph [257] incorporates neighbors and paths together to textualize graph structures, which can capture both local connectivity and complex path-based relationships between nodes.</p>
<p>Graph-LLM [260] transforms text-attributed graphs into natural languages and demonstrates that LLMs can understand the graph structures from language descriptions and demonstrate remarkable zero-shot performance on graph tasks.</p>
<p>By converting diverse graph structures into natural language descriptions, these structure-to-text transformation methods enable LLMs to interpret and reason across varying connectivity patterns in a unified manner.These approaches allow LLMs to leverage their pretrained linguistic knowledge to identify valuable relational patterns and facilitate more flexible reasoning across different graph topologies with excessive heterogeneity.</p>
<p>(3) Hybrid Methods for Heterogeneous Structure Modeling While structure-to-text transformation methods offer a straightforward way to bridge the gap between graph structures and the input format of LLMs, they heavily depend on the language comprehension ability of LLMs to correctly interpret the input graph structures.Some recent works explore learning explicit structure representations using traditional graph learning methods like GNNs and aligning these representations with the token space of LLMs, as depicted in Figure 11(b).These hybrid methods leverage the structural modeling capabilities of traditional graph learning methods alongside the comprehension and reasoning strengths of LLMs for more effective graph structure modeling.</p>
<p>Advanced methods bridge the gap between graph structures and LLMs by converting graph data into graph tokens that LLMs can understand.GNP [261] and GraphToken [268] both use a GNN to encode graph structure and then employ a projector to map these embeddings into the token space of the LLM, allowing them to be processed alongside regular text.LLaGA [270] takes a different approach by reorganizing graph nodes into structure-aware sequences before projecting them, which helps the model maintain its general-purpose nature.TEA-GLM [271] refines this alignment with feature-wise contrastive learning, using principal components from the token space to precisely map GNN representations.GraphGPT [269] introduces a dual-stage instruction tuning framework that directly aligns graph tokens with LLMs using both self-supervised and task-specific instruction tuning.To generate graph tokens for heterogeneous graphs, HiGPT [272] introduces an in-context heterogeneous graph tokenizer that encodes diverse node and edge types by using a language-based parameterized heterogeneity projector, dynamically generating graph tokens that represent the heterogeneous semantic relationships.</p>
<p>These hybrid methods incorporate structure representations from traditional graph learning methods to facilitate the structure comprehension ability of LLMs, providing more effective solutions for mitigating the challenges from structural heterogeneity.Additionally, the use of compact graph tokens reduces input size and enables LLMs to process larger structures within their context window, which is crucial for understanding heterogeneous structural patterns across varying scales.</p>
<p>Evaluation for Handling Cross-Domain Heterogeneity in</p>
<p>Graphs We summarize the existing evaluation settings for LLM-based methods for handling cross-domain heterogeneity in graphs, cov-ering benchmark datasets, evaluation metrics, and downstream tasks (see Table 3).</p>
<p>Evaluations typically use well-established traditional graph benchmark datasets.These datasets span a wide range of domains, including citation networks (e.g., Cora [81], Citeseer [85], and PubMed [82]), biomedical graphs (e.g., MIMIC-III [221], MOLHIV [224], and MOLPCBA [224]), commercial and recommendation networks (e.g., MovieLens [218], Netflix [219], and various Amazon product datasets [244]), as well as knowledge graphs (e.g., FB15K237 [94] and WN18RR [124]).While most of these datasets are standard in graph learning, cross-domain evaluations differ from conventional setups.Instead of training and testing within the same domain, LLM-based methods are often trained on source-domain datasets (or without further training) and tested on unseen target-domain datasets [252,250,227].In some cases, few-shot settings are also utilized to assess generalization ability when only a limited number of labeled examples in the target domain are available [226,213].</p>
<p>The choice of evaluation metrics typically depends on the task type.Classification tasks adopt Accuracy, Macro-F1, Micro-F1, or AUC, ranking-oriented tasks like recommendation or link prediction use MRR, NDCG, or Hits@N, and generation tasks such as graph-to-text generation rely on BLEU-4, ROUGE-L, CIDEr, METEOR, or CHRF++.Metrics are usually evaluated under zero-shot or few-shot settings to reflect cross-domain generalization.However, traditional metrics may miss cases where outputs are semantically correct but inconsistent in domain-specific format or style.LLMs, in contrast, can capture nuanced correctness across domains based on their strong language comprehension ability, positioning LLM-as-a-Judge [310] a promising direction for future evaluation.The downstream tasks range from node-level and edge-level tasks to graph-level tasks, including node classification, link prediction, and graph classification, as well as more complex settings like recommendation, question answering, section summarization, and graph-to-text generation.This broad coverage sufficiently evaluates the cross-domain generalization ability of LLM-based methods across diverse task types and application scenarios.</p>
<p>Summary of Heterogeneity</p>
<p>LLMs, pretrained on diverse datasets, are inherently capable of integrating information from various sources into a unified semantic space.The reviewed studies show that LLM-integrated models can transfer knowledge across different graph domains without the need for domain-specific retraining.By leveraging the generalizability of LLMs, these models are able to mitigate different forms of cross-domain heterogeneity, including textual attribute heterogeneity, multimodal attribute heterogeneity, and structural heterogeneity.As a result, they demonstrate superior performance in addressing cross-domain heterogeneity, particularly in scenarios where traditional methods are ineffective or inapplicable due to the lack of sufficient training data.Experiments in these papers show improved performance on cross-domain node classification, link prediction, and graph classification tasks when LLMs are used to either encode heterogeneous content or assist in aligning representations.Many real-world systems, from social networks to knowledge bases, are best represented as graphs whose structure and attributes change over time (see Figure 12).This constant evolution implies a shifting distribution, where patterns observed in the past may not hold true in the future.While gradual evolution poses modeling challenges, the situation becomes significant when the graph undergoes rapid, large-scale, or highly unpredictable changes that alter the graph's characteristics and create significant challenges for model training and inference [311].</p>
<p>Dynamic Instability in Graphs</p>
<p>Dynamic graphs' instability is challenging to model due to their temporal and structural variability [312].In static settings, models learn stable representations from collected graph data.In contrast, dynamic graphs experience frequent changes in nodes, edges, and attributes.Models struggle to generalize without adaptive mechanisms when faced with structural shifts or abrupt transformations [313].Before the introduction of LLMs, researchers addressed dynamic graphs using snapshotbased and incremental methods.These approaches discretized the graph into time-specific snapshots for independent training [35], or updated models as new nodes and edges appeared [29].While effective in some settings, such methods often rely on localized updates and struggle to capture long-range temporal dependencies, particularly in large-scale or rapidly changing graphs.</p>
<p>LLMs have recently emerged as a powerful new tool with the potential to address some challenges in dynamic graph learning (Figure 13).By leveraging their strengths in natural language understanding, sequential data processing, few-shot learning, and complex reasoning, LLMs offer novel ways to interpret and model graph dynamics.They can process textual information associated with nodes or edges and understand the semantic context driving structural changes.This infusion of semantic reasoning capabilities opens promising research routes for creating more robust and adaptive models capable of navigating the complexities of dynamic instability.The relevant references and categorization are presented in Table 4. WIKI [315], YAGO [316], ICEWS14 [126], ICEWS18 [317] MRR, Hits@N Link Prediction zrLLM [318] ICEWS [319], ACLED [320] MRR, Hits@N Link Prediction CoH [321] ICEWS14 [126], ICEWS18 [317], ICEWS05-15 [126] MRR, Hits@N Link Prediction TG-LLM [322] TGQA [323], TimeQA [324], TempReason [325] F1, Accuracy, Exact Match Temporal Reasoning</p>
<p>LLM4DyG [326] Enron [327], DBLP [328], Flights [329] Accuracy, F1, Recall Spatial-Temporal Reasoning, Graph Reasoning and Querying, Link Prediction</p>
<p>QA &amp; Interpretability</p>
<p>TimeR4 [330] MULTITQ [331], TimeQuestions [332] Hits@N Temporal Knowledge Graph Question Answering</p>
<p>GenTKGQA [333] CronQuestion [334], TimeQuestions [332] Hits@N Temporal Knowledge Graph Question Answering Unveiling LLMs [335] FEVER [336], CLIMATE-FEVER [337] Precision, Recall, F1, ROC AUC, Accuracy Claim Verification</p>
<p>Generating and Updating</p>
<p>Generating Structures</p>
<p>FinDKG [338] WIKI [315], YAGO [316], ICEWS14 [126] MRR, Hits@N Link Prediction</p>
<p>GenTKG [339] ICEWS14 [126], ICEWS18 [317], GDELT [340], YAGO [316] Hits@N Link Prediction Up To Date [341] Wikidata [129] Accuracy, Response Rate Fact Validation, Question Answering PPT [342] ICEWS14 [126], ICEWS18 [317], ICEWS05-15 [126] MRR, Hits@N Link Prediction LLM-DA [343] ICEWS14 [126], ICEWS05-15 [126] MRR, Hits@N Link Prediction</p>
<p>Generating Insights &amp; Representations</p>
<p>TimeLlama [344] ICEWS14 [126], ICEWS18 [317] Tmall [347], Alibaba [348] Recall@K, NDCG@K Dynamic Graph Recommendation, Top-K Recommendation</p>
<p>Evaluation and Application</p>
<p>Model Evaluation</p>
<p>Dynamic-TempLAMA [43] DYNAMICTEMPLAMA [349] Accuracy, MRR, ROUGE, F1</p>
<p>Temporal Robustness Evaluation, Factual Knowledge Probing DARG [80] GSM8K [350], BBQ [351], BBH Navigate [352], BBH Dyck Language [352] Accuracy, Complexity-Induced Accuracy Retention Rate, Exact Match, Accuracy Mathematical Reasoning, Social Reasoning, Spatial Reasoning, Symbolic Reasoning</p>
<p>Downstream Applications</p>
<p>AnomalyLLM [102] UCI Messages [103], Blogcatalog [104] AUC Anomaly Detection MoMa-LLM [353] iGibson scenes [354] AUC, Recall Semantic Interactive Object Search TRR [355] Reuters Financial News [356] AUROC Event Detection</p>
<p>Querying and Reasoning in Dynamic Graphs</p>
<p>This category includes work that uses LLMs to query, reason about, or retrieve knowledge from dynamic graphs.These approaches analyze or validate existing graph structures rather than actively generating or modifying node and edge features.LLMs act as information retrieval or reasoning tools, querying, analyzing, or inferring over existing graph structures and knowledge to discover new relationships or validate known ones.The key aspect is that LLMs assist in analyzing and understanding the graph's dynamic evolution.</p>
<p>(1) Traditional Methods Before the emergence of LLMs, querying and reasoning on dynamic graphs relied on specialized graph representation learning and temporal modeling methods.For example, Temporal Graph Networks (TGNs) [312], and related methods use memory modules combined with graph neural networks to process continuous-time dynamic graphs and capture node evolution.Methods like DyRep [36] and TGAT [357] focus on learning node representations that reflect temporal changes in neighborhood structures and interaction patterns.</p>
<p>For reasoning on Temporal Knowledge Graphs (TKGs), such as link prediction, traditional approaches employed embedding techniques or rule-based systems to capture changes in entities and relations over time [358].While effective for specific tasks, these methods struggle to fully use rich semantic information and face challenges in handling zero-shot relations or performing complex multi-step reasoning [359].</p>
<p>(2) Forecasting and Reasoning Current research now explores the semantic understanding and reasoning capabilities of LLMs to address querying and reasoning challenges in dynamic graphs, particularly for TKG forecasting and reasoning.One surprising finding is that LLMs, even without fine-tuning, can achieve performance comparable to specialized TKG models on forecasting tasks simply by using In-Context Learning (ICL) with historical facts converted to text.Their performance holds even when entity names are replaced with numerical IDs, suggesting LLMs can leverage structural and temporal patterns in the context [314].To deepen LLM reasoning, CoH (Chainof-History) [321] proposes a method to explore high-order his- torical information step-by-step, overcoming the limitation of relying only on first-order history and improving temporal reasoning, especially as a plug-in module for graph-based models.TG-LLM [322] trains an LLM to translate text context into a latent temporal graph and then uses Chain-of-Thought (CoT) reasoning over this graph, enhancing generalizable temporal reasoning.To address the challenge of unseen relations in TKGs, zrLLM [318] uses LLMs to generate semantic representations from relation descriptions, enabling embedding models to recognize zero-shot relations via semantic similarity.Furthermore, LLM4DyG [326] introduces a systematic evaluation of LLMs' spatial-temporal understanding on general dynamic graphs.It introduces a benchmark and proposes the "Disentangled Spatial Temporal Thoughts" prompting method to improve performance on tasks like link prediction and node classification, although challenges remain with large or dense dynamic graphs.</p>
<p>(3) QA and Interpretability Another line of research focuses on using LLMs for more complex information retrieval from dynamic graphs, such as Temporal Knowledge Graph Question Answering (TKGQA), and understanding the LLMs' own reasoning processes.TKGQA requires understanding complex temporal constraints in questions and retrieving answers from dynamic knowledge, a task where traditional methods struggle with semantics.TimeR4 [330] introduces a time-aware Retrieve-Rewrite-Retrieve-Rerank framework.It uses LLMs and retrieved TKG facts to handle time constraints and reduce temporal hallucination by rewriting questions and reranking retrieved facts.Similarly, GenTKGQA [333] employs a two-stage approach: the LLM first mines constraints to guide subgraph retrieval, then generates answers by fusing GNN signals with text representations via instruction tuning.These methods demonstrate the potential of retrieval-augmented LLMs for complex dynamic knowledge QA.Additionally, to understand how LLMs process factual knowledge, Unveiling LLMs [335] uses dynamic knowledge graphs as a tool to interpret LLM reasoning.It decodes internal token representations layer-wise during fact verification, revealing how factual representations evolve within the LLM.</p>
<p>Generating and Updating in Dynamic Graphs</p>
<p>This category includes papers that use LLMs to actively generate new nodes, edges, or their features or to update dynamic graph structures.These approaches typically serve to complete, reconstruct, or adapt the graph structure to dynamic changes.LLMs function as generators or updaters, creating new node/edge attributes or descriptions to fill in missing information or updating the graph structure in real time based on changes.The focus is on utilizing LLMs' generative capabilities to actively create or modify the graph structure to cope with dynamics.</p>
<p>(1) Traditional Methods Before LLMs became widespread, addressing dynamic graph generation and updates relied mainly on graph generative models and dynamic embedding techniques.Models like GraphRNN [360] and GRAN [361] focused on generating static graph structures; extending them to dynamic settings proved challenging.For updating representations in dynamic graphs, methods like DyGEM [362] adapted node embeddings at each time step to reflect graph changes.Knowledge graph updates often depended on manual editing, rule-based systems, or specific database maintenance procedures, which were difficult to automate and could not respond rapidly to realworld changes.These traditional methods generally lacked the ability to use external unstructured information (like text) to guide updates and had limitations in generating semantically consistent and structurally complex graph evolution patterns.</p>
<p>(2) Generating Structures Researchers now use LLMs' generative power to create or maintain evolving knowledge structures.The FinDKG [338] demonstrates the potential of LLMs as dynamic knowledge graph (DKG) generators; their ICKG model builds a DKG directly from financial news text to capture market trends.To address the issue of outdated information in knowledge graphs, Up To Date [341] proposes a method combining LLM reasoning and RAG.It automatically identifies potentially outdated facts in a KG and retrieves information from trusted sources to generate accurate corrections, enabling automated KG maintenance.For TKG completion and forecasting, PPT [342] converts TKG facts and time intervals into prompted natural language sequences, using a masked language modeling task for completion.GenTKG [339] also adopts a generative forecasting approach for TKGs.It uses a retrieval-augmented framework with parameter-efficient instruction tuning to generate future facts, achieving strong performance and generalization even with minimal training data.Furthermore, LLM-DA [343] innovatively uses an LLM to generate temporal logical rules from historical TKG data to guide reasoning.A dynamic adaptation strategy updates these rules based on new events, allowing the model to adapt to knowledge evolution without retraining the LLM.</p>
<p>(3) Generating Insights and Representations Beyond directly generating graph structures or facts, LLMs generate higher-level information like explanations, causal hypotheses, or enhanced node representations to aid dynamic graph analysis and applications.TimeLlama [344] focuses on explainable temporal reasoning.It not only predicts future events but also uses an LLM to generate natural language explanations based on historical TKG paths, increasing model trustworthiness.To explore the underlying mechanisms of dynamic systems, RealTCD [345] employs LLMs to process textual information (e.g., system logs) and integrate domain knowledge.Through LLM-guided meta-initialization, it improves the quality of temporal causal discovery, especially in industrial scenarios lacking intervention targets.For dynamic recommendation systems, DynLLM [346] uses LLMs to generate multi-dimensional user profiles (e.g., interests, preferred brands) and their embeddings from the textual features of purchase histories.These LLM-generated, dynamically updated user profiles enrich user node information, improving the recommendation system's ability to adapt to changing user preferences.These studies show that LLMs can produce basic graph elements and high-level semantic information, offering new ways to understand and model the complex evolution of dynamic graphs.</p>
<p>Evaluation and Application in Dynamic Graphs</p>
<p>This category includes work that uses LLMs to evaluate the effectiveness of dynamic graph learning models or apply LLMs in downstream tasks such as link prediction, node classification, or recommendation systems.LLMs serve as evaluation tools or modules within downstream applications, assessing the performance of dynamic graph models or providing additional guidance during training or inference.The emphasis is on using LLMs' comparative, assessment, or decision-support capabilities to handle tasks in dynamic environments.</p>
<p>(1) Traditional Methods Before using LLMs in these tasks, researchers relied on models that predicted future links or node states based on historical graph data [317].However, standard evaluation metrics often fail to capture the predictions' semantic quality or real-world plausibility [363].These metrics also struggled to assess how models handled unexpected structural changes or evolving concepts.In downstream applications, tasks such as dynamic anomaly detection typically use statistical methods to identify shifts in graph structure or connectivity patterns [364].Dynamic recommendation systems often relied on sequential models that analyzed user interaction histories [365] or time-aware collaborative filtering.Although these approaches were effective for specific problems, they lacked the ability to perform deeper semantic reasoning.</p>
<p>(2) Model Evaluation LLMs provide new perspectives for evaluating model performance and robustness in dynamic settings.Dynamic-TempLAMA [43] presents a dynamic benchmarking framework specifically designed to assess how well pretrained language models (MLMs) handle temporal concept drift-the evolution of factual knowledge over time.The framework dynamically creates time-sensitive test sets from Wikidata and evaluates MLMs through multiple views (probing, generation, scoring) to determine if their internal knowledge is outdated.DARG [80] addresses the limitations of static benchmarks by proposing a method to dynamically generate new evaluation data.It introduces changes to the reasoning graphs of existing benchmark samples to create novel test data with controlled complexity and diversity, using a code-augmented LLM to ensure label correctness.This enables adaptive evaluation of LLMs' reasoning capabilities as they evolve.Both works highlight the importance of moving beyond static, snapshot-based evaluations toward more dynamic, adaptive approaches, especially as both models and world knowledge constantly change.</p>
<p>(3) Downstream Applications LLMs are also embedded directly into downstream applications that process dynamic graph data, serving as core reasoning or decision-making components.AnomalyLLM [102] uses LLM knowledge for few-shot anomaly edge detection in dynamic graphs.By aligning edge representations with word embedding prototypes and using in-context learning, the method effectively identifies emerging anomaly types with few labeled examples, demonstrating LLMs' potential for adapting to changing environments like cybersecurity.In robotics, MoMa-LLM [353] integrates an LLM with dynamically updated, open-vocabulary scene graphs representing an explored environment.The LLM reasons over this evolving graph to guide a mobile robot in long-horizon, interactive object search tasks, showcasing how LLMs can integrate dynamic spatial-semantic information for high-level planning.The financial sector also leverages LLMs; TRR (Temporal Relational Reasoning) [355] uses an LLM-based framework mimicking human cognition (memory, attention) to detect potential stock portfolio crashes.It reasons over dynamically generated temporal relational information extracted from news to assess the aggregated impact of evolving events, which is useful for rare events lacking historical data.These applications demonstrate LLMs acting as powerful reasoning engines in complex real-world tasks that require understanding and responding to dynamically changing structured information.</p>
<p>Evaluation for Handling Dynamic Instability in Graphs</p>
<p>We review the evaluation pipeline of LLM-based methods for addressing dynamic instability in graphs, covering benchmark datasets, evaluation metrics, and downstream tasks (see Table 4).Commonly used datasets include temporal knowledge graph benchmarks such as ICEWS14 [126], ICEWS18 [317], ICEWS05-15 [126], YAGO [316], and WIKI [315] for link prediction and temporal reasoning.Event-based corpora such as ACLED [320], GDELT [340], and TempReason [325] are used for temporal forecasting, while domain-specific datasets like Enron [327], DBLP [328], Tmall [347], and Alibaba [348] support communication analysis and recommendation.Temporal KGQA relies on MULTITQ [331], TimeQuestions [332], and CronQuestion [334], and claim verification uses FEVER [336] and CLIMATE-FEVER [337].Anomaly and event detection adopt UCI Messages, Blogcatalog, iGibson scenes, and Reuters Financial News.</p>
<p>Evaluation metrics vary by task: MRR and Hits@N dominate ranking tasks; Accuracy, F1, Precision, Recall, and ROC-AUC are applied in classification; BLEU, ROUGE, and Exact Match assess generative tasks; Recall@K and NDCG@K measure recommendation; and SHD or SID evaluate temporal causal discovery.AUC or AUROC are common for anomaly detection.</p>
<p>The surveyed methods span diverse downstream tasks, including link prediction, temporal reasoning, temporal KGQA, claim verification, fact validation, dynamic graph recommendation, temporal causal discovery, event forecasting, and anomaly or event detection.This diversity highlights the capability of LLMs to integrate temporal reasoning and external knowledge for robust performance in evolving graph environments.</p>
<p>Summary of Dynamic Instability</p>
<p>The integration of LLMs with dynamic graph learning presents a promising direction for addressing the inherent challenges posed by evolving graph structures.Leveraging their powerful sequence modeling capabilities and the flexibility of natural language encoding, LLMs can effectively adapt to distribution shifts and temporal variations that traditional static graph models struggle to capture.Moreover, LLMs can incorporate external temporal knowledge into the graph reasoning process, enhancing predictive power and enabling anticipation of future changes rather than merely reacting to them.Early empirical evidence suggests that LLM-augmented dynamic graph models exhibit greater robustness and sustained performance over time.Nevertheless, these benefits come with increased computational costs and complexity, requiring careful architectural design and error mitigation strategies to prevent error propagation and maintain reliability in long-term deployment.</p>
<p>Limitations, Challenges, and Future Directions</p>
<p>This section builds upon the preceding analysis to consolidate key challenges and limitations in graph-based learning with LLMs, and to outline potential directions for future research.The discussion is organized around several considerations, including efficiency and scalability, explainability, fairness, and faithfulness, and security, robustness, and governance, which together reflect both the current barriers and the broader goals for advancing LLM-graph integration.</p>
<p>Efficiency and Scalability</p>
<p>Scalability is a key challenge in integrating LLMs with graph data, given the high memory and computation costs of large, heterogeneous, and dynamic graphs.We summarize efficiency challenges across scenarios and explore ways to improve tractability, compactness, and inference speed.</p>
<p>Incompleteness</p>
<p>LLM-based methods for graph completion show promise in generating missing nodes, edges, or attributes, but they also introduce risks of hallucination and structural inconsistency.To improve the fidelity of generated content, future research should explore structure-constrained decoding, confidence-aware integration, and post-hoc validation using GNNs or external knowledge bases [121].Incorporating structural priors from graphs directly into training objectives may further enhance the alignment between textual reasoning and graph topology [136].At the same time, reducing representational redundancy is critical for scalability, motivating the development of compact graph-text co-representations.For example, injecting structure tokens or refactoring sequence layouts can reduce the need for full-graph serialization during inference [366,367].Besides, efficiencyoriented designs should also consider concrete resource metrics such as sequence length, memory usage, and per-sample or per-token computation cost [368].</p>
<p>Imbalance</p>
<p>Imbalanced distributions are common in real-world graphs, where a few dominant classes overshadow long-tail entities that often lack sufficient structural and semantic support [188,187].Uniformly applying LLMs across all samples is inefficient and may overfit majority patterns while underperforming on minority nodes.Moreover, hallucinations in underrepresented regions pose reliability risks, especially when topological signals are weak [191].To mitigate these issues, future research should consider selective LLM invocation for hard or minority-class cases, while using GNNs for routine instances and transferring LLM knowledge into lightweight models via parameter-efficient tuning [369,370].Structure-aware validation modules that crosscheck LLM outputs against graph-derived patterns can improve robustness [133], and a promising direction is to construct joint text-graph causal representations to help models infer the semantic origins of structural sparsity and synthesize logically grounded virtual connections [190].</p>
<p>Cross-Domain Heterogeneity</p>
<p>Graphs that span multiple domains or modalities often exhibit heterogeneity in node types, attribute formats, and structural patterns, which increases prompt length and introduces misalignment between textual and topological inputs [86,220].Existing methods struggle to handle these inconsistencies, as they typically rely on shallow normalization or isolated attribute encoding, and LLMs themselves lack permutation invariance when processing graph structures [252].To address these limitations, future efforts should explore context-aware modeling that jointly encodes multimodal attributes, local connectivity, and task-specific signals.Symmetry-preserving techniques such as contrastive learning or explicit prompt designs can help models distinguish meaningful structural variations from order artifacts [256].Additionally, tighter LLM-GNN integration-via dual encoders or knowledge distillation from graphlevel GNNs-may provide a more balanced trade-off between semantic richness and structural fidelity [261,268].</p>
<p>Dynamic Instability</p>
<p>In dynamic graphs, frequent structural or attribute updates introduce substantial overhead for model recomputation and pose challenges for maintaining temporal consistency [314].Current LLM-based approaches often rely on full re-encoding or static snapshots, which fail to reflect evolving semantics and result in stale or inaccurate reasoning [322].To overcome these limitations, future research should incorporate time-aware prompting, localized subgraph updates, and selective verification to reduce redundant computation [368].Caching frequently accessed substructures or intermediate representations can further improve efficiency.More importantly, integrating GNNs with LLMs for temporal causal reasoning, along with pretraining strategies that embed temporal priors, may help models adapt to semantic drift and support robust inference over evolving graph states [321,318].</p>
<p>Explainability, Fairness, and Faithfulness</p>
<p>LLM-based graph systems pose serious challenges in explainability, fairness, and faithful reasoning-especially in scenarios involving incomplete data, long-tail distributions, domain shifts, and temporal updates.While current approaches offer partial solutions, their limitations in interpretability, causal alignment, and robustness across scenarios call for more principled advancements.</p>
<p>Incompleteness</p>
<p>Incomplete graphs often result in LLM outputs that lack faithfulness, as the underlying reasoning is not grounded in observable evidence [134,135,121].This issue is exacerbated by decoding processes that fail to enforce topological constraints and by the absence of systematic post-generation verification.Future work should incorporate structure-constrained decoding objectives and apply graph consistency losses during training [136], while complementing generation with post-hoc validation using GNN-based scorers or confidence filters [91,93].Furthermore, reasoning fidelity deteriorates significantly when only partial graph observations are available [135].To address this, prompting strategies should explicitly encode subgraph structures to help LLMs reason over local topological contexts.In addition, iterative refinement mechanisms that alternate between retrieval, reasoning, and validation can simulate GNN-style aggregation while supporting auditable evidence chains.</p>
<p>Imbalance</p>
<p>Graph imbalance not only degrades predictive performance but also undermines explanation stability and reliability.Due to sparse semantics and weak connectivity, LLMs may struggle to generate faithful rationales for long-tail nodes.Moreover, existing interpretability tools rarely account for such imbalance, leading to inconsistent explanations under input perturbations.Future work should explore causal mechanisms underlying topological sparsity and use LLMs to generate logically consistent virtual subgraphs to restore class-level balance [187].At the same time, interpretability evaluation should emphasize stability: explanations for minority-class nodes should remain consistent across counterfactual edits or input augmentations, and validation modules should reject rationales that do not correspond to truly discriminative substructures.</p>
<p>Cross-Domain Heterogeneity</p>
<p>Inconsistencies in label semantics, graph patterns, and textual attributes across domains make it difficult for LLMs to produce generalizable explanations.These mismatches often introduce domain-specific biases that harm fairness and reasoning quality [371,372].Current models lack the ability to maintain explanation coherence when transferred across domains [373,374].Future systems should develop domain-invariant rationale templates and establish multi-level auditing frameworks covering embedding spaces, output distributions, and textual justifications to detect and mitigate explanation drift.Additionally, aligning the structural reasoning paths produced by GNNs with the textual rationales generated by LLMs can bridge the gap between semantics and topology and enhance explanation faithfulness.</p>
<p>Dynamic Instability</p>
<p>The evolving nature of real-world graphs introduces temporal variations that challenge the stability and trustworthiness of model explanations.As nodes and edges change over time, previously valid rationales may become outdated or misleading, yet current systems lack temporal auditing mechanisms.Future work should introduce rolling, time-aware auditing protocols that explicitly link model outputs with graph update events [371].Moreover, enabling executable explanations-where LLM-generated rationales are translated into symbolic constraints or verifiable subgraph traces-can improve the reliability and traceability of reasoning under temporal drift [375].</p>
<p>Security, Robustness, and Governance</p>
<p>As graph-LLM systems become increasingly applied in highstakes domains such as finance, healthcare, and recommendation, they face growing vulnerabilities arising from incomplete information, class imbalance, cross-domain inconsistency, and evolving graph structures.These challenges expose the system to risks including data poisoning, prompt injection, structure manipulation, and adversarial generalization, many of which are amplified by the flexible yet opaque behavior of LLMs.</p>
<p>Incompleteness</p>
<p>In settings where graph data is sparse or noisy, even minor perturbations to node or edge attributes can lead to significant shifts in completion outcomes, compromising the integrity of the generated graph [376,377].Similarly, LLM-augmented retrieval systems are vulnerable to prompt-level attacks, where indirect injection or document poisoning misleads the model's response [378].To improve robustness, future work should investigate multi-layer defenses, including data-level sanitization and edge pruning, model-level protection through graph denoising techniques such as GNNGuard [379], and certification-based methods like randomized smoothing to quantify the system's tolerance under adversarial conditions [380,381].</p>
<p>Imbalance</p>
<p>Nodes associated with rare categories or low connectivity often lack sufficient structural context, making them particularly susceptible to adversarial influence; subtle perturbations can alter neighborhood aggregation or trigger class boundary shifts [382].Moreover, few-shot trigger constructions may induce undesirable distributional shifts in LLM outputs.To address these issues, future systems should adopt targeted defenses for long-tail nodes, including structural consistency checks and trigger detection via adversarial training regimes that expose the model to rare-case manipulations.</p>
<p>Cross-Domain Heterogeneity</p>
<p>When LLM-based models are deployed across domains with differing label semantics, structure formats, or template designs, inconsistencies can be exploited to induce behavior shifts or prompt corruption.Existing systems often lack robust controls over prompt templates or graph encodings, making them vulnerable to injection and escalation attacks [378].Looking forward, stronger domain-level governance is needed, including adversarial evaluation prior to deployment, enforcement of signed prompt templates, and whitelist-based control over structural tokens to prevent unauthorized modifications.</p>
<p>Dynamic Instability</p>
<p>Dynamic graphs introduce further complexity, as attackers may inject or remove connections during critical time windows or log adversarial traces to shape long-term model behavior [382].These time-sensitive attacks are often stealthy and hard to reverse, leading to compounding damage [381].Future directions should prioritize rolling-window detection and temporal rollback mechanisms, coupled with causal tracing of performance degradation to structural changes.Additional safeguards such as time-aware scoring and path-verifiable explanation generation can help separate legitimate evolution from malicious interference.</p>
<p>Conclusion</p>
<p>This survey has presented a comprehensive and systematic review of how recent advances in LLMs can be leveraged to address four fundamental, data-centric challenges in graph learning: incompleteness of structures or attributes, severe imbalance in node and edge distributions, cross-domain heterogeneity in semantics and structure, and dynamic instability arising from evolving topologies and interactions.To achieve this, we conducted an extensive literature collection and categorization, and organized representative methods.We further summarized benchmark datasets and evaluation metrics used in existing studies, assessed empirical trends.In addition, we identified open technical challenges and outlined promising future research directions.</p>
<p>We have synthesized a broad spectrum of LLM-graph integration strategies under this framework, demonstrating how LLMs bring distinctive capabilities that complement purely graphbased approaches.In the context of incompleteness, LLMs apply semantic reasoning and draw upon external knowledge to infer missing attributes and relationships, serving as intelligent imputers.For imbalanced graphs, they can generate synthetic samples and enrich feature spaces, thereby enhancing minorityclass representations and mitigating bias.In heterogeneous graph scenarios, LLMs facilitate the unification of disparate modalities and domain-specific schemas into coherent embeddings, enabling effective cross-domain alignment.For dynamic graphs, their contextual and temporal reasoning allows for anticipating structural changes, explaining evolution, and supporting continuous adaptation.</p>
<p>Looking ahead, the integration of LLMs with graph learning still faces important open questions.Improving efficiency and scalability is essential for large-scale or real-time applications, while enhancing interpretability and trustworthiness will be critical for deployment in high-stakes domains such as healthcare and finance.Developing mechanisms for continual adaptation without catastrophic forgetting remains a significant challenge, as does bridging the gap between textual knowledge encoded in LLMs and structural graph signals.In addition, controlling hallucination in LLM-generated graph content is vital to ensure both semantic validity and structural consistency.</p>
<p>In conclusion, the convergence of LLMs and graph learning marks a promising new direction, combining the deep semantic understanding of natural language processing with the structured relational modeling of graph machine learning.This synergy has already yielded models that are more robust, knowledgeable, and adaptable than those based on either technology alone.As research advances, we anticipate rapid progress toward graph learning systems with greater generality and intelligence, capable of reasoning effectively over the rich, dynamic networks that underpin real-world data.doi:10.1609/aaai.v34i04.5984.URL https://ojs.aaai.org/index.php/AAAI/article/view/5984</p>
<p>Figure 1 :
1
Figure 1: The four fundamental challenges emerge of real-world graph complexity: (1) Incompleteness in graphs, where nodes, edges, or attributes are missing, (2)Imbalance in graphs, where the distribution of nodes, edges, or labels is highly skewed, (3) Cross-domain heterogeneity in graphs, where graph data from different domains exhibit semantic and structural discrepancies, and (4) Dynamic instability in graphs, where graphs undergo dynamic changes in topology, attributes, or interactions over time.</p>
<p>Figure 2 :
2
Figure 2: Annual publication trends in Graph Learning from January 2015 to July 2025, with a highlighted subset of works related to LLMs.The blue bars represent Graph Learning (non-LLM) publications and the pink segments represent LLM-related publications; the stacked height indicates the total per year.The percentages above each bar denote the proportion of LLM-related papers within the total for that year.Data sourced from Google Scholar (https: //scholar.google.com).</p>
<p>Figure 3 :
3
Figure 3: Comparison between traditional graph representation learning and emerging paradigms that leverage LLMs.(a) Conventional graph learning relies on GNNs to encode structural information into node or graph-level representations.(b) In LLM-based approaches, LLMs can enhance graph structures using auxiliary text (top), extract semantic representations directly from graph inputs (middle), or generate task-specific answers through reasoning over graph structures (bottom).</p>
<p>Figure 6 :
6
Figure 6: Illustration of three representative tasks addressing graph imbalance.Class-imbalanced graph learning (left) refers to scenarios where certain node classes (minority classes) are underrepresented compared to others.Structureimbalanced graph learning (right) highlights disparities in node connectivity, where high-degree nodes dominate information flow while low-degree nodes are underexposed.</p>
<p>Figure 7 :
7
Figure 7: Illustration of an LLM-based pipeline for addressing class imbalance in graphs.Minority class information is first extracted and linearized into textual prompts, which are processed by the LLM to generate semantically enriched nodes and edges.The newly synthesized elements (highlighted in red) are integrated into the original graph to enhance representation and balance class distributions.</p>
<p>Figure 8 :
8
Figure 8: Structural Imbalance Remediation via LLM-Based Text-Topology Co-Optimization.The optimized graph (right) shows the rebalanced connectivity, with new connections indicated in red.</p>
<p>Figure 9 :
9
Figure 9: Illustration of three representative tasks addressing cross-domain heterogeneity in graphs.Text-attributed graph learning (left) involves graphs where nodes or edges are associated with textual content, requiring joint modeling of structure and language.Multimodal attributed graph learning (middle) integrates diverse data types across nodes or relations.Structural heterogeneous graph learning (right) with complex cross-modal dependencies and heterogeneous attribute spaces.</p>
<p>Figure 10 :
10
Figure 10: Handling cross-domain textual attribute heterogeneity with LLMs.Attributes from different domains are first transformed into unified textual descriptions by an LLM, then converted into unified embeddings using a trainable small LM to generate initial features for graph learning methods.</p>
<p>Figure 11 :
11
Figure 11: Two different approachs for handling cross-domain structural heterogeneity with LLMs.(a) LLM-based methods convert graph structures into textual descriptions, enabling direct input to LLMs.(b) Hybrid methods align GNN-generated graph token embeddings with the native text token embeddings of LLMs in a unified representation space, thereby enhancing the structural comprehension capabilities of LLMs.</p>
<p>Figure 12 :
12
Figure 12: Illustration of dynamic instability in graphs.The figure presents three categories of dynamic instability over two timestamps (t 1 → t 2 ): (1) Node Feature Dynamics, where node attributes change over time; (2) Structural Dynamics, showing newly added node (solid dark purple circle) and newly removed node (gray dashed circle); (3) New Node/Edge Types, depicting the emergence of entirely new node and relationship types, represented by a solid dark purple triangle and dashed edges.</p>
<p>Figure 13 :
13
Figure 13: A unified framework integrating LLMs for addressing dynamic instability in graphs.Dynamic changes in node features, structure, and new node or edge types are captured by temporal graph embeddings.These dynamic embeddings, combined with context (triples and historical facts), serve as inputs to the LLM.The LLM then performs reasoning tasks and outputs the final prediction or response.</p>
<p>Table 1 :
1
LLM-based methods for handling incompleteness in graphs, grouped by domain and incomplete type, with representative methods, datasets, metrics, and downstream tasks.
DomainIncompletenessMethodTypical DatasetsCommon MetricsDownstream TasksLLM4NG [41]NodeRobustGraphLearning</p>
<p>Title: Attention is All you Need Abstract: The dominant sequence … LLMs GPT Llama3 Align/ Fusion/ Distillation GNN Embedding Embedding ? ?
Textual DescriptionLLM EncoderIncomplete GraphGNN EncoderFigure 5: The framework addresses incomplete graph structures by integratingstructural signals from GNNs and semantic priors from LLMs. The "Align /Fusion / Distillation" module enables the model to compensate for missing graphinformation by leveraging textual context, resulting in enriched and unified nodeattribute/edge/label representations.</p>
<p>Table 2 :
2
LLM-based methods for handling imbalance in graphs, grouped by domain and task, with representative methods, datasets, metrics, and downstream tasks.
DomainTasksMethodTypical DatasetsCommon MetricsDownstream Tasks</p>
<p>Table 3 :
3
LLM-based methods for handling cross-domain heterogeneity in graphs, grouped by domains and tasks, with representative methods, datasets, metrics, and downstream tasks.
DomainsTasksMethodsTypical DatasetsCommon MetricsDownstream TasksTAPE [86]Cora [81], PubMed [82], Arxiv-2023 [86], ogbn-arxiv [83], ogbn-products [83]AccuracyNode ClassificationTextual AttributeLLMRec [217]MovieLens [218], Netflix [219]Recall, NDCG, PrecisionItem RecommendationAlignmentMINGLE [220]MIMIC-III [221], CRADLE [220]Accuracy, AUC, AUPR, F1Node ClassificationGHGRL [222]Text-AttributedGraphLearning</p>
<p>Table 3 :
3
LLM-based methods for handling cross-domain heterogeneity in graphs, grouped by domains and tasks, with representative methods, datasets, metrics, and downstream tasks.(continued from previous page)
4, ROUGE-L, CIDErNode Classification, Edge Classification, Section Summarization</p>
<p>Table 4 :
4
LLM-based methods for handling dynamic instability in graphs, grouped by tasks, with representative methods, datasets, metrics, and downstream tasks.
DomainCategoryMethodTypical DatasetsCommon MetricsDownstream TasksICL [314]Forecasting&amp; ReasoningQueryingand Reasoning
AcknowledgementsThis work was supported in part by the Shenzhen Science and Technology Program under Grant JCYJ20240813151129038; in part by the National Natural Science Foundation of China under Grant 52172350, Grant 51775565, and Grant W2421069; in part by the Guangdong Basic and Applied Research Foundation under Grant 2022B1515120072; and in part by the Science and Technology Planning Project of Guangdong Province under Grant 2023B1212060029.Declaration of competing interestThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Graph learning: A survey. F Xia, K Sun, S Yu, A Aziz, L Wan, S Pan, H Liu, 10.1109/TAI.2021.3076021IEEE Transactions on Artificial Intelligence. 222021</p>
<p>A survey on graph representation learning methods. S Khoshraftar, A An, 10.1145/3633518ACM Transactions on Intelligent Systems and Technology. 1512024</p>
<p>A comprehensive survey on deep graph representation learning. W Ju, Z Fang, Y Gu, Z Liu, Q Long, Z Qiao, Y Qin, J Shen, F Sun, Z Xiao, 10.1016/j.neunet.2024.106207Neural Networks. 1731062072024</p>
<p>Redundancy is not what you need: An embedding fusion graph auto-encoder for self-supervised graph representation learning. M Li, Y Zhang, S Wang, Y Hu, B Yin, 10.1109/TNNLS.2024.3357080IEEE Transactions on Neural Networks and Learning Systems. 3622025</p>
<p>Graphaware multi-feature interacting network for explainable rumor detection on social network. C Yang, X Yu, J Wu, B Zhang, H Yang, 10.1016/j.eswa.2024.123687Expert Systems with Applications. 2491236872024</p>
<p>Deep adaptive collaborative graph neural network for social recommendation. L Wang, W Zhou, L Liu, Z Yang, J Wen, 10.1016/j.eswa.2023.120410Expert Systems with Applications. 2291204102023</p>
<p>Graph neural networks for intelligent transportation systems: A survey. S Rahmani, A Baghbani, N Bouguila, Z Patterson, 10.1109/TITS.2023.3257759IEEE Transactions on Intelligent Transportation Systems. 2482023</p>
<p>Financial fraud detection using graph neural networks: A systematic review. S Motie, B Raahemi, 10.1016/j.eswa.2023.122156Expert Systems with Applications. 2401221562024</p>
<p>Poisoning medical knowledge using large language models. J Yang, H Xu, S Mirzoyan, T Chen, Z Liu, Z Liu, W Ju, L Liu, Z Xiao, M Zhang, 10.1038/s42256-024-00899-3Nature Machine Intelligence. 6102024</p>
<p>A pseudo-label supervised graph fusion attention network for drug-target interaction prediction. Y Xie, X Wang, P Wang, X Bi, 10.1016/j.eswa.2024.125264Expert Systems with Applications. 2591252642025</p>
<p>W Ju, S Yi, Y Wang, Z Xiao, Z Mao, H Li, Y Gu, Y Qin, N Yin, S Wang, arXiv:2403.04468A survey of graph neural networks in real world: Imbalance, noise, privacy and ood challenges. 2024arXiv preprint</p>
<p>Temporal graph learning for financial world: Algorithms, scalability, explainability &amp; fairness. N Rajput, K Singh, 10.1145/3534678.3542619Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Tdg-mamba: Advanced spatiotemporal embedding for temporal dynamic graph learning via bidirectional information propagation. M Li, J Chen, B Li, Y Zhang, R Zhang, S Gong, X Ma, Z Tian, 10.1109/TCSS.2024.35093992024IEEE Transactions on Computational Social Systems1</p>
<p>Mm-stflownet: A transportation huboriented multi-mode passenger flow prediction method via spatial-temporal dynamic graph modeling. R Zhang, W Xing, M Li, Z Wang, J Chen, X Ma, Z Liu, Z He, arXiv:2504.063252025</p>
<p>Learning on attribute-missing graphs. X Chen, S Chen, J Yao, H Zheng, Y Zhang, I W Tsang, 10.1109/TPAMI.2020.3032189IEEE Transactions on Pattern Analysis and Machine Intelligence. 4422022</p>
<p>Z Liu, Y Li, N Chen, Q Wang, B Hooi, B He, arXiv:2308.13821A survey of imbalanced learning on graphs: Problems, techniques, and future directions. 2023arXiv preprint</p>
<p>Imbalanced graph classification via graph-of-graph neural networks. Y Wang, Y Zhao, N Shah, T Derr, 10.1145/3511808.3557356Proceedings of the 31st ACM international conference on information &amp; knowledge management. the 31st ACM international conference on information &amp; knowledge management2022</p>
<p>Pick and choose: a gnn-based imbalanced learning approach for fraud detection. Y Liu, X Ao, Z Qin, J Chi, J Feng, H Yang, Q He, 10.1145/3442381.3449989Proceedings of the web conference 2021. the web conference 20212021</p>
<p>Classimbalanced learning on graphs: A survey. Y Ma, Y Tian, N Moniz, N V Chawla, 10.1145/3718734ACM Computing Surveys. 2023</p>
<p>Multi-class imbalanced graph convolutional network learning. M Shi, Y Tang, X Zhu, D Wilson, J Liu, 10.24963/ijcai.2020/398Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence202020</p>
<p>A geometric classification of world urban road networks. M Badhrudeen, S Derrible, T Verma, A Kermanshah, A Furno, 10.3390/urbansci60100112022Urban Science</p>
<p>Graph foundation models: Concepts, opportunities and challenges. J Liu, C Yang, Z Lu, J Chen, Y Li, M Zhang, T Bai, Y Fang, L Sun, P S Yu, C Shi, 10.1109/TPAMI.2025.3548729IEEE Transactions on Pattern Analysis and Machine Intelligence. 2025</p>
<p>Domain adaptive network embedding. Y Zhang, G Song, L Du, S Yang, Y Jin, Dane , International Joint Conference on Artificial Intelligence. 2019</p>
<p>When graph meets multimodal: Benchmarking and meditating on multimodal attributed graphs learning. H Yan, C Li, J Yin, Z Yu, W Han, M Li, Z Zeng, H Sun, S Wang, 2024</p>
<p>Cross-domain few-shot graph classification. K Hassani, 10.1609/aaai.v36i6.20642Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>Shift-robust gnns: Overcoming the limitations of localized graph training data. Q Zhu, N Ponomareva, J Han, B Perozzi, Advances in Neural Information Processing Systems. 2021</p>
<p>Cycle: Cross-year contrastive learning in entity-linking. P Zhang, C Cao, K Zaporojets, P Groth, 10.1145/3627673.3679702Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge ManagementNew York, NY, USAAssociation for Computing Machinery2024</p>
<p>TIGER: temporally improved graph entity linker. P Zhang, C Cao, P Groth, 10.3233/FAIA240933Proceedings of the 27th European Conference on Artificial Intelligence. the 27th European Conference on Artificial IntelligenceIOS Press2024392</p>
<p>Representation learning for dynamic graphs: A survey. S M Kazemi, R Goel, K Jain, I Kobyzev, A Sethi, P Forsyth, P Poupart, Journal of Machine Learning Research. 21702020</p>
<p>A singular value thresholding algorithm for matrix completion. J.-F Cai, E J Candès, Z Shen, 10.1137/080738970SIAM Journal on optimization. 2042010</p>
<p>Graphsmote: Imbalanced node classification on graphs with graph neural networks. T Zhao, X Zhang, S Wang, 10.1145/3437963.3441720Proceedings of the 14th ACM international conference on web search and data mining. the 14th ACM international conference on web search and data mining2021</p>
<p>Dualdiscriminative graph neural network for imbalanced graph-level anomaly detection. G Zhang, Z Yang, J Wu, J Yang, S Xue, H Peng, J Su, C Zhou, Q Z Sheng, L Akoglu, Advances in Neural Information Processing Systems. 352022</p>
<p>Adversarial deep network embedding for cross-network node classification. X Shen, Q Dai, F -L. Chung, W Lu, K.-S Choi, 10.1609/aaai.v34i03.5692Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020</p>
<p>Structural re-weighting improves graph domain adaptation. S Liu, T Li, Y Feng, N Tran, H Zhao, Q Qiu, P Li, International Conference on Machine Learning. 2023</p>
<p>Evolvegcn: Evolving graph convolutional networks for dynamic graphs. A Pareja, G Domeniconi, J Chen, T Ma, T Suzumura, H Kanezashi, T Kaler, T Schardl, C Leiserson, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>R Trivedi, M Farajtabar, P Biswal, H Zha, Dyrep: Learning representations over dynamic graphs, International Conference on Learning Representation. </p>
<p>Kllms4rec: Knowledge graph-enhanced llms sentiment extraction for personalized recommendations. Y Cui, K Wang, H Yu, X Guo, H Cao, 10.1016/j.eswa.2025.127430Expert Systems with Applications. 2821274302025</p>
<p>Llmpowered scene graph representation learning for image retrieval via visual triplet-based graph transformation. S Jeong, J Park, M Choi, Y Kwon, S Lim, 10.1016/j.eswa.2025.127926Expert Systems with Applications. 2025</p>
<p>Dynamic link prediction: Using language models and graph structures for temporal knowledge graph completion with emerging entities and relations. R Ong, J Sun, Y.-K Guo, O Serban, 10.1016/j.eswa.2025.126648Expert Systems with Applications. 2025</p>
<p>Framing few-shot knowledge graph completion with large language models. A M Brasoveanu, L Nixon, A Weichselbraun, A Scharl, Joint Workshop Proceedings of the 5th International Workshop on Sem4Tra and SEMANTiCS. 20233510</p>
<p>Leveraging large language models for node generation in fewshot learning on text-attributed graphs. J Yu, Y Ren, C Gong, J Tan, X Li, X Zhang, 10.1609/aaai.v39i12.33428Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>C Liu, B Wu, arXiv:2308.11224Evaluating large language models on graphs: Performance insights and comparative analysis. 2023arXiv preprint</p>
<p>Dynamic benchmarking of masked language models on temporal concept drift with multiple views. K Margatina, S Wang, Y Vyas, N John, Y Benajiba, M Ballesteros, 10.18653/v1/2023.eacl-main.211Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. A Vlachos, I Augenstein, the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Hypergraph transformer neural networks. M Li, Y Zhang, X Li, Y Zhang, B Yin, 10.1145/3565028ACM Transactions on Knowledge Discovery from Data. 1752023</p>
<p>On spectral clustering: Analysis and an algorithm. A Ng, M Jordan, Y Weiss, Proceedings of the International Conference on Neural Information Processing Systems: Natural and Synthetic. the International Conference on Neural Information Processing Systems: Natural and Synthetic200114</p>
<p>Line: Large-scale information network embedding. J Tang, M Qu, M Wang, M Zhang, J Yan, Q Mei, 10.1145/2736277.2741093Proceedings of the International Conference on World Wde Web. the International Conference on World Wde Web2015</p>
<p>Deepwalk: Online learning of social representations. B Perozzi, R Al-Rfou, S Skiena, 10.1145/2623330.2623732Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>A Grover, J Leskovec, 10.1145/2939672.2939754Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining2016node2vec: Scalable feature learning for networks</p>
<p>metapath2vec: Scalable representation learning for heterogeneous networks. Y Dong, N V Chawla, A Swami, 10.1145/3097983.3098036Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. the 23rd ACM SIGKDD international conference on knowledge discovery and data mining2017</p>
<p>T N Kipf, M Welling, arXiv:1611.07308Variational graph auto-encoders. 2016arXiv preprint</p>
<p>S Pan, R Hu, G Long, J Jiang, L Yao, C Zhang, arXiv:1802.04407Adversarially regularized graph autoencoder for graph embedding. 2018arXiv preprint</p>
<p>A Salehi, H Davulcu, arXiv:1905.10715Graph attention auto-encoders. 2019arXiv preprint</p>
<p>Spectral networks and deep locally connected networks on graphs. J Bruna, W Zaremba, A Szlam, Y Lecun, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2014</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. M Defferrard, X Bresson, P Vandergheynst, Proceedings of the Neural Information Processing Systems. the Neural Information Processing Systems201629</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, International Conference on Learning Representations. 2017</p>
<p>Inductive representation learning on large graphs. W L Hamilton, Z Ying, J Leskovec, Proceedings of the Neural Information Processing Systems. the Neural Information Processing Systems2017</p>
<p>Graph attention networks. P Veličković, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, International Conference on Learning Representations. 2018</p>
<p>Simplifying graph convolutional networks. F Wu, A Souza, T Zhang, C Fifty, T Yu, K Weinberger, Proceedings of the International Conference on Machine Learning. the International Conference on Machine LearningPMLR2019</p>
<p>How powerful are graph neural networks?. K Xu, W Hu, J Leskovec, S Jegelka, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations2019</p>
<p>Deep graph infomax. P Velickovic, W Fedus, W L Hamilton, P Liò, Y Bengio, R D Hjelm, Proceedings of the International Conference on Learning Representations. the International Conference on Learning Representations20192</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, T Chen, Z Wang, Y Shen, Advances in Neural Information Processing Systems. 332020</p>
<p>Self-supervised heterogeneous graph neural network with co-contrastive learning. X Wang, N Liu, H Han, C Shi, 10.1145/3447548.3467415Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021</p>
<p>Masked label prediction: Unified message passing model for semi-supervised classification. Y Shi, Z Huang, S Feng, H Zhong, W Wang, Y Sun, 10.24963/ijcai.2021/214Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, ijcai.org, 2021. the Thirtieth International Joint Conference on Artificial Intelligence, ijcai.org, 2021</p>
<p>Recipe for a general, powerful, scalable graph transformer. L Rampášek, M Galkin, V P Dwivedi, A T Luu, G Wolf, D Beaini, Proceedings of the Neural Information Processing Systems. the Neural Information Processing Systems202235</p>
<p>A survey of large language models for graphs. X Ren, J Tang, D Yin, N Chawla, C Huang, 10.1145/3637528.3671460Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>State of the art and potentialities of graph-level learning. Z Yang, G Zhang, J Wu, J Yang, Q Z Sheng, S Xue, C Zhou, C Aggarwal, H Peng, W Hu, 10.1145/3695863ACM Computing Surveys. 5722024</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Akhtar, N Barnes, A Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, 10.1145/3641289ACM transactions on intelligent systems and technology. 1532024</p>
<p>Pretrained language models for text generation: A survey. J Li, T Tang, W X Zhao, J.-Y Nie, J.-R Wen, 10.1145/3649449ACM Computing Surveys. 5692024</p>
<p>Toward expert-level medical question answering with large language models. K Singhal, T Tu, J Gottweis, R Sayres, E Wulczyn, M Amin, L Hou, K Clark, S R Pfohl, H Cole-Lewis, 10.1038/s41591-024-03423-7Nature Medicine. 2025</p>
<p>Y Moslem, R Haque, J D Kelleher, A Way, arXiv:2301.13294Adaptive machine translation with large language models. 2023arXiv preprint</p>
<p>A tutorial on hidden markov models and selected applications in speech recognition. L R Rabiner, 10.1109/5.18626Proceedings of the IEEE. 7721989</p>
<p>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. J Lafferty, A Mccallum, F Pereira, 10.5555/645530.655813Icml. Williamstown, MA200113</p>
<p>Attention is all you need, Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert , Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies2019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 352022</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. D Dai, C Deng, C Zhao, R Xu, H Gao, D Chen, J Li, W Zeng, X Yu, Y Wu, arXiv:2401.060662024arXiv preprint</p>
<p>Darg: Dynamic evaluation of large language models via adaptive reasoning graph. Z Zhang, J Chen, D Yang, arXiv:2406.172712024</p>
<p>Automating the construction of internet portals with machine learning. A K Mccallum, K Nigam, J Rennie, K Seymore, 10.1023/A:1009953814988Information Retrieval. 2000</p>
<p>P Sen, G Namata, M Bilgic, L Getoor, B Galligher, T Eliassi-Rad, 10.1609/aimag.v29i3.2157Collective classification in network data, AI magazine. 2008</p>
<p>Open graph benchmark: Datasets for machine learning on graphs. W Hu, M Fey, M Zitnik, Y Dong, H Ren, B Liu, M Catasta, J Leskovec, Advances in neural information processing systems. 332020</p>
<p>S Sun, Y Ren, C Ma, X Zhang, 10.48550/ARXIV.2311.14324arXiv:2311.14324Large language models as topological structure enhancers for text-attributed graphs. 2023arXiv preprint</p>
<p>Citeseer: An automatic citation indexing system. C L Giles, K D Bollacker, S Lawrence, 10.1145/276675.276685Proceedings of the third ACM conference on Digital libraries. the third ACM conference on Digital libraries1998</p>
<p>Harnessing explanations: LLM-to-LM interpreter for enhanced text-attributed graph representation learning. X He, X Bresson, T Laurent, A Perold, Y Lecun, B Hooi, International Conference on Learning Representations. 2024</p>
<p>Spatiotemporal pre-trained large language model for forecasting with missing values. L Fang, W Xiang, S Pan, F D Salim, Y.-P P Chen, 10.1109/JIOT.2024.3524030IEEE Internet of Things Journal. 2025</p>
<p>Traffic flow forecasting with spatialtemporal graph diffusion network. X Zhang, C Huang, Y Xu, L Xia, P Dai, L Bo, J Zhang, Y Zheng, 10.1609/aaai.v35i17.17761Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2021</p>
<p>Z Chen, H Mao, H Wen, H Han, W Jin, H Zhang, H Liu, J Tang, arXiv:2310.04668Label-free node classification on graphs with large language models (llms). 2023arXiv preprint</p>
<p>P Mernyei, C Cangea, arXiv:2007.02901Wiki-cs: A wikipedia-based benchmark for graph neural networks. 2020arXiv preprint</p>
<p>Z Chai, T Zhang, L Wu, K Han, X Hu, X Huang, Y Yang, 10.48550/ARXIV.2310.05845arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023arXiv preprint</p>
<p>Can language models solve graph problems in natural language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, Advances in Neural Information Processing Systems. 2023</p>
<p>Llm as prompter: Low-resource inductive reasoning on arbitrary knowledge graphs. K Wang, Y Xu, Z Wu, S Luo, 10.18653/v1/2024.findings-acl.224Findings of the Association for Computational Linguistics. 2024</p>
<p>Observed versus latent features for knowledge base and text inference. K Toutanova, D Chen, 10.18653/v1/W15-4007Proceedings of the 3rd workshop on continuous vector space models and their compositionality. the 3rd workshop on continuous vector space models and their compositionality2015</p>
<p>RAILD: towards leveraging relation features for inductive link prediction in knowledge graphs. G A Gesese, H Sack, M Alam, 10.1145/3579051.3579066Proceedings of the 11th International Joint Conference on Knowledge Graphs, ACM, 2022. the 11th International Joint Conference on Knowledge Graphs, ACM, 2022</p>
<p>Deeppath: A reinforcement learning method for knowledge graph reasoning. W Xiong, T Hoang, W Y Wang, 10.18653/v1/d17-1060Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics. the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics2017</p>
<p>On llm-enhanced mixed-type data imputation with highorder message passing. J Wang, K Wang, Y Zhang, W Zhang, X Xu, X Lin, arXiv:2501.021912025arXiv preprint</p>
<p>Uci machine learning repository. A Asuncion, D Newman, 2007</p>
<p>Linkgpt: Teaching large language models to predict missing links. Z He, J Zhu, S Qian, J Chai, D Koutra, 10.48550/ARXIV.2406.04640arXiv:2406.046402024arXiv preprint</p>
<p>Van Den Hengel, Image-based recommendations on styles and substitutes. J Mcauley, C Targett, Q Shi, A , 10.1145/2766462.2767755Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. the 38th international ACM SIGIR conference on research and development in information retrieval2015</p>
<p>An overview of microsoft academic service (mas) and applications. A Sinha, Z Shen, Y Song, H Ma, D Eide, B.-J Hsu, K Wang, 10.1145/2740908.2742839Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Anomalyllm: Few-shot anomaly edge detection for dynamic graphs using large language models. S Liu, D Yao, L Fang, Z Li, W Li, K Feng, X Ji, J Bi, arXiv:2405.076262024</p>
<p>Clustering in weighted networks. T Opsahl, P Panzarasa, 10.1016/j.socnet.2009.02.002Social networks. 3122009</p>
<p>Relational learning via latent social dimensions. L Tang, H Liu, 10.1145/1557019.1557109Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. the 15th ACM SIGKDD international conference on Knowledge discovery and data mining2009</p>
<p>Rethinking graph neural networks for anomaly detection. J Tang, J Li, Z Gao, J Li, International conference on machine learning. PMLR2022</p>
<p>URL. </p>
<p>Llmempowered few-shot node classification on incomplete graphs with real node degrees. Y Li, Y Yang, J Zhu, H Chen, H Wang, 10.1145/3627673.3679861Proceedings of the ACM International Conference on Information and Knowledge Management. the ACM International Conference on Information and Knowledge Management2024</p>
<p>Social proof: The impact of author traits on influence detection. S Rosenthal, K Mckeown, 10.18653/v1/W16-5604Proceedings of the First Workshop on NLP and Computational Social Science. 2016</p>
<p>Hegta: Leveraging heterogeneous graph-enhanced large language models for few-shot complex table understanding. R Jin, Y Li, G Qi, N Hu, Y.-F Li, J Chen, J Wang, Y Chen, D Min, S Bi, arXiv:2403.197232024arXiv preprint</p>
<p>Im-tqa: A chinese table question answering dataset with implicit and multi-type table structures. M Zheng, Y Hao, W Jiang, Z Lin, Y Lyu, Q She, W Wang, 10.18653/v1/2023.acl-long.278Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>M Ghasemi-Gol, P Szekely, arXiv:1802.06290Tabvec: Table vectors for classification of web tables. 2018arXiv preprint</p>
<p>Z Cheng, H Dong, Z Wang, R Jia, J Guo, Y Gao, S Han, J.-G Lou, D Zhang, arXiv:2108.06712Hitab: A hierarchical table dataset for question answering and natural language generation. 2021arXiv preprint</p>
<p>P Pasupat, P Liang, arXiv:1508.00305Compositional semantic parsing on semi-structured tables. 2015arXiv preprint</p>
<p>W Chen, H Wang, J Chen, Y Zhang, H Wang, S Li, X Zhou, W Y Wang, arXiv:1909.02164Tabfact: A large-scale dataset for table-based fact verification. 2019arXiv preprint</p>
<p>Flexkbqa: A flexible llm-powered framework for few-shot knowledge base question answering. Z Li, S Fan, Y Gu, X Li, Z Duan, B Dong, N Liu, J Wang, 10.1609/aaai.v38i17.29823Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Beyond iid: three levels of generalization for question answering on knowledge bases. Y Gu, S Kase, M Vanni, B Sadler, P Liang, X Yan, Y Su, 10.1145/3442381.3449992Proceedings of the web conference 2021. the web conference 20212021</p>
<p>The value of semantic parse labeling for knowledge base question answering. W.-T Yih, M Richardson, C Meek, M.-W Chang, J Suh, 10.18653/v1/p16-2033Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational Linguistics2016</p>
<p>Kqa pro: A dataset with explicit compositional programs for complex question answering over knowledge base. S Cao, J Shi, L Pan, L Nie, Y Xiang, L Hou, J Li, B He, H Zhang, 10.18653/v1/2022.acl-long.422Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2022</p>
<p>Zero-shot knowledge graph question generation via multi-agent llms and small models synthesis. R Zhao, J Tang, W Zeng, Z Chen, X Zhao, 10.1145/3627673.3679805Proceedings of the ACM International Conference on Information and Knowledge Management. the ACM International Conference on Information and Knowledge Management2024</p>
<p>Difficulty-controllable multi-hop question generation from knowledge graphs. V Kumar, Y Hua, G Ramakrishnan, G Qi, L Gao, Y.-F Li, 10.1007/978-3-030-30793-6_22International Semantic Web Conference. Springer2019</p>
<p>An interpretable reasoning network for multi-relation question answering. M Zhou, M Huang, X Zhu, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational Linguistics2018</p>
<p>In-context learning with topological information for llm-based knowledge graph completion. U M Sehwag, K Papasotiriou, J Vann, S Ganesh, ICML 2024 Workshop on Structured Probabilistic Inference {\&amp;} Generative Modeling. </p>
<p>URL. </p>
<p>An open challenge for inductive link prediction on knowledge graphs. M Galkin, M Berrendorf, C T Hoyt, arXiv:2203.015202022arXiv preprint</p>
<p>Gs-kgc: A generative subgraph-based framework for knowledge graph completion with large language models. R Yang, J Zhu, J Man, H Liu, L Fang, Y Zhou, 10.1016/j.inffus.2024.102868Information Fusion. 1171028682025</p>
<p>T Dettmers, P Minervini, P Stenetorp, S Riedel, 10.1609/aaai.v32i1.11573Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201832Convolutional 2d knowledge graph embeddings</p>
<p>Do pre-trained models benefit knowledge graph completion? a reliable evaluation and a reasonable approach. X Lv, Y Lin, Y Cao, L Hou, J Li, Z Liu, P Li, J Zhou, 10.18653/v1/2022.findings-acl.282Association for Computational Linguistics2022</p>
<p>Learning sequence encoders for temporal knowledge graph completion. A García-Durán, S Dumančić, M Niepert, arXiv:1809.032022018</p>
<p>Temporal knowledge graph reasoning based on evolutional representation learning. Z Li, X Jin, W Li, S Guan, J Guo, H Shen, Y Wang, X Cheng, 10.1145/3404835.3462963Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. the 44th international ACM SIGIR conference on research and development in information retrieval2021</p>
<p>Gltw: Joint improved graph transformer and llm via three-word language for knowledge graph completion. K Luo, Y Bai, C Gao, S Si, Y Shen, Z Liu, Z Wang, C Kong, W Li, Y Huang, arXiv:2502.114712025arXiv preprint</p>
<p>D Vrandečić, M Krötzsch, 10.1145/2629489Wikidata: a free collaborative knowledgebase. 201457</p>
<p>Iterative zero-shot llm prompting for knowledge graph construction. S Carta, A Giuliani, L Piano, A S Podda, L Pompianu, S G Tiddia, 10.48550/ARXIV.2307.01128arXiv:2307.011282023arXiv preprint</p>
<p>Exploring large language models for knowledge graph completion. L Yao, J Peng, C Mao, Y Luo, 10.48550/ARXIV.2308.13916arXiv:2308.139162023arXiv preprint</p>
<p>Reasoning with neural tensor networks for knowledge base completion. R Socher, D Chen, C D Manning, A Ng, Advances in neural information processing systems. 262013</p>
<p>Kicgpt: Large language model with knowledge in context for knowledge graph completion. Y Wei, Q Huang, Y Zhang, J Kwok, 10.18653/v1/2023.findings-emnlp.580Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Knowledge graph completion models are few-shot learners: An empirical study of relation labeling in e-commerce with llms. J Chen, L Ma, X Li, N Thakurdesai, J Xu, J H Cho, K Nag, E Korpeoglu, S Kumar, K Achan, 10.48550/ARXIV.2305.09858arXiv:2305.098582023arXiv preprint</p>
<p>Y Xu, S He, J Chen, Z Wang, Y Song, H Tong, G Liu, K Liu, J Zhao, 10.48550/ARXIV.2404.14741arXiv:2404.14741Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering. 2024arXiv preprint</p>
<p>Making large language models perform better in knowledge graph completion. Y Zhang, Z Chen, L Guo, Y Xu, W Zhang, H Chen, 10.1145/3664647.3681327Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>L Yao, C Mao, Y Luo, arXiv:1909.03193Kg-bert: Bert for knowledge graph completion. 2019arXiv preprint</p>
<p>Assessing llms suitability for knowledge graph completion. V I R Iga, G C Silaghi, 10.1007/978-3-031-71170-1_22International Conference on Neural-Symbolic Learning and Reasoning. Springer2024</p>
<p>. 10.1007/978-3-031-71170-1_22</p>
<p>Finetuning generative large language models with discrimination instructions for knowledge graph completion. Y Liu, X Tian, Z Sun, W Hu, 10.1007/978-3-031-77844-5_11International Semantic Web Conference. Springer2024</p>
<p>Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement. R Yang, J Zhu, J Man, L Fang, Y Zhou, 10.1016/j.knosys.2024.112155Knowledge-Based Systems. 3001121552024</p>
<p>Llm-based multilevel knowledge generation for few-shot knowledge graph completion. Q Li, Z Chen, C Ji, S Jiang, J Li, Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial Intelligence2024</p>
<p>Never-ending learning. T Mitchell, W Cohen, E Hruschka, P Talukdar, B Yang, J Betteridge, A Carlson, B Dalvi, M Gardner, B Kisiel, 10.1145/3191513Communications of the ACM. 6152018</p>
<p>Navigating networks by using homophily and degree. Ö Şimşek, D Jensen, 10.1073/pnas.0800497105Proceedings of the National Academy of Sciences. 105352008</p>
<p>Graph recurrent networks with attributed random walks. X Huang, Q Song, Y Li, X Hu, 10.1145/3292500.3330941Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2019</p>
<p>Attributed random walk as matrix factorization. L Chen, J Bruna, M Bronstein, Proceedings of the International Conference on Neural Information Processing Systems. the International Conference on Neural Information Processing Systems2019Graph Representation Learning Workshop</p>
<p>Accurate node feature estimation with structured variational graph autoencoder. J Yoo, H Jeon, J Jung, U Kang, 10.1145/3534678.3539337Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Initializing then refining: A simple graph attribute imputation network. W Tu, S Zhou, X Liu, Y Liu, Z Cai, E Zhu, Z Changwang, J Cheng, 10.24963/ijcai.2022/485Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial Intelligence2022</p>
<p>Simultaneous missing value imputation and structure learning with groups. P Morales-Alvarez, W Gong, A Lamb, S Woodhead, S P Jones, N Pawlowski, M Allamanis, C Zhang, Proceedings of the International Conference on Neural Information Processing Systems. the International Conference on Neural Information Processing Systems2022</p>
<p>Multi-view graph imputation network. X Peng, J Cheng, X Tang, B Zhang, W Tu, 10.1016/j.inffus.2023.102024Information Fusion. 1021020242024</p>
<p>Attribute imputation autoencoders for attribute-missing graphs. R Xia, C Zhang, A Li, X Liu, B Yang, 10.1016/j.knosys.2024.111583Knowledge-Based Systems. 2911115832024</p>
<p>Csat: Contrastive sampling-aggregating transformer for community detection in attributemissing networks. M Li, Y Zhang, W Zhang, S Zhao, X Piao, B Yin, 10.1109/TCSS.2023.3292145IEEE Transactions on Computational Social Systems. 1122024</p>
<p>Scae: Structural contrastive auto-encoder for incomplete multi-view representation learning. M Li, R Zhang, Y Zhang, X Piao, S Zhao, B Yin, 10.1145/3672078ACM Transactions on Multimedia Computing, Communications and Applications. 2092024</p>
<p>Topology-driven attribute recovery for attribute missing graph learning in social internet of things. M Li, J Chen, C Yu, G Jiang, R Zhang, Y Shen, H H Song, 10.48550/ARXIV.2501.10151IEEE Internet of Things Journal. 2025</p>
<p>Temporal decomposition and attribute correlation differentiation at multiple scales: A graph imputation network for incomplete multivariate time series, Knowledge-Based Systems. D Chen, L Zhang, X Lai, W Lu, Z Li, 10.1016/j.knosys.2025.1136362025</p>
<p>On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features. E Rossi, H Kenlay, M I Gorinova, B P Chamberlain, X Dong, M M Bronstein, Learning on Graphs Conference. PMLR2022</p>
<p>Confidence-based feature imputation for graphs with partially known features. D Um, J Park, S Park, J Y Choi, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Attrireboost: A gradient-free propagation optimization method for cold start mitigation in attribute missing graphs. M Li, C Ding, J Chen, W Xing, C Ye, R Zhang, S Zhuang, J Hu, T Z Qiu, H Gao, 10.48550/ARXIV.2501.00743arXiv:2501.007432025arXiv preprint</p>
<p>Cold brew: Distilling graph node representations with incomplete or missing neighborhoods. W Zheng, E W Huang, N Rao, S Katariya, Z Wang, K Subbian, The Tenth International Conference on Learning Representations. 2021</p>
<p>Graph contrastive learning with cohesive subgraph awareness. Y Wu, L Wang, X Han, H.-J Ye, 10.1145/3589334.3645470Proceedings of the ACM Web Conference. the ACM Web Conference2024</p>
<p>Oneshot relational learning for knowledge graphs. W Xiong, M Yu, S Chang, X Guo, W Y Wang, 10.18653/v1/d18-1223Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2018</p>
<p>Graph few-shot learning via knowledge transfer. H Yao, C Zhang, Y Wei, M Jiang, S Wang, J Huang, N Chawla, Z Li, 10.1609/aaai.v34i04.6142Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Few-shot knowledge graph completion. C Zhang, H Yao, C Huang, M Jiang, Z Li, N V Chawla, 10.1609/aaai.v34i03.5698Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202034</p>
<p>Adaptive attentional network for few-shot knowledge graph completion. J Sheng, S Guo, Z Chen, J Yue, L Wang, T Liu, H Xu, 10.18653/v1/2020.emnlp-main.131Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Distilling meta knowledge on heterogeneous graph for illicit drug trafficker detection on social media. Y Qian, Y Zhang, Y Ye, C Zhang, Proceedings of the International Conference on Neural Information Processing Systems. the International Conference on Neural Information Processing Systems2021</p>
<p>Augmenting low-resource text classification with graph-grounded pre-training and prompting. Z Wen, Y Fang, 10.1145/3539618.3591641Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval2023</p>
<p>Justifying recommendations using distantly-labeled reviews and fine-grained aspects. J Ni, J Li, J Mcauley, 10.18653/v1/D19-1018Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)2019</p>
<p>Large language model-based augmentation for imbalanced node classification on text-attributed graphs. L Wang, Y Wang, B Ni, Y Zhao, T Derr, arXiv:2410.168822024arXiv preprint</p>
<p>H Yan, C Li, R Long, C Yan, J Zhao, W Zhuang, J Yin, P Zhang, W Han, H Sun, A comprehensive study on text-attributed graphs: Benchmarking and rethinking. 202336</p>
<p>URL. </p>
<p>Fine-grainedly synthesize streaming data based on large language models with graph structure understanding for data sparsity. X Zhang, L Zhang, D Zhou, G Xu, arXiv:2403.061392024arXiv preprint</p>
<p>Distilling large language models for text-attributed graph learning. B Pan, Z Zhang, Y Zhang, Y Hu, L Zhao, 10.1145/3627673.3679830Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Depression detection in clinical interviews with llmempowered structural element graph. Z Chen, J Deng, J Zhou, J Wu, T Qian, M Huang, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>The distress analysis interview corpus of human and computer interviews. J Gratch, R Artstein, G M Lucas, G Stratou, S Scherer, A Nazarian, R Wood, J Boberg, D Devault, S Marsella, 2014LREC14Reykjavik</p>
<p>Automatic depression detection: An emotional audio-textual corpus and a gru/bilstmbased model. Y Shen, H Yang, L Lin, 10.1109/ICASSP43922.2022.9746569ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2022</p>
<p>When llm meets hypergraph: A sociological analysis on personality via online social networks. Z Shu, X Sun, H Cheng, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>T Zhang, R Yang, M Yan, X Ye, D Fan, Y Lai, arXiv:2412.11983Costeffective label-free node classification with llms. 2024arXiv preprint</p>
<p>Llm-tikg: Threat intelligence knowledge graph construction utilizing large language model. Y Hu, F Zou, J Han, X Sun, Y Wang, 10.1016/j.cose.2024.103999Computers &amp; Security. 1451039992024</p>
<p>Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis. X Liao, C Chen, Z Wang, Y Liu, T Wang, L Cheng, 10.1016/j.aei.2025.103134Advanced Engineering Informatics. 651031342025</p>
<p>Llm-empowered class imbalanced graph prompt learning for online drug trafficking detection. T Ma, Y Qian, Z Wang, Z Zhang, C Zhang, Y Ye, arXiv:2503.019002025arXiv preprint</p>
<p>Enhancing student performance prediction on learnersourced questions with sgnn-llm synergy. L Ni, S Wang, Z Zhang, X Li, X Zheng, P Denny, J Liu, 10.1609/aaai.v38i21.30370Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>X Runfeng, C Xiangyang, Y Zhou, W Xin, X Zhanwei, Z Kai, arXiv:2308.12028Lkpnr: Llm and kg for personalized news recommendation framework. 2023arXiv preprint</p>
<p>Mind: A largescale dataset for news recommendation. F Wu, Y Qiao, J.-H Chen, C Wu, T Qi, J Lian, D Liu, X Xie, J Gao, W Wu, 10.18653/v1/2020.acl-main.331Proceedings of the 58th annual meeting of the association for computational linguistics. the 58th annual meeting of the association for computational linguistics2020</p>
<p>Empowering graph neural network-based computational drug repositioning with large language model-inferred knowledge representation. Y Gu, Z Xu, C Yang, 10.1007/s12539-024-00654-7Interdisciplinary Sciences: Computational Life Sciences. 2024</p>
<p>Low-resource knowledge graph completion based on knowledge distillation driven by large language models. W Hou, W Zhao, N Jia, X Liu, 10.1016/j.asoc.2024.112622Applied Soft Computing. 1691126222025</p>
<p>Hopfe: Knowledge graph representation learning using inverse hopf fibrations, CIKM '21, Association for Computing Machinery. A Bastos, K Singh, A Nadgeri, S Shekarpour, I O Mulang, J Hoffart, 10.1145/3459637.34822632021New York, NY, USA</p>
<p>Graphclip: Enhancing transferability in graph foundation models for text-attributed graphs. Y Zhu, H Shi, X Wang, Y Liu, Y Wang, B Peng, C Hong, S Tang, arXiv:2410.103292024arXiv preprint</p>
<p>Can gnn be good adapter for llms?. X Huang, K Han, Y Yang, D Bao, Q Tao, Z Chai, Q Zhu, 10.1145/3589334.3645627Proceedings of the ACM Web Conference 2024. the ACM Web Conference 20242024</p>
<p>Graphedit: Large language models for graph structure learning. Z Guo, L Xia, Y Yu, Y Wang, Z Yang, W Wei, L Pang, T.-S Chua, C Huang, arXiv:2402.151832024arXiv preprint</p>
<p>Subgraph-aware training of language models for knowledge graph completion using structure-aware contrastive learning. Y Ko, H Yang, T Kim, H Kim, arXiv:2407.127032024arXiv preprint</p>
<p>Kepler: A unified model for knowledge embedding and pre-trained language representation. X Wang, T Gao, Z Zhu, Z Zhang, Z Liu, J Li, J Tang, 10.1162/tacl_a_00360Transactions of the Association for Computational Linguistics. 92021</p>
<p>D Xu, Z Zhang, Z Lin, X Wu, Z Zhu, T Xu, X Zhao, Y Zheng, E Chen, arXiv:2403.01972Multi-perspective improvement of knowledge graph completion with large language models. 2024arXiv preprint</p>
<p>Can large language models improve the adversarial robustness of graph neural networks?. Z Zhang, X Wang, H Zhou, Y Yu, M Zhang, C Yang, C Shi, 10.48550/ARXIV.2408.08685arXiv:2408.086852024arXiv preprint</p>
<p>B Thapaliya, A Nguyen, Y Lu, T Xie, I Grudetskyi, F Lin, A Valkanas, J Liu, D Chakraborty, B Fehri, arXiv:2410.11765Ecgn: A cluster-aware approach to graph neural networks for imbalanced classification. 2024arXiv preprint</p>
<p>W Ju, Z Mao, S Yi, Y Qin, Y Gu, Z Xiao, J Shen, Z Qiao, M Zhang, arXiv:2412.12984Cluster-guided contrastive class-imbalanced graph classification. 2024arXiv preprint</p>
<p>Cluster-guided contrastive graph clustering network. X Yang, Y Liu, S Zhou, S Wang, W Tu, Q Zheng, X Liu, L Fang, E Zhu, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202337</p>
<p>Imgcl: Revisiting graph contrastive learning on imbalanced node classification. L Zeng, L Li, Z Gao, P Zhao, J Li, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence202337</p>
<p>Graphsha: Synthesizing harder samples for class-imbalanced node classification. W.-Z Li, C.-D Wang, H Xiong, J.-H Lai, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Tam: topology-aware margin loss for class-imbalanced node classification. J Song, J Park, E Yang, International Conference on Machine Learning. PMLR2022</p>
<p>Rahnet: Retrieval augmented hybrid network for long-tailed graph classification. Z Mao, W Ju, Y Qin, X Luo, M Zhang, 10.1145/3581783.3612360Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>Towards long-tailed recognition for graph classification via collaborative experts. S.-Y Yi, Z Mao, W Ju, Y.-D Zhou, L Liu, X Luo, M Zhang, 10.1109/TBDATA.2023.3313029IEEE Transactions on Big Data. 962023</p>
<p>Learning knowledge-diverse experts for long-tailed graph classification. Z Mao, W Ju, S Yi, Y Wang, Z Xiao, Q Long, N Yin, X Liu, M Zhang, 10.1145/3705323ACM Transactions on Knowledge Discovery from Data. 1922025</p>
<p>Heterophilic graph invariant learning for out-of-distribution of fraud detection. L Ren, R Hu, Z Wang, Y Xiao, D Li, J Wu, Y Zang, J Hu, Z Huang, 10.1145/3664647.3681312Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>J Liu, M He, G Wang, N Q V Hung, X Shang, H Yin, arXiv:2304.14635Imbalanced node classification beyond homophilic assumption. 2023arXiv preprint</p>
<p>On generalized degree fairness in graph neural networks. Z Liu, T.-K Nguyen, Y Fang, 10.1609/aaai.v37i4.25574Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Toward degree bias in embedding-based knowledge graph completion. H Shomer, W Jin, W Wang, J Tang, 10.1145/3543507.3583544Proceedings of the ACM Web Conference 2023. the ACM Web Conference 20232023</p>
<p>On size-oriented long-tailed graph classification of graph neural networks. Z Liu, Q Mao, C Liu, Y Fang, J Sun, 10.1145/3485447.3512197Proceedings of the ACM web conference 2022. the ACM web conference 20222022</p>
<p>Sailor: Structural augmentation based tail node representation learning. J Liao, J Li, L Chen, B Wu, Y Bian, Z Zheng, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>Hierarchical relational learning for few-shot knowledge graph completion. H Wu, J Yin, B Rajaratnam, J Guo, arXiv:2209.012052022arXiv preprint</p>
<p>Qtiahgnn: Quantity and topology imbalance-aware heterogeneous graph neural network for bankruptcy prediction. Y Liu, Z Gao, X Liu, P Luo, Y Yang, H Xiong, 10.1145/3580305.3599479Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Graph-based data integration and business intelligence with biiig. A Petermann, M Junghanns, R Müller, E Rahm, 10.14778/2733004.2733034Proceedings of the VLDB Endowment. the VLDB Endowment2014</p>
<p>Semantic data integration for knowledge graph construction at query time. D Collarana, M Galkin, I Traverso-Ribón, C Lange, M.-E Vidal, S Auer, 10.1109/ICSC.2017.85International Conference on Semantic Computing. 2017</p>
<p>Multi-source education knowledge graph construction and fusion for college curricula. Z Li, L Cheng, C Zhang, X Zhu, H Zhao, 10.1109/ICALT58122.2023.00111International Conference on Advanced Learning Technologies. 2023</p>
<p>Learning cross-domain landmarks for heterogeneous domain adaptation. Y.-H H Tsai, Y.-R Yeh, Y.-C F Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>One for all: Towards training one graph model for all classification tasks. H Liu, J Feng, L Kong, N Liang, D Tao, Y Chen, M Zhang, International Conference on Learning Representations. 2024</p>
<p>Graph learning under distribution shifts: A comprehensive survey on domain adaptation, out-ofdistribution, and continual learning. M Wu, X Zheng, Q Zhang, X Shen, X Luo, X Zhu, S Pan, arXiv:2402.163742024arXiv preprint</p>
<p>C Peng, J He, F Xia, arXiv:2402.05322Learning on multimodal graphs: A survey. 2024arXiv preprint</p>
<p>Revisiting, benchmarking and understanding unsupervised graph domain adaptation. M Liu, Z Zhang, J Tang, J Bu, B He, S Zhou, Advances in Neural Information Processing Systems. 2024</p>
<p>Llmrec: Large language models with graph augmentation for recommendation. W Wei, X Ren, J Tang, Q Wang, L Su, S Cheng, J Wang, D Yin, C Huang, 10.1145/3616855.3635853Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining2024</p>
<p>The movielens datasets: History and context, Acm transactions on interactive intelligent systems (tiis. F M Harper, J A Konstan, 10.1145/28278722015</p>
<p>Inc Netflix, Netflix prize data. </p>
<p>Multimodal fusion of ehr in structures and semantics: Integrating clinical records and notes with hypergraph and llm. H Cui, X Fang, R Xu, X Kan, J C Ho, C Yang, arXiv:2403.088182024arXiv preprint</p>
<p>Mimic-iii, a freely accessible critical care database. A E Johnson, T J Pollard, L Shen, L.-W H Lehman, M Feng, M Ghassemi, B Moody, P Szolovits, L Anthony Celi, R G Mark, 10.1038/sdata.2016.35Scientific data. 2016</p>
<p>Bootstrapping heterogeneous graph representation learning via large language models: A generalized approach. H Gao, C Zhang, F Wu, J Zhao, C Zheng, H Liu, arXiv:2412.080382024arXiv preprint</p>
<p>Heterogeneous graph neural network. C Zhang, D Song, C Huang, A Swami, N V Chawla, 10.1145/3292500.3330961Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining2019</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, 10.1039/C7SC02664AChemical science. 2018</p>
<p>Chembl: a largescale bioactivity database for drug discovery. A Gaulton, L J Bellis, A P Bento, J Chambers, M Davies, A Hersey, Y Light, S Mcglinchey, D Michalovich, B Al-Lazikani, 10.1093/nar/gkr777Nucleic acids research. 2012</p>
<p>Learning a cross-domain graph foundation model from natural language. Y He, B Hooi, Unigraph, arXiv:2402.136302024arXiv preprint</p>
<p>Y Cheng, Y Zhao, J Yu, X Li, arXiv:2407.19941Boosting graph foundation model from structural perspective. 2024arXiv preprint</p>
<p>Y Feng, S Liu, X Han, S Du, Z Wu, H Hu, Y Gao, arXiv:2503.01203Hypergraph foundation model. 2025arXiv preprint</p>
<p>Graphsaint: Graph sampling based inductive learning method. H Zeng, H Zhou, A Srivastava, R Kannan, V Prasanna, International Conference on Learning Representations. 2020</p>
<p>Multimodal graph learning for generative tasks. M Yoon, J Y Koh, B Hooi, R Salakhutdinov, NeurIPS 2023 Workshop: New Frontiers in Graph Learning. 2023</p>
<p>A suite of generative tasks for multi-level multimodal webpage understanding. A Burns, K Srinivasan, J Ainslie, G Brown, B Plummer, K Saenko, J Ni, M Guo, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Graphadapter: Tuning vision-language models with dual knowledge graph. X Li, D Lian, Z Lu, J Bai, Z Chen, X Wang, Advances in Neural Information Processing Systems. 202436</p>
<p>Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, 10.1109/CVPR.2009.52068482009 IEEE conference on computer vision and pattern recognition. 2009</p>
<p>3d object representations for fine-grained categorization. J Krause, M Stark, J Deng, L Fei-Fei, 10.1109/ICCVW.2013.77Proceedings of the IEEE international conference on computer vision workshops. the IEEE international conference on computer vision workshops2013</p>
<p>Ucf101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.04022012arXiv preprint</p>
<p>Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. L Fei-Fei, R Fergus, P Perona, 10.1109/CVPR.2004.3832004 conference on computer vision and pattern recognition workshop. 2004</p>
<p>Automated flower classification over a large number of classes. M.-E Nilsback, A Zisserman, 10.1109/ICVGIP.2008.47Sixth Indian conference on computer vision, graphics &amp; image processing. 2008. 2008</p>
<p>Sun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, 10.1109/CVPR.2010.5539970IEEE computer society conference on computer vision and pattern recognition. 2010. 2010</p>
<p>Describing textures in the wild. M Cimpoi, S Maji, I Kokkinos, S Mohamed, A Vedaldi, 10.1109/CVPR.2014.461Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2014</p>
<p>Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. P Helber, B Bischke, A Dengel, D Borth, 10.1109/JSTARS.2019.2918242IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. 2019</p>
<p>Fine-grained visual classification of aircraft. S Maji, E Rahtu, J Kannala, M Blaschko, A Vedaldi, arXiv:1306.51512013arXiv preprint</p>
<p>Cats and dogs. O M Parkhi, A Vedaldi, A Zisserman, C Jawahar, 10.1109/CVPR.2012.62480922012 IEEE conference on computer vision and pattern recognition. 2012</p>
<p>Food-101-mining discriminative components with random forests. L Bossard, M Guillaumin, L Van Gool, 10.1007/978-3-319-10599-4_29European conference on computer vision. 2014</p>
<p>Touchup-g: Improving feature representation through graph-centric finetuning. J Zhu, X Song, V Ioannidis, D Koutra, C Faloutsos, 10.1145/3626772.3657978Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Unigraph2: Learning a unified embedding space to bind multimodal graphs. Y He, Y Sui, X He, Y Liu, Y Sun, B Hooi, arXiv:2502.008062025arXiv preprint</p>
<p>J Zhu, Y Zhou, S Qian, Z He, T Zhao, N Shah, D Koutra, Multimodal graph benchmark. 2024arXiv e-prints</p>
<p>Talk like a graph: Encoding graphs for large language models. B Fatemi, J Halcrow, B Perozzi, International Conference on Learning Representations. 2024</p>
<p>Gpt4graph: Can large language models understand graph structured data? an empirical evaluation and benchmarking. J Guo, L Du, H Liu, M Zhou, X He, S Han, arXiv:2305.150662023arXiv preprint</p>
<p>Variational reasoning for question answering with knowledge graph. Y Zhang, H Dai, Z Kozareva, A Smola, L Song, 10.1609/aaai.v32i1.12057Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2018</p>
<p>Graph to visual and textual integration for vision-language graph reasoning. Y Wei, S Fu, W Jiang, Z Zhang, Z Zeng, Q Wu, J Kwok, Y Zhang, Gita , Advances in Neural Information Processing Systems. 2024</p>
<p>Beyond graphs: Can large language models comprehend hypergraphs?. Y Feng, C Yang, X Hou, S Du, S Ying, Z Wu, Y Gao, International Conference on Learning Representations. 2025</p>
<p>J Zhao, L Zhuo, Y Shen, M Qu, K Liu, M Bronstein, Z Zhu, J Tang, arXiv:2310.01089Graphtext: Graph reasoning in text space. 2023arXiv preprint</p>
<p>Geom-gcn: Geometric graph convolutional networks. H Pei, B Wei, K C , .-C Chang, Y Lei, B Yang, International Conference on Learning Representations. 2020</p>
<p>Walklm: A uniform language model fine-tuning framework for attributed graph embedding. Y Tan, Z Zhou, H Lv, W Liu, C Yang, Advances in Neural Information Processing Systems. 2024</p>
<p>Path-llm: A shortest-pathbased llm learning for unified graph representation. W Shang, X Zhu, X Huang, arXiv:2408.054562024arXiv preprint</p>
<p>Language is all a graph needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Findings of the Association for Computational Linguistics: EACL 2024. 2024</p>
<p>Musegraph: Graph-oriented instruction tuning of large language models for generic graph mining. Y Tan, H Lv, X Huang, J Zhang, S Wang, C Yang, arXiv:2403.047802024</p>
<p>Text generation from knowledge graphs with graph transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics2019</p>
<p>Creating training corpora for nlg microplanning. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, 55th Annual Meeting of the Association for Computational Linguistics. 2017</p>
<p>Exploring the potential of large language models (llms) in learning on graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, 10.1145/3655103.3655110ACM SIGKDD Explorations Newsletter. 2024</p>
<p>Graph neural prompting with large language models. Y Tian, H Song, Z Wang, H Wang, Z Hu, F Wang, N V Chawla, P Xu, 10.1609/aaai.v38i17.29875Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Can a suit of armor conduct electricity? a new dataset for open book question answering. T Mihaylov, P Clark, T Khot, A Sabharwal, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>P Clark, I Cowhey, O Etzioni, T Khot, A Sabharwal, C Schoenick, O Tafjord, arXiv:1803.05457Think you have solved question answering? try arc, the ai2 reasoning challenge. 2018arXiv preprint</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Y Bisk, R Zellers, J Gao, Y Choi, 10.1609/aaai.v34i05.6239Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2020</p>
<p>Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. B Y Lin, Z Wu, Y Yang, D.-H Lee, X Ren, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Pubmedqa: A dataset for biomedical research question answering. Q Jin, B Dhingra, Z Liu, W Cohen, X Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. G Tsatsaronis, G Balikas, P Malakasiotis, I Partalas, M Zschunke, M R Alvers, D Weissenborn, A Krithara, S Petridis, D Polychronopoulos, 10.1186/s12859-015-0564-6BMC bioinformatics. 2015</p>
<p>Let your graph do the talking: Encoding structured data for llms. B Perozzi, B Fatemi, D Zelle, A Tsitsulin, M Kazemi, R Al-Rfou, J Halcrow, arXiv:2402.058622024arXiv preprint</p>
<p>Graphgpt: Graph instruction tuning for large language models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, 10.1145/3626772.3657775Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>LLaGA: Large language and graph assistant. R Chen, T Zhao, A K Jaiswal, N Shah, Z Wang, International Conference on Machine Learning. 2024</p>
<p>LLMs as zero-shot graph learners: Alignment of GNN representations with LLM token embeddings. D Wang, Y Zuo, F Li, J Wu, Advances in Neural Information Processing Systems. 2024</p>
<p>Higpt: Heterogeneous graph language model. J Tang, Y Yang, W Wei, L Shi, L Xia, D Yin, C Huang, 10.1145/3637528.3671987Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2024</p>
<p>Graphformers: Gnn-nested transformers for representation learning on textual graph. J Yang, Z Liu, S Xiao, C Li, D Lian, S Agrawal, A Singh, G Sun, X Xie, Advances in Neural Information Processing Systems. 2021</p>
<p>Information heterogeneity between progress notes by physicians and nurses for inpatients with digestive system diseases. Y Mashima, M Tanigawa, H Yokoi, 10.1038/s41598-024-56324-7Scientific Reports. 2024</p>
<p>Generalized funnelling: Ensemble learning and heterogeneous document embeddings for cross-lingual text classification. A Moreo, A Pedrotti, F Sebastiani, 10.1145/3544104ACM Transactions on Information Systems. 2022</p>
<p>Extracting information from the text of electronic medical records to improve case detection: a systematic review. E Ford, J A Carroll, H E Smith, D Scott, J A Cassell, 10.1093/jamiaopen/ooae044Journal of the American Medical Informatics Association. 2016</p>
<p>10.1093/jamiaopen/ooae044URL. </p>
<p>Data integration and machine learning: A natural synergy. X L Dong, T Rekatsinas, 10.1145/3183713.3197387Proceedings of the 2018 International Conference on Management of Data. the 2018 International Conference on Management of Data2018</p>
<p>Natural language processing of clinical notes on chronic diseases: systematic review. S Sheikhalishahi, R Miotto, J T Dudley, A Lavelli, F Rinaldi, V Osmani, 10.2196/12239JMIR medical informatics. 2019</p>
<p>A statistical interpretation of term specificity and its application in retrieval. K Sparck Jones, 10.1108/eb026526Journal of Documentation. 1972</p>
<p>Network representation learning with rich text information. C Yang, Z Liu, D Zhao, M Sun, E Y Chang, International Joint Conference on Artificial Intelligence. 2015</p>
<p>Paper2vec: Combining graph and text information for scientific paper representation. S Ganguly, V Pudi, 10.1007/978-3-319-56608-5_30Advances in Information Retrieval. 2017</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in Neural Information Processing Systems. 2013</p>
<p>Node feature extraction by self-supervised multi-scale neighborhood prediction. E Chien, W.-C Chang, C.-J Hsieh, H.-F Yu, J Zhang, O Milenkovic, I S Dhillon, International Conference on Learning Representations. 2022</p>
<p>E2eg: End-to-end node classification using graph topology and text-based node attributes. T A Dinh, J Boef, J Cornelisse, P Groth, IEEE International Conference on Data Mining Workshops. 2023</p>
<p>Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. B Jin, Y Zhang, Q Zhu, J Han, 10.1145/3580305.3599376Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining. the 29th ACM SIGKDD conference on knowledge discovery and data mining2023</p>
<p>B Jin, W Zhang, Y Zhang, Y Meng, H Zhao, J Han, arXiv:2310.06684Learning multiplex representations on text-attributed graphs with one language model encoder. 2023arXiv preprint</p>
<p>Graph fusion network-based multimodal learning for freezing of gait detection. K Hu, Z Wang, K A E Martens, M Hagenbuchner, M Bennamoun, A C Tsoi, S J Lewis, 10.1109/TNNLS.2021.3105602IEEE Transactions on Neural Networks and Learning Systems. 2021</p>
<p>Multimodal graph convolutional networks for high quality content recognition. J Wang, J Hu, S Qian, Q Fang, C Xu, 10.1016/j.neucom.2020.04.145Neurocomputing. 2020</p>
<p>Mmgcn: Multi-modal graph convolution network for personalized recommendation of micro-video. Y Wei, X Wang, L Nie, X He, R Hong, T.-S Chua, 10.1145/3343031.3351034Proceedings of the 27th ACM International Conference on Multimedia. the 27th ACM International Conference on Multimedia2019</p>
<p>W Wilcke, P Bloem, V De Boer, R Van T Veer, F Van Harmelen, arXiv:2003.12383End-to-end entity classification on multimodal knowledge graphs. 2020arXiv preprint</p>
<p>Graph neural networks for multimodal single-cell data integration. H Wen, J Ding, W Jin, Y Wang, Y Xie, J Tang, 10.1145/3534678.3539213Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Graph transformer geometric learning of brain networks using multimodal mr images for brain age estimation. H Cai, Y Gao, M Liu, 10.1109/TMI.2022.3222093IEEE Transactions on Medical Imaging. 2022</p>
<p>Multimodal graph transformer for multimodal question answering. X He, X E Wang, 10.18653/v1/2023.eacl-main.15Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Joyful: Joint modality fusion and graph contrastive learning for multimoda emotion recognition. D Li, Y Wang, K Funakoshi, M Okumura, 10.18653/v1/2023.emnlp-main.996Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Formnetv2: Multimodal graph contrastive learning for form document information extraction. C.-Y Lee, C.-L Li, H Zhang, T Dozat, V Perot, G Su, X Zhang, K Sohn, N Glushnev, R Wang, 10.18653/v1/2023.acl-long.501Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023</p>
<p>Modeling intra-and inter-modal relations: Hierarchical graph contrastive learning for multimodal sentiment analysis. Z Lin, B Liang, Y Long, Y Dang, M Yang, M Zhang, R Xu, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational Linguistics2022</p>
<p>Unibind: Llmaugmented unified and balanced representation space to bind them all. Y Lyu, X Zheng, J Zhou, L Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Emu: Generative pretraining in multimodality. Q Sun, Q Yu, Y Cui, F Zhang, X Zhang, Y Wang, H Gao, J Liu, T Huang, X Wang, International Conference on Learning Representations. 2024</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, International Conference on Machine Learning. 2023</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, Advances in Neural Information Processing Systems. 2024</p>
<p>Modaverse: Efficiently transforming modalities with llms. X Wang, B Zhuang, Q Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International Conference on Machine Learning. 2021</p>
<p>Physics-informed deep learning for traffic state estimation based on the traffic flow model and computational graph method. J Zhang, S Mao, L Yang, W Ma, S Li, Z Gao, 10.1016/j.inffus.2023.101971Information Fusion. 1011019712024</p>
<p>Multi-frequency spatial-temporal graph neural network for short-term metro od demand prediction during public health emergencies, Transportation (2025) 1-23. J Zhang, S Zhang, H Zhao, Y Yang, M Liang, 10.1007/s11116-025-10582-0</p>
<p>Toward greater intelligence in route planning: A graph-aware deep learning approach. Z Zhuang, J Wang, Q Qi, H Sun, J Liao, 10.1109/JSYST.2019.2922217IEEE Systems Journal. 2019</p>
<p>Graph domain adaptation via theory-grounded spectral regularization. Y You, T Chen, Z Wang, Y Shen, International Conference on Learning Representations. 2023</p>
<p>Domain adaptation on graphs by learning aligned graph bases. M Pilancı, E Vural, 10.1109/TKDE.2020.2984212IEEE Transactions on Knowledge and Data Engineering. 2020</p>
<p>X Zheng, M Zhang, C Chen, S Molaei, C Zhou, S Pan, Gnnevaluator, Evaluating gnn performance on unseen graphs without labels. 2023Advances in Neural Information Processing Systems</p>
<p>U Brandes, M Eiglsperger, J Lerner, C Pich, Graph markup language (graphml). 2010</p>
<p>J Gu, X Jiang, Z Shi, H Tan, X Zhai, C Xu, W Li, Y Shen, S Ma, H Liu, arXiv:2411.15594A survey on llm-as-a-judge. 2024arXiv preprint</p>
<p>Dynamic neural networks: A survey. Y Han, G Huang, S Song, L Yang, H Wang, Y Wang, 10.1109/TPAMI.2021.3117837IEEE Transactions on Pattern Analysis and Machine Intelligence. 44112022</p>
<p>Temporal graph networks for deep learning on dynamic graphs. E Rossi, B Chamberlain, F Frasca, D Eynard, F Monti, M Bronstein, arXiv:2006.106372020</p>
<p>Dysat: Deep neural representation learning on dynamic graphs via self-attention networks. A Sankar, Y Wu, L Gou, W Zhang, H Yang, 10.1145/3336191.3371845Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM '20. the 13th International Conference on Web Search and Data Mining, WSDM '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Temporal knowledge graph forecasting without knowledge using in-context learning. D.-H Lee, K Ahrabian, W Jin, F Morstatter, J Pujara, 10.18653/v1/2023.emnlp-main.36Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Deriving validity time in knowledge graph. J Leblay, M W Chekol, 10.1145/3184558.3191639Companion Proceedings of the The Web Conference 2018, WWW '18, International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva. 2018</p>
<p>F Mahdisoltani, J Biega, F M Suchanek, YAGO3: A Knowledge Base from Multilingual Wikipedias, in: CIDR, Asilomar, United States. 2013</p>
<p>W Jin, M Qu, X Jin, X Ren, 10.18653/v1/2020.emnlp-main.541Recurrent event network: Autoregressive structure inferenceover temporal knowledge graphs. B Webber, T Cohn, Y He, Y Liu, Online2020Association for Computational Linguistics</p>
<p>zrLLM: Zero-shot relational learning on temporal knowledge graphs with large language models. Z Ding, H Cai, J Wu, Y Ma, R Liao, B Xiong, V Tresp, 10.18653/v1/2024.naacl-long.104Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, Mexico20241Association for Computational Linguistics</p>
<p>. E Boschee, J Lautenschlager, S O'brien, S Shellman, J Starz, M Ward, 10.7910/DVN/28075ICEWS Coded Event Data. 2015</p>
<p>Introducing acled: An armed conflict location and event dataset. C Raleigh, H Linke, J Hegre, Karlsen, 10.1177/0022343310378914doi:10.1177/0022343310378914Journal of Peace Research. 4752010</p>
<p>Chain-of-history reasoning for temporal knowledge graph forecasting. Y Xia, D Wang, Q Liu, L Wang, S Wu, X.-Y Zhang, 10.18653/v1/2024.findings-acl.955Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational Linguistics2024</p>
<p>Large language models can learn temporal reasoning. S Xiong, A Payani, R Kompella, F Fekri, L.-W</p>
<p>10.18653/v1/2024.acl-long.563Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. A Ku, V Martins, Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, Thailand20241</p>
<p>Large language models can learn temporal reasoning. S Xiong, A Payani, R Kompella, F Fekri, arXiv:2401.068532024</p>
<p>A dataset for answering time-sensitive questions. W Chen, X Wang, W Y Wang, arXiv:2108.063142021</p>
<p>Q Tan, H T Ng, L Bing, arXiv:2306.08952Towards benchmarking and improving the temporal reasoning capability of large language models. 2023</p>
<p>Llm4dyg: Can large language models solve spatialtemporal problems on dynamic graphs?. Z Zhang, X Wang, Z Zhang, H Li, Y Qin, W Zhu, 10.1145/3637528.3671709Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The enron email dataset database schema and brief statistical report. J Shetty, J Adibi, Information sciences institute technical report, University of Southern California. 20044</p>
<p>Arnetminer: extraction and mining of academic social networks. J Tang, J Zhang, L Yao, J Li, L Zhang, Z Su, 10.1145/1401890.1402008Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '08. the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '08New York, NY, USAAssociation for Computing Machinery2008</p>
<p>Bringing up opensky: A large-scale adsb sensor network for research. M Schäfer, M Strohmeier, V Lenders, I Martinovic, M Wilhelm, 10.1109/IPSN.2014.6846743IPSN-14 Proceedings of the 13th International Symposium on Information Processing in Sensor Networks. 2014</p>
<p>Time-aware retrieval-augmented large language models for temporal knowledge graph question answering. X Qian, Y Zhang, Y Zhao, B Zhou, X Sui, L Zhang, K Song, 10.18653/v1/2024.emnlp-main.394Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA20244</p>
<p>Multi-granularity temporal question answering over knowledge graphs. Z Chen, J Liao, X Zhao, 10.18653/v1/2023.acl-long.637Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, Canada20231Association for Computational Linguistics</p>
<p>Complex temporal question answering on knowledge graphs. Z Jia, S Pramanik, R Saha Roy, G Weikum, 10.1145/3459637.3482416Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21. the 30th ACM International Conference on Information &amp; Knowledge Management, CIKM '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Two-stage generative question answering on temporal knowledge graph using large language models. Y Gao, L Qiao, Z Kan, Z Wen, Y He, D Li, 10.18653/v1/2024.findings-acl.401Findings of the Association for Computational Linguistics: ACL 2024, Association for Computational Linguistics. L.-W Ku, A Martins, V Srikumar, Bangkok, Thailand2024</p>
<p>Question answering over temporal knowledge graphs. A Saxena, S Chakrabarti, P Talukdar, arXiv:2106.015152021</p>
<p>Unveiling llms: The evolution of latent representations in a dynamic knowledge graph. M Bronzini, C Nicolini, B Lepri, J Staiano, A Passerini, arXiv:2404.036232024</p>
<p>Fever: a large-scale dataset for fact extraction and verification. J Thorne, A Vlachos, C Christodoulopoulos, A Mittal, arXiv:1803.053552018</p>
<p>Climate-fever: A dataset for verification of real-world climate claims. T Diggelmann, J Boyd-Graber, J Bulian, M Ciaramita, M Leippold, arXiv:2012.006142021</p>
<p>Findkg: Dynamic knowledge graphs with large language models for detecting global trends in financial markets. X V Li, F Sanna, Passino, 10.1145/3677052.3698603Proceedings of the 5th ACM International Conference on AI in Finance, ICAIF '24. the 5th ACM International Conference on AI in Finance, ICAIF '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>R Liao, X Jia, Y Li, Y Ma, V Tresp, Gentkg , 10.18653/v1/2024.findings-naacl.268Generative forecasting on temporal knowledge graph with large language models. K Duh, H Gomez, S Bethard, Mexico City, MexicoAssociation for Computational Linguistics2024Findings of the Association for Computational Linguistics: NAACL 2024</p>
<p>Gdelt: Global data on events, location, and tone. K Leetaru, P A Schrodt, ISA annual convention. 21979-2012. 2013Citeseer</p>
<p>Up to date: Automatic updating knowledge graphs using llms. S Hatem, G Khoriba, M H Gad-Elrab, M Elhelw, 10.1016/j.procs.2024.10.2066th International Conference on AI in Computational Linguistics. 2024244</p>
<p>Pre-trained language model with prompts for temporal knowledge graph completion. W Xu, B Liu, M Peng, X Jia, M Peng, 10.18653/v1/2023.findings-acl.493Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Large language models-guided dynamic adaptation for temporal knowledge graph reasoning. J Wang, K Sun, L Luo, W Wei, Y Hu, A W , .-C Liew, S Pan, B Yin, arXiv:2405.141702024</p>
<p>Back to the future: Towards explainable temporal reasoning with large language models. C Yuan, Q Xie, J Huang, S Ananiadou, 10.1145/3589334.3645376Proceedings of the ACM Web Conference 2024, WWW '24. the ACM Web Conference 2024, WWW '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Realtcd: Temporal causal discovery from interventional data with large language model. P Li, X Wang, Z Zhang, Y Meng, F Shen, Y Li, J Wang, Y Li, W Zhu, 10.1145/3627673.3680042Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24. the 33rd ACM International Conference on Information and Knowledge Management, CIKM '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Dynllm: When large language models meet dynamic graph recommendation. Z Zhao, F Lin, X Zhu, Z Zheng, T Xu, S Shen, X Li, Z Yin, E Chen, arXiv:2405.075802024</p>
<p>Ijcai-16 brick-and-mortar store recommendation dataset. Tianchi, 2018</p>
<p>Ad display/click data on taobao. Tianchi, 2018</p>
<p>Time-aware language models as temporal knowledge bases. B Dhingra, J R Cole, J M Eisenschlos, D Gillick, J Eisenstein, W W Cohen, Transactions of the Association for Computational Linguistics. 102022</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Bbq: A hand-built bias benchmark for question answering. A Parrish, A Chen, N Nangia, V Padmakumar, J Phang, J Thompson, P M Htut, S R Bowman, 10.18653/v1/2022.findings-acl.1652022Findings of the Association for Computational Linguistics</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, 10.18653/v1/2023.findings-acl.8242023Association for Computational Linguistics</p>
<p>Language-grounded dynamic scene graphs for interactive object search with mobile manipulation. D Honerkamp, M Büchner, F Despinoy, T Welschehold, A Valada, 10.1109/LRA.2024.3441495IEEE Robotics and Automation Letters. 9102024</p>
<p>igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Martín-Martín, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, arXiv:2108.032722021arXiv preprint</p>
<p>Temporal relational reasoning of large language models for detecting stock portfolio crashes. K J L Koa, Y Ma, R Ng, H Zheng, T.-S Chua, arXiv:2410.172662024</p>
<p>Using structured events to predict stock price movement: An empirical investigation. X Ding, Y Zhang, T Liu, J Duan, 10.3115/v1/d14-1148Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), ACL. the 2014 conference on empirical methods in natural language processing (EMNLP), ACL2014</p>
<p>Inductive representation learning on temporal graphs. Da Xu, International Conference on Learning Representations (ICLR). 2020</p>
<p>Temporal knowledge graph question answering: A survey. M Su, Z Li, Z Chen, L Bai, X Jin, J Guo, arXiv:2406.141912024</p>
<p>Temporal knowledge graph completion: a survey. B Cai, Y Xiang, L Gao, H Zhang, Y Li, J Li, 10.24963/ijcai.2023/734Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '23. the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '232023</p>
<p>Graphrnn: Generating realistic graphs with deep auto-regressive models. J You, R Ying, X Ren, W Hamilton, J Leskovec, International conference on machine learning. PMLR2018</p>
<p>Efficient graph generation with graph recurrent attention networks. R Liao, Y Li, Y Song, S Wang, W L Hamilton, D Duvenaud, R Urtasun, R Zemel, 2019Curran Associates IncRed Hook, NY, USA</p>
<p>P Goyal, N Kamra, X He, Y Liu, arXiv:1805.11273Dyngem: Deep embedding method for dynamic graphs. 2018arXiv preprint</p>
<p>A survey of link prediction in temporal networks. J Xiong, A Zareie, R Sakellariou, arXiv:2502.211852025</p>
<p>A Paranjape, A R Benson, J Leskovec, 10.1145/3018661.3018731Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, WSDM '17. the Tenth ACM International Conference on Web Search and Data Mining, WSDM '17New York, NY, USAAssociation for Computing Machinery2017Motifs in temporal networks</p>
<p>Sessionbased recommendations with recurrent neural networks. B Hidasi, A Karatzoglou, L Baltrunas, D Tikk, 4th International Conference on Learning Representations, ICLR 2016. Y Bengio, Y Lecun, San Juan, Puerto RicoMay 2-4, 2016. 2016Conference Track Proceedings</p>
<p>R Xue, X Shen, R Yu, X Liu, arXiv:2312.04737Efficient large language models fine-tuning on graphs. 2023arXiv preprint</p>
<p>E Coppolillo, arXiv:2505.07554Injecting knowledge graphs into large language models. 2025arXiv preprint</p>
<p>Graph machine learning in the era of large language models (llms). S Wang, J Huang, Z Chen, Y Song, W Tang, H Mao, W Fan, H Liu, X Liu, D Yin, 10.1145/3732786ACM Transactions on Intelligent Systems and Technology. 2024</p>
<p>Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 122022</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Advances in neural information processing systems. 362023</p>
<p>Fairness in graph mining: A survey. Y Dong, J Ma, S Wang, C Chen, J Li, 10.1109/TKDE.2023.3265598IEEE Transactions on Knowledge and Data Engineering. 35102023</p>
<p>Bias and fairness in large language models: A survey. I O Gallegos, R A Rossi, J Barrow, M M Tanjim, S Kim, F Dernoncourt, T Yu, R Zhang, N K Ahmed, 10.1162/coli_a_00524Computational Linguistics. 5032024</p>
<p>Gnnexplainer: Generating explanations for graph neural networks. Z Ying, D Bourgeois, J You, M Zitnik, J Leskovec, Advances in neural information processing systems. 322019</p>
<p>URL. </p>
<p>Cf-gnnexplainer: Counterfactual explanations for graph neural networks. A Lucic, M A Ter Hoeve, G Tolomei, M De Rijke, F Silvestri, International Conference on Artificial Intelligence and Statistics. PMLR2022</p>
<p>Faithful chain-of-thought reasoning. Q Lyu, S Havaldar, A Stein, L Zhang, D Rao, E Wong, M Apidianaki, C Callison-Burch, 10.18653/v1/2023.ijcnlp-main.20IJCNLP-AACL2023</p>
<p>Adversarial attacks on neural networks for graph data. D Zügner, A Akbarnejad, S Günnemann, 10.1145/3219819.3220078Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>
<p>Data poisoning attack against knowledge graph embedding. H Zhang, T Zheng, J Gao, C Miao, L Su, Y Li, K Ren, 10.24963/ijcai.2019/674Proceedings of the International Joint Conference on Artificial Intelligence. the International Joint Conference on Artificial Intelligence2019</p>
<p>Rag and roll: An end-to-end evaluation of indirect prompt manipulations in llm-based application frameworks. G De Stefano, L Schönherr, G Pellegrino, arXiv:2408.050252024arXiv preprint</p>
<p>Gnnguard: Defending graph neural networks against adversarial attacks. X Zhang, M Zitnik, Advances in neural information processing systems. 332020</p>
<p>Certified robustness of graph neural networks against adversarial structural perturbation. A Bojchevski, S Günnemann, ; B Wang, J Jia, X Cao, N Z Gong, 10.1145/3447548.3467295Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2019. 202132Advances in Neural Information Processing Systems</p>
<p>W Jin, Y Li, H Xu, Y Wang, J Tang, arXiv:2003.00653Adversarial attacks and defenses on graphs: A review and empirical study. 2020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
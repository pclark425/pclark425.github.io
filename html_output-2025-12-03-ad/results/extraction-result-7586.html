<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7586 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7586</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7586</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-9d81bc8bebf1beb936427c224afb219b54a64f1e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9d81bc8bebf1beb936427c224afb219b54a64f1e" target="_blank">Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Domain Conditional Pointwise Mutual Information is introduced, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task.</p>
                <p><strong>Paper Abstract:</strong> Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition—wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. “computer” and “PC.” Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7586.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7586.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PMI_DC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Conditional Pointwise Mutual Information</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot scoring function that divides the conditional probability of an answer given a prompt by the in-domain (domain-premise) unconditional probability of that answer, compensating for surface-form competition among equivalent answer strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 (multiple sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer language models used as fixed pretrained probability estimators (no fine-tuning); GPT-2 via HuggingFace and GPT-3 via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 350M, 760M, 1.6B, 2.7B, 6.7B, 13B, 175B (various experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot multiple-choice across 13 datasets (continuation, QA, entailment, boolean, text classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select the correct multiple-choice option for questions drawn from datasets such as COPA, CommonsenseQA, RACE, ARC, OBQA, HellaSwag, BoolQ, RTE, CB, SST-2/5, AG's News, TREC.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice prompting: a natural-language premise template followed by candidate answer strings; scoring by model log-probabilities and domain-premise normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot templates (human-written) with a premise (x), a short domain-premise string (x_domain, e.g., 'because' or 'the answer is:'), and n candidate hypotheses y_i; PMI_DC computed as log P(y_i | x) - log P(y_i | x_domain) (i.e., ratio). No model finetuning; scoring uses left-to-right token probabilities; applied to closed- and open-answer tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage of correct multiple-choice selections)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across GPT-3 model sizes, PMI_DC produced the best or tied-best score on the plurality of dataset splits (e.g., Table 2 reports PMI_DC as best/tied on 62.5% of datasets for 175B; see paper table for per-dataset values). Example: on OBQA with GPT-3 175B PMI_DC = 58.0% accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Contrasted with LM (direct P(y|x)), AvG (length-normalized log-likelihood), CC (contextual calibration), and UnC (P(y|x_domain)). Example: on OBQA GPT-3 175B LM = 33.2% accuracy, AvG = 43.8%, PMI_DC = 58.0% (absolute improvement +24.8% over LM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Varies by dataset; examples include +24.8 percentage points absolute vs LM on OBQA (175B), +4.0 pts on COPA (175B: LM 85.2% -> PMI_DC 89.2%), typical wins/ties on a majority of tested datasets and model sizes (see Table 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot scoring using model token log-probabilities; domain premises chosen per dataset (Appendix B templates); GPT-2 via HuggingFace, GPT-3 via OpenAI API; no sampling/decoding (scores derived from next-token probabilities).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7586.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptSensitivity_SST2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt (template) sensitivity on SST-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measured robustness of scoring functions across 15 different human-written prompt templates for SST-2, showing how choice of prompt affects performance and variance for LM vs PMI_DC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 (various sizes reported; example reported: GPT-3 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained causal transformer language models used in zero-shot scoring experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M up to 175B (table reports per-size means and stddev across 15 prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (binary sentiment classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide whether a given sentence/review is positive or negative sentiment.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompted classification: short prompt templates mapping input text to 'positive'/'negative' labels; templates vary (15 different prompts tested).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / template variation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot evaluation over 15 prompt templates (from Zhao et al. 2021); measured mean and standard deviation of accuracy over templates for each scoring method (Unc, LM, PMI_DC). All prompts produce single-token answers (so AvG equivalent to LM).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (mean and standard deviation over prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (SST-2 prompt robustness): GPT-3 175B mean across 15 prompts — LM: 72.5% (std 15.7); PMI_DC: 74.8% (std 14.0). PMI_DC had the highest mean and somewhat lower variance across templates.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>LM mean 72.5% accuracy (175B) used as baseline for template comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>PMI_DC produced a modest absolute improvement of +2.3 percentage points mean accuracy over LM on SST-2 for GPT-3 175B; also produced slightly lower stddev (−1.7 points).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; 15 prompt templates drawn from Zhao et al. (2021); scoring by log-probabilities and PMI_DC subtraction; single-token answers.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7586.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPA_Flipped</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COPA Flipped (scoring-by-premise transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformation of the COPA dataset swapping premise and hypothesis to score P(premise | hypothesis) instead of P(hypothesis | premise), thereby removing surface-form competition among alternative hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 (various sizes; examples reported up to GPT-3 175B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer LMs used as log-probability estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M to 175B (results reported per size)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>COPA (Choice Of Plausible Alternatives)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Choose the more plausible cause/effect among two alternatives given the other part of a causal pair.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice continuation; original COPA scores P(y|x) for different continuations; COPA Flipped reverses items to score P(x|y) (scoring-by-premise).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt transformation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Manual transformation flips cause/effect and relation words ('because' <-> 'so') so the model conditions on answer-strings to predict the same continuation; this removes competition among different output surface forms because the continuation string is identical across options.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (GPT-3 175B): original COPA — LM 85.2% vs PMI_DC 89.2%; COPA Flipped — LM 83.6% and PMI_DC 83.6% (LM, AvG, PMI_DC produce identical scores when surface-form competition is removed).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original COPA LM 85.2% (175B) compared to COPA Flipped LM 83.6% shows that LM's advantage/disadvantage depends on format; PMI_DC remains stable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>On flipping, LM and AvG converge to PMI_DC performance (differences disappear), illustrating that removing output-surface competition eliminates PMI_DC's advantage (i.e., performance differences attributable to presentation format).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; flip applied only where dataset structure permits (COPA); scoring uses model conditional log-probabilities for prefixes vs continuations; no finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7586.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FewShot_PMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PMI_DC applied in few-shot (4-shot) setting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preliminary few-shot experiments showing that PMI_DC's benefits extend to few-shot priming: adding a small number of examples before the test item and scoring with PMI_DC yields higher accuracy than LM in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 family (examples reported up to 1.6B in Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer LMs run in few-shot (in-context learning) regime by prefixing k labeled examples to the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M, 350M, 760M, 1.6B (reported 4-shot results); extension to larger models implied</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 and CommonsenseQA (CQA) few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard SST-2 sentiment classification and CommonsenseQA multiple-choice but with 4 in-context exemplars provided before test prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot (k=4) in-context prompting with examples concatenated to the test prompt; scoring by LM, AvG, PMI_DC.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>few-shot vs zero-shot (prompting style)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Mean and std over 5 random sets of 4 examples used to prime the model; reported for both closed-set (SST-2 single-token labels) and open-set (CQA) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (mean and std over 5 random exemplar sets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example (1.6B model, SST-2, 4-shot): LM = 85.4% (mean), PMI_DC = 89.4% (mean), an absolute +4.0 percentage-point improvement in favor of PMI_DC.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Few-shot LM (1.6B SST-2 4-shot) = 85.4% used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+4.0 percentage points absolute (PMI_DC over LM) in the 1.6B SST-2 4-shot example; similar trend favoring PMI_DC reported for CQA few-shot means though LM superior in two small cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>4-shot in-context priming; no gradient updates; average over 5 random exemplar sets; same templates as zero-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7586.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AvG_LengthNorm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AvG (length-normalized log-likelihood / length normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard scoring baseline that divides the log-likelihood of a candidate by its token length (average log-probability per token), mitigating length bias in token-level probability comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 (multiple sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer LMs; AvG uses per-candidate length-normalized log-likelihoods for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M up to 175B (reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot multiple-choice (various datasets including HellaSwag, ARC-E, SST-2, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select correct option among multiple candidates; AvG used as an alternative to raw probability to compensate for length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice prompting with candidate strings of varying lengths; AvG uses per-token average log-probability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>scoring method / normalization</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>AvG implemented as sum(log P(token|context)) / token_count (BPE tokenization typically used); compared against raw LM, PMI_DC and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AvG outperforms PMI_DC on certain datasets; example HellaSwag with GPT-3 175B: AvG = 77.2% vs LM = 57.6% and PMI_DC = 53.5% (AvG substantially better on HellaSwag).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against LM (raw conditional probability) and PMI_DC; on HellaSwag AvG is +19.6 points above LM and +23.7 points above PMI_DC (175B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Dataset-dependent: in some datasets AvG yields large positive deltas (e.g., +23.7 absolute vs PMI_DC on HellaSwag 175B), while on many other tasks PMI_DC is superior.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; length normalization over BPE-tokenized candidate strings; same prompt templates as other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7586.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContextualCalibration_CC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contextual Calibration (Zhao et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A calibration method that learns an affine transform (weights and biases averaged over content-free inputs) to adjust model logits/probabilities to remove common token/majority/recency biases in few-shot and zero-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrate before use: Improving few-shot performance of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Compared to GPT-3 results in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contextual calibration is applied to LM outputs (requires computing transforms from content-free inputs) and compared to PMI_DC and other scoring functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Evaluated in referenced work; this paper compares CC to PMI_DC on GPT-3 sizes (where available) — CC numbers reported for some datasets/rows in Table 1 (e.g., 175B).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot / few-shot multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same multiple-choice tasks as main experiments; CC is applied to calibrate LM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot/few-shot prompting with additional computation of transforms using content-free inputs (to estimate calibration parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>calibration / prompt processing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>CC adjusts LM scores via learned affine transform (w and b) averaged across several content-free prompts to mitigate dataset-agnostic biases (majority, recency, common token biases). Requires separate runs on content-free inputs to compute calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PMI_DC outperforms CC in the majority of cases reported in this paper; example RTE with GPT-3 175B: PMI_DC = 64.3% vs CC = 57.8% (PMI_DC higher by 6.5 absolute points).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>CC reported as an advanced baseline in several rows of Table 1; compared against LM, AvG, PMI_DC and UnC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>PMI_DC often achieves higher accuracy than CC across tested datasets and sizes; precise deltas vary by dataset (example RTE above: +6.5 abs points for PMI_DC).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot comparison; CC requires content-free input evaluations to compute affine calibration; PMI_DC requires only domain-premise templates.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7586.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7586.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnC_domain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unconditional in-domain baseline (UnC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that scores candidate answers purely by their probability after a short domain-premise prompt (P(y | x_domain)), ignoring the question/premise entirely; used to test how much the model relies on dataset priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 and GPT-3 (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses the pretrained LM's conditional next-token probabilities given a short domain marker (e.g., 'because', 'the answer is:') to rank candidate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M to 175B (reported in tables)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice tasks (esp. BoolQ and other datasets where priors dominate)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assessment of how much candidate prior (in-domain probability) alone predicts the correct label without conditioning on the specific question.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Scoring-only style: provide only a short domain-premise string (x_domain) and measure P(y | x_domain) for each candidate, then rank.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>scoring baseline / ablation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; used as a sanity check whether the model uses question information beyond dataset priors. Applied to same templates but with premise omitted.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On some datasets UnC is surprisingly competitive; the paper cites BoolQ where UnC can outperform other methods in certain settings and where none of the methods beat the majority baseline except PMI_DC with the largest GPT-3 model.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Compared against LM, AvG, PMI_DC; on BoolQ and some other reasoning-heavy sets, UnC sometimes matches or outperforms conditioned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Dataset-dependent; in BoolQ the unconditional baseline sometimes outperforms LM/AvG/PMI_DC (text notes, specific numeric deltas depend on model size; see Table 1 for per-size values).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; only domain-premise provided; same candidate set used.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Calibrate before use: Improving few-shot performance of language models <em>(Rating: 2)</em></li>
                <li>Choice of plausible alternatives: An evaluation of commonsense causal reasoning <em>(Rating: 2)</em></li>
                <li>CommonsenseQA: A question answering challenge targeting commonsense knowledge <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 1)</em></li>
                <li>HellaSwag: Can a machine really finish your sentence? <em>(Rating: 1)</em></li>
                <li>Can a suit of armor conduct electricity? a new dataset for open book question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7586",
    "paper_id": "paper-9d81bc8bebf1beb936427c224afb219b54a64f1e",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "PMI_DC",
            "name_full": "Domain Conditional Pointwise Mutual Information",
            "brief_description": "A zero-shot scoring function that divides the conditional probability of an answer given a prompt by the in-domain (domain-premise) unconditional probability of that answer, compensating for surface-form competition among equivalent answer strings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 (multiple sizes)",
            "model_description": "Causal transformer language models used as fixed pretrained probability estimators (no fine-tuning); GPT-2 via HuggingFace and GPT-3 via OpenAI API.",
            "model_size": "125M, 350M, 760M, 1.6B, 2.7B, 6.7B, 13B, 175B (various experiments reported)",
            "task_name": "Zero-shot multiple-choice across 13 datasets (continuation, QA, entailment, boolean, text classification)",
            "task_description": "Select the correct multiple-choice option for questions drawn from datasets such as COPA, CommonsenseQA, RACE, ARC, OBQA, HellaSwag, BoolQ, RTE, CB, SST-2/5, AG's News, TREC.",
            "problem_format": "Multiple-choice prompting: a natural-language premise template followed by candidate answer strings; scoring by model log-probabilities and domain-premise normalization.",
            "format_category": "question type / prompt style",
            "format_details": "Zero-shot templates (human-written) with a premise (x), a short domain-premise string (x_domain, e.g., 'because' or 'the answer is:'), and n candidate hypotheses y_i; PMI_DC computed as log P(y_i | x) - log P(y_i | x_domain) (i.e., ratio). No model finetuning; scoring uses left-to-right token probabilities; applied to closed- and open-answer tasks.",
            "performance_metric": "Accuracy (percentage of correct multiple-choice selections)",
            "performance_value": "Across GPT-3 model sizes, PMI_DC produced the best or tied-best score on the plurality of dataset splits (e.g., Table 2 reports PMI_DC as best/tied on 62.5% of datasets for 175B; see paper table for per-dataset values). Example: on OBQA with GPT-3 175B PMI_DC = 58.0% accuracy.",
            "baseline_performance": "Contrasted with LM (direct P(y|x)), AvG (length-normalized log-likelihood), CC (contextual calibration), and UnC (P(y|x_domain)). Example: on OBQA GPT-3 175B LM = 33.2% accuracy, AvG = 43.8%, PMI_DC = 58.0% (absolute improvement +24.8% over LM).",
            "performance_change": "Varies by dataset; examples include +24.8 percentage points absolute vs LM on OBQA (175B), +4.0 pts on COPA (175B: LM 85.2% -&gt; PMI_DC 89.2%), typical wins/ties on a majority of tested datasets and model sizes (see Table 1 & 2).",
            "experimental_setting": "Zero-shot scoring using model token log-probabilities; domain premises chosen per dataset (Appendix B templates); GPT-2 via HuggingFace, GPT-3 via OpenAI API; no sampling/decoding (scores derived from next-token probabilities).",
            "statistical_significance": null,
            "uuid": "e7586.0",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "PromptSensitivity_SST2",
            "name_full": "Prompt (template) sensitivity on SST-2",
            "brief_description": "Measured robustness of scoring functions across 15 different human-written prompt templates for SST-2, showing how choice of prompt affects performance and variance for LM vs PMI_DC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 (various sizes reported; example reported: GPT-3 175B)",
            "model_description": "Pretrained causal transformer language models used in zero-shot scoring experiments.",
            "model_size": "125M up to 175B (table reports per-size means and stddev across 15 prompts)",
            "task_name": "SST-2 (binary sentiment classification)",
            "task_description": "Decide whether a given sentence/review is positive or negative sentiment.",
            "problem_format": "Prompted classification: short prompt templates mapping input text to 'positive'/'negative' labels; templates vary (15 different prompts tested).",
            "format_category": "prompt style / template variation",
            "format_details": "Zero-shot evaluation over 15 prompt templates (from Zhao et al. 2021); measured mean and standard deviation of accuracy over templates for each scoring method (Unc, LM, PMI_DC). All prompts produce single-token answers (so AvG equivalent to LM).",
            "performance_metric": "Accuracy (mean and standard deviation over prompts)",
            "performance_value": "Example (SST-2 prompt robustness): GPT-3 175B mean across 15 prompts — LM: 72.5% (std 15.7); PMI_DC: 74.8% (std 14.0). PMI_DC had the highest mean and somewhat lower variance across templates.",
            "baseline_performance": "LM mean 72.5% accuracy (175B) used as baseline for template comparison.",
            "performance_change": "PMI_DC produced a modest absolute improvement of +2.3 percentage points mean accuracy over LM on SST-2 for GPT-3 175B; also produced slightly lower stddev (−1.7 points).",
            "experimental_setting": "Zero-shot; 15 prompt templates drawn from Zhao et al. (2021); scoring by log-probabilities and PMI_DC subtraction; single-token answers.",
            "statistical_significance": null,
            "uuid": "e7586.1",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "COPA_Flipped",
            "name_full": "COPA Flipped (scoring-by-premise transformation)",
            "brief_description": "A transformation of the COPA dataset swapping premise and hypothesis to score P(premise | hypothesis) instead of P(hypothesis | premise), thereby removing surface-form competition among alternative hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 (various sizes; examples reported up to GPT-3 175B)",
            "model_description": "Causal transformer LMs used as log-probability estimators.",
            "model_size": "125M to 175B (results reported per size)",
            "task_name": "COPA (Choice Of Plausible Alternatives)",
            "task_description": "Choose the more plausible cause/effect among two alternatives given the other part of a causal pair.",
            "problem_format": "Multiple-choice continuation; original COPA scores P(y|x) for different continuations; COPA Flipped reverses items to score P(x|y) (scoring-by-premise).",
            "format_category": "question type / prompt transformation",
            "format_details": "Manual transformation flips cause/effect and relation words ('because' &lt;-&gt; 'so') so the model conditions on answer-strings to predict the same continuation; this removes competition among different output surface forms because the continuation string is identical across options.",
            "performance_metric": "Accuracy (percentage correct)",
            "performance_value": "Example (GPT-3 175B): original COPA — LM 85.2% vs PMI_DC 89.2%; COPA Flipped — LM 83.6% and PMI_DC 83.6% (LM, AvG, PMI_DC produce identical scores when surface-form competition is removed).",
            "baseline_performance": "Original COPA LM 85.2% (175B) compared to COPA Flipped LM 83.6% shows that LM's advantage/disadvantage depends on format; PMI_DC remains stable.",
            "performance_change": "On flipping, LM and AvG converge to PMI_DC performance (differences disappear), illustrating that removing output-surface competition eliminates PMI_DC's advantage (i.e., performance differences attributable to presentation format).",
            "experimental_setting": "Zero-shot; flip applied only where dataset structure permits (COPA); scoring uses model conditional log-probabilities for prefixes vs continuations; no finetuning.",
            "statistical_significance": null,
            "uuid": "e7586.2",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "FewShot_PMI",
            "name_full": "PMI_DC applied in few-shot (4-shot) setting",
            "brief_description": "Preliminary few-shot experiments showing that PMI_DC's benefits extend to few-shot priming: adding a small number of examples before the test item and scoring with PMI_DC yields higher accuracy than LM in many cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 family (examples reported up to 1.6B in Table 4)",
            "model_description": "Causal transformer LMs run in few-shot (in-context learning) regime by prefixing k labeled examples to the prompt.",
            "model_size": "125M, 350M, 760M, 1.6B (reported 4-shot results); extension to larger models implied",
            "task_name": "SST-2 and CommonsenseQA (CQA) few-shot",
            "task_description": "Standard SST-2 sentiment classification and CommonsenseQA multiple-choice but with 4 in-context exemplars provided before test prompt.",
            "problem_format": "Few-shot (k=4) in-context prompting with examples concatenated to the test prompt; scoring by LM, AvG, PMI_DC.",
            "format_category": "few-shot vs zero-shot (prompting style)",
            "format_details": "Mean and std over 5 random sets of 4 examples used to prime the model; reported for both closed-set (SST-2 single-token labels) and open-set (CQA) tasks.",
            "performance_metric": "Accuracy (mean and std over 5 random exemplar sets)",
            "performance_value": "Example (1.6B model, SST-2, 4-shot): LM = 85.4% (mean), PMI_DC = 89.4% (mean), an absolute +4.0 percentage-point improvement in favor of PMI_DC.",
            "baseline_performance": "Few-shot LM (1.6B SST-2 4-shot) = 85.4% used as baseline.",
            "performance_change": "+4.0 percentage points absolute (PMI_DC over LM) in the 1.6B SST-2 4-shot example; similar trend favoring PMI_DC reported for CQA few-shot means though LM superior in two small cases.",
            "experimental_setting": "4-shot in-context priming; no gradient updates; average over 5 random exemplar sets; same templates as zero-shot experiments.",
            "statistical_significance": null,
            "uuid": "e7586.3",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "AvG_LengthNorm",
            "name_full": "AvG (length-normalized log-likelihood / length normalization)",
            "brief_description": "A standard scoring baseline that divides the log-likelihood of a candidate by its token length (average log-probability per token), mitigating length bias in token-level probability comparisons.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 (multiple sizes)",
            "model_description": "Causal transformer LMs; AvG uses per-candidate length-normalized log-likelihoods for selection.",
            "model_size": "125M up to 175B (reported)",
            "task_name": "Zero-shot multiple-choice (various datasets including HellaSwag, ARC-E, SST-2, etc.)",
            "task_description": "Select correct option among multiple candidates; AvG used as an alternative to raw probability to compensate for length effects.",
            "problem_format": "Multiple-choice prompting with candidate strings of varying lengths; AvG uses per-token average log-probability.",
            "format_category": "scoring method / normalization",
            "format_details": "AvG implemented as sum(log P(token|context)) / token_count (BPE tokenization typically used); compared against raw LM, PMI_DC and other baselines.",
            "performance_metric": "Accuracy (percentage correct)",
            "performance_value": "AvG outperforms PMI_DC on certain datasets; example HellaSwag with GPT-3 175B: AvG = 77.2% vs LM = 57.6% and PMI_DC = 53.5% (AvG substantially better on HellaSwag).",
            "baseline_performance": "Compared against LM (raw conditional probability) and PMI_DC; on HellaSwag AvG is +19.6 points above LM and +23.7 points above PMI_DC (175B).",
            "performance_change": "Dataset-dependent: in some datasets AvG yields large positive deltas (e.g., +23.7 absolute vs PMI_DC on HellaSwag 175B), while on many other tasks PMI_DC is superior.",
            "experimental_setting": "Zero-shot; length normalization over BPE-tokenized candidate strings; same prompt templates as other experiments.",
            "statistical_significance": null,
            "uuid": "e7586.4",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "ContextualCalibration_CC",
            "name_full": "Contextual Calibration (Zhao et al. 2021)",
            "brief_description": "A calibration method that learns an affine transform (weights and biases averaged over content-free inputs) to adjust model logits/probabilities to remove common token/majority/recency biases in few-shot and zero-shot settings.",
            "citation_title": "Calibrate before use: Improving few-shot performance of language models",
            "mention_or_use": "mention",
            "model_name": "Compared to GPT-3 results in this paper",
            "model_description": "Contextual calibration is applied to LM outputs (requires computing transforms from content-free inputs) and compared to PMI_DC and other scoring functions.",
            "model_size": "Evaluated in referenced work; this paper compares CC to PMI_DC on GPT-3 sizes (where available) — CC numbers reported for some datasets/rows in Table 1 (e.g., 175B).",
            "task_name": "Zero-shot / few-shot multiple-choice",
            "task_description": "Same multiple-choice tasks as main experiments; CC is applied to calibrate LM scores.",
            "problem_format": "Zero-shot/few-shot prompting with additional computation of transforms using content-free inputs (to estimate calibration parameters).",
            "format_category": "calibration / prompt processing",
            "format_details": "CC adjusts LM scores via learned affine transform (w and b) averaged across several content-free prompts to mitigate dataset-agnostic biases (majority, recency, common token biases). Requires separate runs on content-free inputs to compute calibration.",
            "performance_metric": "Accuracy",
            "performance_value": "PMI_DC outperforms CC in the majority of cases reported in this paper; example RTE with GPT-3 175B: PMI_DC = 64.3% vs CC = 57.8% (PMI_DC higher by 6.5 absolute points).",
            "baseline_performance": "CC reported as an advanced baseline in several rows of Table 1; compared against LM, AvG, PMI_DC and UnC.",
            "performance_change": "PMI_DC often achieves higher accuracy than CC across tested datasets and sizes; precise deltas vary by dataset (example RTE above: +6.5 abs points for PMI_DC).",
            "experimental_setting": "Zero-shot comparison; CC requires content-free input evaluations to compute affine calibration; PMI_DC requires only domain-premise templates.",
            "statistical_significance": null,
            "uuid": "e7586.5",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "UnC_domain",
            "name_full": "Unconditional in-domain baseline (UnC)",
            "brief_description": "A baseline that scores candidate answers purely by their probability after a short domain-premise prompt (P(y | x_domain)), ignoring the question/premise entirely; used to test how much the model relies on dataset priors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 and GPT-3 (various sizes)",
            "model_description": "Uses the pretrained LM's conditional next-token probabilities given a short domain marker (e.g., 'because', 'the answer is:') to rank candidate answers.",
            "model_size": "125M to 175B (reported in tables)",
            "task_name": "Multiple-choice tasks (esp. BoolQ and other datasets where priors dominate)",
            "task_description": "Assessment of how much candidate prior (in-domain probability) alone predicts the correct label without conditioning on the specific question.",
            "problem_format": "Scoring-only style: provide only a short domain-premise string (x_domain) and measure P(y | x_domain) for each candidate, then rank.",
            "format_category": "scoring baseline / ablation",
            "format_details": "Zero-shot; used as a sanity check whether the model uses question information beyond dataset priors. Applied to same templates but with premise omitted.",
            "performance_metric": "Accuracy",
            "performance_value": "On some datasets UnC is surprisingly competitive; the paper cites BoolQ where UnC can outperform other methods in certain settings and where none of the methods beat the majority baseline except PMI_DC with the largest GPT-3 model.",
            "baseline_performance": "Compared against LM, AvG, PMI_DC; on BoolQ and some other reasoning-heavy sets, UnC sometimes matches or outperforms conditioned methods.",
            "performance_change": "Dataset-dependent; in BoolQ the unconditional baseline sometimes outperforms LM/AvG/PMI_DC (text notes, specific numeric deltas depend on model size; see Table 1 for per-size values).",
            "experimental_setting": "Zero-shot; only domain-premise provided; same candidate set used.",
            "statistical_significance": null,
            "uuid": "e7586.6",
            "source_info": {
                "paper_title": "Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Calibrate before use: Improving few-shot performance of language models",
            "rating": 2
        },
        {
            "paper_title": "Choice of plausible alternatives: An evaluation of commonsense causal reasoning",
            "rating": 2
        },
        {
            "paper_title": "CommonsenseQA: A question answering challenge targeting commonsense knowledge",
            "rating": 2
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 1
        },
        {
            "paper_title": "HellaSwag: Can a machine really finish your sentence?",
            "rating": 1
        },
        {
            "paper_title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "rating": 1
        }
    ],
    "cost": 0.01840925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Surface Form Competition: Why the Highest Probability Answer Isn't Always Right</h1>
<p>Ari Holtzman ${ }^{1 <em>}$ Peter West ${ }^{1,2 </em>}$ Vered Shwartz ${ }^{1,2}$ Yejin Choi ${ }^{1,2}$ Luke Zettlemoyer ${ }^{1}$<br>${ }^{1}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{2}$ Allen Institute for Artificial Intelligence<br>{ahai, pawest}@cs.washington.edu</p>
<h4>Abstract</h4>
<p>Large language models have shown promising results in zero-shot settings (Brown et al., 2020; Radford et al., 2019). For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form compe-tition-wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. "computer" and "PC." Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated (Zhao et al., 2021) and uncalibrated scoring functions on all GPT2 and GPT-3 models on a variety of multiple choice datasets. *</p>
<h2>1 Introduction</h2>
<p>Despite the impressive results large pretrained language models have achieved in zero-shot settings (Brown et al., 2020; Radford et al., 2019), we argue that current work underestimates the zeroshot capabilities of these models on classification tasks. This is in large part due to surface form competition-a property of generative models that causes probability to be rationed between different valid strings, even ones that differ trivially, e.g., by capitalization alone. Such competition can be largely removed by scoring choices according to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A human wants to submerge himself in water, what should he use?</p>
<p>Humans select options
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Language Models assign probability to every possible string
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 1: While humans select from given options, language models implicitly assign probability to every possible string. This creates surface form competition between different strings that represent the same concept. Example from CommonsenseQA (Talmor et al., 2019).</p>
<p>Domain Conditional Pointwise Mutual Information $\left(\mathrm{PMI}_{\mathrm{DC}}\right)$, which reweighs scores by how much more likely a hypothesis (answer) becomes given a premise (question) within the specific task domain.</p>
<p>Specifically, consider the example question (shown in Figure 1): "A human wants to submerge himself in water, what should he use?" with multiple choice options "Coffee cup", "Whirlpool bath", "Cup", and "Puddle." From the given options, "Whirlpool bath" is the only one that makes sense. Yet, other answers are valid and easier for a language model to generate, e.g., "Bathtub" and "A bathtub." Since all surface forms compete for finite 7038</p>
<p>probability mass, allocating significant probability mass to "Bathtub" decreases the amount of probability mass assigned to "Whirlpool bath." While the total probability of generating some correct answer may be high (i.e., across all valid surface forms), only one of these is a listed option. This is particularly problematic here, because "Whirlpool bath" will be much lower probability than "Bathtub," due to its rarity. More generally, methods that do not account for surface form competition will favor answers with fewer lexical paraphrases.
$\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ factors out the probability of a specific surface form, by instead computing how much more probable a hypothesis is when conditioned on a premise. We use a domain premise string to estimate the unconditional probability of a hypothesis in a given domain. On CommonsenseQA, for example, we compute the probability of each answer option immediately following the string "? the answer is:", and then divide the conditional probability by this estimate to calculate $\mathrm{PMI}</em>$. This scaling factor reweighs answer scores according to the surface form competition that is inherent to the domain or task, e.g. completions of the domain premise that are just inherently unlikely will be upweighted more. This allows us to directly measure how much an answer tells us about the question and vice versa (mutual information is symmetric, see §3). Valid hypotheses no longer need to compete with each other: both "Whirlpool bath" and "Bathtub" will be considered reasonable answers to the question, and so both will attain a high score.}</p>
<p>Extensive experiments show that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms raw, normalized, and calibrated probability scoring methods on zero-shot multiple choice for more than a dozen datasets and it does so for every model in the GPT-2 and GPT-3 families (§4); this holds true across different possible prompts and in preliminary few-shot experiments as well. To better explain these gains, we use the distinct structure of the COPA dataset (Roemmele et al., 2011) to remove surface form competition entirely, showing that all methods perform well in this idealized setting (§5). Additionally, we analyze the only three datasets where $\mathrm{PMI}</em>$ does worse than other methods and put forward a hypothesis for why normalizing log probabilities works better than raw probabilities (§6). We conclude with a discussion of how generative models should be used for selection tasks (§7).}</p>
<h2>2 Background and Related Work</h2>
<p>Zero-shot vs. Few-Shot Zero-shot inference has long been of interest in NLP, Computer Vision, and ML in general (Socher et al., 2013; Guadarrama et al., 2013; Romera-Paredes and Torr, 2015). However, Radford et al. (2019) popularized the notion that language models have many zero-shot capabilities that can be discovered simply by prompting the model, e.g., placing "TL;DR" (internet slang for Too Long; Didn't Read) at the end of a passage causes the model to generate a summary. Efficiently constructing the right prompt for a given task is difficult and has become an active area of research (Reynolds and McDonell, 2021; Lu et al., 2021; Shin et al., 2020; Jiang et al., 2020a,b).</p>
<p>Brown et al. (2020) demonstrated that few-shot learning without fine-tuning is possible with very large language models. Contemporary work has shown it is possible to get smaller models to exhibit few-shot learning behavior using fine-tuning (Hambardzumyan et al., 2021; Gao et al., 2020; Schick and Schütze, 2020a,b,c; Shin et al., 2020), an intermediate learning phase (Ye et al., 2021), or calibration (Zhao et al., 2021), though most assume access to a validation set (Perez et al., 2021). Recent work suggests it may be possible to finetune language models in order to improve their zeroshot and few-shot capabilities on a large swathe of tasks (Wei et al., 2021; Zhong et al., 2021).</p>
<p>Surface Form Competition When applying generative models to multiple choice problems, simply choosing the highest probability answer becomes problematic due to different valid surface forms competing for probability. Indeed, recent work in question answering has demonstrated the importance of considering all multiple choice options together (Khashabi et al., 2020), rather than independently assigning each answer a score and simply choosing the highest. This is a difficult strategy to adapt to left-to-right generative language models, which implicitly choose between all possible strings. Using unsupervised language models pretrained on relatively expansive corpora exacerbates surface form competition because such language models generate a much wider distribution than a given question answering dataset contains.
"What is the most populous nation in North America?" Posed with this question, a language model such as GPT-3 can generate a correct response such as "USA", "United States", or "United</p>
<p>States of America" with high probability. While correct strings like this all contribute to the probability of a correct generation, they may have vastly different probabilities: a common string "United States" will be much more likely than rarer forms like "U.S. of A.". In generative scenarios, as long as most of the probability mass goes to valid strings the generation is likely to be valid. This is not the case for multiple choice problems. Given two options, e.g., "USA" and "Canada", GPT-3 will choose the correct answer by probability. However, if we substitute out "USA" for "U.S. of A.", GPT-3 will assign higher probability to "Canada", a less likely answer conceptually, but a much more likely surface form. Beyond this, incorrect generic answers such as "I don't know" are often assigned high probability, relegating the desired answers to the tail of the distribution where softmax is poorly calibrated (Holtzman et al., 2020).</p>
<p>PMI Work in dialogue has used PMI to promote diversity (Zhou et al., 2019; Yao et al., 2017; Li et al., 2016; Mou et al., 2016; Tang et al., 2019). Recently, Brown et al. (2020) used a scoring function resembling $\mathrm{PMI}_{\mathrm{DC}}$ for zero-shot question answering, though they only use the string "A:" as a prompt for the unconditional probability estimate, whereas we use a task-specific domain premise (see $\S 3$ for details). Furthermore, Brown et al. (2020) only report this scoring method on three datasets (ARC, OpenBookQA, and RACE, included here) out of the more than 20 tested and do not compare scores with their standard method, averaging loglikelihoods (AvG in this work). In contrast, we report a comprehensive comparison on GPT-3 and GPT-2, as well as shedding light on the underlying issue of surface form competition in $\S 5$.</p>
<p>Contextual Calibration Recently, Zhao et al. (2021) describe a new method for calibrating the probabilities of an LM using a learned affine transformation. Though geared towards few-shot learning, the authors devise a clever means of using "content free inputs" for zero-shot learning. Zhao et al. (2021) calibrate for three forms of bias: (1) majority label bias, (2) recency bias, and (3) common token bias. $\mathrm{PMI}_{\mathrm{DC}}$ directly compensates for common token bias by dividing by the domain conditional probability of each answer, and performs superior to contextual calibration (CC) in the majority of cases.</p>
<p>Prompt Sensitivity Recent work highlights LM sensitivity to inputs, and proposes to consider paraphrases of the prompt to overcome this (Davison et al., 2019; Jiang et al., 2020b), as well as noting that certain trigger tokens (Shin et al., 2020) can strongly effect the output of such models. In this work, we focus on the surface form of possible outputs, but do also analyze robustness to different prompts in $\S 4.4$.</p>
<p>Interpreting Language Models Language models tend to model selectional preferences and thematic fit (Pantel et al., 2007; Erk et al., 2010) rather than semantic plausibility (Wang et al., 2018). Probability, possibility and plausibility are distinct (Van der Helm, 2006), but reporting bias (Gordon and Van Durme, 2013) means that language models only model what people are likely to write (on websites that are easily crawled). $\mathrm{PMI}_{\mathrm{DC}}$ aims to adjust for these challenges to better measure the underlying agreement between language models and human judgements, but of course is still subject to the limits and biases of the language model used.</p>
<h2>3 Zero-shot Scoring Strategies</h2>
<p>This paper does not define any new modeling or finetuning methods. Rather, we propose the broad use of $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ scoring for any given model and prompt. $\mathrm{PMI}</em>$ compensates for the fact that different correct answers compete for probability, even though only one will be listed as the correct multiple choice option.}</p>
<p>We begin by describing the two most common methods currently in use.</p>
<h3>3.1 Standard Methods</h3>
<p>Our first baseline is simply selecting the highestprobability option, e.g., baselines in Zhao et al. (2021) and Jiang et al. (2020b), which we refer to as LM. Given a prompt $\mathbf{x}$ (e.g. "The bar closed") and a set of possible answers $\mathbf{y}<em n="n">{1}, \cdots, \mathbf{y}</em>$ (e.g. "it was crowded.", "it was 3 AM."), LM is defined:</p>
<p>$$
\underset{i}{\arg \max } P\left(\mathbf{y}_{i} \mid \mathbf{x}\right)
$$</p>
<p>However, using length normalized log-likelihoods (Brown et al., 2020) has become standard due to its superior performance, and is also commonly used in generation (Mao et al., 2019; Oluwatobi and Mueller, 2020). For causal language models, e.g.,</p>
<p>Template</p>
<p>Premise ( $\mathbf{X}$ ):
The bar closed because
Domain Premise ( $\mathbf{X}<em 1="1">{\text {domain }}$ ):
because
Hypothesis $1\left(\mathbf{y}</em>\right)$ :
it was crowded.
Hypothesis $2\left(\mathbf{y}_{2}\right)$ :
it was 3am.</p>
<h2>Scoring Functions</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 2: An example from COPA (Roemmele et al., 2011) with the template we use as well as the scoring functions we test. LM returns the highest probability option, while AvG length-normalizes log-likelihoods and chooses the highest option. $\mathrm{PMI}<em i="i">{\mathrm{DC}}$ is a measurement of the mutual information between hypothesis and premise, intuitively how much $\mathbf{x}$ explains $\mathbf{y}</em>$ are averaged over solutions that cause "content free inputs" to yield uniform scores over a given label set, see Zhao et al. (2021).}$ and vice versa. CC is an affine transform of LM, where $\mathbf{w}$ and $\mathbf{b</p>
<p>GPT-2 and GPT-3, Equation 1 can be decomposed:</p>
<p>$$
P\left(\mathbf{y}<em j="1">{i} \mid \mathbf{x}\right)=\prod</em>\right)
$$}^{\ell_{i}} P\left(y_{i}^{j} \mid \mathbf{x}, y_{i}^{1}, \cdots, y_{i}^{j-1</p>
<p>where $y_{i}^{j}$ is the $j$ th token of $\mathbf{y}<em i="i">{i}$ and $\ell</em>$. The AvG strategy can thus be defined as:}$ is the number of tokens in $\mathbf{y}_{i</p>
<p>$$
\arg \max <em j="1">{i} \frac{\sum</em>
$$}^{\ell_{i}} \log P\left(y_{i}^{j} \mid \mathbf{x}, \mathbf{y}^{1 \cdots j-1}\right)}{\ell_{i}</p>
<h3>3.2 Domain Conditional PMI</h3>
<p>Our core claim is that direct probability is not an adequate zero-shot scoring function due to surface form competition. A natural solution is to factor out the probability of specific surface forms, which is what Pointwise Mutual Information (PMI) does:</p>
<p>$$
\operatorname{PMI}(\mathbf{x}, \mathbf{y})=\log \frac{P(\mathbf{y} \mid \mathbf{x})}{P(\mathbf{y})}=\log \frac{P(\mathbf{x} \mid \mathbf{y})}{P(\mathbf{x})}
$$</p>
<p>In effect, this is how much more likely the hypothesis ("it was 3 AM.") becomes given the premise ("The bar closed because"), see Figure 2 for the full example. In a multiple-choice setting-where the premise $\mathbf{x}$ does not change across hypotheses-this is proportional to $P(\mathbf{x} \mid \mathbf{y})$, i.e. the probability of the premise given the hypothesis. We call this scoring-by-premise and it is the reverse of $\mathrm{LM}, P(\mathbf{y} \mid \mathbf{x})$. We use scoring-by-premise to show the presence of surface form competition in $\S 5$.</p>
<p>While Equation 2 estimates how related premise $\mathbf{x}$ is to hypothesis $\mathbf{y}$ in general, we found that estimates of $P(\mathbf{y})$ vary wildly. GPT-2 and GPT-3 are not trained to produce unconditional estimates of document excerpts, an issue which is exacerbated by the fact that many possible answers are extremely rare in a large scrape of public web pages. This causes the unconditional probability of such answers to be poorly calibrated for the purposes of a given task.</p>
<p>We are specifically trying to measure $P(\mathbf{y})$ in a given domain, e.g., for the "because" relation in our running example, shown in Figures $2 \&amp; 3$. To quantify this, we propose Domain Conditional PMI:</p>
<p>$$
\begin{aligned}
\mathrm{PMI}<em _domain="{domain" _text="\text">{\mathrm{DC}}(\mathbf{x}, \mathbf{y}, \text { domain }) &amp; =\frac{P(\mathbf{y} \mid \mathbf{x}, \text { domain })}{P(\mathbf{y} \mid \text { domain })} \
&amp; =\frac{P(\mathbf{y} \mid \mathbf{x}, \text { domain })}{P\left(\mathbf{y} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>or how much $\mathbf{x}$ tells us about $\mathbf{y}$ within a domain.
Typically, $P(\mathbf{y} \mid \mathbf{x}$, domain $)=P(\mathbf{y} \mid \mathbf{x})$ because the premise $\mathbf{x}$ typically implies the domain, e.g., "The bar closed because" sets the model up to predict an independent clause that is the cause of some event, without further representation of the domain. In order to estimate $P(\mathbf{y} \mid$ domain $)$-the probability of seeing hypothesis $\mathbf{y}$ in a given domain-we use a short domain-relevant string $\mathbf{x}<em _domain="{domain" _text="\text">{\text {domain }}$, which we call a "domain premise", usually just the ending of the conditional premise $\mathbf{x}$. For example, to predict a causal relation like in Figure 2 we use $\mathbf{x}</em> \mid$ because $)$-how}}=$ "because" and thus divide by $P(\mathbf{y</p>
<p>likely $y$ is to be a "cause" . For examples of each template see Appendix B.</p>
<h3>3.3 Non-standard Baselines</h3>
<p>Unconditional We also compare to the unconditional (in-domain) estimate as a scoring function:</p>
<p>$$
\underset{i}{\arg \max } P\left(\mathbf{y}<em _domain="{domain" _text="\text">{i} \mid \mathbf{x}</em>\right)
$$}</p>
<p>We refer to this as UnC. It ignores the premise completely, only using a domain premise $\mathbf{x}_{\text {domain }}$ (e.g., using $P(\mathbf{y} \mid$ because $)$ as the score). Yet, it is sometimes competitive, for instance on BoolQ (Clark et al., 2019). UnC is a sanity check on whether zero-shot inference is actually using the information in the question to good effect.</p>
<p>Contextual Calibration Finally, we compare to the reported zero-shot numbers of Zhao et al. (2021). Contextual Calibration adjusts LM with an affine transform to make a closed set of answers equally likely in the absence of evidence. Contextual Calibration thus requires computing matrices $\mathbf{w}$ and $\mathbf{b}$ for a number of "content free inputs" and then averaging these weights, see Zhao et al. (2021) for details. In contrast, $\mathrm{PMI}_{\mathrm{DC}}$ requires nothing but a human-written template (as all zero-shot methods do, including Contextual Calibration), can be computed as the difference of two log probabilities, and is naturally applicable to datasets where the set of valid answers varies between questions.</p>
<h2>4 Multiple Choice Experiments</h2>
<h3>4.1 Setup</h3>
<p>We use GPT-2 via the HuggingFace Transformers library (Wolf et al., 2020) and GPT-3 via OpenAI's beta API. ${ }^{\dagger}$ We do not finetune any models, nor do we alter their output. See Appendix B for examples from each dataset in our templated format.</p>
<h3>4.2 Datasets</h3>
<p>We report results on 16 splits of 13 datasets, and briefly describe each dataset here.</p>
<p>Continuation These datasets require the model to select a continuation to previous text, making them a natural way to test language models. Choice of Plausible Alternatives (COPA) (Roemmele et al., 2011) asks for cause and effect relationships, as shown in Figure 2. StoryCloze (SC) (Mostafazadeh et al., 2017) gives the model a choice between two</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>alternative endings to 5 sentence stories. Finally, HellaSwag (HS) (Zellers et al., 2019) uses GPT-2 to generate, BERT to filter, and crowd workers to verify possible continuations to a passage. Following previous work (Brown et al., 2020) we report development set numbers for COPA and HS.</p>
<p>Question Answering RACE-M \&amp; -H (R-M \&amp; R-H) (Lai et al., 2017) are both drawn from English exams given in China, the former being given to Middle Schoolers and the latter to High Schoolers. Similarly, ARC Easy \&amp; Challenge (ARC-E \&amp; ARC-C) (Clark et al., 2018) are standardized tests described as "natural, grade-school science questions," with the "Easy" split found to be solvable by either a retrieval or word co-occurrence system, and the rest of the questions put in the "Challenge" split. Open Book Question Answering (OBQA) (Mihaylov et al., 2018) is similar to both of these, but was derived using (and intended to be tested with) a knowledge source (or "book") available; we do not make use of the given knowledge source, following Brown et al. (2020). Finally, CommonsenseQA (CQA) (Talmor et al., 2019) leverages CONCEPTNET (Speer et al., 2017) to encourage crowd workers to write questions with challenging distractors. We report development set numbers on CQA because their test set is not public.</p>
<p>Open Set vs. Closed Set Datasets The above datasets are all "open set" in that multiple choice answers may be any string. Below we describe "closed set" datasets with a fixed set of answers.</p>
<p>Boolean Question Answering BoolQ (BQ) (Clark et al., 2019) poses yes/no (i.e. Boolean) questions based on a multi-sentence passage.</p>
<p>Entailment Entailment datasets focus on the question of whether a hypothesis sentence B is entailed by a premise sentence A. Recognizing Textual Entailment (RTE) (Dagan et al., 2005) requires predicting an "entailment" or "contradiction" label while Commitment Bank (CB) (De Marneffe et al., 2019) adds a "neutral" label. Following previous work (Brown et al., 2020) we report development set numbers for both RTE and CB.</p>
<p>Text Classification We consider three more complex classification datasets: SST-2 \&amp; -5 (Socher et al., 2013) for various granularities of sentiment classification, AG's News (Zhang et al., 2015) (AGN) for topic classification, and TREC (Li and Roth, 2002) for question classification.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">6.7B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">13B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">175B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">CC</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">79.3</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HS</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R-M</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">55.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R-H</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">32.7</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">22.9</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARC-E</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">21.8</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">10.6</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BQ</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">33.9</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">08.9</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">53.76</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">71.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">18.6</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">AGN</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">22.6</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of scoring algorithms when using GPT-3 for zero-shot inference on multiple choice questions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Percent of Ties or Wins by Method</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: right;">Unc</td>
<td style="text-align: right;">LM</td>
<td style="text-align: right;">Avg</td>
<td style="text-align: right;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: right;">CC</td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">350 M</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">760 M</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{7 5 . 0 0}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">1.6 B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">$\mathbf{8 0 . 0 0}$</td>
<td style="text-align: right;">20.00</td>
</tr>
<tr>
<td style="text-align: left;">2.7B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">$\mathbf{8 6 . 6 6}$</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">6.7B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">25.00</td>
<td style="text-align: right;">25.00</td>
<td style="text-align: right;">$\mathbf{7 5 . 0 0}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">$\mathbf{6 8 . 7 5}$</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">6.25</td>
<td style="text-align: right;">12.50</td>
<td style="text-align: right;">18.75</td>
<td style="text-align: right;">$\mathbf{6 2 . 5 0}$</td>
<td style="text-align: right;">6.25</td>
</tr>
</tbody>
</table>
<p>Table 2: Percentage of datasets that a given method produced the best score or was tied with other methods, aggregated over each model size. The first four rows use GPT-2 (full data available in the Appendix), while the final four rows use GPT-3 and summarize data from Table 1. Since ties are included, rows sometimes sum to more than 100. CC is only measured on the 5 datasets we use where Zhao et al. (2021) also report accuracies.</p>
<h3>4.3 Results</h3>
<p>We report zero-shot results for GPT-3 in Table 1, with GPT-2 results available in Appendix A. A summarized view is shown in Table 2, which aggregates the percentage of splits where a given method achieves the best score or ties for first-place. In this summarized view it is clear that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms other scoring methods when assessed over a variety of datasets. The smallest margin (in number of datasets won or tied) between $\mathrm{PMI}</em>$ and the best competing method is on GPT-3}</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Prompt Robustness on SST-2</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Method</td>
<td style="text-align: right;">Unc</td>
<td style="text-align: right;">LM</td>
<td style="text-align: right;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$56.8_{7.3}$</td>
<td style="text-align: right;">$58.8_{7.6}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">350 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$58.0_{11.3}$</td>
<td style="text-align: right;">$60.3_{11.4}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">760 M</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$57.0_{9.2}$</td>
<td style="text-align: right;">$67.7_{13.4}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">1.6 B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$57.3_{8.2}$</td>
<td style="text-align: right;">$69.8_{13.3}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">2.7B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$56.1_{9.0}$</td>
<td style="text-align: right;">$66.2_{15.7}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">6.7B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$59.5_{10.7}$</td>
<td style="text-align: right;">$67.9_{13.6}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">13B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$63.0_{14.9}$</td>
<td style="text-align: right;">$71.7_{16.1}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">175B</td>
<td style="text-align: right;">$49.9_{0}$</td>
<td style="text-align: right;">$72.5_{15.7}$</td>
<td style="text-align: right;">$74.8_{14.0}$</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 3: The mean and standard deviations over the 15 templates considered for SST-2 in (Zhao et al., 2021). AvG is excluded, as it is equivalent to LM since all the given templates use single-token answers.</p>
<p>175B with AvG, but that margin is over 40 percentage points. This does not imply that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ is always better or that it will be better by a large margin, though it often is. It does suggest that $\mathrm{PMI}</em>$ is a significantly better bet on a new dataset.}</p>
<h3>4.4 Robustness</h3>
<p>To verify that these trends hold across different prompts, we report the mean and standard deviation over the fifteen different prompts considered in (Zhao et al., 2021) for SST-2. Table 3 shows, $\mathrm{PMI}_{\mathrm{DC}}$ always maintains the highest mean, often by a hefty margin. Scores are lower than in Table 1 because many of the prompts used are optimized for few-shot rather than zero-shot scoring.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg $\quad$ PMI $_{\text {DC }}$</td>
</tr>
<tr>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$63.6_{7.4}$</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">$15.5_{0}$</td>
<td style="text-align: center;">$29.9_{1.6}$</td>
<td style="text-align: center;">$32.7_{1.4}$</td>
</tr>
<tr>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$76.3_{13.8}$</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">$16.5_{0}$</td>
<td style="text-align: center;">$37.6_{2.1}$</td>
<td style="text-align: center;">$40.4_{2.3}$</td>
</tr>
<tr>
<td style="text-align: center;">760 M</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$85.9_{7.2}$</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">$16.1_{0}$</td>
<td style="text-align: center;">$41.5_{2.6}$</td>
<td style="text-align: center;">$42.4_{2.5}$</td>
</tr>
<tr>
<td style="text-align: center;">1.6B</td>
<td style="text-align: center;">$49.9_{0}$</td>
<td style="text-align: center;">$85.4_{1.7}$</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">$16.0_{0}$</td>
<td style="text-align: center;">$46.2_{1.5}$</td>
<td style="text-align: center;">$47.7_{1.9}$</td>
</tr>
</tbody>
</table>
<p>Table 4: The mean and standard deviation for 5 randomly sampled sets of 4 examples used for few-shot inference. We include a closed answer dataset (SST-2) and an open answer dataset (CQA). For SST-2 AVG is equivalent to LM due to using single-token answers.</p>
<h3>4.5 Few-shot</h3>
<p>While our focus in this paper is on zero-shot scoring, $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ is just as applicable to few-shot scenarios. In Table 4 we report 4 -shot results on one closed set dataset (SST-2) and one open set dataset (CQA). We show the mean of 5 randomly sampled sets of 4 examples that are used to prime the model for the task, along with standard deviations. The overall trend on both datasets clearly favors $\mathrm{PMI}</em>$, though LM is superior for two models on SST-2.}</p>
<h2>5 Removing Surface Form Competition</h2>
<p>What if we used the probability of the premise given the hypothesis, $P\left(\mathbf{x} \mid \mathbf{y}<em i="i">{i}\right)$, instead? While we are still measuring the probability of a surface form (e.g. "the bar closed."), it is the same surface form across different options ("It was crowded so", "It was 3 AM so"), eliminating the surface form competition. $\mathbf{y}</em>$ to be likely. We call this scoring-by-premise.}$ and $\mathbf{y}_{i}^{\prime}$ can now both attain high scores if they are both correct answers, by causing $\mathbf{x</p>
<p>Causal language models like GPT-3 cannot measure this directly, because they are only capable of conditioning on past tokens to predict future tokens. We exploit the structure of the COPA dataset to create "COPA Flipped" via a simple transformation, shown in Figure 3. COPA consists of cause and effect pairs (CAUSE so EFFECT, and EFFECT because CAUSE). In the original dataset, whatever comes second (either CAUSE or EFFECT) has two options that a model must choose between. These can be reversed by switching CAUSE and EFFECT, then substituting the natural inverse relation ("because" $\rightarrow$ "so" and "so" $\rightarrow$ "because" ).</p>
<p>Removing Surface Form Competition</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">COPA Flipped</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
</tr>
<tr>
<td style="text-align: center;">125 M</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.2</td>
</tr>
<tr>
<td style="text-align: center;">350 M</td>
<td style="text-align: center;">55.8</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">66.4</td>
</tr>
<tr>
<td style="text-align: center;">760 M</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;">1.6B</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">68.4</td>
</tr>
<tr>
<td style="text-align: center;">6.7B</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">76.8</td>
</tr>
<tr>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">79.0</td>
</tr>
<tr>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">83.6</td>
</tr>
</tbody>
</table>
<p>Table 5: LM does better on COPA Flipped than COPA because surface form competition is removed when scoring-by-premise, see $\S 5$. Methods that don't directly adjust for competing surface forms (LM and AVG) have the same score as $\mathrm{PMI}_{\mathrm{DC}}$ on COPA Flipped.</p>
<h3>5.1 Results</h3>
<p>Table 5 shows scores on COPA and COPA Flipped side-by-side. On COPA Flipped everything except UnC produces the exact same result. This is because flipping the hypothesis and premise means that it's the context that changes and not the continuation. LM, AVG, and $\mathrm{PMI}_{\mathrm{DC}}$ only differ from each other over different continuations, not over different contexts for the same continuation.</p>
<p>On COPA Flipped all methods generally perform similarly to $\mathrm{PMI}_{\mathrm{DC}}$ on the unflipped version. This is because surface form competition has been eradicated: we are measuring how well different prefixes condition a model to predict a fixed continuation rather than which continuation is highest probability. Unlike LM, where different answers compete for probability, in COPA Flipped it only matters how likely each answer can make the question. This is not subject to surface form competition because there is only one string being so scored, so it is not competing with any other strings for probability mass.</p>
<p>Not all datasets are so easily flippable, so manually flipping individual questions to remove surface form competition is not a generally applicable strategy. Luckily, $\mathrm{PMI}_{\mathrm{DC}}$ is symmetric:</p>
<p>$$
\begin{aligned}
&amp; \arg \max <em i="i">{i} \frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}, \text { domain }\right)}{P\left(\mathbf{y<em i="i">{i} \mid \text { domain }\right)} \
= &amp; \arg \max </em>} \frac{P\left(\mathbf{x} \mid \mathbf{y<em i="i">{i}, \text { domain }\right)}{P(\mathbf{x} \mid \text { domain })} \
= &amp; \arg \max </em>\right)
\end{aligned}
$$} P\left(\mathbf{x} \mid \mathbf{y}_{i}, \text { domain </p>
<p>In theory, the answer selected by $\mathrm{PMI}_{\mathrm{DC}}$ should be the same between COPA and COPA Flipped</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: In $\S 5$ we experiment with with flipping the premise and hypothesis so that the highest probability premise is chosen as the answer, i.e. scoring-by-premise. The transformation above the dashed line shows the experimental setup used in $\S 5.1$, while the extra distractor below the dashed line is used for illustrative purposes in $\S 5.2$.
as PMI is symmetric, though we expect some differences due to "so" and "because" not being perfect inverses and shuffled references. Thus, $\mathrm{PMI}_{\mathrm{DC}}$ does better on COPA than COPA Flipped, likely due to more natural phrasing in the original dataset.</p>
<p>These results suggest that surface form competition is the primary cause of the depressed performance of LM and AVG in comparison to $\mathrm{PMI}_{\mathrm{DC}}$.</p>
<h3>5.2 In-depth Example</h3>
<p>Scoring-by-Premise Improves LM Figure 3 shows an example of transforming one question from COPA to COPA Flipped. In the example depicted, when we use GPT-3 to calculate $P$, we get:</p>
<p>$$
P\left(\mathbf{y}<em 2="2">{1} \mid \mathbf{x}\right)&gt;P\left(\mathbf{y}</em>\right)
$$} \mid \mathbf{x</p>
<p>which is wrong, since bars usually close at fixed, late-night closing times, rather than because of being overcrowded. However we also find that</p>
<p>$$
\begin{aligned}
P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}<em 1="1">{2}\right) &amp; &gt;P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \
\frac{P\left(\mathbf{y}<em 2="2">{2} \mid \mathbf{x}\right)}{P\left(\mathbf{y}</em>} \mid \mathbf{x<em 1="1">{\text {domain }}\right)} &amp; &gt;\frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}\right)}{P\left(\mathbf{y<em _domain="{domain" _text="\text">{1} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>indicating that scoring-by-premise causes the right answer to be selected and that $\mathrm{PMI}_{\mathrm{DC}}$ successfully simulates scoring by premise in this example.</p>
<p>Stability Over Valid Answers To see how scoring-by-premise allows multiple correct options to achieve high scores, consider the slightly perturbed $\mathbf{y}<em 2="2">{2}^{\prime}$ and $\hat{\mathbf{x}}</em>$ in Figure 3. The inequalities shown above still hold when substituting
$\mathbf{y}}^{\prime<em 2="2">{2} \rightarrow \mathbf{y}</em>}^{\prime}$ and $\hat{\mathbf{x}<em 2="2">{2} \rightarrow \hat{\mathbf{x}}</em>$ :}^{\prime</p>
<p>$$
\begin{aligned}
P\left(\mathbf{y}<em 2="2">{1} \mid \mathbf{x}\right) &amp; &gt;P\left(\mathbf{y}</em>\right) \
P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}}^{\prime} \mid \mathbf{x<em 1="1">{2}^{\prime}\right) &amp; &gt;P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \
\frac{P\left(\mathbf{y}<em 2="2">{2}^{\prime} \mid \mathbf{x}\right)}{P\left(\mathbf{y}</em>}^{\prime} \mid \mathbf{x<em 1="1">{\text {domain }}\right)} &amp; &gt;\frac{P\left(\mathbf{y}</em>} \mid \mathbf{x}\right)}{P\left(\mathbf{y<em _domain="{domain" _text="\text">{1} \mid \mathbf{x}</em>
\end{aligned}
$$}}\right)</p>
<p>with the key difference that the conditional probability of $\mathbf{y}_{2}^{\prime}$ is much lower:</p>
<p>$$
\begin{aligned}
&amp; \log P\left(\mathbf{y}<em 2="2">{2} \mid \mathbf{x}\right) \approx-16 \
&amp; \log P\left(\mathbf{y}</em>\right) \approx-20
\end{aligned}
$$}^{\prime} \mid \mathbf{x</p>
<p>This is undesirable, as both $\mathbf{y}<em 2="2">{2}$ and $\mathbf{y}</em>}^{\prime}$ are correct answers with similar meanings. Yet, when scoring-by-premise the conditional probability of $\hat{\mathbf{y}}$ is stable when substituting $\hat{\mathbf{x}<em 2="2">{2} \rightarrow \hat{\mathbf{x}}</em>$ :}^{\prime</p>
<p>$$
\begin{aligned}
&amp; \log P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}<em 2="2">{2}\right) \approx-12 \
&amp; \log P\left(\hat{\mathbf{y}} \mid \hat{\mathbf{x}}</em>\right) \approx-12
\end{aligned}
$$}^{\prime</p>
<p>This suggests that eliminating surface form competition allows different correct answers to score well, as they are no longer competing for probability mass. Specifically, "it was 3 AM" and "it was 3:30AM" score wildly differently in COPA but nearly identically in COPA Flipped.</p>
<h2>6 Analysis</h2>
<p>Failure Cases There are three datasets where $\mathrm{PMI}_{\mathrm{DC}}$ does not consistently outperform other methods: HellaSwag, ARC Easy, and BoolQ. Surprisingly, each is dominated by a different method.</p>
<p>HellaSwag is most amenable to Avg. On examination we find that HellaSwag is more focused on the internal coherence of the hypotheses, rather than external coherence, i.e. how much a premise and hypothesis match. This is likely due to HellaSwag being generated by GPT-2 (Radford et al., 2019) and filtered with BERT, as it contains relatively on-topic but intrinsically strange hypotheses that humans can distinguish from natural data.</p>
<p>ARC Easy yields the highest scores to LM, i.e., selecting the highest probability option. Clark et al. (2018) note that ARC Easy questions can be solved by a retrieval or word co-occurrence baseline, while examples that were answered incorrectly by both were put into the Challenge split. This suggests a bias towards a priori likely phrases. Manual inspection reveals many stock answers, e.g., "[clouds are generated when] ocean water evaporates and then condenses in the air," supporting our hypothesis.</p>
<p>Finally, BoolQ, a reading comprehension dataset in which all answers are either "yes" or "no", is best solved by an unconditional baseline. This is because the dataset presents truly complex questions that require more reasoning than GPT-2 or 3 are capable of out of the box. Indeed, none of the methods reported do better than the majority baseline, except $\mathrm{PMI}_{\mathrm{DC}}$ with the largest GPT-3 model.</p>
<p>Why does length normalization work? Past work offers little explanation for why AvG should be a successful strategy, other than the intuition that estimates are strongly length biased and require compensation. Length bias may be caused by the final softmax layer of current language models assigning too much probability mass to irrelevant options at each time-step, as noted in open-ended generation, character-level language modeling, and machine translation (Holtzman et al., 2020; AlRfou et al., 2019; Peters et al., 2019).</p>
<p>Another (not mutually exclusive) argument is that length normalization may account for unconditional probability in a similar way to $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$. Length normalization is often measured over Byte Pair Encoding (BPE) tokens (Sennrich et al., 2016) and BPE tends to produce vocabularies where most tokens are equally frequent (Wang et al., 2020). Recent evidence suggests that language is approximately uniformly information dense (Levy, 2018; Levy and Jaeger, 2007; Jaeger, 2006). As such, length in BPE tokens may correspond roughly to a unigram estimate of log-probability, supposing that BPE tokens have approximately uniform uni-
gram frequency. The adjustment made by AvG is still somewhat different than $\mathrm{PMI}</em>$, (division of log terms rather than subtraction) but could have a similar effect, if length and probability correlate.}</p>
<h2>7 Discussion</h2>
<p>Language Models are density estimation functions that assign probability to every possible string, but there are often many strings that could represent a given idea equally well. Our key observation is that a generative model assigning probability to a string that represents a certain option isn't equivalent to selecting the concept an option corresponds to. We expect surface form competition anywhere that generative models are used where more than one string could represent the same concept.
$\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ aligns the predictions being made by the model more closely with the actual task posed by multiple choice questions: "choose the hypothesis that explains the premise" rather than "generate the exact surface form of the hypothesis". From this perspective, $\mathrm{PMI}</em>$ does not go far enough, because the model still cannot consider the given set of options altogether when selecting its choice. This matters when answers interact with each other, e.g., "all of the above".}</p>
<h2>8 Conclusion</h2>
<p>We conduct a large-scale comparison of standard and recent scoring functions for zero-shot inference across all GPT-2 and GPT-3 models. We show that $\mathrm{PMI}<em _mathrm_DC="\mathrm{DC">{\mathrm{DC}}$ consistently outperforms previous scoring functions on a wide variety of multiple choice datasets. We also argue that compensating for surface form competition is the cause of this boost, by demonstrating that other methods work just as well as $\mathrm{PMI}</em>$ when surface form competition is eliminated. In future work we would like to explore how surface form competition affects generation, as we hypothesize that it may be the cause of overly generic outputs under high model uncertainty.}</p>
<h2>Acknowledgments</h2>
<p>This work was supported in part by the ARO (AROW911NF-16-1-0121), the NSF (IIS1562364), DARPA under the MCS program through NIWC Pacific (N66001-19-2-4031) and the Allen Institute for AI (AI2). We thank Mitchell Wortsman, Gabriel Ilharco, Tim Dettmers, and Rik Koncel-Kedziorski for thorough and insightful feedback on preliminary drafts.</p>
<h2>References</h2>
<p>Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3159-3166.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Workshop, pages 177-190. Springer.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPICNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107-124.</p>
<p>Katrin Erk, Sebastian Padó, and Ulrike Padó. 2010. A flexible, corpus-driven model of regular and inverse selectional preferences. Computational Linguistics, 36(4):723-763.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Jonathan Gordon and Benjamin Van Durme. 2013. Reporting bias and knowledge acquisition. In Proceedings of the 2013 workshop on Automated knowledge base construction, pages 25-30.</p>
<p>Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. 2013.</p>
<p>Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In Proceedings of the IEEE international conference on computer vision, pages 27122719.</p>
<p>Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. Warp: Word-level adversarial reprogramming. arXiv preprint arXiv:2101.00121.</p>
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. International Conference on Learning Representations.</p>
<p>Tim Florian Jaeger. 2006. Redundancy and syntactic reduction in spontaneous speech. Ph.D. thesis, Stanford University Stanford, CA.</p>
<p>Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2020a. How can we know when language models know? arXiv preprint arXiv:2012.00955.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.</p>
<p>Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785794.</p>
<p>Roger Levy. 2018. Communicative efficiency, uniform information density, and the rational speech act theory. In $\operatorname{CogSci}$.</p>
<p>Roger Levy and T Florian Jaeger. 2007. Speakers optimize information density through syntactic reduction. Advances in neural information processing systems, 19:849.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William B Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110-119.</p>
<p>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786.</p>
<p>Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, and Garrison Cottrell. 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5990-5995.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391.</p>
<p>Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46-51.</p>
<p>Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. 2016. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3349-3358.</p>
<p>Olabiyi Oluwatobi and Erik Mueller. 2020. DLGNet: A transformer-based model for dialogue response generation. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 54-62, Online. Association for Computational Linguistics.</p>
<p>Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy Chklovski, and Eduard Hovy. 2007. ISP: Learning inferential selectional preferences. In Hu man Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 564-571, Rochester, New York. Association for Computational Linguistics.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. arXiv preprint arXiv:2105.11447.</p>
<p>Ben Peters, Vlad Niculae, and André FT Martins. 2019. Sparse sequence-to-sequence models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504-1519.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, pages 90-95.</p>
<p>Bernardino Romera-Paredes and Philip Torr. 2015. An embarrassingly simple approach to zero-shot learning. In International conference on machine learning, pages 2152-2161. PMLR.</p>
<p>Timo Schick and Hinrich Schütze. 2020a. Exploiting cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676.</p>
<p>Timo Schick and Hinrich Schütze. 2020b. Fewshot text generation with pattern-exploiting training. arXiv preprint arXiv:2012.11926.</p>
<p>Timo Schick and Hinrich Schütze. 2020c. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17151725 .</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4222-4235, Online. Association for Computational Linguistics.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158.</p>
<p>Jianheng Tang, Tiancheng Zhao, Chenyan Xiong, Xiaodan Liang, Eric Xing, and Zhiting Hu. 2019. Targetguided open-domain conversation. In Proceedings</p>
<p>of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5624-5634.</p>
<p>Ruud Van der Helm. 2006. Towards a clarification of probability, possibility and plausibility: how semantics could help futures practice to improve. Foresight.</p>
<p>Changhan Wang, Kyunghyun Cho, and Jiatao Gu. 2020. Neural machine translation with byte-level subwords. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 91549160 .</p>
<p>Su Wang, Greg Durrett, and Katrin Erk. 2018. Modeling semantic plausibility by injecting world knowledge. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 303-308, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan Zhao, and Rui Yan. 2017. Towards implicit contentintroducing for generative short-text conversation systems. In Proceedings of the 2017 conference on empirical methods in natural language processing, pages 2190-2199.</p>
<p>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021. Crossfit: A few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv:2104.08835.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. Advances in Neural Information Processing Systems, 28:649-657.</p>
<p>Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. arXiv preprint arXiv:2102.09690.</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. arXiv preprint arXiv:2109.01652.</p>
<p>Kun Zhou, Kai Zhang, Yu Wu, Shujie Liu, and Jingsong Yu. 2019. Unsupervised context rewriting for open domain conversation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1834-1844.</p>
<h1>A GPT-2 Results</h1>
<p>Table 6 shows the results for zero-shot multiple choice using GPT-2.</p>
<h2>B Templates</h2>
<p>Table 7 shows an example of each template used for each dataset.</p>
<p>Multiple Choice Accuracy on GPT-2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Params.</th>
<th style="text-align: center;">125M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">350M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">760M</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">1.6B</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">$\mathrm{PMI}_{\mathrm{DC}}$</td>
<td style="text-align: center;">Unc</td>
<td style="text-align: center;">LM</td>
</tr>
<tr>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">0.564</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.632</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.700</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.698</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.694</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: center;">SC</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.600</td>
<td style="text-align: center;">0.615</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.667</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.661</td>
<td style="text-align: center;">0.688</td>
<td style="text-align: center;">0.734</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">0.676</td>
</tr>
<tr>
<td style="text-align: center;">HS</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.295</td>
<td style="text-align: center;">0.291</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.328</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.350</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.351</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.384</td>
</tr>
<tr>
<td style="text-align: center;">R-M</td>
<td style="text-align: center;">0.222</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.409</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.214</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">0.415</td>
</tr>
<tr>
<td style="text-align: center;">R-H</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">0.344</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.363</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.345</td>
<td style="text-align: center;">0.383</td>
<td style="text-align: center;">0.219</td>
<td style="text-align: center;">0.330</td>
</tr>
<tr>
<td style="text-align: center;">ARC-E</td>
<td style="text-align: center;">0.313</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.494</td>
<td style="text-align: center;">0.434</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.562</td>
</tr>
<tr>
<td style="text-align: center;">ARC-C</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.282</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.254</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.266</td>
<td style="text-align: center;">0.316</td>
<td style="text-align: center;">0.211</td>
<td style="text-align: center;">0.252</td>
</tr>
<tr>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.302</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.194</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.224</td>
</tr>
<tr>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.307</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.165</td>
<td style="text-align: center;">0.309</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.170</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.445</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.386</td>
</tr>
<tr>
<td style="text-align: center;">BQ</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.588</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">0.497</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.563</td>
</tr>
<tr>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.516</td>
<td style="text-align: center;">0.498</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.473</td>
<td style="text-align: center;">0.477</td>
</tr>
<tr>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.500</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.500</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.636</td>
<td style="text-align: center;">0.671</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.770</td>
<td style="text-align: center;">0.856</td>
<td style="text-align: center;">0.499</td>
<td style="text-align: center;">0.840</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">0.181</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.203</td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.220</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.304</td>
</tr>
<tr>
<td style="text-align: center;">AGN</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.574</td>
<td style="text-align: center;">0.630</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.643</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.607</td>
<td style="text-align: center;">0.641</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.648</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.364</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.122</td>
<td style="text-align: center;">0.216</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.228</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.228</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of scoring algorithms when using GPT-2 for zero-shot inference on multiple choice questions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Continuation</td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">The man broke his tree [p [because]] [he got a hole in his neck. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">StoryClose</td>
<td style="text-align: center;">[I tipped the bottle] [p [m]] [the liquid in the bottle from. $]. \mathrm{UH}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Jennifer has a big exam tomorrow. She got so stressed, she pulled an all-nighter. She went into class the next day, weary as can be. Her teacher stated that the test is postponed for next week.]p [The story continues:] [p [Jennifer felt bittersweet about it. $]. \mathrm{UH}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HellaSweg</td>
<td style="text-align: center;">[A female chef in white uniform shows a stack of baking pans in a large kitchen presenting them. the pans] [ [contain egg yolks and baking soda. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">QA</td>
<td style="text-align: center;">RACE</td>
<td style="text-align: center;">There is not enough oil in the world now. As time goes by, it becomes less and less, so what are we going to do when it runs out $[\ldots]]$. question: [According to the passage, which of the following statements is true] $[\mathrm{P}]$ ? $]$ ] answer: [There is more petroleum than we can use now. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ARC</td>
<td style="text-align: center;">What carries oxygen throughout the body? [p [the answer is:] [p [red blood cells. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OBQA</td>
<td style="text-align: center;">Which of these would let the most heat travel through? [p [the answer is:] [p [a steel spoon in a cafeteria. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CQA</td>
<td style="text-align: center;">Where can I stand on a river to see water falling without getting wet? [p [the answer is:] [p [bridge. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Boolean QA</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">title: [The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016 [...] [p question: [Have the San Jose Sharks won a Stanley Cup? [p [answer:] [p [No. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Entailment</td>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">Time Warner is the world's largest media and Internet company. [p question: [Time Warner is the world's largest company.] $]$ ] true or false? answer: $]_{\text {DP }}$ [true. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">question: Given that [What fun to hear Artemis laugh. She's such a serious child.] Is [I didn't know she had a sense of humor. ] $]$ true, false, or neither? [the answer is:] [p [true. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;">Text <br> Classification</td>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">"[Illuminating if overly salky documentary] $]$ " [The quote] has a tone that is] [p [positive. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">"[Illuminating if overly salky documentary] $]$ " [The quote] has a tone that is] [p [neutral. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AG's News</td>
<td style="text-align: center;">title: [Economic growth in Japan slows down as the country experiences a drop in domestic and corporate [...] [p summary: Expansion slows in Japan] [ [topic:] [p [Sports. $]. \mathrm{UH}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">Who developed the vaccination against polio? [p [The answer to this question will be] [p [a person. $]. \mathrm{UH}$</td>
</tr>
</tbody>
</table>
<p>Table 7: The templates used for each task, along with an example instance (with a single random candidate answer). Original questions (premises) are colored blue, and original answers (hypotheses) are colored red. Long premises are abbreviated with "[...]". The full premises, conditional hypotheses and domain premises are marked in $[\cdot]<em _mathrm_UH="\mathrm{UH">{\mathrm{P}}$, $\left[{ }^{\mathrm{P}}\right]</em>$ respectively. For a complete description of our templating methodology, please see our code at https://github.com/peterwestuw/surface-form-competition}}$, and $\left[{ }^{\mathrm{R}}\right]_{\mathrm{DP}</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ https://beta.openai.com/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
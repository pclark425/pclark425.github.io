<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9003 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9003</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9003</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-257636780</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.11436v2.pdf" target="_blank">Mind meets machine: Unravelling GPT-4's cognitive psychology</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive psychology delves on understanding perception, attention, memory, language, problem-solving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9003.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9003.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 (accessed via ChatGPT-Plus) was evaluated on the CommonsenseQA benchmark to probe commonsense reasoning; the paper reports a high accuracy substantially above earlier language-model baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language model from OpenAI; accessed via ChatGPT-Plus in this study. Paper does not provide architecture or training-data details beyond referencing GPT-4 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice (5 options) dataset of 12,247 questions designed to evaluate commonsense knowledge using ConceptNet; cognitive domain: commonsense reasoning / semantic knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>83.2% (paper also reports ~84% in places)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>≈89% (reported by CommonsenseQA authors)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM approaches but does not fully reach human baseline (~83–84% vs ~89%); substantially above earlier reported LM baseline (55.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Model accessed via ChatGPT-Plus (OpenAI). The paper reports accuracy numbers but gives few methodological specifics (e.g., prompting format, number of shots, or temperature) other than showing example prompts and responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper gives no detailed prompt-engineering/protocol; some inconsistencies in reported percentages (83.2% vs ~84%). Comparison uses dataset-level accuracies but lacks statistical details (e.g., confidence intervals).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9003.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9003.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on MATH dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated on the MATH benchmark (contest-style math problems) with per-category performance reported, showing strong performance on some categories (prealgebra) and much weaker on others (geometry).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language model from OpenAI; accessed via ChatGPT-Plus in this study. No model parameter counts or training corpus details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Approximately 12,500 contest-style mathematics problems across topics (prealgebra, geometry, algebra, etc.) and difficulty levels 1–5; cognitive domain: mathematical problem solving and multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported per-category: ~82% on prealgebra, ~35% on geometry (overall category breakdowns reported; no single overall accuracy for GPT-4 explicitly stated).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Varies widely: reported examples include ~40% for a CS PhD (low preference for maths) and ~90% for a three-time IMO gold medallist; human performance depends strongly on solver ability and problem difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 shows strong performance on some subdomains (prealgebra) but remains well below expert human performance on harder categories (geometry); prior LMs (GPT-2/GPT-3) had very low accuracies on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluations shown with example prompts and chain-of-thought style outputs; paper does not report detailed prompting protocol (e.g., chain-of-thought prompting or few-shot vs zero-shot) or evaluation normalization specifics beyond citing MATH description.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper reports category-level numbers but lacks full methodology (prompting details, exact split used). Prior-reported LLM accuracies on MATH were very low, so comparisons depend on experimental setup; no statistical testing reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9003.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9003.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (SuperGLUE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated on the SuperGLUE benchmark, a suite of challenging NLU tasks intended to measure gaps between models and human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language model from OpenAI; evaluated via ChatGPT-Plus in this study. The paper does not provide model architecture or parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A benchmark of multiple challenging natural language understanding tasks (many with <10k examples) assessing reasoning, reading comprehension, coreference, and other NLU capabilities; cognitive domain: higher-level language understanding and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported: 91.2% (as reported in the paper for GPT-4 on SuperGLUE).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not given as a single figure in this paper; prior SuperGLUE reporting used human baselines and noted BERT is ~20 points worse than humans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper claims GPT-4 greatly surpasses the older language-model baselines (e.g., BERT) and narrows the gap with human performance; exact comparison to human baseline not numerically specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated using ChatGPT-Plus; the paper provides examples of prompts/responses but does not detail prompting strategy (e.g., few-shot, zero-shot), evaluation splits, or hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human baseline not given explicitly in paper; comparisons to older models (BERT) are descriptive rather than rigorously controlled within this study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9003.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9003.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (HANS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was tested on the HANS diagnostic dataset for NLI heuristics; the paper reports perfect performance on the used HANS subset but cautions possible heuristic memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art large language model from OpenAI accessed via ChatGPT-Plus for this study; no parameter-count or training details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HANS</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Diagnostic dataset designed to detect shallow syntactic/lexical heuristics in NLI models (lexical overlap, subsequence, component heuristics); cognitive domain: syntactic/semantic inference and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported: 100% on the paper's HANS data subset (paper notes this may be due to dataset subset composition).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Reported human accuracy range: 76%–97% (as stated in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 appears to outperform prior NLI models on the HANS subset used (reported 100%), and well above older model performance (older models often underperformed markedly); however, the paper notes this result may reflect memorization or the use of a subset formed entirely of non-entailment examples.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Model accessed via ChatGPT-Plus; paper notes the HANS subset used in experiments may contain only non-entailment examples and that experiments with mixed HANS data are ongoing. No detailed prompting protocol provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors explicitly caution that 100% may be due to the dataset subset (all non-entailment) and potential memorization of heuristics; experiments on full/mixed HANS are ongoing, so current result may not reflect robust generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9003.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9003.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2/GPT-3 (MATH, mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 and GPT-3 reported on MATH (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior reports that GPT-2 and GPT-3 achieved very low accuracy on the MATH dataset (below 10%).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 / GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Earlier-generation large language models from OpenAI; paper does not provide architecture/training details for these models and only cites their low MATH performance from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Contest-style mathematics problems assessing multi-step mathematical problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as "below 10%" accuracy for GPT-2/GPT-3 on MATH (as cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human performance varies widely (examples given: ~40% for a CS PhD, ~90% for IMO gold medallist).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-2/GPT-3 are substantially below human baselines and below GPT-4's reported category-level performance on some subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>This paper only cites prior-reported results; it does not run GPT-2/GPT-3 experiments itself.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No experimental details provided here for how GPT-2/GPT-3 were evaluated (prompting, splits, or exact setup), so comparisons are approximate and taken from prior literature.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9003.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9003.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (SuperGLUE / HANS, mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT as baseline on SuperGLUE and HANS (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references BERT as a baseline in SuperGLUE reporting and as one of the traditional NLI models evaluated on HANS, noting substantial gaps relative to humans and failure modes on HANS diagnostic splits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Widely used transformer-based language model (baseline cited in SuperGLUE and compared on HANS); this paper references its baseline performance but does not itself run BERT experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SuperGLUE / HANS</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>SuperGLUE: suite of challenging NLU tasks; HANS: diagnostic dataset for syntactic heuristics in NLI. Cognitive domains: advanced NLU and syntactic/semantic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SuperGLUE: described as ~20 points worse than humans in original study when using BERT as baseline; HANS: BERT and other NLI models "performed below 10% on the non-entailment category" (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human baselines are higher (SuperGLUE humans exceed BERT by ~20 points according to original SuperGLUE reporting; HANS human accuracy reported 76%–97%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>BERT is described as substantially below human performance on SuperGLUE and as failing specific HANS diagnostic subsets (very low accuracy on non-entailment examples).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>This paper cites prior findings for BERT; it does not provide new BERT experiments or prompting details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Cited comparisons rely on prior studies; this paper does not re-run BERT under the same conditions as GPT-4, so cross-model comparisons are descriptive and not experimentally controlled within this work.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Commonsenseqa: A question answering challenge targeting commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Superglue: A stickier benchmark for general-purpose language understanding systems <em>(Rating: 2)</em></li>
                <li>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Hellaswag: Can a machine really finish your sentence? <em>(Rating: 1)</em></li>
                <li>Winogrande: An adversarial winograd schema challenge at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9003",
    "paper_id": "paper-257636780",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-4 (CommonsenseQA)",
            "name_full": "GPT-4 evaluated on CommonsenseQA",
            "brief_description": "GPT-4 (accessed via ChatGPT-Plus) was evaluated on the CommonsenseQA benchmark to probe commonsense reasoning; the paper reports a high accuracy substantially above earlier language-model baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art large language model from OpenAI; accessed via ChatGPT-Plus in this study. Paper does not provide architecture or training-data details beyond referencing GPT-4 technical report.",
            "model_size": null,
            "test_battery_name": "CommonsenseQA",
            "test_description": "Multiple-choice (5 options) dataset of 12,247 questions designed to evaluate commonsense knowledge using ConceptNet; cognitive domain: commonsense reasoning / semantic knowledge.",
            "llm_performance": "83.2% (paper also reports ~84% in places)",
            "human_baseline_performance": "≈89% (reported by CommonsenseQA authors)",
            "performance_comparison": "LLM approaches but does not fully reach human baseline (~83–84% vs ~89%); substantially above earlier reported LM baseline (55.9%).",
            "experimental_details": "Model accessed via ChatGPT-Plus (OpenAI). The paper reports accuracy numbers but gives few methodological specifics (e.g., prompting format, number of shots, or temperature) other than showing example prompts and responses.",
            "limitations_or_caveats": "Paper gives no detailed prompt-engineering/protocol; some inconsistencies in reported percentages (83.2% vs ~84%). Comparison uses dataset-level accuracies but lacks statistical details (e.g., confidence intervals).",
            "uuid": "e9003.0"
        },
        {
            "name_short": "GPT-4 (MATH)",
            "name_full": "GPT-4 evaluated on MATH dataset",
            "brief_description": "GPT-4 was evaluated on the MATH benchmark (contest-style math problems) with per-category performance reported, showing strong performance on some categories (prealgebra) and much weaker on others (geometry).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art large language model from OpenAI; accessed via ChatGPT-Plus in this study. No model parameter counts or training corpus details provided in this paper.",
            "model_size": null,
            "test_battery_name": "MATH",
            "test_description": "Approximately 12,500 contest-style mathematics problems across topics (prealgebra, geometry, algebra, etc.) and difficulty levels 1–5; cognitive domain: mathematical problem solving and multi-step reasoning.",
            "llm_performance": "Reported per-category: ~82% on prealgebra, ~35% on geometry (overall category breakdowns reported; no single overall accuracy for GPT-4 explicitly stated).",
            "human_baseline_performance": "Varies widely: reported examples include ~40% for a CS PhD (low preference for maths) and ~90% for a three-time IMO gold medallist; human performance depends strongly on solver ability and problem difficulty.",
            "performance_comparison": "GPT-4 shows strong performance on some subdomains (prealgebra) but remains well below expert human performance on harder categories (geometry); prior LMs (GPT-2/GPT-3) had very low accuracies on MATH.",
            "experimental_details": "Evaluations shown with example prompts and chain-of-thought style outputs; paper does not report detailed prompting protocol (e.g., chain-of-thought prompting or few-shot vs zero-shot) or evaluation normalization specifics beyond citing MATH description.",
            "limitations_or_caveats": "Paper reports category-level numbers but lacks full methodology (prompting details, exact split used). Prior-reported LLM accuracies on MATH were very low, so comparisons depend on experimental setup; no statistical testing reported.",
            "uuid": "e9003.1"
        },
        {
            "name_short": "GPT-4 (SuperGLUE)",
            "name_full": "GPT-4 evaluated on SuperGLUE",
            "brief_description": "GPT-4 was evaluated on the SuperGLUE benchmark, a suite of challenging NLU tasks intended to measure gaps between models and human performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art large language model from OpenAI; evaluated via ChatGPT-Plus in this study. The paper does not provide model architecture or parameter count.",
            "model_size": null,
            "test_battery_name": "SuperGLUE",
            "test_description": "A benchmark of multiple challenging natural language understanding tasks (many with &lt;10k examples) assessing reasoning, reading comprehension, coreference, and other NLU capabilities; cognitive domain: higher-level language understanding and reasoning.",
            "llm_performance": "Reported: 91.2% (as reported in the paper for GPT-4 on SuperGLUE).",
            "human_baseline_performance": "Not given as a single figure in this paper; prior SuperGLUE reporting used human baselines and noted BERT is ~20 points worse than humans.",
            "performance_comparison": "Paper claims GPT-4 greatly surpasses the older language-model baselines (e.g., BERT) and narrows the gap with human performance; exact comparison to human baseline not numerically specified in this paper.",
            "experimental_details": "Evaluated using ChatGPT-Plus; the paper provides examples of prompts/responses but does not detail prompting strategy (e.g., few-shot, zero-shot), evaluation splits, or hyperparameters.",
            "limitations_or_caveats": "Human baseline not given explicitly in paper; comparisons to older models (BERT) are descriptive rather than rigorously controlled within this study.",
            "uuid": "e9003.2"
        },
        {
            "name_short": "GPT-4 (HANS)",
            "name_full": "GPT-4 evaluated on HANS (Heuristic Analysis for NLI Systems)",
            "brief_description": "GPT-4 was tested on the HANS diagnostic dataset for NLI heuristics; the paper reports perfect performance on the used HANS subset but cautions possible heuristic memorization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art large language model from OpenAI accessed via ChatGPT-Plus for this study; no parameter-count or training details provided here.",
            "model_size": null,
            "test_battery_name": "HANS",
            "test_description": "Diagnostic dataset designed to detect shallow syntactic/lexical heuristics in NLI models (lexical overlap, subsequence, component heuristics); cognitive domain: syntactic/semantic inference and generalization.",
            "llm_performance": "Reported: 100% on the paper's HANS data subset (paper notes this may be due to dataset subset composition).",
            "human_baseline_performance": "Reported human accuracy range: 76%–97% (as stated in the paper).",
            "performance_comparison": "GPT-4 appears to outperform prior NLI models on the HANS subset used (reported 100%), and well above older model performance (older models often underperformed markedly); however, the paper notes this result may reflect memorization or the use of a subset formed entirely of non-entailment examples.",
            "experimental_details": "Model accessed via ChatGPT-Plus; paper notes the HANS subset used in experiments may contain only non-entailment examples and that experiments with mixed HANS data are ongoing. No detailed prompting protocol provided.",
            "limitations_or_caveats": "Authors explicitly caution that 100% may be due to the dataset subset (all non-entailment) and potential memorization of heuristics; experiments on full/mixed HANS are ongoing, so current result may not reflect robust generalization.",
            "uuid": "e9003.3"
        },
        {
            "name_short": "GPT-2/GPT-3 (MATH, mentioned)",
            "name_full": "GPT-2 and GPT-3 reported on MATH (mentioned)",
            "brief_description": "The paper cites prior reports that GPT-2 and GPT-3 achieved very low accuracy on the MATH dataset (below 10%).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-2 / GPT-3",
            "model_description": "Earlier-generation large language models from OpenAI; paper does not provide architecture/training details for these models and only cites their low MATH performance from prior work.",
            "model_size": null,
            "test_battery_name": "MATH",
            "test_description": "Contest-style mathematics problems assessing multi-step mathematical problem solving.",
            "llm_performance": "Reported as \"below 10%\" accuracy for GPT-2/GPT-3 on MATH (as cited in this paper).",
            "human_baseline_performance": "Human performance varies widely (examples given: ~40% for a CS PhD, ~90% for IMO gold medallist).",
            "performance_comparison": "GPT-2/GPT-3 are substantially below human baselines and below GPT-4's reported category-level performance on some subdomains.",
            "experimental_details": "This paper only cites prior-reported results; it does not run GPT-2/GPT-3 experiments itself.",
            "limitations_or_caveats": "No experimental details provided here for how GPT-2/GPT-3 were evaluated (prompting, splits, or exact setup), so comparisons are approximate and taken from prior literature.",
            "uuid": "e9003.4"
        },
        {
            "name_short": "BERT (SuperGLUE / HANS, mentioned)",
            "name_full": "BERT as baseline on SuperGLUE and HANS (mentioned)",
            "brief_description": "The paper references BERT as a baseline in SuperGLUE reporting and as one of the traditional NLI models evaluated on HANS, noting substantial gaps relative to humans and failure modes on HANS diagnostic splits.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BERT",
            "model_description": "Widely used transformer-based language model (baseline cited in SuperGLUE and compared on HANS); this paper references its baseline performance but does not itself run BERT experiments.",
            "model_size": null,
            "test_battery_name": "SuperGLUE / HANS",
            "test_description": "SuperGLUE: suite of challenging NLU tasks; HANS: diagnostic dataset for syntactic heuristics in NLI. Cognitive domains: advanced NLU and syntactic/semantic inference.",
            "llm_performance": "SuperGLUE: described as ~20 points worse than humans in original study when using BERT as baseline; HANS: BERT and other NLI models \"performed below 10% on the non-entailment category\" (as cited).",
            "human_baseline_performance": "Human baselines are higher (SuperGLUE humans exceed BERT by ~20 points according to original SuperGLUE reporting; HANS human accuracy reported 76%–97%).",
            "performance_comparison": "BERT is described as substantially below human performance on SuperGLUE and as failing specific HANS diagnostic subsets (very low accuracy on non-entailment examples).",
            "experimental_details": "This paper cites prior findings for BERT; it does not provide new BERT experiments or prompting details.",
            "limitations_or_caveats": "Cited comparisons rely on prior studies; this paper does not re-run BERT under the same conditions as GPT-4, so cross-model comparisons are descriptive and not experimentally controlled within this work.",
            "uuid": "e9003.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "rating": 2,
            "sanitized_title": "commonsenseqa_a_question_answering_challenge_targeting_commonsense_knowledge"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "rating": 2,
            "sanitized_title": "superglue_a_stickier_benchmark_for_generalpurpose_language_understanding_systems"
        },
        {
            "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "rating": 2,
            "sanitized_title": "right_for_the_wrong_reasons_diagnosing_syntactic_heuristics_in_natural_language_inference"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Hellaswag: Can a machine really finish your sentence?",
            "rating": 1,
            "sanitized_title": "hellaswag_can_a_machine_really_finish_your_sentence"
        },
        {
            "paper_title": "Winogrande: An adversarial winograd schema challenge at scale",
            "rating": 1,
            "sanitized_title": "winogrande_an_adversarial_winograd_schema_challenge_at_scale"
        }
    ],
    "cost": 0.0109895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Sifatkaur Dhingra sifatkaurd13@gmail.com 
Manmeet Singh manmeet.cat@tropmet.res.in 
Vaisakh Sb vaisakh.sb@tropmet.res.in 
Neetiraj Malviya neetirajmalviya@gmail.com 
Sukhpal Singh Gill s.s.gill@qmul.ac.uk </p>
<p>Department of Psychology
Indian Institute of Tropical Meteorology Pune
Indian Institute of Tropical Meteorology Pune
Defence Institute Of Advanced Technology Pune
Nowrosjee Wadia College Pune
India, India, India, India</p>
<p>Queen Mary University of London London
United Kingdom</p>
<p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Cognitive psychology delves on understanding perception, attention, memory, language, problemsolving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, Super-GLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
<p>Introduction</p>
<p>Cognitive psychology aims to decipher how humans learn new things, retain knowledge, and recall it when needed. Cognitive psychologists seek to understand how the mind works by conducting studies on people's thoughts and actions and by using other experimental methods like brain imaging and computer modelling. Understanding the human mind and developing our cognitive skills to excel in a variety of areas is the ultimate objective of cognitive psychology. Language models have come a long way since the first statistical models for modelling language were introduced. With the advent of deep learning and the availability of large amounts of data, recent years have seen a rapid evolution of language models that have achieved human-like performance on many language tasks. Large Language Models (LLMs) are a type of artificial intelligence framework that have garnered significant attention in recent years due to their remarkable language processing capabilities (Harrer 2023). These models are trained on vast amounts of text data and are able to generate coherent, human-like responses to natural language queries. One of the key features of LLMs is their ability to generate novel and creative responses to text-based prompts, which has led to their increasing use in fields such as chatbots, question answering systems, and language translation. The use of self-attention has been a key factor in this success, as it allows for more efficient and accurate modeling of long-range dependencies within the input sequence, resulting in better performance compared to traditional RNN-based models. LLMs have demonstrated impressive performance on a wide range of language tasks, including language modeling, machine translation, sentiment analysis, and text classification. These capabilities have led to the increased use of LLMs in various fields, including language-based customer service, virtual assistants, and creative writing. One of the key areas measuring intelligence in humans, other species and machines is the cognitive psychology. There are several tasks that are considered to be the benchmarks for testing cognitive psychology. Some of them are text interpretation, computer vision, planning and reasoning. For cognitive psychology to work, we rely on a complex and potent social practise: the attribution and assessment of thoughts and actions [1]. The scientific psychology of cognition and behaviour, a relatively recent innovation, focuses primarily on the information-processing mechanisms and activities that characterise human cognitive and behavioural capabilities. Researchers have attempted to create systems that could use natural language to reason about their surroundings [2] or that could use a world model to get a more profound comprehension of spoken language [3]. The report introducing GPT-4 [4] has tested the HellaSwag [5] and WinoGrande [6] datasets for cognitive psychology. Although, these tests are relevant, they lack the sophistication required to understand deep heuristics of GPT-4. Hellaswag entails the task of finishing a sentence and WinoGrande involves identifying the correct noun for the pronouns in a sentence, which are quite simple. Other tasks and standardized datasets [7] which test the psychology are needed in order to perform a comprehensive assessment of cognitive psychology for GPT-4. Moreover GPT-4 needs to go through complex reasoning tasks than just predicting the last word of the sentence such as in Hellaswag, to emerge as a model capable of high-level intelligence. [8] note that SuperGLUE [9], CommonsenseQA [10], MATH [11] and HANS [12] are four such datasets that are needed to be tested for a comprehensive cognitive psychology evaluation of AI models. In this study, we evaluate the performance of GPT-4 on the SuperGLUE, CommonsenseQA, MATH and HANS datasets. This is a work in progress and we are performing continuous tests with the other datasets as suggested by [8]. Our study can be used to build up higher-order psychological tests using GPT-4.</p>
<p>Datasets and Methodology</p>
<p>In this study, four datasets have been used to test the cognitive psychology capabilities of GPT-4. The four datasets are CommonsenseQA, MATH, SuperGLUE and HANS. They are described as below:</p>
<p>CommonsenseQA</p>
<p>CommonsenseQA is a dataset composed for testing commonsense reasoning. There are 12,247 questions in the dataset, each with 5 possible answers. Workers using Amazon's Mechanical Turk were used to build the dataset. The goal of the dataset is to evaluate the commonsense knowledge using CONCEPTNET to generate difficult questions. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %.</p>
<p>MATH</p>
<p>The MATH dataset includes almost 12,500 problems from scholastic mathematics contests. Machine learning models take a mathematical problem as input and produce an answer-encoding sequence, such as f rac23. After normalisation, their answers are distinct, therefore MATH may be evaluated using exact match instead of heuristic metrics like BLEU. Problems in seven different areas of mathematics, including geometry, are categorised by complexity from 1 to 5, and diagrams can be expressed in text using the Asymptote language. This allows for a nuanced evaluation of problem-solving skills in mathematics across a wide range of rigour and content. Problems now have comprehensive, detailed, step-by-step answers. To improve learning and make model outputs more interpretable, models can be trained on these to develop their own step-by-step solutions. The MATH dataset presents a significant challenge, with accuracy rates for big language models ranging from 3.0% to 6.9%. Models attain up to 15% accuracy on the least difficulty level and can develop step-by-step answers that are coherent and on-topic even when erroneous, suggesting that they do possess some mathematical knowledge despite their low accuracies. The results of human evaluations on MATH show that it may be difficult for humans as well; a computer science PhD student who does not really like mathematics scored about 40%, while a three-time IMO gold medallist scored 90%.</p>
<p>SuperGLUE</p>
<p>SuperGLUE is an updated version of the GLUE benchmark that includes a more challenging set of language understanding tasks. Using the gap between human and machine performance as a metric, SuperGLUE improves upon the GLUE benchmark by defining a new set of difficult Natural Language Understanding (NLU) problems. About half of the tasks in the SuperGLUE benchmark have fewer than 1k instances, and all but one have fewer than 10k examples, highlighting the importance of different task formats and low-data training data problems. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential.</p>
<p>HANS</p>
<p>The strength of neural networks lies in their ability to analyse a training set for statistical patterns and then apply those patterns to test instances that come from the same distribution. This advantage is not without its drawbacks, however, as statistical learners, such as traditional neural network designs, tend to rely on simplistic approaches that work for the vast majority of training samples rather than capturing the underlying generalisations. The loss function may not motivate the model to learn to generalise to increasingly difficult scenarios in the same way a person would if heuristics tend to produce mostly correct results. This problem has been observed in several applications of AI. Contextual heuristics mislead object-recognition neural networks in computer vision, for example; a network that can accurately identify monkeys in a normal situation may mistake a monkey carrying a guitar for a person, since guitars tend to co-occur with people but not monkeys in the training set. Visual question answering systems are prone to the same heuristics. This problem is tackled by HANS (Heuristic Analysis for NLI Systems), which uses heuristics to determine if a premise sentence entails (i.e., suggests the truth of) a hypothesis sentence. Neural Natural Language Inference (NLI) models have been demonstrated to learn shallow heuristics based on the presence of specific words, as has been the case in other fields. As not often appears in the instances of contradiction in normal NLI training sets, a model can categorise all inputs containing the word not as contradiction. HANS prioritises heuristics that are founded on elementary syntactic characteristics. Think about the entailment-focused phrase pair below:</p>
<p>Premise: The judge was paid by the actor.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>An NLI system may accurately label this example not by deducing the meanings of these lines but by assuming that the premise involves any hypothesis whose terms all occur in the premise. Importantly, if the model is employing this heuristic, it will incorrectly classify the following as entailed even when it is not.</p>
<p>Premise: The actor was paid by the judge.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>HANS is intended to detect the presence of such faulty structural heuristics. The authors focus on the lexical overlap, subsequence, and component heuristics. These heuristics are not legitimate inference procedures despite often producing correct labels. Rather than just having reduced overall accuracy, HANS is meant to ensure that models using these heuristics fail on specific subsets of the dataset. Four well-known NLI models, including BERT, are compared and contrasted using the HANS dataset. For this dataset, all models significantly underperformed the chance distribution, with accuracy just exceeding 0% in most situations.</p>
<p>Methodology</p>
<p>We test the four datasets as described above to test the cognitive psychology capabilities of GPT-4. The model is accessed using the ChatGPT-Plus offered by OpenAI. We evaluate these models as shown in the results and discussion section.</p>
<p>Results</p>
<p>We will first discuss the human and machine skill of the different models traditionally used in the datasets used to test cognitive psychology. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %. The accuracy of humans on HANS dataset ranged from 76-97 % and the authors show   </p>
<p>Conclusions</p>
<p>GPT-4, which is a state-of-the-art large language model, is a revolution in the field of psychology since it gives psychologists unprecedented resources to use in their studies and work. This sophisticated AI model offers psychologists and psychiatrists to learn more about the human mind and come up with novel treatment theories and approaches. It provides an avenue for improved efficacy of psychological therapies and allowing professionals to spend more time with clients, leading to deeper and more fruitful therapeutic bonds. The potential applications of GPT-4 can only be realized if the model is thoroughly tested on basic tests of reasoning and cognition. Cognitive psychology enables the humans to perform various activities [13] in their personal and professional lives. We show that the performance of GPT-4 greatly surpasses the language model used in the original studies from where the different datasets are sourced, thus it can make a tool of day-to-day utility for psychologists. This development can lead to cascading benefits in addressing the mental health challenges faced by today's society. Problem: $ABCD$, a rectangle with $AB = 12$ and $BC = 16$, is the base of pyramid $P$, which has a height of $24$. A plane parallel to $ABCD$ is passed through $P$, dividing $P$ into a frustum $F$ and a smaller pyramid $P'$. Let $X$ denote the center of the circumsphere of $F$, and let $T$ denote the apex of $P$. If the volume of $P$ is eight times that of $P'$, then the value of $XT$ can be expressed as $ \frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Compute the value of $m + n$.</p>
<p>Finally, we have $m + n = 108 + 10 = \boxed{118}$. </p>
<p>Figure 1 :
1Datasets used in the study with the different categories contained in them. that the BERT model performed below 10 % on the non-entailment category. The human performance on MATH varied from 40-90 % and GPT-2/GPT-3 showed accuracies below 10 %.</p>
<p>Figure 1
1shows that GPT-4 has an accuracy of 83.2 % on CommonSenseQA, data, we find that GPT-4 has an accuracy of around 84%, 82 % on prealgebra, 35% on geometry, 100% on HANS and 91.2 % on SuperGLUE. It is to be noted that the perfect results on HANS data might be because all the examples used are of non-entailment, as the model might be memorizing this particular heuristic. The experiments to generate GPT-4 results with mixed data from HANS are ongoing.</p>
<p>She caught the first snowflake of Sher life on her tongue, she was naively excited to be spending a Swhat in a region that actually had snow? question concept: punishing choices: SA:cloud B:december C:air D:africa E:winterwhat in a region that actually had snow? question concept: punishing choices:Problem: Find $a+b+c$ if the graph 
of the equation $y=ax^2+bx+c$ 
is a parabola with vertex $(5,3)$, 
vertical axis of symmetry, and contains 
the point $(2,0)$. Level: Level 5 Type: 
Algebra </p>
<p>Non entailment
entailmentProblem: What is the greatest integer $x$ for which $\frac79 &gt; \frac{x} {13}$? Level: Level 3 Type: Prealgebra Solution: Sentence1 Tanks were developed by Britain and France, and were first used in combat by the British during a battle. Sentence2 Tanks were developed by Britain and France, and were first used in combat by the British during a battle with German forces. Entailment or Non-EntailmentFigure 2: Examples of sample prompts and the respective responses of GPT4 on CommonsenseQA, MATH and SuperGLUE datasets$a+b+c = -
\frac{7}{3}$. </p>
<p>the greatest 
integer $x$ that 
satisfies the 
inequality is $x = 
10$. </p>
<p>E:winter </p>
<p>Dataset/Task 
Sample Prompt 
GPT4 
response </p>
<p>Commonsense 
reasoning </p>
<p>Algebra </p>
<p>Prealgebra </p>
<p>SuperGLUE </p>
<p>Geometry </p>
<p>Text and patterns: For effective chain of thought. Aman Madaan, Amir Yazdanbakhsh, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasksAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasks</p>
<p>A basis for a mathematical theory of computation. John Mccarthy, Studies in Logic and the Foundations of Mathematics. Elsevier26John McCarthy. A basis for a mathematical theory of computation. In Studies in Logic and the Foundations of Mathematics, volume 26, pages 33-70. Elsevier, 1959.</p>
<p>Understanding natural language. Terry Winograd, Cognitive psychology. 31Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.</p>
<p>Gpt-4 technical report. Openai, OpenAI. Gpt-4 technical report. 2023.</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Yuxuan Li, James L Mcclelland, arXiv:2210.00400arXiv preprintYuxuan Li and James L McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks. arXiv preprint arXiv:2210.00400, 2022.</p>
<p>Probing the psychology of ai models. Richard Shiffrin, Melanie Mitchell, Proceedings of the National Academy of Sciences. 120102300963120Richard Shiffrin and Melanie Mitchell. Probing the psychology of ai models. Proceedings of the National Academy of Sciences, 120(10):e2300963120, 2023.</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874arXiv preprintDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Thomas Mccoy, Ellie Pavlick, Tal Linzen, arXiv:1902.01007arXiv preprintR Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.</p>
<p>Using large language models to simulate multiple humans. Gati Aher, I Rosa, Adam Tauman Arriaga, Kalai, arXiv:2208.10264arXiv preprintGati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
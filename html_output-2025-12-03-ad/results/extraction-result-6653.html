<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6653 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6653</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6653</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-b272513916b45c8517d289d7abee4a53e6832187</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b272513916b45c8517d289d7abee4a53e6832187" target="_blank">ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools, thereby amalgamating the analytical prowess of language and the computational efficiency of tools.</p>
                <p><strong>Paper Abstract:</strong> Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6653.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6653.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-Integrated Reasoning Agents (ToRA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A suite of open-source models fine-tuned to interleave natural-language rationales with program-based tool use (sympy, Python execution) for solving mathematical problems; trained via imitation learning on GPT-4-generated interactive tool-use trajectories (ToRA-CORPUS) and refined with output-space shaping (sampling + teacher correction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToRA</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>multiple (7B, 13B, 70B; see variants)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on ToRA-CORPUS (≈16k GPT-4-generated tool-interactive trajectories from GSM8k and MATH training sets) and further trained with output-space shaping (additional sampled valid trajectories and corrected invalid ones); includes program/code examples using sympy and other Python libraries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH (and 8 out-of-distribution arithmetic/math datasets including MAWPS, GSM-Hard, SVAMP, ASDiv, TabMWP)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems, algebra, geometry, number theory, counting/probability, and basic arithmetic datasets</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems and multiple-choice (MATH) and some tabular formats (TabMWP)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ranging from grade-school arithmetic (GSM8k, MAWPS) to competition-level mathematics (MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>trained to perform Tool-integrated Reasoning (interleaving natural-language rationales and program generation/execution); imitation learning from GPT-4 trajectories and output-space shaping (nucleus sampling + teacher corrections)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact numeric answer matching after parsing/rounding with sympy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Depending on size: ToRA-7B MATH 40.1% (GSM8k 68.8%); ToRA-13B MATH 43.0% (GSM8k 72.7%); ToRA-70B MATH 49.7% (GSM8k 84.3%). Reported average improvements vs prior open-source baselines of ~13–19% across 10 tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No neuron-level mechanistic probing reported; analysis focuses on behavioral and tool-usage patterns: which python/sympy libraries are invoked per MATH subtopic, ablations on interleaving rationale vs program-only vs rationale-only, and quantified benefit of output-space shaping; rationale presence improves planning and multi-round self-correction; output shaping increases trajectory diversity and fixes improper tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Human-annotated failure categories on MATH (sample of 100): Reasoning errors 38%, Diagram understanding 21%, Inappropriate tool use 10%, Syntax errors 9%, Runtime errors 9%, Hallucination 5%, Rationale-only/formalization errors 3%, False negatives 5%.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>ToRA performance improves with model size; output-space shaping provides consistent +~3–4% absolute gains (larger benefit on smaller models and on harder/longer problems); training on code (ToRA-CODE) yields ~5% higher accuracy than ToRA of same size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6653.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToRA-CODE-34B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToRA Code variant, 34B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CodeLLaMA-based ToRA model (CodeLLaMA fine-tuned with ToRA-CORPUS and output-space shaping) optimized for program-based tool use and mathematical problem solving; achieves state-of-the-art open-source results on MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ToRA-CODE-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (code-specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued training of CodeLLaMA-34B on ToRA-CORPUS (GPT-4 generated trajectories) plus output-space shaping (sampling of 64 trajectories per question, keep valid, teacher-corrected invalid ones), includes many program/code examples using sympy and Python.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GSM8k (and 8 additional math datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level math problems (MATH), grade-school word problems (GSM8k), algebra, number theory, geometry, counting & probability</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems; some multiple-choice (MATH), symbolic expressions parsed via sympy</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition-level (MATH) down to grade-school (GSM8k)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tool-integrated Reasoning (interleaving rationale and program synthesis/execution); greedy decoding for inference with up to 3 tool executions; trained with imitation learning + output-space shaping</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MATH: 50.8% accuracy; GSM8k: 80.7% accuracy; average across 10 tasks: 74.8% (Table 2). This model exceeds GPT-4 CoT on MATH (50.8% vs GPT-4 CoT 42.5%) and is competitive with GPT-4 running code (GPT-4-PAL 51.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analyses show heavy use of sympy/internal solvers for algebra topics, matrix libraries for precalculus, gcd/lcm algorithms for number theory, and rational/fraction handling for geometry; ablations indicate removing rationales or shaping drops performance substantially (w/o rationale -8.9 to -10.2 absolute on MATH overall).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same taxonomy as ToRA: majority are reasoning errors; also syntax/runtime/inappropriate-tool-use errors due to program generation; diagram misinterpretation affects geometry-rich problems.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Code-finetuning (ToRA-CODE) gives ~5% absolute improvement over non-code ToRA of same size; output-space shaping adds ~3–4% absolute; larger models (34B, 70B) perform better on long/harder problems but gaps remain at highest difficulty (level 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6653.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large language model evaluated as a baseline and also used to synthesize ToRA training trajectories; it was tested with chain-of-thought (CoT), PAL (program-aided) prompting, and the Tool-integrated Reasoning format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (proprietary, very large)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper; generally trained on large-scale web, books, code, and other corpora (paper does not provide details).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH (and MATH subtopics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems, competition-level math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language; multiple-choice for MATH; also used to generate program + rationale trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought (CoT), PAL (program synthesis), and Tool-integrated Reasoning (interleaving rationale and code) were evaluated in the paper; GPT-4 was also used with few-shot examples and nucleus sampling when generating ToRA-CORPUS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported baselines: GSM8k CoT 92.0% accuracy; MATH CoT 42.5% accuracy. With PAL (code execution) GPT-4 achieves MATH 51.8%. When applying Tool-integrated Reasoning, GPT-4 reaches higher MATH scores (reported in Appendix: Tool-integrated Reasoning overall 61.6% on test in Table 6 under a specific setup).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper does not perform mechanistic analysis of GPT-4 internals; it uses GPT-4 for high-quality trajectory generation and shows that Tool-integrated Reasoning applied to GPT-4 outperforms PAL/CoT on MATH, implying synergistic benefit of interleaving language reasoning and tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not exhaustively analyzed for GPT-4 in this paper; general failure categories on MATH (as annotated for ToRA) include reasoning errors, diagram understanding, and tool-related errors—these serve as context for GPT-4 comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not analyzed across sizes (proprietary single model); however, GPT-4 benefits from program execution (PAL) and further benefits when using Tool-integrated Reasoning according to reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6653.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used proprietary LLM (gpt-3.5 family) evaluated as a baseline with CoT and PAL prompting on math benchmarks in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (proprietary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (smaller than GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in paper; generally trained on internet text and instruction-tuning; likely includes web/text but limited code compared to code-specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language; multiple-choice for MATH</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-Thought (CoT) and PAL (program-aided) prompting evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8k CoT: 80.8% (paper); MATH CoT: 35.5%; ChatGPT PAL MATH: 38.7% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic probing reported; compared behaviorally to ToRA and GPT-4 under different prompting formats.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper does not detail ChatGPT-specific failure breakdown; general failure categories from ToRA analysis apply as comparative context.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>N/A within this paper (single model baseline); PAL improves ChatGPT modestly over CoT in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6653.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 (Meta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source family of decoder-only transformer models used as base models for ToRA fine-tuning; evaluated with CoT and as starting weights for ToRA variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B (evaluated sizes in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on large web text corpora (paper does not detail math-specific data); ToRA fine-tunes these with code and tool-interactive trajectories for math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH, MAWPS and others</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems and higher-level math</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural language</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition-level (after fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Baseline uses CoT prompting; ToRA fine-tunes LLaMA-2 to use Tool-integrated Reasoning (rationale + program) via imitation learning and shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Vanilla LLaMA-2 (7B) CoT: GSM8k 13.3%, MATH 4.1%; after ToRA finetuning LLaMA-2-based ToRA-7B MATH 40.1% and GSM8k 68.8%. Larger LLaMA-2 sizes follow similar uplift when ToRA-trained.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No layer/neuron-level analysis; paper shows dramatic behavioral improvements from format change and finetuning (CoT SFT/RFT vs ToRA interleaving).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same taxonomy as ToRA when finetuned; vanilla LLaMA-2 predominantly fails due to inability to perform precise computation and programmatic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Base LLaMA-2 shows clear performance gains when scaled and finetuned with ToRA methods; larger LLaMA-2 models benefit from ToRA training but still lag behind GPT-4 unless tool integration and shaping are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6653.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeLLaMA (PAL baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeLLaMA (code-specialized LLaMA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized LLaMA family that excels at program synthesis; used as a base for ToRA-CODE variants and evaluated with PAL prompting and tool-integration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CodeLLaMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (code-focused)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 34B (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained with large code corpora and code-savvy datasets (paper uses CodeLLaMA checkpoints as starting points for ToRA-CODE fine-tuning on ToRA-CORPUS).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8k, MATH, MAWPS, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>program synthesis to solve math problems, multi-step arithmetic and algebra</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language converted to program+execution (PAL/Tool-integrated)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to competition-level</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>PAL (program-aided language modeling) and Tool-integrated Reasoning fine-tuning (ToRA-CODE variants)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CodeLLaMA (PAL baseline) 34B: GSM8k 53.3%, MATH 23.9% (Table 2). After ToRA-CODE fine-tuning, CodeLLaMA-34B (ToRA-CODE-34B) reaches GSM8k 80.7% and MATH 50.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper contrasts program-only (PAL) vs interleaved approach; CodeLLaMA benefits substantially from interleaving rationale and tool calls, indicating code-specialized models require rationale to plan and decompose problems effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Program-synthesis-specific issues: syntax/runtime errors, inappropriate tool use, and failure to plan non-algorithmic parts of problems without rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Code-specialized models benefit more from ToRA fine-tuning; ToRA-CODE yields consistent gains across sizes and narrows gap with closed-source GPT-4 when program execution is available.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6653.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6653.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool-integrated Reasoning (format)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool-integrated Reasoning (interleaving natural language rationales and program-based tool use)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reasoning format and training objective where models alternate between natural-language rationales and program generation for tool execution, producing interactive trajectories r1, a1, o1, ..., rn where program outputs feed back into subsequent reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Tool-integrated Reasoning (format/method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>method for transformer LLMs (format-level intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>applicable to any model size (demonstrated 7B–70B)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Implemented by synthesizing trajectories with GPT-4 (ToRA-CORPUS) then imitation learning and output-space shaping on open-source models; programs target Python libraries (sympy, math, fractions, gcd/lcm, matrix ops).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Applied to GSM8k, MATH, and other math datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic, algebra, geometry, number theory, and program-executable subroutines</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>interleaved natural-language questions with generated python code and tool outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>improves across difficulty spectrum, particularly effective on medium-to-hard problems requiring computation and symbolic manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot prompts for GPT-4 to generate trajectories during corpus creation; models trained to produce sequences: rationale -> program -> execute -> incorporate output -> continue; inference via iterative generation with up to 3 executions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Applied to GPT-4 and open-source models: yields large gains over CoT and program-only methods; example: GPT-4 Tool-integrated Reasoning MATH 61.6% (Appendix Table 6), ToRA LLaMA-2 models show +29% and +6.7% over rationale-only and program-only respectively (LLaMA-2 results in Fig. 4).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Behavioral analyses demonstrate the interleaving better balances abstract planning (natural language) with exact computation/symbolic solving (tool outputs); ablation removing rationale or shaping substantially reduces performance, showing complementary roles.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When interleaving fails: inappropriate tool selection/use, syntax/runtime errors in generated code, and remaining high-level reasoning errors (the latter being the largest single source of failure).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Interleaving format benefits both closed-source and open-source models and yields larger relative improvements for smaller models; performance further increases when combined with output-space shaping and code-specialized base models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>PAL: Program-aided Language Models <em>(Rating: 2)</em></li>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks <em>(Rating: 2)</em></li>
                <li>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing <em>(Rating: 1)</em></li>
                <li>Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6653",
    "paper_id": "paper-b272513916b45c8517d289d7abee4a53e6832187",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "ToRA",
            "name_full": "Tool-Integrated Reasoning Agents (ToRA)",
            "brief_description": "A suite of open-source models fine-tuned to interleave natural-language rationales with program-based tool use (sympy, Python execution) for solving mathematical problems; trained via imitation learning on GPT-4-generated interactive tool-use trajectories (ToRA-CORPUS) and refined with output-space shaping (sampling + teacher correction).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ToRA",
            "model_family": "decoder-only transformer",
            "model_size": "multiple (7B, 13B, 70B; see variants)",
            "training_data_description": "Fine-tuned on ToRA-CORPUS (≈16k GPT-4-generated tool-interactive trajectories from GSM8k and MATH training sets) and further trained with output-space shaping (additional sampled valid trajectories and corrected invalid ones); includes program/code examples using sympy and other Python libraries.",
            "benchmark_name": "GSM8k, MATH (and 8 out-of-distribution arithmetic/math datasets including MAWPS, GSM-Hard, SVAMP, ASDiv, TabMWP)",
            "task_type": "multi-step math word problems, algebra, geometry, number theory, counting/probability, and basic arithmetic datasets",
            "problem_format": "natural-language word problems and multiple-choice (MATH) and some tabular formats (TabMWP)",
            "difficulty_level": "ranging from grade-school arithmetic (GSM8k, MAWPS) to competition-level mathematics (MATH)",
            "prompting_method": "trained to perform Tool-integrated Reasoning (interleaving natural-language rationales and program generation/execution); imitation learning from GPT-4 trajectories and output-space shaping (nucleus sampling + teacher corrections)",
            "performance_metric": "accuracy (exact numeric answer matching after parsing/rounding with sympy)",
            "performance_value": "Depending on size: ToRA-7B MATH 40.1% (GSM8k 68.8%); ToRA-13B MATH 43.0% (GSM8k 72.7%); ToRA-70B MATH 49.7% (GSM8k 84.3%). Reported average improvements vs prior open-source baselines of ~13–19% across 10 tasks.",
            "internal_analysis": "No neuron-level mechanistic probing reported; analysis focuses on behavioral and tool-usage patterns: which python/sympy libraries are invoked per MATH subtopic, ablations on interleaving rationale vs program-only vs rationale-only, and quantified benefit of output-space shaping; rationale presence improves planning and multi-round self-correction; output shaping increases trajectory diversity and fixes improper tool use.",
            "failure_modes": "Human-annotated failure categories on MATH (sample of 100): Reasoning errors 38%, Diagram understanding 21%, Inappropriate tool use 10%, Syntax errors 9%, Runtime errors 9%, Hallucination 5%, Rationale-only/formalization errors 3%, False negatives 5%.",
            "scaling_trend": "ToRA performance improves with model size; output-space shaping provides consistent +~3–4% absolute gains (larger benefit on smaller models and on harder/longer problems); training on code (ToRA-CODE) yields ~5% higher accuracy than ToRA of same size.",
            "uuid": "e6653.0",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ToRA-CODE-34B",
            "name_full": "ToRA Code variant, 34B parameters",
            "brief_description": "A CodeLLaMA-based ToRA model (CodeLLaMA fine-tuned with ToRA-CORPUS and output-space shaping) optimized for program-based tool use and mathematical problem solving; achieves state-of-the-art open-source results on MATH.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ToRA-CODE-34B",
            "model_family": "decoder-only transformer (code-specialized)",
            "model_size": "34B",
            "training_data_description": "Continued training of CodeLLaMA-34B on ToRA-CORPUS (GPT-4 generated trajectories) plus output-space shaping (sampling of 64 trajectories per question, keep valid, teacher-corrected invalid ones), includes many program/code examples using sympy and Python.",
            "benchmark_name": "MATH, GSM8k (and 8 additional math datasets)",
            "task_type": "competition-level math problems (MATH), grade-school word problems (GSM8k), algebra, number theory, geometry, counting & probability",
            "problem_format": "natural-language word problems; some multiple-choice (MATH), symbolic expressions parsed via sympy",
            "difficulty_level": "competition-level (MATH) down to grade-school (GSM8k)",
            "prompting_method": "Tool-integrated Reasoning (interleaving rationale and program synthesis/execution); greedy decoding for inference with up to 3 tool executions; trained with imitation learning + output-space shaping",
            "performance_metric": "accuracy",
            "performance_value": "MATH: 50.8% accuracy; GSM8k: 80.7% accuracy; average across 10 tasks: 74.8% (Table 2). This model exceeds GPT-4 CoT on MATH (50.8% vs GPT-4 CoT 42.5%) and is competitive with GPT-4 running code (GPT-4-PAL 51.8%).",
            "internal_analysis": "Behavioral analyses show heavy use of sympy/internal solvers for algebra topics, matrix libraries for precalculus, gcd/lcm algorithms for number theory, and rational/fraction handling for geometry; ablations indicate removing rationales or shaping drops performance substantially (w/o rationale -8.9 to -10.2 absolute on MATH overall).",
            "failure_modes": "Same taxonomy as ToRA: majority are reasoning errors; also syntax/runtime/inappropriate-tool-use errors due to program generation; diagram misinterpretation affects geometry-rich problems.",
            "scaling_trend": "Code-finetuning (ToRA-CODE) gives ~5% absolute improvement over non-code ToRA of same size; output-space shaping adds ~3–4% absolute; larger models (34B, 70B) perform better on long/harder problems but gaps remain at highest difficulty (level 5).",
            "uuid": "e6653.1",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A proprietary large language model evaluated as a baseline and also used to synthesize ToRA training trajectories; it was tested with chain-of-thought (CoT), PAL (program-aided) prompting, and the Tool-integrated Reasoning format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_family": "decoder-only transformer (proprietary)",
            "model_size": "not specified (proprietary, very large)",
            "training_data_description": "Not specified in paper; generally trained on large-scale web, books, code, and other corpora (paper does not provide details).",
            "benchmark_name": "GSM8k, MATH (and MATH subtopics)",
            "task_type": "multi-step word problems, competition-level math",
            "problem_format": "natural-language; multiple-choice for MATH; also used to generate program + rationale trajectories",
            "difficulty_level": "grade-school to competition-level",
            "prompting_method": "Chain-of-Thought (CoT), PAL (program synthesis), and Tool-integrated Reasoning (interleaving rationale and code) were evaluated in the paper; GPT-4 was also used with few-shot examples and nucleus sampling when generating ToRA-CORPUS.",
            "performance_metric": "accuracy",
            "performance_value": "Reported baselines: GSM8k CoT 92.0% accuracy; MATH CoT 42.5% accuracy. With PAL (code execution) GPT-4 achieves MATH 51.8%. When applying Tool-integrated Reasoning, GPT-4 reaches higher MATH scores (reported in Appendix: Tool-integrated Reasoning overall 61.6% on test in Table 6 under a specific setup).",
            "internal_analysis": "Paper does not perform mechanistic analysis of GPT-4 internals; it uses GPT-4 for high-quality trajectory generation and shows that Tool-integrated Reasoning applied to GPT-4 outperforms PAL/CoT on MATH, implying synergistic benefit of interleaving language reasoning and tool use.",
            "failure_modes": "Not exhaustively analyzed for GPT-4 in this paper; general failure categories on MATH (as annotated for ToRA) include reasoning errors, diagram understanding, and tool-related errors—these serve as context for GPT-4 comparisons.",
            "scaling_trend": "Not analyzed across sizes (proprietary single model); however, GPT-4 benefits from program execution (PAL) and further benefits when using Tool-integrated Reasoning according to reported comparisons.",
            "uuid": "e6653.2",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (gpt-3.5-turbo)",
            "brief_description": "A widely used proprietary LLM (gpt-3.5 family) evaluated as a baseline with CoT and PAL prompting on math benchmarks in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_family": "decoder-only transformer (proprietary)",
            "model_size": "not specified (smaller than GPT-4)",
            "training_data_description": "Not specified in paper; generally trained on internet text and instruction-tuning; likely includes web/text but limited code compared to code-specialized models.",
            "benchmark_name": "GSM8k, MATH",
            "task_type": "multi-step math word problems",
            "problem_format": "natural-language; multiple-choice for MATH",
            "difficulty_level": "grade-school to competition-level",
            "prompting_method": "Chain-of-Thought (CoT) and PAL (program-aided) prompting evaluated",
            "performance_metric": "accuracy",
            "performance_value": "GSM8k CoT: 80.8% (paper); MATH CoT: 35.5%; ChatGPT PAL MATH: 38.7% (Table 2).",
            "internal_analysis": "No internal mechanistic probing reported; compared behaviorally to ToRA and GPT-4 under different prompting formats.",
            "failure_modes": "Paper does not detail ChatGPT-specific failure breakdown; general failure categories from ToRA analysis apply as comparative context.",
            "scaling_trend": "N/A within this paper (single model baseline); PAL improves ChatGPT modestly over CoT in some settings.",
            "uuid": "e6653.3",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLaMA-2 (baseline)",
            "name_full": "LLaMA-2 (Meta)",
            "brief_description": "An open-source family of decoder-only transformer models used as base models for ToRA fine-tuning; evaluated with CoT and as starting weights for ToRA variants.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2",
            "model_family": "decoder-only transformer",
            "model_size": "7B, 13B, 70B (evaluated sizes in paper)",
            "training_data_description": "Pretrained on large web text corpora (paper does not detail math-specific data); ToRA fine-tunes these with code and tool-interactive trajectories for math tasks.",
            "benchmark_name": "GSM8k, MATH, MAWPS and others",
            "task_type": "multi-step arithmetic word problems and higher-level math",
            "problem_format": "natural language",
            "difficulty_level": "grade-school to competition-level (after fine-tuning)",
            "prompting_method": "Baseline uses CoT prompting; ToRA fine-tunes LLaMA-2 to use Tool-integrated Reasoning (rationale + program) via imitation learning and shaping.",
            "performance_metric": "accuracy",
            "performance_value": "Vanilla LLaMA-2 (7B) CoT: GSM8k 13.3%, MATH 4.1%; after ToRA finetuning LLaMA-2-based ToRA-7B MATH 40.1% and GSM8k 68.8%. Larger LLaMA-2 sizes follow similar uplift when ToRA-trained.",
            "internal_analysis": "No layer/neuron-level analysis; paper shows dramatic behavioral improvements from format change and finetuning (CoT SFT/RFT vs ToRA interleaving).",
            "failure_modes": "Same taxonomy as ToRA when finetuned; vanilla LLaMA-2 predominantly fails due to inability to perform precise computation and programmatic manipulation.",
            "scaling_trend": "Base LLaMA-2 shows clear performance gains when scaled and finetuned with ToRA methods; larger LLaMA-2 models benefit from ToRA training but still lag behind GPT-4 unless tool integration and shaping are applied.",
            "uuid": "e6653.4",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CodeLLaMA (PAL baseline)",
            "name_full": "CodeLLaMA (code-specialized LLaMA variant)",
            "brief_description": "A code-specialized LLaMA family that excels at program synthesis; used as a base for ToRA-CODE variants and evaluated with PAL prompting and tool-integration.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "CodeLLaMA",
            "model_family": "decoder-only transformer (code-focused)",
            "model_size": "7B, 13B, 34B (evaluated)",
            "training_data_description": "Pretrained with large code corpora and code-savvy datasets (paper uses CodeLLaMA checkpoints as starting points for ToRA-CODE fine-tuning on ToRA-CORPUS).",
            "benchmark_name": "GSM8k, MATH, MAWPS, etc.",
            "task_type": "program synthesis to solve math problems, multi-step arithmetic and algebra",
            "problem_format": "natural-language converted to program+execution (PAL/Tool-integrated)",
            "difficulty_level": "grade-school to competition-level",
            "prompting_method": "PAL (program-aided language modeling) and Tool-integrated Reasoning fine-tuning (ToRA-CODE variants)",
            "performance_metric": "accuracy",
            "performance_value": "CodeLLaMA (PAL baseline) 34B: GSM8k 53.3%, MATH 23.9% (Table 2). After ToRA-CODE fine-tuning, CodeLLaMA-34B (ToRA-CODE-34B) reaches GSM8k 80.7% and MATH 50.8%.",
            "internal_analysis": "Paper contrasts program-only (PAL) vs interleaved approach; CodeLLaMA benefits substantially from interleaving rationale and tool calls, indicating code-specialized models require rationale to plan and decompose problems effectively.",
            "failure_modes": "Program-synthesis-specific issues: syntax/runtime errors, inappropriate tool use, and failure to plan non-algorithmic parts of problems without rationale.",
            "scaling_trend": "Code-specialized models benefit more from ToRA fine-tuning; ToRA-CODE yields consistent gains across sizes and narrows gap with closed-source GPT-4 when program execution is available.",
            "uuid": "e6653.5",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Tool-integrated Reasoning (format)",
            "name_full": "Tool-integrated Reasoning (interleaving natural language rationales and program-based tool use)",
            "brief_description": "A reasoning format and training objective where models alternate between natural-language rationales and program generation for tool execution, producing interactive trajectories r1, a1, o1, ..., rn where program outputs feed back into subsequent reasoning steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Tool-integrated Reasoning (format/method)",
            "model_family": "method for transformer LLMs (format-level intervention)",
            "model_size": "applicable to any model size (demonstrated 7B–70B)",
            "training_data_description": "Implemented by synthesizing trajectories with GPT-4 (ToRA-CORPUS) then imitation learning and output-space shaping on open-source models; programs target Python libraries (sympy, math, fractions, gcd/lcm, matrix ops).",
            "benchmark_name": "Applied to GSM8k, MATH, and other math datasets",
            "task_type": "multi-step arithmetic, algebra, geometry, number theory, and program-executable subroutines",
            "problem_format": "interleaved natural-language questions with generated python code and tool outputs",
            "difficulty_level": "improves across difficulty spectrum, particularly effective on medium-to-hard problems requiring computation and symbolic manipulation",
            "prompting_method": "few-shot prompts for GPT-4 to generate trajectories during corpus creation; models trained to produce sequences: rationale -&gt; program -&gt; execute -&gt; incorporate output -&gt; continue; inference via iterative generation with up to 3 executions",
            "performance_metric": "accuracy",
            "performance_value": "Applied to GPT-4 and open-source models: yields large gains over CoT and program-only methods; example: GPT-4 Tool-integrated Reasoning MATH 61.6% (Appendix Table 6), ToRA LLaMA-2 models show +29% and +6.7% over rationale-only and program-only respectively (LLaMA-2 results in Fig. 4).",
            "internal_analysis": "Behavioral analyses demonstrate the interleaving better balances abstract planning (natural language) with exact computation/symbolic solving (tool outputs); ablation removing rationale or shaping substantially reduces performance, showing complementary roles.",
            "failure_modes": "When interleaving fails: inappropriate tool selection/use, syntax/runtime errors in generated code, and remaining high-level reasoning errors (the latter being the largest single source of failure).",
            "scaling_trend": "Interleaving format benefits both closed-source and open-source models and yields larger relative improvements for smaller models; performance further increases when combined with output-space shaping and code-specialized base models.",
            "uuid": "e6653.6",
            "source_info": {
                "paper_title": "ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "PAL: Program-aided Language Models",
            "rating": 2
        },
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "rating": 2
        },
        {
            "paper_title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "rating": 1
        },
        {
            "paper_title": "Solving challenging math word problems using GPT-4 code interpreter with code-based self-verification",
            "rating": 1
        }
    ],
    "cost": 0.01722975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TORA: A TOOL-INTEGRATED REASONING AGENT for Mathematical Problem Solving</h1>
<p>Zhibin Gou ${ }^{1,2 <em>}$, Zhihong Shao ${ }^{1,2 </em>}$, Yeyun Gong ${ }^{2 \dagger}$, Yelong Shen ${ }^{3}$<br>Yuju Yang ${ }^{1 \dagger}$, Minlie Huang ${ }^{1 \dagger}$, Nan Duan ${ }^{2}$, Weizhu Chen ${ }^{3}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Microsoft Research ${ }^{3}$ Microsoft Azure AI<br>{gzb22, szh19}@mails.tsinghua.edu.cn<br>{yegong, yeshe, nanduan, wzchen}@microsoft.com</p>
<h4>Abstract</h4>
<p>Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose TORA, a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train TORA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, TORA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with $13 \%-19 \%$ absolute improvements on average. Notably, TORA-7B reaches $44.6 \%$ on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by $22 \%$ absolute. TORACODE-34B is also the first open-source model that achieves an accuracy exceeding $50 \%$ on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research ${ }^{1}$.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparing TORA with baselines on LLaMA-2 base models from 7B to 70B. TORA models exhibit remarkable improvements over previous state-of-the-art approaches across all scales. In particular, TORA-70B notably outperforms GPT-4's CoT result on MATH and attains comparable results to GPT-4 solving problems with code.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of three reasoning formats for mathematical reasoning: (a) Rationale-based methods (e.g., CoT prompting) generate step-by-step natural language rationales, (b) Program-based methods (e.g., PAL prompting) solve tasks with program synthesis, and (c) our proposed Toolintegrated Reasoning format interleaves rationales with program-based tool use. For brevity, we present a simple example of single-round tool interaction, where the model creates rationale $r_{1}$ for analysis, writes program $a_{1}$ to call an external solver, obtains the execution output $o_{1}$, and then generates rationale $r_{2}$ to finalize the answer.</p>
<h1>1 INTRODUCTION</h1>
<p>Large language models (LLMs), such as GPT-4 <em>(OpenAI, 2023)</em> and PaLM-2 <em>(Anil et al., 2023)</em>, have demonstrated remarkable progress in a wide range of language tasks, particularly in the longstanding challenge of mathematical reasoning <em>(Feigenbaum et al., 1963; Hosseini et al., 2014)</em>. However, open-source models, such as LLaMA-2 <em>(Touvron et al., 2023a, b)</em> and Falcon <em>(Penedo et al., 2023)</em>, still struggle with advanced mathematical reasoning tasks.</p>
<p>Existing works improve mathematical performance of language models either with step-by-step natural language reasoning <em>(Wei et al., 2022)</em> as illustrated in Fig 2 (a), or by synthesizing and executing programs to obtain the answers <em>(Gao et al., 2022; Chen et al., 2022)</em>, as depicted in Fig 2 (b). Both approaches exhibit complementary advantages. Natural language is suitable for semantic analysis, planning, and abstract reasoning (e.g., commonsense reasoning), but struggles with precise computation, symbolic manipulation, and algorithmic processing. Conversely, programs excel in rigorous operations, and can outsource intricate calculations to specialized tools like equation solvers.</p>
<p>To leverage the benefits of both natural language reasoning and program-based tool use, we train open-source models such as LLaMA-2 to reason in a way where natural language reasoning is interleaved with program-based tool use synergistically (as depicted in Fig 2 (c)), thereby largely reducing the gap with closed-source models like GPT-4 in mathematical reasoning. Specifically, we first design the interleaving format of reasoning, curate corresponding interactive tool-use trajectories for mathematical problems from the popular GSM8k <em>(Cobbe et al., 2021)</em> and MATH <em>(Hendrycks et al., 2021)</em> dataset, and then apply imitation learning on the high-quality annotations, leading to a better performance than any existing open-source model. Furthermore, since the curated data is far from exhausting all valid trajectories for a problem, relying solely on imitation learning restricts a model's output space, hindering the flexibility in exploring plausible trajectories during testing. To improve the diversity of plausible reasoning steps and mitigate improper tool-use behavior, we apply output space shaping which additionally trains the models on both self-sampled valid trajectories and invalid ones that have been corrected by a teacher model (e.g., a 34B model can serve as the teacher</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training ToRA contains two steps. (1) Imitation Learning: Prompt LLMs like GPT-4 to generate Tool-integrated Reasoning trajectories (ToRA-CORPUS) and use this corpus to fine-tune a model $\mathcal{M}$; (2) Output Space Shaping: Sample diverse tool-use trajectories with $\mathcal{M}$, keep the valid ones, correct the invalid ones with a teacher model $\mathcal{M}^{\prime}$, and retrain $\mathcal{M}$ on the union of sampled valid trajectories, corrected ones, and the initial ToRA-CORPUS to obtain ToRA.
for a 7B model). Output space shaping significantly boosts reasoning, allowing open-source models to attain an accuracy exceeding $50 \%$ on the competition-level MATH dataset for the first time.</p>
<p>We evaluate the resulting suite of Tool-integrated Reasoning Agents (ToRA) ranging from 7B to 70B on 10 diverse mathematical reasoning datasets. As shown in Fig 1, ToRA series significantly outperform open-source models across all scales. Notably, on the competition-level MATH dataset, ToRA-7B outperforms the previous SoTA WizardMath-70B (Luo et al., 2023) by 22\% absolute. ToRA-Code-34B beats GPT-4's CoT result (Bubeck et al., 2023) by $8.3 \%$ absolute ( $50.8 \%$ vs. $42.5 \%$ ), and is competitive with GPT-4 solving problems with code (GPT-4-Code, 51.8\%). In addition, we analyze the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future work.</p>
<h1>2 ToRA: Tool-Integrated Agents for Mathematical Reasoning</h1>
<h3>2.1 OVERVIEW</h3>
<p>ToRA series solve challenging mathematical problems by leveraging both natural language reasoning and program-based tool use. As shown in Fig 2 (c), given a mathematical problem $q$, ToRA reasons with natural language, producing $r_{1}$. When reaching a point where program-based tool use is more appropriate for the subsequent task, e.g., equation solving, ToRA generates a program $a_{1}$ for tool use following natural language guidance $r_{1}$. The execution output $o_{1}$ will be fed to ToRA for subsequent processing including tool use adjustments, sub-tasks solving, or answer finalization. We repeat the process until the model places its answer within " $\backslash$ boxed()". The resulting trajectory is denoted as $\tau=r_{1} a_{1} o_{1} \ldots r_{n-1} a_{n-1} o_{n-1} r_{n}$, where $r_{n}$ contains the answer.</p>
<p>Fig 3 presents the training pipeline of ToRA. We first collect interactive tool-use trajectories on popular mathematical datasets. We then apply imitation learning on the resulting annotations, as well as output space shaping to further refine models' reasoning behavior.</p>
<h3>2.2 Collecting Interactive Tool-Use Trajectories</h3>
<p>Existing mathematical reasoning datasets primarily contain annotations in either natural language or code, posing a challenge for training tool-integrated agents due to the absence of interactive tool-use annotations. To address this, we utilize GPT-4 to synthesize high-quality trajectories on the GSM8k and MATH training sets. We select GSM8k and MATH as they exhibit diverse reasoning patterns, spanning multiple domains and difficulty levels.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Inference</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">Tool</span><span class="o">-</span><span class="n">Integrated</span><span class="w"> </span><span class="n">Reasoning</span>
<span class="n">Require</span><span class="p">:</span><span class="w"> </span><span class="n">problem</span><span class="w"> </span>\<span class="p">(</span><span class="n">q</span>\<span class="p">),</span><span class="w"> </span><span class="n">model</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">prompt</span><span class="w"> </span>\<span class="p">(</span><span class="n">p</span>\<span class="p">),</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="n">tools</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">),</span><span class="w"> </span><span class="n">stop</span><span class="w"> </span><span class="n">condition</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Stop</span><span class="p">}(</span><span class="n">cdot</span><span class="p">)</span>\<span class="p">),</span><span class="w"> </span><span class="n">maximum</span><span class="w"> </span><span class="n">iteration</span><span class="w"> </span><span class="n">rounds</span><span class="w"> </span>\<span class="p">(</span><span class="n">n</span>\<span class="p">)</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">tau_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Trajectory</span><span class="w"> </span><span class="n">Initialization</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">i</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="mi">1</span>\<span class="p">)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span>\<span class="p">(</span><span class="n">n</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span>\<span class="p">(</span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathbb</span><span class="p">{</span><span class="n">P</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">cdot</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">p</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">q</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Rationale</span><span class="w"> </span><span class="n">Generation</span><span class="w"> </span><span class="p">(</span><span class="n">Eq</span><span class="o">.</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">Stop</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Stopping</span><span class="w"> </span><span class="n">Criteria</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">tau_</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">        </span><span class="n">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">a_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">sim</span><span class="w"> </span>\<span class="n">mathbb</span><span class="p">{</span><span class="n">P</span><span class="p">}</span><span class="n">_</span><span class="p">{</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">G</span><span class="p">}}</span>\<span class="n">left</span><span class="p">(</span>\<span class="n">cdot</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">p</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">q</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Program</span><span class="w"> </span><span class="n">Generation</span><span class="w"> </span><span class="p">(</span><span class="n">Eq</span><span class="o">.</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">            </span>\<span class="p">(</span><span class="n">o_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">a_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Tool</span><span class="w"> </span><span class="n">Execution</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">tau_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">tau_</span><span class="p">{</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">oplus</span><span class="w"> </span><span class="n">o_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="n">triangleright</span>\<span class="p">)</span><span class="w"> </span><span class="n">Trajectory</span><span class="w"> </span><span class="n">Update</span><span class="w"> </span><span class="p">(</span><span class="n">Eq</span><span class="o">.</span><span class="w"> </span><span class="mi">3</span><span class="p">)</span>
<span class="w">    </span><span class="n">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">tau_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="p">)</span>
</code></pre></div>

<p>Prompt Curation We compose instructions along with diverse few-shot examples, utilizing an interleaved format as depicted in Fig 2 (c). These examples showcase interactive tool usage trajectories, incorporating descriptive variable names and combined program outputs. Please refer to Appendix E for the assembled prompts.</p>
<p>Inference Procedure We follow Algorithm 1 and feed GPT-4 $(\mathcal{G})$ with the composed prompt $p$ to generate a tool-use trajectory $\tau$ for each question $q$ from the training set. The trajectory is initialized as an empty string $\tau_{0}$, for each interaction round $i$, we first generate a rationale:</p>
<p>$$
r_{i} \sim \mathbb{P}<em i-1="i-1">{\mathcal{G}}\left(\cdot \mid p \oplus q \oplus \tau</em>\right)
$$</p>
<p>where $\oplus$ means concatenation. If $r_{i}$ includes an answer within " $\backslash$ boxed ( )" (i.e., the stopping condition $\operatorname{Stop}\left(r_{i}\right)$ ), we cease generation, otherwise the model continues to write a program for tool use:</p>
<p>$$
a_{i} \sim \mathbb{P}<em i-1="i-1">{\mathcal{G}}\left(\cdot \mid p \oplus q \oplus \tau</em>\right)
$$} \oplus r_{i</p>
<p>In line with Gou et al. (2023), if the model triggers the code execution stop words like " ' ' 'output", we supply it with the corresponding execution message and output $o_{i}$ by calling tools with $o_{i} \leftarrow \mathcal{E}\left(a_{i}\right)$, facilitating the generation of subsequent steps. Then, we update the trajectory by concatenating it with the newly generated rationale $r_{i}$, program $a_{i}$, and output $o_{i}$ :</p>
<p>$$
\tau_{i} \leftarrow \tau_{i-1} \oplus r_{i} \oplus a_{i} \oplus o_{i}
$$</p>
<p>We repeat the above interaction process until we reach the maximum rounds $n$.
Trajectory Sampling We set $n=3$ and perform inference using GPT-4 with greedy decoding, retaining trajectories that yield correct answers. For questions where GPT-4 fails with greedy decoding, we apply nucleus sampling with a sample size of 10 and keep up to 4 valid trajectories per question. Ultimately, we successfully annotate trajectories for $98.2 \%$ of GSM8k questions and $83.1 \%$ of MATH questions. After filtering out invalid trajectories with tool-use errors or wrong answers, we obtain 16 k annotations which constitute our dataset ToRA-CORPus. Table 1 compares ToRACORPUS with recently proposed mathematical reasoning datasets, while Table 6 in the Appendix displays MATH annotation accuracy details.</p>
<h1>2.3 TRAINING</h1>
<p>Imitation Learning We apply imitation learning on ToRA-CORPUS by minimizing negative log-likelihood loss on the trajectory $\tau$ conditioned on the problem $q$ :</p>
<p>$$
\mathcal{M}=\arg \min <em _tau="\tau" q_="q,">{\mathcal{M}} \sum</em>} \sum_{i=1}^{n-1}-\log \mathbb{P<em i_1="i+1">{\mathcal{M}}\left(r</em>\right)
$$} a_{i+1} \mid q, r_{1} \ldots o_{i</p>
<p>where $\mathcal{M}$ is the resulting model. After imitation learning, we can simply apply the same procedure in Algorithm 1 by setting prompt to empty $p=$ "" for inference. Imitation learning leads to state-of-theart mathematical reasoning performance despite the small scale of ToRA-CORPUS.</p>
<p>Table 1: Compared with mathematical reasoning datasets, ToRA-CORPUS uniquely combines natural language rationales with program-based tool usage. Note that ToRA-CORPUS only employ questions from the original training set of MATH and GSM8k.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">#Annotation</th>
<th style="text-align: center;">Tool</th>
<th style="text-align: center;">Interleaving</th>
<th style="text-align: center;">LLM Used</th>
<th style="text-align: center;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RFT (Yuan et al., 2023)</td>
<td style="text-align: center;">$&gt;100 \mathrm{k}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">GSM8k</td>
</tr>
<tr>
<td style="text-align: left;">Open-Platypus Lee et al. (2023)</td>
<td style="text-align: center;">25 k</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">11 datasets with MATH</td>
</tr>
<tr>
<td style="text-align: left;">WizardMath (Luo et al., 2023)</td>
<td style="text-align: center;">$&gt;96 \mathrm{k}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">MATH \&amp; GSM8k</td>
</tr>
<tr>
<td style="text-align: left;">Lila (Mishra et al., 2022)</td>
<td style="text-align: center;">134 k</td>
<td style="text-align: center;">$\checkmark(\mathrm{PoT})$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20 datasets with MATH \&amp; GSM8k</td>
</tr>
<tr>
<td style="text-align: left;">MathInstruct (Yue et al., 2023)</td>
<td style="text-align: center;">260 k</td>
<td style="text-align: center;">$\checkmark(\mathrm{PoT})$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">14 datasets with MATH \&amp; GSM8k</td>
</tr>
<tr>
<td style="text-align: left;">ToRA-CORPUS (ours)</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">MATH \&amp; GSM8k</td>
</tr>
</tbody>
</table>
<p>Output Space Shaping For each question, ToRA-CORPUS mostly demonstrates only one valid interactive tool-use trajectory, which may restrict a model's output space, rendering it inflexible in exploring plausible trajectories during testing. We therefore propose output space shaping in order to encourage the diversity of plausible reasoning steps and reduce improper tool-use behavior.
To explore diverse valid trajectories, we apply nucleus sampling to imitation learning models $\mathcal{M}$ to sample 64 trajectories per training question $q$, following the inference procedure in Section 2.2. We retain valid trajectories with correct answers and no tool-use errors. As many samples are duplicates, to further improve diversity and in an attempt to correct models' improper behavior, we seek to leverage invalid trajectories as well. We observe that trajectories with wrong answers are mostly incorrect halfway (Li et al., 2023), and the preceding reasoning is still plausible; in other words, we can obtain valid trajectories by correcting the subsequent portions. Specifically, a wrong trajectory $\widetilde{\tau}$, when written in text, can be represented as a sequence of lines separated by line breaks, i.e., $\widetilde{\tau}=l_{1} \ldots l_{m}$, where $m$ is the total number of lines in $\widetilde{\tau}$. We enumerate possible preceding portions of wrong trajectories, i.e., $\widetilde{\tau}[: j]=l_{1} \ldots l_{j}$, and leverage a teacher model $\mathcal{M}^{\prime}$ to complete the subsequent steps with greedy decoding: $\tau \leftarrow \mathbb{P}<em _mathcal_M="\mathcal{M">{\mathcal{M}^{\prime}}(\cdot|q \oplus \widetilde{\tau}[: j])$ where we abuse the notation $\mathbb{P}</em>(\cdot)$ to denote the interactive tool use process following Section 2.2. Finally, corrected trajectories as well as valid trajectory samples will be used for model training, thereby shaping the output space.
In our experiments, we always use CodeLLaMA-34B trained on ToRA-CORPUS as the teacher model, and apply sampling with the CodeLLaMA series (ranging from 7B to 34B, with imitation learning). We obtain a total of 233 k distinct valid trajectory samples and 69 k corrected ones. From this combined dataset, we randomly select up to 4 trajectories per GSM8k and MATH problem, merge them with ToRA-CORPUS, and then train all ToRA models on the resulting 69k annotations.}^{\prime}</p>
<h1>3 EXPERIMENTS</h1>
<h3>3.1 IMPLEMENTATION DETAILS</h3>
<p>We fine-tuned LLaMA-2 (Touvron et al., 2023b) and CodeLLaMA (Rozière et al., 2023) series (ranging from 7B to 70B) using ToRA-CORPUS with output space shaping, yielding the ToRA and ToRA-CODE series respectively. We used a learning rate of $2 \mathrm{e}-5$ by default except that we used 1e-5 for the 34B and 70B models. We set the global batch size to 128 and used a linear scheduler with a $3 \%$ warm-up period for 3 epochs. We trained all models with DeepSpeed ZeRO Stage3 (Rajbhandari et al., 2021) and Flash-Attention 2 (Dao, 2023). We used greedy decoding for all results, with the maximum sequence length set to 2,048 and the maximum number of tool executions set to 3 .</p>
<h3>3.2 Evaluation Setup</h3>
<p>Datasets We evaluated models on GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), along with 8 out-of-distribution datasets, namely GSM-Hard (Gao et al., 2022), SVAMP (Patel et al., 2021), ASDIV (Miao et al., 2020), TabMWP (Lu et al., 2023), SingleEQ, SingleOP, AddSub, and MultiArith (Koncel-Kedziorski et al., 2016), as illustrated in Table 5 in Appendix. The 10 assorted datasets collectively encompass mathematical problems spanning basic arithmetic to competition level, covering middle and high school curricula and various mathematical domains. The problem formats comprise tabular-based, free-form, and multiple-choice questions, ensuring a thorough assessment of the model's mathematical reasoning aptitude.</p>
<p>Table 2: Results on 10 mathematical reasoning tasks. MAWPS results are averaged over four tasks: Singleeq, Singleop, Addsub, and MultArith. Vanilla models are tested with CoT. The best results in each section are in blue, the second-best results are underlined, while the results of our best model are bolded. ${ }^{*}$ ZS: Zero-shot inference without demonstrations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Tools</th>
<th style="text-align: center;">ZS $^{\text {a }}$</th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">GSM-Hard</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">TabMWP</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;">AVG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Used for training?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proprietary Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">67.1</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">78.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (PAL) 4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">72.3</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (PAL) 4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: center;">Claude-2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2</td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">29.4</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 SFT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 RFT</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Platypus-2</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">28.3</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: center;">CodeLLaMA (PAL) 4</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">47.4</td>
</tr>
<tr>
<td style="text-align: center;">Toolformer ${ }^{2}$ 5</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ToRA 6</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-CODE 7</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">66.5 (+19)</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">39.5</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 SFT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">42.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 RFT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Platypus-2</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">38.0</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">CodeLLaMA (PAL) 4</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">62.4</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">65.3</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">53.1</td>
</tr>
<tr>
<td style="text-align: center;">ToRA 6</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">65.9</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-CODE 7</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">71.3 (+18)</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-1 RFT</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CodeLLaMA (PAL) 4</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">60.7</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-CODE 8</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">74.8 (+14)</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 SFT</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">56.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 RFT</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Platypus-2</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">53.0</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">63.8</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-2 (PAL) 4</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">60.3</td>
</tr>
<tr>
<td style="text-align: center;">ToRA 6</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">76.9 (+13)</td>
</tr>
</tbody>
</table>
<p>Metrics We report accuracies of predicted answers. Following Lightman et al. (2023), we round numerical values and use sympy ${ }^{2}$ for parsing expressions. Since the SingleEQ, SingleOP, AddSub, and MultiArith datasets focus on different aspects of basic arithmetic, we report their average results under the collective term MAWPS (Koncel-Kedziorski et al., 2016) for all methods.</p>
<h1>3.3 BASELINES</h1>
<p>Proprietary Models We present results from an array of SoTA LLMs, such as OpenAI's GPT-4, ChatGPT (gpt-3.5-turbo), Google's PaLM-2, and Anthropic's Claude-2. By default, we report CoT prompting results, and include PAL (Gao et al., 2022) prompting results for selected models.</p>
<p>Open-Source Models Base models comprise LLaMA-2 and CodeLLaMA with CoT and PAL prompting. Supervised Fine-Tuning (SFT) employs CoT rationales from the original GSM8k and MATH dataset ( 15 k samples) for fine-tuning. Rejection sampling Fine-Tuning (RFT) leverages multiple models to generate diverse reasoning paths for fine-tuning (Yuan et al., 2023). WizardMath augments data using ChatGPT, and conducts SFT and RLHF. Platypus-2, the top model on the LLM Leaderboard ${ }^{3}$, is fine-tuned with Open-Platypus reasoning datasets (Lee et al., 2023). We also compare ToRA with Toolformer (Schick et al., 2023) which is a model trained to utilize calculators.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Results on MATH subtopics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Tool</th>
<th style="text-align: center;">Intermediate <br> Algebra</th>
<th style="text-align: center;">Precalculus</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Number <br> Theory</th>
<th style="text-align: center;">Counting \&amp; <br> Probability</th>
<th style="text-align: center;">Prealgebra</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Proprietary Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (PAL) $\Leftrightarrow$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (PAL) $\Leftrightarrow$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">Open-Source Models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\mathcal{B}$</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">11.2</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-Code (1)</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.1 ( +28.9 )</td>
<td style="text-align: center;">31.0 ( +25.0 )</td>
<td style="text-align: center;">24.0 ( +17.5 )</td>
<td style="text-align: center;">50.7 ( +43.1 )</td>
<td style="text-align: center;">30.6 ( +21.1 )</td>
<td style="text-align: center;">55.0 ( +36.9 )</td>
<td style="text-align: center;">61.7 ( +45.4 )</td>
<td style="text-align: center;">44.6 ( +33.4 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Shaping</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">29.7 ( -5.4 )</td>
<td style="text-align: center;">25.1 ( -5.9 )</td>
<td style="text-align: center;">17.7 ( -6.3 )</td>
<td style="text-align: center;">46.9 ( -3.8 )</td>
<td style="text-align: center;">32.3 ( +1.7 )</td>
<td style="text-align: center;">51.9 ( -3.1 )</td>
<td style="text-align: center;">55.7 ( -6.0 )</td>
<td style="text-align: center;">40.2 ( -4.4 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Rationale</td>
<td style="text-align: center;">7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">25.5 ( -9.6 )</td>
<td style="text-align: center;">14.7 ( -16.3 )</td>
<td style="text-align: center;">15.4 ( -8.6 )</td>
<td style="text-align: center;">45.9 ( -4.8 )</td>
<td style="text-align: center;">29.7 ( -0.9 )</td>
<td style="text-align: center;">51.0 ( -4.0 )</td>
<td style="text-align: center;">52.4 ( -9.3 )</td>
<td style="text-align: center;">36.8 ( -7.8 )</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\mathcal{B}$</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">6.6</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-Code (2)</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.7 ( +29.3 )</td>
<td style="text-align: center;">31.1 ( +24.5 )</td>
<td style="text-align: center;">25.7 ( +14.2 )</td>
<td style="text-align: center;">55.6 ( +46.0 )</td>
<td style="text-align: center;">39.5 ( +28.5 )</td>
<td style="text-align: center;">58.7 ( +30.2 )</td>
<td style="text-align: center;">66.7 ( +45.6 )</td>
<td style="text-align: center;">48.1 ( +33.1 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Shaping</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">32.8 ( -2.9 )</td>
<td style="text-align: center;">26.0 ( -5.1 )</td>
<td style="text-align: center;">24.0 ( -1.7 )</td>
<td style="text-align: center;">52.6 ( -3.0 )</td>
<td style="text-align: center;">38.4 ( -1.1 )</td>
<td style="text-align: center;">55.6 ( -3.1 )</td>
<td style="text-align: center;">61.2 ( -5.5 )</td>
<td style="text-align: center;">44.6 ( -3.5 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Rationale</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">27.1 ( -8.6 )</td>
<td style="text-align: center;">15.8 ( -15.3 )</td>
<td style="text-align: center;">16.3 ( -9.4 )</td>
<td style="text-align: center;">50.4 ( -5.2 )</td>
<td style="text-align: center;">36.9 ( -2.6 )</td>
<td style="text-align: center;">55.3 ( -3.4 )</td>
<td style="text-align: center;">56.5 ( -10.2 )</td>
<td style="text-align: center;">40.2 ( -7.9 )</td>
</tr>
<tr>
<td style="text-align: center;">ToRA-Code (3)</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">50.8</td>
</tr>
<tr>
<td style="text-align: center;">w/o Shaping</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">34.0 ( -4.9 )</td>
<td style="text-align: center;">29.9 ( -4.7 )</td>
<td style="text-align: center;">24.6 ( -2.7 )</td>
<td style="text-align: center;">55.6 ( -2.2 )</td>
<td style="text-align: center;">41.6 ( +0.2 )</td>
<td style="text-align: center;">63.8 ( +0.1 )</td>
<td style="text-align: center;">61.4 ( -6.3 )</td>
<td style="text-align: center;">47.4 ( -3.4 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Rationale</td>
<td style="text-align: center;">34B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">28.3 ( -10.6 )</td>
<td style="text-align: center;">15.8 ( -18.8 )</td>
<td style="text-align: center;">18.0 ( -9.3 )</td>
<td style="text-align: center;">52.4 ( -5.4 )</td>
<td style="text-align: center;">40.7 ( -0.7 )</td>
<td style="text-align: center;">58.6 ( -5.1 )</td>
<td style="text-align: center;">57.5 ( -10.2 )</td>
<td style="text-align: center;">41.9 ( -8.9 )</td>
</tr>
<tr>
<td style="text-align: center;">WizardMath</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\mathcal{B}$</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">24.1</td>
</tr>
<tr>
<td style="text-align: center;">ToRA (4)</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">37.1 ( +28 )</td>
<td style="text-align: center;">30.4 ( +17 )</td>
<td style="text-align: center;">30.1 ( +13.2 )</td>
<td style="text-align: center;">54.6 ( +38.1 )</td>
<td style="text-align: center;">40.3 ( +21.1 )</td>
<td style="text-align: center;">64.9 ( +22.2 )</td>
<td style="text-align: center;">66.6 ( +31.6 )</td>
<td style="text-align: center;">49.7 ( +25.6 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Shaping</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">33.8 ( -3.3 )</td>
<td style="text-align: center;">28.9 ( -1.5 )</td>
<td style="text-align: center;">27.1 ( -3 )</td>
<td style="text-align: center;">53.0 ( -1.6 )</td>
<td style="text-align: center;">38.0 ( -2.3 )</td>
<td style="text-align: center;">62.2 ( -2.7 )</td>
<td style="text-align: center;">64.2 ( -2.4 )</td>
<td style="text-align: center;">47.3 ( -2.4 )</td>
</tr>
<tr>
<td style="text-align: center;">w/o Rationale</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">26.7 ( -10.4 )</td>
<td style="text-align: center;">14.7 ( -15.7 )</td>
<td style="text-align: center;">20.3 ( -9.8 )</td>
<td style="text-align: center;">48.9 ( -5.7 )</td>
<td style="text-align: center;">39.2 ( -1.1 )</td>
<td style="text-align: center;">59.8 ( -5.1 )</td>
<td style="text-align: center;">57.6 ( -9 )</td>
<td style="text-align: center;">41.5 ( -8.2 )</td>
</tr>
</tbody>
</table>
<h1>3.4 Main ReSults</h1>
<p>Table 2 presents the results of ToRA on 10 mathematical datasets, highlighting the following salient observations: (1) Using interleaved formatting and output space shaping, ToRA consistently surpasses prior state-of-the-art open-source models across all scales, achieving $13 \%$ to $19 \%$ absolute improvements across 10 tasks. (2) ToRA-70B substantially outperforms ChatGPT with both CoT and PAL prompting on GSM8k ( $84.3 \%$ vs. $80.4 \%$ ) and MATH ( $49.7 \%$ vs. $38.7 \%$ ), while ToRA-CODE-34B is competitive with GPT-4 solving competition-level MATH dataset with code ( $50.8 \%$ vs. $51.8 \%$ ). (3) The accuracy of ToRA-CODE is about $5 \%$ higher than ToRA of the same size, demonstrating that continued training on code data significantly benefits program-based tool use. (4) While rationale-based fine-tuning negatively affects out-of-distribution generalization, ToRA displays superior generalization. For instance, WizardMath-70B underperforms the base model on TabMWP ( $49.8 \%$ vs. $57.5 \%$ ), while ToRA-70B effectively generalizes to this tabular reasoning task (74.0\%). (5) ToRA attains fast zero-shot inference speed, averaging 1.02 tool interaction rounds per problem, while effectively addressing problems that require interactive tool utilization.</p>
<h3>3.5 Ablation Study</h3>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Comparison of three formats: (1) Rationale-only: step-by-step natural language reasoning like CoT; (2) Program-only: solving problems with programs like PAL; (3) Tool-integrated Reasoning used by ToRA: interweaving rationale and program execution to solve problems. We evaluated GPT-4 with few-shot prompting. We trained LLaMA-2 models to reason in the three types of formats, respectively. For a fair comparison, we do not apply output space shaping for all LLaMA-2 models.</p>
<h1>3.5.1 Comparisons of Formatting</h1>
<p>To evaluate the efficacy of the reasoning format adopted by ToRA which interleaves rationales with programs, we compared it with Rationale-only and Program-only formats using GPT-4 and LLaMA-2 trained with the same size of data from MATH. As shown in Fig 4, the ToRA method consistently surpasses Rationale-only and Program-only approaches. Remarkably, using LLaMA-2, the ToRA method achieves substantial improvements of $29.0 \%$ and $6.7 \%$ over Rationale-only and Program-only, respectively. With the closed-source GPT-4, the improvements are $19.1 \%$ and $9.8 \%$, respectively. This emphasizes the effectiveness of integrating natural language rationales with programs.</p>
<h3>3.5.2 Effects of Output Space Shaping</h3>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ablation on output space shaping strategies using CodeLLaMA: (1) ToRA ${ }<em _-Correction="{-Correction" _text="\text">{\text {-Corrected }}$ is trained on ToRA-CORPUS without shaping. (2) ToRA ${ }</em>$ employs only the sampling strategy for shaping, trained with up to 4 additional valid trajectory samples per problem. (3) TORA utilizes both the sampling and correction, also trained with up to 4 additional trajectories per problem.}</p>
<p>We assess the effectiveness of the output space shaping strategies presented in Section 2.3, specifically sampling and correction. As shown in Fig 5 and Table 3: (1) Output space shaping yields a considerable average improvement of $3.4 \%$ and $4.0 \%$ absolute for GSM8k and MATH, respectively, with greater benefits for smaller models; (2) Applying the sampling strategy results in a $2.7 \%$ absolute improvement on average, while additionally incorporating correction offers a more substantial boost of up to $4.5 \%$, without using more training data; (3) Output space shaping benefits even the largest model ToRA-70B, with a notable improvement from $47.3 \%$ to $49.7 \%$ on MATH. These findings highlight the effectiveness of our shaping strategies across different model sizes and datasets.</p>
<h3>3.6 ANALYSIS</h3>
<p>We investigate the benefits, detailed patterns, and remaining challenges of tool interaction for mathematical reasoning on the challenging MATH dataset. Performance breakdowns on all subtopics of MATH are reported in Table 3.</p>
<p>Benefits from Tool-Integration for MATH Sub-topics As shown in Table 3, ToRA outperforms WizardMath by around $45 \%$ in Algebra and Number Theory, which is attributed to stimulating and shaping tool-use behavior. Problems from the two sub-topics typically need intricate computation and data manipulation. Algebra mainly focuses on solving equations and application problems, while many Number Theory problems can be tackled using brute-force approaches through code.
Patterns of Library Usage for Problem Solving Fig 6 presents the most frequently used libraries for different sub-topics and the corresponding accuracies of their solutions. Tool-use behavior on different mathematical areas demonstrates distinct patterns. sympy and its internal solvers are primarily employed for algebra-related topics. Precalculus exhibits extensive matrix operations via matrices, resulting in a high accuracy. Number Theory depends on algorithms like gcd and lcm. Geometry mainly uses the rational library for fraction-based computations, while the application of other tools is limited, signifying the potential for improvement.
Detailed Impact of Rationale on Different Topics Table 3 shows that using an interleaved format, in contrast to merely writing the program, leads to significant improvements across all subtopics, especially in Precalculus, Algebra, and Geometry, where notable increases range from $8.6 \%$ to $18.8 \%$.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Library usage frequency and accuracy on each sub-topic of MATH.</p>
<p>Appendix F. 1 provides representative examples demonstrating how the rationale aids in planning, multi-round self-correction, and finalizing answers.</p>
<p>Table 4: The failure modes of the ToRA on MATH, and their corresponding percentages in random samples analyzed by humans. We include specific examples of each failure mode in Appendix F.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Error Type</th>
<th style="text-align: left;">Definition</th>
<th style="text-align: center;">\%</th>
<th style="text-align: center;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Reasoning Error</td>
<td style="text-align: left;">Mistakes due to incorrect reasoning steps or missing conditions.</td>
<td style="text-align: center;">$38 \%$</td>
<td style="text-align: center;">Ex. 5</td>
</tr>
<tr>
<td style="text-align: left;">Hallucination</td>
<td style="text-align: left;">Fabrication of numbers or answers.</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">Ex. 6</td>
</tr>
<tr>
<td style="text-align: left;">Diagram Understanding</td>
<td style="text-align: left;">Misinterpretation of the input diagram.</td>
<td style="text-align: center;">$21 \%$</td>
<td style="text-align: center;">Ex. 7</td>
</tr>
<tr>
<td style="text-align: left;">Inappropriate Tool Use</td>
<td style="text-align: left;">Incorrect use of external tools, especially when <br> the problem can't be solved directly with libraries.</td>
<td style="text-align: center;">$10 \%$</td>
<td style="text-align: center;">Ex. 8</td>
</tr>
<tr>
<td style="text-align: left;">Syntax Error</td>
<td style="text-align: left;">Persistent syntax errors despite multiple correction attempts.</td>
<td style="text-align: center;">$9 \%$</td>
<td style="text-align: center;">Ex. 9</td>
</tr>
<tr>
<td style="text-align: left;">Runtime Error</td>
<td style="text-align: left;">Errors during program execution, unresolved by retrying.</td>
<td style="text-align: center;">$9 \%$</td>
<td style="text-align: center;">Ex. 10</td>
</tr>
<tr>
<td style="text-align: left;">Rationale-only Error</td>
<td style="text-align: left;">Cannot be formalized into a program and the rationale is incorrect.</td>
<td style="text-align: center;">$3 \%$</td>
<td style="text-align: center;">Ex. 11</td>
</tr>
<tr>
<td style="text-align: left;">False Negative</td>
<td style="text-align: left;">Correct answers that don't fully match the ground truth.</td>
<td style="text-align: center;">$5 \%$</td>
<td style="text-align: center;">Ex. 12</td>
</tr>
</tbody>
</table>
<p>Remaining Challenges in Mathematical Reasoning for ToRA To better understand the failure modes and remaining challenges, we manually annotated 100 randomly selected trajectories from the MATH test set, identifying and categorizing their failure modes. The results are shown in Table 4: Primarily, incorrect reasoning steps constitute the primary source of errors for ToRA on complex math reasoning tasks ( $38 \%$ ), with some hallucination issues also evident during problem interpretation and answer finalization (5\%). Secondly, the misinterpretation of input diagrams contributes significantly to the error rate ( $21 \%$ ). This is particularly noticeable in Geometry, Precalculus, and Intermediate Algebra. The diagrams in the MATH dataset are usually detailed in text using the Asymptote language (Hendrycks et al., 2021), thus making it challenging for ToRA to comprehend diagrams purely from textual descriptions. Thirdly, issues with tool usage include Inappropriate Tool Usage (10\%), Syntax Error (9\%), and Runtime Error (9\%). These problems frequently arise when ToRA fails to use tools correctly after several corrections or attempts. There are certain inputs that fail to formalize well as programs (3\%), which require abstract reasoning rather than computation. Finally, we also found that there are false negatives when using automatic indicators, i.e., correct predictions that are misjudged as wrong, but the proportion is relatively small (5\%).</p>
<h1>4 CONCLUSION</h1>
<p>This paper presents ToRA, a series of novel Tool-integrated Reasoning Agents that synergistically combines natural language rationale with program-based tool-use for mathematical problem solving. Our approach demonstrates the potential of integrating external tools in the reasoning process, enabling language models to effectively tackle complex quantitative tasks. ToRA achieves state-of-the-art performance on 10 diverse mathematical reasoning tasks, substantially outperforming existing rationale-based and program-based approaches. Furthermore, our systematic analysis of the benefits and remaining challenges of tool interaction provides valuable insights for future research, contributing to the development of more advanced and versatile reasoning agents.</p>
<h1>Author Contributions</h1>
<p>Zhibin Gou proposed the interleaved tool-use format of ToRA and curated ToRA-CORPUS dataset, implemented the training and evaluation pipeline, conducted experiments and analysis on all datasets, implemented baselines, and was a main contributor to the paper writing. Zhihong Shao proposed the project, conducted preliminary experiments, proposed and implemented the training and evaluation pipelines, proposed and trained all ToRA models with output space shaping as well as ToRA variants in the ablation study, designed and oversaw experimental analysis, and contributed to many parts of the paper writing. Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen provided research mentorship, oversaw project coordination, and advised and contributed to many parts of the writing.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>Zhibin Gou and Yujiu Yang were supported by the National Natural Science Foundation of China (Grant No. U1903213) and the Shenzhen Science and Technology Program (JSGG20220831110203007). Zhihong Shao and Minlie Huang were supported by the NSFC projects (Key project with No. 61936010 ), and were also supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604).</p>
<h1>REFERENCES</h1>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pp. 2206-2240. PMLR, 2022.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Túlio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. doi: 10.48550/arXiv.2303.12712. URL https://doi.org/10. 48550/arXiv.2303.12712.</p>
<p>Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. $535-541,2006$.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168.</p>
<p>Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
Edward A Feigenbaum, Julian Feldman, et al. Computers and thought, volume 7. New York McGraw-Hill, 1963.</p>
<p>Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 10421-10430. PMLR, 2023. URL https://proceedings.mlr.press/v202/fu23d.html.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.</p>
<p>Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.</p>
<p>Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14852-14882, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.830. URL https://aclanthology.org/ 2023.acl-long. 830 .</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, 2014.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. CoRR, abs/2210.11610, 2022. doi: 10.48550/arXiv. 2210. 11610. URL https://doi.org/10.48550/arXiv.2210.11610.</p>
<p>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T Kwok. Backward reasoning in large language models for verification. arXiv preprint arXiv:2308.07758, 2023.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136.</p>
<p>Ariel N Lee, Cole J Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. arXiv preprint arXiv:2308.07317, 2023.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-5333, 2023.</p>
<p>Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DHyHRBwJUTN.</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.92. URL https://aclanthology.org/ 2020.acl-main. 92.</p>
<p>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main. 168. URL https://aclanthology.org/2021.naacl-main. 168.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/2306.01116.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020.</p>
<p>Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-14, 2021.</p>
<p>Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>Zhihong Shao, Fei Huang, and Minlie Huang. Chaining simultaneous thoughts for numerical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 2533-2547. Association for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-emnlp.187. URL https://doi.org/10.18653/v1/2022. findings-emnlp. 187 .</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 30706-30775. PMLR, 2023a. URL https://proceedings.mlr.press/v202/shao23a.html.</p>
<p>Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. CoRR, abs/2305.15294, 2023b. doi: 10.48550/arXiv.2305.15294. URL https://doi.org/10. 48550/arXiv. 2305.15294.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/ 10.48550/arXiv.2307.09288.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=_VjQlMeSB_J.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X.</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022.</p>
<p>Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji-Rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. arXiv preprint arXiv:2306.02408, 2023.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL https://openreview.net/pdf?id=WZH7099tgfM.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. Solving math word problems via cooperative reasoning induced language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4471-4485, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.245. URL https://aclanthology.org/ 2023.acl-long. 245 .</p>
<h1>CONTENTS</h1>
<p>A Related Works ..... 15
B Evaluation Datasets ..... 15
C Additional Experiments and Analysis ..... 17
C. 1 Accuracies of Closed-Source Models on MATH ..... 17
C. 2 Effects of # Valid Trajectories for Output Space Shaping ..... 17
C. 3 Impact of Output Space Shaping in Relation to Question Difficulty ..... 17
D Detailed Information of ToRA-CORPUS ..... 18
E Prompts ..... 20
F Examples ..... 22
F. 1 Success Cases ..... 22
F. 2 Failure Cases ..... 24</p>
<h2>A Related Works</h2>
<p>Mathematical Reasoning Recent research has greatly improved reasoning in LLMs with step-by-step natural language reasoning (Polu \&amp; Sutskever, 2020; Wei et al., 2022; Zhou et al., 2023b; Zhu et al., 2023; Huang et al., 2022; Liang et al., 2023). However, natural language reasoning struggles with complex computations and symbolic manipulations. To overcome the limitations, recent research has exploited tools like calculators (Cobbe et al., 2021; Shao et al., 2022), code interpreters (Mishra et al., 2022), and symbolic solvers (Zhang et al., 2023). Program-based methods (Gao et al., 2022; Chen et al., 2022; Shao et al., 2023a) transform reasoning tasks into program synthesis tasks, thus offering complementary advantages over natural language reasoning, but they face challenges in nuanced reasoning, planning, and error handling (Gou et al., 2023), where natural language reasoning should be more suitable.</p>
<p>Tool-Augmented Language Models Augmenting LLMs with tools can largely alleviate LLMs' limitations and improve reasoning and generation performance (Parisi et al., 2022; Mialon et al., 2023; Yao et al., 2023). Recent work demonstrates the benefits of integrating retrievers (Borgeaud et al., 2022; Shao et al., 2023b), search engines (Nakano et al., 2021), and multi-tool approaches (Schick et al., 2023; Paranjape et al., 2023; Gou et al., 2023) to improve generation.</p>
<p>Knowledge Distillation Knowledge distillation (KD) transfers knowledge from teacher models to student models (Buciluǎ et al., 2006; Hinton et al., 2015). Using LLM-generated trajectories for fine-tuning is a form of KD (Fu et al., 2023; Taori et al., 2023; Peng et al., 2023; Ho et al., 2023). Our proposed ToRA shows that learning interactive tool-use trajectories is a promising direction to adapt language models to reasoning tasks.</p>
<h2>B Evaluation Datasets</h2>
<p>We present statistics and examples of the ten evaluation datasets in Table 5.</p>
<p>Table 5: Statistics and examples of the 10 evaluation datasets. In the main result table, we present the average accuracy of SingleEq, SingleOp, AddSub, and MultiArith under the collective name MAWPS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">OOD?</th>
<th style="text-align: center;">#Samples</th>
<th style="text-align: center;">Example Problem</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GSM8k (Cobbe et al., 2021)</td>
<td style="text-align: center;">IND</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">The ice cream parlor was offering a deal, buy 2 scoops of ice cream, get 1 scoop free. Each scoop cost $\$ 1.50$. If Erin had $\$ 6.00$, how many scoops of ice cream should she buy?</td>
</tr>
<tr>
<td style="text-align: center;">MATH (Hendrycks et al., 2021)</td>
<td style="text-align: center;">IND</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">For a constant $c$, in cylindrical coordinates $(r, \theta, z)$, find the shape described by the equation $z=c$. <br> (A) Line (B) Circle (C) Plane (D) Sphere (E) Cylinder (F) Cone. Enter the letter of the correct option.</td>
</tr>
<tr>
<td style="text-align: center;">GSM-Hard (Gao et al., 2022)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Jean has 30 lollipops. Jean eats 8714250 of the lollipops. With the remaining lollipops, Jean wants to package 8714250 lollipops in one bag. How many bags can Jean fill?</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { SVAMP } \quad \text { (Patel } \ &amp; \text { et al., 2021) } \end{aligned}$</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">During summer break 819058 kids from Lawrence county go to camp and the other 668278 kids stay home. How many more kids spent their summer break at the camp compared to those who stayed home?</td>
</tr>
<tr>
<td style="text-align: center;">ASDiv (Miao et al., 2020)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">2215</td>
<td style="text-align: center;">Mrs. Hilt saw an iPod for sale. The price tag said the iPod cost $\$ 128$, but a sign announced that it was on sale for " $35 \%$ off." How much would the iPod cost after the discount?</td>
</tr>
<tr>
<td style="text-align: center;">TabMWP (Lu et al., 2023)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">Stem Leaf</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2 $3,6,7,8,8$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">3 $0,7,9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">4 1,5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">5 2,3,3,4,8,8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">7 $3,4,4,7,9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">8 5,5</td>
</tr>
<tr>
<td style="text-align: center;">SingleEq (KoncelKedziorski et al., 2016)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">508</td>
<td style="text-align: center;">Alyssa's dog had puppies. She gave 7 to her friends.She now has 5 puppies left. How many puppies did she have to start with?</td>
</tr>
<tr>
<td style="text-align: center;">SingleOp (KoncelKedziorski et al., 2016)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">562</td>
<td style="text-align: center;">Rachel removes 47 bottle caps from a jar. There were originally 87 bottle caps in the jar. How many bottle caps are left in the jar?</td>
</tr>
<tr>
<td style="text-align: center;">AddSub (KoncelKedziorski et al., 2016)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">Sam went to 14 football games this year. He went to 29 games last year. How many football games did Sam go to in all?</td>
</tr>
<tr>
<td style="text-align: center;">MultArith (KoncelKedziorski et al., 2016)</td>
<td style="text-align: center;">OOD</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">Paige had 43 math problems and 12 science problems for homework. If she finished 44 of the problems at school, how many problems did she have to do for homework?</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracies of ChatGPT and GPT-4 on the MATH dataset, with breakdown w.r.t. different mathematical subjects. We apply PAL prompting and the Tool-integrated Reasoning method used by ToRA to the two closed-source models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Tool</th>
<th style="text-align: center;">Intermediate <br> Algebra</th>
<th style="text-align: center;">Precalculus</th>
<th style="text-align: center;">Geometry</th>
<th style="text-align: center;">Number <br> Theory</th>
<th style="text-align: center;">Counting \&amp; <br> Probability</th>
<th style="text-align: center;">Prealgebra</th>
<th style="text-align: center;">Algebra</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Test Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT (PAL) 4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">19.2</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (PAL) 4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">51.8</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Tool-integrated Reasoning)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">75.8</td>
<td style="text-align: center;">61.6</td>
</tr>
<tr>
<td style="text-align: center;">Training Set</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Tool-integrated Reasoning) w/ best@10</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">64.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">83.1</td>
</tr>
</tbody>
</table>
<h1>C ADDITIONAL EXPERIMENTS AND ANALYSIS</h1>
<h2>C. 1 Accuracies of Closed-Source Models on MATH</h2>
<p>Table 6 presents the detailed accuracies of GPT-4 on the MATH dataset. The Tool-integrated Reasoning method used by ToRA significantly outperforms PAL prompting when directly applied to the closed-source GPT-4, further demonstrating the benefits of synergizing natural language reasoning and program-based tool use.</p>
<h2>C. 2 Effects of # Valid Trajectories for Output Space Shaping</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Effects of using different numbers of additional valid trajectories per question for output space shaping.</p>
<p>As shown in Fig 7, it is beneficial to increase the number of additional valid trajectories for output space shaping.</p>
<h2>C. 3 IMPACT OF OUTPUT SPACE SHAPING IN RELATION TO QUESTION DifficultY</h2>
<p>We compare the effects of output space shaping on MATH problems of different difficulty levels (from level 1 to level 5) in Figure 8, and present the statistics of MATH problems at different levels in Table 7. As can be seen:</p>
<ul>
<li>Across these different difficulty levels and model sizes, output space shaping generally brings a significant improvement of $4.0 \%$ on average across different model sizes.</li>
<li>Output space shaping brings significant improvements for difficult, long problems. E.g., with ToRA-CODE-13B, shaping does not significantly improve level 1 to level 2 problems, but it brings a substantial improvement of $5.4 \%$ to $5.7 \%$ for level 3 to level 5 problems.</li>
</ul>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Impact of Output Space Shaping in Relation to Question Difficulty.</p>
<ul>
<li>After using shaping, ToRA-CODE-34B outperforms GPT-4 PAL on problems from Level 1 to Level 4, but there is still a gap at Level $5(27.3 \%$ vs. $30.0 \%)$. These problems are usually longer (average about 248.4 characters), require more reasoning steps ( $&gt;1,000$ characters) to solve, and more often include diagram inputs (about 20\%). These observations may guide future work to focus more on solving these more difficult problems.</li>
</ul>
<p>Table 7: Statistics of MATH problems at different levels. Average Answer Length indicates the average length of ToRA outputs; Training query coverage indicates the proportion of queries with at least one valid trajectory in ToRA-CORPUS relative to the total queries in the original dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Level 1</th>
<th style="text-align: right;">Level 2</th>
<th style="text-align: right;">Level 3</th>
<th style="text-align: right;">Level 4</th>
<th style="text-align: right;">Level 5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Test Samples</td>
<td style="text-align: right;">437</td>
<td style="text-align: right;">894</td>
<td style="text-align: right;">1131</td>
<td style="text-align: right;">1214</td>
<td style="text-align: right;">1324</td>
</tr>
<tr>
<td style="text-align: left;">Avg Question Length</td>
<td style="text-align: right;">123.8</td>
<td style="text-align: right;">150.9</td>
<td style="text-align: right;">169.1</td>
<td style="text-align: right;">203.0</td>
<td style="text-align: right;">248.4</td>
</tr>
<tr>
<td style="text-align: left;">Avg Answer Length</td>
<td style="text-align: right;">503.1</td>
<td style="text-align: right;">655.8</td>
<td style="text-align: right;">751.2</td>
<td style="text-align: right;">881.6</td>
<td style="text-align: right;">1083.8</td>
</tr>
<tr>
<td style="text-align: left;">Training query coverage</td>
<td style="text-align: right;">$97.7 \%$</td>
<td style="text-align: right;">$91.6 \%$</td>
<td style="text-align: right;">$86.5 \%$</td>
<td style="text-align: right;">$81.3 \%$</td>
<td style="text-align: right;">$68.0 \%$</td>
</tr>
</tbody>
</table>
<h1>D Detailed Information of ToRA-CORPUS</h1>
<p>We provide a more detailed introduction to the data construction process, quality control, and data statistical information, beyond Sec. 2.2.</p>
<p>Data Format and Quality Control In our preliminary experiments, we found that the toolintegrated reasoning trajectory format generated by zero-shot prompting was somewhat chaotic. Therefore, we designed a few-shot prompting to control the reasoning format, which effectively improved data quality. On the other hand, we increased the annotation success rate by sampling, ensuring more comprehensive coverage of the training query.</p>
<p>Data Filtering Process For the data constructed, we filtered out paths that produced incorrect answers by matching them with standard answers. To prevent the model from learning incorrect</p>
<p>Table 8: Accuracy of ToRA-CORPUS on GSM8k and MATH training set. ToRA-CORPUS-Greedy uses only the greedy trajectories, while ToRA-Corpus-16k combines sampled trajectories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Intermediate <br> Algebra</td>
<td style="text-align: center;">Precalculus</td>
<td style="text-align: center;">Geometry</td>
<td style="text-align: center;">Number <br> Theory</td>
<td style="text-align: center;">Counting $\&amp;$ <br> Probability</td>
<td style="text-align: center;">Prealgebra Algebra</td>
</tr>
<tr>
<td style="text-align: left;">ToRA-CORPUS-Greedy</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: left;">ToRA-CORPUS-16k</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">95.5</td>
</tr>
</tbody>
</table>
<p>Table 9: Statistics of ToRA-CORPUS-16k</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">GSM8k</th>
<th style="text-align: right;">MATH</th>
<th style="text-align: right;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Train Samples</td>
<td style="text-align: right;">7,657</td>
<td style="text-align: right;">7,881</td>
<td style="text-align: right;">15,538</td>
</tr>
<tr>
<td style="text-align: left;">Avg Question Length</td>
<td style="text-align: right;">236</td>
<td style="text-align: right;">189</td>
<td style="text-align: right;">211</td>
</tr>
<tr>
<td style="text-align: left;">Avg Trajectory Length</td>
<td style="text-align: right;">678</td>
<td style="text-align: right;">704</td>
<td style="text-align: right;">691</td>
</tr>
<tr>
<td style="text-align: left;">Min Trajectory Length</td>
<td style="text-align: right;">218</td>
<td style="text-align: right;">119</td>
<td style="text-align: right;">119</td>
</tr>
<tr>
<td style="text-align: left;">Max Trajectory Length</td>
<td style="text-align: right;">1,713</td>
<td style="text-align: right;">2,486</td>
<td style="text-align: right;">2,486</td>
</tr>
</tbody>
</table>
<p>intermediate reasoning processes, we further filtered out data samples with intermediate program execution errors.</p>
<p>Dataset Statistics In Table 8, we compared the annotation accuracy (i.e., sample coverage) of the training set on GSM8k, MATH, and MATH subtopics of ToRA-CORPUS-Greedy using only the greedy trajectories, and ToRA-CORPUS-16k combined with sampled trajectories. Furthermore, in Table 9, we reported the statistical data of ToRA-CORPUS-16k, such as the number of samples, average question length, average, minimum, and maximum trajectory length, as shown in the following tables.</p>
<p>Rationale as Hints As described in Section 2.2, we annotated interactive tool-use trajectories for the training questions from MATH with GPT-4. GPT-4 achieves a success rate below $65 \%$ using greedy decoding. As MATH was originally annotated with natural language rationales, to improve the annotation success rate, we tried to provide GPT-4 with the human rationales as hints (Zelikman et al., 2022). However, when using this method, GPT-4 tends to replicate the hints and ignore tool-use outputs especially when the outputs are inconsistent with the hints, thus failing to produce high-quality trajectories. Hence, we deferred the utilization of the already-annotated natural language rationales for future investigations. Instead, we employed nucleus sampling to recall valid trajectories for questions that remained unsolved through greedy decoding. This approach significantly boosted annotation accuracy to $83.1 \%$.</p>
<h1>E Prompts</h1>
<p>We present instructions and example few-shot prompts of Tool-integrated Reasoning for querying GPT-4.</p>
<p>Listing 1: Prompts of Tool-integrated Reasoning for GPT-4.
Integrate step-by-step reasoning and Python code to solve math problems using the following guidelines:</p>
<ul>
<li>Analyze the question and write functions to solve the problem; the function should not take any arguments.</li>
<li>Present the final result in LaTeX using a '\boxed{}' without any units.</li>
<li>Utilize the 'pi' symbol and 'Rational'' from Sympy for $\$ \backslash$ pi\$ and fractions, and simplify all fractions and square roots without converting them to decimal values.</li>
</ul>
<p>Here are some examples you may refer to:</p>
<p>Question: Convert the point $\${0,-3 \backslash \operatorname{sqrt}{3}, 3} \$$ in rectangular coordinates to spherical coordinates. Enter your answer in the form \$ ${\backslash$ rho, $\backslash$ theta, $\backslash$ phi $}, \$$ where $\$ \backslash$ rho $&gt;0, \$ 0 \backslash$ le $\backslash$ theta $&lt;2 \backslash$ pi, $\$$ and $\$ 0$ $\backslash$ le $\backslash$ phi $\backslash$ le $\backslash$ pi. $\$$</p>
<p>Solution:
To convert the point $\$\left{x, y, z\right} \$$ in rectangular coordinates to spherical coordinates $\${\backslash$ rho, $\backslash$ theta, $\backslash$ phi} \$, we use the following formulas:</p>
<ol>
<li>$\$ \backslash$ rho $=\backslash \operatorname{sqrt}\left{x^{\wedge} 2+y^{\wedge} 2+z^{\wedge} 2\right} \$$</li>
<li>$\$ \backslash$ theta $=\backslash \arctan {\backslash \operatorname{frac}{y}{x}} \$$</li>
<li>$\$ \backslash$ phi $=\backslash \arccos {\backslash \operatorname{frac}{z}{\backslash$ rho $} } \$$</li>
</ol>
<p>Let's write a function:
'''python
from sympy import sqrt, atan2, acos, pi
def rectangular_to_spherical():
$\mathrm{x}, \mathrm{y}, \mathrm{z}=0,-3 * \operatorname{sqrt}{3}, 3$
rho $=\operatorname{sqrt}\left(x * * 2+y * * 2+z * * 2\right)$
theta $=$ atan2 $(y, x)$
phi $=\operatorname{acos}(z /$ rho $)$
return rho, theta, phi
spherical_coordinates = rectangular_to_spherical()
print(spherical_coordinates)
・・
'''output
${6,-p i / 2, \mathrm{pi} / 3}$
・・
The required form of the answer is $\${\backslash$ rho, $\backslash$ theta, $\backslash$ phi} \$, where $\$ \backslash$ rho $&gt;$ $0 \$, \$ 0 \le \backslash t h e t a&lt;2 \backslash$ pi\$, and $\$ 0 \backslash$ le $\backslash$ phi $\backslash$ le $\backslash$ pi\$. Since our initial output has $\$ \backslash$ theta $=-\backslash$ pi $/ 2 \$$, which doesn't meet the condition for $\$ \backslash$ theta\$, we add $\$ 2 \backslash$ pi $\$$ to adjust its value.</p>
<p>Hence, the spherical coordinates for the point $\$\left{0,-3 \backslash \operatorname{sqrt}{3}, 3\right} \$$ are $\$$ $\backslash$ boxed ${{6, \frac{3 \backslash \mathrm{pi}}{2}, \backslash \operatorname{frac}(\backslash \mathrm{pi}}{3}}) \$$.
$----$</p>
<p>Question: Compute $\$ 1011 _2+101 _2-1100 _2+1101 _2 \$$. Express your answer in base 2 .</p>
<div class="codehilite"><pre><span></span><code><span class="n">Solution</span><span class="o">:</span>
<span class="n">Let</span><span class="s1">&#39;s write a single python function for binary computation:</span>
<span class="s1">&#39;&#39;&#39;</span><span class="n">python</span>
<span class="n">def</span><span class="w"> </span><span class="n">binary_sum_diff</span><span class="o">():</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Convert</span><span class="w"> </span><span class="n">binary</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">decimal</span>
<span class="w">    </span><span class="n">num1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">int</span><span class="o">(</span><span class="s2">&quot;1011&quot;</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">)</span>
<span class="w">    </span><span class="n">num2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">int</span><span class="o">(</span><span class="s2">&quot;101&quot;</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">)</span>
<span class="w">    </span><span class="n">num3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">int</span><span class="o">(</span><span class="s2">&quot;1100&quot;</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">)</span>
<span class="w">    </span><span class="n">num4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">int</span><span class="o">(</span><span class="s2">&quot;1101&quot;</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Perform</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">operations</span>
<span class="w">    </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">num1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num2</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">num3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num4</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Convert</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="n">back</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">binary</span>
<span class="w">    </span><span class="n">result_binary</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">format</span><span class="o">(</span><span class="n">result</span><span class="o">,</span><span class="w"> </span><span class="s2">&quot;b&quot;</span><span class="o">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">result_binary</span>
<span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">binary_sum_diff</span><span class="o">()</span>
<span class="n">print</span><span class="o">(</span><span class="n">result</span><span class="o">)</span>
<span class="s1">&#39;&#39;&#39;</span>
<span class="s1">&#39;&#39;&#39;</span><span class="n">output</span>
<span class="mi">10001</span>
<span class="n">The</span><span class="w"> </span><span class="n">answer</span><span class="w"> </span><span class="n">expressed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">base</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">$boxed</span><span class="o">(</span><span class="mi">10001</span><span class="n">_2</span><span class="o">)</span><span class="n">$</span><span class="o">.</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">corresponds</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">projecting</span><span class="w"> </span><span class="n">onto</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">vector</span>
<span class="w">    </span><span class="o">\</span><span class="n">$</span><span class="o">\</span><span class="n">begin</span><span class="o">{</span><span class="n">pmatrix</span><span class="o">}</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">\\</span><span class="w"> </span><span class="o">-</span><span class="mi">3</span><span class="w"> </span><span class="o">\</span><span class="n">end</span><span class="o">{</span><span class="n">pmatrix</span><span class="o">}.\</span><span class="n">$</span>
</code></pre></div>

<p>Solution:
'''python
from sympy import Matrix, simplify
def projection_matrix():
$\mathrm{v}=\operatorname{Matrix}([2,-3])$
$\mathrm{P}=(\mathrm{v} * \mathrm{v} . \mathrm{T}) /(\mathrm{v} . \mathrm{T} * \mathrm{v})[0]$
# Simplify the elements
P = P.applyfunc(simplify)
return P
result = projection_matrix()
print (result)</p>
<div class="codehilite"><pre><span></span><code>&#39;&#39;&#39;output
Matrix([[4/13, -6/13], [-6/13, 9/13]])
</code></pre></div>

<p>The matrix is $\$ \backslash$ boxed{\begin{pmatrix} $\backslash \operatorname{frac}{4}{13} \&amp;-\backslash \operatorname{frac}{6}{13} }-\backslash$ $\operatorname{frac}{6}{13} \&amp; \backslash \operatorname{frac}{9}{13} \backslash$ end{pmatrix}}.\$
$----$</p>
<p>Question: Solve the inequality
$\backslash[\backslash \operatorname{frac}\left{x^{\wedge} 2-25\right}{x+5}&lt;0 . \backslash]$
Solution:
Let's use 'sympy' to solve the inequality:
```python
from sympy import symbols, simplify</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://www.sympy.org
${ }^{3}$ https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
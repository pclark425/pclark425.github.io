<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6438 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6438</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6438</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-273185938</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.05822v2.pdf" target="_blank">Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception</a></p>
                <p><strong>Paper Abstract:</strong> We depend on our own memory to encode, store, and retrieve our experiences. However, memory lapses can occur. One promising avenue for achieving memory augmentation is through the use of augmented reality head-mounted displays to capture and preserve egocentric videos, a practice commonly referred to as lifelogging. However, a significant challenge arises from the sheer volume of video data generated through lifelogging, as the current technology lacks the capability to encode and store such large amounts of data efficiently. Further, retrieving specific information from extensive video archives requires substantial computational power, further complicating the task of quickly accessing desired content. To address these challenges, we propose a memory augmentation agent that involves leveraging natural language encoding for video data and storing them in a vector database. This approach harnesses the power of large vision language models to perform the language encoding process. Additionally, we propose using large language models to facilitate natural language querying. Our agent underwent extensive evaluation using the QA-Ego4D dataset and achieved state-of-the-art results with a BLEU score of 8.3, outperforming conventional machine learning models that scored between 3.4 and 5.8. Additionally, we conducted a user study in which participants interacted with the human memory augmentation agent through episodic memory and open-ended questions. The results of this study show that the agent results in significantly better recall performance on episodic memory tasks compared to human participants. The results also highlight the agent’s practical applicability and user acceptance.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6438.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6438.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory Augmentation Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-Encoded Episodic Memory Augmentation Agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered agent that encodes egocentric video frames into language via a fine-tuned egocentric vision-language model (Ego-LLaVA), converts those text encodings into vector embeddings, stores them in a vector database (Chroma), and answers open-ended episodic memory questions by retrieving relevant chunks and prompting a large LLM (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Language-Encoded Episodic Memory Augmentation Agent (Memory Augmentation Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Encodes egocentric video frames into textual descriptions (Ego-LLaVA), segments the text into overlapping chunks, embeds chunks with OpenAI text-embedding-ada-002, stores embeddings+metadata in a Chroma vector DB, answers queries by embedding the query, performing similarity search in Chroma to retrieve context chunks, and supplying retrieved context + query to GPT-4 to generate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (used as the QA LLM; size not disclosed); Ego-LLaVA fine-tuning used LLaVA base and mentions Vicuna-13B and MPT-7B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (retrieval-augmented memory using a vector database)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual language-encoded descriptions of video frames segmented into 1024-token chunks (256-token overlap), represented as dense vector embeddings produced by OpenAI text-embedding-ada-002</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Query text is embedded (text-embedding-ada-002) and used to perform similarity search in Chroma; writes are made by chunking newly produced textual encodings and uploading their embeddings and metadata to Chroma (append/upload), chunking policy: fixed-size 1024 tokens, 256 overlap, discard short chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA-Ego4D (Episodic Memory Question Answering on egocentric video)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic memory / video question answering / retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BLEU = 8.3 (percent; SacreBLEU scale as used in paper), METEOR = 42.3, ROUGE-L (F1) = 54.7 (on QA-Ego4D test set)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, METEOR, ROUGE-L (f-score)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Trade-offs discussed include: latency/compute (they use 4 fps frame extraction + multiprocessing to approach near real-time), storage vs vision-based approach (language encoding is far smaller: ~0.246–0.345 TB/year vs ~5.74 TB/year for low-bitrate video), and privacy benefits (textual encodings argued to be more privacy-friendly). Also noted: frame-based language encoding is lightweight and device-agnostic but sacrifices temporal information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Frame-by-frame language encoding fails to capture temporal correlations, hurting dynamic activity questions (e.g., distinguishing 'place' vs 'pick up'), counting questions (low resolution limits accurate counting), and some dynamic reasoning tasks; occasional failure modes in temporal/action understanding; no ablation reported comparing with-memory vs without-memory; hallucination issues reported for some alternative encoding methods (Video-LLaMA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shen J., Dudley J. J., Kristensson P. O., 2024. Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception. (arXiv preprint / conference paper as provided)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6438.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6438.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture that augments a controller network with a differentiable, addressable external memory matrix designed to learn read/write operations for complex reasoning and algorithmic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid computing using a neural network with dynamic external memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline memory-augmented neural model used for EMQA comparison in the paper; DNC provides an external differentiable memory matrix with learned read/write controllers to store and retrieve information during processing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external memory (learned read/write memory matrix)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>learned memory matrix (key/value-like vectors) internal to the DNC architecture</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>learned read and write controllers (content- and address-based attention) - (DNC canonical mechanism; the paper cites DNC but does not re-implement its internals beyond referencing it as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA-Ego4D (EMQA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic memory / video QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper (baseline): BLEU ≈ 3.4 (the paper states baseline BLEU scores in the range ~3.4–5.8); Table values in the paper show DNC near the low end (paper's Table 1 lists DNC among low-performing baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, METEOR, ROUGE-L (f-score)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes baseline models (including DNC) perform poorly on EMQA mainly due to constant-size memory constraints and relevance selection, implying scalability and memory-compression trade-offs for such methods on long egocentric videos.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Baseline memory-augmented models (including DNC) 'barely surpassed random guessing' on EMQA according to the paper; they struggle under the EMQA constant-size memory constraint and in selecting relevant information from long videos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Graves A., Wayne G., Reynolds M., et al., 2016. Hybrid computing using a neural network with dynamic external memory. Nature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6438.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6438.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-attentive-Associative-Memory-based Two-memory Model (STM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline two-memory model using self-attention and associative memory concepts referenced as a prior approach for tasks requiring external memory-like capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-attentive associative memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-attentive-Associative-Memory-based Two-memory Model (STM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced baseline model for EMQA that relies on self-attention and associative memory mechanisms (paper cites STM as a conventional memory model baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>associative / self-attentive memory (as per model name; paper cites it as a memory-based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA-Ego4D (EMQA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic memory / video QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as a baseline with low BLEU (paper indicates baselines in low single-digit BLEU); exact numbers for STM are reported in the paper's Table 1 (baseline range cited ~3.4–5.8 BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, METEOR, ROUGE-L (f-score)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper comments STM's simplistic implementation (reliance on a single hidden vector in their instantiation) may be inadequate for EMQA, indicating limitations in representational capacity under the constant-size memory constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported to perform poorly on EMQA relative to the language-encoded approach; struggles with relevance selection and compression into fixed-size memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Le H., Tran T., Venkatesh S., 2020. Self-attentive associative memory. (ICML workshop / conference reference as cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6438.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6438.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LT-CT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LT-CT (Long-term / Compressive-like Transformer baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline transformer-based model for long-range sequence modeling referenced among EMQA baselines (paper cites LT-CT among models tested).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LT-CT (baseline long-range model)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Referenced baseline for EMQA; paper includes it in quantitative comparisons but gives no detailed reimplementation description within this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-range sequence memory / transformer compression-style memory (paper references long-range transformer methods as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA-Ego4D (EMQA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic memory / long-range retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper's Table 1 lists LT-CT with BLEU ≈ 5.3, METEOR ≈ 18.5, ROUGE-L ≈ 27.5 (as reported in the paper's results table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, METEOR, ROUGE-L (f-score)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper generally notes that long-range models face challenges with the EMQA constant-size memory constraint and relevance selection for long egocentric videos.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performed worse than language-encoded Ego-LLaVA approach on QA-Ego4D; limited ability to compress/retain all relevant information under fixed memory constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6438.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6438.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rehearsal Memory (RM / Learning to Rehearse)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rehearsal-memory-based baseline listed among models compared on the EMQA task; rehearsal memory methods periodically replay or rehearse stored items to maintain long-term information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to rehearse in long sequence memorization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rehearsal Memory (RM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline approach that uses rehearsal/experience replay-style memory mechanisms intended to preserve sequence information over long sequences; used as a comparative baseline on QA-Ego4D.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>rehearsal / replay memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA-Ego4D (EMQA benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>episodic memory / long-sequence memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Paper's Table 1 lists RM with BLEU ≈ 4.5, METEOR ≈ 17.7, ROUGE-L ≈ 26.6 (as reported in the paper's results table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU-4, METEOR, ROUGE-L (f-score)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper indicates rehearsal-style baselines still underperform on EMQA due to memory compression and relevance-selection issues under the constant-size constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance than language-encoded Ego-LLaVA; struggles with selecting and preserving relevant episodic content from long egocentric videos.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zhang Z., Zhou C., Ma J., et al., 2021. Learning to rehearse in long sequence memorization. (as cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Where did i leave my keys?-episodicmemory-based question answering on egocentric videos <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Self-attentive associative memory <em>(Rating: 2)</em></li>
                <li>Learning to rehearse in long sequence memorization <em>(Rating: 2)</em></li>
                <li>Compressive transformers for long-range sequence modelling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6438",
    "paper_id": "paper-273185938",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Memory Augmentation Agent",
            "name_full": "Language-Encoded Episodic Memory Augmentation Agent",
            "brief_description": "A human-centered agent that encodes egocentric video frames into language via a fine-tuned egocentric vision-language model (Ego-LLaVA), converts those text encodings into vector embeddings, stores them in a vector database (Chroma), and answers open-ended episodic memory questions by retrieving relevant chunks and prompting a large LLM (GPT-4).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Language-Encoded Episodic Memory Augmentation Agent (Memory Augmentation Agent)",
            "agent_description": "Encodes egocentric video frames into textual descriptions (Ego-LLaVA), segments the text into overlapping chunks, embeds chunks with OpenAI text-embedding-ada-002, stores embeddings+metadata in a Chroma vector DB, answers queries by embedding the query, performing similarity search in Chroma to retrieve context chunks, and supplying retrieved context + query to GPT-4 to generate answers.",
            "model_size": "GPT-4 (used as the QA LLM; size not disclosed); Ego-LLaVA fine-tuning used LLaVA base and mentions Vicuna-13B and MPT-7B in experiments",
            "memory_used": true,
            "memory_type": "external vector store (retrieval-augmented memory using a vector database)",
            "memory_representation": "Textual language-encoded descriptions of video frames segmented into 1024-token chunks (256-token overlap), represented as dense vector embeddings produced by OpenAI text-embedding-ada-002",
            "memory_access_mechanism": "Query text is embedded (text-embedding-ada-002) and used to perform similarity search in Chroma; writes are made by chunking newly produced textual encodings and uploading their embeddings and metadata to Chroma (append/upload), chunking policy: fixed-size 1024 tokens, 256 overlap, discard short chunks.",
            "task_name": "QA-Ego4D (Episodic Memory Question Answering on egocentric video)",
            "task_category": "episodic memory / video question answering / retrieval-augmented QA",
            "performance_with_memory": "BLEU = 8.3 (percent; SacreBLEU scale as used in paper), METEOR = 42.3, ROUGE-L (F1) = 54.7 (on QA-Ego4D test set)",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "BLEU-4, METEOR, ROUGE-L (f-score)",
            "tradeoffs_reported": "Trade-offs discussed include: latency/compute (they use 4 fps frame extraction + multiprocessing to approach near real-time), storage vs vision-based approach (language encoding is far smaller: ~0.246–0.345 TB/year vs ~5.74 TB/year for low-bitrate video), and privacy benefits (textual encodings argued to be more privacy-friendly). Also noted: frame-based language encoding is lightweight and device-agnostic but sacrifices temporal information.",
            "limitations_or_failure_cases": "Frame-by-frame language encoding fails to capture temporal correlations, hurting dynamic activity questions (e.g., distinguishing 'place' vs 'pick up'), counting questions (low resolution limits accurate counting), and some dynamic reasoning tasks; occasional failure modes in temporal/action understanding; no ablation reported comparing with-memory vs without-memory; hallucination issues reported for some alternative encoding methods (Video-LLaMA).",
            "citation": "Shen J., Dudley J. J., Kristensson P. O., 2024. Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception. (arXiv preprint / conference paper as provided)",
            "uuid": "e6438.0",
            "source_info": {
                "paper_title": "Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer (DNC)",
            "brief_description": "A neural architecture that augments a controller network with a differentiable, addressable external memory matrix designed to learn read/write operations for complex reasoning and algorithmic tasks.",
            "citation_title": "Hybrid computing using a neural network with dynamic external memory",
            "mention_or_use": "use",
            "agent_name": "Differentiable Neural Computer (DNC)",
            "agent_description": "Baseline memory-augmented neural model used for EMQA comparison in the paper; DNC provides an external differentiable memory matrix with learned read/write controllers to store and retrieve information during processing.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external memory (learned read/write memory matrix)",
            "memory_representation": "learned memory matrix (key/value-like vectors) internal to the DNC architecture",
            "memory_access_mechanism": "learned read and write controllers (content- and address-based attention) - (DNC canonical mechanism; the paper cites DNC but does not re-implement its internals beyond referencing it as a baseline)",
            "task_name": "QA-Ego4D (EMQA benchmark)",
            "task_category": "episodic memory / video QA",
            "performance_with_memory": "Reported in paper (baseline): BLEU ≈ 3.4 (the paper states baseline BLEU scores in the range ~3.4–5.8); Table values in the paper show DNC near the low end (paper's Table 1 lists DNC among low-performing baselines).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "BLEU-4, METEOR, ROUGE-L (f-score)",
            "tradeoffs_reported": "Paper notes baseline models (including DNC) perform poorly on EMQA mainly due to constant-size memory constraints and relevance selection, implying scalability and memory-compression trade-offs for such methods on long egocentric videos.",
            "limitations_or_failure_cases": "Baseline memory-augmented models (including DNC) 'barely surpassed random guessing' on EMQA according to the paper; they struggle under the EMQA constant-size memory constraint and in selecting relevant information from long videos.",
            "citation": "Graves A., Wayne G., Reynolds M., et al., 2016. Hybrid computing using a neural network with dynamic external memory. Nature.",
            "uuid": "e6438.1",
            "source_info": {
                "paper_title": "Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "STM",
            "name_full": "Self-attentive-Associative-Memory-based Two-memory Model (STM)",
            "brief_description": "A baseline two-memory model using self-attention and associative memory concepts referenced as a prior approach for tasks requiring external memory-like capabilities.",
            "citation_title": "Self-attentive associative memory",
            "mention_or_use": "use",
            "agent_name": "Self-attentive-Associative-Memory-based Two-memory Model (STM)",
            "agent_description": "Referenced baseline model for EMQA that relies on self-attention and associative memory mechanisms (paper cites STM as a conventional memory model baseline).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "associative / self-attentive memory (as per model name; paper cites it as a memory-based baseline)",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "QA-Ego4D (EMQA benchmark)",
            "task_category": "episodic memory / video QA",
            "performance_with_memory": "Reported as a baseline with low BLEU (paper indicates baselines in low single-digit BLEU); exact numbers for STM are reported in the paper's Table 1 (baseline range cited ~3.4–5.8 BLEU).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "BLEU-4, METEOR, ROUGE-L (f-score)",
            "tradeoffs_reported": "Paper comments STM's simplistic implementation (reliance on a single hidden vector in their instantiation) may be inadequate for EMQA, indicating limitations in representational capacity under the constant-size memory constraint.",
            "limitations_or_failure_cases": "Reported to perform poorly on EMQA relative to the language-encoded approach; struggles with relevance selection and compression into fixed-size memory.",
            "citation": "Le H., Tran T., Venkatesh S., 2020. Self-attentive associative memory. (ICML workshop / conference reference as cited in paper)",
            "uuid": "e6438.2",
            "source_info": {
                "paper_title": "Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LT-CT",
            "name_full": "LT-CT (Long-term / Compressive-like Transformer baseline)",
            "brief_description": "A baseline transformer-based model for long-range sequence modeling referenced among EMQA baselines (paper cites LT-CT among models tested).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "LT-CT (baseline long-range model)",
            "agent_description": "Referenced baseline for EMQA; paper includes it in quantitative comparisons but gives no detailed reimplementation description within this work.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "long-range sequence memory / transformer compression-style memory (paper references long-range transformer methods as baselines)",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "QA-Ego4D (EMQA benchmark)",
            "task_category": "episodic memory / long-range retrieval",
            "performance_with_memory": "Paper's Table 1 lists LT-CT with BLEU ≈ 5.3, METEOR ≈ 18.5, ROUGE-L ≈ 27.5 (as reported in the paper's results table).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "BLEU-4, METEOR, ROUGE-L (f-score)",
            "tradeoffs_reported": "Paper generally notes that long-range models face challenges with the EMQA constant-size memory constraint and relevance selection for long egocentric videos.",
            "limitations_or_failure_cases": "Performed worse than language-encoded Ego-LLaVA approach on QA-Ego4D; limited ability to compress/retain all relevant information under fixed memory constraints.",
            "citation": null,
            "uuid": "e6438.3",
            "source_info": {
                "paper_title": "Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "RM",
            "name_full": "Rehearsal Memory (RM / Learning to Rehearse)",
            "brief_description": "A rehearsal-memory-based baseline listed among models compared on the EMQA task; rehearsal memory methods periodically replay or rehearse stored items to maintain long-term information.",
            "citation_title": "Learning to rehearse in long sequence memorization",
            "mention_or_use": "use",
            "agent_name": "Rehearsal Memory (RM)",
            "agent_description": "Baseline approach that uses rehearsal/experience replay-style memory mechanisms intended to preserve sequence information over long sequences; used as a comparative baseline on QA-Ego4D.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "rehearsal / replay memory",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "QA-Ego4D (EMQA benchmark)",
            "task_category": "episodic memory / long-sequence memorization",
            "performance_with_memory": "Paper's Table 1 lists RM with BLEU ≈ 4.5, METEOR ≈ 17.7, ROUGE-L ≈ 26.6 (as reported in the paper's results table).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "BLEU-4, METEOR, ROUGE-L (f-score)",
            "tradeoffs_reported": "Paper indicates rehearsal-style baselines still underperform on EMQA due to memory compression and relevance-selection issues under the constant-size constraint.",
            "limitations_or_failure_cases": "Lower performance than language-encoded Ego-LLaVA; struggles with selecting and preserving relevant episodic content from long egocentric videos.",
            "citation": "Zhang Z., Zhou C., Ma J., et al., 2021. Learning to rehearse in long sequence memorization. (as cited in paper)",
            "uuid": "e6438.4",
            "source_info": {
                "paper_title": "Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Where did i leave my keys?-episodicmemory-based question answering on egocentric videos",
            "rating": 2,
            "sanitized_title": "where_did_i_leave_my_keysepisodicmemorybased_question_answering_on_egocentric_videos"
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2,
            "sanitized_title": "hybrid_computing_using_a_neural_network_with_dynamic_external_memory"
        },
        {
            "paper_title": "Self-attentive associative memory",
            "rating": 2,
            "sanitized_title": "selfattentive_associative_memory"
        },
        {
            "paper_title": "Learning to rehearse in long sequence memorization",
            "rating": 2,
            "sanitized_title": "learning_to_rehearse_in_long_sequence_memorization"
        },
        {
            "paper_title": "Compressive transformers for long-range sequence modelling",
            "rating": 1,
            "sanitized_title": "compressive_transformers_for_longrange_sequence_modelling"
        }
    ],
    "cost": 0.014844249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception
18 Oct 2024</p>
<p>Junxiao Shen 
University of Cambridge</p>
<p>John J Dudley 
University of Cambridge</p>
<p>PerOla Kristensson 
University of Cambridge</p>
<p>Encode-Store-Retrieve: Augmenting Human Memory through Language-Encoded Egocentric Perception
18 Oct 2024B8AFE20ED22AD41DE57F830576FEB3B2arXiv:2308.05822v3[cs.CV]Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented realityInformation systems-Information retrieval-Retrieval tasks and goals-Question answeringComputing methodologies-Artificial intelligence-Computer vision-Computer vision tasksVisual content-based indexing and retrieval
Figure1: Left: Demonstration of a user interact with the memory augmentation agent.Right: The operation of the memory augmentation agent using language-encoded egocentric perception unfolds as follows.Initially, egocentric videos are encoded into linguistic representations using our bespoke large multimodal model, an egocentric vision language model.These language-encoded outputs are then held in a buffer, segmented into numerous chunks, featured as embeddings, and uploaded to a vector database.When a user poses a question-a common memory augmentation task such as "Where did I leave my keys?"-the process begins by taking the user's query to generate a corresponding embedding.This produced embedding vector is then used to query the Database Interface with the intention of identifying relevant chunks related to the question by using vector similarity search algorithms.The retrieved chunks are subsequently amalgamated with the question to form a prompt for the large language model.The final response is then generated and presented to the user.</p>
<p>INTRODUCTION</p>
<p>Augmented reality (AR) enhances user perception by superimposing virtual content onto their environment, traditionally augmenting vision with digital overlays on real-world scenes [1,38].However, AR extends beyond visual augmentation to include auditory and tactile feedback [9,26,35,36,53].These multimodal capabilities make AR accessible and beneficial for a wider audience, enabling immersive and interactive experiences for all users, regardless of their abilities.This paper introduces a human memory augmentation agent that utilizes both the audio feedback and the egocentric frontfacing camera of an AR device.</p>
<p>Human memory, crucial for various cognitive functions, often relies on external aids like photographs and reminders for enhancing recollection of past events [24,25,27].However, these aids are limited in scope.Some researchers suggest that lifelogging can be employed to augment memory [6,12,21].Lifelogging involves the capture of images or videos, along with a wide array of personal data such as one's location, audio recordings, and physiological data [19,22,23].</p>
<p>The introduction of smart glasses, including AR headsets with cameras, has enhanced lifelogging by enabling seamless capture of egocentric data along with additional sensor inputs.However, using these devices to enhance memory presents challenges, notably in efficiently encoding and storing the large, complex video data to maintain quality and enable real-time processing.Additionally, compliance with data privacy regulations is essential [18].Once the data has been encoded and stored, the next challenge is to ensure that this information can be retrieved in a useful and timely manner.Therefore, we need to design a system that is able to process large volumes of complex data and return useful results to the user, while also managing potential privacy concerns.</p>
<p>To address these shortcomings, the approach presented in this paper leverages a language-encoded episodic memory agent that consists of three essential components that are demonstrated in Figure 1.First, we use language encoding to represent egocentric videos by harnessing the power of a large vision language model.Second, we store the language embeddings in a vector database for efficient retrieval.Third, we leverage a large language model to facilitate open-ended question answering in episodic memory tasks, using the vector database to enable the language model to access encoded memories.Using this approach, we focus on finetuning a large vision language model called Large Language and Vision Assistant (LLaVA) [34], specifically for egocentric data.For question answering (QA) in episodic memory, we use OpenAI GPT-4 [43] and integrate it with Chroma [8], a vector database that enables long-term memory storage and retrieval.</p>
<p>Barman et al. [3] introduced the QA-Ego4D dataset as a novel dataset designed for the memory augmentation task.It is offered as an extension of the Ego4D dataset [16].Each sample in the QA-Ego4D dataset consists of a video, a natural language question, and an answer.The QA-Ego4D dataset is unique in its focus on the Episodic Memory Question Answering (EMQA) task, which introduces a constant-size memory constraint.This makes it particularly suitable for scenarios where the video content has long duration, or the system needs to be used in a life-long manner.Barman et al. [3] used multiple conventional vision-based machine learning models for this task, yet they demonstrated substantial limitations.Their performance barely surpassed a random guessing strategy, attaining BLEU scores between 3.4 and 5.8.In comparison, the method introduced in this paper achieves state-of-the-art results, outperforming the models by Barman et al. [3] with a notable BLEU score of 8.3 for this task.</p>
<p>To assess the usability of our agent, we implemented it on the HoloLens 2 device, an AR headset, and allowed participants to ask open-ended questions to probe the agent's capabilities.The results of this evaluation underscore the agent's practicality and acceptance among users.</p>
<p>Hence, our research paper presents three contributions as outlined below:</p>
<p>• To the best of our knowledge, we present the first memory augmentation agent that integrates vision language encoding with episodic memory tasks, while also using a vector database for efficient storage and retrieval.• We report the results of a lagre-scale quantitative evaluation of our agent using the Episodic Memory QA benchmark, specifically with the QA-Ego4D dataset.The results of these evaluations demonstrate the effectiveness of our agent in tackling episodic memory tasks.• We report the results of a user study to explore the application potential of our agent.Our study showcases the benefits of our agent in various scenarios.By involving users and obtaining their feedback, we gain valuable insights into the practical implications and user satisfaction associated with our agent.</p>
<p>RELATED WORK 2.1 Augmented Reality: Augmenting User Perceptions</p>
<p>Augmented reality (AR) is a technology that enriches the user's perception of the world by superimposing virtual content onto their physical environment [38].Typically, AR enhances user vision by overlaying digital images, videos, or 3D models onto real-world objects or scenes, creating an immersive and interactive experience [1].However, AR encompasses more than just augmenting vision, as relying solely on visual augmentation would exclude individuals with disabilities, particularly those who are blind or have visual impairments [9].AR technology has the potential to be inclusive by extending augmentation beyond vision to other sensory modalities, such as auditory or tactile feedback [35].For instance, AR applications could provide audio descriptions or tactile cues to convey information about the user's surroundings, enabling people with visual impairments to navigate and interact with the augmented environment [26,36,53].By embracing multimodal augmentation, AR becomes accessible to a broader range of users, ensuring that everyone can benefit from its immersive and interactive experiences, regardless of their abilities or limitations.Memory augmentation, for instance, is a notable application where AR can assist users in recalling information more effectively [49].By providing contextual cues and audio feedback, AR can help users remember important details, tasks, or information associated with specific objects or locations.This broader scope of AR highlights its potential to enhance various facets of human perception and cognition, making it a versatile and impactful technology in numerous fields.</p>
<p>Memory Augmentation through Lifelogging</p>
<p>Lifelogging, present for over 30 years, has evolved with technology [20,21].Initially, it involved bulky equipment like helmets and battery packs [55], but has since progressed to wearable devices like glasses [21].A key development was Microsoft's SenseCam in 2006, a notable lifelogging device [13,37].Lifelogging now includes data from GPS, audio, heart rate, emails, calendar events, and social media.Significant progress has been made in memory augmentation through lifelogging [21].Le et al. [30] focused on video summaries for memory recall but didn't address data selection challenges.Davie et al. [11] highlighted privacy and security concerns in pervasive computing but lacked comprehensive solutions.Byrne et al. [4] developed a method for content relevance in visual lifelogs but it was limited to everyday concepts.Our work uniquely implements a memory augmentation agent enabling open-ended episodic memory queries within a wearable headset.</p>
<p>Video Content Analysis</p>
<p>Natural Language Video Localization (NLVL) and Video Question Answering (VideoQA) are distinct yet related tasks in video content analysis.NLVL focuses on finding a video segment matching a natural language query, requiring the model to understand both video and query context [15,16,28,47].VideoQA, on the other hand, involves answering questions based on video content, demanding a deep understanding of the video and the ability to provide precise answers [31,32,39,41,52].</p>
<p>Episodic Memory Question Answering (EMQA) is a specific subtask of VideoQA, introduced by Bärmann et al. [3].It differs in its memory constraints, shifting from offline analysis (VideoQA) to an online algorithm and setting a maximum limit on memory usage for computation, thus suitable for long-term or life-long use.</p>
<p>This paper focuses on EMQA due to its advantages over NLVL and traditional VideoQA.While NLVL produces non-textual output and VideoQA has scalability issues, EMQA offers textual outputs and a constant-size memory constraint, enhancing efficiency for long videos.</p>
<p>AGENT DESIGN</p>
<p>Human memory involves encoding, storing, and retrieving information.Encoding is key for converting information into a format suitable for memory storage.Storage maintains this information until needed, and retrieval accesses and reinstates it into consciousness.Our agent mimics these biological memory processes [61].</p>
<p>Initially, each video frame v is transformed into a language encoding l using the encoding function E, denoted as l = E(v).These language encodings are accumulated over time, forming a cumulative history L history .</p>
<p>An embedding model acts as a transformation function T to convert chunks C of L history into vector representation g, expressed as g = T (C).The vector g is stored in a vector database via a storage function S, where S(g) → Database.</p>
<p>For retrieval, the agent uses the same transformation function T to convert a natural language query q into a query vector q v : q v = T (q).The retrieval function R then fetches the most relevant language encodings c as context based on q v , formulated as R(q v , Database) → c.This context c and the query q are concatenated and processed by large language models to generate an answer based on the context.</p>
<p>Encode</p>
<p>We employ language as a means to encode egocentric visual perceptions.Specifically, our focus lies on adopting a frame-based approach rather than encoding clips using a sliding window.This decision stems from the fact that encoding clips using a sliding window can result in excessively long inference times, rendering it impractical for real-time usage.Furthermore, the field of video captioning is still in its early stages, and even state-of-the-art models are unable to provide accurate and detailed encodings [54,[56][57][58].Consequently, we opt to encode videos by their individual frames.</p>
<p>We present a novel model, Ego-LLaVA, for egocentric video encoding.This model is fine-tuned from LLaVA (Large Language and Vision Assistant) [34] on egocentric data, which captures firstperson experiences in a 3D environment.This fine-tuning procedure leads to better performance in understanding first-person data which involves interpreting human-object interactions and complex social behaviors.</p>
<p>To tackle this issue, we curated our own egocentric video frame description dataset from Ego4D [16] and fine-tuned the LLaVA model to learn egocentric features.</p>
<p>The fine-tuning process is described below:</p>
<p>• Training Data: We begin by employing LLaVA using a descriptor prompt P, to generate detailed descriptions for a randomly sampled set of 3,000 images.Subsequently, we engaged three research assistants from our institution to correct the descriptions in scenarios where objects were inaccurately identified or significant objects within the frames were missed.This process results in a collection of 3,000 image/video frame-text/description pairs.It has been observed by Zhu et al. [60] that a set of 3,000 training pairs is adequate.Zhu et al. [60] successfully fine-tuned a visual-language model using only 3,500 image-text pairs, which yielded exceptional performance in tasks such as image question answering.</p>
<p>The practice of training language models using responses generated by larger language models has become increasingly common due to the robustness of these models.Vicuna-13B [7] is an example of a model trained by fine-tuning the LLaMA-13B [58] base model with approximately 70,000 user-shared conversations gathered from ShareGPT [50], a website that collects conversational data from OpenAI ChatGPT.Similarly, MiniGPT-4 [60] and LLaVA [34] are trained using large language model-generated content, achieving state-of-the-art results and saving significant time on human labeling.• Fine-Tuning: In our experiment, as shown in Figure 2, we use Vicuna-13B [7] as the 13B language model and MPT-7B [40] as the 7B language model.MPT (MosaicML Pretrained Transformer) is optimized for efficient training and fast inference, utilizing FlashAttention [10] and FasterTransformer [42] techniques.More specifically, Ego-LLaVA is fine-tuned on image-text pairs where the descriptor question P prompts a description of the video frame v, and the ground truth prediction answer l is the original detailed description.During training, the weights of both the visual encoder and LLM are kept constant, and the probability p(l|v, P) of the target answers l is maximized by only training the parameters of the linear projection layer between the visual encoder and the LLM.This process allows for the alignment of the video frame features H v with the pre-trained LLM word embedding.</p>
<p>Store and Retrieve</p>
<p>Our approach involves segmenting language-encoded data into fixedsize chunks of 1024 tokens with a 256-token overlap to maintain semantic context, discarding any chunks under five characters.We attach relevant metadata to each chunk for enhanced search capabilities in vector databases.For capturing semantic meanings, we use OpenAI's text-embedding-ada-002 to create vector embeddings, which are then stored in the Chroma [8] vector database.This setup facilitates efficient management and retrieval of high-dimensional vector data.</p>
<p>The retrieval system operates by converting a user's input question into an embedding using OpenAI's text-embedding-ada-002 model.This embedding serves as a query to the Chroma [8] database, which searches for closely related vector-indexed data chunks.These chunks are then incorporated into a prompt along with the question for OpenAI GPT-4 to generate a response [5,51].</p>
<p>STUDY 1: LARGE-SCALE EVALUATION OF THE MEMORY AUGMENTATION AGENT</p>
<p>To study the proposed memory augmentation agent's performance we carry out a large-scale quantitative evaluation using the public dataset QA-Ego4D.The evaluation focuses on the EMQA task, which is detailed in Section 2.3.</p>
<p>Dataset -QA-Ego4D</p>
<p>The QA-Ego4D dataset, an extension of the Ego4D dataset's Natural Language Query (NLQ) subtask, features egocentric videos paired with natural language questions, answers, and annotations for answer-relevant video segments [3,16].Figure 3 illustrates an example QA pair from the QA-Ego4D dataset [3].Each video averages eight minutes in length.The dataset includes 19.2K queries from 227 hours of video across 34 scenarios from ten universities.Queries average 8.3 words, with response windows averaging 9.3 seconds, presenting a search challenge.The dataset omits "When?" questions due to undefined natural language answers.It's divided into training, validation, and test sets, with 997 training videos, 162 for validation, and 166 for testing, comprising 10,746, 1,913, and 1,854 question-answer pairs for each set respectively.The test data uses half of the validation set's canonical videos, as Ego4D's test data is unpublished.Explicit measures were implemented during the splitting process of the train and validation datasets to avoid any overlap, thereby preventing model overfitting [16].</p>
<p>Baseline Models</p>
<p>In our comparison, we include models from the QA-Ego4D paper [3]: Differentiable Neural Computer (DNC) [17], Self-attentive-Associative-Memory-based Two-memory Model (STM) [29], Long-  The Egocentric Vision-Language Model is developed through a process called fine-tuning.This process involves extracting knowledge from a large model and transferring it to smaller models, resulting in improved accuracy and faster inference times.The Egocentric Vision-Language Model combines the power of vision and language to effectively process and understand egocentric video data.13B and 7B refer to large language models with 13 billion and 7 billion parameters.Figure 3: An example QA pair from the QA-Ego4D dataset adopted from [3].</p>
<p>Term Comprehensive Transformer (LT-CT) [46], and Rehearsal Memory (RM) [59].</p>
<p>We also employ alterations to the encoding methods:</p>
<p>• Language-Encoded QA (with Video-LLaMA [58]): We employ a sliding window approach with a width and stride of 6 seconds each to encode video clips into language.Video-LLaMA is a state-of-the-art video QA model which is suitable for video captioning.</p>
<p>• Language-Encoded QA (with LLaVA [34]): We use the original LLaVA as the encoding method.</p>
<p>We prompt both the above two models using the same prompt as for Ego-LLaVA.These contrasting models provide a comprehensive comparison for the model proposed in this paper.</p>
<p>Evaluation Metrics</p>
<p>We report standard Natural Language Processing metrics for EMQA tasks, including BLEU-4 [44], METEOR [2], and ROUGE-L (fscore) [33].These metrics all measure the similarity between a machine-generated sentence and a reference sentence.We report the values in percentage values.A higher value indicates better performance.More specifically: BLEU-4 [44] measures the precision of n-grams (up to four words) in the generated text against the reference text, METEOR [2] evaluates based on precision, recall, and alignment, incorporating synonymy and stemming, and ROUGE-L (f-score) [33] assesses the longest common subsequence between generated and reference texts.</p>
<p>Results</p>
<p>Table 1 shows the outcomes of the various methods explained above applied to the QA-Ego4D test set.We observe that the languageencoded approach significantly outperforms conventional machine learning models.Additionally, we find that Ego-LLaVA surpasses the original LLaVA as an encoding method.This improvement can be attributed to the fine-tuning of LLaVA using egocentric data.</p>
<p>Model BLEU METEOR ROUGE DNC [17] 3.4 17.9 27.0 STM [29] 5 Despite the relatively small size of the fine-tuning dataset, which consists of only 3,000 image-text pairs, the model successfully learns to align image embedding features with text features.The performance in using Video LLaMA as the encoding model is poor because it suffers from severe hallucination issues.Table 2 shows the agent's performance on various question templates.We observe that simpler questions, such as "Where is object X?" or "Where did I put X?", generally yield better results due to their straightforward nature, demanding less complex reasoning from the agent.Conversely, questions involving more intricate reasoning or understanding of dynamic elements, such as "Where is object X before / after event Y?" or "What X did I Y?", may not perform as well.This is attributed to the current encoding method's limitations in capturing temporal correlations, which are crucial for comprehending dynamic activities.Quantity-based questions, such as "How many X's?", pose a challenge due to the encoding model's resolution limitations, making accurate object counting difficult.</p>
<p>Questions about the state of an object could also be challenging if the state involves fine details, or dynamic elements that change over time.Without the ability to apply attention to the data, the agent might not capture these dynamic subtleties, leading to a significant loss of crucial information.In essence, the agent's performance on different question templates largely depends on the question's complexity, the required level of detail, and the agent's ability to understand dynamic elements and temporal correlations.Future improvements in these areas could potentially enhance the agent's performance on more complex question templates.</p>
<p>STUDY 2: USABILITY STUDY FOR OPEN-ENDED QUES-</p>
<p>TIONS</p>
<p>Having established quantitative performance benefits of the memory augmentation agent in theory it is natural to ask whether the agent is usable in practice.To this end, we carried out a user study with two objectives.The first objective is to evaluate and contrast the</p>
<p>Category Template BLEU</p>
<p>Objects</p>
<p>Where is object X before / after event Y? 8.7</p>
<p>Where is object X? 8.9 What did I put in X?The templates span a wide range of inquiries that individuals can make use of to enhance their memory, and retrieve information about various objects, locations, and individuals they encounter in their daily lives.We also show the average BLEU score for the proposed memory augmentation agent for each template.We adopted the same scale as prior work [3], which uses a percentage as given by SacreBLEU [45].;</p>
<p>performance of human participants with that of the memory augmentation agent in answering a set of episodic memory questions.</p>
<p>The second objective is to explore the framework's capability in handling open-ended questions, which could potentially demand strong reasoning power and access to an external knowledge base.We did not incorporate another memory augmentation agent as a comparison because we are the first to propose such a memory augmentation framework and hence no such baseline exist.The conventional machine learning models have very limited usability as suggested by the results of the large-scale evaluation we described previously.</p>
<p>Methodology</p>
<p>Participants</p>
<p>We recruited a total of 12 participants using opportunity sampling to take part in the study (average age = 26.7,sd = 5.2; 7 males and 5 females).</p>
<p>A G*Power's analysis [14] based on a t-test suggested a sample size of 12 as being adequate for the study based on an effect size of 0.81 (calculated from the collected results), an error probability of 0.05, and a power of 0.8.Among the 12 participants in the user study, four were students, five were employed, and three were self-employed.We envisage the intended users of this memory augmentation agent to be early adopters within the age range of 25 to 34, possessing AR/VR devices.</p>
<p>Materials</p>
<p>The study used a HoloLens 2 device which has an inbuilt front camera to stream egocentric videos.The encoding, storage, and retrieval tasks were performed by calling APIs hosted on our server.</p>
<p>Protocol</p>
<ol>
<li>Study procedure: The study consisted of two stages.During the first stage, participants were equipped with a HoloLens 2 device and were instructed to perform a series of tasks.These tasks were divided into five different scenarios: (1) looking at a painting in a living room; (2) switching TV channels in a living room; (3) cooking eggs in a kitchen; (4) reading a book in a study room; and (5) selecting a movie on a laptop.These tasks took place in an actual house equipped with a variety of furniture and items.Figure 4 illustrates the settings of the different scenarios and indicate the duration of each scenario.Participants were encouraged to freely engage in the tasks to simulate a normal daily life experiences.Between five and seven days later, participants proceeded to the second stage of the study.This delay was used based on the concept proposed by Rivera-Lares et al. [48].They suggest that after a week, the amount of retained information may have diminished to a level referred to as the "floor", making it challenging to detect or observe any additional instances of forgetting.This can be represented by the forgetting curve, which hypothesizes a decline in memory retention over time in the absence of deliberate attempts to retain information.</li>
</ol>
<p>Episodic memory questions to the participants and the agent:</p>
<p>During the second stage of the study the participants were presented with a set of questions modeled after Table 2, which were derived from the tasks they performed.Standard questions included "Where did you place the TV remote?", "Name the list of movies you browsed on the laptop?","What was the dominant color of the painting you observed?","How many eggs did you cook?", and "Name the book you read?".In addition to these task-related questions, there were other queries that were not directly linked to the tasks but remained relevant to the scenarios, including "What color was the guitar beside the painting?","What was the person you interacted with (study facilitator) wearing?", "What is the color of the kettle beside the pan" and "How many vases did you see on the dining table?".An example of a description type question for the third category of queries is the following: "Describe the painting in detail".We asked each participant these ten questions.Same questions were used to ask the memory augmentation agent to generate responses.3. Open-ended questions to the agent: Subsequently, participants were encouraged to ask the memory augmentation agent five openended questions through an interactive conversational interface.Examples of questions were "What movie would you recommend for next time?","Based on what you know, do you think I eat healthily, and if not, what suggestions do you have for my diet?", and "What are the steps to better cook an egg?". 4. Rate the agent's responses: Thereafter the participants were asked to rate the responses generated by the agent as well as their own answers using a scale ranging from 1 (very bad) to 5 (very good).</p>
<p>Note that the order of queries and the order of which responses to score were randomized.Participants were also requested to score the agent's responses to open-ended questions using the same scale.5. Post-study Likert scale questions: We then used post-study questionnaire to gather feedback from the participants regarding their subjective opinions on the overall experience with the agent.The participants were asked to respond to the following Likert scale questions: (1) The memory augmentation capability is valuable; (2) The information provided by the memory augmentation agent is accurate; (3) The response to my open-ended questions by the memory augmentation agent is creative; (4) I am willing to wear an always-on camera for language-encoded memory augmentation; and (5) I am willing for others in my close vicinity to wear an always-on camera for language-encoded memory augmentation.The participant responses are distributed across five categories: Strongly Disagree, Disagree, Neutral, Agree, and Strongly Agree, for different aspects of the memory augmentation agent, including its capability, accuracy, creativity, and willingness to use.</p>
<p>Implementation</p>
<p>The process begun with the uploading of the video to the server.To optimize the speed of the encoding process, we extracted four frames per second from the video.To expedite this process and approach near real-time encoding, we used multi-process threading.This technique allows multiple encoding tasks to be executed simultaneously, significantly reducing the overall time required.Once the frames were encoded, they were stored in a vector database.We used LangChain [5] and Chroma [8] as the implementation framework for vector data storage and retrieval, and OpenAI GPT-4 as the large language model to implement the conversational AI assistant that performs question-answering for memory augmentation.</p>
<p>Results</p>
<p>Episodic Memory Questions</p>
<p>Figure 5 illustrates a comparison of scores for the memory augmentation agent's response and human responses across various episodic memory questions.To clarify how a rating score corresponds with a response, we relate findings to the scored responses.Consider an example question: "What was the person you interacted with wearing?"An answer of "grey t-shirt" would receive a score of 5/5, while a response of "grey" would score 4/5, and a completely unrelated answer, such as "red" or "I don't know" would receive a score of 1/5.For color and quantity questions, the closer the answer is to the actual response, the higher the score.For instance, "blue" would be rated around 4/5 in response to the color "green", and a response of "4" would get a score around 4/5 for a quantity of "5".The memory augmentation agent outperforms humans in general, as evidenced by the performance shown in Figure 5, particularly for standard episodic memory questions.It excels in tasks that require detailed descriptions, as it demonstrates the capability to provide thorough descriptions of paintings.The ego-LLaVA decoding method surpasses human memory in capturing finer details.</p>
<p>However, the language encoding method exhibits a weakness in tasks that involve counting, such as determining the number of eggs or vases.Moreover, in tasks involving dynamic elements, such as the question "Where did you place the TV remote?", while the ego-LLaVA accurately identifies the TV remote, it encounters a failure mode in distinguishing between "pick up" and "place".Sometimes, it provides an answer based on the former action, and when participants do not place the TV remote in the same location, the agent may provide an incorrect response.We also observed that human memory is highly proficient in specific tasks but the participants tended to struggle with remembering things they did not pay attention to, such as the color of a kettle, or the presence of a guitar, when unrelated to their current tasks.Additionally, recalling a list of five movie names poses a challenge for human memory, resulting in subpar performance.</p>
<p>In summary, the memory augmentation agent proves useful in several scenarios, including: (1) memory-intensive tasks; (2) tasks requiring detailed descriptions; and (3) situations where people do not focus on, or pay attention to, certain aspects.The memory augmentation agent yielded a higher mean response with 4.1/5 compared to a human response of 2.5/5, indicating its superiority over human performance.A Friedman's test revealed a statistically significant difference between the memory augmentation and human responses (χ 2 = 37.928, df = 1, p = 0.0009), providing evidence to reject the null hypothesis of no difference.Further, the memory augmentation agent resulted in a smaller standard deviation (1.1/5, compared with 1.7/5 for humans), suggesting increased stability in responses compared to the less reliable human memory.Overall, the memory augmentation agent performs better and exhibits greater consistency compared to human responses.</p>
<p>Open-Ended Questions</p>
<p>Participants rated the responses provided by the memory augmentation agent in response to open-ended questions at an average score of 3.97/5, and a standard deviation of 0.604.This suggests the additional value of using a large language model like OpenAI GPT-4 as a conversational interface with access to an external knowledge base, enhancing its context awareness regarding memories.Some frequent questions are to recommend a movie or a book.Then the agent responded with "Certainly!Based on the context provided, I suggest you watch The Godfather.It's a classic and highly acclaimed film that you might enjoy.",or "I would recommend checking out The Dark Knight as it seems to be a popular choice in the given scenarios.It is a movie and comic book series that you might find interesting."These responses are usually rated as 5/5.There are also questions related to instructions, such as "How can I cook my egg better?".The memory augmentation agent responds by listing very detailed steps for cooking the eggs while also being aware of the quantity of the eggs the user cooks.Such responses are often rated as 4/5 to 5/5.</p>
<p>Post-Study Likert Scale Questions</p>
<p>Figure 6 shows the distribution of the participant responses for the memory augmentation agent.Participants value the memory augmentation agent, indicating a strong recognition of its utility in enhancing memory recall effectively.The accuracy of the information provided by the agent also received positive feedback, affirming its reliability.Responses about the agent's creativity, though mixed, still highlight a level of engagement that could be built upon.Some participants showed a willingness to wear an always-on camera, suggesting an openness to integrating this technology into daily life despite potential privacy concerns.This acceptance is mirrored in their comfort with others using the technology, pointing towards its feasibility in social or communal settings.</p>
<p>Open-Ended Survey</p>
<p>From the open-ended survey, we observed that participants emphasized the importance and value of having a memory augmentation agent, highlighting various applicable scenarios such as biomedical experiments, conferences, attending lectures/meetings, and exploring new places (P1, P3, P4, P6, P7, P10, P11, P12).However, participants expressed concerns about the agent being socially awkward to wear (P1, P2, P6).Ethical concerns were also raised, such   as the potential degradation of people's memorization capabilities if they rely solely on the agent (P4, P10).Additionally, some participants highlight privacy issues concerning individuals donning it and recording their actions.For example, P3 noted that "the agent's powerful and accurate capabilities could pose safety risks if breached", while P2 expressed worries about "discomfort with others wearing the system and recording their activities".However, as the understanding of the agent's function through language encoding grew, the majority of concerns diminished (P1, P3, P5, P6, P7, P8, P12), although a few participants still felt uneasy (P1, P2, P9, P11).Participants proposed specific improvements to address these issues, including incorporating indicators to make people aware of the agent's operation and limiting its use to specific scenarios such as teaching and conferences rather than everyday life (P1, P3, P4, P12).Additionally, participants suggested making the always-on camera device as lightweight and inconspicuous as possible to minimize social awkwardness and increase social acceptance (P2, P5, P6, P8, P9, P11).</p>
<p>DISCUSSION</p>
<p>In this paper we have demonstrated a novel memory augmentation agent and demonstrated its performance.Besides being lightweight, as it is reliant on language-encoding as opposed to vision-based, we discuss three additional advantages of this approach: performance, privacy, and device agnosticism.</p>
<p>Ego-LLaVA Paired with Language Encoding</p>
<p>Achieved State-of-the-Art in QA-Ego4D</p>
<p>The evaluation of the system using the QA-Ego4D dataset demonstrated superior performance, surpassing traditional machine learning models with a substantial BLEU score of 8.3.On the other hand, the baseline models exhibit subpar performance on the EMQA task due to primarily three factors.First, memory constraints: the EMQA task imposes a constant-size memory constraint, requiring models to compress all potentially relevant information into a fixed-size representation.Baseline models not designed to handle this constraint may struggle to effectively manage and utilize the limited memory space.Second, model specialization: certain baseline models, such as STM, use a simplistic implementation that may not be sufficient for the EMQA task.For example, STM's reliance on a single hidden vector in its implementation could account for its low performance.Third, relevance selection: the EMQA task necessitates the model to appropriately select relevant information due to the memory constraints.This could be a challenge for some models, leading to poor performance.</p>
<p>Language Encoding is Lightweight</p>
<p>In this paper, the Language Encoding Approach and the Visionbased Approach are compared.Language Encoding stores textual data from video, requiring around 0.517 TB/year uncompressed, reduced to 0.246-0.345TB with compression, while the Visionbased Approach needs 5.74 TB/year for low bitrate 720p video.</p>
<p>Language Encoding is Device Agnostic</p>
<p>For question and answering, the language encoding approach introduced in this paper gives rise device agnosticism due to its design.This contrasts with vision-based QA models which may exhibit diminished accuracy, or necessitate fine-tuning, when transitioning across different devices.Moreover, the device-agnostic nature is carried through in our language encoding model.The novel egocentric vision language model we introduced in this paper is cultivated using a diverse array of devices including GoPro, Vuzix Blade, Pupil Labs, ZShades, ORDRO EP6, iVue Rincon 1080, and Weeview, to capture egocentric videos.This breadth of training sources fortifies our framework's compatibility with any device capable of delivering egocentric video streaming.While we use the HoloLens 2 as the AR headset, its usage is solely as a conduit for streaming egocentric videos, further illustrating the adaptability of the model.</p>
<p>LIMITATIONS AND FUTURE WORK</p>
<p>We introduce an encoding method that operates on an individual frame basis.However, this method struggles to capture temporal correlations, which are essential for understanding dynamic features like activities. Figure 5 shows the lower performance on the two questions related to dynamic elements, including "place" and "cook".Such movements or scene changes, are better understood when temporal correlations between frames are considered.Without this, the encoding method may miss these dynamic subtleties, leading to a significant loss of crucial information.Activities usually occur over a series of frames.Ignoring temporal correlations can make it challenging to fully understand these activities.For example, the action of a person picking up an object involves a sequence of movements across several frames.Despite these shortcomings, this encoding method excels in capturing static features, as each frame is encoded separately.Another limitation is that while human memory augmentation has been shown to augment users' memory in a controlled lab setting, as suggested by the results in Study 2, we acknowledge that this has not been tested in real-world scenarios.Additionally, a longitudinal study is lacking to assess its long-term effectiveness.</p>
<p>Looking forward, we suggest research could focus on two main areas.First, to develop a high-resolution, accurate egocentric visionlanguage model.This model will integrate vision and language processing from a first-person perspective, similar to human perception of their surroundings.The high-resolution aspect of this model will allow it to capture intricate details in visual data, potentially resulting in significant performance improvements.Second, to create a vision-language model that can process dynamic video data.This model will provide a more holistic view of the environment by capturing temporal changes and movements over time.This approach will yield richer and more contextual information, enhancing the model's understanding of the visual scene.However, it is important to note that processing video data is more complex and computationally demanding than handling static images, presenting its own unique challenges.</p>
<p>CONCLUSION</p>
<p>We have presented, memory augmentation agent, a novel system that combines language encoding with episodic memory tasks.The agent incorporates several essential elements made possible using natural vision language encoders, including other features, such as long-term memory integration, a lightweight implementation using hosted APIs, and real-time inference capabilities.This integration of language encoding and episodic memory represents a state-of-the-art approach in the field, enabling enhanced performance and efficiency.Moreover, while privacy is a significant concern in lifelogging and memory augmentation agents, the use of language encoding presents a viable solution forward.By transforming video data into language data and carefully managing the encoding process, privacy can be effectively safeguarded while still providing a useful tool for memory augmentation.The agent was evaluated to verify that it is effective and efficient.In comparison to traditional machine learning models, the new model demonstrated superior performance on the QA-Ego4D dataset.Further, a user study revealed that the agent yielded statistically significant better performance when handling episodic memory tasks.Taken together, the results demonstrate the memory augmentation agent to be viable in practical applications and another example of how AI can work in tandem with AR to create new ways of supporting users in their daily lives.</p>
<p>Figure 2 :
2
Figure2: The Egocentric Vision-Language Model is developed through a process called fine-tuning.This process involves extracting knowledge from a large model and transferring it to smaller models, resulting in improved accuracy and faster inference times.The Egocentric Vision-Language Model combines the power of vision and language to effectively process and understand egocentric video data.13B and 7B refer to large language models with 13 billion and 7 billion parameters.</p>
<p>Figure 4 :
4
Figure 4: The settings of the different scenarios and the duration of each scenario.</p>
<p>6 .
6
Open-ended survey: Finally, we asked four open-ended questions: (1) Under what circumstances would you use this memory augmentation feature?; (2) Do you have any concerns regarding the memory augmentation capability?; (3) What improvements would you suggest for the memory augmentation agent?and (4) Do you have any other feedback or suggestions regarding the memory augmentation feature?</p>
<p>Figure 5 :
5
Figure 5: Comparative analysis of scores for the memory augmentation agent and Human responses across various questions.Each question has multiple pairs of AI and human scores represented by the bars.The x-axis enumerates different questions, while the y-axis shows the scores ranging from 1 to 5. The bars are color-coded, with one color representing AI and another representing human scores.The legend on the top-right corner outside the plot area distinguishes between AI and human bars.</p>
<p>Figure 6 :
6
Figure 6: The five-point Likert responses to the post-study questionnaire.Q1.The memory augmentation capability is valuable; Q2.The information provided by the memory augmentation agent is accurate; Q3.The response to my open-ended question by the memory augmentation agent is creative; Q4.I am willing to wear an always-on camera for memory augmentation through language encoding; Q5.I am willing for others in my close vicinity to wear an always-on camera for memory augmentation through language encoding.</p>
<p>Table 1 :
1.817.626.2LT-CT [46]5.318.527.5RM [59]4.517.726.6Language-Encoded QA (with Video-LLaMA [58])5.819.330.7Language-Encoded QA (with LLaVA [34])7.436.150.7Language-Encoded QA (With Ego-LLaVA)8.342.354.7
EMQA results on the QA-Ego4D test set.</p>
<p>Table 2 :
2
Where did you place the TV remote?Name the list of movies you browsed?What was the dominant color of the painting?How many eggs did you cook?Name the book you read?What color was the guitar beside the painting?What was the person you interacted with wearing?What is the color of the kettle beside the pan?How many vases did you see on the dining table?Describe the painting in detail.
A survey of augmented reality. R T Azuma, Presence: Teleoperators &amp; Virtual Environments. 641997</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization. the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization2005</p>
<p>Where did i leave my keys?-episodicmemory-based question answering on egocentric videos. L Bärmann, A Waibel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Everyday concept detection in visual lifelogs: validation, relationships and trends. D Byrne, A R Doherty, C G Snoek, G J Jones, A F Smeaton, Multimedia Tools and Applications. 201049</p>
<p>. H Chase, Langchain, 2022. May-202325</p>
<p>Augmenting human memory using personal lifelogs. Y Chen, G J Jones, Proceedings of the 1st augmented human international conference. the 1st augmented human international conference2010</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>Chroma-core/chroma: A vector database. C Core, Online. 2023</p>
<p>Inclusive ar/vr: accessibility barriers for immersive technologies. C Creed, M Al-Kalbani, A Theil, S Sarcar, I Williams, Universal Access in the Information Society. 2312024</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. T Dao, D Fu, S Ermon, A Rudra, C Ré, Advances in Neural Information Processing Systems. 202235</p>
<p>Security and privacy implications of pervasive memory augmentation. N Davies, A Friday, S Clinch, C Sas, M Langheinrich, G Ward, A Schmidt, 10.1109/MPRV.2015.13IEEE Pervasive Computing. 142015</p>
<p>Memory augmentation through lifelogging: opportunities and challenges. Technology-Augmented Perception and Cognition. T Dingler, P E Agroudy, R Rzayev, L Lischke, T Machulla, A Schmidt, 2021</p>
<p>Experiences of aiding autobiographical memory using the sensecam. A R Doherty, K Pauly-Takacs, N Caprani, C Gurrin, C J Moulin, N E O'connor, A F Smeaton, Human-Computer Interaction. 271-22012</p>
<p>G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. F Faul, E Erdfelder, A.-G Lang, A Buchner, 2007</p>
<p>Tall: Temporal activity localization via language query. J Gao, C Sun, Z Yang, R Nevatia, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)2017</p>
<p>Ego4d: Around the world in 3,000 hours of egocentric video. K Grauman, A Westbury, E Byrne, Z Chavis, A Furnari, R Girdhar, J Hamburger, H Jiang, M Liu, X Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Hybrid computing using a neural network with dynamic external memory. A Graves, G Wayne, M Reynolds, T Harley, I Danihelka, A Grabska-Barwińska, S G Colmenarejo, E Grefenstette, T Ramalho, J Agapiou, Nature. 53876262016</p>
<p>A privacy by design approach to lifelogging. C Gurrin, R Albatal, H Joho, K Ishii, Digital enlightenment yearbook. IOS Press2014. 2014</p>
<p>Lifelogging: Personal big data. C Gurrin, A F Smeaton, A R Doherty, Foundations and Trends® in information retrieval. 812014</p>
<p>Exploring the technical challenges of large-scale lifelogging. C Gurrin, A F Smeaton, Z Qiu, A R Doherty, ACM Transactions on Intelligent Systems and Technology. 2013</p>
<p>Remembering through lifelogging: A survey of human memory augmentation. M Harvey, M Langheinrich, G Ward, Pervasive and Mobile Computing. 272016</p>
<p>The personal audio loop: Designing a ubiquitous audio-based memory aid. G R Hayes, S N Patel, K N Truong, G Iachello, J A Kientz, R Farmer, G D Abowd, Mobile Human-Computer Interaction-MobileHCI 2004: 6th International Symposium, Mobile-HCI. Glasgow, UKSpringerSeptember 13-16, 2004. 20046</p>
<p>Sensecam: A retrospective memory aid. S Hodges, L Williams, E Berry, S Izadi, J Srinivasan, A Butler, G Smyth, N Kapur, K Wood, UbiComp 2006: Ubiquitous Computing: 8th International Conference. UbiComp; Orange County, CA, USASpringer2006. September 17-21, 2006. 20068</p>
<p>External memory aids and their relation to memory. M J Intons-Peterson, Cognitive psychology applied. Psychology Press2014</p>
<p>External memory aids: Effects and effectiveness. M J Intons-Peterson, G L Newsome, Iii , Memory improvement: Implications for memory theory. Springer1992</p>
<p>Navig: Guidance system for the visually impaired using virtual augmented reality. B F Katz, F Dramas, G Parseihian, O Gutierrez, S Kammoun, A Brilhault, L Brunet, M Gallay, B Oriola, M Auvray, Technology and Disability. 2422012</p>
<p>What memory is. S B Klein, Wiley Interdisciplinary Reviews: Cognitive Science. 612015</p>
<p>Densecaptioning events in videos. R Krishna, K Hata, F Ren, L Fei-Fei, J Carlos Niebles, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)2017</p>
<p>Self-attentive associative memory. H Le, T Tran, S Venkatesh, International Conference on Machine Learning. PMLR2020</p>
<p>Impact of video summary viewing on episodic memory recall -design guidelines for video summarizations. H V Le, S Clinch, C Sas, T Dingler, N Henze, N Davies, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsACM2016</p>
<p>Hierarchical conditional relation networks for video question answering. T Le, H Nguyen, L Duan, D Q Pham, M L Shyu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>Tvqa+: Spatio-temporal grounding for video question answering. J Lei, L Yu, T Berg, M Bansal, Proceedings of the Association for Computational Linguistics (ACL). the Association for Computational Linguistics (ACL)2020</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text Summarization Branches Out. Association for Computational Linguistics2004</p>
<p>H Liu, C Li, Q Wu, Y J Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>Augmented reality powers a cognitive assistant for the blind. Y Liu, N R Stiles, M Meister, 2018ELife7e37841</p>
<p>Tinnirello. A navigation and augmented reality system for visually impaired people. A Lo Valvo, D Croce, D Garlisi, F Giuliano, L Giarré, I , Sensors. 21930612021</p>
<p>Sensecam. Microsoft Research2004Month Year</p>
<p>A taxonomy of mixed reality visual displays. P Milgram, F Kishino, IEICE TRANSACTIONS on Information and Systems. 77121994</p>
<p>Watch, listen, and answer: Openended videoqa with modulated multi-stream 3d convnets. T Miyanishi, M Kawanabe, European Signal Processing Conference. 2021</p>
<p>Model parallel transformers from 100 million to 7 billion parameters. Mosaicml, Mpt, 2022</p>
<p>Just ask: An interactive learning framework for vision and language navigation. J Mun, P H Seo, I Jung, B Han, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020</p>
<p>. Nvidia, Fastertransformer, 2023</p>
<p>Gpt-4: The fourth generation of generative pre-trained transformers. 2023OpenAI BlogOpenAI</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>A call for clarity in reporting BLEU scores. M Post, 10.18653/v1/W18-6319Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational LinguisticsOct. 2018</p>
<p>Compressive transformers for long-range sequence modelling. J W Rae, A Potapenko, S M Jayakumar, T P Lillicrap, arXiv:1911.055072019arXiv preprint</p>
<p>Grounding action descriptions in videos. M Regneri, M Rohrbach, D Wetzel, S Thater, B Schiele, M Pinkal, Transactions of the Association for Computational Linguistics. 12013</p>
<p>Rate of forgetting is independent of initial degree of learning. K Rivera-Lares, R Logie, A Baddeley, 10.3758/s13421-021-01271-1Memory &amp; Cognition. 502022</p>
<p>Nevermind: Using augmented reality for memorization. O Rosello, M Exposito, P Maes, Adjunct Proceedings of the 29th Annual ACM Symposium on User Interface Software and Technology. 2016</p>
<p>. Sharegpt, Sharegpt, 2023</p>
<p>The term vector database: fast access to indexing terms for web pages. R Stata, K Bharat, F Maghoul, Computer Networks. 331-62000</p>
<p>Video question answering: A survey of models and datasets. G Sun, L Liang, T Li, B Yu, M Wu, B Zhang, 2021Mobile Networks and Applications26</p>
<p>Aiguide: An augmented reality hand guidance application for people with visual impairments. N D Troncoso Aldas, S Lee, C Lee, M B Rosson, J M Carroll, V Narayanan, Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility. the 22nd International ACM SIGACCESS Conference on Computers and Accessibility2020</p>
<p>J Wang, Z Yang, X Hu, L Li, K Lin, Z Gan, Z Liu, C Liu, L Wang, arXiv:2205.14100Git: A generative image-to-text transformer for vision and language. 2022arXiv preprint</p>
<p>Lifelogging: You're wearing a camera?. K Wolf, A Schmidt, A Bexheti, M Langheinrich, IEEE Pervasive Computing. 1332014</p>
<p>mplug-2: A modularized multi-modal foundation model across text. H Xu, Q Ye, M Yan, Y Shi, J Ye, Y Xu, C Li, B Bi, Q Qian, W Wang, arXiv:2302.004022023arXiv preprint</p>
<p>Msr-vtt: A large video description dataset for bridging video and language. J Xu, T Mei, T Yao, Y Rui, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Video-llama: An instruction-finetuned visual language model for video understanding. H Zhang, X Li, L Bing, 2023</p>
<p>Learning to rehearse in long sequence memorization. Z Zhang, C Zhou, J Ma, Z Lin, J Zhou, H Yang, Z Zhao, 2021PMLR</p>
<p>D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.10592Minigpt-4: Enhancing vision-language understanding with advanced large language models. 2023arXiv preprint</p>
<p>Memory: An extended definition. G Zlotnik, A Vansintjan, Frontiers in psychology. 2019102523</p>            </div>
        </div>

    </div>
</body>
</html>
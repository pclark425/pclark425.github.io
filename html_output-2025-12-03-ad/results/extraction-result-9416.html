<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9416 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9416</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9416</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-164.html">extraction-schema-164</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-278788464</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16477v1.pdf" target="_blank">Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</a></p>
                <p><strong>Paper Abstract:</strong> With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method. LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology. However, challenges such as hallucinations and reliability persist. In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility. With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively. However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9416.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9416.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>fMRI hypothesis-likelihood selection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-assisted selection of high-likelihood functional hypotheses for fMRI voxels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper describes work where LLMs propose functional hypotheses for brain voxels (from fMRI data) and hypotheses are ranked by calculating the probability of observing the activation data given each hypothesis; high-likelihood hypotheses are selected to guide further hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper reports a workflow in which LLM(s) generate candidate functional explanations for voxel activations; the candidates are then evaluated by computing the likelihood of the observed fMRI data under each hypothesis. Specific model architecture, training data, and size are not specified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Functional explanation(s) of human brain voxels (i.e., which function/hypothesis best explains observed fMRI activation patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>LLM generates candidate hypotheses; each hypothesis is evaluated by computing P(data | hypothesis) (the probability / likelihood of observing the fMRI activation data given the hypothesis). High-likelihood hypotheses are selected.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Likelihoods / probabilities of observed data given a hypothesis (numerical probability / likelihood scores used to rank hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Hypotheses are evaluated by their computed likelihood P(data | hypothesis); top-ranked hypotheses are taken forward for further hypothesis generation and validation. The paper does not report specific calibration metrics (e.g., Brier score) for these likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The review states that this approach was used to select high-probability hypotheses and to aid in generating improved hypotheses, but provides no numerical accuracy, calibration metrics, or quantitative performance numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The review notes reliance on LLM-generated hypotheses and on the correctness of the likelihood computation; potential errors can arise from LLM hallucinations, mis-specified hypotheses, and domain mismatch (LLMs not tailored to neuroimaging). No calibration or reliability numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>No explicit quantitative comparison to human experts, classical statistical model selection, or other automated methods is given in this review (only the approach is described and referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>The review suggests iterative validation loops, formal-system checks where possible, use of domain-specific models, RAG for grounding, and human-in-the-loop validation to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9416.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9416.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency voting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistent / majority-vote sampling to estimate uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper discusses the self-consistent method in which an LLM answers the same question multiple times and the most frequent answer is chosen, with the distribution of answers used as an empirical estimate of uncertainty or confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) (examples in literature include GPT-family models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auto-regressive transformer LLM(s) sampled repeatedly (temperature/noise settings unspecified) to produce multiple independent outputs; frequency/majority across samples is used as a proxy confidence measure.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Used to estimate the probability/confidence that an LLM-generated answer (or hypothesis) is correct; not tied in the review to a single specific future scientific discovery but proposed as a general uncertainty-estimation technique for LLM outputs in scientific workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Repeated sampling from the LLM for the same prompt and taking the empirical frequency (majority vote) of each distinct answer; the frequency serves as a surrogate probability or confidence score.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Empirical frequency / percentage (i.e., fraction of generated outputs endorsing a given answer/hypothesis).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>The paper reports that this approach can improve answer selection and provide uncertainty cues; no specific calibration metrics (Brier score, log-likelihood) or detailed numeric evaluations are given in this review, though the referenced literature provides empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Qualitatively reported to improve robustness relative to single-shot answers and to provide an uncertainty signal; the review does not include numerical accuracy or calibration statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Empirical frequencies are not guaranteed to be well-calibrated probabilities; repeated sampling increases compute cost; majority frequency can still be confidently wrong if the model's distribution is biased; method does not fully solve hallucination or deeper reasoning failures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared qualitatively to single-shot LLM outputs; reported to be better in many cited studies, but no quantitative numbers are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Combine with chain-of-thought, RAG grounding, ensemble of different models/agents, calibration procedures and external verification (formal checks or tool execution) to improve reliability and calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9416.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9416.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (multi-agent verification of LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The review describes methods where separate LLM agents or verification steps decompose and check claims (chain-of-verification), using multiple agents and retrieval to reduce hallucination and improve factual reliability of generated hypotheses/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Verification Reduces Hallucination in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM(s) used as primary generator and verifier agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi-agent architecture in which one or more LLMs generate candidate outputs and additional LLM agents (or retrieval and factual-checking tools) individually verify components (fact checkpoints) and feed back corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Verification / confidence in factual claims or scientific hypotheses produced by LLMs; used to filter or correct outputs prior to downstream experiments or communications.</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Decompose outputs into verifiable checkpoints; use separate LLM agents and retrieval-augmented evidence to check each checkpoint and update the original answer (verification score aggregated across checkpoints can be used as a reliability signal).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Not strictly a probability in the review; outputs produce verification judgments per checkpoint that can be aggregated into a composite confidence or pass/fail signal (binary verification or score).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>The review cites that chain-of-verification reduces hallucination in cited works; evaluation typically uses factuality benchmarks and comparison of verified vs. unverified outputs, but the review itself provides no numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>Reported qualitatively to reduce hallucinations and improve factuality and trustworthiness of outputs; no numerical calibration or predictive-performance numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Verification depends on the verifier agent(s) and retrieval quality; if verifiers share the same model biases they may not reliably detect falsehoods; verification steps add computational overhead and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to single-agent generation (no verification) and to simple retrieval grounding (RAG); chain-of-verification is reported as an improvement qualitatively but no head-to-head metrics are supplied in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Combine verifier agents with external authoritative sources (RAG), incorporate formal-validation tools where possible, and use diverse verifier architectures to reduce correlated failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9416.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9416.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to predict or assign probabilities to specific future real-world scientific discoveries, including how the probabilities are generated, how accuracy is evaluated, and any results, limitations, or comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal verification via LEAN / AlphaProof-style feedback</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-formalization and formal-system verification (e.g., LEAN) to evaluate LLM-generated conjectures / proofs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper highlights approaches that translate LLM-generated hypotheses/solutions into formal languages (e.g., LEAN) and use theorem provers/compilers to accept or reject them, using the binary verification outcome as a feedback signal to train/refine models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs fine-tuned with reinforcement learning from formal-verifier feedback (example class exemplified by AlphaProof-style systems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are trained or fine-tuned to produce formal-language proofs/statements; the formal verifier (e.g., LEAN compiler) deterministically checks correctness and provides binary feedback that is used as a learning signal (reinforcement learning). Specific model architectures and sizes are not detailed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Correct formal proofs / correctness of formalized mathematical conjectures (in cited work, performance on IMO-style problems rather than future empirical scientific discoveries).</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_method</strong></td>
                            <td>Translate LLM natural-language outputs into a formal language (auto-formalization) and run a formal proof checker; acceptance by the checker indicates correctness (binary), and this signal is used to refine the model (e.g., reinforcement learning).</td>
                        </tr>
                        <tr>
                            <td><strong>probability_format</strong></td>
                            <td>Binary accept / reject from the formal checker; in some pipelines aggregates of accepted solutions or success rates across problems are used as performance metrics rather than calibrated probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Deterministic verification via the formal system (proof accepted or rejected); performance reported as success rates on formalized problem sets and competition benchmarks (e.g., IMO-level problems in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The review reports that AlphaProof-style approaches achieved IMO-competition-level performance (noted as silver-medal standard in cited work), demonstrating that formal-verifier feedback can greatly improve correctness on formal tasks. No calibrated probability outputs for real-world scientific-discovery forecasting are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>This approach requires feasible formalizations of problems; many empirical scientific hypotheses cannot be fully formalized, limiting the method's applicability for real-world discovery forecasting. Auto-formalization itself can introduce translation errors and is resource-intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to purely natural-language LLM evaluation, formal verification provides a stronger, deterministic correctness signal; the review notes major performance gains in formal domains but cautions about domain applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>methods_for_improvement</strong></td>
                            <td>Improving auto-formalization, integrating formal checks with probabilistic calibration for non-formal domains, and hybrid pipelines that combine formal verification where possible with empirical validation and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Verification Reduces Hallucination in Large Language Models <em>(Rating: 2)</em></li>
                <li>Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment <em>(Rating: 2)</em></li>
                <li>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation <em>(Rating: 2)</em></li>
                <li>Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization <em>(Rating: 1)</em></li>
                <li>AlphaProof: AI achieves silver-medal standard solving International Mathematical Olympiad problems <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9416",
    "paper_id": "paper-278788464",
    "extraction_schema_id": "extraction-schema-164",
    "extracted_data": [
        {
            "name_short": "fMRI hypothesis-likelihood selection",
            "name_full": "LLM-assisted selection of high-likelihood functional hypotheses for fMRI voxels",
            "brief_description": "The paper describes work where LLMs propose functional hypotheses for brain voxels (from fMRI data) and hypotheses are ranked by calculating the probability of observing the activation data given each hypothesis; high-likelihood hypotheses are selected to guide further hypothesis generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s)",
            "model_description": "The paper reports a workflow in which LLM(s) generate candidate functional explanations for voxel activations; the candidates are then evaluated by computing the likelihood of the observed fMRI data under each hypothesis. Specific model architecture, training data, and size are not specified in this review.",
            "model_size": null,
            "prediction_target": "Functional explanation(s) of human brain voxels (i.e., which function/hypothesis best explains observed fMRI activation patterns).",
            "prediction_method": "LLM generates candidate hypotheses; each hypothesis is evaluated by computing P(data | hypothesis) (the probability / likelihood of observing the fMRI activation data given the hypothesis). High-likelihood hypotheses are selected.",
            "probability_format": "Likelihoods / probabilities of observed data given a hypothesis (numerical probability / likelihood scores used to rank hypotheses).",
            "evaluation_method": "Hypotheses are evaluated by their computed likelihood P(data | hypothesis); top-ranked hypotheses are taken forward for further hypothesis generation and validation. The paper does not report specific calibration metrics (e.g., Brier score) for these likelihoods.",
            "results": "The review states that this approach was used to select high-probability hypotheses and to aid in generating improved hypotheses, but provides no numerical accuracy, calibration metrics, or quantitative performance numbers in this paper.",
            "limitations_or_challenges": "The review notes reliance on LLM-generated hypotheses and on the correctness of the likelihood computation; potential errors can arise from LLM hallucinations, mis-specified hypotheses, and domain mismatch (LLMs not tailored to neuroimaging). No calibration or reliability numbers are provided.",
            "comparison_to_baselines": "No explicit quantitative comparison to human experts, classical statistical model selection, or other automated methods is given in this review (only the approach is described and referenced).",
            "methods_for_improvement": "The review suggests iterative validation loops, formal-system checks where possible, use of domain-specific models, RAG for grounding, and human-in-the-loop validation to improve reliability.",
            "uuid": "e9416.0",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-consistency voting",
            "name_full": "Self-consistent / majority-vote sampling to estimate uncertainty",
            "brief_description": "The paper discusses the self-consistent method in which an LLM answers the same question multiple times and the most frequent answer is chosen, with the distribution of answers used as an empirical estimate of uncertainty or confidence.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) (examples in literature include GPT-family models)",
            "model_description": "Auto-regressive transformer LLM(s) sampled repeatedly (temperature/noise settings unspecified) to produce multiple independent outputs; frequency/majority across samples is used as a proxy confidence measure.",
            "model_size": null,
            "prediction_target": "Used to estimate the probability/confidence that an LLM-generated answer (or hypothesis) is correct; not tied in the review to a single specific future scientific discovery but proposed as a general uncertainty-estimation technique for LLM outputs in scientific workflows.",
            "prediction_method": "Repeated sampling from the LLM for the same prompt and taking the empirical frequency (majority vote) of each distinct answer; the frequency serves as a surrogate probability or confidence score.",
            "probability_format": "Empirical frequency / percentage (i.e., fraction of generated outputs endorsing a given answer/hypothesis).",
            "evaluation_method": "The paper reports that this approach can improve answer selection and provide uncertainty cues; no specific calibration metrics (Brier score, log-likelihood) or detailed numeric evaluations are given in this review, though the referenced literature provides empirical tests.",
            "results": "Qualitatively reported to improve robustness relative to single-shot answers and to provide an uncertainty signal; the review does not include numerical accuracy or calibration statistics.",
            "limitations_or_challenges": "Empirical frequencies are not guaranteed to be well-calibrated probabilities; repeated sampling increases compute cost; majority frequency can still be confidently wrong if the model's distribution is biased; method does not fully solve hallucination or deeper reasoning failures.",
            "comparison_to_baselines": "Compared qualitatively to single-shot LLM outputs; reported to be better in many cited studies, but no quantitative numbers are provided in this review.",
            "methods_for_improvement": "Combine with chain-of-thought, RAG grounding, ensemble of different models/agents, calibration procedures and external verification (formal checks or tool execution) to improve reliability and calibration.",
            "uuid": "e9416.1",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Chain-of-Verification",
            "name_full": "Chain-of-Verification (multi-agent verification of LLM outputs)",
            "brief_description": "The review describes methods where separate LLM agents or verification steps decompose and check claims (chain-of-verification), using multiple agents and retrieval to reduce hallucination and improve factual reliability of generated hypotheses/answers.",
            "citation_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "mention_or_use": "mention",
            "model_name": "unspecified LLM(s) used as primary generator and verifier agents",
            "model_description": "A multi-agent architecture in which one or more LLMs generate candidate outputs and additional LLM agents (or retrieval and factual-checking tools) individually verify components (fact checkpoints) and feed back corrections.",
            "model_size": null,
            "prediction_target": "Verification / confidence in factual claims or scientific hypotheses produced by LLMs; used to filter or correct outputs prior to downstream experiments or communications.",
            "prediction_method": "Decompose outputs into verifiable checkpoints; use separate LLM agents and retrieval-augmented evidence to check each checkpoint and update the original answer (verification score aggregated across checkpoints can be used as a reliability signal).",
            "probability_format": "Not strictly a probability in the review; outputs produce verification judgments per checkpoint that can be aggregated into a composite confidence or pass/fail signal (binary verification or score).",
            "evaluation_method": "The review cites that chain-of-verification reduces hallucination in cited works; evaluation typically uses factuality benchmarks and comparison of verified vs. unverified outputs, but the review itself provides no numeric metrics.",
            "results": "Reported qualitatively to reduce hallucinations and improve factuality and trustworthiness of outputs; no numerical calibration or predictive-performance numbers provided in this paper.",
            "limitations_or_challenges": "Verification depends on the verifier agent(s) and retrieval quality; if verifiers share the same model biases they may not reliably detect falsehoods; verification steps add computational overhead and complexity.",
            "comparison_to_baselines": "Compared to single-agent generation (no verification) and to simple retrieval grounding (RAG); chain-of-verification is reported as an improvement qualitatively but no head-to-head metrics are supplied in the review.",
            "methods_for_improvement": "Combine verifier agents with external authoritative sources (RAG), incorporate formal-validation tools where possible, and use diverse verifier architectures to reduce correlated failures.",
            "uuid": "e9416.2",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Formal verification via LEAN / AlphaProof-style feedback",
            "name_full": "Auto-formalization and formal-system verification (e.g., LEAN) to evaluate LLM-generated conjectures / proofs",
            "brief_description": "The paper highlights approaches that translate LLM-generated hypotheses/solutions into formal languages (e.g., LEAN) and use theorem provers/compilers to accept or reject them, using the binary verification outcome as a feedback signal to train/refine models.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs fine-tuned with reinforcement learning from formal-verifier feedback (example class exemplified by AlphaProof-style systems)",
            "model_description": "LLMs are trained or fine-tuned to produce formal-language proofs/statements; the formal verifier (e.g., LEAN compiler) deterministically checks correctness and provides binary feedback that is used as a learning signal (reinforcement learning). Specific model architectures and sizes are not detailed in this review.",
            "model_size": null,
            "prediction_target": "Correct formal proofs / correctness of formalized mathematical conjectures (in cited work, performance on IMO-style problems rather than future empirical scientific discoveries).",
            "prediction_method": "Translate LLM natural-language outputs into a formal language (auto-formalization) and run a formal proof checker; acceptance by the checker indicates correctness (binary), and this signal is used to refine the model (e.g., reinforcement learning).",
            "probability_format": "Binary accept / reject from the formal checker; in some pipelines aggregates of accepted solutions or success rates across problems are used as performance metrics rather than calibrated probabilities.",
            "evaluation_method": "Deterministic verification via the formal system (proof accepted or rejected); performance reported as success rates on formalized problem sets and competition benchmarks (e.g., IMO-level problems in cited work).",
            "results": "The review reports that AlphaProof-style approaches achieved IMO-competition-level performance (noted as silver-medal standard in cited work), demonstrating that formal-verifier feedback can greatly improve correctness on formal tasks. No calibrated probability outputs for real-world scientific-discovery forecasting are reported.",
            "limitations_or_challenges": "This approach requires feasible formalizations of problems; many empirical scientific hypotheses cannot be fully formalized, limiting the method's applicability for real-world discovery forecasting. Auto-formalization itself can introduce translation errors and is resource-intensive.",
            "comparison_to_baselines": "Compared to purely natural-language LLM evaluation, formal verification provides a stronger, deterministic correctness signal; the review notes major performance gains in formal domains but cautions about domain applicability.",
            "methods_for_improvement": "Improving auto-formalization, integrating formal checks with probabilistic calibration for non-formal domains, and hybrid pipelines that combine formal verification where possible with empirical validation and human oversight.",
            "uuid": "e9416.3",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofverification_reduces_hallucination_in_large_language_models"
        },
        {
            "paper_title": "Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment",
            "rating": 2,
            "sanitized_title": "scientific_hypothesis_generation_by_a_large_language_model_laboratory_validation_in_breast_cancer_treatment"
        },
        {
            "paper_title": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation",
            "rating": 2,
            "sanitized_title": "the_virtual_lab_ai_agents_design_new_sarscov2_nanobodies_with_experimental_validation"
        },
        {
            "paper_title": "Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization",
            "rating": 1,
            "sanitized_title": "dont_trust_verify_grounding_llm_quantitative_reasoning_with_autoformalization"
        },
        {
            "paper_title": "AlphaProof: AI achieves silver-medal standard solving International Mathematical Olympiad problems",
            "rating": 2,
            "sanitized_title": "alphaproof_ai_achieves_silvermedal_standard_solving_international_mathematical_olympiad_problems"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
            "rating": 1,
            "sanitized_title": "crisprgpt_an_llm_agent_for_automated_design_of_geneediting_experiments"
        }
    ],
    "cost": 0.01656,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</p>
<p>Yanbo Zhang 
Allen Discovery Center at Tufts University
MedfordMAUSA</p>
<p>Sumeer A Khan 
Kingdom of Saudi Arabia 3. Intelligent Infrastructure Team
Living Systems Lab
KAUST
Thuwal</p>
<p>Network Rail
UK</p>
<p>SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence
23952ThuwalSaudi Arabia</p>
<p>Adnan Mahmud 
Department for AI in Society, Science, and Technology
Zuse Institute Berlin
Germany</p>
<p>Huck Yang 
NVIDIA Research 6. Biological and Environmental Science and Engineering Division
King Abdullah University of Science and Technology
ThuwalSaudi Arabia</p>
<p>Alexander Lavin 
Pasteur Labs
BrooklynNYUSA</p>
<p>Michael Levin 
Allen Discovery Center at Tufts University
MedfordMAUSA</p>
<p>Wyss Institute for Biologically Inspired Engineering
Harvard University
BostonMAUSA</p>
<p>Jeremy Frey 
Department of Chemistry
University of Southampton
University RoadSouthamptonHampshireUK</p>
<p>Jared Dunnmon 
Department of Biomedical Data Science
Stanford University
StanfordCAUSA</p>
<p>James Evans 
Department of Sociology
Knowledge Lab
University of Chicago
ILUSA</p>
<p>Santa Fe Institute
NMUSA</p>
<p>School of Informatics
The University of Edinburgh
UK</p>
<p>Department of Knowledge Technologies
Jo≈æef Stefan Institute
LjubljanaSlovenia</p>
<ol>
<li>Biological and Environmental Science and Engineering Division
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalSaudi Arabia</li>
</ol>
<p>Department of Medicine
Center for Molecular Medicine
Unit of Computational Medicine
Karolinska Institutet</p>
<p>Karolinska University Hospital
L8:05, SE-171 76StockholmSweden</p>
<ol>
<li>Computer, Electrical and Mathematical Sciences and Engineering Division
Science for Life Laboratory
King Abdullah University of Science and Technology (KAUST)
Saudi Arabia. 19, Tomtebodavagen 23A23955-6900, SE-17165Thuwal, SolnaSweden</li>
</ol>
<p>School of Biomedical Engineering and Imaging Sciences
Algorithmic Dynamics Lab, Research Departments of Biomedical Computing and Digital Twins
King's College London
UK</p>
<p>Institute for Artificial Intelligence
LondonUK</p>
<p>The Alan Turing Institute
LondonUK</p>
<p>Oxford Immune Algorithmics
Oxford University Innovation and London Institute for Healthcare Engineering
LondonUK</p>
<p>Cancer Interest Group
Francis Crick Institute
LondonUK</p>
<p>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery
D21A9C51027B9862C00BF620469151CD
With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method.LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology.However, challenges such as hallucinations and reliability persist.In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery.We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics.The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility.With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively.However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.</p>
<p>Introduction</p>
<p>Recent advances in artificial intelligence (AI) have transformed multiple areas of society, the world economy, and academic and scientific practice.Generative AI and Large Language Models (LLMs) present unprecedented opportunities to transform scientific practice, advance Science, and accelerate technological innovation.Nobel Prizes in Physics and Chemistry were awarded to several AI leaders for their contributions to AI and frontier models, such as Large Language Models (LLMs).This promises to transform or contribute to scientific research by enhancing productivity and supporting various stages of the scientific method.The use of AI in science is booming across numerous scientific areas and is impacting different parts of the scientific method.</p>
<p>Despite the potential of LLMs for hypothesis generation and data synthesis, AI and LLMs face challenges in fundamental science and scientific discovery.Hence, our premise in our perspective is that AI, in general, has so far been limited in its impact on fundamental science, which is defined here as the discovery of new principles or new scientific laws.</p>
<p>Here, we review how LLMs are currently used -as a technological tool -to augment the scientific process in practice and how they may be used in the future as they become more powerful tools and develop into powerful scientific assistants.Combining data-driven techniques with symbolic systems, such a system could fuse into hybrid engines that may lead to novel research directions.We aim to describe the gap between LLMs as technical tools and "creative engines" that could enable new high-quality scientific discoveries and pose novel questions and hypotheses to human scientists.We first review the current use of LLMs in Science, aiming to identify limitations that need to be addressed when moving toward creative engines.</p>
<p>There is solid recognition and excitement for the transformative potential of AI in Science.For example, leading machine learning conferences (NeurIPS, ICML) have recently (2021-2023) arranged targeted workshops on AI4Science.Some recent reviews and papers include  . Thisdemonstrates the energy and potential of using automated (i.e., AI tools) for Science.This "dream" can be traced back to the times of Turing and the emergence of Artificial Intelligence in the 1950s 39 .With recent advancements in computational techniques, vastly increased production of scientific data, and the rapid evolution of machine learning, this long-held vision can be transformed into reality.Yet, most current reviews and original papers focus on specifically designed machine learning architectures targeting particular application domains or problems.</p>
<p>For example, recent reviews have explored how to use variants of Deep Learning, Geometric Deep Learning, or Generative AI in its generality (including different architectures such as CNNs, GNNs, GANs, diffusion models, VAEs, and Transformers) as a tool for assisting Science 3,11,13,15,19,22 .For example, Wang et al. 1 , reviews breakthroughs in how specific techniques such as geometric deep learning, self-supervised learning, neural operators, and language modelling have augmented Science in protein folding, nuclear fusion, and drug discovery.An essential thread in their review is the vital notion of representation, pointing out that different AI architectures can support valuable representations of scientific data and thereby augment Science.Recent papers demonstrate the appeal and the potential of using AI-driven and augmented tools for automating science 1,4,13,40 .Traditional scientific advancements have been primarily driven by hypothesis-led experimentation and theoretical development, often limited by human cognitive capacities and manual data processing.For example, the formulation of Newtonian mechanics required meticulous observation and mathematical formalization over extended periods.Here, the rise of AI4Science represents a paradigmatic revolution that could reach beyond human cognitive limitations.AI-driven advancements promise to enable rapid processing and analysis of massive data sets, revealing complex patterns that surpass human analytical capabilities.For example, DeepMind's AlphaFold dramatically transformed protein structure prediction, a longstanding scientific challenge, using deep learning to predict protein folding accurately.Furthermore, AI4Science could reverse the slowdown in scientific productivity in recent years, where literature search and peer-review evaluation [41][42][43] are bottlenecks.</p>
<p>In contrast to previous reviews, here we first address the use of LLMs, regardless of the specific underlying architecture, and their use as a tool for the scientific process.We assess how different areas of science use LLMs in their respective scientific process.This analysis sets the stage for asking how LLMs can synthesize information, generate new ideas and hypotheses, guide the scientific method, and augment fundamental scientific discoveries.Here, we ask to what extent AI can be described as a "general method of invention," which could open up new paradigms and directions of scientific investigations.</p>
<p>Hence, complementary to a purely representational and architectural viewpoint of AI4Science, we find it constructive to ask and assess to what extent the nature of the scientific process, both its inductive and deductive components, can and should be transformed by AI techniques.</p>
<p>Current use of LLMs -From Specialised Scientific Copilots to LLMassisted Scientific Discoveries</p>
<p>The ability of Large Language Models (LLMs) to process and generate human-like text, handle vast amounts of data, and analyse complex patterns with potentially some reasoning capabilities has increasingly set the stage for them to be used in scientific research across various disciplines.Their applications range from simple tasks, such as acting as copilots to assist scientists, to complex tasks, such as autonomously performing experiments and proposing novel hypotheses.We will first introduce the fundamental concepts of LLMs and then review their various applications in scientific discovery.</p>
<p>Prompting LLMs: From Chatbot to Prompt Engineering</p>
<p>Current mainstream LLMs are primarily conditional generative models, where the input, such as the beginning of a sentence or instructions, serves as a condition, and the output is the generated text, such as a reply.This text is typically sampled auto-regressively: the next token (considered the building block of words) is sampled from a predicted distribution.See Figure 1A.Given LLMs' capabilities in computation and emerging potential for reasoning, which we define as the ability to solve tasks that require reasoning, they can be considered programming languages that use human language as the code that instructs them to perform desired tasks.This code takes the form of "prompts."For instruct-tuned LLMs, the prompt often consists of three parts: the system prompt and the user prompt, with an LLM's reply considered the assistant prompt.Hence, a chat is frequently composed of <system><user><assistant><user><assistant>, see Figure 1B.The system prompt typically includes general instructions for the LLMs, such as behaviour, meta-information, format, etc.The user prompt usually contains detailed instructions and questions.Using these prompts, the LLMs generate replies under the role of "assistant.</p>
<p>Since LLMs do not have background knowledge about the user, and prompts are their major input, designing a good prompt is often critical to achieving the desired output and superior performance.Researchers have shown that specific prompts, including accuracy, creativity, and reasoning, can significantly improve output performance.Specifically, the chain-of-thought (CoT) method 44 can instruct LLMs to think step-by-step, leading to better results.Beyond these, the Retrieval-augmented Generation (RAG) method 45 can incorporate a large amount of context by indexing the contents and retrieving relevant materials, then combining the retrieved information with prompts to generate the output.Due to the importance of prompts and LLM agents, designing prompts is now often called "prompt engineering," and many techniques and tricks have been developed in this area 46,47 , such as asking JSON format outputs, formulating clear instructions, setting temperatures, etc 47,48 .</p>
<p>While carefully designed prompts can accomplish many tasks, they are not robust and reliable enough for complex tasks requiring multiple steps or non-language computations, nor can they explore autonomously.LLM agents are developed for these requirements, especially for complex tasks.LLM agents are autonomous systems powered by LLMs, which can actively seek to observe environments, make decisions, and perform actions using external tools 49 .In many cases, we need to ensure reliability, achieve highperformance levels, enable automation, or process large amounts of context.These tasks cannot be accomplished solely with LLMs and require integrating LLMs into agent systems.Early examples include AutoGPT 50 and BabyAGI 51 , where LLMs are treated as essential tools within the agent system (Figure 1C).In scientific discovery, LLM agents become even more critical due to the complexity of science and its high-performance requirements.Many tools have also been developed to provide easy access to these prompting and agent methods, such as LangChain 52 and LlamaIndex 53 .Automated prompt design methods, such as DSPy 54 and TextGrad 55 , are also being developed to design prompts and LLM agents in a data-driven way.</p>
<p>LLMs as Practical Scientific Copilots</p>
<p>The ability of LLMs to work with a large body of text is being exploited in the practice of science.For example, LLMs assist in proposing novel ideas, writing scientific papers and generating computer code, thereby improving productivity; they also adapt texts for diverse audiences ranging from experts to broader audiences, thus supporting communication in science.</p>
<p>Furthermore, LLMs can sift through vast bodies of scientific literature to identify relevant papers, findings, and trends.Such reviewing of the relevant literature helps investigators quickly digest and identify gaps in enormous bodies of scientific knowledge.</p>
<p>These capabilities can also mitigate discursive barriers across different scientific fields, supporting interdisciplinary scientific collaborations and knowledge sharing.Recently, chatbots have emerged in several disciplines as virtual assistants answering scientific queries posed by scientists.Such tools exploit the power of LLMs to extract and detect patterns, data, and knowledge.These techniques may also serve as important tools in science education and communication.</p>
<p>These examples demonstrate the rise of LLMs in extracting and sharing information and the exciting open research frontier of the potential of reasoning that they represent in different scientific domains [56][57][58] .For instance, Caufield et al. proposed the SPIRES method 59 , which uses LLMs to extract structured data from the literature.Beyond data extraction, LLMs have also shown evidence of outperforming annotation tasks 60,61 , enabling scientists to scale data annotation.Some domain-specific models also show superior performance in classification, annotation, and prediction tasks [62][63][64] .With the help of RAG methods 45 , LLMs can directly apply their information extraction and distillation capabilities to large amounts of text data.With the combination of diverse capabilities of LLMs interconnected through LLM-agents, the recent "AI co-scientist 65 " demonstrates impressive ability in generating novel research ideas by leveraging existing literature, engaging in internal LLM-agent debates, and refining its outputs.This process leads to constructive progress when applied to real scientific tasks.</p>
<p>Moreover, LLMs are currently used to automate the experimental design and the execution of experiments.For example, Boiko, et al. 66 propose an autonomous LLM capable of performing chemical experiments.This work employs an LLM planner to manage the experimental process, such as drawing patterns on plates or conducting more complex chemical syntheses.Compared to hard-coded planners, the LLM-based planner is more flexible and can handle unexpected situations.Similar kinds of loop and tool usage are also shown in 67 , which includes literature tools, consulting with humans, experimental tools, and safety tools.</p>
<p>In the biological domain, for instance, the CRISPR-GPT 68</p>
<p>Foundation Models for Science</p>
<p>A key observation when using LLMs as clever text engines or exploiting the underlying machine learning (neural) architecture for solving specific scientific problems was the importance of scale.Larger models trained on larger amounts of data, or spending larger amounts of computation during inference time yielded an increase in performance 56,70,71 .</p>
<p>The discovery of such scaling laws 72 demonstrated that LLMs' performance improves as the number of parameters increases.Thus, we can expect the above trends to grow in importance as these systems are trained on ever larger amounts of data.Emergent behaviours, such as reasoning were suggested when models increased in scale 73 .</p>
<p>Concurrent with the appreciation of scaling laws came the realisation that instead of using LLMs for specialised problems or as text engines, one could potentially train them on large amounts of data, not necessarily text, but different modalities of scientific data.This is the idea of a foundation model.These are large-scale pre-trained models that, when trained with a sufficient amount of data of different types, such models "learn" or "encapsulate" knowledge of a large scientific domain, thus reaching beyond a specific scientific problem.</p>
<p>When fine-tuned to particular tasks, such models can solve a wide range of downstream tasks.The notion of foundation models refers to their generality in that they can be adapted to many different applications, unlike task-specific engineered models solving a specialised task such as protein folding.Notably, the famous transformer architecture that fuels LLMs has become the architecture of choice when constructing the foundational models in different domains of science.These self-supervised models are usually pretrained on extensive and diverse datasets.This enables them to learn from massive unlabelled data since masking parts of the data and then requiring the model to predict the occluded parts provides foundation models with their learning objective.This technique is used when training LLMs on large amounts of text.The idea is thus exploited in scientific domains where multi-modal data is used to train self-supervised foundation models.Once trained, the model can be fine-tuned for various downstream tasks without requiring additional training.Consequently, the same model can be applied to a wide range of downstream tasks.The foundation model encapsulates a large body of scientific "knowledge" inherent in the training data.</p>
<p>Leveraging these ideas, there has been a rise in the number of foundation models of science.For example, the Evo and Evo 2 models enable prediction and generation tasks from the molecular to the genome scale 74 .While Evo is trained on millions of prokaryotic and phage genomes, Evo 2 75 includes massive eukaryotic genomes, and both demonstrate zero-shot function prediction across DNA, RNA, and protein modalities.It excels at multimodal generation tasks, as shown by generating synthetic CRISPR-Cas molecular complexes and transposable systems.The functional activity of Evo-generated CRISPR-Cas molecular complexes and IS200 and IS605 transposable systems was experimentally validated, representing the first examples of protein-RNA and protein-DNA co-design using a language model.Similarly, scGPT is for learning single cell transcriptional data 76 , ChemBERT encodes molecular structures as strings, which then can be used for different downstream tasks such as drug discovery and material science 77 .Similarly, OmniJet-Œ± is the first cross-task foundation model in particle physics, enhancing performance with reduced training needs 78 .Additionally, multiple physics pretraining (MPP) introduces a task-agnostic approach to modelling multiple physical systems, improving predictions across various physics applications without extensive fine-tuning 79 .The LLM-SR 80 implements similar symbolic regression methods iteratively, generating and evaluating hypotheses, using the evaluation signal to refine and search for more hypotheses.</p>
<p>Incorporating diverse scientific data modalities, which represent different "languages" to interact with observations beyond natural language, is crucial.There are two major approaches emerging: 1) End-to-end training on domain-specific modalities: Models like ChemBERT 77 (using chemical SMILES strings) and scGPT 76 (using singlecell data), as mentioned above, are directly trained on these specialized data types.2)</p>
<p>Separate training with compositional capabilities: This involves training separate encoders for new modalities or enabling LLM agents to utilize tools that interact with these modalities.For instance, models like BiomedCLIP 81 connect biological images with natural language, while PaperCLIP 82 and AstroCLIP 83 link astronomical images and spectral data to textual descriptions.Furthermore, frameworks like ChemCrow 84 leverage the tool-using abilities of LLMs to connect with non-natural-language modalities, such as chemical analysis tools.</p>
<p>Yet, as with text-based LLMs, several challenges remain.These include potential biases in datasets, which can bias the performance and output of these models.Since science is mainly about understanding systems, the scale, and opaqueness of these models make interpretation a particularly challenging problem.Also, several observations, such as their capability for generalisation, multi-modality, and apparent emergent capabilities, have led to intense discussions at the research frontier on the extent to which these foundation models can reason within and beyond their training regimes.The text-based LLMs (or models incorporated with text modality) discussed above are constructed using these techniques.Examples include GPT-4 (OpenAI) 85 , BERT (Bidirectional Encoder Representation from Transformers) 86 , CLIP (Contrastive Language-Image Pre-training, OpenAI) 87 , and DALL-E from OpenAI 88 .</p>
<p>These foundation models have the potential to achieve professional human-level performance or even surpass human capabilities when trained using reinforcement learning, particularly with feedback from reliable formal systems.For example, AlphaProof 89 has become state-of-the-art in automated theorem-proving systems, achieving mathematical capabilities comparable to human competitors at IMO 2024.</p>
<p>Approximately one million informal mathematical problems were translated into the formal language LEAN, a mathematical proof verification language, enabling the LLM to be trained through reinforcement learning.Solutions generated by the LLM in LEAN are either proved or disproved by the LEAN compiler, with the resulting correct or incorrect solutions serving as feedback to refine the LLM.While this approach has been explicitly applied within the mathematical domain, it demonstrates significant potential for training LLMs to surpass human performance in highly complex and deductive reasoning.</p>
<p>Although developing formal systems for general tasks remains challenging, reinforcement learning methods are employed to build foundation models with enhanced deductive capabilities, leading to the rise of reasoning models such as OpenAI o1/o3 70 , Deepseek R1 56 , and others.In scientific domains such as physics, external and reliable feedback mechanisms are already used to improve answer quality 90 , highlighting the potential for creating domain-specific foundation models.</p>
<p>In conclusion, the rise of foundation models will continue to affect and disrupt science due to their powerful nature, scaling properties, and ability to handle very different data modalities.However, for our purposes, the question remains of what extent foundation models could be a proper gateway to making fundamental scientific discoveries.To what extent can foundation models be creative and reason outside their training domains?</p>
<p>Toward Large Language Models as Creative Engines for Fundamental Science</p>
<p>Here, we ask how AI can impact fundamental Science?That is, what is required for an AI to be able to discover new principles of scientific laws from observations, available conjectures, and data analysis?Broadly, can generative AI develop to become a "creative engine" that can make fundamental scientific discoveries and pose new questions and hypotheses?Einstein famously stated, "If I had an hour to solve a problem, I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions".This underscores the importance of carefully considering the question or problem itself, as posing hypotheses effectively can be the most intellectually demanding part of Science.As a first approximation, the ability to pose novel hypotheses is -at least for us humanswhat appears to be essential for making novel discoveries.Thus, what is required for an AI to advance beyond a valuable tool for text generation and engineered systems for solving a particular problem?Or could foundation models provide a possible path forward?</p>
<p>In our view, if LLMs are to contribute to fundamental Science, it is necessary to assess what putative roles LLMs can play in the core of the scientific process.To this end, we discuss below how LLMs can augment the scientific method.This includes how LLMs could support observations, automate experimentation, and generate novel hypotheses.We will also explore how human scientists can collaborate with LLMs.</p>
<p>Augmenting the Scientific Method</p>
<p>As a first approximation, scientific discovery can be described as a reward-searching process, where scientists propose hypothetical ideas and verify or falsify them through experiments 91 .Under this Popperian formulation, LLMs can assist scientific discovery in two ways (Figure 2): On the one hand, LLMs could assist in the hypothesis-proposing stage, helping scientists find novel, valuable, or potentially high-reward directions or even propose hypotheses that human scientists might have difficulty generating.On the other hand, LLMs have the potential to make experiments more efficient, accelerate the search process, and reduce experimental costs.</p>
<p>At the stage of proposing hypotheses, scientists choose unknown areas to explore, which requires a deep command of domain knowledge, incorporating observational data, and manipulating existing knowledge in novel ways 45,92 .Their expertise and creativity could carry the potential for proposing novel research hypotheses.</p>
<p>Then, at the verification stage, experiments are conducted to obtain relevant information and test hypotheses.This requires the ability to plan and design experiments effectively.Given LLMs' planning capabilities and potential understanding of causality 93- 95 , they can help scientists design experiments.By incorporating tool-using abilities 96 , LLMs can directly implement experiments.LLM agents can perform complex workflows and undertake repetitive explorations that are time-consuming for human scientists.This allows us to search for novel results efficiently, which is key to scientific discovery 97,98 .This process often involves a trial-and-error loop for a research topic or question.Thus, scientific discovery requires the following steps: observation, hypothesis proposal, experimentation, and automating the loop.</p>
<p>Expanding or Narrowing the Observation Process</p>
<p>Scientists rely upon observational results for guidance in proposing hypotheses, designing and refining experiments, evaluating experimental results, and validating their hypotheses.</p>
<p>In general, observations act as dimension reduction methods 99 , which include annotating, classification, and information extraction.</p>
<p>General purpose LLMs, such as GPT-4, Llama, can be good observers for language and image data for general purposes.Their in-context and zero-shot learning capabilities can be used as universal classifiers to extract specific information from these data, such as annotation and evaluation.In domains like NLP and Social Science, annotating and evaluating language data at scale is a fundamental task for downstream experiments.</p>
<p>Trained humans or crowd-workers have often done such jobs.However, LLMs, such as ChatGPT, can perform higher or comparable performance levels relative to crowd-workers on annotation tasks, especially on more challenging tasks 60,61 .</p>
<p>Besides language processing, scientists must also describe complex behaviours at scale qualitatively.LLMs show potential in describing such complex black-box systems, where we observe only their inputs and outputs without knowledge of their underlying mechanisms.Although deciphering such systems can often become a stand-alone research question, having a qualitative description can still be helpful when faced with large-scale data.With LLMs, black-box systems, such as language input-output, mathematical inputoutput pairs, fMRI data 100 , or observational data, can be described using natural language 100 .Beyond text and text-represented systems, different data modalities represent different "languages" to interact with observations, and domain-specific modalities are extremely important for scientific discovery.Scientific research often involves other data types, including image, video, audio, table 101,102 , or even general files 103 , as well as domainspecific modalities like genomic sequences, chemical graphs, or spectra 76,77,82,83 .Multimodality LLMs can play the observer role vis-a-vis these data.However, most multimodality LLMs are still struggling to handle some domain-specific data formats, such as genomic data or chemical compounds, which may require converting and where information may be lost during the conversion process.</p>
<p>For highly specialised domains, domain-specific LLMs trained on specialised data can achieve superior performance within their respective fields (representing the end-to-end approach discussed earlier).For example, with the same number of parameters, BioGPT 64 outperforms GPT-2 medium 104 when trained on domain-specific data.Even with fewer parameters, models like PubMedBERT 62 can perform at a level comparable to GPT-2 medium.In the chemical domain, LLMs have been pre-trained on chemical SMILES data 105 , enabling them to infer molecular properties and biological activities 51 .LLMinspired models are also useful for case-specific tasks.In 106 , transformers are trained on cellular automata to study the relationship between complexity and intelligence.This highlights the importance of exploring domain-specific and case-specific LLM and the opportunities for further exploration in this area.</p>
<p>Experimentation and Automation</p>
<p>The experiment is a critical part of all research steps, including making observations and validating the hypothesis.Both humans and LLMs need external tools to implement experiments.Specifically, this involves calling external functions or directly generating and running code.LLMs that have been fine-tuned for tool usage 85,96 can generate structured entities (often in JSON) that contain the function name and inputs to be implemented by external functions.These functions are versatile and can include simple calculations, laboratory control functions, external memory, requests for assistance from human scientists, etc. LLMs can also direct programming by generating and running code for complex experiments requiring fine-grained control or enhancing the calculation abilities of LLMs 107,108 .Beyond this, generated programs can also call other functions or be saved into a function library, enabling the combinatory development of complex actions 109 .</p>
<p>For complex experiments, planning becomes important, which involves setting up an objective and decomposing it into practical steps.This is critical to solving complex tasks while sustaining coherent behaviour.While the planning capabilities of LLMs are questioned in many studies, certain tools and methods still demonstrate valuable assistance.</p>
<p>The chain-of-thought (CoT) 44 method significantly improves various tasks by decomposing a question into steps.In complex tasks with more steps, where LLMs seek long-term objectives and interact with environments, they can generate plans in natural language based on given objectives 110 .It is also important to adapt to observations and unexpected results.For this reason, methods like Reflexion 111 , ReAct 112 combine the CoT and planning, dynamically update its plans, manage exceptions, and utilizes external information.And it also overcomes hallucination and error propagation in the chain-ofthought.</p>
<p>Automation is a significant aspect of LLM-assisted research, serving as a key contributor to accelerating scientific discovery.Automation involves repetition and feedback loops 113 .LLMs can be seen as a function --prompt in, reply out -with human users as the driving force behind making LLMs produce output.To automate such a system, the key is to replace the human user.For instance, an LLM-powered chemical reaction system can perform Suzuki and Sonogashira reactions by incorporating an LLMbased planner, which replaces the human user.The planner reasons through the given task and determines the next steps, including searching the internet for information on both reactions, writing Python code to calculate experimental parameters, and finally calling external tools to conduct the experiments.At each step, the results, i.e., the search outcomes and calculation results, are fed back to the LLM-based planner to automate the system 66 .</p>
<p>Another approach is to replace the human user with multiple LLMs and allow them to communicate with each other 114 .Since such automation is not fully hard-coded and the core of this automation is also an LLM, they can exhibit some emergent behaviour 66,114 , adapting unexpected situations, which is vital for exploring new knowledge.Specifically, automated LLMs can help in three dimensions of scientific discovery: scaling, enhancing, and validation.</p>
<p>Scaling: Automated LLM agents can scale previously challenging experiments for large-scale studies.Examples include inferring underlying functions from input-output pairs 115 .The LLMs perform multiple rounds of trial and error to find the correct function.</p>
<p>This approach can extend to neuron interpretation of GPT-2 using GPT-4, which has billions of parameters 116 .This method involves two layers of loops: the trial-and-error process and the application to all the billions of neurons 117,118 .Both layers are timeconsuming for human scientists, and LLMs make such studies feasible.Another example is when LLMs are used to infer the functionality of human brain voxels from fMRI activation data, their proposed functions are first validated by calculating the probability of observing the activation data given a specific functional hypothesis.Subsequently, the hypotheses with the high probability are selected to aid in generating new hypotheses and improving overall performance 100 .Lab experiments can also be parallelized with the help of LLMs, which further accelerate the experiment speed and increase the potential for scaling the scope of experiments 110,119 .</p>
<p>Enhancing:</p>
<p>The aforementioned scientific methods, such as hypothesis generation, experiments, and observations, can all be enhanced by automation.One direct application is using LLMs as optimisers: by iteratively providing historical solutions and scores, LLMs can propose new solutions and ultimately achieve superior performance 120 .In both the hypothesis-validation loop and in experimental trials, failed cases constitute valuable feedback.When evaluators and reflection are incorporated into the workflow, LLMs can improve their decisions, showing significant performance improvements compared to simply using LLMs 111 .Iteration can also enhance the hypothesis generation stage.By comparing hypotheses with existing literature on related topics, LLMs can iteratively improve novelty by using this literature as a source of negative examples 121 .Another enhancement comes from accumulating knowledge, which is critical to research success.</p>
<p>Many exploration tasks require accumulating knowledge and developing new strategies based on this knowledge 122 .For example, Voyager 109 uses GPT-4 to examine the space of the Minecraft game.This study consists of three main parts: an automatic curriculum to propose exploration objectives, an iterative prompting mechanism to write code to control the game, and a skill library to accumulate the knowledge and skills gained during the exploration, which is then reused in future explorations.Equipped with all these components, this LLM-assisted explorer can explore the game more efficiently.While game environments in silico are a non-trivial departure from real worlds in situ, they are not too dissimilar from the biochemical simulation engines 123 that scientists rely on today.</p>
<p>However, the current "physics" engines in-game systems are still inconsistent with the physical sciences, and new simulation software technologies are needed to allow for any AI-based exploration of multi-physics environments 113 .From a macroscopic viewpoint, scientific discovery can also be considered a quality-diversity search process 124,125  This loop is essential given the probabilistic nature of LLMs 126 and the hallucination problem of LLMs 127,128 .Experiments show that repeatedly verifying the results from LLMs' observations and proposed hypotheses increases the likelihood of obtaining reliable results 129,130 .A promising direction is leveraging formal systems to validate results and hypotheses by translating generated hypotheses and answers into formal languages, such as LEAN or Prover9 131,132 .For instance, in 131 , LLMs first generate multiple answers.</p>
<p>These answers are then translated into the LEAN language and verified using the LEAN compiler to choose the correct responses.With these filtered answers, LLMs can aggregate toward a final answer.Another example involves using Python code to aid validation.</p>
<p>While general programming languages are often not considered formal systems, they can still disprove certain hypotheses.In 133 , LLMs were prompted to solve the Abstraction and Reasoning Corpus (ARC) tasks 134 , which involve identifying underlying laws and making predictions based on new initial states.LLMs initially propose hypotheses, which are then translated into Python code.This Python code is used to disprove incorrect hypotheses.</p>
<p>Although these non-formal systems cannot fully validate hypotheses, they partially perform validation and improve predictive accuracy.While humans could also conduct such translation and validation processes, the high speed of hypothesis generation by LLMs makes automated approaches more suitable.A limitation, however, is the reliance on LLMs to translate hypotheses into formal languages, which may introduce errors in the process.This suggests the need for caution when interpreting results, even if they have been tested using formal systems.</p>
<p>Expanding the Literature Review and the Hypothesis Horizon</p>
<p>In brief, advancing beyond current knowledge includes using LLMs to explore unknown territories in knowledge space, encompassing human discoverable, human-machine discoverable, non-human-machine discoverable, and the entirety of the knowledge space, as illustrated in Evidence indicates that LLMs can propose novel ideas, such as drug combinations 135 , with designed prompting, thus underscoring the importance of prompting, as discussed previously.An example is the use of LLMs for drug discovery: In 135 LLMs are prompted to propose novel combinations of drugs for treating MCF7 breast cancer cells while incorporating additional constraints such as avoiding harm to healthy cells and prioritizing FDA-approved and readily accessible drugs.The experiment results demonstrate that LLMs can effectively propose hypothetical drug combinations.More advanced techniques can also improve novelty, such as asking LLMs to role-play as scientists 136 or iteratively provide feedback on existing similar ideas 66 .This is further exemplified by the Virtual Lab project 137 , where AI agents, powered by LLMs, were used to design novel nanobody binders against SARS-CoV-2 variants.LLMs effectively functioned as hypothesis generators, facilitating rapid and innovative scientific discovery that translates to validated experimental results in real-world applications.Although some human evaluations show that LLM-generated ideas have lower novelty 138 , the fast speed at which LLMs propose ideas can still be valuable.With proper instruction and background knowledge, LLMs can act as zero-shot hypothesis generators 139 .LLMs can also generate hypotheses semantically or numerically based on observations about the underlying mechanisms for language processing and mathematical tasks 140,141 .With neuron activation heatmaps, GPT-4 can propose potential explanations for neuron behaviour 116 .</p>
<p>Besides directly proposing hypotheses, a significant part of creativity combines existing knowledge, making literature research critical.With their vast stored compressed knowledge 142,143 , LLMs can be viewed as databases queried using natural language 123 .This not only accelerates the search but also breaks down barriers of domain terminology, making it easier to access interdisciplinary knowledge.For accessing more up-to-date and domain-specific information, LLMs can help scientists by using the RAG method and accessing internet information, see Figure 2. Generally, text embedding is used for semantically searching vector databases 45,92 .For example, STORM 144  This case also highlights the importance of the hypothesis-experiment-observation loop, where each step is critical: hypotheses rely on observations, experiments require hypotheses and planning, and observations depend on experiments.Such a self-dependent loop is typical in scientific discovery and can be initiated either by starting with a tangible step in the hypothesis-experiment-observation process or by allowing human intervention.</p>
<p>Human Scientists in the Loop</p>
<p>While we showcase the capabilities of LLMs in assisting scientific discovery, human scientists remain indispensable.During the literature review stage, with the help of LLM agents, humans can contribute by providing deeper perspectives or guiding the focus toward the needs of human scientists 144 .In the reasoning processes, by identifying uncertain reasoning and thoughts, humans can correct LLMs.This significantly improves the accuracy of the chain-of-thought method, making the LLMs more reliable 147 .Human scientists can be involved in further improving safety and reliability.For example, ORGANA 110 , an LLM-powered chemical task robot, uses LLMs to interact with humans via natural language and actively engages with users for disambiguation and troubleshooting.Beyond this, humans can assist LLMs to enhance performance with a reduced workload.For example, by involving humans in the hypothesis-proposing stage to select generated hypotheses, LLMs can perform similarly to humans 133 .At the experiment stage, many lab experiments still require human implementation and correction of invalid experimental plans 67 , and LLMs can request human help on these experiments 66 .</p>
<p>While the methods described above focus on LLMs as drivers of scientific inquiry, we must clarify that human-in-the-loop is more aptly cast as LLM-in-the-loop, emphasising "assistance" or augmentation as the practical value-added dimension of LLMs.The opportunities described in this paper show potential to shift this mode of scientific practice to be more reliant on AI-driven approaches, but not without significant advances in AIScience approaches with respect to physics-infused ML and causal reasoning and in rigorous testing systems for LLMs interacting with the natural world.Hypotheses -Literature review: using an LLM's own trained knowledge [ 142,143,148 ], or using the RAG method to access up-to-date information [ 45,92,121 ].</p>
<p>-Novelty: hallucinations of LLMs can sometimes benefit novelty [ 149 ]; using the role-play method, LLMs can increase their novelty [ 136 ]; LLMs can also propose novel ideas iteratively [ 121 ].</p>
<p>-Observation-based Hypotheses: LLMs can propose hypotheses based on [ 100,115,116,139,141</p>
<p>Challenges and Opportunities</p>
<p>While LLMs have shown signs of delivering promising results and of having positive impacts on scientific discovery, investigators have recognised their limitations, such as hallucinations, limited reasoning capabilities, and lack of transparency.Compared to everyday usage, when applied to scientific domains, these limitations require careful consideration, as scientific processes and discoveries require high standards of truthfulness, complex reasoning, and interpretability.The scientific community's increasing recognition and communication of these limitations of LLMs is essential to enabling solutions while also limiting expectations.Such rigour is a cornerstone of science and engineering, and a requirement if LLMs are to play a practical role.</p>
<p>Beyond all this, LLMs also affect scientific research at the scientific community level.While many papers and reviews involve LLMs' assistance, LLMs still face challenges in producing qualified reviews.</p>
<p>Hallucinations as Putative Sources of Novel Hypotheses</p>
<p>Hallucinations produced by LLMs, also called confabulation or delusion, refer to artificially intelligent systems generating responses that contain false or misleading information presented as fact.This is analogous to hallucination in human psychology, though, for LLMs, it manifests as unjustified responses or beliefs rather than perceptual hallucinations 151 .Hallucinating LLMs can make unsupported claims, thus failing to meet a prior set of standards.While some "incorrect" LLM responses may reflect nuances in the training data not apparent to human reviewers 152 , this argument has been challenged as not robust to real-world use 153 .</p>
<p>In scientific discovery, hallucination becomes a critical hurdle when applying</p>
<p>LLMs to literature review, data processing, or reasoning.Various methods have been developed to mitigate hallucinations 154 .Using the RAG method, LLMs can reference accurate source contexts and up-to-date information, which can reduce hallucinations 155 .</p>
<p>Knowledge graphs can also provide reliable information to reduce hallucinations 64 .</p>
<p>A self-RAG method can also reduce hallucinations, where the LLMs generate and verify the reference contexts, and outputs are also verified by the LLMs themselves 151 . 156proposes an even simpler solution: create answers for the same query multiple times and vote for the final answer.This method can significantly improve the accuracy of outputs.</p>
<p>Repetition from prompt variation and reiteration can also detect hallucinations-by finding contradictions 157 .By repeatedly generating the same context, LLMs may sometimes generate contradictory content, which can be fixed by the LLMs themselves iteratively.</p>
<p>Another method to mitigate hallucinations is through self-verification.This often involves decomposing the generated content into multiple fact checkpoints.For example, the Chain-of-Verification method uses separate LLM agents to verify them individually and update the original answer 158 .Such a verification process can also adopt RAG methods for greater reliability 159 .</p>
<p>An important origin of hallucinations is the auto-regressive generation process of mainstream LLMs, where errors may accumulate during generation 146 .Hence, as discussed above, a general way to mitigate hallucinations is to decompose the end-to-end generation process using chain-of-thoughts, the RAG method, multiple agents, feedback, and iteration loops.</p>
<p>While significant research efforts target the challenge of how to control or limit hallucinations, we may ask to what extent hallucinations are a bug or a feature.For example, could hallucination provide a gateway to creativity in that it could represent a steady stream of novel conjectures?An LLM could then be used to filter such a string of hallucinated hypotheses and rank them to recommend which ones to test.This remains unexplored territory, as far as we can tell.</p>
<p>Another approach to treating hallucinations is to move beyond a binary perspective of trust versus distrust.Instead, similar to statistical confidence, we may quantify the extent to which research conducted by LLM agents can be trusted.Current studies primarily focus on confidence measurements at the foundation model level [160][161][162] and the output level 163 .Some research has also proposed multidimensional assessments of LLM trustworthiness 164 .Additionally, efforts have been made to enable LLMs to express their confidence levels 160,165 .However, confidence measurements at the LLM agent level are primarily limited to success rates rather than trustworthiness, particularly when dealing with open-ended tasks.Moreover, existing measurements predominantly rely on post-hoc quantifications, which restrict their applicability in scientific research 166 .Therefore, predictive trustworthiness quantification frameworks for LLM agents that collectively consider foundation models, tasks, tools usage, workflow, and external feedback are needed.</p>
<p>The Value of Reasoning and Interpretation in LLM-led Science</p>
<p>While LLMs have been suggested to perform reasoning on some tasks, they exhibit severe defects in logical reasoning and serious limitations with respect to common sense reasoning.Notably, while LLMs can correctly answer "X is the capital of Y", they struggle to accurately determine that "Y's capital is X."This is known as the "reversal curse" 167 .</p>
<p>Another example is shuffling the order of conditions in a query, which may reduce the performance of LLMs.When the conditions are provided logically, the LLMs can often perform correct inferences but may fail when the conditions do not follow a specific order 168 .LLMs can also fail at simple puzzles, such as determining the odd-numbered dates on a list of famous people's birthdays 169 , or in simple questions like "Alice has N brothers, and she also has M sisters.How many sisters does Alice's brother have?"Many LLMs, while achieving high performance on other benchmarks, have shown a lower success rate on this task 170 .When faced with unseen tasks, which are common for human scientists in research, LLMs exhibit a significant drop in accuracy even on simple questions, such as performing 9-base number addition or writing Python code with indexing starting at 1 instead of 0. This suggests that LLMs may rely more on pattern matching than on reasoning, contrary to what many assume [171][172][173] .Consequently, caution is advised when applying LLMs to novel reasoning tasks, and incorporating human oversight into the process is recommended 173,174 .Another crucial aspect of reasoning is planning capability.</p>
<p>As discussed earlier, planning is essential for implementing experiments.While techniques such as ReAct and Reflection demonstrate some planning capabilities in LLMs, their effectiveness remains questionable.Current state-of-the-art LLMs often fail at simple planning tasks 175 , such as Blocksword, and are unable to verify the correctness of their plans 175 .In contrast, humans generally excel at creating effective plans for such tasks.</p>
<p>However, studies also indicate that integrating LLMs with traditional methods or solvers can enhance planning success rates, reduce research time 175 , and provide more flexible ways to interact when developing plans 176 .Some reasoning improvement methods, such as self-correction, can also fail.When</p>
<p>LLMs receive feedback without new information or correct labels, self-correction can often lead to compromised performance 177 .Such self-correction prompts may even bias the LLMs, causing them to turn correct answers into incorrect ones.To mitigate this problem, directly including all requirements and criteria in the query prompt is suggested instead of providing them as feedback.This result also indicates that to make corrections, effective feedback needs to include external information, such as experimental results and trustworthy sources 177 .</p>
<p>While some progress has been made, more advanced methods are needed to address these reasoning-related challenges.One crucial aspect to consider is consistency -when different LLM agents generate different responses to the same query, the result is considered inconsistent.Notably, the self-consistent method 178 uses LLMs to answer the same question multiple times and chooses the most frequent answer.The answers also help people estimate uncertainty 178 , given that LLMs often behave too confidently 179 .Similar methods have also been proposed in 156 .Other methods use different LLM agents to suggest different ideas and then conduct a multi-round debate to arrive at a final answer 180 .As illustrated in Figure 2, these LLM-agent methods can benefit all steps in the hypothesisexperiment-observation loop.</p>
<p>A straightforward but challenging route to scientific discovery is to fine-tune or directly train a model.In 181 , the authors propose an innovative solution to the Reversal Curse through "reverse training," which involves training models with both the original and reversed versions of factual statements.Considering the requirements for rigor and prudence in scientific research, attention must be given to the limitations of reasoning tasks.This is particularly important given that LLMs often exhibit reduced performance in reasoning correctness when encountering novel tasks-a frequent occurrence in scientific research, where the focus is on exploring unknown knowledge.</p>
<p>The Challenge to Understand LLMs, and the Opportunity to Understand by using LLMs A comprehensive scientific interpretation stimulates discussion and further discoveries among scientists.This is especially important for LLM-assisted scientific discovery-given that current LLMs are mostly black boxes, it becomes difficult to trust LLM outputs.To understand LLMs' behaviour, LLMs' language capabilities can be leveraged.There are two types of methods.First, LLMs' hidden states can be used in what are known as probing methods.The Logit Lens method 182 applies the unembedding layer to hidden states or transformed hidden states, enabling semantic understanding of LLMs' hidden states.Representation engineering methods 183 can further detect and control emotion, dishonesty, harmfulness, etc., at the token level, allowing people to read their hidden activities.Besides these, dictionary learning methods can also be used to understand LLMs' hidden states and activations, leading to a fine-grained understanding of LLMs.</p>
<p>The second method is to ask LLMs to explain their reasoning.For instance, the CoT method or reasoning models 56 can explain the thought process before generating results or ask LLMs to explain their reasoning after generating results.However, the selfexplanation of LLMs is also questionable.Their explanations are often inconsistent with their behaviours, and we cannot use their explanations to predict their behaviours in counterfactual scenarios 184 .This suggests that LLMs' self-explanation may not accurate and not generalizable.Beyond this, LLMs may also hallucinate in their self-explanations, including content that is not factually grounded 184 , making their self-explanations even less trustworthy.</p>
<p>Despite the many difficulties in understanding LLMs, they present a significant opportunity for understanding other systems -they can be used to understand data, interpret other systems, and then prompt humans.By directly showing input and output pairs to LLMs, including language input-output pairs 115 , mathematical function inputoutput pairs, or experimental data, LLMs can be made to explain these black-box systems, including, fMRI data, complex systems like GPT-2, or, potentially, papers written by human scientists that are becoming increasingly difficult to reproduce because of various forces at play (e.g., an increasing number of publications and dubious incentives).This indicates the potential of applying LLMs to explain data and other systems, even though understanding them may still be challenging 116 .This capacity to interpret systems is not limited to human language.Foundation models in specific scientific domains offer domain-grounded interpretability, distinct from the pitfalls of LLM self-explanation.Pre-trained on vast, specialized data, these models learn the "language" inherent in that data.For instance, scGPT in single-cell biology demonstrates this: its learned representations align with established biological knowledge, and it utilizes attention-map visualizations to enhance transparency, elucidating gene interactions that are subsequently validated by domain-specific evidence 76 .Although the faithfulness of attention weight interpretability has been questioned in various cases 185,186 , it remains widely used in many AI-for-science applications 187 .Models, including LLMs that use transformer architectures, often inherently benefit from such emergent interpretability for both feature attribution and interaction highlighting.These approaches support viewing complex domain representations, such as Gene Regulatory Networks (GRNs), as a form of decipherable "domain language."Understanding these intrinsic languages through models whose interpretations are rooted in verifiable domain semantics, rather than potentially unreliable self-explanations, provides a robust method for advancing scientific discovery in specialized fields.</p>
<p>The Impact on Scientific Practice and the Community</p>
<p>An open question is how much human science and scientists will be willing to let AI, through technology like LLMs, drive scientific discovery.Would scientists be satisfied letting LLMs set the agenda and conduct experiments with little to no supervision, or do we expect to supervise AI always, driven by the fear of multiple levels of misalignment?This is a misalignment between human scientific interests and the actual practice of science, possibly forcing AI and LLMs to produce data as humans do, with its advantages or disadvantages, including constraining the search space and, therefore, the solution space.</p>
<p>Beyond the individual deployment of the scientific method, scientific discovery also happens at the community level, where scientists publish their work, share ideas, and collaborate.We can consider the scientists and even entire scientific community as an agent that learns from experiments and research publications in a manner similar to reinforcement learning processes 188 .However, learning from failed research (or negative results) is just as important as learning from successful studies 189,190 , yet it is currently undervalued 191 .This may be because failed research is far more common than successful research.However, with the massive text-processing capabilities of LLMs, we now have the opportunity to systematically share and learn from failures.Therefore, we advocate for journals and conference to encourage the publication of failed studies and negative results.</p>
<p>This learning process also depend on human values emphasising communication, mutual understanding, and peer review.Evidence shows a significant adoption of LLMassisted paper writing and peer review in recent years.Estimates indicate that 1% to 10% of papers are written with LLM assistance 192 .In computer science, up to 17.5% of research papers are estimated to be assisted by LLMs, a figure that mainly reflects the output of researchers with strict time constraints 193 .Beyond papers, estimates also show that around 7% ‚àº 15% of reviews are written with LLM assistance 194,195 .</p>
<p>While LLMs can provide feedback that shows a high degree of overlap with human reviewers, they are not proficient at assessing the quality and novelty of research 196 .This limitation is especially significant for high-quality research 197 .Beyond this, LLM-assisted reviews tend to assign higher scores to papers than human reviewers evaluating the same papers 194 .Upon closer examination, LLMs also exhibit a homogenisation problem -they tend to provide similar critiques for different papers 196,198 .Despite LLMs displaying limitations at tasks such as peer reviewing and raising ethical concerns in directly generating academic content, they may still benefit scientific communication.For example, most researchers today are non-native English speakers, so they can benefit from LLMs' language capabilities that fit their diverse demands for proofreading 198 , helping alleviate the current bias towards Western Science.On another application, LLMs' code explanation capabilities may help scientists understand poorly documented code, making existing knowledge and work more accessible to a broader range of scientists 198 .With significant growth in using LLMs for writing papers, such impacts will become increasingly important 193 .[ 192,194,195 ] -LLMs are widely used in paper and review writing [192][193][194][195] LLMs can mitigate the disadvantage of non-native English speakers [ 198,[205][206][207] ] LLMs can help interdisciplinary research [ 198 ] Table 2: Challenges and opportunities in applying LLM in scientific discovery.There is much room for AI4Science as a field to fill in gaps under the listed themes, especially as LLM research matures over the next several years; consistent with all innovations in scientific practices and engineering standards, the demonstration-through-validation of said innovations requires time and resources several fold greater than are available in "safer" domains (where LLMs are currently embedded).</p>
<p>Challenges</p>
<p>Conclusions</p>
<p>In this perspective paper, we reviewed the rapid development and integration of large language models (LLMs) in scientific research, highlighting the profound implications of these models for the scientific process.LLMs have evolved from tools of convenienceperforming tasks like summarising literature, generating code, and analysing datasets-to emerging as pivotal aids in hypothesis generation, experimental design, and even process automation.As AI advances, foundation models have emerged, representing adaptable, scalable models with the potential to apply across diverse scientific domains, reinforcing the collaborative synergy between humans and machines.</p>
<p>LLMs have reshaped how researchers approach the vast amounts of scientific information available today.By efficiently summarising literature and detecting knowledge gaps, scientists can speed up literature review and idea generation.Despite the promise, current limitations pose significant hurdles to fully realising LLMs as independent scientific agents.Among these are reasoning limitations, interpretability issues, and challenges like "hallucinations"-where LLMs generate plausible-sounding but inaccurate information.While helpful in generating hypotheses, these models require careful oversight to prevent misleading or unverified information from influencing scientific processes.</p>
<p>The challenges of reasoning and hallucinations pose serious concerns regarding the use of LLMs in scientific discovery.Instead of treating LLMs as simply trustworthy or untrustworthy in a binary manner, we suggest an analogy to statistical confidence, using a continuous value-it may term as algorithmic confidence-to quantify the trustworthiness of an LLM agent system in scientific research.We further suggest that all LLM-assisted research should either be verified by humans or undergo algorithmic confidence testing.</p>
<p>The interpretability of LLMs also remains a complex issue.Their black-box nature can obstruct transparency, limiting trust in outputs that affect high-stakes scientific decision-making.Consequently, researchers continue to explore methods such as probing, logit lens techniques, and visualisation of neuron activations to demystify the decisionmaking processes within these models.Increased interpretability will be critical as we strive for ethically responsible and scientifically sound applications.On the other hand, it is essential to recognize that LLMs are showcasing their potential to explain other blackbox systems through their language and reasoning capabilities.</p>
<p>Integrating LLMs into scientific workflows brings ethical considerations, particularly regarding transparency and fairness.For instance, LLMs hold the potential for democratising access to scientific information, aiding researchers from non-English speaking backgrounds in publication and collaborative research.However, they also risk perpetuating biases present in training data, thereby influencing scientific outputs and potentially reinforcing existing disparities in research.</p>
<p>Another concern involves the over-reliance on AI in scientific processes.As we incorporate LLMs deeper into workflows, human oversight becomes essential to maintaining scientific rigor and addressing potential misalignments between AI-generated outputs and human-defined research goals.The question of how much autonomy AI should have in guiding scientific inquiries raises ongoing debate about accountability and the evolving role of human oversight.</p>
<p>To harness LLMs as creativity engines, moving beyond task-oriented applications to generate new scientific hypotheses and theories is paramount.For LLMs to contribute meaningfully to fundamental scientific discoveries, they must be equipped to recognize patterns and autonomously generate novel, insightful questions-a hallmark of scientific creativity.This would require advancements in prompt engineering, automated experimentation, iterative reasoning, and building an AI that evolves its approach based on experimental feedback.However, a significant gap in general reasoning capabilities separates current models from domain-specific superhuman systems like AlphaGo/AlphaZero 209,210 .AlphaGo leveraged a critical symmetry where an "answer" (a move) inherently generates a new "question" (the next board state challenge)-a dynamic largely absent in today's reasoning models, yet key for mastering novel tasks.For scientific discovery, developing this symmetry is crucial, as the ability to ask questions is as important as answering them; though some preliminary work has explored this 211 , it remains an unsolved and highly challenging problem.</p>
<p>The evolution of LLMs and foundation models signals a transformative era for science.While current applications largely support scientists in managing data and expediting workflows, the future may see these models as integral components of the scientific process.By addressing challenges in accuracy, interpretability, and ethical concerns, we can enhance their reliability and pave the way for responsible AI in scientific contexts.</p>
<p>Looking ahead, the collaboration between AI and human scientists will likely define the next generation of discovery.As we refine foundation models to become more adaptable and creative, they may transition from merely assisting to potentially leading explorations into uncharted scientific domains.The challenge lies in responsibly developing these models to ensure they complement and elevate human expertise without compromising scientific integrity.Ultimately, LLMs and foundation models may come to represent a synthesis of human and artificial intelligence, each amplifying the strengths of the other.With continued research and ethical vigilance, LLMs have the potential to accelerate and deepen scientific discovery, heralding a new era where AI not only supports but inspires new frontiers in science 212 .This includes embracing and learning from scientific failures and leveraging them to drive comprehensive exploration.As LLMs evolve, they may reshape scientific methodologies, impacting how science values discovery and reproducibility and may ultimately redefine the purpose of scientific inquiry.</p>
<p>Figure 1 :
1
Figure 1: (A) LLMs generate sentences in an auto-regressive manner, sampling tokens from a predicted distribution at each step.(B) A typical prompt for LLMs consists of a system prompt and a user prompt.The LLM will then respond as an assistant.A multiround dialogue will repeat the user and assistant contents.(C) LLM agents are systems that use a large language model as its core reasoning and decision-making engine, enabling it to interpret instructions, plan actions, and autonomously interact with external tools, environments, or other LLM agents to fulfil a given goal.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the scientific discovery process: Scientific research can be formulated as a search for rewards in an abstract knowledge space.By synthesizing existing knowledge -represented by blue disks (human-discovered) and stars (humanmachine discovered)) -in novel ways, new knowledge (indicated by red stars) can be explored.For specific research, scientists or LLMs need to traverse the hypothesesexperiment-observation loop, where hypotheses are proposed based on existing knowledge (including LLM knowledge, and additional literature provided via RAG methods), observation, and the creativity of LLMs.Then, with aid of external tools such as programming languages, formal validations, and other methodologies, experiments are conducted to test the hypotheses or gather data for further analysis.The experimental results can be observed and described through the observation process, facilitated by domain-specific models and the multi-modality capabilities of language models.All these parts-observation, proposing hypotheses, conducting experiments, and automation-can be assisted by LLMs and LLM-agents, considering the non-trivial implementations of scientific environments in silico.</p>
<p>, and thisVoyager study has shown how LLMs can assist diversity search in a new way by proposing objectives, iteratively solving problems, and contributing to and utilising literature (skill library).Validation: Automated LLM agents are critical for validating hypothesis.Beyond scaling and enhancing performance, research often involves multiple rounds of the hypothesis experiment loop to meet scientific discovery's rigor and safety requirements.</p>
<p>Figure 2 .
2
Namely, to perform hypothesis generation and develop predictive models of more complex systems.Hence, can LLMs do open-ended Exploration of the Hypothesis Space?Can LLMs also explore complex environments in an open-ended way?These are open-ended challenges addressing the (unknown) limits of the capabilities of LLMs and Generative AI.Proposing hypotheses is a crucial step in scientific discovery, perhaps the most important since it often involves significant creativity and innovation.Scientists propose hypotheses to explore unknown topics or address research questions.This step often involves novel ideas, recombining existing literature, and key insights.Experiment design and subsequent verification are based on these hypotheses.Thus, hypothesis proposing is a central step that connects observation and experiments.</p>
<p>proposes an LLMpowered literature review agent that, for a given topic, actively searches literature on the internet from different perspectives and automatically generates follow-up questions to improve depth and thoroughness.Another important example is Deep Research 145,146 , which integrates internet browsing and reasoning to deliver more in-depth and relevant literature review results.LLMs can propose novel hypotheses by retrieving related literature as inspiration, finding semantically similar content, connecting concepts, and utilising citations based on research topics and motivation 121 .Alternatively, it may require additional ingredients or experiments to extrapolate and search outside current knowledge domains.</p>
<p>-</p>
<p>Replacing human evaluation and annotations [60,61 ] -Simplifying observed data by providing qualitative descriptions [100,115 ] -Domain-specific LLMs can perform better on classification and prediction tasks [[62][63][64]</p>
<p>Furthermore,</p>
<p>LLMs facilitate interdisciplinary research, bridging the knowledge divide by summarising complex ideas across fields, thereby fostering collaborations previously limited by domain-specific language and methods.Beyond these benefits, the massive textprocessing capabilities of LLMs create new opportunities for utilizing failed research failed research, which has received limited attention.Therefore, we encourage the scientific community to promote the publication of negative results and failed research.The utility of LLMs in designing experiments is another notable advancement.Models like CRISPR-GPT in biology exemplify this by automating gene-editing experiment designs, significantly accelerating genomics research.Moreover, LLMpowered autonomous systems like BioDiscoveryAgent indicate a shift towards AI-driven experimental processes that can augment researchers' efficiency and, more importantly, enable scientific exploration previously constrained by resource limitations.So, Large Language Models (LLMs) present two contrasting roles in scientific discovery: accuracy in experimental phases and creativity in hypothesis generation.On the one hand, scientific research requires LLMs to be reliable, accurate, and capable of logical reasoning, particularly for experimental validation.On the other, there is value in promoting creative "hallucinations" or speculative ideas at the hypothesis stage, which mirrors human intuition and expands research boundaries 208 .Besides the general foundation models like GPT-4, Claude and Deepseek, domain specific foundation models have shown special potential for applying LLMs in scientific research Notable examples such as Evo and ChemBERT showcase the success of domainspecific adaptations in genomics and chemistry, where they excel in predicting gene interactions and molecular properties.These foundational models also highlight a promising approach by treating genomic, chemical, and other scientific data as new modalities for LLMs, similar to how images, videos, and audio are considered as modalities.Integrating these modalities often follows two main strategies: end-to-end training, where models like ChemBERT develop deep, intrinsic capabilities on specialized data, potentially exceeding human performance on specific tasks; and compositional approaches, which offer greater flexibility by leveraging intermediate modalities common to human scientists (like vision and text) or specialized tools.While end-to-end methods provide depth, compositional flexibility is crucial for adapting to diverse and rapidly changing scientific demands.Consequently, combining and scaling these scientific modalities, particularly when models can be seamlessly inserted into various scientific workflows, has the potential to profoundly transform scientific research.</p>
<p>-</p>
<p>]. Implement experiment: LLMs can use external tools [ 85,96 ], API calls [ 66 ], or directly write code [ 150 ] to implement experiments.Experiment planning: chain-of-thought [ 44 ], ReAct [ 112 ].Hardcoded pipeline for safety [ 67 ]; Human confirmation [ 50 ]
AutomationExperiment
--Safety:</p>
<p>-</p>
<p>LLM agent: LLM-based planner [66], multi-LLM agent [ 109 ] Scaling: complex tasks [ 116-118 ]; knowledge accumulation [ 109 ] Iteratively optimising proposed hypotheses [ 121 ], and experiments [ 112 ].
-Human-in-the-loop
--Enhance:</p>
<p>Table 1 :
1
How LLMs can assist scientific methods at different stages.</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Can Scientific Discovery Be Automated? The Atlanctic. A Alkhateeb, Aeon, 2017</p>
<p>GFlowNets for AI-driven scientific discovery. M Jain, Digital Discovery. 22023</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. H Kitano, NPJ Syst Biol Appl. 7292021</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. C Cornelio, Nature Communications. 1412023. 2023</p>
<p>Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery. S Kim, IEEE Trans Neural Netw Learn Syst. 322021</p>
<p>Amplify scientific discovery with artificial intelligence: Many human activities are a bottleneck in progress. Y Gil, M Greaves, J Hendler, H Hirsh, Science. 3461979. 2014</p>
<p>Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery. H Kitano, AI Mag. 372016</p>
<p>P Berens, K Cranmer, N D Lawrence, U Von Luxburg, J Montgomery, AI for Science: An Emerging Agenda. 2023</p>
<p>. Z Li, J Ji, Y From Zhang, Kepler To Newton, Explainable AI for Science Discovery. 2022</p>
<p>N Baker, 10.2172/1478744Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence. 2019</p>
<p>GFlowNets for Causal Discovery: an Overview. C D Manta, E Hu, Y Bengio, OpenReview. 2023</p>
<p>The transformative potential of machine learning for experiments in fluid mechanics. R Vinuesa, S L Brunton, B J Mckeon, Nature Reviews Physics. 52023</p>
<p>Synthesizing domain science with machine learning. Z Del Rosario, M Del Rosario, Nat Comput Sci. 22022</p>
<p>M Krenn, J Landgraf, T Foesel, F Marquardt, Artificial Intelligence and Machine Learning for Quantum Technologies. 2022</p>
<p>How artificial intelligence and machine learning can help healthcare systems respond to COVID-19. M Van Der Schaar, Mach Learn. 1102021</p>
<p>AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation. T Zhang, RICE-N.2022</p>
<p>Learning from learning machines: a new generation of AI technology to meet the needs of science. L Pion-Tonachini, 2021</p>
<p>Combining Machine Learning and Computational Chemistry for Predictive Insights into Chemical Systems. J A Keith, 10.1021/acs.chemrev.1c00107Chemical Reviews. 1212021</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nature Reviews Physics. 52023</p>
<p>How machines could teach physicists new scientific concepts. I Georgescu, Nature Reviews Physics. 42022</p>
<p>Machine Learning for Molecular Simulation. F No√©, A Tkatchenko, K.-R M√ºller, C Clementi, 10.1146/annurev-physchem-042018Annu Rev Phys Chem. 2020</p>
<p>Foundation models for generalist medical artificial intelligence. M Moor, Nature. 6162023</p>
<p>AI in health and medicine. P Rajpurkar, E Chen, O Banerjee, E J Topol, Nat Med. 282022</p>
<p>Multimodal biomedical AI. J N Acosta, G J Falcone, P Rajpurkar, E J Topol, 10.1038/s41591-022-01981-2Nat Med. 2022</p>
<p>Welcoming new guidelines for AI clinical research. E J Topol, Nat Med. 262020</p>
<p>. E Topol, Deep-Medicine, 2019</p>
<p>Self-supervised learning in medicine and healthcare. R Krishnan, P Rajpurkar, E J Topol, 10.1038/s41551-022-00914-1Nat Biomed Eng. 2022</p>
<p>The imperative of interpretable machines. J Stoyanovich, J J Bavel, T V Van &amp; West, Nat Mach Intell. 22020</p>
<p>The imperative for regulatory oversight of large language models (or generative AI) in healthcare. B Mesk√≥, E J Topol, NPJ Digit Med. 62023</p>
<p>The imperative of physics-based modeling and inverse theory in computational science. K E Willcox, O Ghattas, P Heimbach, Nat Comput Sci. 12021</p>
<p>Six ways large language models are changing healthcare. P Webster, Nat Med. 292023</p>
<p>Use of GPT-4 to Diagnose Complex Clinical Cases. A V Eriksen, S M√∂ller, J Ryg, NEJM AI. 12023</p>
<p>From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities. F Ishmam, M Sakib, H Shovon, M F Mridha, N Dey, 2023Preprint at</p>
<p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants. C Li, 2023</p>
<p>Large language models in medicine: the potentials and pitfalls. J A Omiye, H Gui, S J Rezaei, J Zou, R Daneshjou, 2023Preprint at</p>
<p>Maithra Raghu, Eric Schmidt, arXiv:2003.11755A Survey of Deep Learning for Scientific Discovery. 2020arXiv preprint</p>
<p>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. S Song, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>. J Mccarthy, M M , R N , S C E , Dartmouth Summer Research Project on Artificial Intelligence. in Dartmouth Summer Research Project on Artificial Intelligence. 1956</p>
<p>A Review of Large Language Models and Autonomous Agents in Chemistry. M C Ramos, C J Collison, A D White, 2024</p>
<p>Artificial intelligence in scientific discovery: Challenges and opportunities. H Zenil, R King, Science: Challenges, Opportunities and the Future of Research. ParisOECD Publishing2023</p>
<p>A framework for evaluating the AI-driven automation of science. H Zenil, R King, Science: Challenges, Opportunities and the Future of Research. ParisOECD Publishing2023</p>
<p>The Far Future of AI in Scientific Discovery. H Zenil, R King, Choudhary, F. and T. H.)2023World Scientific</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, Adv Neural Inf Process Syst. 352022</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, Adv Neural Inf Process Syst. 332020</p>
<p>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. J White, 2023</p>
<p>The Prompt Report: A Systematic Survey of Prompting Techniques. S Schulhoff, 2024</p>
<p>ChatGPT Prompt Engineering for Developers. I Fulford, A Ng, 2024</p>
<p>Exploring large language model based intelligent agents: Definitions, methods, and prospects. Y Cheng, arXiv:2401.034282024arXiv preprint</p>
<p>Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions. H Yang, S Yue, Y He, 2023</p>
<p>. Y Nakajima, Babyagi, 2024Preprint at</p>
<p>. H Chase, Langchain, 2022</p>
<p>. J Liu, Llamaindex, 2022</p>
<p>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. O Khattab, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>TextGrad: Automatic 'Differentiation' via Text. M Yuksekgonul, 2024</p>
<p>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. -Ai Deepseek, 2025</p>
<p>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. Z Shao, 2024</p>
<p>From System 1 to System 2: A Survey of Reasoning Large Language Models. Z.-Z Li, 2025</p>
<p>Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. J H Caufield, Bioinformatics. 40e1042024</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, Proceedings of the National Academy of Sciences. 120e23050161202023</p>
<p>Y Liu, 10.18653/V1/2023.EMNLP-MAIN.153NLG Evaluation using Gpt-4 with Better Human Alignment. EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 2511-2522 (2023</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, ACM Transactions on Computing for Healthcare (HEALTH). 32021</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach Learn Sci Technol. 3150222022</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, Brief Bioinform. 234092022</p>
<p>Towards an AI co-scientist. J Gottweis, 2025</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Augmenting large language models with chemistry tools. M Bran, A , Nat Mach Intell. 2024</p>
<p>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments. Y Qu, 10.1101/2024.04.25.591003v2doi:10.1101/2024.04.25.5910032024</p>
<p>BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments. Y Roohani, arXiv:2405.176312024arXiv preprint</p>
<p>Openai, arXiv:2305.14947v2OpenAI o1 System Card. 2024</p>
<p>Learning to Reason with LLMs. Openai, 2024</p>
<p>Scaling laws for neural language models. J Kaplan, arXiv:2001.083612020arXiv preprint</p>
<p>J Wei, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Sequence modeling and design from molecular to genome scale with Evo. E Nguyen, Science. 3861979. 2024</p>
<p>Genome modeling and design across all domains of life with Evo 2. bioRxiv 2025.02. G Brixi, 10.1101/2025.02.18.638918202518</p>
<p>scGPT: toward building a foundation model for single-cell multiomics using generative AI. H Cui, Nature Methods. 212024. 2024</p>
<p>Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. S Chithrananda, G Grand, B Ramsundar, Chemberta, 2020</p>
<p>OmniJet-$\alpha$: The first cross-task foundation model for particle physics. J Birk, A Hallin, G Kasieczka, Mach Learn Sci Technol. 52024</p>
<p>Multiple Physics Pretraining for Physical Surrogate Models. M Mccabe, 2023</p>
<p>Scientific Equation Discovery via Programming with Large Language Models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, Llm-Sr, ArXiv abs/2404.184002024</p>
<p>BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. S Zhang, 2023</p>
<p>Associating Astronomical Observations and Natural Language with Multi-Modal Models. S Mishra-Sharma, Y Song, J Thaler, Paperclip, 2024</p>
<p>AstroCLIP: a cross-modal foundation model for galaxies. L Parker, Mon Not R Astron Soc. 5312024</p>
<p>Augmenting large language models with chemistry tools. M Bran, A , Nature Machine Intelligence. 65 62024. 2024</p>
<p>. Openai, 2023GPT-4 Technical Report</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M W Chang, K Lee, K Toutanova, Bert, NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Learning Transferable Visual Models From Natural Language Supervision. A Radford, Proc Mach Learn Res. 1392021</p>
<p>Zero-Shot Text-to-Image Generation. A Ramesh, Proc Mach Learn Res. 1392021</p>
<p>Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems. 2024</p>
<p>. E Zelikman, Y Wu, J Mu, N D Goodman, Star, 2022Bootstrapping Reasoning With Reasoning</p>
<p>K Karl Popper, Popper, The Logic of Scientific Discovery. 1959</p>
<p>A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. W Fan, 10.1145/3637528.3671470Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2024</p>
<p>Prompting Large Language Models for Counterfactual Generation: An Empirical Study. Y Li, M Xu, X Miao, S Zhou, T Qian, Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 -Main Conference Proceedings. 2024. 2023</p>
<p>E Kƒ±cƒ±man, R Ness, A Sharma, C Tan, arXiv:2305.00050Causal reasoning and large language models: Opening a new frontier for causality. 2023arXiv preprint</p>
<p>Efficient Causal Graph Discovery Using Large Language Models. T Jiralerspong, X Chen, Y More, V Shah, Y Bengio, arXiv:2402.012072024arXiv preprint</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, Adv Neural Inf Process Syst. 362023</p>
<p>The Philosopher's Stone for Science--The Catalyst Change of AI for Scientific Creativity. Pin and Wang, Dashun, The Philosopher's Stone for Science--The Catalyst Change of AI for Scientific Creativity. Q Chen, Y.-J I Ho, P Sun, D Wang, March 5, 2024. 2024</p>
<p>An Algorithmic Information Calculus for Causal Discovery and Reprogramming Systems. H Zenil, 201919</p>
<p>Evolving self-reference: matter, symbols, and semantic closure. Laws, Language and Life: Howard Pattee's classic papers on the physics of symbols with contemporary commentary. H H Pattee, J RƒÖczaszek-Leonardi, H H Pattee, 2012</p>
<p>Explaining patterns in data with language models via interpretable autoprompting. C Singh, J X Morris, J Aneja, A M Rush, J Gao, arXiv:2210.018482022arXiv preprint</p>
<p>Table-gpt: Table-tuned gpt for diverse table tasks. P Li, arXiv:2310.092632023arXiv preprint</p>
<p>Anygpt: Unified multimodal llm with discrete sequence modeling. J Zhan, arXiv:2402.122262024arXiv preprint</p>
<p>Beyond Language Models: Byte Models are. S Wu, Digital World Simulators. 2024</p>
<p>Language models are unsupervised multitask learners. A Radford, OpenAI blog. 192019</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J Chem Inf Comput Sci. 281988</p>
<p>S Zhang, Edge of Chaos. 2024</p>
<p>Large Language Models as Code Executors: An Exploratory Study. C Lyu, 2024</p>
<p>A Survey on Large Language Models for Code Generation. J Jiang, F Wang, J Shen, S Kim, S Kim, 2024</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, arXiv:2305.162912023arXiv preprint</p>
<p>ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization. K Darvish, 2024</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Adv Neural Inf Process Syst. 362023</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, arXiv:2210.036292022arXiv preprint</p>
<p>A Lavin, ArXiv abs/2112.03235Simulation Intelligence: Towards a New Generation of Scientific Methods. 2021</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. J S Park, 10.1145/3586183.3606763UIST 2023 -Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023</p>
<p>Explaining black box text modules in natural language with language models. C Singh, arXiv:2305.098632023arXiv preprint</p>
<p>Language models can explain neurons in language models. S Bills, 14. 05. 20232023</p>
<p>Quantifying the dynamics of failure across science, startups and security. Y Yin, Y Wang, J A Evans, D Wang, Nature. 5752019</p>
<p>. S A Kauffman, Investigations, 2000Oxford University Press</p>
<p>A mobile robotic chemist. B Burger, Nature. 5832020. 2020</p>
<p>Large language models as optimizers. C Yang, arXiv:2309.034092023arXiv preprint</p>
<p>Q Wang, D Downey, H Ji, T Hope, Scimon, arXiv:2305.14259Scientific Inspiration Machines Optimized for Novelty. 2023arXiv preprint</p>
<p>DreamCoder: growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. K Ellis, Philosophical Transactions of the Royal Society A. 3812020</p>
<p>Exploring The Behavior of Bioelectric Circuits using Evolution Heuristic Search. H Hazan, M Levin, Bioelectricity. 42022</p>
<p>Recombinant uncertainty in technological search. L Fleming, Manage Sci. 472001</p>
<p>Optimal Search for the Best Alternative. M Weitzman, 1978Department of Energy78</p>
<p>ChatGPT: five priorities for research. E A M Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, Nature. 6142023</p>
<p>Why ChatGPT and Bing Chat are so good at making things up. B Edwards, Ars Technica. 2023</p>
<p>Shaking the foundations: delusions in sequence models for interaction and control. Deepmind, 2023</p>
<p>R Wang, arXiv:2309.05660Hypothesis search: Inductive reasoning with language models. 2023arXiv preprint</p>
<p>Technology readiness levels for machine learning systems. A Lavin, Nat Commun. 132020</p>
<p>Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization. J P Zhou, ICLR 202412th International Conference on Learning Representations. 2024</p>
<p>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers. T X Olausson, 10.18653/V1/2023.EMNLP-MAIN.313EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 202351535176</p>
<p>Hypothesis Search: Inductive Reasoning with Language Models. R Wang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>On the Measure of Intelligence. F Chollet, 2019</p>
<p>Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment. A Abdel-Rehim, Royal Society Interface. 22202406742025</p>
<p>Assessing and Understanding Creativity in Large Language Models. Y Zhao, arXiv:2401.12491[cs.CL]2024</p>
<p>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, 10.1101/2024.11.11.6230042024</p>
<p>Ideas are dimes a dozen: Large language models for idea generation in innovation. K Girotra, L Meincke, C Terwiesch, K T Ulrich, Available at SSRN. 45260712023</p>
<p>Large Language Models are Zero Shot Hypothesis Proposers. B Qi, arXiv:2311.059652023arXiv preprint</p>
<p>Explaining black box text modules in natural language with language models. C Singh, 2023</p>
<p>LLMs learn governing principles of dynamical systems. T J B Liu, N Boull√©, R Sarfati, C J Earls, arXiv:2402.007952024arXiv preprintrevealing an</p>
<p>Language Modeling Is Compression. G Del√©tang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>. Y Huang, J Zhang, Z Shan, He, J. Compression Represents Intelligence Linearly. 2024</p>
<p>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. Y Shao, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 1. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 12024</p>
<p>Introducing Perplexity Deep Research. A I Perplexity, 2025</p>
<p>Human-in-the-Loop through Chain-of-Thought. Z Cai, B Chang, W Han, 2023</p>
<p>F Petroni, 10.18653/v1/d19-1250Language Models as Knowledge Bases? EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference 2463-2473. 2019</p>
<p>A mathematical investigation of hallucination and creativity in gpt models. M Lee, Mathematics. 1123202023</p>
<p>Evaluating Large Language Models Trained on Code. M Chen, 2021</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, ACM Comput Surv. 552022</p>
<p>Artificial Intelligence May Not 'Hallucinate' After All. L Matsakis, 2019</p>
<p>A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Example Researchers Need to Expand What is Meant by 'Robustness. J Gilmer, D Hendrycks, 20194</p>
<p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. S M T I Tonmoy, 2024</p>
<p>N Varshney, W Yao, H Zhang, J Chen, D Yu, Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. 2023</p>
<p>More Agents Is All You Need. J Li, Q Zhang, Y Yu, Q Fu, D Ye, 2024</p>
<p>Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. N M√ºndler, J He, S Jenko, M Vechev, 2023</p>
<p>Chain-of-Verification Reduces Hallucination in Large Language Models. S Dhuliawala, 10.18653/V1/2024.FINDINGS-ACL.212Findings of the Association for Computational Linguistics ACL. 20242024</p>
<p>RARR: Researching and Revising What Language Models Say, Using Language Models. L Gao, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2023</p>
<p>On the Calibration of Large Language Models and Alignment. C Zhu, B Xu, Q Wang, Y Zhang, Z Mao, 2022</p>
<p>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. J Li, X Cheng, W X Zhao, J Y Nie, J R Wen, 10.18653/V1/2023.EMNLP-MAIN.397EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 20236449</p>
<p>Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, Truthfulqa, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2021</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. S Min, 10.18653/v1/2023.emnlp-main.741EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing, Proceedings 12076-12100. 2023</p>
<p>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. Y Liu, 2023</p>
<p>Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection. M Li, 2024</p>
<p>How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. Q Ye, H Y Fu, X Ren, R Jia, 10.18653/v1/2023.findings-emnlp.503EMNLP. 20232023Findings of the Association for Computational Linguistics</p>
<p>The Reversal Curse: LLMs trained on 'A is B' fail to learn. L Berglund, arXiv:2309.122882023B is A'. arXiv preprint</p>
<p>Premise Order Matters in Reasoning with Large Language Models. X Chen, R A Chi, X Wang, D Zhou, Proc Mach Learn Res. 2352024</p>
<p>Physics of Language Models: Part 3.2, Knowledge Manipulation. Z Allen-Zhu, Yuanzhi Labs, F Li Yuanzhili, 2023</p>
<p>Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models. M Nezhurina, L Cipolina-Kun, M Cherti, J Jitsev, 2024</p>
<p>Faith and Fate: Limits of Transformers on Compositionality. N Dziri, Adv Neural Inf Process Syst. 362023</p>
<p>G Del√©tang, ICLR 2023Neural Networks and the Chomsky Hierarchy. 11th International Conference on Learning Representations. 2022</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, Proc Natl Acad Sci U S A. 121e23224201212024</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Z Wu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 1. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 12024</p>
<p>LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks. S Kambhampati, 2024</p>
<p>On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). V Pallagani, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling202434</p>
<p>Large Language Models Cannot Self-Correct Reasoning Yet. J Huang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, ICLR 202311th International Conference on Learning Representations. 2022</p>
<p>Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. K Zhou, J D Hwang, X Ren, M Sap, arXiv:2401.067302024arXiv preprint</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, Proc Mach Learn Res. 2352023</p>
<p>. O Golovneva, Z Allen-Zhu, J Weston, S Sukhbaatar, arXiv:2403.137992024arXiv preprintReverse Training to Nurse the Reversal Curse</p>
<p>interpreting GPT: the logit lens -LessWrong. </p>
<p>Representation engineering: A top-down approach to ai transparency. A Zou, arXiv:2310.014052023arXiv preprint</p>
<p>Do models explain themselves? Counterfactual simulatability of natural language explanations. Y Chen, arXiv:2307.086782023arXiv preprint</p>
<p>Attention is not not Explanation. S Wiegreffe, Y Pinter, 10.18653/V1/D19-1002EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 201911</p>
<p>Attention is not Explanation. S Jain, B C Wallace, 10.18653/V1/N19-1357Proceedings of the 2019 Conference of the North. the 2019 Conference of the North2019</p>
<p>Attention is all you need: utilizing attention in AI-enabled drug discovery. Y Zhang, Brief Bioinform. 252023</p>
<p>Aviary: training language agents on challenging scientific tasks. S Narayanan, 2024</p>
<p>Learning from negative findings. M I Taragin, Isr J Health Policy Res. 82019</p>
<p>Publishing negative results is good for science. E M Bik, Access Microbiol. 67922024</p>
<p>Researcher's Perceptions on Publishing "Negative" Results and Open Access. L Echevarri√°, A Malerba, V Arechavala-Gomeza, Nucleic Acid Ther. 311852021</p>
<p>ChatGPT 'contamination': estimating the prevalence of LLMs in the scholarly literature. A Gray, 10.1002/leap.15782024</p>
<p>Mapping the Increasing Use of LLMs in Scientific Papers. W Liang, 2024</p>
<p>G R Latona, M H Ribeiro, T R Davidson, V Veselovsky, R West, The AI Review Lottery: Widespread AI-Assisted Peer Reviews Boost Paper Scores and Acceptance Rates. 2024</p>
<p>Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews. W Liang, 2024</p>
<p>Can Large Language Models Provide Useful Feedback on Research Papers? A Large-Scale Empirical Analysis. W Liang, NEJM AI. 12024</p>
<p>Can ChatGPT evaluate research quality. M Thelwall, Journal of Data and Information Science. 92024</p>
<p>ChatGPT and large language models in academia: opportunities and challenges. J G Meyer, BioData Min. 16202023</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, arXiv:2302.128132023arXiv preprint</p>
<p>Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. L Luo, Y.-F Li, G Haffari, S Pan, 2023</p>
<p>Towards Mitigating LLM Hallucination via Self Reflection. Z Ji, 10.18653/V1/2023.FINDINGS-EMNLP.1232023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, Adv Neural Inf Process Syst. 362023</p>
<p>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning. X Ye, G Durrett, Adv Neural Inf Process Syst. 352022</p>
<p>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models. A Ghandeharioun, A Caciularu, A Pearce, L Dixon, M Geva, Proc Mach Learn Res. 2352024</p>
<p>Academic publishing and the myth of linguistic injustice. K Hyland, J Second Lang Writ. 312016</p>
<p>Awkward wording. Rephrase"': linguistic injustice in ecological journals. M Clavero, 2010</p>
<p>Shakespeare and the English poets: The influence of native speaking English reviewers on the acceptance of journal articles. P Strauss, Publications. 7202019</p>
<p>Night science. I Yanai, M Lercher, Genome Biol. 202019</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, Nature. 5292016. 2016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, Science. 3621979. 2018</p>
<p>Absolute Zero: Reinforced Self-play Reasoning with Zero Data. A Zhao, 2025</p>
<p>Computational disease modeling -Fact or fiction?. J N Tegn√©r, BMC Syst Biol. 32009</p>            </div>
        </div>

    </div>
</body>
</html>
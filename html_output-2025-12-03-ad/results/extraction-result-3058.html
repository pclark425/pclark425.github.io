<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3058 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3058</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3058</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-265157947</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.07687v1.pdf" target="_blank">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks. CALM, a popular approach, leverages linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games. In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire. We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs. We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3058.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3058.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-in-the-Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model-in-the-Loop (this paper's method)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online finetuning loop where a pretrained GPT-2 generates action candidates for a DRRN agent and is periodically updated using in-game transitions sampled from replay buffers (D+, D-), with sampling heuristics and weighted losses to bias learning toward useful transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop (GPT-2 + DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 (117M, GPT-2 base) is first finetuned on ClubFloyd, then used to generate action candidates at each step; a DRRN (separate encoders for observation and action + MLP Q head) selects actions. The GPT-2 model is further finetuned during gameplay on sampled in-game transitions (D+, D-) using causal LM objective with optional importance weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (10 selected games, incl. Zork1, Zork3, Ludicorp, Inhumane, Ztuu)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate / recommend textual action candidates for an agent playing interactive fiction games; the DRRN selects among candidates to maximize episodic reward (game score).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external episodic experience replay (categorized buffers D+, D-); importance-weighted experience</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Transitions (o_t, a_t, o_{t+1}, r_{t+1}) are stored in FIFO replay buffers of max size 100K, split into positive (D+) and negative (D-) according to heuristics (state-feature change, reward trace) or kept uncategorized; sampling probability between buffers is p+; every k game steps d_LM samples are drawn and GPT-2 is finetuned for 2000 gradient steps using weighted cross-entropy where weight h(·) can be uniform, exp(β·A(o,a)) (UT_EA), or 1+β·A(o,a) (UT_LA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Using state-feature categorized in-game replay (OC) the paper reports avg.norm = 24.0% (Table 1), compared to CALM baseline avg.norm = 20.1% (absolute +3.9 percentage points); LM-in-the-Loop also accelerated convergence (authors report ~2–3× faster to reach CALM's best).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline (CALM) where GPT-2 is only adapted on ClubFloyd and not updated in-game: avg.norm = 20.1% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Improved final normalized average score (OC: 24.0% vs baseline 20.1%), faster convergence (2–3× fewer environment steps to reach baseline best), and reduced reliance on large human-annotated adaptation sets (OC with 10% ClubFloyd outperformed CALM with 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Benefits depended strongly on how transitions were selected; naive uncategorized replay (UT) reduced performance (avg.norm 19.1%); importance-weighting variants yielded marginal gains only; in-game training reduced generalization/transferability to other games; using GPT-2 alone (no DRRN) still failed, indicating replay-updated LM did not learn sufficient planning.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use explicit transition selection heuristics (state-feature based OC using location changes/reward increases) when constructing replay buffers; tune p+ (ratio of positive samples) and LM finetuning frequency k; be cautious about relying solely on simple advantage-weighted reweighting (UT_EA/UT_LA) as it produced marginal gains compared to OC; expect limited zero-shot transfer—retrain or adapt when moving to different games.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3058.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CALM (Keep CALM and explore: Language models for action generation in text-based games; Yao et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior approach that adapts GPT-2 on a large human gameplay corpus (ClubFloyd) to generate candidate actions for downstream DRRN agents, but keeps the LM frozen during in-game RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and explore: Language models for action generation in textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CALM (GPT-2 adapted on ClubFloyd, frozen during RL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 pretrained then adapted on ClubFloyd human gameplay transcripts to bias generation toward plausible game actions; the adapted LM provides candidate actions to a DRRN which is trained on environment rewards while the LM remains fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (as baseline across same selected games)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate plausible action candidates for DRRN in text-based games using human gameplay adaptation; goal is to improve sample efficiency and final score of RL agent.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>CALM (LM not updated in-game) avg.norm reported 20.1% when GPT-2 was adapted with full ClubFloyd dataset (Table 1); with only 10% adaptation the avg.norm dropped to 18.5% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Relies heavily on the size/coverage of the human-annotated adaptation set (performance drops with less adaptation data); no in-game adaptation means no leveraging of game-specific transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>If limited human adaptation data is available, consider supplementing with in-game replay-based finetuning (LM-in-the-Loop) to recover or exceed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3058.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2-as-Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 used as a policy network (ablation in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation where the adapted/trained GPT-2 selects argmax action directly (no DRRN); evaluated both frozen and after in-game finetuning to test whether LM alone can serve as decision maker.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-2 (policy: argmax over LM probabilities)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Domain-adapted GPT-2 (ClubFloyd) or GPT-2 subsequently finetuned with in-game transitions; action chosen is the most-likely token sequence (argmax) produced by the LM, without a downstream Q-network.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho (same selected games; ablation reported in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Directly choose actions in text games using LM decoding (test whether LM can replace DRRN planning)</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>When used as policy in ablation, performance was evaluated both with LM frozen and with LM that had been updated in-game; however, the action selection itself used only LM parameters (no replay buffer at decision time).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Performance was poor: normalized score reported ~1.1% (Table 3) and many individual games had 0 score when GPT-2 acted as the policy, irrespective of whether LM had been finetuned in-game or kept frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>None observed — in-game finetuning of LM did not allow GPT-2 to act as a competent planner by itself.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>LM-as-policy failed on most games (zeros in many cases), indicating the LM did not internalize planning/value estimates from game interaction sufficient for decision-making; DRRN performed the planning/heavy-lifting.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Do not rely on LM argmax as the sole decision mechanism for complex text games; pair generative LMs with value/action-evaluation modules (e.g., DRRN) and use replay to adapt LM suggestions rather than replace RL planners.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3058.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Experience Replay (D+, D-)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Categorized Experience Replay Buffers (D+ and D-)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replay memory design where in-game transitions are split into positive (D+) and negative (D-) buffers according to heuristics (state-feature change, reward trajectories), and sampled with a tunable probability p+ to finetune the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Replay-augmented LM-in-the-Loop (GPT-2 + DRRN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same LM+DRRN architecture; the distinctive element is the categorized replay storing transitions labelled useful/unuseful and used to update the LM periodically.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use categorized replay as memory to select which in-game transitions to use for LM finetuning, aiming to bias LM toward generating higher-value actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external categorized episodic replay (positive/negative buffers)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>FIFO buffers of size up to 100K; D+ holds transitions judged useful (by OC/RT heuristics), D- holds others; sampling draws d_LM uniformly from chosen buffer(s) according to p+; LM updated every k steps for 2000 gradient steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When OC categorization used to populate D+ (state-feature labels), avg.norm = 24.0% (best reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Uncategorized single buffer (UT) produced avg.norm = 19.1% — worse than baseline, showing that naive replay can hurt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Properly curated replay (OC) produced the largest gains and fastest convergence; allows reducing human adaptation data needs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Naive/unfiltered replay (UT) reduced performance; requires good heuristics to mark useful transitions; heuristics may be game-specific and limit transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Prefer state-feature-based labeling (OC) when possible (e.g., detect location change or reward increase) and tune p+, buffer sizes, and finetune frequency k; avoid naive uncategorized replay.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3058.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State-Feature Categorization (OC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oracle/State-Feature Categorized Transition Selection (OC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic labeling transitions as positive if they produce reward increase or change agent location (room change), used to populate D+ and prioritize LM finetuning samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop with OC transition selection</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same GPT-2 + DRRN pipeline; transitions are categorized using environment state features (oracle-style signals like 'location changed' or 'reward > 0') to bias LM updates toward exploration-useful transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select informative transitions to update LM so generated candidates better support exploration and task progress.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>annotated episodic memory (oracle-labeled replay)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>OC labels a transition positive if action produced reward increase or location change; these populate D+ which is sampled preferentially (p+ tuned) to finetune GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Best-performing LM-in-the-Loop variant: avg.norm = 24.0% and higher per-game scores (Table 1); accelerated convergence (~2–3×).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared against UT (uncategorized) which yielded avg.norm = 19.1% (lower than baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Largest gains in final score and convergence speed among tested memory-selection strategies; reduced dependence on large human adaptation datasets (OC with 10% ClubFloyd surpassed CALM with 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Requires access to environment state signals (e.g., location), which may not be available in all text-game setups and may be game-specific; acts as a loose upper bound (oracle) rather than a general solution.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>If environment provides state features (location/reward signals), use them to label and prioritize replay samples for LM finetuning; otherwise, design domain-appropriate proxies for transition usefulness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3058.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward Trajectories (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward-based Transition Categorization (RT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Categorize transitions as positive if they lie on trajectories leading to non-zero rewards (all transitions up to an earlier non-zero reward are D+), then use D+ to finetune the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop with Reward Trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-2 + DRRN where transitions are classified into positive/negative based on observed reward sequences and used to populate replay buffers for LM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select transitions that are on reward-yielding trajectories to bias LM toward actions that lead to reward.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic replay filtered by reward trajectory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>All transitions from episode start until the most recent non-zero reward are considered positive and stored in D+; sampled for LM finetuning similarly to OC.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>RT variant produced marginal improvement over UT (Table 1): reported avg.norm ~20.7% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>UT baseline avg.norm = 19.1%; OC performs better (24.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Provides modest gains over naive replay by biasing LM updates toward reward-yielding behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Smaller gains than state-feature OC; reward sparsity can limit the amount of positive data; still requires careful design to avoid overfitting to short reward trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use reward-based selection when state features are unavailable; combine with other heuristics or weighting to improve utility in sparse-reward environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3058.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3058.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weighted LM Loss (UT_EA / UT_LA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Advantage-weighted LM Finetuning (UT_EA and UT_LA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants that reweight LM finetuning samples by the action advantage A(o,a) to upweight high-advantage transitions (UT_EA: exponential scaling; UT_LA: linear additive scaling similar to unlikelihood training).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LM-in-the-Loop with advantage-weighted LM loss</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Single-buffer replay (UT) but finetuning loss is scaled per-sample by h(o,a)=exp(β·A) (UT_EA) or h(o,a)=1+β·A (UT_LA), where A is advantage estimated by DRRN, to encourage LM to prefer high-advantage actions.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Incorporate action-value estimates from the RL agent into LM finetuning to align LM generation likelihoods with downstream action utility.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay with importance weighting by advantage</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Transitions stored in single buffer D; per-sample weight h(·) computed from DRRN advantage A(o,a) and used to scale cross-entropy loss during LM updates; hyperparameter β controls scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>UT_EA and UT_LA produced small improvements over UT: reported avg.norm ~20.9% (UT_EA) and ~20.6% (UT_LA) vs UT 19.1% and baseline 20.1% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>UT (vanilla cross-entropy on single buffer) avg.norm = 19.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Advantage-weighting provided modest improvements relative to uncategorized replay, by incorporating agent value signals into LM updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Gains were marginal compared to OC; choice of β and handling of negative advantages matter (UT_EA clamps to non-negative weights), and instability or scaling issues could limit benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Incorporate action-value signals into LM finetuning when oracle state features are unavailable, but expect only modest improvements; carefully tune weighting schedule and manage negative advantages (e.g., use bounded transforms).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Keep CALM and explore: Language models for action generation in textbased games <em>(Rating: 2)</em></li>
                <li>Pre-trained language models for interactive decisionmaking <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 1)</em></li>
                <li>Experience grounds language <em>(Rating: 1)</em></li>
                <li>Reading and acting while blindfolded: The need for semantics in text game agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3058",
    "paper_id": "paper-265157947",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "LM-in-the-Loop",
            "name_full": "Language Model-in-the-Loop (this paper's method)",
            "brief_description": "An online finetuning loop where a pretrained GPT-2 generates action candidates for a DRRN agent and is periodically updated using in-game transitions sampled from replay buffers (D+, D-), with sampling heuristics and weighted losses to bias learning toward useful transitions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop (GPT-2 + DRRN)",
            "agent_description": "GPT-2 (117M, GPT-2 base) is first finetuned on ClubFloyd, then used to generate action candidates at each step; a DRRN (separate encoders for observation and action + MLP Q head) selects actions. The GPT-2 model is further finetuned during gameplay on sampled in-game transitions (D+, D-) using causal LM objective with optional importance weighting.",
            "game_or_benchmark_name": "Jericho (10 selected games, incl. Zork1, Zork3, Ludicorp, Inhumane, Ztuu)",
            "task_description": "Generate / recommend textual action candidates for an agent playing interactive fiction games; the DRRN selects among candidates to maximize episodic reward (game score).",
            "uses_memory": true,
            "memory_type": "external episodic experience replay (categorized buffers D+, D-); importance-weighted experience",
            "memory_implementation_details": "Transitions (o_t, a_t, o_{t+1}, r_{t+1}) are stored in FIFO replay buffers of max size 100K, split into positive (D+) and negative (D-) according to heuristics (state-feature change, reward trace) or kept uncategorized; sampling probability between buffers is p+; every k game steps d_LM samples are drawn and GPT-2 is finetuned for 2000 gradient steps using weighted cross-entropy where weight h(·) can be uniform, exp(β·A(o,a)) (UT_EA), or 1+β·A(o,a) (UT_LA).",
            "performance_with_memory": "Using state-feature categorized in-game replay (OC) the paper reports avg.norm = 24.0% (Table 1), compared to CALM baseline avg.norm = 20.1% (absolute +3.9 percentage points); LM-in-the-Loop also accelerated convergence (authors report ~2–3× faster to reach CALM's best).",
            "performance_without_memory": "Baseline (CALM) where GPT-2 is only adapted on ClubFloyd and not updated in-game: avg.norm = 20.1% (Table 1).",
            "has_performance_comparison": true,
            "memory_benefits": "Improved final normalized average score (OC: 24.0% vs baseline 20.1%), faster convergence (2–3× fewer environment steps to reach baseline best), and reduced reliance on large human-annotated adaptation sets (OC with 10% ClubFloyd outperformed CALM with 100%).",
            "memory_limitations_or_failures": "Benefits depended strongly on how transitions were selected; naive uncategorized replay (UT) reduced performance (avg.norm 19.1%); importance-weighting variants yielded marginal gains only; in-game training reduced generalization/transferability to other games; using GPT-2 alone (no DRRN) still failed, indicating replay-updated LM did not learn sufficient planning.",
            "best_practices_or_recommendations": "Use explicit transition selection heuristics (state-feature based OC using location changes/reward increases) when constructing replay buffers; tune p+ (ratio of positive samples) and LM finetuning frequency k; be cautious about relying solely on simple advantage-weighted reweighting (UT_EA/UT_LA) as it produced marginal gains compared to OC; expect limited zero-shot transfer—retrain or adapt when moving to different games.",
            "uuid": "e3058.0",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "CALM",
            "name_full": "CALM (Keep CALM and explore: Language models for action generation in text-based games; Yao et al., 2020)",
            "brief_description": "Prior approach that adapts GPT-2 on a large human gameplay corpus (ClubFloyd) to generate candidate actions for downstream DRRN agents, but keeps the LM frozen during in-game RL.",
            "citation_title": "Keep CALM and explore: Language models for action generation in textbased games",
            "mention_or_use": "use",
            "agent_name": "CALM (GPT-2 adapted on ClubFloyd, frozen during RL)",
            "agent_description": "GPT-2 pretrained then adapted on ClubFloyd human gameplay transcripts to bias generation toward plausible game actions; the adapted LM provides candidate actions to a DRRN which is trained on environment rewards while the LM remains fixed.",
            "game_or_benchmark_name": "Jericho (as baseline across same selected games)",
            "task_description": "Generate plausible action candidates for DRRN in text-based games using human gameplay adaptation; goal is to improve sample efficiency and final score of RL agent.",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": null,
            "performance_with_memory": null,
            "performance_without_memory": "CALM (LM not updated in-game) avg.norm reported 20.1% when GPT-2 was adapted with full ClubFloyd dataset (Table 1); with only 10% adaptation the avg.norm dropped to 18.5% (Table 2).",
            "has_performance_comparison": true,
            "memory_benefits": null,
            "memory_limitations_or_failures": "Relies heavily on the size/coverage of the human-annotated adaptation set (performance drops with less adaptation data); no in-game adaptation means no leveraging of game-specific transitions.",
            "best_practices_or_recommendations": "If limited human adaptation data is available, consider supplementing with in-game replay-based finetuning (LM-in-the-Loop) to recover or exceed performance.",
            "uuid": "e3058.1",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-2-as-Policy",
            "name_full": "GPT-2 used as a policy network (ablation in this paper)",
            "brief_description": "Ablation where the adapted/trained GPT-2 selects argmax action directly (no DRRN); evaluated both frozen and after in-game finetuning to test whether LM alone can serve as decision maker.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-2 (policy: argmax over LM probabilities)",
            "agent_description": "Domain-adapted GPT-2 (ClubFloyd) or GPT-2 subsequently finetuned with in-game transitions; action chosen is the most-likely token sequence (argmax) produced by the LM, without a downstream Q-network.",
            "game_or_benchmark_name": "Jericho (same selected games; ablation reported in Table 3)",
            "task_description": "Directly choose actions in text games using LM decoding (test whether LM can replace DRRN planning)",
            "uses_memory": false,
            "memory_type": null,
            "memory_implementation_details": "When used as policy in ablation, performance was evaluated both with LM frozen and with LM that had been updated in-game; however, the action selection itself used only LM parameters (no replay buffer at decision time).",
            "performance_with_memory": null,
            "performance_without_memory": "Performance was poor: normalized score reported ~1.1% (Table 3) and many individual games had 0 score when GPT-2 acted as the policy, irrespective of whether LM had been finetuned in-game or kept frozen.",
            "has_performance_comparison": true,
            "memory_benefits": "None observed — in-game finetuning of LM did not allow GPT-2 to act as a competent planner by itself.",
            "memory_limitations_or_failures": "LM-as-policy failed on most games (zeros in many cases), indicating the LM did not internalize planning/value estimates from game interaction sufficient for decision-making; DRRN performed the planning/heavy-lifting.",
            "best_practices_or_recommendations": "Do not rely on LM argmax as the sole decision mechanism for complex text games; pair generative LMs with value/action-evaluation modules (e.g., DRRN) and use replay to adapt LM suggestions rather than replace RL planners.",
            "uuid": "e3058.2",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Experience Replay (D+, D-)",
            "name_full": "Categorized Experience Replay Buffers (D+ and D-)",
            "brief_description": "Replay memory design where in-game transitions are split into positive (D+) and negative (D-) buffers according to heuristics (state-feature change, reward trajectories), and sampled with a tunable probability p+ to finetune the LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Replay-augmented LM-in-the-Loop (GPT-2 + DRRN)",
            "agent_description": "Same LM+DRRN architecture; the distinctive element is the categorized replay storing transitions labelled useful/unuseful and used to update the LM periodically.",
            "game_or_benchmark_name": "Jericho",
            "task_description": "Use categorized replay as memory to select which in-game transitions to use for LM finetuning, aiming to bias LM toward generating higher-value actions.",
            "uses_memory": true,
            "memory_type": "external categorized episodic replay (positive/negative buffers)",
            "memory_implementation_details": "FIFO buffers of size up to 100K; D+ holds transitions judged useful (by OC/RT heuristics), D- holds others; sampling draws d_LM uniformly from chosen buffer(s) according to p+; LM updated every k steps for 2000 gradient steps.",
            "performance_with_memory": "When OC categorization used to populate D+ (state-feature labels), avg.norm = 24.0% (best reported).",
            "performance_without_memory": "Uncategorized single buffer (UT) produced avg.norm = 19.1% — worse than baseline, showing that naive replay can hurt performance.",
            "has_performance_comparison": true,
            "memory_benefits": "Properly curated replay (OC) produced the largest gains and fastest convergence; allows reducing human adaptation data needs.",
            "memory_limitations_or_failures": "Naive/unfiltered replay (UT) reduced performance; requires good heuristics to mark useful transitions; heuristics may be game-specific and limit transfer.",
            "best_practices_or_recommendations": "Prefer state-feature-based labeling (OC) when possible (e.g., detect location change or reward increase) and tune p+, buffer sizes, and finetune frequency k; avoid naive uncategorized replay.",
            "uuid": "e3058.3",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "State-Feature Categorization (OC)",
            "name_full": "Oracle/State-Feature Categorized Transition Selection (OC)",
            "brief_description": "A heuristic labeling transitions as positive if they produce reward increase or change agent location (room change), used to populate D+ and prioritize LM finetuning samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop with OC transition selection",
            "agent_description": "Same GPT-2 + DRRN pipeline; transitions are categorized using environment state features (oracle-style signals like 'location changed' or 'reward &gt; 0') to bias LM updates toward exploration-useful transitions.",
            "game_or_benchmark_name": "Jericho",
            "task_description": "Select informative transitions to update LM so generated candidates better support exploration and task progress.",
            "uses_memory": true,
            "memory_type": "annotated episodic memory (oracle-labeled replay)",
            "memory_implementation_details": "OC labels a transition positive if action produced reward increase or location change; these populate D+ which is sampled preferentially (p+ tuned) to finetune GPT-2.",
            "performance_with_memory": "Best-performing LM-in-the-Loop variant: avg.norm = 24.0% and higher per-game scores (Table 1); accelerated convergence (~2–3×).",
            "performance_without_memory": "Compared against UT (uncategorized) which yielded avg.norm = 19.1% (lower than baseline).",
            "has_performance_comparison": true,
            "memory_benefits": "Largest gains in final score and convergence speed among tested memory-selection strategies; reduced dependence on large human adaptation datasets (OC with 10% ClubFloyd surpassed CALM with 100%).",
            "memory_limitations_or_failures": "Requires access to environment state signals (e.g., location), which may not be available in all text-game setups and may be game-specific; acts as a loose upper bound (oracle) rather than a general solution.",
            "best_practices_or_recommendations": "If environment provides state features (location/reward signals), use them to label and prioritize replay samples for LM finetuning; otherwise, design domain-appropriate proxies for transition usefulness.",
            "uuid": "e3058.4",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Reward Trajectories (RT)",
            "name_full": "Reward-based Transition Categorization (RT)",
            "brief_description": "Categorize transitions as positive if they lie on trajectories leading to non-zero rewards (all transitions up to an earlier non-zero reward are D+), then use D+ to finetune the LM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop with Reward Trajectories",
            "agent_description": "GPT-2 + DRRN where transitions are classified into positive/negative based on observed reward sequences and used to populate replay buffers for LM updates.",
            "game_or_benchmark_name": "Jericho",
            "task_description": "Select transitions that are on reward-yielding trajectories to bias LM toward actions that lead to reward.",
            "uses_memory": true,
            "memory_type": "episodic replay filtered by reward trajectory",
            "memory_implementation_details": "All transitions from episode start until the most recent non-zero reward are considered positive and stored in D+; sampled for LM finetuning similarly to OC.",
            "performance_with_memory": "RT variant produced marginal improvement over UT (Table 1): reported avg.norm ~20.7% (Table 1).",
            "performance_without_memory": "UT baseline avg.norm = 19.1%; OC performs better (24.0%).",
            "has_performance_comparison": true,
            "memory_benefits": "Provides modest gains over naive replay by biasing LM updates toward reward-yielding behaviors.",
            "memory_limitations_or_failures": "Smaller gains than state-feature OC; reward sparsity can limit the amount of positive data; still requires careful design to avoid overfitting to short reward trajectories.",
            "best_practices_or_recommendations": "Use reward-based selection when state features are unavailable; combine with other heuristics or weighting to improve utility in sparse-reward environments.",
            "uuid": "e3058.5",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Weighted LM Loss (UT_EA / UT_LA)",
            "name_full": "Advantage-weighted LM Finetuning (UT_EA and UT_LA variants)",
            "brief_description": "Variants that reweight LM finetuning samples by the action advantage A(o,a) to upweight high-advantage transitions (UT_EA: exponential scaling; UT_LA: linear additive scaling similar to unlikelihood training).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LM-in-the-Loop with advantage-weighted LM loss",
            "agent_description": "Single-buffer replay (UT) but finetuning loss is scaled per-sample by h(o,a)=exp(β·A) (UT_EA) or h(o,a)=1+β·A (UT_LA), where A is advantage estimated by DRRN, to encourage LM to prefer high-advantage actions.",
            "game_or_benchmark_name": "Jericho",
            "task_description": "Incorporate action-value estimates from the RL agent into LM finetuning to align LM generation likelihoods with downstream action utility.",
            "uses_memory": true,
            "memory_type": "experience replay with importance weighting by advantage",
            "memory_implementation_details": "Transitions stored in single buffer D; per-sample weight h(·) computed from DRRN advantage A(o,a) and used to scale cross-entropy loss during LM updates; hyperparameter β controls scaling.",
            "performance_with_memory": "UT_EA and UT_LA produced small improvements over UT: reported avg.norm ~20.9% (UT_EA) and ~20.6% (UT_LA) vs UT 19.1% and baseline 20.1% (Table 1).",
            "performance_without_memory": "UT (vanilla cross-entropy on single buffer) avg.norm = 19.1%.",
            "has_performance_comparison": true,
            "memory_benefits": "Advantage-weighting provided modest improvements relative to uncategorized replay, by incorporating agent value signals into LM updates.",
            "memory_limitations_or_failures": "Gains were marginal compared to OC; choice of β and handling of negative advantages matter (UT_EA clamps to non-negative weights), and instability or scaling issues could limit benefit.",
            "best_practices_or_recommendations": "Incorporate action-value signals into LM finetuning when oracle state features are unavailable, but expect only modest improvements; carefully tune weighting schedule and manage negative advantages (e.g., use bounded transforms).",
            "uuid": "e3058.6",
            "source_info": {
                "paper_title": "Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in textbased games",
            "rating": 2,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Pre-trained language models for interactive decisionmaking",
            "rating": 2,
            "sanitized_title": "pretrained_language_models_for_interactive_decisionmaking"
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        },
        {
            "paper_title": "Experience grounds language",
            "rating": 1,
            "sanitized_title": "experience_grounds_language"
        },
        {
            "paper_title": "Reading and acting while blindfolded: The need for semantics in text game agents",
            "rating": 1,
            "sanitized_title": "reading_and_acting_while_blindfolded_the_need_for_semantics_in_text_game_agents"
        }
    ],
    "cost": 0.01647375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023</p>
<p>Arjun Vaithilingam Sudhakar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>Prasanna Parthasarathi 
Janarthanan Rajendran 
Mila -Quebec AI Institute</p>
<p>University of Montreal</p>
<p>Sarath Chandar 
Mila -Quebec AI Institute</p>
<p>Ecole Polytechnique de Montreal</p>
<p>CIFAR AI Chair
Canada</p>
<p>Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games
13 Nov 2023E9CFC669378BBD22F5BE438C7D800830arXiv:2311.07687v1[cs.CL]
Large Language Models (LLMs) have demonstrated superior performance in language understanding benchmarks.CALM, a popular approach, leverages linguistic priors of LLMs-GPT-2-for action candidate recommendations to improve the performance in text games in Jericho without environment-provided actions.However, CALM adapts GPT-2 with annotated human gameplays and keeps the LLM fixed during the learning of the text based games.In this work, we explore and evaluate updating LLM used for candidate recommendation during the learning of the text based game as well to mitigate the reliance on the human annotated gameplays, which are costly to acquire.We observe that by updating the LLM during learning using carefully selected in-game transitions, we can reduce the dependency on using human annotated game plays for fine-tuning the LLMs.We conducted further analysis to study the transferability of the updated LLMs and observed that transferring in-game trained models to other games did not result in a consistent transfer.</p>
<p>Introduction</p>
<p>Large Language models (Devlin et al., 2019a;Radford et al., 2018b;Ouyang et al., 2022) (LLMs) trained on large corpora of unstructured text corpora are the state-of-the-art models in several Natural Language Understanding (NLU) benchmarks.Bender and Koller (2020) argue in their position paper that the models trained largely from static benchmarks rely to the form rather than understanding the meaning.While it is imperative to understand the learning dynamics of LLMs (Rogers et al., 2020;Webson and Pavlick, 2021), introducing novel language understanding challenges pushes the frontiers for LLMs' applications.There has been a recent interest in interactive training of large language models in situated learning environments.Bisk et al. (2020); McClelland et al. (2020) Figure 1: Sample gameplay from zork1 game in Jericho using LM for action recommendation: LM recommends action candidates based on the observation from env.The RL agent selects an action from the candidates.point out the necessity for LMs to have enhanced language understanding and meaning through interacting with the physical world.Also, Lake and Murphy (2021) argues that LMs fall short in their communicative usage, requiring reasoning over intents despite their success in static datasets.</p>
<p>Training decision making agents over textual information for playing text-based games (Hausknecht et al., 2020;Côté et al., 2018) has been a recent usecase for LLM.While decision making has been the front of text-game playing, such games introduce novel challenges for language understanding, and domain adaptation for LLMs.Yao et al. (2020) used GPT-2 (Radford et al., 2018b) to generate candidate actions for the decision making DRRN module (He et al., 2016) in Jericho benchmark of text based games.Such a set up allows for qualitatively understanding the LLMs' abilities to understand, reason, and adapt to novel situations.In a typical text-based game, as in Figure 1, an agent receives a textual observation about its environment that it has to understand and reason over the possible actions to pick one and proceed.</p>
<p>While learning from scratch is time-consuming, Yao et al. (2020) make use of linguistic priors in LLMs to prune the combinatorially large action space.The authors adapt GPT-2 for the task with a corpus of human game play on similar games-ClubFloyd.After the adaptation phase, the model remains frozen throughout the learning that happens within the game.</p>
<p>Further, Yao et al. (2020) also note that the performance on the text-based games in Jericho benchmark was sensitive to the size of the annotated human gameplay corpus; such reliance adds to the cost.On the one hand in-game transitions remain unutilized for training the LLM, and on the other there is a need to mitigate the reliance on human annotated transitions to scale applications of LLMs.Although one can make use of the transitions to train the model, the solution requires a comprehensive analysis on what such a LM-in-the-Loop training entails.Toward that, we explore LM-inthe-Loop by building over the setup in Yao et al. (2020) by training GPT-2 using in-game generated transitions.Further, we analyze such a set up along the metrics of: (1) Improvement in performance, (2) Acceleration in convergence, (3) Reliance on human annotated transitions, (4) Replacing GPT-2 as a policy network, (5) comparing reward, state based transitions selection for LM training, and (6) Generalization of LM-in-the-Loop trained LM to other games.The main findings of the approach are summarized as follows:</p>
<p>• LM-in-the-Loop reduces emphasis on human annotated transitions and enables accelerated convergence.</p>
<p>• State feature based transitions selection provided greater gains than other alternates.</p>
<p>• LM-in-the-Loop does not always transfer to other games.</p>
<p>• Although LM-in-the-Loop improved candidate suggestion, GPT-2 as policy network did faired poorly across games.</p>
<p>Related Work</p>
<p>Text Games: Jericho (Hausknecht et al., 2020) is a popular learning environment that supports 32 human-written interactive fiction games.These games are designed to be difficult for human players, serving as a more realistic training ground to evaluate language understanding agents.Compared with frameworks like TextWorld (Côté et al., 2018), these games have significantly more linguistic variety and larger action space.Jericho environment provides a smaller list of candidate actions that can be used to train reinforcement learning (RL) agents.Approaches like DRRN (He et al., 2016), TDQN (Hausknecht et al., 2020), and KGA2C (Ammanabrolu and Hausknecht, 2020) have used handicap to operate on small action space and learn only through in-game rewards.Towards using large LMs, environment provided actions are replaced with LM generated actions like with GPT-2 (Yao et al., 2020), or BERT (Singh et al., 2021).Li et al. (2022) train LMs to remember optimal trajectories to swiftly move to novel game regions.</p>
<p>Data Efficiency: LLMs (Devlin et al., 2019b;Brown et al., 2020) are pretrained with tremendous amount of unstructured text data from the web using a generic language modeling objective.Adapting the models to a downstream tasks (Khashabi et al., 2020;Rajpurkar et al., 2016;Zhang et al., 2015;Maas et al., 2011), however, has been shown to greatly affected by the quality of supervision and the size of the dataset.As reliance on annotated data makes their application hard to scale, techniques like data augmentation (Feng et al., 2021), using distilled models (Radford et al., 2018a), learning from toyish data (Wu et al., 2022) has been explored has alternatives.However, the approach of making LLMs interactive to be trained in a situated learning environment to reduce the need for annotations is only recently getting popular.</p>
<p>3 Background</p>
<p>Text Games</p>
<p>In text-based games, at each step t, a learning agent interacts with the game environment by generating a textual action a t ∈ A t that is relevant to the textual observation o t .The agent receives a scalar reward r t = R t (o t , a t ).The agent maximizes the expected cumulative rewards (r 0 , r 1 , r 2 , . . .r N ), until the end of an N -step long episode.</p>
<p>DRRN and Advantage Function</p>
<p>A popular deep RL method used in text-based games is the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016).The observation (o) and actions (a) are first encoded using separate recurrent neural network encoders (such as a GRU (Chung et al., 2014)) f o and f a respectively.A decoder g then combines the representations to obtain the Q-value using a network parameterized by Φ:
Q Φ (o, a) = g(f o (o), f a (a)).(1)
The DRRN learns to estimate the Q-value through iteratively updating Φ with experience sampled from a prioritized experience replay buffer with the temporal difference (TD) loss:
L T D (Φ) = r + γ max a ′ ∈A Q Φ (o ′ , a ′ ) − Q Φ (o, a) 2 ,
(2) where r and o ′ are the reward and the observation received after taking action a upon observing o, and γ represents the discount factor.</p>
<p>Advantage function: An estimate how good an action, a, is when chosen in a state, o, is obtained by subtracting the value of the state (V (o))a weighted average of the Q-values-from the Q(o, a) of that particular action in that state.
A(o, a) = Q Φ (o, a) − V ψ (o)(3)
Q-Value estimates the expected reward after a specific action was played, whereas V ψ (o) is the parameterized estimate of the expected reward from being in o before an action was selected.</p>
<p>LLM for Action Recommendation</p>
<p>Consider a dataset D of N transitions of human gameplay across different games organized in context-action pairs as ((o j−1 , a j−1 , o j ), a j ).For example: a sample could be like, " [CLS]. . . to the north is a restaurant where the mayor ate often.to the east is the mayor's home.2020) uses ClubFloyd to adapt a pretrained GPT-2 model with causal language modeling task.The motivation is to enable the linguistic prior of GPT-2 to adapt to the games and provide better action recommendations to the DRRN.</p>
<p>Methodology</p>
<p>LM-in-the-Loop to recommend Actions</p>
<p>The game playing agent follows trajectories that are rewarded according to the rules of the game in the Jericho environment.The environment has two scenarios-with and without handicap-which correspond to whether the actions can be generated from within the possible actions suggested by the environment or without any limitations by the environment respectively.The with handicap set up evaluates the agent exclusively on planning with the actions provided, while the without handicap requires the agent in addition to understanding the observation also generate acceptable candidates.In Yao et al. (2020), the LLM is kept constant throughout the gameplay and that assumption could be only validated if Jericho games share significant similarity with the transitions in ClubFloyd.However, MAUVE-Score1 between the human transitions and the game transitions in Jericho did not overlap significantly (Table 6 in §A.4), suggesting that the adapting from in-game transitions is needed.</p>
<p>Toward that, we explore the feasibility, prospects, and challenges that entail training LM-in-theloop post finetuning with human gameplays in ClubFloyd adaptation as in Table 1.We use a similar set up for action recommendation as in Yao et al. (2020), where a pretrained GPT-2 LM is adapted with Clubfloyd dataset to recommend actions to DRRN agent.In addition to training the DRRN agent with TD-learning (Equation 2), we collect the transitions (o t , a t , o t+1 , r t+1 ) throughout the game episode, e T D , and populate them in D + and D − based on a heuristic that depends on-reward, return, and the game states.</p>
<p>First, with LM parameterized by θ and generating action candidates, we train DRRN for n RL consecutive episodes.After n RL episodes, we sample d LM sized dataset from D + , and D − with probabil- ities p + and 1 − p + respectively for 2000 gradient steps at finetuned after every k game steps.To train LM we use a weighted cross-entropy loss:
L LM (θ) = −E (at,ot)∼(D + ,D − ) log P θ (a t | o t )•h (•)
(4) Then, we plug-in back the in-game trained LM to recommend actions for the DRRN agent.The maximum buffer size of D + , D − , p + , d LM , and n RL are all game-specific hyperparameters.The h (•) is defined as a function of reward r t , or action-advantage, A(o t , a t ), or assumed 1 uniformly ∀(o, a) ∈ O × A. We evaluate different approaches based on the sampling of transitions, and the loss function (L), used for training the language model.Approaches for LM-in-the-Loop based on the construction of D, and sampling are:</p>
<p>Uncategorized Transitions (UT): In this setting the transitions stored in the buffer are not categorized by any special heuristic function.We simplify this approach by maintaining a single buffer, D in place of two.This is a weaker baseline than other heuristics to select useful transitions based on their importance.</p>
<p>State Feature Categorized (OC): In this, the transitions are labeled as useful or not based on whether an action a t resulted in reward increase or if the agent's location changed.i.e., moved from one room to another.As the location information received is an artifact of the game framework, we consider this as the Oracle.Further, we vary p + to maximize the transitions that encourage exploration to eventually result in improved performance in the game.Here, h (•) is fixed as
1 uniformly ∀(o, a) ∈ O × A.
Reward Trajectories (RT): The reward from transitions, r t , is used to categorize positive and negative trajectories.When r t &gt; 0 all transitions up until the earlier non-zero reward are considered positive and added to D + .</p>
<p>Further, we explore utilizing the return, reward, and advantage function of actions to re-weight L LM using the h (•) function over UT setting as above.We describe them as follows:</p>
<p>Weighted Cross-Entropy: In this, the transition data is kept in a single buffer D similar to in the UT setting.To finetune the language model using the weighted cross-entropy loss (Equation 4), we use the exponential weighted advantage function (Equation 3).We use two variants to the weights, wherein UT EA is non-negative using h(•) function:
h(o t , a t ) = e β•A(ot,at) ,(5)
where, β ∈ R + is a hyperparameter.The other variant, UT LA , allows for negative weights with h(•) as follows:
h(o t , a t ) = 1 + β • A(o t , a t ),(6)
where, β ∈ R + is a hyperparameter.</p>
<p>Experiments</p>
<p>We perform comprehensive experiments2 with LMin-the-loop set up to study the following questions:</p>
<ol>
<li>
<p>Does including the language model in the training loop improve performance?</p>
</li>
<li>
<p>Does LM-in-the-Loop mitigate the reliance on human gameplay transitions?</p>
</li>
<li>
<p>Should the transitions be categorized for improved learning?</p>
</li>
<li>
<p>Can we make LM itself a policy network without DRRN with LM-in-the-Loop?</p>
</li>
<li>
<p>Does training LM-in-the-Loop affect generalization to other games?</p>
</li>
</ol>
<p>Task Adaptation Dataset</p>
<p>ClubFloyd dataset (Yao et al., 2020) is a collection of crawled data from the ClubFloyd website.The dataset comprises of gameplay from experienced players; however, they may not be familiar with the particular games.The data is preprocessed and contains around 217K pairs of context an in the form of ((o j−1 , a j−1 , o j ), a j ).</p>
<p>Benchmark and the Metric</p>
<p>Jericho (Hausknecht et al., 2020) is a learning environment that supports human-written interactive fiction games as described in Figure 1.We chose 10 games based on the diversity in the challenges faced in each game such as large action space, solution length, and reward sparsity as mentioned in Hausknecht et al. (2020).We use the average of the last 100-episodes' score with standard error for individual games (Hausknecht et al., 2020) as our metric for evaluation.</p>
<p>In addition, we report the average score normalized (avg.norm) against the maximum score possible in each of the games, which estimates the human-machine gap in text-based games.Finally, we also report the relative performance percentage difference between the baseline and the best approach mentioned as ∆% in Table 1 to capture the improvement as the range of the scores in each game is different.</p>
<p>Model Details</p>
<p>Language model (GPT-2) is first finetuned on ClubFloyd dataset.</p>
<p>Given the context, (o j−1 , a j−1 , o j ), the finetuned GPT-2 proposes action candidates for DRRN to choose.Following that, every action candidate and context is encoded with a GRU.Then a decoder combines the representations to estimate the Q-value using a multilayer Perceptron (MLP) and updates the DRRN agent parameter Φ.During the training process of the DRRN agents, the context-action pairs are stored in the replay buffers.After k steps, we sample d LM sized dataset from D + , and D − with probabilities p + and 1 − p + respectively and update the language model with in-game transitions.Then, the updated language model is used to propose the action candidates.</p>
<p>The buffer size is defined as 100K for replay buffers that uses First-In-First-Out (FIFO) strategy to replace samples.To train, d LM samples are sampled uniformly at random from the two buffers D + and D − .However, the probability of choosing the buffers are defined by p + and p − (1 − p + ) respectively.The number of gradient steps for LM training is fixed at 2000 across the set ups.And, across games we experiment with the hyperparameter p + ∈ [0, 1] in 0.1 increment, and the value for LM finetuning frequency k ∈ [2k, 5k, 10k, 20k].The results tabled are estimated from 5 runs.</p>
<p>Results</p>
<p>We follow the questions enumerated in §5 to analyze the effect of in-game learning of language models for action recommendations.</p>
<p>Effect on Performance</p>
<p>To understand the effect on performance with LMin-the-Loop, we follow the experimental set up in §5.3 to evaluate on Jericho benchmark.Table 5 compares the different methods detailed in §4.1 with reproduced norm score of CALM (Yao et al., 2020) as the baseline.We see that categorizing the transitions using state features (OC) scored the highest in all tasks, suggesting that LM-in-the-Loop enables improved performance.This was also reflected in the avg.norm score with an improvement of ≈ 4% over the baseline.This is ≈ 53% more avg.improvement over the scores obtained by the baseline model.Although the performances of OC are closer to the baseline in many games, the in-game training accelerated the convergence in most games.However, the improvement with OC is, in a way, a loose upperbound to in-game learning with LMin-the-Loop, as special techniques to reweight the transitions (UT LA , and UT EA ), or reward based categorization RT only improved the avg.norm score by ≈ 0.6%.On the other hand, the avg.norm score with Uncategorized Transitions (UT)   dropped to 19.2% which is ∼ 1% below the baseline performance.The difference in performance between UT, and OC with the baseline suggests that LM-in-the-loop for action recommendation is helpful but requires careful selection of transitions for training the language model.In Figure 3, we compare the % of steps in-game learning methods took in average to achieve k% of CALM model's best performance across the games.We see that LM-in-the-Loop techniques enabled atleast 2× on average 3 acceleration in convergence, although the weaker alternatives to OC with reward based categorization, and reweighted techniques only provided meagre improvements 3 Individual comparison of each method across the games is in §B.</p>
<p>over the baseline (Table 5).This shows that the adaptation offered with the ClubFloyd dataset was insufficient, and off-the-shelf techniques can drastically accelerate convergence.</p>
<p>Emphasis on Human Annotations</p>
<p>CALM model-the baseline-uses all of the ∼ 220K transitions in the ClubFloyd dataset to adapt GPT-2 model for action recommendation.But, by using in-game transitions for LM-in-the-Loop training, the LM is provided with game specific information.So, the requirement for adapting GPT-2 with human annotated transitions should be minimal.Yao et al. (2020) show that CALM's performance decreased significantly when adaptation was done with 10% of ClubFloyd dataset.The reproduced results of CALM with 10% of adaptation data shows the avg.norm score as 18.5% across the games in Table 2. Using State features (OC) with 10% of the adaptation date achieved an average norm score of 21.8%, which was more than even using 100% of the adaptation data with CALM.Although there was a small decline in the performance of the detective game, it was insignificant because it was still within the standard error.These results suggest empirically that we can reduce the burden of collecting human-played or human-annotated data by doing in-game learning.</p>
<p>Effect of Weight Adjusted LM Loss</p>
<p>Categorization of transitions, although possible in most games, often requires game specific functions to identify what is a good and a bad transition.However, a generalized technique would be to use a notion of the usefulness of transitions that don't require game specific mechanisms.We explore reweighted cross entropy loss as in Equation 4with variations of the h(•) functions from being uniformly distributed as 1 over (o, a) ∈ O × A to using advantage function with two variations as in Equation 5and Equation 6.While UT uses vanilla cross-entropy loss to train the LM on transitions sampled from buffer D, UT EA and UT LA adjusts the experience according to the advantage, A(o, a), of the actions chosen in those observations.We use causal language modeling to train the GPT-2 LM to discourage the LM in generating a useful action in a state and discouraging the not useful.As A(o, a) ∈ [−∞, +∞], it is important to understand how it affects the language model.A negative advantage for a ′ in o ′ should discourage the LM from suggesting a ′ in o ′ .UT EA rescales the LM-loss with h(•) ∈ [0, 1), while UT LA works similar to Unlikelihood training as proposed in Welleck et al. (2019) by maintaining the same scale as A(o, a).But, from the restuls we see that the differences in reweighting did not tangible affect the performance as seen in Table 5 (Columns UT EA and UT LA ).</p>
<p>GPT-2 as Policy Network?</p>
<p>So far, we have explored the performance of LMin-the-Loop training of GPT-2 for suggesting candidate actions for the DRRN, but to disambiguate the role of GPT-2 and DRRN, we conduct an ablation experiment.Instead of providing action candidates to DRRN agent, what if GPT-2 chose the argmax action?The experiment addresses two questions:</p>
<p>(1) Is the improvement in the performance and acceleration as in §6.1 largely from the GPT-2 training?and (2) Does the max action of the LM reflect the game dynamics?</p>
<p>Games</p>
<p>Frozen LM In-game LM
Zork1 3.3 [5.7] 3.3 [5.7] Inhumane 0 [0] 0 [0] Detective 23.3 [5.7] 15 [7] Zork3 0 [0] 0 [0] Omniquest 0 [0] 1.6 [2.8] Library 0 [0] 0 [0] Balances 0 [0] 0 [0] Ludicorp 5.3 [2.0] 4.1 [2.9] Dragon 0 [0.0] 0 [0.] Ztuu 0 [0.0] 0 [0.0] Norm 1.1% 1.1%
Table 3: Irrespective of whether the LM was maintained frozen or trained with LM-in-the-Loop, GPT-2 model as policy network yielded zero in the majority of games when DRRN is not used for decision-making.</p>
<p>McClelland et al. ( 2020) motivate the set up of a language model placed in situated learning set up, where it can interact and learn from the environment.However, other than the study conducted in this work, there exists little evidence for interactive learning of an LM from the game transition.Table 3 shows the results of how domain adapted pretrained GPT-2 fairs in the ultimate goal of learning solely from interaction on the text games.We observe that the model's performance is 0 in most games when not using DRRN for decision making.Irrespective of whether the LM was kept frozen or trained with in-game transitions, there was no palpable evidence of language understanding through game semantics observed.</p>
<p>But, the possible explanation for the performance in §6.1 is that the language model learns more game specific actions, though not optimal, leading to DRRN contributing significantly to the performance observed.</p>
<p>Generalization to Other Games</p>
<p>We observed from the previous results that the agent performing well could be attributed to the actions suggested by the LM that that adapted from the transitions in-game.While that is encouraging, it also risks the generality of such an agent in being transferrable to other games.To quantify the loss in generality, we use the LM-in-the-Loop trained GPT-2 from zork1 game and continue to train with 4 different target games-zork3, Ludicorp, inhumane, and Ztuu.</p>
<p>For the settings to compare, we used using state features (OC), and Uncategorized Transitions (UT).To train the LM model, we set the buffer size as 100K in both the settings.The results tabled
0 [0] 1 [2.1]
Table 4: Transferring LM-in-the-Loop trained GPT-2 did not provide guarantee improvements over the baseline.Also, the performance remained unexplainable with A ≈ and (A × O) ≈ .</p>
<p>in Table 4 shows that in-game learning techniques suffered from extending to other games when compared with the baseline performance of CALM.As LM-in-the-Loop training performed well on these games when trained in isolation as seen from Table 5, it would be interesting to observe if that depended on some notion of similarity between the source and the target games.</p>
<p>To that end, we define two measures of similarity using the actions, A ≈ , and the action-observation combinations (O × A) ≈ with the target games.For A ≈ we populate the possible actions available from the Jericho environment on the source game, zork1, and each of the target games considered.We estimate the BLEU-2 (Papineni et al., 2002) score for every action in the source, a ∈ A s , with all actions in the target game, A t , as the reference.The average over the corpus BLEU is tabled in the A ≈ column in Table 4. But, A ≈ did not have any reasonable correlation over the performance observed.While zork3 had an expected action space similarity with the source game and did not have a significant performance drop, the counterfactual scenarios for when A ≈ is much lower in other games, the performance was mixed.Although in inhumane and Ztuu, OC performed significantly lower than CALM, in Ludicorp the performance was better than the baseline rendering the action similarity score, A ≈ , ineffective in explaining the results.Also, the MAUVE score measured between the game transitions, (O × A) ≈ , was too low suggesting a weak semantic overlap between the game spaces.If generality were to be affected with lower similarity between the source and target games measured along action, action×observations, the results were inconsistent in that regard.</p>
<p>Although Yao et al. (2021) observed that the models did not naturally respect the notion of semantics, the results that the learnability in the target games being strongly affected when the LM-in-the-Loop is adapted to zork1 does not entail the LMs being agnostic to notions of semantics.At the same time, the results we observed doesn't either sug-gest that semantics guide the results.Such mixed observations probably only suggest that notions of semantics through automatic evaluations are not the tools to interpret LMs in text games.</p>
<p>Discussion</p>
<p>The comparison of LM-in-the-Loop with baseline and their absolute performances from Table 1 shows that there is more room for improvement.Despite the LMs having strong linguistic priors from pretraining, the large action space when it comes to generative task is one of the significant challenges in adapting LMs to text-based games.Although interactive learning is promising, towards realizing interactive task solving agents, it is imperative to address the issues due to scalability and data-efficiency.The results in the paper through exploring the possibility of adapting language models for action suggestions through utilizing the ingame generated transitions opens up discussions on several key questions:</p>
<p>While there is improvement in performance, and acceleration in comparison to not learning from the game transitions, the absolute improvement with respect to the games has still a long way to go.When DRRN module was plugged out for ablation, the argmax action of LM was not even close to a reasonable performance indicating the heavy lifting in planning was from DRRN.Towards realizing LMs in situated learning environments, adapting LMs to different games is a challenging language understanding milestone.Specifically, it is important to align LM's action generation likelihood to reflect the action value function.</p>
<p>Despite the acceleration and a reduced need for human transitions to adapt LMs for action suggestion, interpreting their performance through the conventional lens of automatic semantic and syntax scores is less effective.It is, then, only imperative to make the application of LMs in text games interpretable through automatic metrics that identifies important transitions to train LM-in-the-Loop.</p>
<p>Limitations</p>
<p>The paper analyzes the possibility and challenges in LM-in-the-Loop training of GPT-2 model for action recommendation in text based games.The claims in the work can be further supported with experiments on different LLM.Similarly, the generalization experiments could have added more support to the lack of evidence with additional games.However, these are compute intensive experiments and the claims are largely made in consideration to the limitations in the set up.</p>
<p>[SEP]  northeast[SEP] . . .you are carrying nothing.you are still on the streets. . . .[SEP] northeast''.[SEP] and [CLS] are special tokens specific to LM-training.Yao et al. (</p>
<p>Figure 2 :
2
Figure 2: Training LM-in-the-Loop post-human-annotated dataset adaptation: RL agent (DRRN) picks the action recommended by the language model (at T ), which is GPT-2.The context pairs are stored in the replay buffers that are categorized by some heuristic.Then the Language model is updated with in-game transitions after k learning steps in the game.Finally, the updated language model (T + k) actions are recommended.</p>
<p>Figure 3 :
3
Figure 3: We see that LM-in-the-Loop techniques only need half of the steps to achieve the best of CALM.Whereas, using state feature based categorization (OC) achieved better acceleration and performance over the rest.</p>
<p>Figure 4 :
4
Figure 4: Comparison of learning dynamics of the different LM-in-the-Loop techniques with the baseline CALM agent across the selected 10 games in Jericho.</p>
<p>[2.7] 288.5 [1.5] 289.3 [0.2] 288.3 [1.3] 285.1 [5.6] 288.5 [1.5]
GamesCALMUTUT LAUT EARTOC∆(%)MaxScoreZork130.7 [4.8]32.6 [4.4]30.4 [8.5]35.6 [5.7]30.7 [3.8]38.0 [1.7]23%350Inhumane24.8 [2.7]21.9 [5.24] 28.9 [11]27.3 [3.1]29.1 [12.7]43.4 [3.8]75%90Detective290.9 0%360Zork30.3 [0.09]0.3 [0.14]0.4 [0.1]0.6 [0.1]0.6 [0.1]0.7 [0.2]133%7Omniquest6.7 [0.3]6.0 [0.6]6.6 [0.9]6.6 [1]6.0 [0.79]7.8 [1.7]16%50Library11.2 [1.3]9.3 [1.1]9.5 [1]10.3 [0.2]10.3 [1.8]12.1 [0.7]8%30Balances9.3 [0.2]9.6 [0.1]9.6 [0.2]9.5 [0.2]9.7 [0.2]9.7 [0.1]4%51Ludicorp10.4 [0.7]11.4 [2.6]12.5 [1.1]11.9 [2.6]11.3 [3.1]15.1 [0.8]45%150Dragon0.1 [0.06]0.1 [0.1]0.3 [0.3]0.3 [0.3]0.1 [0.12]0.3 [0.2]200%25Ztuu3.8 [0.18]4.4 [0.0]4.5 [0.2]4.4 [0.1]4.3 [0.1]4.5 [0.1]18%100Norm Score 20.1%19.1%20.6%20.9%20.7 %24.0%52.37% 100%</p>
<p>Table 1 :
1
From the results, it can be consistently seen that LM-in-the-Loop provides a performance improvement over CALM.Especially, categorizing the transitions with state features (OC) scored the highest with ∼ 53% improvement over the scores obtained by the baseline model.</p>
<p>Table 2 :
2
Using State Features (OC) achieved an average norm score of 21.8% with 10%, which was more than even with CALM using 100% of the adaptation data.
GamesCALMCALMOC100%10%10%Zork130.7 [4.8]29 [3.4]35.1 [2.3]Inhumane24.8 [2.7]15.7 [14.7] 27.5 [6.8]Detective290.9 [2.7] 289.5 [0.2] 289.6 [0.2]Zork30.3 [0.09]0.6 [0]0.7 [0.3]Omniquest 6.7 [0.3]5.9 [0.8]6.0 [1]Library11.2 [1.3]10.5 [1.5]10.2 [1.8]Balances9.3 [0.2]6.6 [3.5]8.6 [1.6]Ludicorp10.4 [0.7]10.2 [0.4]13.7 [0.4]Dragon0.1 [0.06]0.1 [0.06]0.3 [0.2]Ztuu3.8 [0.18]3.6 [0.1]4.1 [0.1]Norm20.1%18.5%21.8 %
 MAUVE-Score (Pillutla et al.,<br />
) measures semantic relatedness of an LM generated text with that of human generated text distribution using an LLM representation of the texts.
The codebase for all experiments will be released after the anonymity period.
AcknowledgementsSarath Chandar is supported by a Canada CI-FAR AI Chair and an NSERC Discovery Grant.The authors acknowledge the computational resources provided by the Digital Research Alliance of Canada and Mila Compute resources.We are thankful to Siva Reddy for their helpful feedback in this work.A AppendixA.1 Language Model SetupWe use a GPT-2 (Base)(Radford et al., 2018b)model with 12-layers, 768-hidden units, and 12attention heads with 117M parameters pre-trained on the WebText corpus.This model's implementation and pretrained weights are obtained from(Wolf et al., 2020, Huggingface).We train for 3 epochs on the ClubFloyd dataset following(Yao et al., 2020)to minimize the crossentropy loss, as shown in Table5.We use AdamW to optimize model's weights to minimize the loss, with the learning rate as 2 × 10 −6 and Adam epsilon as 1 × 10 −9 .We use a linear schedule with a warmup of 0.1 for the learning rate.Finally, we clip gradients with a maximum gradient norm of 1.Following(Yao et al., 2020)'s finetuning process, we exclude using Jericho-related transcripts by setting the flag as 1.We used random seeds to select the dataset to avoid bias in selecting data for the LM training.Model MetricFinalA.2 Reinforcement Learning Agent Setup:We train on 10 interactive fiction games from the Jericho benchmark(Hausknecht et al., 2020).The states are observations concatenated with items in possession of the player and their current location description provided by the game engine using commands inventory and look.A single game episode runs for 100 environment steps at max or gets terminated before the game is over or won.We use the look and inventory commands to add location and inventory descriptions to observations, followingHausknecht et al. (2020).We train DRRN asynchronously on 8 parallel instances of the game environment for 100, 000 steps for each game.At each step, the Q-value is estimated using the DRRN agent, and the action is selected based on the soft-exploration policy.Action's admissibility is predicted based on the textual response of the game.Then, inadmissible are filtered out using a FastText model(Joulin et al., 2017).The agent is optimized using adam optimizer with a learning rate of 10 −5 .We sample transitions of batch size 64 from priority buffer with a priority fraction of 0.5.The discount factor in determining the future reward's importance is 0.9.The size of the embedding dimension is 128, and the hidden dimension is 128.Finally, the gradient is clipped with a maximum gradient norm of 5. We train 5 separate runs for each game and report the average score.We use the average of the last 100 episode scores to calculate the final score.A.3 Software DetailsWe used PyTorch for the code implementation and Huggingface to load pre-trained language models.We used Weights &amp; Biases(Biewald, 2020)for experiment tracking and visualizations to develop insights for this paper.Finally, the seaborn package is used to generate plots.A.4 Mauve Score
Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Alex Hausman, Daniel Herzog, Jasmine Ho, Julian Hsu, Brian Ibarz, Alex Ichter, Eric Irpan, Rosario Jang, Kyle Jauregui Ruano, Sally Jeffrey, Jesmonth, J Nikhil, Ryan Joshi, Dmitry Julian, Yuheng Kalashnikov, Kuang-Huei Kuang, Sergey Lee, Yao Levine, Linda Lu, Luu, 10.48550/ARXIV.2204.01691Peter Pastor. Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng, Carolina Parada2022Do as i can. not as i say: Grounding language in robotic affordances</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew Hausknecht, International Conference on Learning Representations. 2020</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. Emily M Bender, Alexander Koller, 10.18653/v1/2020.acl-main.463Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Experiment tracking with weights and biases. Software available from wandb. Lukas Biewald, 2020</p>
<p>Experience grounds language. Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian, 10.18653/v1/2020.emnlp-main.703Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Decision transformer: Reinforcement learning via sequence modeling. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch, Advances in Neural Information Processing Systems. 2021</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio, NIPS 2014 Workshop on Deep Learning. 2014. December 2014</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, Textworld: A learning environment for text-based games. 2018</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019a. June 2-7, 20191</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/n19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019Minneapolis, MN, USAAssociation for Computational Linguistics2019b. June 2-7, 20191</p>
<p>A survey of data augmentation approaches for nlp. Varun Steven Y Feng, Jason Gangal, Sarath Wei, Soroush Chandar, Teruko Vosoughi, Eduard Mitamura, Hovy, 10.1609/aaai.v34i05.6297arXiv:2105.03075Proceedings of the AAAI Conference on Artificial Intelligence. Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, the AAAI Conference on Artificial Intelligence2021. 202034arXiv preprintInteractive fiction games: A colossal adventure</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Offline reinforcement learning as one big sequence modeling problem. Michael Janner, Qiyang Li, Sergey Levine, Advances in Neural Information Processing Systems. Curran Associates, Inc202134</p>
<p>Bag of tricks for efficient text classification. Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European ChapterShort Papers; Valencia, SpainAssociation for Computational Linguistics20172</p>
<p>UNIFIEDQA: Crossing format boundaries with a single QA system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, 10.18653/v1/2020.findings-emnlp.171Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational Linguistics2020</p>
<p>Word meaning in minds and machines. M Brenden, Gregory L Lake, Murphy, 2021Psychological review</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, Yuke Zhu, arXivPre-trained language models for interactive decisionmaking. 2022</p>
<p>Learning word vectors for sentiment analysis. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, Christopher Potts, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesPortland, Oregon, USA2011Association for Computational Linguistics</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Felix James L Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schütze, Proceedings of the National Academy of Sciences. 117422020</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Advances in Neural Information Processing Systems. Jan Leike, and Ryan Lowe. 2022</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>Stabilizing transformers for reinforcement learning. Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, C ¸aglar Gülc ¸ehre, M Siddhant, Max Jayakumar, Raphael Jaderberg, Aidan Lopez Kaufman, Clark, CoRR, abs/1910.06764Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. 2019</p>
<p>Mauve: Measuring the gap between neural text and human text using divergence frontiers. Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, Zaid Harchaoui, 2021In NeurIPS</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018a</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2018b</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Can wikipedia help offline reinforcement learning?. Machel Reid, Yutaro Yamada, Shixiang Shane Gu, CoRR, abs/2201.121222022</p>
<p>Anna Rogers, Olga Kovaleva, Anna Rumshisky, A primer in bertology: What we know about how bert works. 20208</p>
<p>Pre-trained language models as prior knowledge for playing text-based games. Ishika Singh, Gargi Singh, Ashutosh Modi, CoRR, abs/2107.084082021</p>
<p>Prompts and pre-trained language models for offline reinforcement learning. Denis Tarasov, Vladislav Kurenkov, Sergey Kolesnikov, ICLR 2022 Workshop on Generalizable Policy Learning in Physical World. 2022</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2022</p>
<p>Do promptbased models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.012472021arXiv preprint</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston, arXiv:1908.04319Neural text generation with unlikelihood training. 2019arXiv preprint</p>
<p>Transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnline. Association for Computational Linguistics2020</p>
<p>Insights into pre-training via simpler synthetic tasks. Yuhuai Wu, Felix Li, Percy Liang, arXiv:2206.101392022arXiv preprint</p>
<p>Deep reinforcement learning with transformers for text adventure games. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang, Chengqi Zhang, 10.1109/CoG47356.2020.92316222020 IEEE Conference on Games (CoG). 2020</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>Keep CALM and explore: Language models for action generation in textbased games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Character-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in Neural Information Processing Systems. Curran Associates, Inc201528</p>            </div>
        </div>

    </div>
</body>
</html>
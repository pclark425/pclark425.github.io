<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9265 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9265</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9265</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-276161589</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.04295v3.pdf" target="_blank">Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code is available at https://github.com/HenryLau7/CFPO.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9265.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9265.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFPO_BBC_joint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content-Format Integrated Prompt Optimization — BigBench Classification evaluation (joint content+format)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Joint optimization of prompt content and format (CFPO) yields higher exact-match accuracy on a BigBench classification task compared to content-only, format-only, sequential, and prior content-optimization baselines for LLaMA-3.1-8B and LLaMA-3-8B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BigBench - (classification subset reported as BBC in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A classification task drawn from the Big-Bench benchmark (implicatures / classification-style queries), evaluated via exact-match of model response.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured prompt template (components: Task Detail, Output Format, Few-shot Examples, Query) with CFPO-optimized content and CFPO-selected format. Initialization: single in-context example without additional instruction (except for GrIPS baseline). Few-shot examples and prompt length were allowed to change via optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CFPO_c (content-only, format held constant), CFPO_f (format-only, content held constant), CFPO_c+f (sequential content then format), ProTeGi (content-optimization baseline), GrIPS (phrase-level edits baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA-3.1-8B exact-match: CFPO = 90.00%; LLaMA-3-8B-Instruct exact-match: CFPO = 91.00%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA-3.1-8B: CFPO_c+f = 88.00%, CFPO_c = 85.00%, CFPO_f = 83.00%, ProTeGi = 81.00%. LLaMA-3-8B-Instruct: CFPO_c+f = 89.00%, CFPO_c = 85.00%, CFPO_f = 86.00%, ProTeGi = 82.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.0% absolute (CFPO vs CFPO_c+f) on LLaMA-3.1-8B; +2.0% absolute on LLaMA-3-8B-Instruct (CFPO vs CFPO_c+f).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The paper attributes gains to the interdependence of content and format: selecting optimal formats for each content candidate and jointly optimizing both dimensions produces synergistic improvements; format-only or content-only pipelines miss this interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation metric: exact match. Evaluation subsets sampled from training splits (sizes: 50, 500, 500, 300 across tasks); beam search budget 8 for prompt mutations; max 20 optimization iterations with early stopping; optimizer LLM for mutations: GPT-4 (used for content and format generation); target evaluation models accessed via vLLM; target-model decoding params: top_p=0.1, temperature=0.0, max_tokens=256; initial prompt: single in-context example.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9265.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFPO_GSM8K_joint</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Content-Format Integrated Prompt Optimization — GSM8K evaluation (joint content+format)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CFPO's integrated content-and-format optimization improves chain-of-thought reasoning exact-match accuracy on GSM8K compared to content-only, format-only, sequential, and prior content-optimization baselines, for LLaMA-3.1-8B and LLaMA-3-8B-Instruct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical word-problem reasoning benchmark requiring chain-of-thought; final numerical answer extracted and compared by exact match.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Structured prompt template with chain-of-thought-style examples rendered by a chosen Query Format and Prompt Renderer. CFPO produced optimized content (including in-context examples and step-by-step style) and searched formats in a dynamic pool; initial prompt: single example; reasoning prompts evaluated with CoT extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CFPO_c (content-only), CFPO_f (format-only), CFPO_c+f (sequential), ProTeGi baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA-3.1-8B exact-match: CFPO = 63.38%; LLaMA-3-8B-Instruct exact-match: CFPO = 80.74%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLaMA-3.1-8B: CFPO_c+f = 61.94%, CFPO_c = 58.07%, CFPO_f = 52.46%, ProTeGi = 54.74%. LLaMA-3-8B-Instruct: CFPO_c+f = 79.30%, CFPO_c = 77.71%, CFPO_f = 76.65%, ProTeGi = 75.36%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>LLaMA-3.1-8B: +1.44% absolute (CFPO vs CFPO_c+f) and +8.64% absolute vs ProTeGi. LLaMA-3-8B-Instruct: +1.44% absolute (CFPO vs CFPO_c+f) and +5.38% vs ProTeGi.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Reasoning tasks are especially sensitive to prompt structure; joint optimization allows CFPO to find formats that better elicit stepwise reasoning and final-answer extraction, leading to measurable gains. The paper hypothesizes that task difficulty moderates gains (GSM8K showed larger relative improvements than the harder MATH500).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Exact-match metric on extracted final numerical answer; training-subset sizes and beam-search settings as above; CoT examples and final-answer formatting enforced via Output Format component; optimizer LLM GPT-4 for content and format generation; each experiment capped at 20 iterations; early stopping used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9265.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format_Generation_Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of LLM-assisted Format Generation (with vs without format generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Allowing the LLM to generate new format variants (LLM_f_gen) and adding them to the format pool improves performance relative to using only an initial static format pool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBC (BigBench classification) and GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification and reasoning tasks used to measure the benefit of dynamic format generation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Comparison between (A) initial static format pool (no LLM format generation) and (B) dynamic format pool augmented by formats generated by an LLM (GPT-4) and evaluated via UCT selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>w/o Format Generation (static pool) vs with Format Generation (LLM_f_gen produced k new formats per round).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BBC (LLaMA-3.1-8B): w/o format gen = 88.00% -> with format gen = 90.00%. BBC (LLaMA-3-8B-Instruct): 87.00% -> 91.00%. GSM8K (LLaMA-3.1-8B): 62.70% -> 63.38%. GSM8K (LLaMA-3-8B-Instruct): 78.85% -> 80.74%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See 'performance' above (direct comparison of w/o vs with format generation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>BBC: +2.00% (LLaMA-3.1-8B), +4.00% (LLaMA-3-8B-Instruct). GSM8K: +0.68% (LLaMA-3.1-8B), +1.89% (LLaMA-3-8B-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLM-assisted generation expands the format search space and helps escape local optima; dynamically generated formats that incorporate high-performing traits yield additional gains beyond the initial predefined formats.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Format optimizer evaluated 2k formats per round: k selected by UCT from pool and k newly generated formats via LLM_f_gen; UCT coefficient α set to 1e-3; each evaluated format updated Q(f) and N(f) after testing on evaluation dataset; evaluation metric m(·) = average accuracy for reasoning/classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9265.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format_Selection_UCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Upper Confidence Tree (UCT) format selection vs random and greedy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A UCT-based selection criterion for choosing which formats to evaluate (balancing exploitation and exploration) outperforms random selection and greedy selection (α=0) in the format-search stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B / LLaMA-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BBC (BigBench classification) and GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks used to compare format-selection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Format optimizer uses UCT score UCT(f) = Q(f)/N(f) + α * sqrt( ln(N_total) / N(f) ) (paper provides analogous UCT-style formula), selecting top-k formats for evaluation; compared to (1) random selection from format pool, and (2) greedy selection (α=0).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random selection vs UCT with α=0 (greedy) vs UCT (ours, α>0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BBC (LLaMA-3.1-8B): Random = 85.00%, UCT(α=0) = 86.00%, UCT(ours) = 90.00%. BBC (LLaMA-3-8B-Instruct): Random = 87.00%, UCT(α=0) = 88.00%, UCT(ours) = 91.00%. GSM8K (LLaMA-3.1-8B): Random = 62.40%, UCT(α=0) = 63.23%, UCT(ours) = 63.38%. GSM8K (LLaMA-3-8B-Instruct): Random = 78.82%, UCT(α=0) = 79.08%, UCT(ours) = 80.74%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See 'performance' above (direct comparisons between selection strategies).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Up to +5.0% absolute on BBC (LLaMA-3.1-8B: UCT vs Random = +5.0%); modest gains on GSM8K (≈+1.0% absolute).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Balancing exploration and exploitation via UCT allows the optimizer to both refine promising formats and discover new, higher-performing formats; pure greedy or random selection either under-exploits known good formats or fails to discover improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>UCT selection hyperparameter α used to trade exploration/exploitation; paper sets α = 1e-3 in reported experiments; selection chooses top-k formats per round and also generates k new formats via LLM_f_gen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9265.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InContext_Length_Preference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of in-context example count and prompt length on model performance (pretrained vs instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pre-trained models tend to prefer longer prompts with more in-context examples, while instruction-tuned models are relatively insensitive to prompt length and number of in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pre-trained models (e.g., Mistral-7B-v0.1, LLaMA-3.1-8B) vs instruction-tuned models (LLaMA-3-8B-Instruct, Phi-3-Mini-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / unspecified for Phi-3-Mini-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (GSM8K, MATH500, BigBench classification, ARC-Challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple tasks spanning reasoning and classification used to analyze prompt-length and in-context-example preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Optimized prompts discovered by CFPO across tasks; analysis of number of in-context examples and text length for optimized prompts (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Not a single alternative format — observation is a cross-model trend comparing pre-trained vs instruction-tuned models' optimized prompt lengths and in-context example counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Not reported as a single numeric metric; observation: pre-trained models achieved better performance when CFPO increased prompt text length and number of in-context examples, whereas instruction-tuned models showed less sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Pre-trained models lack task-specific fine-tuning and therefore benefit from explicit context and detailed reasoning steps provided by longer prompts and more examples; instruction-tuned models already encode task-style behavior and thus rely less on extensive prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Figure 5 summarizes optimized prompt lengths and in-context example counts across tasks and models; the paper reports this as a qualitative/quantitative observation (no single effect-size computed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9265.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Convergence_Format_vs_Content</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convergence behavior: format optimization vs content optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Format optimization tended to converge faster than content optimization in CFPO (observed in Mistral-7B-v0.1 on GSM8K), suggesting different resource allocation schedules may improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-v0.1 (reported example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (example convergence analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Mathematical reasoning benchmark used to compare convergence speed of format and content optimization sub-rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Alternating content and format optimization sub-rounds within CFPO; improvements tracked per sub-round (Figure 6 shows per sub-round performance and per-subround gains).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>N/A (analysis concerns sub-round progress within CFPO rather than alternative external formats).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Trend reported: format optimization sub-rounds showed earlier convergence and sometimes larger immediate gains; content optimization continued to produce gains across more iterations. Exact numeric progression illustrated in Figure 6 (not tabulated numerically in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Format generation explores variations of existing renderers (lower diversity) and thus its UCT scores converge more rapidly; content optimization resamples diverse correct/incorrect cases per round, sustaining longer search. Authors hypothesize allocating more resources to format optimization early, then shifting to content, may improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Convergence plot provided for Mistral-7B-v0.1 on GSM8K (Figure 6); per-sub-round gains shown in parentheses. No single numeric convergence rate reported beyond the figure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9265.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format_Gen_LLM_Robustness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robustness to choice of LLM used for format generation (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replacing GPT-4 (used for format generation) with other LLMs (Gemini-2.0, DeepSeekR1, Claude-3.5-Haiku, Llama-3.3-70B-Instruct) produced only minor variation in downstream GSM8K accuracy, indicating CFPO's format-generation step is not critically dependent on a single LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B (target evaluation model for these ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (format-generation ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reasoning benchmark used to test sensitivity to which LLM generates new formats in CFPO.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Format pool augmented by formats generated by alternative LLMs; rest of CFPO pipeline unchanged (GPT-4 used in content-optimization for these ablations per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Formats generated by GPT-4 vs formats generated by alternative LLMs (Gemini-2.0, DeepSeekR1, Claude-3.5-Haiku, Llama-3.3-70B-Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as 'relatively minor impact' — exact numeric table (Table 9) was referenced but not fully included in main text; the paper states downstream GSM8K performance remained stable across generators.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>As long as the format-generation LLM can produce diverse, syntactically valid format descriptions and code, CFPO's downstream effectiveness is preserved; the core CFPO mechanism is robust to the choice of format-generator LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablation replaced the format-generation LLM while keeping GPT-4 for content optimization and LLaMA-3.1-8B as the evaluation target. The paper reports consistent GSM8K accuracy across generators (details in Appendix D.2 / Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9265.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9265.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QueryFormat_Variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query Format variants (Separator/Casing/Renderer examples) and their role</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>CFPO's initial format pool and format-generation schema include micro-variations such as casing (title/upper/lower), separator tokens (e.g., '\n', '--', '||', '<sep>'), rendering of QA pairs, and prompt renderer choices; these micro-format choices are explicitly searched and shown to affect model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used across target models (Mistral-7B-v0.1, LLaMA-3.1-8B, LLaMA-3-8B-Instruct, Phi-3-Mini-Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B / 8B / unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K, MATH500, ARC-Challenge, BigBench classification (examples shown in appendices)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks with diverse answer formats (numeric reasoning, multi-choice, classification) used to demonstrate format rendering effects.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Concrete Query Format examples in Appendix: QA (Q: ... A: ...), Question-Answer, MultiChoice_QA, QA_Titlecase_Separator, QA_Brackets_Colon_Newline, 'Directly Joint' prompt renderer, and others. Variations include where to place 'Let's think step by step', final answer formatting 'The answer is: <ANSWER>', and separators between Q/A and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Multiple Query Format variants compared implicitly during CFPO's format pool search; specific comparisons include different separators, casing styles, and renderer choices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>The paper shows that choosing among these Query Format variants as part of CFPO improves task performance (see GSM8K, MATH500 examples). No single Query Format universally best — model- and content-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Format micro-choices (spacing, separators, casing, explicit final-answer templates) change how models parse and attend to instruction and examples; optimal choices vary by model and content, underlining content-format interdependence.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Appendix B.4 provides example renderers for GSM8K and MATH500. Format generation meta-prompts explicitly control variation axes (CASING and SEPARATOR). The format pool initially contains commonly used format variations (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements. <em>(Rating: 2)</em></li>
                <li>Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. <em>(Rating: 2)</em></li>
                <li>Does prompt formatting have any impact on llm performance? <em>(Rating: 2)</em></li>
                <li>Automatic prompt optimization with "gradient descent" and beam search. <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9265",
    "paper_id": "paper-276161589",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "CFPO_BBC_joint",
            "name_full": "Content-Format Integrated Prompt Optimization — BigBench Classification evaluation (joint content+format)",
            "brief_description": "Joint optimization of prompt content and format (CFPO) yields higher exact-match accuracy on a BigBench classification task compared to content-only, format-only, sequential, and prior content-optimization baselines for LLaMA-3.1-8B and LLaMA-3-8B-Instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "BigBench - (classification subset reported as BBC in paper)",
            "task_description": "A classification task drawn from the Big-Bench benchmark (implicatures / classification-style queries), evaluated via exact-match of model response.",
            "presentation_format": "Structured prompt template (components: Task Detail, Output Format, Few-shot Examples, Query) with CFPO-optimized content and CFPO-selected format. Initialization: single in-context example without additional instruction (except for GrIPS baseline). Few-shot examples and prompt length were allowed to change via optimization.",
            "comparison_format": "CFPO_c (content-only, format held constant), CFPO_f (format-only, content held constant), CFPO_c+f (sequential content then format), ProTeGi (content-optimization baseline), GrIPS (phrase-level edits baseline).",
            "performance": "LLaMA-3.1-8B exact-match: CFPO = 90.00%; LLaMA-3-8B-Instruct exact-match: CFPO = 91.00%",
            "performance_comparison": "LLaMA-3.1-8B: CFPO_c+f = 88.00%, CFPO_c = 85.00%, CFPO_f = 83.00%, ProTeGi = 81.00%. LLaMA-3-8B-Instruct: CFPO_c+f = 89.00%, CFPO_c = 85.00%, CFPO_f = 86.00%, ProTeGi = 82.00%.",
            "format_effect_size": "+2.0% absolute (CFPO vs CFPO_c+f) on LLaMA-3.1-8B; +2.0% absolute on LLaMA-3-8B-Instruct (CFPO vs CFPO_c+f).",
            "explanation_or_hypothesis": "The paper attributes gains to the interdependence of content and format: selecting optimal formats for each content candidate and jointly optimizing both dimensions produces synergistic improvements; format-only or content-only pipelines miss this interaction.",
            "null_or_negative_result": false,
            "experimental_details": "Evaluation metric: exact match. Evaluation subsets sampled from training splits (sizes: 50, 500, 500, 300 across tasks); beam search budget 8 for prompt mutations; max 20 optimization iterations with early stopping; optimizer LLM for mutations: GPT-4 (used for content and format generation); target evaluation models accessed via vLLM; target-model decoding params: top_p=0.1, temperature=0.0, max_tokens=256; initial prompt: single in-context example.",
            "uuid": "e9265.0",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CFPO_GSM8K_joint",
            "name_full": "Content-Format Integrated Prompt Optimization — GSM8K evaluation (joint content+format)",
            "brief_description": "CFPO's integrated content-and-format optimization improves chain-of-thought reasoning exact-match accuracy on GSM8K compared to content-only, format-only, sequential, and prior content-optimization baselines, for LLaMA-3.1-8B and LLaMA-3-8B-Instruct.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "GSM8K",
            "task_description": "Mathematical word-problem reasoning benchmark requiring chain-of-thought; final numerical answer extracted and compared by exact match.",
            "presentation_format": "Structured prompt template with chain-of-thought-style examples rendered by a chosen Query Format and Prompt Renderer. CFPO produced optimized content (including in-context examples and step-by-step style) and searched formats in a dynamic pool; initial prompt: single example; reasoning prompts evaluated with CoT extraction.",
            "comparison_format": "CFPO_c (content-only), CFPO_f (format-only), CFPO_c+f (sequential), ProTeGi baseline.",
            "performance": "LLaMA-3.1-8B exact-match: CFPO = 63.38%; LLaMA-3-8B-Instruct exact-match: CFPO = 80.74%",
            "performance_comparison": "LLaMA-3.1-8B: CFPO_c+f = 61.94%, CFPO_c = 58.07%, CFPO_f = 52.46%, ProTeGi = 54.74%. LLaMA-3-8B-Instruct: CFPO_c+f = 79.30%, CFPO_c = 77.71%, CFPO_f = 76.65%, ProTeGi = 75.36%.",
            "format_effect_size": "LLaMA-3.1-8B: +1.44% absolute (CFPO vs CFPO_c+f) and +8.64% absolute vs ProTeGi. LLaMA-3-8B-Instruct: +1.44% absolute (CFPO vs CFPO_c+f) and +5.38% vs ProTeGi.",
            "explanation_or_hypothesis": "Reasoning tasks are especially sensitive to prompt structure; joint optimization allows CFPO to find formats that better elicit stepwise reasoning and final-answer extraction, leading to measurable gains. The paper hypothesizes that task difficulty moderates gains (GSM8K showed larger relative improvements than the harder MATH500).",
            "null_or_negative_result": false,
            "experimental_details": "Exact-match metric on extracted final numerical answer; training-subset sizes and beam-search settings as above; CoT examples and final-answer formatting enforced via Output Format component; optimizer LLM GPT-4 for content and format generation; each experiment capped at 20 iterations; early stopping used.",
            "uuid": "e9265.1",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Format_Generation_Ablation",
            "name_full": "Impact of LLM-assisted Format Generation (with vs without format generation)",
            "brief_description": "Allowing the LLM to generate new format variants (LLM_f_gen) and adding them to the format pool improves performance relative to using only an initial static format pool.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "BBC (BigBench classification) and GSM8K",
            "task_description": "Classification and reasoning tasks used to measure the benefit of dynamic format generation.",
            "presentation_format": "Comparison between (A) initial static format pool (no LLM format generation) and (B) dynamic format pool augmented by formats generated by an LLM (GPT-4) and evaluated via UCT selection.",
            "comparison_format": "w/o Format Generation (static pool) vs with Format Generation (LLM_f_gen produced k new formats per round).",
            "performance": "BBC (LLaMA-3.1-8B): w/o format gen = 88.00% -&gt; with format gen = 90.00%. BBC (LLaMA-3-8B-Instruct): 87.00% -&gt; 91.00%. GSM8K (LLaMA-3.1-8B): 62.70% -&gt; 63.38%. GSM8K (LLaMA-3-8B-Instruct): 78.85% -&gt; 80.74%.",
            "performance_comparison": "See 'performance' above (direct comparison of w/o vs with format generation).",
            "format_effect_size": "BBC: +2.00% (LLaMA-3.1-8B), +4.00% (LLaMA-3-8B-Instruct). GSM8K: +0.68% (LLaMA-3.1-8B), +1.89% (LLaMA-3-8B-Instruct).",
            "explanation_or_hypothesis": "LLM-assisted generation expands the format search space and helps escape local optima; dynamically generated formats that incorporate high-performing traits yield additional gains beyond the initial predefined formats.",
            "null_or_negative_result": false,
            "experimental_details": "Format optimizer evaluated 2k formats per round: k selected by UCT from pool and k newly generated formats via LLM_f_gen; UCT coefficient α set to 1e-3; each evaluated format updated Q(f) and N(f) after testing on evaluation dataset; evaluation metric m(·) = average accuracy for reasoning/classification tasks.",
            "uuid": "e9265.2",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Format_Selection_UCT",
            "name_full": "Upper Confidence Tree (UCT) format selection vs random and greedy",
            "brief_description": "A UCT-based selection criterion for choosing which formats to evaluate (balancing exploitation and exploration) outperforms random selection and greedy selection (α=0) in the format-search stage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B / LLaMA-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "BBC (BigBench classification) and GSM8K",
            "task_description": "Tasks used to compare format-selection strategies.",
            "presentation_format": "Format optimizer uses UCT score UCT(f) = Q(f)/N(f) + α * sqrt( ln(N_total) / N(f) ) (paper provides analogous UCT-style formula), selecting top-k formats for evaluation; compared to (1) random selection from format pool, and (2) greedy selection (α=0).",
            "comparison_format": "Random selection vs UCT with α=0 (greedy) vs UCT (ours, α&gt;0).",
            "performance": "BBC (LLaMA-3.1-8B): Random = 85.00%, UCT(α=0) = 86.00%, UCT(ours) = 90.00%. BBC (LLaMA-3-8B-Instruct): Random = 87.00%, UCT(α=0) = 88.00%, UCT(ours) = 91.00%. GSM8K (LLaMA-3.1-8B): Random = 62.40%, UCT(α=0) = 63.23%, UCT(ours) = 63.38%. GSM8K (LLaMA-3-8B-Instruct): Random = 78.82%, UCT(α=0) = 79.08%, UCT(ours) = 80.74%.",
            "performance_comparison": "See 'performance' above (direct comparisons between selection strategies).",
            "format_effect_size": "Up to +5.0% absolute on BBC (LLaMA-3.1-8B: UCT vs Random = +5.0%); modest gains on GSM8K (≈+1.0% absolute).",
            "explanation_or_hypothesis": "Balancing exploration and exploitation via UCT allows the optimizer to both refine promising formats and discover new, higher-performing formats; pure greedy or random selection either under-exploits known good formats or fails to discover improvements.",
            "null_or_negative_result": false,
            "experimental_details": "UCT selection hyperparameter α used to trade exploration/exploitation; paper sets α = 1e-3 in reported experiments; selection chooses top-k formats per round and also generates k new formats via LLM_f_gen.",
            "uuid": "e9265.3",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "InContext_Length_Preference",
            "name_full": "Effect of in-context example count and prompt length on model performance (pretrained vs instruction-tuned)",
            "brief_description": "Pre-trained models tend to prefer longer prompts with more in-context examples, while instruction-tuned models are relatively insensitive to prompt length and number of in-context examples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "pre-trained models (e.g., Mistral-7B-v0.1, LLaMA-3.1-8B) vs instruction-tuned models (LLaMA-3-8B-Instruct, Phi-3-Mini-Instruct)",
            "model_size": "7B / 8B / unspecified for Phi-3-Mini-Instruct",
            "task_name": "Various (GSM8K, MATH500, BigBench classification, ARC-Challenge)",
            "task_description": "Multiple tasks spanning reasoning and classification used to analyze prompt-length and in-context-example preferences.",
            "presentation_format": "Optimized prompts discovered by CFPO across tasks; analysis of number of in-context examples and text length for optimized prompts (Figure 5).",
            "comparison_format": "Not a single alternative format — observation is a cross-model trend comparing pre-trained vs instruction-tuned models' optimized prompt lengths and in-context example counts.",
            "performance": "Not reported as a single numeric metric; observation: pre-trained models achieved better performance when CFPO increased prompt text length and number of in-context examples, whereas instruction-tuned models showed less sensitivity.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Pre-trained models lack task-specific fine-tuning and therefore benefit from explicit context and detailed reasoning steps provided by longer prompts and more examples; instruction-tuned models already encode task-style behavior and thus rely less on extensive prompt context.",
            "null_or_negative_result": null,
            "experimental_details": "Figure 5 summarizes optimized prompt lengths and in-context example counts across tasks and models; the paper reports this as a qualitative/quantitative observation (no single effect-size computed).",
            "uuid": "e9265.4",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Convergence_Format_vs_Content",
            "name_full": "Convergence behavior: format optimization vs content optimization",
            "brief_description": "Format optimization tended to converge faster than content optimization in CFPO (observed in Mistral-7B-v0.1 on GSM8K), suggesting different resource allocation schedules may improve efficiency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-v0.1 (reported example)",
            "model_size": "7B",
            "task_name": "GSM8K (example convergence analysis)",
            "task_description": "Mathematical reasoning benchmark used to compare convergence speed of format and content optimization sub-rounds.",
            "presentation_format": "Alternating content and format optimization sub-rounds within CFPO; improvements tracked per sub-round (Figure 6 shows per sub-round performance and per-subround gains).",
            "comparison_format": "N/A (analysis concerns sub-round progress within CFPO rather than alternative external formats).",
            "performance": "Trend reported: format optimization sub-rounds showed earlier convergence and sometimes larger immediate gains; content optimization continued to produce gains across more iterations. Exact numeric progression illustrated in Figure 6 (not tabulated numerically in main text).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Format generation explores variations of existing renderers (lower diversity) and thus its UCT scores converge more rapidly; content optimization resamples diverse correct/incorrect cases per round, sustaining longer search. Authors hypothesize allocating more resources to format optimization early, then shifting to content, may improve efficiency.",
            "null_or_negative_result": null,
            "experimental_details": "Convergence plot provided for Mistral-7B-v0.1 on GSM8K (Figure 6); per-sub-round gains shown in parentheses. No single numeric convergence rate reported beyond the figure.",
            "uuid": "e9265.5",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Format_Gen_LLM_Robustness",
            "name_full": "Robustness to choice of LLM used for format generation (ablation)",
            "brief_description": "Replacing GPT-4 (used for format generation) with other LLMs (Gemini-2.0, DeepSeekR1, Claude-3.5-Haiku, Llama-3.3-70B-Instruct) produced only minor variation in downstream GSM8K accuracy, indicating CFPO's format-generation step is not critically dependent on a single LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B (target evaluation model for these ablations)",
            "model_size": "8B",
            "task_name": "GSM8K (format-generation ablation)",
            "task_description": "Reasoning benchmark used to test sensitivity to which LLM generates new formats in CFPO.",
            "presentation_format": "Format pool augmented by formats generated by alternative LLMs; rest of CFPO pipeline unchanged (GPT-4 used in content-optimization for these ablations per paper).",
            "comparison_format": "Formats generated by GPT-4 vs formats generated by alternative LLMs (Gemini-2.0, DeepSeekR1, Claude-3.5-Haiku, Llama-3.3-70B-Instruct).",
            "performance": "Reported as 'relatively minor impact' — exact numeric table (Table 9) was referenced but not fully included in main text; the paper states downstream GSM8K performance remained stable across generators.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "As long as the format-generation LLM can produce diverse, syntactically valid format descriptions and code, CFPO's downstream effectiveness is preserved; the core CFPO mechanism is robust to the choice of format-generator LLM.",
            "null_or_negative_result": false,
            "experimental_details": "Ablation replaced the format-generation LLM while keeping GPT-4 for content optimization and LLaMA-3.1-8B as the evaluation target. The paper reports consistent GSM8K accuracy across generators (details in Appendix D.2 / Table 9).",
            "uuid": "e9265.6",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "QueryFormat_Variants",
            "name_full": "Query Format variants (Separator/Casing/Renderer examples) and their role",
            "brief_description": "CFPO's initial format pool and format-generation schema include micro-variations such as casing (title/upper/lower), separator tokens (e.g., '\\n', '--', '||', '&lt;sep&gt;'), rendering of QA pairs, and prompt renderer choices; these micro-format choices are explicitly searched and shown to affect model outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used across target models (Mistral-7B-v0.1, LLaMA-3.1-8B, LLaMA-3-8B-Instruct, Phi-3-Mini-Instruct)",
            "model_size": "7B / 8B / unspecified",
            "task_name": "GSM8K, MATH500, ARC-Challenge, BigBench classification (examples shown in appendices)",
            "task_description": "Tasks with diverse answer formats (numeric reasoning, multi-choice, classification) used to demonstrate format rendering effects.",
            "presentation_format": "Concrete Query Format examples in Appendix: QA (Q: ... A: ...), Question-Answer, MultiChoice_QA, QA_Titlecase_Separator, QA_Brackets_Colon_Newline, 'Directly Joint' prompt renderer, and others. Variations include where to place 'Let's think step by step', final answer formatting 'The answer is: &lt;ANSWER&gt;', and separators between Q/A and examples.",
            "comparison_format": "Multiple Query Format variants compared implicitly during CFPO's format pool search; specific comparisons include different separators, casing styles, and renderer choices.",
            "performance": "The paper shows that choosing among these Query Format variants as part of CFPO improves task performance (see GSM8K, MATH500 examples). No single Query Format universally best — model- and content-dependent.",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Format micro-choices (spacing, separators, casing, explicit final-answer templates) change how models parse and attend to instruction and examples; optimal choices vary by model and content, underlining content-format interdependence.",
            "null_or_negative_result": null,
            "experimental_details": "Appendix B.4 provides example renderers for GSM8K and MATH500. Format generation meta-prompts explicitly control variation axes (CASING and SEPARATOR). The format pool initially contains commonly used format variations (Figure 4).",
            "uuid": "e9265.7",
            "source_info": {
                "paper_title": "Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements.",
            "rating": 2,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting.",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Does prompt formatting have any impact on llm performance?",
            "rating": 2,
            "sanitized_title": "does_prompt_formatting_have_any_impact_on_llm_performance"
        },
        {
            "paper_title": "Automatic prompt optimization with \"gradient descent\" and beam search.",
            "rating": 1,
            "sanitized_title": "automatic_prompt_optimization_with_gradient_descent_and_beam_search"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.01782675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization
21 May 2025</p>
<p>Yuanye Liu 
Fudan University Microsoft Research Asia</p>
<p>Jiahang Xu jiahangxu@microsoft.com 
Fudan University Microsoft Research Asia</p>
<p>⋄ Li 
Fudan University Microsoft Research Asia</p>
<p>Lyna Zhang 
Fudan University Microsoft Research Asia</p>
<p>Qi Chen 
Fudan University Microsoft Research Asia</p>
<p>Xuan Feng 
Fudan University Microsoft Research Asia</p>
<p>Yang Chen 
Fudan University Microsoft Research Asia</p>
<p>Zhongxin Guo 
Fudan University Microsoft Research Asia</p>
<p>Yuqing Yang 
Fudan University Microsoft Research Asia</p>
<p>Peng Cheng 
Fudan University Microsoft Research Asia</p>
<p>Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization
21 May 20253081D92FF854F65C23DDDA38B9656B1AarXiv:2502.04295v3[cs.CL]
Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design.While recent research has focused on optimizing prompt content, the role of prompt formatting-a critical but often overlooked dimension-has received limited systematic investigation.In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process.CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options.Our extensive evaluations across multiple tasks and opensource LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods.This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance.Code is available at https://github.com/HenryLau7/CFPO.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated impressive achievements across various domains (OpenAI, 2024a).The effectiveness of LLMs in real-world applications is fundamentally dependent on the design of effective prompts, which serve as an essential interface between human users or developers and the LLM system.Studies have shown that expert-designed prompts could significantly enhance LLM performance (Brown et al., 2020;Wei et al., 2023;Schulhoff et al., 2024).</p>
<p>Despite its significance, manual prompt design is fraught with challenges.LLMs are notoriously Content-format interdependence: The optimal format for a prompt depends on its content, highlighting the need for joint optimization.</p>
<p>sensitive to subtle variations in prompt phrasing and structure, with performance often differing markedly across models and tasks based on these nuances (Zhuo et al., 2024a;Chatterjee et al., 2024;Jiang et al., 2022;Salinas and Morstatter, 2024;Zhuo et al., 2024b;Sclar et al., 2024).To alleviate these difficulties, automated prompt optimization techniques, often leveraging enhanced LLMs to optimize prompts, have proven to be effective to adapt and refine prompts (Pryzant et al., 2023;Schnabel and Neville, 2024;Yang et al., 2024).</p>
<p>However, existing research primarily focuses on optimizing prompt content, while overlooking a critical and unexplored dimension: the prompt formatting.Prompt formatting refers to the arrangement and presentation of prompt content while preserving its semantic meaning.As LLMs are applied to increasingly complex tasks, structuring prompts into distinct components (e.g., instructions, examples, queries) becomes paramount for effectively conveying the desired task and context.Thus, the manner in which a prompt is formatted can significantly impact performance.</p>
<p>Our preliminary investigations (Figure 1) high-light the significant impact of prompt format on LLM performance.We observed that different LLMs exhibit distinct format preferences, with formats performing well on one model sometimes failing on another.This underscores the existence of sophisticated, model-specific format biases (Sclar et al., 2024).Furthermore, even within a single LLM, the optimal format varies depending on the specific prompt content.This complex interplay between content and format suggests that a onesize-fits-all approach to prompt formatting is unlikely to succeed, highlighting the need for joint optimization strategies that consider content and format as interdependent variables.</p>
<p>To address these limitations, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that concurrently optimizes both prompt content and format through an iterative refinement process.CFPO employs distinct optimization strategies tailored to the unique search spaces of content and format.Content optimization is guided by performance feedback and Monte Carlo sampling, leveraging natural language mutations to enhance prompt effectiveness.For format optimization, CFPO explores a discrete set of format options through a dynamic exploration strategy designed to identify optimal formats without requiring prior knowledge.</p>
<p>Specifically, CFPO's format optimizer draws upon principles of structured thinking and defines a hierarchical template that clearly demarcates content elements from their formatting attributes.This allows for targeted optimization of both intracomponent styling (e.g., how an example is presented) (Voronov et al., 2024a;Salinas and Morstatter, 2024) and inter-component arrangement (e.g., the order and connectors between components) (He et al., 2024), adapting to the specific needs of different prompt components and their interactions.</p>
<p>Our primary contributions are threefold: • We propose CFPO, an innovative approach to simultaneously optimizes prompt content and format using an iterative process.</p>
<p>• We introduce an efficient strategy for dynamic format optimization that iteratively generates and evaluates format candidates using a scoring system to select superior options.</p>
<p>• We conduct extensive evaluations across diverse tasks and open-source LLMs, showing that CFPO consistently enhances model performance in a measurable and effective way.</p>
<p>2 Related Work</p>
<p>Optimization via LLM</p>
<p>The remarkable capacity of LLMs has been demonstrated in various tasks as optimizers, leveraging their ability to enhance performance, such as code generation (Haluptzok et al., 2023;Zelikman et al., 2024;Askari et al., 2024), tool-making (Cai et al., 2024), and agent system design (Hu et al., 2024).However, recent studies indicate that LLMs face significant challenges in achieving completely automatic optimization.These models often rely on human intervention for designing workflows and struggle with tasks requiring complex decomposition and iterative refinement (Zhang et al., 2024;Li et al., 2024).</p>
<p>Prompt Optimization Techniques</p>
<p>Automatic Prompt Optimization Automatic prompt optimization plays a crucial role in enhancing the performance of LLMs by refining prompts without requiring human intervention.Various approaches have been explored to search for the optimal prompt, including reinforcement learning (Zhang et al., 2023), Monte Carlo Search (Zhou et al., 2023), Monte Carlo Tree Search (MCTS) (Wang et al., 2024b), and agentdriven frameworks (Wang et al., 2024a;Khattab et al., 2024;WHO, 2023).Notably, feedbackbased methods have emerged as a significant approach (Pryzant et al., 2023;Das et al., 2024).These methods iteratively evaluate prompts on a batch of evaluation data, using error cases to guide subsequent mutation and improvement (Pryzant et al., 2023;Das et al., 2024).In each iteration, the prompt optimizer evaluates the prompt on the evaluation set and feeds the incorrectly predicted test cases back to the optimizer to guide improvement in the next mutation step.However, existing automatic prompt optimization techniques often lack the capacity for fine-grained modifications.</p>
<p>While (Khattab et al., 2024;Schnabel and Neville, 2024) introduce phrase-level mutations, a systematic approach to optimizing prompt format is lacking.This leaves a gap to comprehensively adapt prompt structure for optimal performance.Prefix Tunning offers an effective strategy for adapting large language models (LLMs) to specific tasks by learning continuous, task-specific vectors that are prepended to the input sequence (Li and Liang, 2021;Wang et al., 2023;Guo et al., 2024;Gu et al., 2022;Wang et al., 2023).While demonstrating impressive performance gains, a key limitation of Prefix Tuning lies in its reliance on access to the LLM's internal parameters for training.This requirement presents a significant obstacle when working with black-box LLMs accessed through APIs or in resource-constrained environments where full fine-tuning is infeasible.</p>
<p>Prompt Structure and Sensitivity</p>
<p>Structured Prompting, which organizes prompts into components like instructions and examples, is a widely recommended practice (OpenAI, 2024b;Google, 2024).Frameworks like LangGPT (Wang et al., 2024a) further advance this paradigm by introducing reusable prompt designs inspired by software engineering principles, showcasing the potential of structured approaches (Fernando et al., 2023).While these efforts have primarily focused on optimizing the content and organization of prompt components, less attention has been paid to the impact of prompt formatting within this structured context.Sensitivity of Prompt Variations LLMs exhibit significant sensitivity to prompt variations.Studies have shown that even semantically similar but unseen instructions can lead to performance degradation (Sun et al., 2023;Mizrahi et al., 2024).To address this brittleness, researchers have proposed methods and metrics to systematically evaluate and understand prompt sensitivity (Zhuo et al., 2024b;Chatterjee et al., 2024).These findings underscore the necessity of prompt optimization for robust LLM performance.Format Sensitivity of Prompt Studies have highlighted the impact of formatting on prompt performance (Salinas and Morstatter, 2024).Sclar et al. (2024) revealed that modifications to separators and spacing within a query could substantially im-pact performance.</p>
<p>CFPO: Content-Format Integrated Prompt Optimization</p>
<p>The effectiveness of LLMs is profoundly influenced by both the content and format of prompts.</p>
<p>To address this, we propose Content-Format Integrated Prompt Optimization (CFPO), a novel framework designed to jointly optimize prompt content and structure for enhanced LLM performance.CFPO, illustrated in Figure 2, employs a dual-optimizer approach in each iteration.The subsequent sections provide a detailed explanation of our methodology, beginning with the structured prompt template we utilize (Section 3.1), followed by a description of the Component-wise Content Optimization (Section 3.2), the Format Optimization (Section 3.3), and finally, the integration of these optimizers into the complete CFPO framework (Section 3.4).Implementation details and meta-prompts are provided in Appendix A.</p>
<p>Structured Prompt Template</p>
<p>To facilitate fine-grained and targeted optimization, our framework adopts a structured prompt template inspired by prompt engineering guidelines from OpenAI (2024b) and Google (2024).Our template decomposes prompts into distinct functional com- • Task Detail offers supplementary task-specific information, including resolution steps.</p>
<p>• Output Format specifies the desired output structure (e.g., JSON, bullet points, etc.).</p>
<p>• Few-shot Examples provide specific instances for contextual learning patterns.</p>
<p>• Query shows the question or request to be answered by the LLM.The Format-based Components dictate how the prompt is assembled and presented, including: • Prompt Renderer defines how to aggregate all components into a structured prompt.</p>
<p>• Query Format: defines how to structure the rendering of examples and queries.Note that this template is highly adaptable.Users can readily adjust the template by adding or deleting components, or incorporating additional formatting elements (e.g., tables, structured documents) within existing components to suit their specific requirements.By decoupling format from content, this structured template empowers users to perform targeted and precise optimizations, leading to improved prompt effectiveness.</p>
<p>Component-wise Content Optimization</p>
<p>Building upon existing prompt optimization strategies, which often rely on either feedback-driven refinement or Monte-Carlo sampling (Section 2.2), CFPO introduces a more targeted and efficient approach to content optimization.While feedbackdriven methods diagnose weaknesses through failure analysis, and Monte-Carlo sampling diversifies perspectives with semantic variations, CFPO innovates along two key dimensions.</p>
<p>First, CFPO expands the diagnostic phase beyond traditional failure analysis by incorporating correct cases.This allows the LLM to identify and reinforce successful aspects of the prompt, complementing error correction.Second, CFPO adopts a component-wise optimization strategy.Instead of treating the prompt as a monolithic block, CFPO targets specific content-based components for individual refinement, enabling more precise and effective optimization.</p>
<p>Format Optimization Design</p>
<p>To efficiently navigate the prompt format space, CFPO incorporates a format optimizer based on a dynamic format pool and an LLM-assisted generation module.The optimizer iteratively explores, evaluates, and refines formatting choices.</p>
<p>Format Pool with Scoring System</p>
<p>The format pool maintains a diverse collection of prompt format configurations, organized hierarchically to represent variations at both macro (e.g., overall prompt structure) and micro (e.g., component-internal rendering) levels.In this work, we prototype this structure with two distinct renderers: a Prompt Renderer defining the global structure, and a Query Format governing the rendering of individual components, such as examples and queries, as shown in Figure 4.</p>
<p>Each format f in the pool is associated with a performance score, Q(f ), which cumulatively reflects its effectiveness across different prompt contents.When a format f is evaluated on a new content c, its score is updated as follows: Q(f ) ← Q(f ) + m(c, f ), where m(c, f ) is a task-specific metric function.For example, in reasoning tasks, m(•) represents the average accuracy (0 or 1) on an evaluation dataset.The number of evaluations for each format, N (f ), is also tracked to enable score normalization and fair comparison.</p>
<p>To initiate the format exploration process, we define an initial format search space, F, consisting of a set of predefined and commonly used formats (Figure 4), along with diverse variations that introduce subtle changes (e.g., spacing, punctuation, special symbols).This initial format pool serves as the foundation for subsequent exploration.</p>
<p>LLM-assisted Format Generation</p>
<p>To overcome the limitations of a static format pool and promote continuous adaptation, we introduce an LLM-based format generator, LLM f _gen .This module autonomously generates novel formats by leveraging information about the existing formats in the pool and their performance characteristics.By dynamically creating new formats, we aim to es-cape local optima and discover potentially superior formatting strategies.</p>
<p>In each optimization round, the format generator is provided with the existing formats and their normalized performance scores, Q(f ) N (f ) , and tasked with creating new formats or variations that might improve performance.This iterative process not only diversifies the format pool but also ensures that our system can adapt to and incorporate a wide range of formats, thereby enhancing its utility and effectiveness.Additional details and generated examples about the format generation process are available in Appendix A.3 and Appendix D.1.</p>
<p>Search Format via Format Optimizer</p>
<p>For each content candidate generated by the content optimizer, the format optimizer aims to identify the most appropriate format from format pool.To navigate the balance between exploring new formats and exploiting known effective ones, we implemented the Upper Confidence Bounds applied to Trees (UCT) algorithm (Kocsis and Szepesvári, 2006).The UCT algorithm employs a selection criterion given by:
U CT (f ) = Q(f ) N (f ) + α f N (f ) N (f )(1)
where α serves as a balancing hyper-parameter, adjusting the trade-off between exploration and exploitation.</p>
<p>The overall process, outlined in Algorithm 1, selects 2k formats for evaluation in each optimiza-Algorithm 1 Searching Optimal Format Given a Prompt Candidate Input: p 0 = (c 0 , f 0 ): initial prompt, p = (c, •):</p>
<p>current prompt candidate(with content c), F: dynamic format pool, k: number of formats, m(•): evaluation metric, D: evaluation data.1: Initialize:
Q(f ) ← m(c 0 , f ), N (f ) ← 1 for all f ∈ F 2: Format Selection: F select ← {f ∈ F : f is in the top k w.r.t. U CT (f )} 3: Format Generation: 4: for each i = 0, 1, ..., k do 5: Generate format: f new ← LLM f _gen (F) 6:
Collect f new to F gen , and add f new to F 7: end for 8: Format Evaluation:
9: for each f ∈ F select ∪ F gen do 10: Evaluate m(c, f ) with dataset D 11: Q(f ) ← Q(f ) + m(c, f ) 12: N (f ) ← N (f ) + 1 13: Update U CT (f ) by Eq. 1 14: end for 15: f ← arg max f ∈F select ∪Fgen m(c, f ) Output:
The optimal format f for content c tion round: k existing formats from the pool (based on UCT score), and k new formats generated by the LLM f _gen .The selected formats from both the existing pool (F select ) and the newly generated pool (F gen ) are then evaluated using a predefined metric function m(•), and the best-performing format among the tested candidates will be identified.The result is then incorporated into the pool for future iterations.By iteratively evaluating formats, the format optimizer ensures a balance between exploring new formats and refining current ones, converging to the best format configuration.</p>
<p>Integrated Optimizer Design</p>
<p>CFPO integrates content and format optimization within an iterative loop, as illustrated in Figure 2. In each iteration, the content optimizer proposes candidate prompts, and the format optimizer identifies the most effective format for each candidate.This ensures that each prompt benefits from the most effective formatting.Furthermore, effective formats can improve the diversity and performance of content candidates, thereby helping content optimizer for beam search in the next iteration.</p>
<p>In summary, the content and format optimizers work in tandem, leveraging the LLM to enable rapid adaptation and customization.This iterative and collaborative process optimizes both the content and the format of prompts, leading to significant improvements in prompt quality.</p>
<p>Experiments</p>
<p>4.1 Experimental Setups Models.</p>
<p>We selected four open-source Large Language Models (LLMs) as our primary evaluation targets, including two foundational models, Mistral-7B-v0.1 (Jiang et al., 2023) and LLaMA-3.1-8B(Meta, 2024b), as well as two instruction-tuned models, LLaMA-3-8B-Instruct (Meta, 2024a) and Phi-3-Mini-Instruct (Microsoft, 2024).For content mutation and format generation during the optimization process, we employed GPT-4 (2024-05-01-preview) (OpenAI, 2024a).Datasets.Our evaluation benchmark was designed to comprehensively assess model performance across a range of task types and difficulty levels, emphasizing diverse query formats.Specifically, we employed the following tasks:</p>
<p>• Classification: The Implicatures task from the Big-Bench benchmark (bench authors, 2023).</p>
<p>• Multiple-choice: ARC-Challenge (Clark et al., 2018) task.</p>
<p>• Reasoning: GSM8K (Cobbe et al., 2021) and MATH500 (Hendrycks et al., 2021;Lightman et al., 2023) requiring complex reasoning abilities.Baselines.We compared CFPO with several commonly used and popular baselines.GrIPS (Prasad et al., 2023) performs syntactic phrase-level edits in instruction, representing a non-LLM-based optimization approach.APE (Zhou et al., 2023) and ProTeGi (Pryzant et al., 2023) both employ LLM to optimize prompt content, but differ in mutation strategy.APE adopts an instruction induction approach, while ProTeGi leverages test cases feedback with LLM to guide the mutation process.SAMMO (Schnabel and Neville, 2024) introduces a structured framework that incorporates a preliminary format mutation strategy, which relies on random selection from a predefined format pool.All methods were evaluated using consistent experimental configurations to ensure a fair comparison.We also report common baseline prompts for reasoning tasks, including 8-shot for GSM8K and 4shot for MATH500.Implementation Details.To evaluate the generated prompts, we sample subsets from the training split for each benchmark (sizes: 50, 500, 500, and 300 examples respectively).Beam search (budget of 8) is used during prompt mutations.Each experiment was capped at 20 iterations.Early stopping was implemented, halting the process if performance plateaus.The number of prompt components the LLM could modify decreased linearly from 4 to 1 over the iterations (see Appendix B.1).</p>
<p>Method
Mistral-7B-v0.1 LLaMA-3.1-8B LLaMA-3-8B-Instruct Phi-3-Mini-Instruct Big-
We start with a single in-context example without any further instruction as the initial prompt for each model and task, except for GrIPS which requires an initial instruction.Detailed parameter settings and procedure are in the Appendix B.</p>
<p>Main Results</p>
<p>Table 1 summarizes the performance of CFPO compared with several state-of-the-art methods across four datasets.The results highlight the superior performance of CFPO, significantly outperforming the baseline prompt and competing methods.</p>
<p>The effectiveness of CFPO is particularly evident when compared to methods like GRIPS, which relies solely on phrase-level mutations, yield-ing only marginal improvements.This highlights the importance of iterative feedback for effective prompt refinement, a feature shared by ProTeGi, SAMMO, and CFPO.Furthermore, CFPO incorporates integrated format optimization that contributes significantly to the performance gains, particularly for previously challenging tasks.</p>
<p>The benefits of CFPO are particularly pronounced in reasoning tasks, such as GSM8K and MATH, known for their sensitivity to prompt structure.While improvements are observed on both datasets, the impact is more noticeable on GSM8K compared to the more complex MATH dataset, suggesting that task difficulty can moderate the attainable performance gains from prompt optimization.</p>
<p>Beyond performance metrics, we further evaluate the stability of CFPO and the computational cost, as detailed in Appendix C. Examples of optimal prompts are detailed in Appendix E.</p>
<p>Ablation Study</p>
<p>Impact of the CFPO Pipeline Components.To understand the contribution of each component in CFPO, we compared the full CFPO approach against the following ablated variants: (1) CFPO f  for generation.As shown in Table 3, the version with format generation consistently outperforms the variant using only the initial pool.This highlights the benefit of our format exploration mechanism in expanding the prompt space.(See Appendix D.2 for the ablation study with different format generation models.)Analysis of Format Selection Strategies.We further examined the effectiveness of our UCT-based format selection strategy.We compared it against two baselines: random selection from the format pool and a greedy selection approach (equivalent to setting α = 0 in Eq. ( 1), disabling exploration).Table 3 shows that CFPO with UCT-based selection consistently achieves the best performance across all settings, demonstrating the efficacy of balancing exploration and exploitation in format searching.</p>
<p>Exploring Different Optimizer Models.While GPT-4 was used as the primary optimizer model for CFPO, we also investigated the feasibility of using a more accessible model , Qwen2.5-14B-Instruct.Table 4 presents the performance of CFPO with Qwen2.5-14B-Instruct as the optimizer.The results are encouraging, demonstrating that CFPO can be effectively adapted to smaller-sized open-sourced models, albeit with some performance trade-offs, increasing its potential for broader applicability.</p>
<p>Conclusion</p>
<p>This paper introduces Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that concurrently optimizes both prompt content and format.By leveraging a structured prompt template, CFPO decouples these elements, enabling integrated optimization and addressing a significant gap in current research that largely overlooks the critical influence of format.</p>
<p>Our results demonstrate the substantial significant influence of format on LLM performance, underscoring the necessity of a joint optimization approach.These findings emphasize the importance of integrating content and format considerations in prompt engineering.CFPO represents a significant advancement, empowering developers to design effective and robust prompts and unlocking the full potential of LLMs across diverse applications.</p>
<p>Limitations While the proposed method demonstrates promising results, there are several limitations worth noting.First, the effectiveness of the approach is task-and model-dependent.While the method generates promising prompts for specific tasks and models, it may not generalize as effectively to others-particularly tasks that are less sensitive to prompt structure or models that already possess strong reasoning capabilities, thereby limiting its broader applicability.Moreover, the iterative nature of the optimization process, with multiple mutation strategies, introduces computational complexity, which could hinder scalability in resource-constrained environments.Finally, we acknowledge the inherent difficulty in definitively separating content and format, especially in realworld scenarios where structured data can be embedded within content.Future work will focus on addressing these limitations, including exploring the application of CFPO to complex agentic tasks and improving the stability and scalability of the optimization process.</p>
<p>A Appendix: Detailed Optimization Process and Meta-Prompts</p>
<p>A.1 Meta-Prompt Header Setup</p>
<p>At the beginning of the prompt, we introduce the task and provide a detailed explanation of the prompt's components, followed by the current version of the prompt.Below is the structure of the meta-prompt header, where placeholders are denoted in [ALL CAPS]:
I'm trying to write a prompt to [TASK INTENTION].
The current prompt consists of several key components, including:
[DESCRIPTION OF COMPONENTS]
The complete prompt is as follows:</p>
<p>"""[CURRENT PROMPT]"""</p>
<p>A.2 Content Optimization</p>
<p>A.2.1 Case-diagnosis and Revision</p>
<p>As described in Section 3.4, content optimization is achieved through an iterative process of casediagnosis and feedback guided mutation.To facilitate this process, we utilize three distinct metaprompts, each tailored to a specific task within content optimization.</p>
<p>Case Diagnosis Meta-Prompt.This meta-prompt analyzes the current prompt's performance against a set of test cases.It identifies areas for improvement and suggests specific modifications for the next iteration.</p>
<p>[</p>
<p>A.3 Format Generation</p>
<p>Our format generation process is a two-step procedure designed to create diverse and effective prompt formats.We focus on generating two key components of a prompt's format: the Prompt Renderer and the Query Format.The appendix presents examples of the format generated using this pipeline.</p>
<p>Step 1: Format Description Generation.For each component (i.e., Prompt Renderer and the Query Format), we first generate a natural language description of the format, alongside an example of how this format would render a sample input.This description acts as a blueprint, guiding the subsequent code generation.We utilize a meta-prompt to instruct an LLM to perform this task.The metaprompt takes existing format examples as context and generates new format descriptions along with rendered results.As an illustrative example, here is a conceptual outline of the meta-prompt employed for generating new Query Format descriptions: The variation can focus on two parts, CASING and SEPARATOR:</p>
<p>CASING refers to both the capitalization of the text (e.g., f(x) = x.title(),f(x) = x.upper(),f(x) = x.lower()) and the specific wording or phrasing used (e.g., changing "question" to " instruction" or "input").</p>
<p>SEPARATOR: the punctuation or symbols used to separate the question and answer, there are some candidates as for your reference {{'', ' ', '\ n', '--', ';\n', ' ||', '<sep>', ' \n', ':', '.'}}.</p>
<p>Note that focus solely on the format itself without altering the content of the question and answer.The format should remain focused on the existing structure (e.g., Question/Answer or Instruction/Response) without modifying the content or introducing any new sections.Avoid the use of underlines or any unconventional formatting styles among words.The format name should only include alphanumeric characters and underscores.Special characters such as <code>|</code>, <code>!</code>, <code>#</code>, <code>@</code>, and spaces should be avoided.</p>
<p>Please encapsulate the new query format using the following format:</p>
<p><START> <Format name: [format name]> <Description: [format description]> [The example rendered by the newly generated format] <END></p>
<p>Step 2: Format Code Generation.Based on the natural language description and rendered example produced in Step 1, we subsequently generate the corresponding code implementation of the new format.This code will be used by the system to render prompts according to the defined format.We again leverage a meta-prompt to instruct the LLM, this time to generate the executable code.As an illustrative example, here is a conceptual outline of the meta-prompt employed for generating the code representation of a new Query Format:
[META PROMPT HEADER]
We For format optimization, 4 UCT-selected formats and 4 newly generated formats are used to generate new prompts.The coefficient in the UCT selection process α is set to 1e − 3.</p>
<p>Each experiment was capped at a maximum of 20 optimization iterations.An early stopping criterion was implemented, halting the process if the performance did not improve for a specified number of consecutive iterations.</p>
<p>B.2 Model Generation Parameters</p>
<p>Key generation parameters for both the LLM optimizer (GPT-4) and the target evaluation models are detailed below.</p>
<p>• LLM Optimizer (GPT-4): We utilized the following settings to generate and refine prompts: top_p=1.0,max_tokens=4096, seed=42, and temperature=1.0.</p>
<p>• Evaluation Models:</p>
<p>The target evaluation model, evaluated on the generated prompts, was configured with: top_p=0.1,max_tokens=256, seed=42, temperature=0.0,repetition_penalty= 1.0, and stop='\n'.The stop parameter ensured the model ceased generation upon encountering a newline character.All generation processes were implemented using the vLLM library (Kwon et al., 2023).</p>
<p>B.3 Evaluation Metric</p>
<p>Performance across all tasks was evaluated using the exact match metric.For direct answer tasks (BigBench-Classification and ARC-Challenge), the model's direct prediction was assessed for an exact string match with the ground truth.For Chain-of-Thought reasoning tasks (GSM8K and MATH500), the final numerical answer was extracted from the generated explanation and compared to the correct solution for an exact match.Finally, the bestperforming prompt identified on the evaluation set for each optimization method is reported on the corresponding test set.</p>
<p>B.4.3 GSM8K</p>
<p>Prompt Renderer: Directly Joint Query Format: QA Q: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?</p>
<p>A: There are 15 trees originally.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 6.The answer is 6. {{Query placeholder}}</p>
<p>B.4.4 MATH500</p>
<p>Prompt Renderer: Directly Joint Query Format: Question-Answer</p>
<p>A chat between a curious user and an AI assistant.The assistant gives step-by-step solutions to the user's questions.In the end of assistant's response, a final answer is given in the format of "The answer is: <ANSWER>.".</p>
<p>Here are some examples:
Question: Let [f(x) = \left{ \begin{array}{cl} ax+3, &amp;\text{ if }x&gt;2, \ x-5 &amp;\text{ if } -2 \le x \le 2, \ 2x-b &amp;\text{ if } x &lt;-2. \end{array} \right.
]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper ).</p>
<p>Answer: Let's think step by step.For the piecewise function to be continuous, the cases must "meet" at $2$ and $-2$.For example, $ax+3$ and $x-5$ must be equal when $x=2$.This implies $a(2)+3=2-5$, which we solve to get $2a =-6 \Rightarrow a=-3$.Similarly, $x-5$ and $2xb$ must be equal when $x=-2$.Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$.The answer is: $a+b=-3+3=\boxed{0}$.To further evaluate the Content-Format Integrated Prompt Optimization (CFPO) approach, we assessed its performance on the MMLU benchmark.Specifically, we focused on high school history categories (including high_school_european_history, high_school_us_history and high_school_world _history) to evaluate CFPO's effectiveness on knowledge-based tasks.Table 5 summarizes the MMLU performance, showcasing improvements achieved with CFPO, which reinforce the generalizability of CFPO, extending its demonstrated benefits from reasoning tasks (as presented in the main paper) to knowledgebased tasks.This expanded validation will be integrated into the revised manuscript.</p>
<p>C.2 Impact of In-context Examples and Prompt Length</p>
<p>Figure 5 presents an overview of the number of incontext examples and the text length of optimized prompts across various tasks and models.An interesting pattern emerges: pre-trained models consistently prefer prompts with longer text and more in-context examples compared to instruction-tuned models.This observation suggests that pre-trained models benefit more from explicit context and detailed reasoning steps, which align with their less task-specialized nature.In contrast, the relative insensitivity of instruction-tuned models to prompt length and in-context examples supports the notion that these models have already trained with taskspecific knowledge during fine-tuning, reducing their dependence on highly detailed prompts.</p>
<p>C.3 Stability and Convergence Analysis</p>
<p>A crucial aspect of our framework is its stability.</p>
<p>To quantify this, we performed multiple independent runs of CFPO and analyzed the variance in performance.Table 6 presents a comparison of the originally reported CFPO scores with the mean and standard deviation of scores obtained from four independent runs.The consistently low standard deviations across all models demonstrate the robustness and stability of our CFPO framework.Furthermore, we examined the convergence behavior of content and format optimization.Preliminary analysis using Mistral-7B-v0.1 on GSM8K (Figure 6) indicates that format optimization may converge faster than content optimization.The performance gains in each sub-round (content or format optimization) are shown in parentheses.This suggests that the optimization schedule could be refined.</p>
<p>This difference in convergence speed could stem from the distinct optimization strategies: during content optimization, a diverse set of correct and incorrect examples is resampled from the training set in each round.In contrast, format optimization relies on generating variations of existing formats, leading to reduced diversity as the UCT score converges.We hypothesize that allocating more resources to format optimization in early rounds, and subsequently shifting focus to content optimization, could improve overall efficiency.This is an area for further investigation in future work.</p>
<p>C.4 Cost Analysis</p>
<p>An important consideration is the computational cost associated with CFPO.Table 7 breaks down the average token usage, API calls, and estimated cost per round of optimization for each operation within CFPO.These costs are calculated using current API pricing for the utilized LLMs.Furthermore, CFPO incorporates an early stopping mechanism that terminates the optimization process when performance improvement plateaus.This prevents unnecessary iterations and reduces overall cost.Table 8 details the average cost and time required to reach the reported performance on each benchmark task.As demonstrated in Table 8, the average computational cost per task remains manageable.</p>
<p>D Appendix: Format Generation</p>
<p>Examples and Ablation Study</p>
<p>D.1 Examples of Generated Format</p>
<p>Here we select several format generated by GPT-4 in CFPO process.Table 9: GSM8K Accuracy with Different LLMs for Format Generation (Using GPT-4 as content optimizer and LLaMA-3.1-8B as the target model).</p>
<p>D.1.1 Query Format Highlight_Separator_Case</p>
<p>The function should take two numbers as input and return their sum.</p>
<p>-</p>
<p>D.2 Ablation Study of Format Generation Models</p>
<p>To investigate the potential reliance on a specific LLM for format generation within our framework, we conducted an ablation study.This study evaluated the performance of several alternative LLMs in generating the formats used by CFPO, as shown in Table 9.We systematically replaced the original format generation LLM (GPT-4) with alternative models: Gemini-2.0,DeepSeekR1, Claude-3.5-Haiku,and Llama-3.3-70B-Instruct.In these experiments, GPT-4 was consistently used for content optimization, and LLaMA-3.1-8Bremained the target model for evaluating performance on the GSM8K benchmark.</p>
<p>The results demonstrate that the choice of LLM for format generation has a relatively minor impact on overall accuracy.While variations in the specific generated formats are observed across different LLMs, the consistent performance suggests that CFPO's effectiveness is not critically dependent on a single LLM.Specifically, as long as the LLM demonstrates the capability to generate variations and code based on the provided schema, the downstream performance on GSM8K remains stable.This robustness suggests that the core principles of CFPO are transferable and not tied to the idiosyncrasies of a particular LLM.This suggests CFPO is not overly sensitive to the choice of format generation model.</p>
<p>E Appendix: Examples of CFPO Optimal Prompt</p>
<p>Here we selected several optimal prompts searched by CFPO.The club starts with 120 members.In the first year, it increases by 10%, which is 0.10 * 120 = 12, so there are 120 + 12 = 132 members after the first year.In the second year, the club loses 5% of its members, which is 0.05 * 132 = 6.6, but since the number of members must be an integer, we consider a loss of 7 members ( assuming the figure is rounded up for practical reasons).Therefore, there are 132 -7 = 125 members at the end of the second year.</p>
<p>LLaMA</p>
<p>Question: Martin saves $10 every week.In addition, every third week, he earns an extra $15 from helping his neighbor.How much has Martin saved after 9 weeks?/ ANSWER: Think through the problem step by step, diving into each segment for a thorough exploration to piece together the final answer.Martin saves $10 each week, so over 9 weeks, he saves 9 * $10 = $90.Additionally, every third week, he earns an extra $15, which occurs three times within 9 weeks (in the 3rd, 6th, and 9th weeks).So, he earns an extra 3 * $15 = $45 from helping his neighbor.Therefore, the total amount Martin has saved after 9 weeks is $90 + $45 = $135.</p>
<p>Question: A teacher divides a class into groups for a project.If the ratio of boys to girls in the class is 3 to 2, and there are 30 students in the class, how many boys are in the class?/ ANSWER: Think through the problem step by step, diving into each segment for a thorough exploration to piece together the final answer.The total ratio units for boys to girls in the class is 3 + 2 = 5.With 30 students in the class, each ratio unit represents 30 / 5 = 6 students.Therefore, the number of boys, represented by 3 parts of the ratio, is 3 * 6 = 18.The answer is 18.</p>
<p>Question: Grandma wants to order 5 personalized backpacks for each of her grandchildren's first days of school.The backpacks are 20% off of $20 .00,and having their names monogrammed on the backpack will cost $12.00each.How much will the backpacks cost in total?/ ANSWER: Think through the problem step by step, diving into each segment for a thorough exploration to piece together the final answer.The backpacks are 20% off of $20.00, so the price after the discount is $20.00 -($20.00* 20%) = $20.00 -$4.00 = $16.00each.The monogramming costs an additional $12.00 per backpack.Therefore, the total cost for each backpack is $16.00 + $12.00 = $28.00.For 5 backpacks, the total cost will be 5 * $28.00 = $140.00.The correct answer is $140.00.</p>
<p><strong>Query</strong> {{query}}</p>
<p>LLaMA-3-8B-Instruct on MATH-500 -Task Instruction: A chat between a curious user and an AI assistant focused on solving mathematical and reasoning tasks.The assistant is expected to deliver step-by-step solutions to the user's questions, emphasizing mathematical accuracy and rigor throughout the process.It must ensure that each mathematical operation and logical deduction is carefully examined and validated to derive the correct solution.At the conclusion of the response, the final answer should be presented in the format of "The answer is: <ANSWER>.",thereby confirming the solution 's validity and demonstrating a thorough understanding of the problem-solving approach.</p>
<p>-Task Detail: In addressing equation-based inquiries, precision in algebra, geometry, piecewise functions, complex numbers, and financial mathematics is paramount.This involves a detailed analysis of each equation, assessing every element and specific condition.</p>
<p>For piecewise functions, it's critical to ensure continuity by solving for variables that maintain consistency across sections.In geometry, integrating measurements such as angles, lengths, and areas is fundamental.Algebraic queries require a consideration of all potential solutions and constraints, ensuring a comprehensive resolution.The addition of complex numbers into this mix necessitates a thorough understanding of their properties and operations to accurately determine both real and imaginary solutions.Similarly, tackling financial mathematics problems demands a deep comprehension of concepts such as compound interest, present value, and future value to make precise financial forecasts and comparisons.This holistic approach confirms that all aspects of the problem are considered and that the solution accounts for every requirement, assuring mathematical integrity in the resolution process.</p>
<p>-Output Format: 1. Solutions that involve fractions, square roots, or crucial mathematical figures (e.g., pi) must be simplified to their most fundamental form.This includes reducing fractions to their lowest terms and expressing square roots in their least complex radical form.</p>
<ol>
<li>
<p>Avoid the use of decimals unless the question explicitly requires it or they are necessary for conveying the most precise value possible.</p>
</li>
<li>
<p>Present solutions involving square roots in their reduced radical form, ensuring the simplification process enhances comprehension without diluting mathematical integrity.4. In scenarios involving complex numbers, represent answers in their standard form (a + bi ), ensuring both 'a' and 'b' are presented in their simplest, most refined state.This emphasizes the need for a clear, coherent representation of solutions encompassing complex numbers.5. Conclude your explanation with the statement: "The answer is: [<ANSWER>].",reinforcing consistency and clarity across various mathematical challenges.This concluding statement should encapsulate the solution in its simplest and most direct form, reflecting a thorough simplification and rationalization process.</p>
</li>
</ol>
<p>Your explanation must delineate a detailed, stepby-step progression leading to the final solution.This approach is not merely about arriving at the correct answer but about illuminating the path taken to get there, ensuring a deep understanding and clear demonstration of the reasoning behind each step.</p>
<p>-Examples: Here are some examples: ### Instruction:</p>
<p>A rectangle ABCD has sides AB = 8 units and BC = 6 units.A circle with a radius r units is inscribed within this rectangle.Calculate the radius r of the inscribed circle, ensuring the answer is in its simplest form.</p>
<h3>Response:</h3>
<p>We'll approach this problem by breaking it down into manageable steps.We start by understanding that the radius of the inscribed circle is equal to the distance from the center of the rectangle to any of its sides because the circle is perfectly inscribed.In a rectangle, this distance is half the length of the rectangle's shorter side.Therefore, the radius r of the inscribed circle is half the length of BC, which is $6 \div 2 = 3$ units.The answer is: $r=3$.</p>
<h3>Instruction: Given a triangle where two sides are represented by complex numbers (3 + 4i) units and (1 -2i) units, and the angle between them is 90 degrees, calculate the length of the hypotenuse.Ensure your answer includes a comprehensive breakdown of complex number operations and geometric principles applied.</h3>
<h3>Response:</h3>
<p>We'll approach this problem by breaking it down into manageable steps.We start by acknowledging that the length of a side represented by a complex number can be found using the modulus of that number.The modulus of the first side is $ \sqrt{3^2 + 4^2} = 5$ units, and the modulus of the second side is $\sqrt{1^2 + (-2)^2} = \sqrt {5}$ units.Since these sides form a right triangle and we are given that the angle between them is 90 degrees, we can apply the Pythagorean theorem to find the length of the hypotenuse.The hypotenuse's length squared will be the sum of the squares of the lengths of the other two sides, which is $5^2 + (\sqrt{5})^2 = 25 + 5 = 30$.Thus, the length of the hypotenuse is $\sqrt{30}$ units.The answer is: $\sqrt{30}$.</p>
<p>-Query: {{query}}</p>
<p>Figure 1 :
1
Figure 1: The crucial role of prompt formatting and its interaction with content.(A): Model-specific format sensitivity: LLM performance varies significantly across different prompt formats on GSM8K task.(B):Content-format interdependence: The optimal format for a prompt depends on its content, highlighting the need for joint optimization.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the CFPO pipeline within a single iteration round.In the initial Component-wise Content Optimization stage, case-diagnosis and Monte-Carlo sampling are employed for content mutation.Subsequently, the Format Optimization stage identifies the most suitable format for each content candidate.The yellow dashed line indicates where the LLM optimizer is employed to guide the optimization process.</p>
<p>Figure 3 :
3
Figure 3: An illustrative example of our Structured Prompt Template.This template systematically organizes the prompt into distinct components, each serving a specific functional role.When formulating a prompt, the template first employs a Query format to present examples and queries, and then integrates all content components via the Prompt Renderer to construct the comprehensive prompt string.</p>
<p>Figure 4 :
4
Figure 4: Built-in formats and rendering effects in our initial format pool.The final format configuration is achieved by selecting and combining elements from both the Prompt Renderer and the Query Format categories.</p>
<p>1: 'You do this often?' Speaker 2: 'It's my first time.'Output: no {{Query placeholder}} B.4.2 ARC-Challenge Prompt Renderer: Directly Joint Query Format: MultiChoice_QA You are a commonsense helper.I will provide several examples and a presented question.Your goal is to pick the most reasonable answer among the given options for the current question.Please respond with the corresponding label (A/B /C/D) for the correct answer.Here are some examples: Question: Forests have been cut and burned so that the land can be used to raise crops.Which consequence does this activity have on the atmosphere of Earth?Choices: A: It reduces the amount of carbon dioxide production B: It reduces the production of oxygen C: It decreases the greenhouse effect D: It decreases pollutants in the air Answer: B {{Query placeholder}}</p>
<p>Figure 5 :
5
Figure 5: Overview of in-context examples and text lengths for optimized prompts on various tasks and models.</p>
<p>Figure 6 :
6
Figure 6: Convergence behavior of format and content optimization (Mistral-7B-v0.1 on GSM8K).Values indicate performance scores, with improvements from the previous sub-round in parentheses.</p>
<p>1 | Every element of a group generates a cyclic subgroup of the group.Statement 2 | The symmetric group S_10 has 10 elements.Options: -A True, True -B False, False -C True, False</p>
<p>Table 2 :
2
(Pryzant et al., 2023) of the full CFPO pipeline against ablated variants with format-only (CFPO f ), content-only (CFPO c ), and sequential content-then-format (CFPO c+f ) optimization.CFPO c also compared to ProTeGi(Pryzant et al., 2023).
TaskMethodLlama3.1 Llama3-InsProTeGi81.0082.00CFPO f83.0086.00BBCCFPOc85.0085.00CFPO c+f88.0089.00CFPO90.0091.00ProTeGi54.7475.36CFPO f52.4676.65GSM8KCFPOc58.0777.71CFPO c+f61.9479.30CFPO63.3880.74TaskMethodLlama3.1 Llama3-InsImpact of Format GenerationBBCw/o Format Gen with Format Gen88.00 90.0087.00 91.00GSM8Kw/o Format Gen with Format Gen62.70 63.3878.85 80.74Different Format Selection StrategiesRandom85.0087.00BBCUCT(α = 0)86.0088.00UCT(ours)90.0091.00Random62.4078.82GSM8K UCT(α = 0)63.2379.08UCT(ours)63.3880.74</p>
<p>Table 3 :
3
Ablation of format generation and comparison of format selection strategies.
(format-only): optimizes the prompt format whileholding content constant, (2) CFPO c (content-only): optimizes the prompt content while hold-ing format constant, and (3) CFPO c+f (sequential):performs iterative content optimization followedby a separate, single-step format optimization. Wealso include ProTeGi (Pryzant et al., 2023) as abaseline. While CFPO c shares the goal of contentoptimization with ProTeGi, CFPO c incorporatescorrect cases in diagnosis and component-wise mu-tation as key inovations. Table 2 presents the re-sults of this analysis. CFPO c significantly outper-forms ProTeGi, demonstrating the effectiveness ofour content optimization strategy. Furthermore,the ablated variants all underperform compared tothe complete CFPO pipeline. These results under-score the interdependence of content and format inprompt optimization, highlighting the importanceof joint optimization for best performance.Analysis of Format Generation. We comparedthe full CFPO approach against variant that usesformat from initial format pool without using LLM</p>
<p>Table 4 :
4
Performance of CFPO using different optimizer models.</p>
<p>Table 5 :
5
Ablation of format generation and comparison of format selection strategies.
TaskMethod Llama3.1 Llama3-InsMMLUBaseline CFPO78.84 81.7482.03 83.77</p>
<p>Table 6 :
6
Stability Analysis: Comparison of reported CFPO scores with reproduced results (4 runs each).</p>
<p>Table 7 :
7
Average token usage, API calls, and estimated cost per round of CFPO optimization.Tokens (I/O): typical input/output tokens per round, API Calls: average API calls per round, Est.Cost: estimated cost in $.TaskAvg.Cost to Tar.Avg.Time to Tar.
OperationTokens (I/O) API Calls Est. CostCase-diagnose2k/0.2k32$ 0.83Apply Feedback0.5k/0.1k110$ 0.93Gen Variation0.5k/0.2k81$0.89Gen Format0.5k/0.1k2$0.016Gen Format Code0.6k/0.2k2$0.024Total (per Round) 159.6k/36.6k237$2.69BBC$14.105.24 hARC-C$16.816.25 hGSM8K$28.2510.50 hMATH500$19.857.38 h</p>
<p>Table 8 :
8
Average cost and time required for CFPO to reach the reported target state-of-the-art performance on each benchmark.</p>
<p>Answer: C QA_Titlecase_Separator Question || In 3 years, Jayden will be half of Ernesto's age.If Ernesto is 11 years old, how many years old is Jayden now?Answer || Let's think step by step.Ernesto = 11 + 3 = &lt;&lt;11+3=14&gt;&gt;14 Jayden = 14/2 = &lt;&lt;14/2=7&gt;&gt;7 in 3 years Now = 7 -3 = &lt;&lt;7-3=4&gt;&gt;4 Jayden is 4 years old.QA_Brackets_Colon_Newline[Question]: In 3 years, Jayden will be half of Ernesto's age.If Ernesto is 11 years old, how many years old is Jayden now?[Answer]: Let's think step by step.Ernesto = 11 + 3 = &lt;&lt;11+3=14&gt;&gt;14 Jayden = 14/2 = &lt;&lt;14/2=7&gt;&gt;7 in 3 years Now = 7 -3 = &lt;&lt;7-3=4&gt;&gt;4 Jayden is 4 years old.
Magic: Generating self-correction guideline for incontext text-to-sql. Arian Askari, Christian Poelitz, Xinye Tang, 2024</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. BIG bench authors. 2023</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Proceedings of the Annual Conference on Neural Information Processing Systems (NeurIPS). the Annual Conference on Neural Information Processing Systems (NeurIPS)Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei</p>
<p>Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, 2024</p>
<p>Posix: A prompt sensitivity index for large language models. Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty, arXiv:2410.021852024Preprint</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018Preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Greater: Gradients over reasoning makes smaller language models strong prompt optimizers. Sarkar Snigdha, Sarathi Das, Ryo Kamoi, Bo Pang, Yusen Zhang, Caiming Xiong, Rui Zhang, 2024</p>
<p>Promptbreeder: Self-referential self-improvement via prompt evolution. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, Tim Rocktäschel, arXiv:2309.167972023Preprint</p>
<p>Google, Prompting guide. 2024101</p>
<p>PPT: Pre-trained prompt tuning for few-shot learning. Yuxian Gu, Xu Han, Zhiyuan Liu, Minlie Huang, 10.18653/v1/2022.acl-long.576Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Q-tuning: Queue-based prompt tuning for lifelong few-shot language learning. Yanhui Guo, Shaoyuan Xu, Jinmiao Fu, Jia Liu, Chaosheng Dong, Bryan Wang, arXiv:2404.146072024Preprint</p>
<p>Language models can teach themselves to program better. Patrick Haluptzok, Matthew Bowers, Adam Tauman, Kalai , 2023</p>
<p>Does prompt formatting have any impact on llm performance?. Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, Sadid Hasan, arXiv:2411.105412024Preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Automated design of agentic systems. Shengran Hu, Cong Lu, Jeff Clune, arXiv:2408.084352024Preprint</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>Promptmaker: Prompt-based prototyping with large language models. Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, Carrie J Cai, 10.1145/3491101.3503564Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems, CHI EA '22. New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, Christopher Potts, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. Springer2006</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>More agents is all you need. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye, Transactions on Machine Learning Research. 2024</p>
<p>Prefixtuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, arXiv:2101.001902021arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step. Bowen Baker, Teddy LeeJanarXiv preprint</p>
<p>Introducing meta llama3: The most capable openly available llm to date. Meta. 2024a</p>
<p>The llama 3 herd of models. arXiv:2407.21783Meta. 2024bPreprint</p>
<p>Phi-3 technical report: A highly capable language model locally on your phone. arXiv:2404.142192024MicrosoftPreprint</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, arXiv:2401.005952024Preprint</p>
<p>arXiv:2303.08774OpenAI. 2024a. Gpt-4 technical report. Preprint</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, arXiv:2203.072812023Preprint</p>
<p>Automatic prompt optimization with "gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2023</p>
<p>The butterfly effect of altering prompts: How small changes and jailbreaks affect large language model performance. Abel Salinas, Fred Morstatter, arXiv:2401.037292024Preprint</p>
<p>Symbolic prompt program search: A structure-aware approach to efficient compile-time prompt optimization. Tobias Schnabel, Jennifer Neville, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)2024</p>
<p>. Sander Schulhoff, Michael Ilie, Nishant Balepur, Konstantine Kahadze, Amanda Liu, Chenglei Si, Yinheng Li, Aayush Gupta, Hyojung Han, Sevien Schulhoff, Pranav Sandeep Dulepet, Saurav Vidyadhara, Dayeon Ki, Sweta Agrawal, Chau Pham, Gerson Kroiz, Feileen Li, Hudson Tao, Ashay Srivastava, Hevander Da Costa, Saloni Gupta, Megan L Rogers, Inna Goncearenco, Giuseppe Sarli, Igor Galynker, Denis Peskoff, Marine Carpuat, Jules White, Shyamal Anadkat, Alexander Hoyleand Philip Resnik. 2024. The prompt report: A systematic survey of prompting techniques</p>
<p>Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>Evaluating the zero-shot robustness of instructiontuned language models. Jiuding Sun, Chantal Shaib, Byron C Wallace, arXiv:2306.112702023Preprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, arXiv:2401.067662024aPreprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, 10.18653/v1/2024.findings-acl.375Findings of the Association for Computational Linguistics: ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b</p>
<p>Langgpt: Rethinking structured reusable prompt design framework for llms from the programming language. Ming Wang, Yuanzhong Liu, Xiaoyu Liang, Songlian Li, Yijie Huang, Xiaoming Zhang, Sijia Shen, Chaofeng Guan, Daling Wang, Shi Feng, Huaiwen Zhang, Yifei Zhang, Minghui Zheng, Chi Zhang, 2024a</p>
<p>Promptagent: Strategic planning with language models enables expert-level prompt optimization. Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P Xing, Zhiting Hu, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024b</p>
<p>Multitask prompt tuning enables parameter-efficient transfer learning. Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim, arXiv:2303.028612023Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>WHO. 2023. Auto-gpt. </p>
<p>Large Language Models as Optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>Self-Taught Optimizer (STOP): Recursively Self-Improving Code Generation. Eric Zelikman, Eliana Lorch, Lester Mackey, Adam Tauman, Kalai , Conference On Language Model (COLM). 2024</p>
<p>Aflow: Automating agentic workflow generation. Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, Chenglin Wu, 2024</p>
<p>Tempera: Test-time prompting via reinforcement learning. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102023Preprint</p>
<p>ProSA: Assessing and understanding the prompt sensitivity of LLMs. Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen, 10.18653/v1/2024.findings-emnlp.108Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>Prosa: Assessing and understanding the prompt sensitivity of llms. Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen, arXiv:2410.124052024bPreprint</p>            </div>
        </div>

    </div>
</body>
</html>
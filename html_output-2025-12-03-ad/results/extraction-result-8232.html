<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8232 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8232</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8232</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-279465470</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.15841v2.pdf" target="_blank">MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents</a></p>
                <p><strong>Paper Abstract:</strong> Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries. Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance. This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths. We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks. At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning. This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information. To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences. Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5x while reducing memory usage by 3.7x compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon. Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8232.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8232.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEM1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-Efficient Mechanism via learning 1-step integrated reasoning and consolidation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-trained language-agent framework that maintains a compact, consolidated internal state (<IS>) updated each turn to perform reasoning and memory consolidation, enabling near-constant memory across arbitrarily long multi-turn tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MEM1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An end-to-end RL agent that interleaves reasoning and action and keeps only a single compact internal state (<IS>) in the context; it prunes prior steps and consolidates prior <IS>, <query>, and <info> into a new <IS> each turn. Training uses PPO with a masked-trajectory and 2D attention masking to respect the evolving context.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7BBase (fine-tuned into MEM1 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base LM (Qwen2.5-7B) fine-tuned with PPO; actor & critic learning rates and training details described (actor LR 1e-6, critic LR 1e-5); all MEM1 variants are fine-tuned from Qwen2.5-7BBase.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-objective multi-hop QA / WebShop web navigation / RAG QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Composed multi-objective multi-hop QA (compositions of HotpotQA and Natural Questions) requiring multiple retrievals and reasoning steps; WebShop web navigation requiring iterative browsing and product selection; RAG-based Wikipedia QA and zero-shot online web-QA.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning; multi-turn question answering; web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working/internal consolidated state (agent-internal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>At each turn the model outputs a new internal state token block <IS_t> which summarizes prior knowledge and reasoning; after generating an action (<query> or <answer>) and receiving <info>, the agent consolidates (<IS_t>, <query_t>, <info_t>) into <IS_{t+1}> and prunes all prior tags from the prompt; training uses a masked trajectory and a 2D attention mask so tokens only attend to what was available at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compact textual internal state combining prior memory and intermediate reasoning (summaries / extracted key facts / plan), stored as a single <IS> element representing consolidated thoughts + memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based / implicit: only the most recent consolidated <IS> is kept and directly prefixed to the prompt; no explicit vector DB retrieval from the internal state (external RAG retrievals are used for environment info but discarded after consolidation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On a 16-objective multi-hop QA task MEM1-7B achieved a 3.5× performance improvement while reducing memory usage by 3.7× compared to Qwen2.5-14B-Instruct (paper abstract); at 16-objective MEM1 used 27.1% of the peak tokens and 29.3% of total inference time relative to Qwen2.5-14B-Instruct. On single-objective Wikipedia RAG evaluation MEM1-QA reported EM=0.405, F1=0.471 with peak token usage 5.63×10^2 (Table 3). In WebShop MEM1-WebShop reported Avg Final Reward=70.87, Peak Token ≈0.81×10^3, Dependency ≈0.15×10^6, Inference Time ≈2.61s (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to (1) full-context baselines (Qwen2.5-7B/14B) MEM1 maintains near-constant peak tokens as objectives scale and surpasses larger models at long horizons; (2) A-MEM and truncation variants: MEM1 achieves lower peak token usage and faster inference; (3) format-reward ablation: enforcing format tags speeds training but reduces final EM (format-constrained EM=0.466 vs outcome-only EM=0.709 on the 2-objective test set). Also SFT vs RL: supervised fine-tuned MEM1 collapses beyond ~6 objectives while RL-trained MEM1 generalizes to up to 16 objectives (Table 4 / Sec. D.1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learning memory consolidation as part of reasoning (single compact internal state) enables constant memory usage and substantial inference efficiency gains, while matching or exceeding baseline performance at long horizons; RL training (PPO) is critical for learning robust consolidation policies; masked-trajectory attention is necessary for correct policy gradients under dynamic contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Assumes access to verifiable rewards; format rewards accelerate format adherence but can harm final task performance; the attention masking approximation (no duplication of <IS> with separate position ids) is used for efficiency and does not perfectly recover original attention; open-ended tasks with ambiguous/noisy rewards remain challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8232.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEM1-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MEM1 (Supervised Fine-Tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuning variant of the MEM1 pipeline trained on high-quality trajectories (SFT) using the same MEM1 prompt/rollout, but without RL optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MEM1-QA (SFT)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same MEM1 prompt and runtime consolidation mechanism but trained via supervised fine-tuning on curated trajectories (Swift), not RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7BBase (SFT fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SFT variant derived from the same base as MEM1 but optimized with supervised learning on trajectory data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-objective multi-hop QA (composed tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above: composed multi-hop QA requiring multiple retrievals and memory consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning; multi-turn QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working/internal consolidated state (same as MEM1)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Uses the same <IS>-based consolidation and truncated prompt rollout at inference as MEM1; learned via supervised examples rather than RL.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compact <IS> summaries like MEM1.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based consolidated state kept as the only prior memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Underperforms RL: SFT variant reports lower EM and collapses on longer objectives; Table 4 shows SFT underperforms RL across increasing objectives and collapses beyond ~6 objectives (e.g., large negative gaps in high-objective counts). Specific numbers: MEM1-RL EM (2-objective) 0.709 vs MEM1-SFT 0.302 (Table 4 entries described in Sec. D.1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparison (ablation of training method) shows RL generalizes far better than SFT for multi-objective tasks; SFT collapses when number of objectives grows (>6), while RL-trained MEM1 remains robust.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SFT alone cannot substitute for RL when learning to consolidate memory for long-horizon composed tasks — RL yields stronger generalization and robustness to longer horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>SFT models trained on limited objective counts fail to generalize to longer-horizon tasks and can 'collapse' performance as objectives increase.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8232.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-context baselines (Qwen2.5-14B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-14B-Instruct (full-context baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger instruct-tuned LM baseline that the paper compares to; baseline systems append full interaction history to the prompt leading to growing context length and high inference memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2.5 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Qwen2.5-14B-Instruct (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A 14B parameter instruct-tuned LLM used as an uncollapsed full-context baseline; uses traditional approach of appending all past observations, thoughts, and actions into the context each turn.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-14B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large LM (14B parameters) instruct-tuned; used as a strong baseline in QA and WebShop experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-objective multi-hop QA / single-objective QA / WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks as MEM1 where the baseline retains full dialogue/history in prompt each turn (leading to increased peak tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning; multi-turn QA; web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic / full prompt accumulation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompt concatenation of all past turns (thoughts, actions, observations) — unboundedly growing context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>All prior tokens from previous steps: intermediate thoughts, actions, queries, retrieved snippets, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / attention over entire appended context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Strong baseline performance at small objective counts; initially outperforms MEM1 for small numbers of objectives but is overtaken by MEM1 at longer horizons. Paper reports MEM1 eventually surpasses this 14B baseline at high-objective counts; at 16-objective MEM1 uses 27.1% of the peak tokens and 29.3% of inference time relative to Qwen2.5-14B-Instruct (paper). Specific numeric EM/F1 for Qwen2.5-14B are reported in Table 3 (e.g., Qwen2.5-14B-Inst EM ≈0.422, F1 ≈0.534 on Wiki RAG rows of Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparison shows full-context growth causes linear scaling of peak token usage with objectives; MEM1 keeps near-constant peak tokens. MEM1 surpasses this larger model at large objective counts while using substantially less memory and inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Full-context appending yields high memory and time costs and worse generalization to longer horizons; learned consolidation (MEM1) can match or exceed larger full-context models at scale while being far more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Full-context method is simple but not scalable: inference cost and context length grow, hurting deployment and long-horizon generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8232.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-7B-Instruct (truncate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-7B-Instruct (truncation variant using MEM1 prompt/rollout)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B instruct model of the same size family as MEM1 used as a truncation baseline by applying MEM1's prompt/rollout without the learned consolidation policy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Qwen2.5 technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Qwen2.5-7B-Instruct (truncate)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same model family as MEM1 but using MEM1's truncation prompt & rollout pipeline to isolate benefits of prompting/rollout design (no learned consolidation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>7B instruct-tuned LM used as baseline for comparisons of pipeline and prompt effects.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-objective QA / Wikipedia RAG / Web Q&A</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same evaluation tasks; applied MEM1-style truncation to compare naive truncation benefits vs learned consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-turn QA; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>truncated prompt / partial context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Programmatic truncation of context following MEM1 rollout template (but without learned consolidation behavior), limiting memory by prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent few turns retained per truncation heuristic.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based truncation / prompt concatenation of truncated history.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Used as an ablation to show benefits of learned consolidation; MEM1 outperforms truncation-only baseline in either performance or efficiency in many reported settings (Tab. 1 & 3). Example: Qwen2.5-7B-Instruct (truncate) had lower EM/F1 and higher peak tokens than MEM1 on several evaluations (Table 3 rows).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Shows that prompt/rollout truncation alone yields some efficiency gains but does not match the accuracy-efficiency tradeoff of MEM1 which learns what to keep in memory via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompting and rollout truncation provide partial benefits, but learned consolidation (MEM1) yields superior scaling and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Truncation heuristics can omit necessary information and are less adaptive than a learned consolidation policy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8232.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>A-MEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A-MEM (Agentic Memory for LLM agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An external memory baseline that augments an instruct model with a vector database for memory retrieval to capture effects of retrieval-based external memory modules in agentic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A-mem: Agentic memory for llm agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>A-MEM (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An instruct model augmented with a vector DB (external memory) to store/retrieve past information; evaluated in combination with MEM1's prompt/rollout pipeline as an external-memory comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>External memory module (vector DB + retriever) paired with an Instruct LM of comparable parameter size to MEM1 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-objective QA / WebShop / Wikipedia RAG</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks; A-MEM stores agentic memory in a vector DB and retrieves as needed.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning; memory-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval-augmented memory (vector DB)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Vector database that stores past content and retrieves by similarity during the agent's turn; integrated externally (not end-to-end with policy).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Embeddings of past tokens/notes/summaries stored in vector DB (external documents / intermediate thoughts).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search via vector similarity (Faiss-style retrieval pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>A-MEM variants were evaluated; MEM1 generally achieved better efficiency and competitive or better task performance compared to A-MEM augmented instruct models (Tables 1 & 3). Specific numbers: A-MEM versions show reduced peak tokens relative to naive full-context but typically do not match MEM1's combination of high accuracy and low peak tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to A-MEM, MEM1 attains similar or better effectiveness while being simpler (no separate retrieval module) and more memory/time efficient because consolidation is learned end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External memory modules can help but introduce engineering complexity and are not optimized end-to-end with the policy; integrated learned consolidation (MEM1) can outperform A-MEM in the studied settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>External memory requires separate training/engineering and is not jointly optimized with the agent policy, potentially limiting synergy between reasoning and memory management.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8232.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepResearcher: Scaling deep research via reinforcement learning in real-world environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent RL-based research agent baseline that interleaves reasoning and web/tool use; cited as a comparator for research-style agents and emergent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepresearcher: Scaling deep research via reinforcement learning in real-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DeepResearcher</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL-trained agent for deep research-style web tasks that exhibits iterative search and verification behaviors; used as a performance baseline for QA/web tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior RL agent (details in cited paper); in this paper it serves as a baseline comparator for single-objective web-QA and research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-objective Online Web-QA / research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain web QA where the agent issues web searches and synthesizes answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-domain QA; multi-turn web interaction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>full prompt accumulation / chain-of-thought in prompt (implicit episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Appends past reasoning and actions into prompt (traditional chain-of-thought + tool outputs) rather than learning a compact consolidated state (per paper's related-work description).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate thoughts, past searches and retrieved snippets kept in context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / attention over history</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as a baseline: DeepResearcher optimized F1 on its task and is competitive on single-objective online web-QA; MEM1 achieves comparable or better efficiency and competitive effectiveness in zero-shot transfer (Table 3 shows DeepResearcher F1 and EM numbers vs MEM1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared in single-objective online web-QA transfer; MEM1 shows improved inference efficiency with comparable effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DeepResearcher-type agents demonstrate many search strategies also observed in MEM1 (verification, decomposition), but MEM1's learned consolidation yields efficiency advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prior research agents often rely on full-context or separate modules and may not scale efficiently to long-horizon, multi-objective compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8232.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-trained baseline specialized on 1-objective search-and-answer tasks (Wiki RAG) used for comparison; trained with EM reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Search-r1: Training llms to reason and leverage search engines with reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Search-R1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL agent trained on 1-objective QA with EM as reward; used as a comparative baseline for MEM1 on Wiki RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior RL agent from referenced work; trained specifically for 1-objective RAG tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Wiki RAG / single-objective QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question answering with retrieval from a Wikipedia datastore; agent issues retrievals and synthesizes final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>retrieval-augmented QA; multi-turn retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>full-context / accumulated history</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Accumulates chain-of-thought, queries and retrieved snippets in prompt across turns (standard approach for many RL agents), rather than a compact consolidated internal state.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>All prior tokens including thoughts, queries, and retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation; retrieval of external documents via RAG for environment info.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Search-R1 is a strong baseline on Wiki RAG (Table 3: EM ~0.445, F1 ~0.516 reported in the table rows in the paper); MEM1 achieves comparable EM/F1 with much lower peak token usage and faster inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Used to show MEM1 trained on 2-objective tasks can match or approach performance of agents trained specifically on 1-objective tasks while being more memory efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Agents specialized on 1-objective tasks (Search-R1) do well on their domain, but MEM1's consolidation approach trained on composed tasks generalizes and is more efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Search-R1 (like many baselines) uses full history and so scales poorly in memory/time as objective counts increase.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8232.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that interleaves reasoning (chain-of-thought) and actions (tool calls) and is cited as background for agentic systems that append thoughts/actions to context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting pattern that interleaves natural-language reasoning and explicit actions/tool-use; commonly implemented by appending thoughts/actions to the prompt across turns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting methodology rather than a single LM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multi-turn tool-using agent tasks (background)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Interactive tasks where the agent alternates reasoning and environment actions, building up context over turns.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-turn reasoning; tool-using agents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>scratchpad / chain-of-thought stored in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompt concatenation of prior thoughts/actions/observations each turn (explicit chain-of-thought tokens remain in the prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate thoughts (<think>), actions, observations appended to context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation and attention over history</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited to motivate problem: existing agents that continuously append thoughts/actions suffer unbounded context growth, motivating MEM1's consolidation approach.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct-style append-to-context is effective for short-horizon tasks but leads to unbounded memory growth and degraded performance for long-horizon interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not scalable: context grows unboundedly, incurring O(N^2) compute (or O(N) with KV caching) and impairing performance on out-of-distribution long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8232.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8232.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-FLAN / Agent-R / AgentLM (WebShop baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agent-FLAN / Agent-R / AgentLM (various agent baselines for WebShop)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agent-FLAN, Agent-R, and AgentLM are prior web-agent baselines evaluated on WebShop; they represent different training/data choices and memory handling strategies (mostly full-context accumulation or specialized training).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Agent-flan: Designing data and methods of effective agent tuning for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Agent-FLAN / Agent-R / AgentLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent-FLAN: agent tuning methodology (Agent-FLAN-7B reported). Agent-R: reflection/self-training agent (Agent-R-8B). AgentLM: other agent baselines (7B/13B) used in WebShop comparisons. These models are compared with MEM1-WebShop for reward and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Agent-FLAN-7B, Agent-R-8B, AgentLM-7B/13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various instruct/agent-tuned LMs of sizes 7B–13B reported in Table 2; their memory usage and training details are from cited works.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Browse an online shop iteratively and select/buy products matching a natural-language description.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web navigation; multi-step interaction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>typically full-context accumulation or agent-specific state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Mostly appending past interactions and retrieved page content to the prompt; Agent-R uses iterative self-training/reflection mechanisms described in its paper; AgentLM specifics not detailed in MEM1 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past page states, clicks, intermediate reasoning preserved in prompt or internal agent mechanisms depending on the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / agent-specific mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 reports: Agent-FLAN-7B Avg Final Reward=40.35 (Peak Token ≈3.37×10^3, Inference Time ≈9.95s); Agent-R-8B and AgentLM variants report higher final rewards in some configurations but with larger token/dependency costs. MEM1-WebShop achieves Avg Final Reward=70.87 with much lower peak tokens (≈0.81×10^3) and faster inference (≈2.61s).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>MEM1-WebShop substantially outperforms Agent-FLAN in reward and is more efficient than other agent baselines in peak token usage and inference time (see Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>On WebShop, MEM1 achieves top final reward while being more memory- and time-efficient than several agent baselines, demonstrating that learned consolidation generalizes beyond QA to web navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Some baseline details (e.g., precise AgentLM citation) not provided in-paper; baselines may be closed-source making direct comparisons dependent on reported numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A-mem: Agentic memory for llm agents. <em>(Rating: 2)</em></li>
                <li>Search-r1: Training llms to reason and leverage search engines with reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models. <em>(Rating: 1)</em></li>
                <li>Agent-flan: Designing data and methods of effective agent tuning for large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8232",
    "paper_id": "paper-279465470",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "MEM1",
            "name_full": "Memory-Efficient Mechanism via learning 1-step integrated reasoning and consolidation",
            "brief_description": "An RL-trained language-agent framework that maintains a compact, consolidated internal state (&lt;IS&gt;) updated each turn to perform reasoning and memory consolidation, enabling near-constant memory across arbitrarily long multi-turn tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MEM1",
            "agent_description": "An end-to-end RL agent that interleaves reasoning and action and keeps only a single compact internal state (&lt;IS&gt;) in the context; it prunes prior steps and consolidates prior &lt;IS&gt;, &lt;query&gt;, and &lt;info&gt; into a new &lt;IS&gt; each turn. Training uses PPO with a masked-trajectory and 2D attention masking to respect the evolving context.",
            "model_name": "Qwen2.5-7BBase (fine-tuned into MEM1 variants)",
            "model_description": "Base LM (Qwen2.5-7B) fine-tuned with PPO; actor & critic learning rates and training details described (actor LR 1e-6, critic LR 1e-5); all MEM1 variants are fine-tuned from Qwen2.5-7BBase.",
            "task_name": "Multi-objective multi-hop QA / WebShop web navigation / RAG QA",
            "task_description": "Composed multi-objective multi-hop QA (compositions of HotpotQA and Natural Questions) requiring multiple retrievals and reasoning steps; WebShop web navigation requiring iterative browsing and product selection; RAG-based Wikipedia QA and zero-shot online web-QA.",
            "task_type": "multi-step reasoning; multi-turn question answering; web navigation",
            "memory_used": true,
            "memory_type": "working/internal consolidated state (agent-internal)",
            "memory_mechanism": "At each turn the model outputs a new internal state token block &lt;IS_t&gt; which summarizes prior knowledge and reasoning; after generating an action (&lt;query&gt; or &lt;answer&gt;) and receiving &lt;info&gt;, the agent consolidates (&lt;IS_t&gt;, &lt;query_t&gt;, &lt;info_t&gt;) into &lt;IS_{t+1}&gt; and prunes all prior tags from the prompt; training uses a masked trajectory and a 2D attention mask so tokens only attend to what was available at generation time.",
            "memory_representation": "Compact textual internal state combining prior memory and intermediate reasoning (summaries / extracted key facts / plan), stored as a single &lt;IS&gt; element representing consolidated thoughts + memory.",
            "memory_retrieval_method": "Recency-based / implicit: only the most recent consolidated &lt;IS&gt; is kept and directly prefixed to the prompt; no explicit vector DB retrieval from the internal state (external RAG retrievals are used for environment info but discarded after consolidation).",
            "performance_with_memory": "On a 16-objective multi-hop QA task MEM1-7B achieved a 3.5× performance improvement while reducing memory usage by 3.7× compared to Qwen2.5-14B-Instruct (paper abstract); at 16-objective MEM1 used 27.1% of the peak tokens and 29.3% of total inference time relative to Qwen2.5-14B-Instruct. On single-objective Wikipedia RAG evaluation MEM1-QA reported EM=0.405, F1=0.471 with peak token usage 5.63×10^2 (Table 3). In WebShop MEM1-WebShop reported Avg Final Reward=70.87, Peak Token ≈0.81×10^3, Dependency ≈0.15×10^6, Inference Time ≈2.61s (Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to (1) full-context baselines (Qwen2.5-7B/14B) MEM1 maintains near-constant peak tokens as objectives scale and surpasses larger models at long horizons; (2) A-MEM and truncation variants: MEM1 achieves lower peak token usage and faster inference; (3) format-reward ablation: enforcing format tags speeds training but reduces final EM (format-constrained EM=0.466 vs outcome-only EM=0.709 on the 2-objective test set). Also SFT vs RL: supervised fine-tuned MEM1 collapses beyond ~6 objectives while RL-trained MEM1 generalizes to up to 16 objectives (Table 4 / Sec. D.1).",
            "key_findings": "Learning memory consolidation as part of reasoning (single compact internal state) enables constant memory usage and substantial inference efficiency gains, while matching or exceeding baseline performance at long horizons; RL training (PPO) is critical for learning robust consolidation policies; masked-trajectory attention is necessary for correct policy gradients under dynamic contexts.",
            "limitations_or_challenges": "Assumes access to verifiable rewards; format rewards accelerate format adherence but can harm final task performance; the attention masking approximation (no duplication of &lt;IS&gt; with separate position ids) is used for efficiency and does not perfectly recover original attention; open-ended tasks with ambiguous/noisy rewards remain challenging.",
            "uuid": "e8232.0",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MEM1-SFT",
            "name_full": "MEM1 (Supervised Fine-Tuned variant)",
            "brief_description": "A supervised fine-tuning variant of the MEM1 pipeline trained on high-quality trajectories (SFT) using the same MEM1 prompt/rollout, but without RL optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MEM1-QA (SFT)",
            "agent_description": "Same MEM1 prompt and runtime consolidation mechanism but trained via supervised fine-tuning on curated trajectories (Swift), not RL.",
            "model_name": "Qwen2.5-7BBase (SFT fine-tuned)",
            "model_description": "SFT variant derived from the same base as MEM1 but optimized with supervised learning on trajectory data.",
            "task_name": "Multi-objective multi-hop QA (composed tasks)",
            "task_description": "As above: composed multi-hop QA requiring multiple retrievals and memory consolidation.",
            "task_type": "multi-step reasoning; multi-turn QA",
            "memory_used": true,
            "memory_type": "working/internal consolidated state (same as MEM1)",
            "memory_mechanism": "Uses the same &lt;IS&gt;-based consolidation and truncated prompt rollout at inference as MEM1; learned via supervised examples rather than RL.",
            "memory_representation": "Compact &lt;IS&gt; summaries like MEM1.",
            "memory_retrieval_method": "Recency-based consolidated state kept as the only prior memory.",
            "performance_with_memory": "Underperforms RL: SFT variant reports lower EM and collapses on longer objectives; Table 4 shows SFT underperforms RL across increasing objectives and collapses beyond ~6 objectives (e.g., large negative gaps in high-objective counts). Specific numbers: MEM1-RL EM (2-objective) 0.709 vs MEM1-SFT 0.302 (Table 4 entries described in Sec. D.1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Direct comparison (ablation of training method) shows RL generalizes far better than SFT for multi-objective tasks; SFT collapses when number of objectives grows (&gt;6), while RL-trained MEM1 remains robust.",
            "key_findings": "SFT alone cannot substitute for RL when learning to consolidate memory for long-horizon composed tasks — RL yields stronger generalization and robustness to longer horizons.",
            "limitations_or_challenges": "SFT models trained on limited objective counts fail to generalize to longer-horizon tasks and can 'collapse' performance as objectives increase.",
            "uuid": "e8232.1",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Full-context baselines (Qwen2.5-14B)",
            "name_full": "Qwen2.5-14B-Instruct (full-context baseline)",
            "brief_description": "A larger instruct-tuned LM baseline that the paper compares to; baseline systems append full interaction history to the prompt leading to growing context length and high inference memory.",
            "citation_title": "Qwen2.5 technical report.",
            "mention_or_use": "use",
            "agent_name": "Qwen2.5-14B-Instruct (baseline)",
            "agent_description": "A 14B parameter instruct-tuned LLM used as an uncollapsed full-context baseline; uses traditional approach of appending all past observations, thoughts, and actions into the context each turn.",
            "model_name": "Qwen2.5-14B-Instruct",
            "model_description": "Large LM (14B parameters) instruct-tuned; used as a strong baseline in QA and WebShop experiments.",
            "task_name": "Multi-objective multi-hop QA / single-objective QA / WebShop",
            "task_description": "Same evaluation tasks as MEM1 where the baseline retains full dialogue/history in prompt each turn (leading to increased peak tokens).",
            "task_type": "multi-step reasoning; multi-turn QA; web navigation",
            "memory_used": true,
            "memory_type": "episodic / full prompt accumulation",
            "memory_mechanism": "Prompt concatenation of all past turns (thoughts, actions, observations) — unboundedly growing context.",
            "memory_representation": "All prior tokens from previous steps: intermediate thoughts, actions, queries, retrieved snippets, etc.",
            "memory_retrieval_method": "Prompt concatenation / attention over entire appended context",
            "performance_with_memory": "Strong baseline performance at small objective counts; initially outperforms MEM1 for small numbers of objectives but is overtaken by MEM1 at longer horizons. Paper reports MEM1 eventually surpasses this 14B baseline at high-objective counts; at 16-objective MEM1 uses 27.1% of the peak tokens and 29.3% of inference time relative to Qwen2.5-14B-Instruct (paper). Specific numeric EM/F1 for Qwen2.5-14B are reported in Table 3 (e.g., Qwen2.5-14B-Inst EM ≈0.422, F1 ≈0.534 on Wiki RAG rows of Table 3).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Comparison shows full-context growth causes linear scaling of peak token usage with objectives; MEM1 keeps near-constant peak tokens. MEM1 surpasses this larger model at large objective counts while using substantially less memory and inference time.",
            "key_findings": "Full-context appending yields high memory and time costs and worse generalization to longer horizons; learned consolidation (MEM1) can match or exceed larger full-context models at scale while being far more efficient.",
            "limitations_or_challenges": "Full-context method is simple but not scalable: inference cost and context length grow, hurting deployment and long-horizon generalization.",
            "uuid": "e8232.2",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Qwen2.5-7B-Instruct (truncate)",
            "name_full": "Qwen2.5-7B-Instruct (truncation variant using MEM1 prompt/rollout)",
            "brief_description": "A 7B instruct model of the same size family as MEM1 used as a truncation baseline by applying MEM1's prompt/rollout without the learned consolidation policy.",
            "citation_title": "Qwen2.5 technical report.",
            "mention_or_use": "use",
            "agent_name": "Qwen2.5-7B-Instruct (truncate)",
            "agent_description": "Same model family as MEM1 but using MEM1's truncation prompt & rollout pipeline to isolate benefits of prompting/rollout design (no learned consolidation).",
            "model_name": "Qwen2.5-7B-Instruct",
            "model_description": "7B instruct-tuned LM used as baseline for comparisons of pipeline and prompt effects.",
            "task_name": "Multi-objective QA / Wikipedia RAG / Web Q&A",
            "task_description": "Same evaluation tasks; applied MEM1-style truncation to compare naive truncation benefits vs learned consolidation.",
            "task_type": "multi-turn QA; multi-step reasoning",
            "memory_used": true,
            "memory_type": "truncated prompt / partial context",
            "memory_mechanism": "Programmatic truncation of context following MEM1 rollout template (but without learned consolidation behavior), limiting memory by prompt engineering.",
            "memory_representation": "Recent few turns retained per truncation heuristic.",
            "memory_retrieval_method": "Recency-based truncation / prompt concatenation of truncated history.",
            "performance_with_memory": "Used as an ablation to show benefits of learned consolidation; MEM1 outperforms truncation-only baseline in either performance or efficiency in many reported settings (Tab. 1 & 3). Example: Qwen2.5-7B-Instruct (truncate) had lower EM/F1 and higher peak tokens than MEM1 on several evaluations (Table 3 rows).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Shows that prompt/rollout truncation alone yields some efficiency gains but does not match the accuracy-efficiency tradeoff of MEM1 which learns what to keep in memory via RL.",
            "key_findings": "Prompting and rollout truncation provide partial benefits, but learned consolidation (MEM1) yields superior scaling and generalization.",
            "limitations_or_challenges": "Truncation heuristics can omit necessary information and are less adaptive than a learned consolidation policy.",
            "uuid": "e8232.3",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "A-MEM",
            "name_full": "A-MEM (Agentic Memory for LLM agents)",
            "brief_description": "An external memory baseline that augments an instruct model with a vector database for memory retrieval to capture effects of retrieval-based external memory modules in agentic systems.",
            "citation_title": "A-mem: Agentic memory for llm agents.",
            "mention_or_use": "use",
            "agent_name": "A-MEM (baseline)",
            "agent_description": "An instruct model augmented with a vector DB (external memory) to store/retrieve past information; evaluated in combination with MEM1's prompt/rollout pipeline as an external-memory comparison.",
            "model_name": null,
            "model_description": "External memory module (vector DB + retriever) paired with an Instruct LM of comparable parameter size to MEM1 in experiments.",
            "task_name": "Multi-objective QA / WebShop / Wikipedia RAG",
            "task_description": "Same tasks; A-MEM stores agentic memory in a vector DB and retrieves as needed.",
            "task_type": "multi-step reasoning; memory-augmented QA",
            "memory_used": true,
            "memory_type": "external retrieval-augmented memory (vector DB)",
            "memory_mechanism": "Vector database that stores past content and retrieves by similarity during the agent's turn; integrated externally (not end-to-end with policy).",
            "memory_representation": "Embeddings of past tokens/notes/summaries stored in vector DB (external documents / intermediate thoughts).",
            "memory_retrieval_method": "Semantic search via vector similarity (Faiss-style retrieval pipeline)",
            "performance_with_memory": "A-MEM variants were evaluated; MEM1 generally achieved better efficiency and competitive or better task performance compared to A-MEM augmented instruct models (Tables 1 & 3). Specific numbers: A-MEM versions show reduced peak tokens relative to naive full-context but typically do not match MEM1's combination of high accuracy and low peak tokens.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to A-MEM, MEM1 attains similar or better effectiveness while being simpler (no separate retrieval module) and more memory/time efficient because consolidation is learned end-to-end.",
            "key_findings": "External memory modules can help but introduce engineering complexity and are not optimized end-to-end with the policy; integrated learned consolidation (MEM1) can outperform A-MEM in the studied settings.",
            "limitations_or_challenges": "External memory requires separate training/engineering and is not jointly optimized with the agent policy, potentially limiting synergy between reasoning and memory management.",
            "uuid": "e8232.4",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "DeepResearcher",
            "name_full": "DeepResearcher: Scaling deep research via reinforcement learning in real-world environments",
            "brief_description": "A recent RL-based research agent baseline that interleaves reasoning and web/tool use; cited as a comparator for research-style agents and emergent behaviors.",
            "citation_title": "Deepresearcher: Scaling deep research via reinforcement learning in real-world environments.",
            "mention_or_use": "use",
            "agent_name": "DeepResearcher",
            "agent_description": "An RL-trained agent for deep research-style web tasks that exhibits iterative search and verification behaviors; used as a performance baseline for QA/web tasks.",
            "model_name": null,
            "model_description": "Prior RL agent (details in cited paper); in this paper it serves as a baseline comparator for single-objective web-QA and research tasks.",
            "task_name": "Single-objective Online Web-QA / research tasks",
            "task_description": "Open-domain web QA where the agent issues web searches and synthesizes answers.",
            "task_type": "open-domain QA; multi-turn web interaction",
            "memory_used": true,
            "memory_type": "full prompt accumulation / chain-of-thought in prompt (implicit episodic memory)",
            "memory_mechanism": "Appends past reasoning and actions into prompt (traditional chain-of-thought + tool outputs) rather than learning a compact consolidated state (per paper's related-work description).",
            "memory_representation": "Intermediate thoughts, past searches and retrieved snippets kept in context.",
            "memory_retrieval_method": "Prompt concatenation / attention over history",
            "performance_with_memory": "Reported as a baseline: DeepResearcher optimized F1 on its task and is competitive on single-objective online web-QA; MEM1 achieves comparable or better efficiency and competitive effectiveness in zero-shot transfer (Table 3 shows DeepResearcher F1 and EM numbers vs MEM1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared in single-objective online web-QA transfer; MEM1 shows improved inference efficiency with comparable effectiveness.",
            "key_findings": "DeepResearcher-type agents demonstrate many search strategies also observed in MEM1 (verification, decomposition), but MEM1's learned consolidation yields efficiency advantages.",
            "limitations_or_challenges": "Prior research agents often rely on full-context or separate modules and may not scale efficiently to long-horizon, multi-objective compositions.",
            "uuid": "e8232.5",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Search-R1",
            "name_full": "Search-R1: Training LLMs to reason and leverage search engines with reinforcement learning",
            "brief_description": "An RL-trained baseline specialized on 1-objective search-and-answer tasks (Wiki RAG) used for comparison; trained with EM reward.",
            "citation_title": "Search-r1: Training llms to reason and leverage search engines with reinforcement learning.",
            "mention_or_use": "use",
            "agent_name": "Search-R1",
            "agent_description": "An RL agent trained on 1-objective QA with EM as reward; used as a comparative baseline for MEM1 on Wiki RAG.",
            "model_name": null,
            "model_description": "Prior RL agent from referenced work; trained specifically for 1-objective RAG tasks.",
            "task_name": "Wiki RAG / single-objective QA",
            "task_description": "Question answering with retrieval from a Wikipedia datastore; agent issues retrievals and synthesizes final answer.",
            "task_type": "retrieval-augmented QA; multi-turn retrieval",
            "memory_used": true,
            "memory_type": "full-context / accumulated history",
            "memory_mechanism": "Accumulates chain-of-thought, queries and retrieved snippets in prompt across turns (standard approach for many RL agents), rather than a compact consolidated internal state.",
            "memory_representation": "All prior tokens including thoughts, queries, and retrieved passages.",
            "memory_retrieval_method": "Prompt concatenation; retrieval of external documents via RAG for environment info.",
            "performance_with_memory": "Search-R1 is a strong baseline on Wiki RAG (Table 3: EM ~0.445, F1 ~0.516 reported in the table rows in the paper); MEM1 achieves comparable EM/F1 with much lower peak token usage and faster inference.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Used to show MEM1 trained on 2-objective tasks can match or approach performance of agents trained specifically on 1-objective tasks while being more memory efficient.",
            "key_findings": "Agents specialized on 1-objective tasks (Search-R1) do well on their domain, but MEM1's consolidation approach trained on composed tasks generalizes and is more efficient.",
            "limitations_or_challenges": "Search-R1 (like many baselines) uses full history and so scales poorly in memory/time as objective counts increase.",
            "uuid": "e8232.6",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting framework that interleaves reasoning (chain-of-thought) and actions (tool calls) and is cited as background for agentic systems that append thoughts/actions to context.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "agent_name": "ReAct (framework)",
            "agent_description": "A prompting pattern that interleaves natural-language reasoning and explicit actions/tool-use; commonly implemented by appending thoughts/actions to the prompt across turns.",
            "model_name": null,
            "model_description": "Prompting methodology rather than a single LM.",
            "task_name": "General multi-turn tool-using agent tasks (background)",
            "task_description": "Interactive tasks where the agent alternates reasoning and environment actions, building up context over turns.",
            "task_type": "multi-turn reasoning; tool-using agents",
            "memory_used": true,
            "memory_type": "scratchpad / chain-of-thought stored in prompt",
            "memory_mechanism": "Prompt concatenation of prior thoughts/actions/observations each turn (explicit chain-of-thought tokens remain in the prompt).",
            "memory_representation": "Intermediate thoughts (&lt;think&gt;), actions, observations appended to context.",
            "memory_retrieval_method": "Prompt concatenation and attention over history",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited to motivate problem: existing agents that continuously append thoughts/actions suffer unbounded context growth, motivating MEM1's consolidation approach.",
            "key_findings": "ReAct-style append-to-context is effective for short-horizon tasks but leads to unbounded memory growth and degraded performance for long-horizon interactions.",
            "limitations_or_challenges": "Not scalable: context grows unboundedly, incurring O(N^2) compute (or O(N) with KV caching) and impairing performance on out-of-distribution long contexts.",
            "uuid": "e8232.7",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Agent-FLAN / Agent-R / AgentLM (WebShop baselines)",
            "name_full": "Agent-FLAN / Agent-R / AgentLM (various agent baselines for WebShop)",
            "brief_description": "Agent-FLAN, Agent-R, and AgentLM are prior web-agent baselines evaluated on WebShop; they represent different training/data choices and memory handling strategies (mostly full-context accumulation or specialized training).",
            "citation_title": "Agent-flan: Designing data and methods of effective agent tuning for large language models.",
            "mention_or_use": "use",
            "agent_name": "Agent-FLAN / Agent-R / AgentLM",
            "agent_description": "Agent-FLAN: agent tuning methodology (Agent-FLAN-7B reported). Agent-R: reflection/self-training agent (Agent-R-8B). AgentLM: other agent baselines (7B/13B) used in WebShop comparisons. These models are compared with MEM1-WebShop for reward and efficiency.",
            "model_name": "Agent-FLAN-7B, Agent-R-8B, AgentLM-7B/13B",
            "model_description": "Various instruct/agent-tuned LMs of sizes 7B–13B reported in Table 2; their memory usage and training details are from cited works.",
            "task_name": "WebShop web navigation",
            "task_description": "Browse an online shop iteratively and select/buy products matching a natural-language description.",
            "task_type": "web navigation; multi-step interaction",
            "memory_used": true,
            "memory_type": "typically full-context accumulation or agent-specific state",
            "memory_mechanism": "Mostly appending past interactions and retrieved page content to the prompt; Agent-R uses iterative self-training/reflection mechanisms described in its paper; AgentLM specifics not detailed in MEM1 paper.",
            "memory_representation": "Past page states, clicks, intermediate reasoning preserved in prompt or internal agent mechanisms depending on the baseline.",
            "memory_retrieval_method": "Prompt concatenation / agent-specific mechanisms",
            "performance_with_memory": "Table 2 reports: Agent-FLAN-7B Avg Final Reward=40.35 (Peak Token ≈3.37×10^3, Inference Time ≈9.95s); Agent-R-8B and AgentLM variants report higher final rewards in some configurations but with larger token/dependency costs. MEM1-WebShop achieves Avg Final Reward=70.87 with much lower peak tokens (≈0.81×10^3) and faster inference (≈2.61s).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "MEM1-WebShop substantially outperforms Agent-FLAN in reward and is more efficient than other agent baselines in peak token usage and inference time (see Table 2).",
            "key_findings": "On WebShop, MEM1 achieves top final reward while being more memory- and time-efficient than several agent baselines, demonstrating that learned consolidation generalizes beyond QA to web navigation.",
            "limitations_or_challenges": "Some baseline details (e.g., precise AgentLM citation) not provided in-paper; baselines may be closed-source making direct comparisons dependent on reported numbers.",
            "uuid": "e8232.8",
            "source_info": {
                "paper_title": "MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A-mem: Agentic memory for llm agents.",
            "rating": 2,
            "sanitized_title": "amem_agentic_memory_for_llm_agents"
        },
        {
            "paper_title": "Search-r1: Training llms to reason and leverage search engines with reinforcement learning.",
            "rating": 2,
            "sanitized_title": "searchr1_training_llms_to_reason_and_leverage_search_engines_with_reinforcement_learning"
        },
        {
            "paper_title": "Deepresearcher: Scaling deep research via reinforcement learning in real-world environments.",
            "rating": 2,
            "sanitized_title": "deepresearcher_scaling_deep_research_via_reinforcement_learning_in_realworld_environments"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models.",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Agent-flan: Designing data and methods of effective agent tuning for large language models.",
            "rating": 1,
            "sanitized_title": "agentflan_designing_data_and_methods_of_effective_agent_tuning_for_large_language_models"
        }
    ],
    "cost": 0.021363249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents
17 Jul 2025</p>
<p>Zijian Zhou zhou_zijian@u.nus.edu 
Ao Qu 
Zhaoxuan Wu 
Singapore-MIT Alliance for Research and Technology Centre</p>
<p>Sunghwan Kim 
Yonsei University</p>
<p>Alok Prakash 
Singapore-MIT Alliance for Research and Technology Centre</p>
<p>Daniela Rus 
Jinhua Zhao 
Bryan Kian 
Hsiang Low 
Paul Pu Liang 
MIT</p>
<p>National University of Singapore</p>
<p>MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents
17 Jul 2025162DEA5E72606ADB2C11574CEE427960arXiv:2506.15841v2[cs.CL]
Modern language agents must operate over long-horizon, multi-turn interactions, where they retrieve external information, adapt to observations, and answer interdependent queries.Yet, most LLM systems rely on full-context prompting, appending all past turns regardless of their relevance.This leads to unbounded memory growth, increased computational costs, and degraded reasoning performance on out-of-distribution input lengths.We introduce MEM1, an end-to-end reinforcement learning framework that enables agents to operate with constant memory across long multi-turn tasks.At each turn, MEM1 updates a compact shared internal state that jointly supports memory consolidation and reasoning.This state integrates prior memory with new observations from the environment while strategically discarding irrelevant or redundant information.To support training in more realistic and compositional settings, we propose a simple yet effective and scalable approach to constructing multi-turn environments by composing existing datasets into arbitrarily complex task sequences.Experiments across three domains, including internal retrieval QA, open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves performance by 3.5× while reducing memory usage by 3.7× compared to Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes beyond the training horizon.Our results demonstrate the promise of reasoning-driven memory consolidation as a scalable alternative to existing solutions for training long-horizon interactive agents, where both efficiency and performance are optimized.Code can be found at https://github.com/MIT-MI/MEM1.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown remarkable performance in single-turn tasks such as question answering, summarization, and code generation [7,51,3].However, emerging realworld applications increasingly operate over multiple turns-searching documents, interacting with environments [70], and making decisions based on evolving external information [53].Examples include research agents such as OpenAI and Gemini Deep Research [37,18] that automate complex tasks by iteratively gathering information, and web-navigation agents such as OpenManus [50] and BrowserUse [34], which must complete goals across dozens of interactive turns.</p>
<p>Unlike traditional tasks where the input is static or self-contained, long-horizon settings often involve answering a sequence of related questions, requiring the agent to continuously retrieve new Figure 1: Comparison of memory management between MEM1 and existing reasoning agents.While existing agents for long-horizon tasks [24,64,69] continuously append thoughts (typically enclosed in <think></think>), actions, and observations, resulting in an ever-growing context, our MEM1 agent learns to keep updating an internal state (enclosed in <IS></IS>) that blends thought and memory, discarding the contents from previous steps to achieve constant memory usage during the task.On the other hand, while existing environments and datasets focus on single-objective tasks, our task augmentation method effectively scales up these tasks to enable long-horizon agent training.information, revise beliefs, and adapt to evolving contexts over time.For instance, consider a research assistant tasked with "What's the evidence for X?".Subsequent queries like "Who published it?"require further information retrieval, while "Is the source credible?"calls for self-reflection and assessment.Each query builds on the previously collected and accumulated information.Similarly, a shopping assistant may be first asked "Which product is cheapest?",then "What are its reviews?", and "Is it compatible with my device?".These interactions span multiple turns, featuring evolving contexts and compound reasoning.</p>
<p>In systems designed for long-horizon settings, a common approach is to append all past observations, actions, and thoughts to the context at every turn [55,61].This forces the model to operate with an unboundedly growing context, which introduces three key challenges.(1) Growing inference cost and memory usage.Transformer-based LLMs typically incur O(N 2 ) compute cost (or O(N ) with Key-Value caching) and O(N ) memory usage as the context length N increases [52].Consequently, deploying these models requires reserving large GPU memory on modern inference frameworks to accommodate the growing context [27,68], often leading to significant wastage of computing resources.(2) Generalization limits beyond the training horizon.Ongoing conversations with context length exceeding that in the training data become out-of-distribution for the model.The model struggles to manage and reason over such unfamiliar long-horizon inputs [63].(3) Overloaded and inefficient context.The accumulation of irrelevant or redundant content dilutes the model's attention.This reduces its ability to reason effectively, even when relevant information is technically still present within the prompt [2,30,56].</p>
<p>Recent progress in long-context modeling largely targets static inputs (e.g., long documents) and does not address multi-turn interaction with external environments [6,19].Some other approaches introduce external memory modules (e.g., summarizers or retrievers) [63,29,13,57], but these are typically trained separately and cannot be optimized end-to-end with the agent's policy.This also introduces additional engineering overhead, as engineers must manage and integrate two separate models.Meanwhile, existing works on tool-using agent systems trained with reinforcement learning leave memory management unsolved, letting the prompt length grow unboundedly [24,69].A natural question is raised: Can a language model learn to consolidate its memory as part of its reasoning process so that it retains only what is essential for solving the task?</p>
<p>Motivated by this question, we present MEM1:</p>
<p>Memory-Efficient Mechanism via learning 1-step integrated reasoning and consolidation-a method for training LLM agents that maintain constant memory usage across arbitrarily long horizons.As illustrated in Fig. 1, at each turn, the model updates a consolidated state composed of prior memory and newly obtained information.This consolidated state becomes the agent's only retained memory, allowing all external tool outputs to be discarded after use, which prevents prompt expansion altogether as illustrated by Fig. 2. A key insight of our method is that inference-time reasoning [55,15,33,65] serves a dual function: it not only provides deeper insight into the current query but also acts as a form of "working memory" [4], extracting key components from gathered information to build an evolving understanding.By unifying reasoning and memory consolidation, MEM1 offers an elegant solution where the agent learns to reason and remember within a shared representational space without requiring additional modules or architectural changes.</p>
<p>We train this behavior end-to-end with reinforcement learning (RL) [48,71], optimizing for task success via verifiable rewards [43].Although not explicitly optimized for memory efficiency through reward signals, the agent learns to manage memory as part of its policy, resulting in near-constant memory usage across long horizons.Additionally, we notice that current training and evaluation environments predominantly focus on single-objective tasks [26,59,38], limiting their ability to fully prepare agents for realistic, long-horizon scenarios that inherently involve multiple sequential objectives.To address this challenge, we introduce a scalable task augmentation approach, transforming existing single-objective QA datasets into complex multi-objective tasks through compositions of N multi-hop questions.This approach enables us to repurpose standard benchmarks in our community to more effectively train and evaluate agents on long-horizon reasoning, an increasingly important capability in real-world applications.</p>
<p>To evaluate our method comprehensively, we employ diverse multi-turn environments, including internal retrieval-augmented QA [26,59], open-domain Web QA [69], and complex multi-turn agent shopping scenarios in WebShop [60].Across these scenarios, MEM1 consistently rivals the performance of leading baselines while delivering efficiency gains up to 3.5× in memory usage.Moreover, agents trained on our augmented 2-objective compositions generalize effectively to tasks involving up to 16-objective compositions.Notably, at the 16-objective level, our MEM1 achieves superior accuracy compared to all baseline methods, along with 1.27× lower peak memory usage and 1.78× faster inference compared to the respective best uncollapsed baseline.</p>
<p>Related Work</p>
<p>LLM agents in multi-turn environment.LLM-based agents have evolved from handling singleturn queries to serving as autonomous agents capable of multi-turn interactions such as web navigation [60,70] and complex research [69].To enable such capabilities, Yao et al. [61] introduced the ReAct (i.e., Reason + Act) framework, which enhances LLMs' ability to interact with external environments by interleaving reasoning and action.Building on this reasoning-acting prompting paradigm, subsequent works have explored ways to improve agent performance through natural language feedback, enabling iterative refinement [45,32].Recently, inference-time scaling has emerged as a promising direction for enabling complex reasoning, with prior research incorporating evaluators (e.g., verifier, reward model) [46,31] or world models [9].In addition, there are two major lines of training approaches: (1) behavior cloning (BC), which involves imitating expert trajectories to guide agent behavior by supervised fine-tuning (SFT) [62,16,12], and (2) reinforcement learning (RL), which optimizes agent policies by incentivizing desirable outcomes through rewards [47,5,39].These methods aim to align the agents' behaviors with task objectives, enabling more robust and generalizable performance.</p>
<p>Memory management for LLM agents.A widely adopted approach to memory management in LLM-based agent systems involves appending all prior information, such as observations, intermediate thoughts, and actions, into the prompt at each interaction turn [61].While this method is straightforward and effective when the number of interactions required is small, it results in unbounded context growth, leading to linearly scaled inference memory.Moreover, long contexts often contain irrelevant or redundant information, which impairs the model's reasoning capabilities [2,30,56].To mitigate these issues, recent studies have proposed external memory frameworks, including retrieval-augmented generation and summarization modules [63,29,13,57].However, these methods are typically trained or used independently of the agent's policy, creating a disconnect between memory and the reasoning process.In addition, managing and integrating such modules often incurs extra computational overhead and system complexity.Despite these advancements, many RL approaches for training LLM agents still rely on accumulating the full interaction history as memory [24,69,39], leaving memory management during training an underexplored area.In this work, we seek to bridge this gap by tightly integrating memory with the agent's reasoning process, thereby enabling more efficient and context-aware decision-making.</p>
<p>MEM1</p>
<p>Complex reasoning tasks often require an iterative process of information gathering and synthesis, as seen in applications such as "deep research" [37,23] and web-based agents [35,20].Recent advances in agent design involve interaction loops that interleave chain-of-thought reasoning [55,15], environment interaction, and real-world feedback collection.To explicitly capture these core elements, we annotate each component using XML-style tags: <IS> for internal state (reasoning), <query> for environment queries, <answer> for the agent's responses, and <info> for external observations or tool outputs.MEM1 adopts a learned approach to iterative state updating and consolidation, ensuring that only the most recent set of <IS>, <query>, <answer>, and <info> elements is retained in the prompt at any given time.This design maintains a bounded and semantically relevant context, promoting efficient and coherent multi-step reasoning.</p>
<p>Memory as Part of Reasoning</p>
<p>To achieve a constant memory, MEM1 is particularly trained to iteratively refine its understanding by processing new information in conjunction with a consolidation of its prior state.At each turn t, the agent produces a new <IS_t> element, which summarizes past information and reasons about subsequent actions.Following this, the agent generates an action-either a <query_t> to interact with the environment, or an <answer_t> if a direct response is warranted.If the agent issues a <query_t>, the corresponding feedback from the environment is appended as <info_t>.At the next turn, t + 1, the agent consolidates the tuple <IS_t>, <query_t>, <info_t> into a new <IS_(t+1)>, which serves as the basis for further interactions.After each turn, all tags from the previous turn t are pruned from the context, effectively compressing memory and preventing prompt bloat.Fig. 3 (bottom left) illustrates the evolution of the model's context over time.At any given turn, the agent retains at most two <IS> elements, two <query> elements, and one <info> element, ensuring bounded and efficient memory usage.The detailed rollout algorithm is in Alg. 1 of App.A.5.</p>
<p>RL offers a powerful mechanism for shaping agent behavior through reward signals [49].In MEM1, we leverage this framework to incentivize effective state consolidation by designing environments in which the agent is rewarded only when it strategically retains and integrates useful information.Specifically, we construct tasks that require numerous interactions with the environment to arrive at a correct answer (see Sec. 3.3).Success depends on the agent's ability to rely on information collected along the inference path.At each turn, we prune the agent's context to retain only the most recent <IS>, forcing the agent to perform memory consolidation as part of its reasoning process.Without access to full historical context, the agent must learn to preserve and update relevant knowledge internally in order to reap the reward.This learning procedure mirrors how humans  The mask is applied during the forward pass to compute action log-probabilities for the actor model and state value estimates for the critic model.During the policy update stage, the information mask is then applied to the full trajectory, masking out tokens that were not generated by the model itself.</p>
<p>cultivate memorization skills through structured tasks such as Sudoku or crosswords [1], where success hinges on selectively attending to key information and building upon it.Over time, such tasks help individuals develop cognitive strategies that jointly support efficient memorization and reasoning, similar to our RL method for training MEM1.</p>
<p>Masked Trajectory for Policy Optimization</p>
<p>In the previous section, we detailed the rollout process of our RL pipeline.However, unlike conventional multi-turn agents that preserve a static context during generation, MEM1 introduces a unique challenge: its mechanism continuously updates the context at each turn by consolidating prior memory and pruning irrelevant tokens.This dynamic context update disrupts the continuity of the token generation trajectory, complicating the estimation of token-wise advantages in policy optimization algorithms such as PPO and Reinforce++ [41,22], where trajectories are typically assumed to be linear.</p>
<p>To address this, we introduce a masked trajectory approach that reconstructs a logically coherent full trajectory by stitching together multiple interaction turns with evolving contexts.This unified trajectory mimics a standard multi-turn rollout and comprises a sequence of tuples
τ t = (<IS_t>, <query_t>, <info_t>) for t ∈ [1, T − 1],
where T denotes the total number of interaction turns.The T -th turn outputs the final answer τ T = (<IS_t>, <answer_t>).The full trajectory encodes all information needed for accurate policy learning while respecting MEM1's memory consolidation at each turn.Fig. 3 (bottom left) demonstrates the evolution of the agent's context.</p>
<p>To ensure that policy gradients are correctly computed under this consolidated memory regime, we apply a two-dimensional attention mask [40] across the full trajectory.This mask restricts each token's attention to only the tokens retained in memory at the time that token was generated.Specifically, for a token position k, we mask out all prior tokens that are not part of the consolidated memory corresponding to the context at that turn.This masking strategy enables accurate computation of the policy objective: letting s k denote the masked input state and a k the predicted token, the logprobability ratio ρ k (θ) = π θ (a k |s k ) π θ old (a k |s k ) remains valid and tractable, in turn ensuring that the advantage, KL penalty, and value estimation are correct.A visualization is in Fig. 3 (bottom right).Furthermore, following [24], we incorporate a one-dimensional attention mask over retrieved external information during policy updates for both the actor and critic networks.This ensures that gradient updates are localized to only tokens generated by the agent.Fig. 3 (bottom right) shows the masking mechanism that enables stable and accurate policy optimization under MEM1's memory-constrained execution.</p>
<p>Multi-Objective Task Design</p>
<p>Although our proposed method is designed to address the critical challenges of agentic multiturn interaction with the external world, there are limited publicly available datasets that support training for such long-horizon interactive processes.Existing benchmarks, such as HotpotQA [59], Bamboogle [38], and 2wiki [21], are often cited as multi-hop benchmarks, yet they typically involve only two information-seeking steps.Moreover, these datasets are not explicitly structured to support long-horizon interactions that necessitate the agent to manage the memory state.</p>
<p>To bridge this gap, we introduce a novel task-multi-objective question &amp; answering (QA)-that extends the number of reasoning steps required to solve a problem.Building on existing multi-turn datasets such as HotpotQA and Natural Question [59,26], we interleave multiple questions from the original QA corpus and construct a single composite query that requires answering all constituent subquestions, shown in Prompt 1 of App.A.3.This formulation compels the agent to perform multiple search queries, each targeting a distinct sub-question or sub-objective, and then integrate the retrieved answers to form a comprehensive final response.Compared to the original tasks, our synthesized multi-objective, multi-hop setting significantly increases the number of search and reasoning turns required, leading to more complex, memory-intensive interactions.</p>
<p>Experiments &amp; Results</p>
<p>We empirically demonstrate the effectiveness of our approach in training the MEM1 agent to perform multi-turn tasks while preserving a near-constant-sized memory state.We evaluate MEM1 against several baselines using a comprehensive set of metrics categorized into accuracy (e.g., Exact Match, F1 score, Environment Reward) and efficiency (e.g., Peak Token Usage, Dependency Length, Inference Time).All MEM1 variants are fine-tuned from the Qwen2.5-7BBase model [58].We use PPO [41] as the RL algorithm as it computes token-level advantages, bringing stability to the training process.While we also experimented with instruction-tuned and supervised fine-tuned models using curated high-quality trajectories, reinforcement learning from the base model consistently yielded the best performance and generalization.</p>
<p>Our experiments are conducted in two standard environments, each reflecting real-world scenarios that require multi-turn agent interactions.The first environment is question answering with retrievalaugmented generation (RAG) [26,59], where the agent must answer queries by retrieving relevant information from an external knowledge store (either a database or an online search engine).We trained on RAG with a local database (i.e., Wikipedia Corpus) and evaluated on tasks involving open web browsing.For QA, following Sec.3.3, we construct multi-objective tasks and tested the model performance on tasks with more questions than seen in the training.The second environment is WebShop navigation [60], where the agent assists users in online shopping by browsing a website and selecting items based on natural language descriptions.This task requires the agent to iteratively read page content and make navigation decisions, following protocols similar to those in WebGPT [35].</p>
<p>Implementation Details</p>
<p>Datasets and evaluation metrics.We train two versions of MEM1 agent for both long-horizon QA and web navigation.For long-horizon QA, we augment the multi-hop QA dataset from [24] that mixes data from both HotpotQA [59] and Natural Question [26] to form a 2-objective composite task.</p>
<p>For the web agent, we use the WebShop environment [60], which also produces a reward during training [64].For all datasets, the train-test split follows the original papers.During RL training, we employ the exact match (EM) metric for QA tasks (details in App.A.4.1) and the environment reward for WebShop [60,64].To evaluate the effectiveness of various approaches, we measure the EM and F1 score for QA tasks and final reward for the WebShop environment [60,64].To evaluate the efficiency, we consider the peak token usage, average dependency, and average inference time.The test datasets are obtained from the original papers which consist of out-of-distribution data.The former two metrics measure the memory efficiency, while the latter measures the time efficiency.</p>
<p>The detailed definitions of the metrics are in App.A.4.1.The prompt and format can be found in App.A.3.</p>
<p>Baselines.To evaluate the accuracy and efficiency of MEM1, we compare it against a diverse set of baselines designed to either enhance task performance or manage context effectively.For the QA environment, we benchmark accuracy against Search-R1 [24], DeepResearcher [69], and a larger-scale model, Qwen2.5-14B-Instruct[58].Details about Search-R1 and DeepResearcher can be found in App.A.4.2.For the WebShop environment, we compare against Agent-FLAN [11], Agent-R [64], and AgentLM [66].To assess efficiency, we consider two context compression baselines using models of the same parameter size as MEM1.First, we apply MEM1's agentic truncation prompt template and rollout to a standard instruct model, isolating the benefits of prompt and rollout design alone.Second, we evaluate A-MEM [57], which augments an Instruct model with a vector database for memory retrieval, capturing the effect of external memory modules in agentic systems.We additionally train a supervised fine-tuned (SFT) model using trajectories curated from GPT-4o [36] based on MEM1's rollout and compare it with the RL-trained agent.</p>
<p>Meta info injection.In our agentic pipeline, the agent's context is programmatically truncated at each turn-immediately after it generates a search query or an answer-following the procedure outlined in Sec. 3. As past context is truncated, the agent may have difficulty determining when to terminate.To address this, we prepend a hint [HINT: YOU HAVE {turns_left} TURNS LEFT] at the beginning of each <info> tag to remind the agent of its remaining turns budget.For all experiments, we set the maximally allowed turns to 6 for 1-objective to 4-objective tasks and 20 for more difficult tasks to avoid excessively long trajectories.</p>
<p>MEM1 on Multi-Objective Multi-Hop Tasks</p>
<p>One key advantage of MEM1 agents lies in their efficient management of long-horizon interactions with the environment.To demonstrate this, we train our MEM1 agent with a 2-objective augmentation of the QA dataset, and subsequently test it against other models, using held-out multi-objective test datasets similarly augmented from the original test datasets.As elaborated in Sec.3.3, these multiobjective tasks require a significantly larger number of turns of environment interactions to complete, hence serving as better benchmarks for memory management.As shown in Tab. 1, when evaluated on 2-objective datasets, MEM1 achieves better performance (in terms of EM and F1 scores) than other 7B counterparts, while incurring significantly lower peak token usage and achieving faster inference time.</p>
<p>The advantage of MEM1 becomes even more evident in tasks requiring longer-horizon interactive processes.To highlight such scalability of MEM1, we further compare the models on 3, 4, 6, 8, and 16-objective tasks in Fig. 4 and Tab. 1. Fig. 4 illustrates the scaling trends of task performance (measured by EM count) and memory efficiency (measured by Peak Token Usage) for MEM1 relative to other models and memory management baselines.As the number of objectives increases, the Peak Token Usage of all other methods and models scales nearly linearly.In contrast, MEM1 maintains an almost constant peak token count with only a slight increase, as also shown in Tab. 1.</p>
<p>Notably, while MEM1 initially underperforms Qwen2.5-14B-Instruct, its performance gradually catches up as the number of objectives increases, eventually surpassing the 14B model, which has double the parameter count.MEM1 also demonstrates remarkable efficiency.In the 16-objective task, it requires only 27.1% of the peak tokens and 29.3% of the total inference time compared to Qwen2.5-14B-Instruct.This efficiency translates to significantly reduced GPU memory requirements and overall computing resource demands.</p>
<p>MEM1 on Single-Objective Multi-Hop Tasks</p>
<p>While MEM1 is designed to train agents for very long-horizon tasks, our training method also delivers improved capability with existing multi-hop tasks while achieving much greater efficiency at the same time, all without being explicitly trained on the single-objective versions of these tasks.Note that single-objective tasks also require multiple turns of interaction to produce the desired output.</p>
<p>Long-horizon web navigation in WebShop.Beyond QA tasks, we further evaluate the effectiveness of MEM1 in managing long-horizon interactions in the form of web navigation.We show   Single-objective QA in Wikipedia.Tab. 3 presents the accuracy and efficiency metrics for evaluations on single-objective QA tasks on Wikipedia [24], where the agent can make retrieval requests from the Wikipedia datastore via RAG.The MEM1 used in this evaluation is the same as the one detailed in Sec.4.2, which is trained solely on a 2-objective task.Overall, MEM1 demonstrates superior efficiency across all three evaluated efficiency metrics, while simultaneously achieving the highest EM score and an F1 score comparable to that of Qwen2.5-14B-Instruct.This improvement in efficiency is attributed to the MEM1 agent's ability to consolidate memory from previous interactions into a compact internal state, which reduces the number of tokens used in the context.We also observe that SFT significantly underperforms RL, highlighting the necessity for RL-based training.
EM ↑ F1 ↑ Peak (×10 2 ) ↓ Time (s) ↓ EM ↑ F1 ↑ Peak (×10 2 ) ↓ Time (s) ↓ EM ↑ F1 ↑ Peak (×10 2 ) ↓ Time (s) ↓ Qwen2.
Zero-shot transfer to Online Web-QA.To validate the transferability and generalizability of the trained MEM1 agent, we perform a zero-shot transfer to an online web-QA environment, which is unseen by the agent.In this environment, agents conduct web searches through an API service that returns results including titles, snippets, and URLs.As shown in Tab. 3, MEM1 consistently exhibited improved efficiency alongside comparable effectiveness in this unseen setting via zero-shot transfer.</p>
<p>Analysis on Emergent Agent Behaviors</p>
<p>Through analyzing MEM1's multi-turn interaction traces trained on 2-objective QA, we observe a range of emergent behaviors that are critical for handling long-horizon, multi-objective tasks, demonstrating capabilities well beyond simple retrieval.First, MEM1 learns to manage multiple questions concurrently by maintaining a structured internal state.As shown in Fig. 5(a), when faced with two multi-hop questions, the agent stores and updates memory for each question separately, guiding subsequent searches based on the identified information gaps.In (b), MEM1 exhibits the ability to shift focus when progress on one question stalls, recognizing difficulty and prioritizing the more tractable objective.Meanwhile, MEM1 learns to interleave reasoning and memory in its internal state <IS>, weaving important information into its decision-making process to support both information retention and action selection.In Fig. 5 (c), MEM1 explicitly extracts important information from previous search results and leverages it to formulate the next query that best addresses the current information gap.In addition, (d) shows that when new, relevant information is retrieved, MEM1 explicitly reasons about its significance and selectively updates its memory.We believe that learning these interleaved behaviors is key to achieving efficiency gains in memory without degrading performance.Beyond behaviors unique to our multi-objective setup and memory architecture, MEM1 also exhibits several general-purpose search strategies.In (e), the agent performs self-verification, correcting an earlier misconception and issuing a new query for confirmation.In (f), complex queries are decomposed into manageable subgoals before initiating the search.In (g), for questions requiring multi-turn information gathering, MEM1 extracts key information from search results and uses it to inform the next search.In (h), when overly specific queries fail, MEM1 re-scopes its query to improve retrieval.Notably, many of these behaviors, including verification, making a plan, and iterative search, are also reported in recent studies on deep research agents [24,69].</p>
<p>Conclusion, Limitations, and Future Work</p>
<p>We introduced MEM1, a reinforcement learning framework that enables language agents to perform long-horizon reasoning with consolidated memory.By integrating inference-time reasoning and memory consolidation into a unified internal state, MEM1 addresses the scalability challenges of prompt growth and achieves competitive performance across QA and web navigation benchmarks, with substantially reduced memory usage and inference latency.Despite these advantages, MEM1 assumes access to environments with well-defined and verifiable rewards.While this assumption holds in domains such as QA, math, and web navigation, many open-ended tasks present ambiguous or noisy reward structures.Fully realizing the potential of MEM1 therefore requires advances in modeling such tasks and designing suitable reward mechanisms-challenges that lie beyond the scope of this work.A promising future direction is to explore methods for training MEM1 agents in these open-ended settings where reward signals are sparse, delayed, or implicit.</p>
<p>A Details of MEM1</p>
<p>A.</p>
<p>Computing Resources and Training Details</p>
<p>All trainings of MEM1 are conducted on 4 H100 or H200 GPUs.We use the veRL framework [44] for RL and Swift [14] for SFT.For RL, both the data batch size and mini batch size are set to 64.</p>
<p>Learning rate is set to 10 −6 for the actor model and 10 −5 for the critic model with a linear warmup of 50 steps.Temperature is set to 1 during training and 0.01 during inference.</p>
<p>All evaluations are conducted on a single H200 GPU, which serves the respective models as an API service using the vLLM framework [27] with automatic prefix caching enabled.</p>
<p>A.2 RAG Configuration</p>
<p>For RAG on local Wiki corpus, we use Faiss-GPU [17] serving an E5 Base model [54].The Wiki corpus is taken from a Wikipedia 2018 dump [25].The number of passages for each retrieval is set to 3 for a fair comparison with other methods.</p>
<p>For online web search queries, we use Serper API [42], which offers Google search results including titles, snippets, and URLs.For each search, we return the top 10 results to the agent as external information.We do not ask the agent to retrieve the content of specific webpages.</p>
<p>A.3 Prompts</p>
<p>Prompt 1: Multi-Objective Task (QA)</p>
<p>You will answer multiple complex questions using iterative reasoning, summarization, and web search.</p>
<p>At each step, you will see the questions, a cumulative summary of relevant information, the current search query, and search results (except in the first step, where only the questions are provided).Your task is to:</p>
<ol>
<li>Perform reasoning and update a cumulative, concise summary within <think> ... </think>.This acts as persistent memory and must include all essential information from previous <think> and <information> tags.</li>
</ol>
<p>Then choose one of the following actions:</p>
<p>-If any question remains unanswered, issue a single query for one question inside <search> ... </search>.The query should consist of keywords or a short phrase.Only search one question at a time.</p>
<p>-If all questions are answered, provide the final answers-separated by semicolons-within <answer> answer1; answer2; ... </answer>.The answers must be concise, contain only essential words, and avoid any explanations.</p>
<p>Important:</p>
<p>-Always follow this structure after <information> or the initial questions: <think> ... </think><search> ... </search> or <think> ... </think><answer> ... </answer>.</p>
<p>-Do not search multiple queries or questions simultaneously.</p>
<p>Answer the following questions:[QUESTIONS]</p>
<p>Prompt 2: Single-Objective Task (QA)</p>
<p>You will answer a complex question through iterative reasoning, summarization, and web searches.</p>
<p>At each step, you can see the question, previous summary in <think> ... </think>, search query in <search> ... </search>, and the returned information in <information> ... </information> (except the first step where you will be given only the question).Then, you should:</p>
<ol>
<li>
<p>Conduct reasoning, and then update a concise, cumulative summary with essential information inside <think> </think>.This is your persistent memory and should include all important information from previous <think> </think> and <information> </information> (i.e.information and answers already found for questions).</p>
</li>
<li>
<p>Then choose one: -Issue a query (i.e., key words / phrases for search) inside <search> </search> (you may search repeatedly until the answer is clear).This query will be used to conduct search and return the results in <information> results </information> -Provide the final concise answer (no explanations) if no additional information is needed inside <answer> </answer>.The answer should be concise and only contain the words necessary to answer the question.</p>
</li>
</ol>
<p>After <information> </information> (or question at the beginning), you should always follow the order: <think> ... </think><search> ... </search> or <think> ... </think><answer> ... </answer>.</p>
<p>Question: [QUESTION]</p>
<p>Prompt 3: Single-Objective Task (WebShop)</p>
<p>You are browsing an online shop.Your goal is to find a product that matches the given description.You will interact with the site step-by-step.F1 score.The F1 score computes the harmonic mean between the precision p and recall r.In the case of string matching, we split both the predicted answer and the ground truth.For example, if the ground truth is "United States of America", it is split into a list with lower-case words: "united", "states", "of", "america".The same works for the predicted answer.Then, denote the number of common words as c.Further denote the number of words in the predicted answer as l and the number of words in the ground truth as g.Then, precision is calculated as p := c/l and recall is calculated as r := c/g.The F1 score is finally computed as
F1 := 2 × p × r p + r .
If multiple ground truths are present, the maximum of all F1 scores is chosen.For multi-objective tasks, the final F1 is the sum of the F1 scores for each sub-question.</p>
<p>Peak token usage.Peak token usage is calculated as the maximum number of tokens (using GPT-4o-mini tokenizer) in any single sequence throughout the agent's entire trajectory.For fair comparison in our experiments, the system prompt is excluded when computing this sequence length.The peak token usage serves as a proxy for the inference-time memory requirement.</p>
<p>Dependency length.Following [67], the dependency metric is defined as the total number of historical tokens on which each generated token effectively depends.Let T denote the total number of interaction steps.For each step i ∈ [T ], let n (i) p be the number of prefix tokens and n
(i)
o be the number of output tokens generated.The dependency metric is then calculated as
Dependency := i∈[T ] (2n (i) o + n (i) p ) × n (i) o2
.</p>
<p>At a high level, this metric quantifies the cumulative computational cost associated with the generation of an output trajectory.It is important to note that in MEM1, prefix tokens from previous steps are consolidated into a new internal state, rather than being continuously accumulated.In our experiments, we ignore the tokens in the system prompt when calculating the dependency metric.</p>
<p>Inference time.Inference time for each trajectory is recorded as the total elapsed time required to generate the complete output trajectory.For all experiments, these measurements are conducted on a single H200 GPU, operating with 10 concurrent threads.The vLLM inference framework is utilized, with its automatic prefix caching feature enabled.</p>
<p>A.4.2 Baselines</p>
<p>Search-R1.As detailed in [24], the model is trained on the 1-objective task with the same dataset as MEM1.Search-R1 also uses exact match as its reward function.In comparison, MEM1 is trained exclusively on 2-objective tasks.</p>
<p>Deep Researcher.As detailed in [69], the model is trained on 1-objective task with a curated set from various QA datasets including HotPotQA and Natural Questions.Deep Researcher adopts the F1 score as the reward function.</p>
<p>A.5 Algorithm</p>
<p>We provide an outline of the rollout of MEM1, which actively manages its context in Alg. 1. Parts of the pseudo-code follow [24].</p>
<p>implementation, effectively, it can be viewed as simply adding white spaces in the training trajectories and has no significant impact on the experimental results.</p>
<p>B Broader Impacts</p>
<p>MEM1 opens up the potential to enable more scalable, efficient, and intelligent AI agents capable of sustaining long, goal-directed interactions in dynamic environments.As AI systems are increasingly deployed in complex real-world tasks-such as scientific research, legal analysis, personalized education, and digital customer service-models must go beyond single-turn capabilities and manage evolving contexts over many steps.MEM1's memory-consolidation mechanism allows language models to maintain high performance without the growing computational and environmental costs typically associated with long-context processing.By reducing inference-time memory and compute demands, MEM1 paves the way for more sustainable and scalable AI deployment, making advanced reasoning agents accessible to a wider range of users and institutions, including those with limited resources.Moreover, MEM1's unified framework of reasoning and context consolidation sets a precedent for future research on intelligence that can learn to adapt, reflect, and summarize information autonomously, inspiring more trustworthy, interpretable, and human-aligned AI systems.We present the training dynamics of the 2-objective QA-trained MEM1 in Fig. 6, where several distinct phases emerge during the learning process.In the initial exploration phase (first 50 steps), the agent demonstrates little task proficiency.The reward remains consistently low, while the entropy loss is high, suggesting random or undirected behavior.The ratio of valid actions hovers around 0.55, indicating that the agent frequently fails to follow the expected output format.During this period, MEM1</p>
<p>C Training Trajectory Analysis of MEM1</p>
<p>has not yet learned to reliably use the required structure involving <query> and <answer> tags.</p>
<p>Shortly after, we observe the onset of format acquisition.The agent gradually improves its structural consistency, reflected in the rising ratio of valid actions.This improved adherence to format correlates with an increase in reward, suggesting that proper formatting directly contributes to the agent's task success.By around step 150, a notable behavioral shift occurs.The number of valid searches begins to drop sharply, while the reward continues to increase.This implies that the agent has discovered a shortcut: by reducing the number of searches-perhaps to avoid format violations-it can maintain high format fidelity and improve its reward without fully solving the task.This short-horizon optimization suggests the agent is exploiting the reward structure, favoring formatting compliance over content completeness.</p>
<p>Between steps 150 and 200, the agent enters a phase of refined format mastery.The ratio of valid actions steadily climbs, but the number of searches remains low.During this phase, reward growth slows, and entropy begins to flatten.The plateau in entropy indicates that the agent is looking for new policies to boost the reward.At this stage, the agent has reached a local optimum: it's producing valid but under-informed answers.</p>
<p>After step 200, a second behavioral shift occurs.The number of valid searches begins to rise again, suggesting that the agent is learning to extend its interaction horizon to gather more information.The agent learns to balance formatting constraints with information acquisition.As a result, the reward increases more sharply.Finally, after step 250, the agent enters a phase of policy consolidation.The entropy loss drops sharply-signaling a transition from exploration to exploitation-as the agent settles into a more deterministic, high-reward policy.By this stage, the agent effectively combines format compliance, sufficient searching, and high-quality answer generation.</p>
<p>D Analysis on Implementation Details</p>
<p>D.2 Format Reward Accelerates Convergence but Degrades Final Performance</p>
<p>It is common to incorporate format reward when training reasoning models and multi-turn reasoning agents [15,69,24].In our study, we experimented with a format reward that enforces the agent to produce outputs using specific structural tags: <IS>, <query>, and <answer>.If the agent fails to use the expected tags correctly, the turn is terminated and a penalty of -1 is applied.</p>
<p>As shown in Fig. 7, using the format reward leads to faster convergence during training but results in worse final performance.The format-constrained agent achieves an exact match score of 0.466, compared to 0.709 for MEM1 trained with only outcome-based reward on the same testing set for the 2-objective QA task.Additionally, the format-constrained agent generates fewer tokens, with an average peak of 514.9 tokens, whereas the outcome-reward-trained MEM1 reaches an average peak of 640 tokens.</p>
<p>Figure 2 :
2
Figure 2: A conceptual comparison of context length between the MEM1 agent and existing reasoning agents when handling long-horizon tasks.Our agent learns to discard the previous context (except for the prompt and initial query) immediately after generating a new internal state and action, resulting in nearconstant memory usage.</p>
<p>Figure 3 :
3
Figure 3: (Top): the RL pipeline used to train MEM1.(Bottom left): The evolution of context in MEM1-old <IS>, <query>, <info> are cleared as new states enter the context.The mechanism is used in the rollout.(Bottom right): the 2D attention mask used during the objective computation stage.The mask is applied during the forward pass to compute action log-probabilities for the actor model and state value estimates for the critic model.During the policy update stage, the information mask is then applied to the full trajectory, masking out tokens that were not generated by the model itself.</p>
<p>Figure 4 :
4
Figure4: Performance and efficiency scaling of MEM1 (trained on 2-objective QA) with the number of objectives in multi-objective tasks.MEM1 outperforms the other models and baselines while having an almost constant scaling in memory usage.Note that at 16-objective, the context of baseline models does not increase anymore since their model performance has degraded (some collapsed).</p>
<p>Figure 5 :
5
Figure 5: Snippets of internal states and actions showing MEM1's Emergent Behaviors in 2-objective QA tasks.Light Blue denotes behaviors related to multi-objective tasks.Beige denotes behaviors related to memory in internal state.Pastel Green denotes behaviors related to general search strategies.</p>
<p>Figure 6 :
6
Figure 6: Metrics of training progresses for MEM1 with RL.</p>
<p>D. 1
1
RL Generalizes Better Than SFTA natural question arises: can Supervised Fine-Tuning (SFT) with high-quality trajectories match the performance of reinforcement learning (RL)?To investigate this, we compare MEM1-QA trained via RL against MEM1-QA (SFT), where both models are trained on the 2-objective QA task.Additionally, the SFT model is further trained on 1-objective and 3-objective QA tasks to enhance its generalization ability.As shown in Tab. 4, the SFT model consistently underperforms compared to its RL counterpart across tasks with varying numbers of questions (objectives).Notably, when the number of objectives exceeds six, the performance of the SFT model collapses, whereas the RL-trained model continues to demonstrate strong robustness and scalability.</p>
<p>Figure 7 :
7
Figure 7: Training curves comparing MEM1 trained with and without format reward.</p>
<p>Table 1 :
1Model2-Objective8-Objective16-Objective
[57]arison of models on multi-objective multi-hop QA tasks.Arrows indicate the desired directions.Numbers in red indicate collapsed model behavior (extremely low performance).(truncate)meansusing MEM1's prompt and rollout pipeline.(A-MEM)meansusing MEM1's prompt and rollout pipeline with A-Mem's external memory module[57].MEM1-QA means MEM1 trained on 2-objective QA task.</p>
<p>Table 2 :
2
The experimental results for WebShop.For a fair comparison, we do not report GPT's inference time.For Agent-R, scores are taken from the original paper, as the model is closed source.MEM1-WebShop means MEM1 trained on WebShop environment.ModelAvg Final Reward ↑ Peak Token (×10 3 ) ↓ Dependency (×10 6 ) ↓ Inference Time Per Traj (s) ↓
GPT-4o25.485.30 ± 1.233.99 ± 1.16N/AGPT-4o (truncate)13.820.99 ± 0.990.81 ± 0.23N/AGPT-4o (A-MEM)24.501.84 ± 0.060.31 ± 0.11N/AQwen2.5-7B-Instruct18.425.64 ± 1.343.38 ± 0.8912.31 ± 1.82Qwen2.5-14B-Instruct12.345.44 ± 0.923.30 ± 0.6118.17 ± 2.32Agent-FLAN-7B40.353.37 ± 1.122.18 ± 1.629.95 ± 6.19Agent-R-8B63.91N/AN/AN/AAgentLM-7B63.602.24 ± 0.400.28 ± 0.073.91 ± 1.07AgentLM-13B70.802.36 ± 0.460.30 ± 0.085.23 ± 1.59MEM1-WebShop70.870.81 ± 0.100.15 ± 0.162.61 ± 0.48</p>
<p>Table 3 :
3
Performance comparison across environments for single-objective tasks.Arrows indicate the desired direction.(SFT) means training with SFT and applying MEM1's prompt and rollout.Note that DeepResearcher is specifically trained on the single-objective Online Web-QA task with F1 score as the optimization objective, and Search-R1 is specifically trained on the single-objective Wiki-RAG task with EM as the objective.
Qwen2.5-7B-Inst (truncate) 0.287 0.3826.28 ± 0.051.65 ± 0.042.26 ± 0.04Qwen2.5-7B-Inst (A-MEM) 0.246 0.3738.47 ± 0.120.92 ± 0.0311.2 ± 0.40Wiki RAGQwen2.5-7B-Inst Qwen2.5-14B-Inst0.269 0.390 0.422 0.5349.32 ± 0.19 8.89 ± 0.211.17 ± 0.04 2.22 ± 0.102.31 ± 0.04 6.73 ± 0.24Search-R10.445 0.51611.0 ± 0.251.50 ± 0.052.23 ± 0.14DeepResearcher0.419 0.50313.3 ± 0.347.04 ± 0.333.86 ± 0.09MEM1-QA (SFT)0.302 0.3586.54 ± 0.053.30 ± 0.134.84 ± 0.21MEM1-QA0.405 0.4715.63 ± 0.030.76 ± 0.023.79 ± 0.07Qwen2.5-7B-Inst0.334 0.4518.37 ± 0.181.39 ± 0.062.20 ± 0.04Online Web-QADeepResearcher0.372 0.49210.27 ± 0.192.86 ± 0.142.87 ± 0.06MEM1-QA0.397 0.4855.79 ± 0.060.44 ± 0.021.84 ± 0.03
EnvironmentSystem EM ↑ F1 ↑ Peak Token (×10 2 ) ↓ Dependency (×10 5 ) ↓ Inference Time ↓</p>
<p>Each step gives you a <state>...</state> representing the current webpage.You must decide what action to take next until you identify the correct product.After you navigate and find the product that best fits the user goal, you should click[buy now] to buy the product at the product page when the buy now button is available.In QA tasks, we use exact match (EM) as both the verifiable reward for the RL pipeline and the evaluation metric for the final output.The final response is extracted from between <answer> and </answer>.In multi-objective settings, the response should contain answers to each question separated by semicolons.If the XML tags are mismatched, or if the number of provided answers does not correspond to the number of questions, a score of 0 is assigned.Otherwise, 1 point is credited for each correct answer.During RL training, we do not provide any other intermediate rewards or format penalties, as we find that such manual interventions can interfere with the agent's learning process (see more in Sec.4.4).
Available actions (shown in the <state> tag) depend on the page:-On the search page: search[<keywords>]-On search result pages: click[<item url>] to view a product, orclick[next &gt;] to go to the next results page-On product pages: click[description], click[features],click[color], click[size], click[buy now]-To return to search: click[back to search]Example goal: "Find a gingko light and 20x20 pillow cover that ishand painted." Example first action: <answer>search[gingko light20x20 pillow cover hand painted]</answer> Only respond with validactions formatted as: search[...], click[...], etc.Product Description: [PRODUCT DESCRIPTION]A.4 Implementation Details of Metrics and BaselinesA.4.1 MetricsExact match.</p>
<p>Table 4 :
410.410 0.3000.110+36.7%20.709 0.4330.276+63.7%30.976 0.6480.328+50.6%41.120 0.6260.494+78.9%61.630 0.0881.542+1752%81.870 0.0271.843+6826%16 1.900 0.0001.900-
Comparison of RL and SFT on increasing number of multi-turn questions.Exact match scores ↑ is better.Gap shows absolute difference.Red numbers show collapsed SFT behavior.#Q RL ↑ SFT ↑ Gap ↑ RL Gain (%) ↑</p>
<p>Algorithm 1 MEM1 RolloutRequire: Task prompt x, policy model π θ , world model W, maximum turn T Ensure: Final response y 1: Initialize rollout sequence y ← ∅ 2: Initialize turn count t ← 0 3: while t &lt; T do 4:Initialize current policy rollout sequence y t ← ∅ 5:while True do Mark the sample as invalid 24:end if25:Increment turn count t ← t + 1 26: end while 27: return final generated response yA.6 MEM1 on Webshop Training DetailsWe use the same rollout pipeline and policy update mechanism for training MEM1 on WebShop.Compared to the QA tasks, we use a tailored prompt that retains the gist of memory consolidation with instructions specific to the WebShop environment, as shown in Prompt.3. Another distinction is that the WebShop environment comes with its own reward function corresponding to each state.Therefore, we do not use exact match but the built-in reward function as the reward signal when training in WebShop environment.The training and test splits also follow the original paper[60], with the first 1000 samples as the test set, the 1000th to 1500th as the val set, and the remaining as the train set.A.7 Additional Discussion on the Attention Matrix Design.We wish to note that our modification to the attention matrix does not fully recover the attention of the original trajectories because of the change in position ids.Specifically, prior works[28,10,8]that utilized the attention matrix to compress multiple trajectories mainly targeted tree-exploration, i.e., generating multiple sequences with the same prefix.For these works, on top of the attention matrix, they adjusted the position ids as well, so each trajectory follows a consecutive increasing position ids.However, in MEM1, the prefix does not remain the same because of memory consolidation.This results in each <IS> having two possible position ids, one for the previous turn and one for the next turn.To completely recover the original attention, we need to duplicate each <IS> and assign different position ids to the two copies.However, such duplication can significantly slow down training because the training trajectories are now much longer.As such, for training efficiency, we do not duplicate the <IS> and assign the position ids for the previous trajectory to each <IS>.While this modification slightly deviates from the "ideal"
surprising-exercises-that-will-sharpen-your-short-term-memory. A Cognitive Connection. January 2024Surprising exercises that will sharpen your shortterm memory</p>
<p>Why does the effective context length of llms fall short?. Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, Lingpeng Kong, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025</p>
<p>The claude 3 model family: Opus, sonnet. Anthropic, 2024</p>
<p>Working memory. Alan D Baddeley, Graham J Hitch, Psychology of learning and motivation. Gordon H Bower, Academic Press19748</p>
<p>Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, Aviral Kumar, Advances in Neural Information Processing Systems (NeurIPS). 202437</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Language models are few-shot learners. Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry, Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Medusa: Simple LLM inference acceleration framework with multiple decoding heads. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D Lee, Deming Chen, Tri Dao, 10.48550/arXiv.2401.10774Proceedings of the 41st International Conference on Machine Learning. the 41st International Conference on Machine LearningVienna, AustriaJul 2024235Proceedings of Machine Learning Research</p>
<p>Hyungjoo Chae, Namyoung Kim, Kai Tzu-Iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, Jinyoung Yeo, arXiv:2410.13232Web agents with world models: Learning and leveraging environment dynamics in web navigation. 2024arXiv preprint</p>
<p>Accelerating large language model decoding with speculative sampling. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, John Jumper, 10.48550/arXiv.2302.01318arXiv:2302.01318February 2023arXiv preprint</p>
<p>Agent-flan: Designing data and methods of effective agent tuning for large language models. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao, Findings of the Association for Computational Linguistics (ACL). 2024</p>
<p>Seeclick: Harnessing gui grounding for advanced visual gui agents. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Li Yantao, Jianbing Zhang, Zhiyong Wu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)2024</p>
<p>Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav, arXiv:2504.19413Mem0: Building production-ready ai agents with scalable long-term memory. 2025arXiv preprint</p>
<p>SWIFT: A scalable lightweight infrastructure for fine-tuning. 2024</p>
<p>Deepseek-Ai , Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z F Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Mind2web: Towards a generalist agent for the web. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems (NeurIPS). 202336</p>
<p>Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, Hervé Jégou, arXiv:2401.08281The faiss library. 2024arXiv preprint</p>
<p>Google, Gemini, Try deep research and gemini 2.0 flash experimental. 2024</p>
<p>When attention sink emerges in language models: An empirical view. Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, Akiko Aizawa, Proceedings of the 28th International Conference on Computational Linguistics (COLING). the 28th International Conference on Computational Linguistics (COLING)2020</p>
<p>Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models. Jian Hu, Jason Klein Liu, Wei Shen, arXiv:2501.032622025. Apr 20253arXiv preprint</p>
<p>Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. Yucheng Jiang, Yijia Shao, Dekun Ma, Sina Semnani, Monica Lam, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Yaser Al-Onaizan, Mohit Bansal, Yun-Nung Chen, the 2024 Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Hansi Bowen Jin, Zhenrui Zeng, Jinsung Yue, Sercan Yoon, Dong Arik, Hamed Wang, Jiawei Zamani, Han, arXiv:2503.095162025arXiv preprint</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Natural questions: A benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, Slav Petrov, 2019Transactions of the Association for Computational Linguistics7</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Fast inference from transformers via speculative decoding. Yaniv Leviathan, Matan Kalman, Yossi Matias, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningHonolulu, Hawaii, USAJul 2023202Proceedings of Machine Learning Research</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Frank Guerin, Chenghua Lin, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, Transactions of the Association for Computational Linguistics. 122024</p>
<p>Inference-time scaling for generalist reward modeling. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, Yu Wu, arXiv:2504.024952025arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems (NeurIPS). 202336</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Magnus Müller, Gregor Žunič, Browser use: Enable ai to control your browser. 2024</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, arXiv:2112.09332Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. 2022arXiv preprint</p>
<p>Gpt-4o system card. 2024OpenAI</p>
<p>Introducing deep research. Openai, February 2025</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, Findings of the Association for Computational Linguistics (EMNLP). 2023</p>
<p>Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, arXiv:2411.02337Training llm web agents via self-evolving online curriculum reinforcement learning. 2024arXiv preprint</p>
<p>. S Ruslan, 2024Hugging Face Community Blog4d masks support in transformers</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Serper, Serper api: Fast and affordable google search api. 2025</p>
<p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y K Li, Y Wu, Daya Guo, 2024</p>
<p>Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, Chuan Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems (NeurIPS). 202336</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>Trial and error: Exploration-based trajectory optimization of llm agents. Yifan Song, Xiang Da Yin, Jie Yue, Sujian Huang, Bill Li, Lin Yuchen, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)2024</p>
<p>Reinforcement Learning: An Introduction. Richard S Sutton, Andrew G Barto, 2018MIT PressCambridge, MAsecond edition</p>
<p>Policy gradient methods for reinforcement learning with function approximation. David Richard S Sutton, Satinder Mcallester, Yishay Singh, Mansour, Advances in Neural Information Processing Systems (NeurIPS). 200012</p>
<p>Openmanus: Open-source ai agent framework. 2025</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). 201730</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen, Frontiers of Computer Science. 1832024</p>
<p>Text embeddings by weakly-supervised contrastive pre-training. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv:2212.035332022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems (NeurIPS). 202235</p>
<p>Longmemeval: Benchmarking chat assistants on long-term interactive memory. Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu, International Conference on Learning Representations (ICLR). 2025</p>
<p>A-mem: Agentic memory for llm agents. Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang, arXiv:2502.121102025arXiv preprint</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, arXiv:2412.15115Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. Xingzhang Ren, Xuancheng Ren,2024arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2023</p>
<p>Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Yuchen Choi, Lin, Lumos, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2023</p>
<p>Compact: Compressing retrieved documents actively for question answering. Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, Jaewoo Kang, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2024</p>
<p>Agent-r: Training language model agents to reflect via iterative self-training. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, Jiecao Chen, arXiv:2501.114252025arXiv preprint</p>
<p>Inference scaling for long-context retrieval augmented generation. Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2025</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang, arXiv:2502.15589Lightthinker: Thinking step-by-step compression. 2025arXiv preprint</p>
<p>Sglang: Efficient execution of structured language model programs. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E Gonzalez, Clark Barrett, Ying Sheng, Advances in Neural Information Processing Systems (NeurIPS). 2024</p>
<p>Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu, arXiv:2504.031602025arXiv preprint</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2024</p>
<p>Principled reinforcement learning with human feedback from pairwise or k-wise comparisons. Banghua Zhu, Michael Jordan, Jiantao Jiao, Proceedings of the International Conference on Machine Learning (ICML). the International Conference on Machine Learning (ICML)2023</p>
<p>As a result, the agent learns to produce shorter responses with valid syntax but develops less effective internal state representations. We hypothesize that the format reward accelerates structural learning but constrains exploration of effective reasoning strategies. leading to degraded task performance</p>            </div>
        </div>

    </div>
</body>
</html>
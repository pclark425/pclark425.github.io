<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8390 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8390</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8390</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91" target="_blank">Chain of Thought Prompting Elicits Knowledge Augmentation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> CoT-KA is proposed, a Chain-of-Thought-based method that augments knowledge for deep learning that avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods.</p>
                <p><strong>Paper Abstract:</strong> The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8390.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8390.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002, 175B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter autoregressive transformer (GPT-3) used in this paper to generate chain-of-thought (CoT) reasoning traces via few-shot and zero-shot prompts (temperature sampling T=0.7). It is used as a knowledge/reasoning source to produce natural-language step-by-step solutions for arithmetic and other reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer (GPT-3) with 175 billion parameters; used via OpenAI API (text-davinci-002) to generate Chain-of-Thought outputs with few-shot demonstrations or the zero-shot prompt "Let's think step by step" and temperature sampling T=0.7.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems / arithmetic reasoning benchmarks (GSM8K, AQuA/AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal mechanistic neuron-level analysis provided; described as encoding large-scale pre-trained knowledge in parameters and able to produce natural-language intermediate reasoning (CoT) that appears to function as an algorithmic or procedural rationale when prompted.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Prompt engineering (Few-Shot-CoT with demonstrations; Zero-Shot-CoT using "Let's think step by step"); sampling strategies including temperature (T=0.7) and self-consistency (sampling multiple CoTs and majority voting); token log-prob scoring of generated chains for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When parsing answers directly from GPT-3 CoTs (reported or cited results): GSM8K Zero-Shot-CoT ~40.7% (cited); Few-Shot-CoT ~46.9% (cited). AQuA (AQUA-RAT) Zero-Shot-CoT ~33.5% (cited). SVAMP ~63.7% (cited). (These starred results in the paper are taken from Wei et al. 2022 / Kojima et al. 2022 as referenced.)</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>CoTs can contain incorrect answers; when CoTs are incorrect they often still influence downstream models (high alignment), meaning incorrect CoTs can mislead; self-consistency requires many sampled CoTs to be effective (increased compute). No low-level failure-mode analysis (e.g., digit-level carry errors) is provided for GPT-3 itself in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical observation that prompting GPT-3 with CoT prompts elicits multi-step natural-language reasoning traces used to derive answers; effectiveness demonstrated via improved parsed-answer performance in cited prior work and by using these CoTs to augment downstream fine-tuning (CoT-KA). No internal activation/attention/neuron evidence presented.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>CoT outputs can be wrong and still persuasive; self-consistency needs many samples (e.g., 40 in prior work) which is computationally expensive; selecting CoTs by average token probability (log-prob) from GPT-3 did not significantly improve downstream CoT-KA performance in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Knowledge Augmentation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8390.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8390.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer) fine-tuned for NLG arithmetic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task-relevant pre-trained encoder-decoder transformer (T5) used as the downstream model for natural-language generation arithmetic benchmarks; fine-tuned on inputs augmented with GPT-3-generated CoTs (CoT-KA), producing large accuracy gains over the baseline fine-tuned T5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 family architecture used as the task-relevant model for NLG arithmetic tasks; exact size unspecified in paper, fine-tuned on CoT-augmented inputs for 2000 steps with AdamW (learning rate 1e-5 default unless noted).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems / multi-step arithmetic reasoning (GSM8K, AQUA/AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal representational mechanism claimed; arithmetic capability achieved by learning to exploit appended natural-language CoT evidence during fine-tuning — i.e., CoTs act as auxiliary textual features that the model conditions on to produce correct numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Fine-tuning on augmented inputs (CoT-KA) where multiple CoTs produced by GPT-3 are concatenated to the question separated by [EXT] tokens; sampling CoTs from a generated set (10) using different seeds and selecting subsets (1–5) for training; temperature sampling in CoT generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline (T5) accuracies were very low on arithmetic: GSM8K Dev 5.3% / Test 4.4%; SVAMP Dev 8.0% / Test 8.5%; MultiArith Dev 12.5% / Test 8.3%; SingleEq Dev 5.9% / Test 2.9%; AddSub Dev 6.3% / Test 6.3%. CoT-KA (5 Zero-Shot-CoTs, T5) improved to GSM8K Dev 58.9% / Test 57.3%; SVAMP Dev 64.2% / Test 82.3%; MultiArith Dev 82.7% / Test 93.3%; SingleEq Dev 62.9% / Test 73.3%; AddSub Dev 80.3% / Test 74.9%; Last Letter Dev 75.9%. CoT-KA (5 Few-Shot-CoTs, T5) further: GSM8K Dev 61.2% / Test 61.5%; SVAMP Dev 71.8% / Test 70.8%; MultiArith Dev 81.8% / Test 95.3%; SingleEq Dev 76.7% / Test 75.7%; AddSub Dev 86.6% / Test 78.7%; Last Letter Dev 71.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Paper does not provide fine-grained arithmetic error taxonomy (e.g., carry/borrowing, place-value mistakes); observed failure modes include sensitivity to quality and correctness of appended CoTs (models often align with CoT answers and can be misled by incorrect CoT answers), and input-length limits that restrict number of CoTs that can be appended.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical performance gains after fine-tuning on CoT-augmented inputs suggest T5 learns to rely on the textual CoT evidence; ablation-like observation: removing answers from CoTs (keeping reasoning steps) degrades performance, implying that the answer tokens in CoTs are especially influential.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>DeBERTa (a different PLM) sometimes exhibits smaller gains or even slight degradation on some tasks compared to ALBERT when using CoTs; CoT selection by average token probability provided negligible improvement (see Table 6); input-length limitations cap number of CoTs usable for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Knowledge Augmentation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8390.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8390.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-KA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Knowledge Augmentation (CoT-KA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses LLM-generated Chain-of-Thoughts as auxiliary textual knowledge: generate multiple CoTs via an LLM (GPT-3), append them to each sample (with [EXT] tokens), and fine-tune a smaller task-relevant PLM on the augmented data to improve reasoning tasks including arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT-KA pipeline (GPT-3 -> CoT generation -> append CoTs -> fine-tune PLM e.g., T5/ALBERT/DeBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline rather than single model: uses GPT-3 (text-davinci-002, 175B) to generate multiple CoTs per example (few-shot or zero-shot prompting, temperature 0.7), concatenates up to m CoTs to original input with an [EXT] marker, and fine-tunes a downstream PLM (T5 for NLG arithmetic tasks) for the target task.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems and symbolic arithmetic datasets (GSM8K, AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub); both multi-operation and single-equation problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>No internal mechanistic proof, but operational mechanism is that CoTs supply explicit intermediate reasoning / answer signals as textual features that downstream PLMs learn to condition on; paper emphasizes that answer tokens inside CoTs are particularly influential compared to reasoning steps alone.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Interventions studied: zero-shot vs few-shot CoT generation; number of sampled CoTs appended (1–5) and its effect on downstream performance; self-consistency (sampling and majority voting) compared as baseline; CoT selection via per-token probability (average token prob) tested; fine-tuning PLMs on augmented corpora used as the primary intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>CoT-KA substantially improves arithmetic performance versus baseline T5: see entry for T5 (examples): GSM8K from ~5% to ~59–61%; SVAMP from ~8% to ~64–72%; MultiArith from ~12% to ~82–95%; SingleEq from ~3–6% to ~63–77%; AddSub from ~6% to ~80–87%. Gains vary with PLM and whether CoTs were few-shot or zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Key failure modes: (1) Incorrect CoT answers can mislead downstream PLMs — models often align with CoT answers (e.g., StrategyQA alignment ratio 82.9% when CoT answer incorrect), (2) Removing answers from CoTs (keeping only steps) hurts performance, showing reliance on answer tokens, (3) Input length limits restrict number of CoTs appended, reducing potential gains, (4) CoT selection via log-prob did not yield notable improvements, (5) performance gains are PLM-dependent (ALBERT showed more consistent gains than DeBERTa on some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical ablations/analyses in paper: (a) comparison of baseline fine-tuning vs CoT-augmented fine-tuning shows large accuracy increases, (b) analysis of influence of CoTs on predictions: among changed predictions, 61.4% were positive; models align with CoT answers at high rates (95% when CoT answer correct), (c) attempt to retain reasoning steps but remove answers led to performance degradation, supporting the hypothesis that appended CoT answers are a primary signal used by downstream PLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Selecting CoTs by mean token probability did not significantly improve CoT-KA's performance (Table 6); self-consistency (aggregating many CoTs on the LLM side) can compete but requires many samples; DeBERTa sometimes shows limited or negative gains for some datasets when adding CoTs; generalization and scaling limited by PLM input-length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Knowledge Augmentation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8390.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8390.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (Zero-Shot and Few-Shot variants; plus self-consistency)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits multi-step natural-language reasoning traces from LLMs (CoTs) which can be parsed for answers or used as auxiliary knowledge to fine-tune downstream models; includes Few-Shot-CoT (demonstrations), Zero-Shot-CoT (e.g., "Let's think step by step"), and self-consistency (sample multiple CoTs and aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoT prompting applied to large LLMs (e.g., GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Technique rather than a single model: Few-Shot-CoT uses hand-crafted demonstrations to guide generation; Zero-Shot-CoT uses a single prompt like "Let's think step by step"; self-consistency samples multiple CoTs and aggregates answers (majority vote).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic word problems and reasoning benchmarks (GSM8K, AQUA, SVAMP, MultiArith, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Posited mechanism: CoT prompting induces chain-like, step-by-step natural-language traces that reflect intermediate computations/reasoning encoded in the LLM parameters; these traces can serve as explicit procedural representations the model uses to arrive at answers or that can be used as externalized knowledge for downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Prompt engineering (few-shot demonstrations; zero-shot template), decoding strategies (temperature sampling), self-consistency (sample multiple CoTs and majority vote), and using CoTs as additional training data for fine-tuning downstream PLMs (CoT-KA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Cited/used results: Zero-Shot-CoT and Few-Shot-CoT improve LLM parsing-based accuracy on arithmetic benchmarks (examples cited in paper: GSM8K Zero-Shot ~40.7%; Few-Shot parsing ~46.9% (cited); SVAMP ~63.7% (cited); AQuA ~33.5% (cited)). Self-consistency (5 sampled CoTs) shows intermediate gains; CoT-KA (using CoTs to augment downstream PLMs) often outperforms directly parsing LLM CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Self-consistency requires many CoTs to be most effective (increased compute); CoTs contain incorrect steps/answers which can mislead both direct parsing and downstream models; CoT prompting does not reveal low-level arithmetic failure modes in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical: CoT prompting (few- and zero-shot) improves raw LLM answer rates in cited prior work; this paper demonstrates that CoTs used as augmentation (CoT-KA) substantially improve downstream PLM arithmetic performance, and analyses show models often align with CoT answers (i.e., CoTs are used as knowledge). No neuron-level or attention-based evidence is provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>CoTs can be incorrect and yet strongly influence predictions; removing answers from CoTs reduces downstream gains, indicating CoT steps alone may be insufficient; selecting higher-probability CoTs by token log-prob did not meaningfully help CoT-KA; input-length constraints limit number of appended CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Knowledge Augmentation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8390",
    "paper_id": "paper-a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GPT-3 (text-davinci-002)",
            "name_full": "GPT-3 (text-davinci-002, 175B parameters)",
            "brief_description": "A 175B-parameter autoregressive transformer (GPT-3) used in this paper to generate chain-of-thought (CoT) reasoning traces via few-shot and zero-shot prompts (temperature sampling T=0.7). It is used as a knowledge/reasoning source to produce natural-language step-by-step solutions for arithmetic and other reasoning tasks.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002)",
            "model_description": "Autoregressive transformer (GPT-3) with 175 billion parameters; used via OpenAI API (text-davinci-002) to generate Chain-of-Thought outputs with few-shot demonstrations or the zero-shot prompt \"Let's think step by step\" and temperature sampling T=0.7.",
            "arithmetic_task_type": "Math word problems / arithmetic reasoning benchmarks (GSM8K, AQuA/AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub)",
            "mechanism_or_representation": "No internal mechanistic neuron-level analysis provided; described as encoding large-scale pre-trained knowledge in parameters and able to produce natural-language intermediate reasoning (CoT) that appears to function as an algorithmic or procedural rationale when prompted.",
            "probing_or_intervention_method": "Prompt engineering (Few-Shot-CoT with demonstrations; Zero-Shot-CoT using \"Let's think step by step\"); sampling strategies including temperature (T=0.7) and self-consistency (sampling multiple CoTs and majority voting); token log-prob scoring of generated chains for selection.",
            "performance_metrics": "When parsing answers directly from GPT-3 CoTs (reported or cited results): GSM8K Zero-Shot-CoT ~40.7% (cited); Few-Shot-CoT ~46.9% (cited). AQuA (AQUA-RAT) Zero-Shot-CoT ~33.5% (cited). SVAMP ~63.7% (cited). (These starred results in the paper are taken from Wei et al. 2022 / Kojima et al. 2022 as referenced.)",
            "error_types_or_failure_modes": "CoTs can contain incorrect answers; when CoTs are incorrect they often still influence downstream models (high alignment), meaning incorrect CoTs can mislead; self-consistency requires many sampled CoTs to be effective (increased compute). No low-level failure-mode analysis (e.g., digit-level carry errors) is provided for GPT-3 itself in this paper.",
            "evidence_for_mechanism": "Empirical observation that prompting GPT-3 with CoT prompts elicits multi-step natural-language reasoning traces used to derive answers; effectiveness demonstrated via improved parsed-answer performance in cited prior work and by using these CoTs to augment downstream fine-tuning (CoT-KA). No internal activation/attention/neuron evidence presented.",
            "counterexamples_or_challenges": "CoT outputs can be wrong and still persuasive; self-consistency needs many samples (e.g., 40 in prior work) which is computationally expensive; selecting CoTs by average token probability (log-prob) from GPT-3 did not significantly improve downstream CoT-KA performance in this paper.",
            "uuid": "e8390.0",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "T5 (fine-tuned)",
            "name_full": "T5 (Text-to-Text Transfer Transformer) fine-tuned for NLG arithmetic tasks",
            "brief_description": "A task-relevant pre-trained encoder-decoder transformer (T5) used as the downstream model for natural-language generation arithmetic benchmarks; fine-tuned on inputs augmented with GPT-3-generated CoTs (CoT-KA), producing large accuracy gains over the baseline fine-tuned T5.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5 (fine-tuned)",
            "model_description": "T5 family architecture used as the task-relevant model for NLG arithmetic tasks; exact size unspecified in paper, fine-tuned on CoT-augmented inputs for 2000 steps with AdamW (learning rate 1e-5 default unless noted).",
            "arithmetic_task_type": "Math word problems / multi-step arithmetic reasoning (GSM8K, AQUA/AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub)",
            "mechanism_or_representation": "No internal representational mechanism claimed; arithmetic capability achieved by learning to exploit appended natural-language CoT evidence during fine-tuning — i.e., CoTs act as auxiliary textual features that the model conditions on to produce correct numeric answers.",
            "probing_or_intervention_method": "Fine-tuning on augmented inputs (CoT-KA) where multiple CoTs produced by GPT-3 are concatenated to the question separated by [EXT] tokens; sampling CoTs from a generated set (10) using different seeds and selecting subsets (1–5) for training; temperature sampling in CoT generation.",
            "performance_metrics": "Baseline (T5) accuracies were very low on arithmetic: GSM8K Dev 5.3% / Test 4.4%; SVAMP Dev 8.0% / Test 8.5%; MultiArith Dev 12.5% / Test 8.3%; SingleEq Dev 5.9% / Test 2.9%; AddSub Dev 6.3% / Test 6.3%. CoT-KA (5 Zero-Shot-CoTs, T5) improved to GSM8K Dev 58.9% / Test 57.3%; SVAMP Dev 64.2% / Test 82.3%; MultiArith Dev 82.7% / Test 93.3%; SingleEq Dev 62.9% / Test 73.3%; AddSub Dev 80.3% / Test 74.9%; Last Letter Dev 75.9%. CoT-KA (5 Few-Shot-CoTs, T5) further: GSM8K Dev 61.2% / Test 61.5%; SVAMP Dev 71.8% / Test 70.8%; MultiArith Dev 81.8% / Test 95.3%; SingleEq Dev 76.7% / Test 75.7%; AddSub Dev 86.6% / Test 78.7%; Last Letter Dev 71.8%.",
            "error_types_or_failure_modes": "Paper does not provide fine-grained arithmetic error taxonomy (e.g., carry/borrowing, place-value mistakes); observed failure modes include sensitivity to quality and correctness of appended CoTs (models often align with CoT answers and can be misled by incorrect CoT answers), and input-length limits that restrict number of CoTs that can be appended.",
            "evidence_for_mechanism": "Large empirical performance gains after fine-tuning on CoT-augmented inputs suggest T5 learns to rely on the textual CoT evidence; ablation-like observation: removing answers from CoTs (keeping reasoning steps) degrades performance, implying that the answer tokens in CoTs are especially influential.",
            "counterexamples_or_challenges": "DeBERTa (a different PLM) sometimes exhibits smaller gains or even slight degradation on some tasks compared to ALBERT when using CoTs; CoT selection by average token probability provided negligible improvement (see Table 6); input-length limitations cap number of CoTs usable for fine-tuning.",
            "uuid": "e8390.1",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "CoT-KA",
            "name_full": "Chain-of-Thought Knowledge Augmentation (CoT-KA)",
            "brief_description": "A method that uses LLM-generated Chain-of-Thoughts as auxiliary textual knowledge: generate multiple CoTs via an LLM (GPT-3), append them to each sample (with [EXT] tokens), and fine-tune a smaller task-relevant PLM on the augmented data to improve reasoning tasks including arithmetic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CoT-KA pipeline (GPT-3 -&gt; CoT generation -&gt; append CoTs -&gt; fine-tune PLM e.g., T5/ALBERT/DeBERTa)",
            "model_description": "Pipeline rather than single model: uses GPT-3 (text-davinci-002, 175B) to generate multiple CoTs per example (few-shot or zero-shot prompting, temperature 0.7), concatenates up to m CoTs to original input with an [EXT] marker, and fine-tunes a downstream PLM (T5 for NLG arithmetic tasks) for the target task.",
            "arithmetic_task_type": "Multi-step math word problems and symbolic arithmetic datasets (GSM8K, AQUA-RAT, SVAMP, MultiArith, SingleEq, AddSub); both multi-operation and single-equation problems.",
            "mechanism_or_representation": "No internal mechanistic proof, but operational mechanism is that CoTs supply explicit intermediate reasoning / answer signals as textual features that downstream PLMs learn to condition on; paper emphasizes that answer tokens inside CoTs are particularly influential compared to reasoning steps alone.",
            "probing_or_intervention_method": "Interventions studied: zero-shot vs few-shot CoT generation; number of sampled CoTs appended (1–5) and its effect on downstream performance; self-consistency (sampling and majority voting) compared as baseline; CoT selection via per-token probability (average token prob) tested; fine-tuning PLMs on augmented corpora used as the primary intervention.",
            "performance_metrics": "CoT-KA substantially improves arithmetic performance versus baseline T5: see entry for T5 (examples): GSM8K from ~5% to ~59–61%; SVAMP from ~8% to ~64–72%; MultiArith from ~12% to ~82–95%; SingleEq from ~3–6% to ~63–77%; AddSub from ~6% to ~80–87%. Gains vary with PLM and whether CoTs were few-shot or zero-shot.",
            "error_types_or_failure_modes": "Key failure modes: (1) Incorrect CoT answers can mislead downstream PLMs — models often align with CoT answers (e.g., StrategyQA alignment ratio 82.9% when CoT answer incorrect), (2) Removing answers from CoTs (keeping only steps) hurts performance, showing reliance on answer tokens, (3) Input length limits restrict number of CoTs appended, reducing potential gains, (4) CoT selection via log-prob did not yield notable improvements, (5) performance gains are PLM-dependent (ALBERT showed more consistent gains than DeBERTa on some tasks).",
            "evidence_for_mechanism": "Empirical ablations/analyses in paper: (a) comparison of baseline fine-tuning vs CoT-augmented fine-tuning shows large accuracy increases, (b) analysis of influence of CoTs on predictions: among changed predictions, 61.4% were positive; models align with CoT answers at high rates (95% when CoT answer correct), (c) attempt to retain reasoning steps but remove answers led to performance degradation, supporting the hypothesis that appended CoT answers are a primary signal used by downstream PLMs.",
            "counterexamples_or_challenges": "Selecting CoTs by mean token probability did not significantly improve CoT-KA's performance (Table 6); self-consistency (aggregating many CoTs on the LLM side) can compete but requires many samples; DeBERTa sometimes shows limited or negative gains for some datasets when adding CoTs; generalization and scaling limited by PLM input-length constraints.",
            "uuid": "e8390.2",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Chain-of-Thought prompting (CoT)",
            "name_full": "Chain-of-Thought prompting (Zero-Shot and Few-Shot variants; plus self-consistency)",
            "brief_description": "Prompting technique that elicits multi-step natural-language reasoning traces from LLMs (CoTs) which can be parsed for answers or used as auxiliary knowledge to fine-tune downstream models; includes Few-Shot-CoT (demonstrations), Zero-Shot-CoT (e.g., \"Let's think step by step\"), and self-consistency (sample multiple CoTs and aggregate).",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "CoT prompting applied to large LLMs (e.g., GPT-3)",
            "model_description": "Technique rather than a single model: Few-Shot-CoT uses hand-crafted demonstrations to guide generation; Zero-Shot-CoT uses a single prompt like \"Let's think step by step\"; self-consistency samples multiple CoTs and aggregates answers (majority vote).",
            "arithmetic_task_type": "Arithmetic word problems and reasoning benchmarks (GSM8K, AQUA, SVAMP, MultiArith, etc.)",
            "mechanism_or_representation": "Posited mechanism: CoT prompting induces chain-like, step-by-step natural-language traces that reflect intermediate computations/reasoning encoded in the LLM parameters; these traces can serve as explicit procedural representations the model uses to arrive at answers or that can be used as externalized knowledge for downstream models.",
            "probing_or_intervention_method": "Prompt engineering (few-shot demonstrations; zero-shot template), decoding strategies (temperature sampling), self-consistency (sample multiple CoTs and majority vote), and using CoTs as additional training data for fine-tuning downstream PLMs (CoT-KA).",
            "performance_metrics": "Cited/used results: Zero-Shot-CoT and Few-Shot-CoT improve LLM parsing-based accuracy on arithmetic benchmarks (examples cited in paper: GSM8K Zero-Shot ~40.7%; Few-Shot parsing ~46.9% (cited); SVAMP ~63.7% (cited); AQuA ~33.5% (cited)). Self-consistency (5 sampled CoTs) shows intermediate gains; CoT-KA (using CoTs to augment downstream PLMs) often outperforms directly parsing LLM CoTs.",
            "error_types_or_failure_modes": "Self-consistency requires many CoTs to be most effective (increased compute); CoTs contain incorrect steps/answers which can mislead both direct parsing and downstream models; CoT prompting does not reveal low-level arithmetic failure modes in this paper.",
            "evidence_for_mechanism": "Empirical: CoT prompting (few- and zero-shot) improves raw LLM answer rates in cited prior work; this paper demonstrates that CoTs used as augmentation (CoT-KA) substantially improve downstream PLM arithmetic performance, and analyses show models often align with CoT answers (i.e., CoTs are used as knowledge). No neuron-level or attention-based evidence is provided here.",
            "counterexamples_or_challenges": "CoTs can be incorrect and yet strongly influence predictions; removing answers from CoTs reduces downstream gains, indicating CoT steps alone may be insufficient; selecting higher-probability CoTs by token log-prob did not meaningfully help CoT-KA; input-length constraints limit number of appended CoTs.",
            "uuid": "e8390.3",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1
        }
    ],
    "cost": 0.01615325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain of Thought Prompting Elicits Knowledge Augmentation</h1>
<p>Dingjun $\mathbf{W u}^{1}$, Jing Zhang ${ }^{2}$; Xinmei Huang ${ }^{2}$<br>${ }^{1}$ Tsinghua Shenzhen International Graduate School, Tsinghua University<br>${ }^{2}$ School of Information, Renmin University of China<br>wudj20@mails.tsinghua.edu.cn<br>{zhang-jing, huangxinmei}@ruc.edu.cn</p>
<h4>Abstract</h4>
<p>The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>The Knowledge-Augmented deep learning (KADL) (Cui et al., 2022) paradigm refers to the deep learning paradigm in which domain knowledge is identified and integrated into the deep model. Adding domain knowledge makes it possible to develop deep learning that is data-efficient, generalizable, and interpretable (Cui et al., 2022). For example, retrieving external knowledge from an external knowledge pool like Wikipedia is typically required for open domain question answering and dialog generation (Izacard and Grave, 2021; Zhang et al., 2023). Logical equivalence laws such as contraposition and transitive laws help extend the implicit logical information (Yu et al., 2019; Wang et al., 2022a).</p>
<p>External knowledge is derived from various sources. For instance, commonsense knowledge can be extracted from commonsense knowledge</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: A various sources of external knowledge. We use LLM as our source of knowledge.
bases like ConceptNet (Speer et al., 2017) and ATOMIC (Sap et al., 2019). Domain-specific knowledge can be retrieved from knowledge bases such as Wikipedia and Freebase (Bollacker et al., 2008). Logic knowledge, on the other hand, can be in the form of human-defined propositional or first-order logic, which is then utilized as rules for reasoning. In summary, existing knowledge augmentation methods typically involve either creating a retriever to gather relevant knowledge or developing a reasoner to leverage the logical rules within the external knowledge sources (Chen et al., 2017; Izacard and Grave, 2021; Wang et al., 2022a; Zhang et al., 2023).</p>
<p>Recently, large language models (LLMs) (Zhao et al., 2023) have shown their potential as both the source and the retriever or reasoner of external knowledge. LLMs are pre-trained on a huge scale of datasets. Thus, they have already embedded a large amount of knowledge into their parameters, which can be considered a source of external knowledge. The reasoning ability of LLMs allows them to provide knowledge from their parameters without needing an extra retriever or a reasoner. The latest chain-of-thought (CoT) prompting technique</p>
<p>(Wei et al., 2022), which elicits LLMs to generate a series of sentences that mimic the reasoning process for arriving at the answers, improves the reasoning ability of LLMs. It has proved to be remarkably effective in a variety of complex reasoning tasks such as math word problems and commonsense question answering (Wei et al., 2022). CoT prompting shows potential as a general technique to retrieve knowledge from LLMs.</p>
<p>In this paper, we propose CoT-KA - a CoTbased method to retrieve knowledge from LLMs for Knowledge-Augmented deep learning. CoTKA utilizes an LLM as a knowledge source, leveraging CoT prompting to guide the LLM in providing knowledge that can serve as evidence to support downstream reasoning from the input to the answer. Unlike conventional KADL approaches, CoT-KA eliminates the need for additional knowledge retrieval or a separate knowledge reasoning model. Specifically, we begin by extracting CoTs as knowledge from the LLM using either few-shot (Wei et al., 2022) or zero-shot (Kojima et al., 2022) CoT prompting. The former involves providing a few demonstrations to guide the LLM's reasoning, while the latter employs a template such as "let's think step by step" to inspire the LLM. The extracted CoTs are then appended to the original inputs, marked by a special token, to create augmented text. Finally, we fine-tune a small taskrelevant pre-trained language model (PLM) on the dataset augmented with CoTs.</p>
<p>We generate CoTs using the public GPT-3 (Brown et al., 2020) (175B parameters) API. For NLU (Natural Language Understanding) tasks, we employ ALBERT (Lan et al., 2019) and DeBERTa (He et al., 2021) as the task-relevant models. T5 (Raffel et al., 2020) is utilized as the task-relevant model for NLG (Natural Language Generation) tasks. We evaluate models' performance using eleven benchmarks, including (i) commonsense reasoning (CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021), Date Understanding, Sports Understanding (Srivastava et al., 2022)); (ii) arithmetic reasoning (AQUA-RAT (Ling et al., 2017), GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014)); (iii) symbolic reasoning (Last Letter Concatenation (Wei et al., 2022)), where all commonsense reasoning benchmarks and AQUA-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>RAT are formulated as NLU tasks, and the other arithmetic reasoning benchmarks and Last Letter Concatenation are formulated as NLG tasks in this paper. Particularly, we convert all of the multichoice question answering tasks into NLU tasks. Extensive experimental results show that in the majority of tasks, CoT-KA outperforms the original fine-tuning results without the use of CoTs as augmented knowledge. CoT-KA also surpasses Few-Shot-CoT and Zero-Shot-CoT on LLMs, which directly parse answers from the generated CoTs.</p>
<h2>2 Related Work</h2>
<p>Knowledge Augmented Technology. The integration of external knowledge into deep learning models through knowledge augmentation approaches has gained significant attention in various NLP tasks, including question answering (Chen et al., 2017; Izacard and Grave, 2021), dialogue generation (Zhang et al., 2023), and logical reasoning (Wang et al., 2022a). For instance, in the context of answering open-domain questions where supporting evidence is not explicitly provided (Izacard and Grave, 2021), Chen et al. (2017) utilized techniques such as bigram hashing and TFIDF matching to retrieve relevant documents from external knowledge sources. Similarly, Fusion-in-Decoder (Izacard and Grave, 2021) employed methods like BM25 (Robertson et al., 1995) and DPR (Karpukhin et al., 2020) for evidence retrieval. By augmenting the questions with these retrieved pieces of evidence, the models can better reason and provide answers. Logic reasoning is another challenging task that requires a deep understanding of the logical structure within a given text to arrive at the correct answer. To facilitate such logic-level analysis, human-defined logic rules are introduced. Wang et al. (2022a) proposed LReasoner, a logicdriven context extension framework that extends implicit logical information by performing logical reasoning using these predefined rules. The framework enhances the original input by verbalizing and concatenating the implicit logical information, enabling subsequent answer reasoning.</p>
<p>Fusion-in-Decoder and LReasoner inspire our work to extend the external knowledge into the original input. However, the knowledge in these knowledge augmentation methods is sourced from external knowledge bases or pre-defined logical rules, requiring a retriever for knowledge extraction or a reasoner for rule application in the process. In</p>
<table>
<thead>
<tr>
<th>Method/Dataset</th>
<th>CSQA</th>
<th>StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline <br> (ALBERT)</td>
<td>63.4</td>
<td>64.8</td>
</tr>
<tr>
<td>Zero-Shot-CoT <br> (ALBERT)</td>
<td>70.1</td>
<td>67.5</td>
</tr>
<tr>
<td>Few-Shot-CoT <br> (ALBERT)</td>
<td>$\mathbf{7 6 . 2}$</td>
<td>$\mathbf{7 3 . 1}$</td>
</tr>
</tbody>
</table>
<p>contrast, we utilize LLMs that eliminate the need for an additional retriever or reasoner to acquire knowledge for augmentation.</p>
<p>Chain of Thought Prompting on LLMs. A CoT is a series of intermediate natural language reasoning steps that lead to the final output, inspired by how humans use a deliberate thinking process to perform complicated tasks. Experimental results using various LLMs, such as GPT-3 <em>Brown et al. (2020)</em> and PaLM <em>Chowdhery et al. (2022)</em>, demonstrate that CoT prompting enhances performance across a range of arithmetic, commonsense, and symbolic reasoning tasks <em>Wei et al. (2022)</em>.</p>
<p><em>Wei et al. (2022)</em> initially propose Few-ShotCoT, which requires the manual design of a few demonstrations to facilitate the generation of reasoning paths. In contrast, <em>Kojima et al. (2022)</em> propose Zero-Shot-CoT, which employs a single zeroshot prompt that elicits CoTs from LLMs. By simply adding "Let’s think step by step" before each answer, Zero-Shot-CoT demonstrates that LLMs are capable zero-shot reasoners without the need for any manually constructed few-shot examples. Furthermore, <em>Wang et al. (2022b)</em> introduce a new decoding strategy called self-consistency, which involves sampling multiple LLM outputs and aggregating them through majority voting. This strategy encourages the model to consider multiple CoTs when generating answers. However, to achieve optimal performance, a large number of reasoning paths (e.g., 40 paths) must be generated, leading to increased computational costs.</p>
<p>All of these CoT prompting methods directly extract the answer from the CoTs. In contrast, our method utilizes the generated CoTs as supplementary knowledge to improve the fine-tuning of task-relevant models. Moreover, our method demonstrates good performance even when a limited number of CoTs are provided, unlike self-consistency, which relies on generating a large number of CoTs.</p>
<h2>3 Pilot Study</h2>
<p>In this section, we explore the effectiveness of CoT-augmented fine-tuning by simply appending one CoT to the original input. We assess the validity of this approach on two commonsense reasoning datasets, CSQA and StrategyQA.</p>
<p>CoT-augmented Fine-tuning. To perform fine-tuning on ALBERT, we extend the original input text by adding a CoT. We utilize ALBERT-large-v2 for our experiments. Specifically, we generate CoTs using both few-shot and zero-shot CoT methods, known as Few-Shot-CoT and Zero-ShotCoT, respectively. Few-Shot-CoT employs the same demonstrations as described in <em>Wei et al. (2022)</em>. For Zero-Shot-CoT, we utilize the template "Let’s think step by step". As the LLM, we employ GPT-3 with 175-billion parameters (text-davinci-002). Subsequently, we extend the generated CoT into the input of each sample within the CSQA and StrategyQA datasets. Finally, we perform fine-tuning on ALBERT using the augmented datasets.</p>
<p>The experiment results in Table 1 show that both the Zero-Shot-CoT and Few-Shot-CoT augmented fine-tuning significantly enhance the performance of the original fine-tuning method.</p>
<p>The Impact of CoT as Additional Knowledge. Given that the answers within CoTs can potentially be incorrect, we hypothesize that this portion of the CoTs will have a negative effect on the fine-tuning and mislead the model’s prediction. To further explore the effect of CoTs on fine-tuning, we compare the fine-tuning result of the PLMs before and after adding CoTs through a variety of data analyses.</p>
<p>We investigate the extent to which the prediction results are altered when the model’s input is expanded with a CoT. We perform fine-tuning on both the original samples (baseline) and the expanded samples (CoT-extended). Subsequently, we evaluate the fine-tuned models using the validation set. For each instance in the validation set, we compare its predictive result between the originally fine-tuned ALBERT and the CoT-augmented fine-tuning version. Additionally, we define three categories of CoTs during the process.</p>
<ul>
<li>A CoT is labeled as a positive CoT if the addition of the CoT changes the prediction result from incorrect to correct. This indicates a beneficial</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The observation when the original question added a CoT. The figure on the left shows the ratio of <em>positive</em>, <em>neutral</em>, and <em>negative CoTs</em> in the validation set of StrategyQA. The figure on the right shows the proportion of model predictions that do not align with the answer in the CoT. "Not misled" denotes that the answer in the CoT is incorrect, but the model is not misled by the CoT and makes accurate predictions. "Not inspired" denotes that the answer in the CoT is correct, but the model does not follow the correct CoT and makes incorrect predictions.</p>
<h3>Influence on the Model's Prediction</h3>
<ul>
<li>Conversely, a CoT is labeled as a <em>negative CoT</em> if the addition of the CoT changes the prediction result from correct to incorrect. This indicates a misleading effect on the model's prediction.</li>
<li>Furthermore, a CoT is labeled as a <em>neutral CoT</em> if the model's prediction result remains the same after the CoT is added. In such cases, it is not easy to judge the impact of this CoT on the model.</li>
</ul>
<p>The left figure in Figure 2 illustrates the ratio of <em>positive</em>, <em>neutral</em>, and <em>negative CoTs</em>. It is observed that among the model's prediction results that change after adding a CoT, the ratio is 36.2% (166 out of 458). Within this group, the ratio of <em>positive CoTs</em> is 61.4%, while the ratio of <em>negative CoTs</em> is 38.6%. These findings suggest that the model successfully resolves 63.3% (102/161, the number of positive CoTs divided by the number of incorrectly predicted samples in the baseline) of the data samples that were incorrectly predicted prior to adding a CoT.</p>
<p>The second objective is to test our hypothesis that an incorrect CoT (the answer in the CoT is incorrect) may have a negative impact on the model and therefore mislead the prediction of the model. If an incorrect CoT is added to the original input text, what impact does it have on the model's prediction? As the right figure in Figure 2 shows, when an incorrect CoT is added to the original input, the model still has a high probability (17.1%) of not being misled by the incorrect CoT and making accurate predictions. Furthermore, we investigate the extent to which the model would mispredict when a correct CoT (the answer in the CoT is correct) is added. As shown in the figure on the right of Figure 2, the model has a low probability (5.0%) of making an incorrect prediction.</p>
<p>In the case of StrategyQA, when the answer in the CoT is incorrect, the alignment ratio is 1 − <em>Ratio</em> (#Not misled), which equals 82.9%; When the answer in the CoT is correct, the alignment ratio is 1 − <em>Ratio</em> (#Not inspired), which equals 95.0%. The result demonstrates that CoT is a powerful feature, and the model's predictions tend to align closely with the answers provided in CoT. On the other hand, the fine-tuning strategy employed causes the model's predictions to treat CoT as a secondary feature of the original input, rather than strictly following it. In cases where the answer in CoT is correct, the model is likely to align its predictions with the answers in CoT. Conversely, when the answer in CoT is incorrect, there is a relatively high probability that the model will deviate from the answer in the CoT, preventing misleading from the incorrect CoT.</p>
<p>In addition, our attempts to preserve the reasoning steps in the CoTs while removing the answers have resulted in a degradation in performance. We recognize that the presence of incorrect answers in some CoTs can have a negative impact. However, we also believe that the inclusion of correct answers in CoTs can yield positive effects, and the answers within CoTs are a more influential factor than the reasoning paths themselves.</p>
<h1>4 CoT-KA</h1>
<p>In this section, we propose CoT-KA – a CoT-based method for knowledge augmentation. Our method leverages multiple CoTs retrieved from LLMs to provide more auxiliary knowledge for KADL. CoT-KA consists of three steps as shown in Figure 3: (1) CoT Generation: Generating multiple CoTs for each sample in the train, dev, and test sets. (2) Input Augmentation: Taking the generated CoTs as the additional knowledge into the original input text for each sample. (3) Task-relevant Model Training: Fine-tuning a task-relevant model using the CoT-augmented samples.</p>
<h2>4.1 CoT Generation</h2>
<p>We try both Few-Shot-CoT and Zero-Shot-CoT prompting on LLM <em>f</em> to generate multiple CoTs. Formally, given an original samples (<em>x</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>), where</p>
<ol>
<li>CoT Generation
<img alt="img-2.jpeg" src="img-2.jpeg" /></li>
</ol>
<p>Figure 3: Overview of the CoT-KA method. Both Zero-Shot-CoT (on the left) and Few-Shot-CoT (on the right) can be used in the CoT generation stage for CoT-KA.
$x_{i}$ is the original input and $y_{i} \in \mathcal{Y}$ denotes the label. We generate a CoT set consisting of multiple CoTs based on the model $f$ :</p>
<p>$$
C o T^{(i)}=f\left(d, x_{i}\right)
$$</p>
<p>where $d$ denotes the CoT demonstrations that inspire model $f$ to generate CoTs, and $\operatorname{CoT}^{(i)}$ is the generated CoT set of the $i$-th sample, which consists of $m$ CoTs:</p>
<p>$$
\operatorname{CoT}^{(i)}=\left{\operatorname{CoT}<em 2="2">{1}^{(i)}, \operatorname{CoT}</em>\right}
$$}^{(i)}, \ldots, \operatorname{CoT}_{m}^{(i)</p>
<p>For each sample, we independently generate $m$ CoT outputs from $f$ in each run.</p>
<h3>4.2 Input Augmentation</h3>
<p>In the second step, we apply the generated CoTs as additional knowledge to enrich the input text of the original samples. The extended input text of each sample is a concatenation of an original input (e.g. a question), and the generated multiple CoTs. For each sample, we construct an extended input text as follows:</p>
<p>$$
\tilde{x}^{(i)}=\operatorname{concat}\left(x^{(i)}, \operatorname{CoT}^{(i)}\right)
$$</p>
<p>where $\tilde{x}^{(i)}$ is the $i$-th extended input text, $x^{(i)}$ is the $i$-th original input, and $\operatorname{CoT}^{(i)}$ is the $i$-th generated CoT set. concat() is a concatenation function that concatenates the original input and the generated CoTs. More concretely:</p>
<p>$$
\operatorname{concat}\left(x^{(i)}, \operatorname{CoT}^{(i)}\right)
$$</p>
<p>$$
=x^{(i)}| |[E X T] \operatorname{CoT}<em m="m">{1}^{(i)} \ldots| |[E X T] \operatorname{CoT}</em>
$$}^{(i)</p>
<p>where $[E X T]$ is the special token to denote a CoT, and $|\mid$ denotes the concatenation operator.</p>
<h2>5 Experiments</h2>
<h3>5.1 Experimental setup</h3>
<p>Tasks and Datasets. We evaluate CoT-KA on the following reasoning benchmarks ${ }^{3}$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Commonsense</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method/Dataset</td>
<td style="text-align: center;">CSQA <br> Dev</td>
<td style="text-align: center;">StrategyQA <br> Dev</td>
<td style="text-align: center;">Date <br> Dev</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sports <br> Dev</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AQuA <br> Test</td>
</tr>
<tr>
<td style="text-align: center;">Zero-Shot-CoT</td>
<td style="text-align: center;">64.6*</td>
<td style="text-align: center;">54.8*</td>
<td style="text-align: center;">67.5*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">52.4*</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.5*</td>
</tr>
<tr>
<td style="text-align: center;">Few-Shot-CoT</td>
<td style="text-align: center;">- (73.5*)</td>
<td style="text-align: center;">68.3 (65.4*)</td>
<td style="text-align: center;">54.7/47.4 (52.1*)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">83.2/86.7 (82.4*)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-/37.9 (35.8*)</td>
</tr>
<tr>
<td style="text-align: center;">Self-Consistency <br> (5 Zero-Shot-CoTs)</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">29.2/35.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">57.6/58.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">33.2/37.0</td>
</tr>
<tr>
<td style="text-align: center;">Self-Consistency <br> (5 Few-Shot-CoTs)</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">53.4/50.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">85.4/90.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.6/40.2</td>
</tr>
<tr>
<td style="text-align: center;">Baseline <br> (ALBERT)</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">25.6</td>
</tr>
<tr>
<td style="text-align: center;">CoT-KA <br> (5 Zero-Shot-CoTs, ALBERT)</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;">CoT-KA <br> (5 Few-Shot-CoTs, ALBERT)</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">46.9</td>
</tr>
<tr>
<td style="text-align: center;">Baseline <br> (DeBERTa)</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">27.8</td>
</tr>
<tr>
<td style="text-align: center;">CoT-KA <br> (5 Zero-Shot-CoTs, DeBERTa)</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: center;">CoT-KA <br> (5 Few-Shot-CoTs, DeBERTa)</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">45.9</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy on five NLU datasets from two categories of reasoning tasks. For CSQA and StrategyQA, we report the evaluation results of the dev set. For the other datasets in which the labels are available, we report the results of both the dev and test. * indicates the results comes from (Wei et al., 2022) and (Kojima et al., 2022). The results of baseline methods and CoT-KA are based on ALBERT-large-v2 and DeBERTa-v3-large. "Baseline" denotes the fine-tuning baseline with original data. " 5 Zero-Shot-CoTs" and " 5 Few-Shot-CoTs" denotes five CoTs used at Self-Consistency and CoT-KA. Bold denotes the best-performed results. For Few-Shot-CoT, the results before and after the "/" symbol indicate the results of directly parsing the answers from the CoT (from Wei et al. (2022)) for the dev and test set, respectively, under our data partitioning. For Self-Consistency, the results before and after the "/" symbol represent the results obtained by parsing the answer from multiple CoTs (We generated) in the dev and test set, respectively, under our data partitioning and then applying majority voting.</p>
<ul>
<li>Commonsense reasoning. We evaluate our method on four commonsense reasoning tasks: CSQA (Talmor et al., 2019), StrategyQA (Geva et al., 2021) and two benchmarks from the BIGbench effort (Srivastava et al., 2022): Date Understanding and Sports Understanding.</li>
<li>Arithmetic reasoning. We use six arithmetic reasoning benchmarks: AQUA-RAT (Ling et al., 2017), GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), SingleEq (Koncel-Kedziorski et al., 2015), AddSub (Hosseini et al., 2014).</li>
<li>Symbolic Reasoning. We use the Last Letter Concatenation from Wei et al. (2022). ${ }^{4}$</li>
</ul>
<h2>Implementation.</h2>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>- CoT Generation Models. We use GPT-3 of the text-davinci-002 engine with 175-billion parameters to generate the CoTs used in CoT-KA.
- CoT Demonstrations. For a fair comparison, we perform Few-Shot-CoT with the same demonstrations as in Wei et al. (2022) and use the same zero-shot prompt as in Kojima et al. (2022) to perform Zero-Shot-CoT.
- Sampling Scheme. To generate diverse CoTs, we apply temperature sampling during the CoT generation. Specifically, we use the same $T=0.7$ as in (Wang et al., 2022b) for a fair comparison.
- Data Preprocessing. For certain undivided datasets, we divide them into train, dev, and test sets for fine-tuning, following a ratio of 6:2:2. Further details regarding the dataset splits can be found in Appendix A.1. Additionally, as the original questions and demonstrations used for CoT generation may include option information (e.g., Answer Choices: (a) ignore ...(e) avoid),</p>
<table>
<thead>
<tr>
<th></th>
<th>Arithmetic</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Symbolic</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Method/Dataset</td>
<td>GSM8K</td>
<td></td>
<td>SVAMP</td>
<td></td>
<td>MultiArith</td>
<td></td>
<td>SingleEq</td>
<td></td>
<td>AddSub</td>
<td></td>
<td>Letter (4)</td>
</tr>
<tr>
<td></td>
<td>Dev</td>
<td>Test</td>
<td>Dev</td>
<td>Test</td>
<td>Dev</td>
<td>Test</td>
<td>Dev</td>
<td>Test</td>
<td>Dev</td>
<td>Test</td>
<td>Dev</td>
</tr>
<tr>
<td>Zero-Shot-CoT</td>
<td>40.7*</td>
<td></td>
<td>63.7*</td>
<td></td>
<td>78.7*</td>
<td></td>
<td>78.7*</td>
<td></td>
<td>74.7*</td>
<td></td>
<td>57.6*</td>
</tr>
<tr>
<td>Few-Shot-CoT</td>
<td>-/46.5 (46.9*)</td>
<td></td>
<td>69.2/69.0 (68.9*)</td>
<td></td>
<td>85.8/90.0 (91.7*)</td>
<td></td>
<td>82.4/87.3 (86.6*)</td>
<td></td>
<td>79.7/65.8 (81.3*)</td>
<td></td>
<td>(59.0**)</td>
</tr>
<tr>
<td>Self-Consistency</td>
<td>51.7/52.2</td>
<td></td>
<td>70.0/73.4</td>
<td></td>
<td>81.7/96.4</td>
<td></td>
<td>64.8/92.0</td>
<td></td>
<td>79.7/73.7</td>
<td></td>
<td>66.3/60.2</td>
</tr>
<tr>
<td>(5 Zero-Shot-CoTs)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Self-Consistency</td>
<td>55.7/56.6</td>
<td></td>
<td>74.7/75.5</td>
<td></td>
<td>94.8/95.7</td>
<td></td>
<td>88.5/91.9</td>
<td></td>
<td>86.8/73.9</td>
<td></td>
<td>59.0/60.5</td>
</tr>
<tr>
<td>(5 Few-Shot-CoTs)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Baseline (T5)</td>
<td>5.3</td>
<td>4.4</td>
<td>8.0</td>
<td>8.5</td>
<td>12.5</td>
<td>8.3</td>
<td>5.9</td>
<td>2.9</td>
<td>6.3</td>
<td>6.3</td>
<td>30.0</td>
</tr>
<tr>
<td>CoT-KA</td>
<td>58.9</td>
<td>57.3</td>
<td>64.2</td>
<td>82.3</td>
<td>82.7</td>
<td>93.3</td>
<td>62.9</td>
<td>73.3</td>
<td>80.3</td>
<td>74.9</td>
<td>75.9</td>
</tr>
<tr>
<td>(5 Zero-Shot-CoTs, T5)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoT-KA</td>
<td>61.2</td>
<td>61.5</td>
<td>71.8</td>
<td>70.8</td>
<td>81.8</td>
<td>95.3</td>
<td>76.7</td>
<td>75.7</td>
<td>86.6</td>
<td>78.7</td>
<td>71.8</td>
</tr>
<tr>
<td>(5 Few-Shot-CoTs, T5)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on six NLG datasets from two categories of reasoning tasks. * indicates the results comes from (Wei et al., 2022) and (Kojima et al., 2022) and ** denotes the result comes from (Zhang et al., 2022).</p>
<table>
<thead>
<tr>
<th>StrategyQA</th>
<th>Question: Would Siduri enjoy an unlimited buffet? <br> Blink: Siduri is a character in the "Epic of Gilgamesh". She is an "alewife", a wise female divinity associated with fermentation (specifically beer and wine). <br> Few Shot CoT: Siduri is a fairy in Irish mythology. She was known for her hospitality, so she would probably enjoy an unlimited buffet. So the answer is yes.</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sports</td>
<td>Question: Will Fuller was perfect from the line? <br> Blink: William Vincent Fuller V (born April 16, 1994) is an American football wide receiver for the Houston Texans of the National Football League (NFL). He was drafted by the Texans in the first round of the 2016 NFL Draft. He played college football at Notre Dame. Few Shot CoT: Will Fuller is a football player. Being perfect from the line is part of basketball, not football. So the answer is no.</td>
</tr>
</tbody>
</table>
<p>Table 4: Knowledge augmentation examples from commonsense reasoning tasks. The first case comes from StrategyQA. In this case, the description of Siduri does not mention the relationship between Siduri and the unlimited buffet, which is the key to answering the question. The second case comes from Sports Understanding. In this case, we need to know that being perfect from the line is part of basketball, and Will Fuller is a football player, while the entity-knowledge can only provide the latter. the generated CoT will also contain option markers (e.g., the answer is (a)). To provide valuable information within the CoTs, we replace the option markers in the generated CoT with their corresponding textual content (e.g., the answer is "ignore").</p>
<ul>
<li>Classifier Models. We conduct the main experiments using two backbone PLMs: ALBERT-large-v2 and DeBERTa-v3-large. The hyperparameters for the training process are reported in Appendix A.2.</li>
</ul>
<p>Baselines. We take three methods as the baselines: Zero-Shot-CoT, Few-Shot-CoT, and SelfConsistency. Furthermore, to demonstrate the extent to which the CoT knowledge elicits the KADL, we also compare our method with the original finetuning baselines, which solely employ the original text for fine-tuning.</p>
<h3>5.2 Main Results</h3>
<p>Table 2 compares the accuracy across eleven datasets from three categories of NLU and NLG tasks. The Zero-Shot-CoT results are taken from Kojima et al. (2022), and the Few-Shot-CoT results are taken from Wei et al. (2022). For SelfConsistency ( 5 sampled CoTs), we report the result based on a majority vote. The CoT-KA results are averaged over at least five random runs (see Appendix for more details), where we use the different seeds to sample 5 CoTs from a CoT set containing 10 generated CoTs in each run.</p>
<p>As shown in Table 2 and 3, the performance of CoT-KA surpasses all baselines on most tasks. We have made several findings: (1) The CoTs generated by Zero-Shot-CoT and Few Shot-CoT can be utilized with CoT-KA, resulting in significantly improved performance compared to the fine-tuning baselines. Additionally, the CoTs generated by Few-Shot-CoT exhibit better performance com-</p>
<p>pared to Zero-Shot-CoT when they are used with CoT-KA. (2) CoT-KA achieves better performance on the NLU tasks than on the NLG tasks. (3) CoTKA shows different robustness on different models. While DeBERTa outperforms ALBERT on most tasks, CoT-KA is more robust on ALBERT and exhibits performance improvements across all tasks.</p>
<h3>5.3 Knowledge Augmentation Comparison</h3>
<p>To compare CoT-KA with other knowledge augmentation methods, we employ BLINK [wu2020blink] to enrich the entity knowledge in the question. BLINK is a two-stage entity linking approach based on BERT [kenton2019bert]. We use BLINK to link the entities mentioned in the question and retrieve their corresponding entity information. BLINK provides a short description for each entity, which we utilize as extensions to enrich the questions.</p>
<table>
<thead>
<tr>
<th>Method/Dataset</th>
<th>StrategyQA</th>
<th>Sports</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Dev</td>
<td>Dev</td>
<td>Test</td>
</tr>
<tr>
<td>Baseline (ALBERT)</td>
<td>62.2</td>
<td>57.2</td>
<td>53.2</td>
</tr>
<tr>
<td>BLink (ALBERT)</td>
<td>58.0</td>
<td>81.3</td>
<td>77.4</td>
</tr>
<tr>
<td>CoT-KA (ALBERT)</td>
<td>75.7</td>
<td>89.9</td>
<td>89.8</td>
</tr>
<tr>
<td>Baseline (DeBERTa)</td>
<td>68.8</td>
<td>84.5</td>
<td>82.8</td>
</tr>
<tr>
<td>BLink (DeBERTa)</td>
<td>67.7</td>
<td>92.5</td>
<td>87.5</td>
</tr>
<tr>
<td>CoT-KA (DeBERTa)</td>
<td>76.9</td>
<td>96.9</td>
<td>95.6</td>
</tr>
</tbody>
</table>
<p>Table 5: Knowledge augmentation comparison.
As shown in Table 5, the entity knowledge-based augmentation method improves performance on Sports Understanding but has a negative impact on StrategyQA, with both performing worse than our method. Additionally, we observe that approximately 29\% of questions in StrategyQA and 3\% in Sports Understanding could not have entities extracted. Furthermore, the average number of recognized entities in a Sports Understanding question is 1.095, while in StrategyQA, it is 0.928. Moreover, Table 4 demonstrates that entity information may not always include the specific information required by the questions. In contrast, our method can add more useful information, resulting in a more substantial improvement.</p>
<h3>5.4 The Effect of CoT Size</h3>
<p>To demonstrate the effect of the number of sampled CoTs, we vary the number of sampled CoTs (1, 2, 3, 4, 5) in CoT-KA and evaluate on StrategyQA. The results are shown in Figure 4. The experimental results indicate that as the number of CoTs increases,</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The impact of the sampled CoT size on CoT-KA. We randomly sampled 1 to 5 CoTs from both the CoT set generated by Zero-Shot-CoT and Few-ShotCoT.</p>
<p>there is a general upward trend in the performance of CoT-KA. This trend becomes more pronounced when the CoTs are generated by Few-Shot-CoT. More results are reported in Appendix B.</p>
<h3>5.5 CoT Selection Strategy</h3>
<p>CoT-KA can only extend a small number of CoTs due to the maximum length limitation of the input sequence that the language model can handle. Therefore, it is natural to consider designing a CoT selection strategy to choose higher-quality CoTs from the generated CoT set for KADL. Each CoT can be expressed as: $t_{i} \in\left{t_{1}, t_{2}, \ldots, t_{K}\right}$, where $t_{i}$ is the $i$-th token. We can get the log prob of each generated token when using GPT3 API to generate reasoning chains. The log prob refers to the natural logarithm of the probability that the token occurs next given the prompt. To select the 5 reasoning chains with higher confidence from the 10 generated CoTs, we score the generated CoTs using the following formula:</p>
<p>$$
\begin{aligned}
\operatorname{score}\left(C o T_{j}\right) &amp; =\frac{\sum_{i=1}^{K_{j}} \exp \left(\log p\left(t_{i}\right)\right)}{K_{j}} \
&amp; =\frac{\sum_{i=1}^{K_{j}} p\left(t_{i}\right)}{K_{j}}
\end{aligned}
$$</p>
<p>where $p\left(t_{i}\right)$ denotes the probability of generating the $i$-th token, and log denotes the logarithm. and $K_{j}$ is the total number of tokens in the $j$-th CoT. The results shown in Table 6 demonstrate that selecting CoTs from the generated set based on the probability of token generation in the sentence does not lead to a significant improvement in the performance of CoT-KA.</p>
<h2>6 Conclusion and Future Work</h2>
<p>This paper introduces a CoT-based method to retrieve knowledge from LLMs for Knowledge-Augmented deep learning (CoT-KA) that elicits</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">StrategyQA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT-KA (ALBERT)</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: left;">CoT-KA (ALBERT) + CoT Selection</td>
<td style="text-align: center;">75.9</td>
</tr>
<tr>
<td style="text-align: left;">CoT-KA (DeBERTa)</td>
<td style="text-align: center;">76.9</td>
</tr>
<tr>
<td style="text-align: left;">CoT-KA (DeBERTa) + CoT Selection</td>
<td style="text-align: center;">76.9</td>
</tr>
</tbody>
</table>
<p>Table 6: CoT selection strategy based on the log prob
knowledge augmentation on a variety of NLU and NLG benchmarks. Unlike conventional knowledge augmentation approaches, our method does not require a retriever or a reasoner, yet it surpasses the performance of conventional knowledge-based methods and other CoT-based approaches across a range of public NLP tasks.</p>
<p>In the future, it is worthwhile to investigate other methods that can provide insights from LLMs. Exploring new approaches for leveraging the capabilities of LLMs to enhance knowledge augmentation represents a promising area for future research.</p>
<h2>7 Limitations</h2>
<p>One limitation of CoT-KA is that it performs finetuning based on the PLMs, and the input sequence length limit of the PLMs allows us to add only a limited number of CoTs. Therefore, it is important to explore and develop a CoT selection strategy in future research. A good CoT selection strategy would enable the identification of highly effective CoTs from a set of CoTs, enhancing the efficiency of KADL.</p>
<h2>Acknowledgments</h2>
<p>This work is supported by National Natural Science Foundation of China 62076245; CCF-Zhipu AI Large Model Fund.</p>
<h2>References</h2>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Zijun Cui, Tian Gao, Kartik Talamadupula, and Qiang Ji. 2022. Knowledge-augmented deep learning and its applications: A survey. arXiv preprint arXiv:2212.00017.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361.</p>
<p>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523-533, Doha, Qatar. Association for Computational Linguistics.</p>
<p>Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Online. Association for Computational Linguistics.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781.</p>
<p>Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171-4186.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67.</p>
<p>Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752.</p>
<p>Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for ifthen reasoning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 3027-3035.</p>
<p>Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of NAACL-HLT, pages 41494158.</p>
<p>Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022a. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Linguistics: ACL 2022, pages 1619-1629.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022b. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zeroshot entity linking with dense entity retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6397-6407.</p>
<p>Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2019. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations.</p>
<p>Jing Zhang, Xiaokang Zhang, Daniel Zhang-Li, Jifan Yu, Zijun Yao, Zeyao Ma, Yiqi Xu, Haohua Wang, Xiaohan Zhang, Nianyi Lin, et al. 2023. Glmdialog: Noise-tolerant pre-training for knowledgegrounded dialogue generation. arXiv preprint arXiv:2302.14401.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Number of samples</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">We divide the dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train</td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CSQA</td>
<td style="text-align: center;">9741</td>
<td style="text-align: center;">1221</td>
<td style="text-align: center;">1140</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">1831</td>
<td style="text-align: center;">458</td>
<td style="text-align: center;">490</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">221</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Sports</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">AQUA</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">5978</td>
<td style="text-align: center;">1495</td>
<td style="text-align: center;">1319</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">MuitiArith</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Single Eq</td>
<td style="text-align: center;">304</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Add Sub</td>
<td style="text-align: center;">237</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: center;">Last Letter</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">Yes</td>
</tr>
</tbody>
</table>
<p>Table 7: Summary of the datasets we use in this paper. For datasets that are not pre-divided into train, dev, and test sets, we conduct the division ourselves.</p>
<p>For some undivided datasets used in this paper, we divide them into train, dev, and test sets for finetuning, following a ratio of 6:2:2. Table 7 shows the division details of each dataset. In the case of AQUA, the raw training set is too large (97467 samples). To mitigate the computational cost of generating multiple CoTs using the public GPT3 API, we select a subset of 5000 samples (the top 5000) from the raw train set as our train set.</p>
<h2>A. 2 Hyper-parameters for Fine-tuning</h2>
<p>All experiments are conducted in a Linux environment with a single (24G) NVidia RTX 3090 GPU. The model is optimized using the AdamW optimizer. We do not perform an exhaustive hyperparameter search, but only adjust the learning rate prior to the formal experiment. For most experiments in this paper, a learning rate of $1 \mathrm{e}-5$ is chosen as the final value for fine-tuning ALBERT and DeBERTa, except in the following cases for CSQA and StrategyQA:</p>
<ul>
<li>CSQA: A learning rate of $2 \mathrm{e}-5$ is used for CoT-KA (1 Zero-Shot-CoT, ALBERT).</li>
<li>StrategyQA: A learning rate of 5e-6 is used for CoT-KA (1 Zero-Shot-CoT, ALBERT), CoT-KA (1 Few-Shot-CoT, DeBERTa) and CoT-KA (5 Few-Shot-CoTs, both ALBERT and DeBERTa).</li>
</ul>
<p>More hyper-parameters are shown in Table 8.
The random seed set utilized for experiments is $[0,10,20,30,40,50,60,70,80,90]$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">ALBERT/DeBERTa</th>
<th style="text-align: center;">T5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">Peak Learning Rate</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">Training Steps</td>
<td style="text-align: center;">2000</td>
<td style="text-align: center;">2000</td>
</tr>
<tr>
<td style="text-align: left;">Warmup Proportion</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Weight Decay</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\epsilon$</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
<td style="text-align: center;">$1 \mathrm{e}-8$</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\beta_{1}$</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">Adam $\beta_{2}$</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0.999</td>
</tr>
</tbody>
</table>
<p>Table 8: Hyper-parameters for fine-tuning.</p>
<p>These seeds are used for both CoT sampling and fine-tuning. For the case of experimental results averaged over five runs, we use the top five seeds from the seed set. For NLU tasks, most experimental results in Table 2 are averaged over ten runs, except for the following cases:</p>
<ul>
<li>CoT-KA (5 Zero-Shot-CoTs) on all NLU tasks are averaged over five runs.</li>
<li>CoT-KA (5 Few-Shot-CoTs) on AQUA is averaged over five runs.</li>
</ul>
<p>For NLG tasks, most results in Table 3 are averaged over ten runs, with the exception of CoTKA (5 Zero-Shot-CoTs) and CoT-KA (5 Few-ShotCoTs), which are averaged over five runs.</p>
<p>The result for Blink in Table 5 are averaged over five runs. All the new results in Section 5.4 and Appendix B, where the number of sampled CoTs ranges from 1 to 4 , are averaged over five runs.</p>
<h2>B More results about the Effect of CoT Size in CoT-KA</h2>
<p>We vary the number of sampled CoTs $(1,5)$ in CoTKA and evaluate its performance on ten tasks, excluding StrategyQA. Figures from 5 to 14 indicate that in most of these tasks, increasing the number of CoTs from 0 to 1 significantly improves task performance. However, when using DeBERTa-v3large as the PLM, the performance gain in CoT-KA for CSQA, Date Understanding, and Sports Understanding is slight and even leads to a degradation. Furthermore, increasing the number of CoTs from 1 to 5 has a relatively small performance gain in CoTKA (DeBERTa), except for improved Date Understanding and continued degradation in CSQA.</p>
<p>We observe that if the baseline, where the dataset is not augmented by a CoT, starts with a lower performance, the performance gain in CoT-KA becomes more significant as the number of CoTs increases.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Accuracy of CSQA. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy of Date Understanding. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Accuracy of Sports Understanding. Performance over various numbers of CoTs used in CoT-KA.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Accuracy of AQUA. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Accuracy of GSM8K. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Accuracy of SVAMP. Performance over various numbers of CoTs used in CoT-KA.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Accuracy of MultiArith. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Accuracy of SingleEq. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: Accuracy of AddSub. Performance over various numbers of CoTs used in CoT-KA.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: Accuracy of Last Letter Concatenation. Performance over various numbers of CoTs used in CoT-KA.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We do not use the Coin Flip dataset for the evaluation because it is a simple classification task for fine-tuning. This is because ALBERT-large-v2 and DeBERTa-v3-large can already achieve $100 \%$ accuracy in the evaluation phase.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
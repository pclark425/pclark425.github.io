<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-987 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-987</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-987</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-7fef0b38926fdb31cc8b8d50a746bd6ec8f19371</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7fef0b38926fdb31cc8b8d50a746bd6ec8f19371" target="_blank">Variational Causal Networks: Approximate Bayesian Inference over Causal Structures</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A parametric variational family modelled by an autoregressive distribution over the space of discrete DAGs, which can be tractably learned by maximising an Evidence Lower Bound and is able to provide a good approximation of the true posterior.</p>
                <p><strong>Paper Abstract:</strong> Learning the causal structure that underlies data is a crucial step towards robust real-world decision making. The majority of existing work in causal inference focuses on determining a single directed acyclic graph (DAG) or a Markov equivalence class thereof. However, a crucial aspect to acting intelligently upon the knowledge about causal structure which has been inferred from finite data demands reasoning about its uncertainty. For instance, planning interventions to find out more about the causal mechanisms that govern our data requires quantifying epistemic uncertainty over DAGs. While Bayesian causal inference allows to do so, the posterior over DAGs becomes intractable even for a small number of variables. Aiming to overcome this issue, we propose a form of variational inference over the graphs of Structural Causal Models (SCMs). To this end, we introduce a parametric variational family modelled by an autoregressive distribution over the space of discrete DAGs. Its number of parameters does not grow exponentially with the number of variables and can be tractably learned by maximising an Evidence Lower Bound (ELBO). In our experiments, we demonstrate that the proposed variational posterior is able to provide a good approximation of the true posterior.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e987.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e987.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VCN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variational Causal Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational Bayesian method to approximate the posterior over DAG structures by parameterising a flexible autoregressive distribution (LSTM) over adjacency matrices and maximising an ELBO with a DAG-aware Gibbs prior and closed-form marginal likelihood (BGe score).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Variational Causal Networks (VCN)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Performs variational inference over discrete DAGs by modelling the variational posterior q_phi(G) as an autoregressive distribution over the non-diagonal entries of the adjacency matrix (implemented with an LSTM). The ELBO is E_q[log p(D | G)] - KL(q || p(G)); the marginal likelihood p(D | G) is computed in closed form for linear Gaussian SCMs via the BGe score. The prior p(G) is a Gibbs distribution that encodes a NOTEARS-style DAG constraint (trace(exp(A)) - d) and optional sparsity regulariser. Gradients of the ELBO use the score-function estimator (REINFORCE) with baselines; sampling from q_phi enables Monte Carlo ELBO estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic ER graphs; Dream4 in-silico gene-expression dataset (in-silico/virtual lab)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Evaluations use synthetic Erdős–Rényi random DAGs (data generated by linear Gaussian SCMs) and the Dream4 in-silico multifactorial gene-expression dataset (10 nodes, in-silico experiments). The framework is designed to operate on purely observational data and to be used to plan interventions (active experimental design) to disambiguate causal structure.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Indirect handling via (1) Bayesian marginalisation which expresses uncertainty about edges and thus downweights spurious edges by low posterior mass, (2) explicit sparsity regulariser in the Gibbs prior (L1 penalty) to discourage fully connected (spurious) graphs, and (3) a DAG constraint prior to prevent cycle-induced spurious structures; additionally the posterior can be used to prioritise interventions to resolve spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Irrelevant variables / spurious edges arising from finite-sample noise and model non-identifiability; cyclic/overconnected graphs (structures with spurious edges); general non-identifiability due to observational data.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit: spurious edges are identified via low marginal/ posterior probability under q_phi(G) (i.e., empirical edge beliefs from samples), and by the Bayesian posterior uncertainty concentrated away from incorrect edges; no explicit statistical test is proposed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>KL regularisation to the Gibbs prior (which penalises cyclic and dense graphs) and the L1 sparsity term in the prior; marginalisation over graphs/parameters causes spurious edges to receive low posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation is enabled conceptually by (a) low posterior mass on suspect edges (allowing one to 'refute' with low confidence), and (b) using the posterior to select informative interventions/experiments (active design, as discussed and referenced) to disambiguate and empirically rule out spurious causal claims. The paper itself does not run an intervention-selection experiment but describes the capability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Not implemented as a core algorithm in this paper, but the authors explicitly state that the posterior uncertainty from VCN can be leveraged to select informative interventions (referencing budgeted experimental design work). Thus the recommended strategy is posterior-based experimental design: pick interventions that maximally reduce posterior uncertainty about edges/effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Qualitative: VCN (autoregressive variational posterior) achieves substantially lower Hellinger distance to the enumerated true posterior in small graphs (d ≤ 4) than a factorised posterior; in higher-dim tests VCN gives lower expected Structural Hamming Distance (E[SHD]) and higher AUROC than the factorised baseline, and compares favourably to non-differentiable baselines (IMAP MCMC, DAG bootstrap), particularly in low-sample (n=10) regimes where spurious correlations are more common. Exact numeric results are reported in the paper figures but not tabulated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>The main non-robust comparator is the factorised Bernoulli variational posterior: it yields worse reconstruction of the true multimodal posterior (higher Hellinger distance), higher E[SHD], and lower AUROC than VCN across evaluated settings. Compared to IMAP MCMC and DAG Bootstrap, VCN is faster (runtime advantage) while achieving comparable or better metrics in many settings, especially n=10.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an autoregressive variational family (LSTM) substantially improves approximation of the multimodal posterior over DAGs versus a factorised Bernoulli posterior, which helps in finite-sample regimes where spurious correlations and non-identifiability create multiple plausible graphs. Incorporating a DAG-aware Gibbs prior (NOTEARS constraint) and sparsity regularisation steers posterior mass away from cyclic/overconnected (spurious) graphs. The posterior uncertainty produced by VCN can be exploited to select interventions to resolve spurious correlations, making it a practical tool for interactive/virtual-lab experimental design.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e987.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Factorised baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Factorised Bernoulli variational posterior (Ke et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational posterior that models each adjacency entry as an independent Bernoulli; easy to optimise but cannot capture dependencies between edges or multimodal posteriors arising from non-identifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning neural causal models from unknown interventions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Factorised Bernoulli variational posterior</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Represents the adjacency matrix entries as independent Bernoulli random variables (fully factorised q). Each edge probability is modelled independently and learned by optimising an ELBO-like objective; this ignores structural dependencies (e.g. mutual exclusion of opposite directed edges) and cannot model multimodality in the posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic ER graphs; Dream4 in-silico dataset (same experimental settings as VCN)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Evaluated on the same synthetic and real (in-silico) datasets as VCN to compare posterior approximation quality and downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Per paper: performs worse than VCN (higher Hellinger distance to true posterior, higher E[SHD], lower AUROC); fails to capture multimodal posteriors in non-identifiable settings.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A factorised variational posterior is insufficient to capture multimodal posteriors caused by non-identifiability, leading to poorer robustness to spurious, sample-dependent edges compared to an autoregressive variational family.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e987.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NOTEARS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAGs with NO TEARS (continuous optimisation for structure learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Converts combinatorial DAG search into a continuous constrained optimisation by relaxing adjacency to weighted matrix W with a smooth acyclicity constraint tr(exp(W)) - d = 0, enabling gradient-based optimisation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dags with no tears: Continuous optimization for structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>NOTEARS continuous optimisation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Reparameterises the adjacency as a real-valued weight matrix W and casts structure learning as continuous optimisation of a score F(W,D) subject to a smooth acyclicity constraint given by tr(exp(W)) - d = 0, solvable with constrained optimisation (e.g., augmented Lagrangian). In this paper, NOTEARS is used both as a baseline causal discovery algorithm and as the basis for a Gibbs prior that penalises cyclic graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic ER graphs; used within DAG Bootstrap baseline and as a prior term in VCN</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Applied to purely observational datasets; when used in bootstrap, NOTEARS is run on resampled datasets to generate candidate DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>When used as a prior (Gibbs prior with DAG constraint) it downweights graphs that violate acyclicity and, together with sparsity penalty, penalises overconnected graphs that could reflect spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Used as baseline; bootstrapped NOTEARS (Boot Notears) performed well on AUROC in the paper's experiments, though DAG Bootstrap has limitations (support limited to graphs found by bootstrap).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NOTEARS provides a differentiable DAG constraint used to (a) form a Gibbs prior that discourages cyclic and dense graphs (thereby reducing spurious cycle-induced structures) and (b) act as a competitive baseline when used in bootstrapping, performing well on AUROC though limited by support and runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e987.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DAG Bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DAG bootstrap (Boot Notears / Boot LiNGAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Estimate posterior over DAGs by resampling the data many times, running an off-the-shelf causal discovery algorithm on each resample, and aggregating frequency counts over discovered graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>abcdstrategy: Budgeted experimental design for targeted causal structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>DAG Bootstrap</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Resample the dataset with replacement many times (bootstrap), run a causal discovery algorithm (e.g., NOTEARS or LiNGAM) on each resampled dataset, collect the set of discovered graphs, and form an empirical posterior by normalised frequency counts over these graphs. Different base algorithms give variants (Boot Notears, Boot LiNGAM).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic ER graphs; Dream4 in-silico dataset (evaluated as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive bootstrap procedure applied to observational datasets; can be computationally expensive because it runs the base discovery algorithm many times.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Empirical frequency-based robustness: spurious edges that appear only in a few bootstrap resamples receive low frequency/weight in the empirical posterior; thus bootstrapping downweights unstable/spurious edges.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Finite-sample noise / unstable edges (spurious correlations that do not replicate across resamples).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical frequency across bootstrap resamples; edges/graphs with low occurrence are treated as less credible.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Frequency weighting in the empirical posterior; rare (likely spurious) graphs/edges have low weight.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Edges with low bootstrap frequency can be considered refuted or flagged as unreliable; the method does not perform interventional refutation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Boot Notears often performs well on AUROC in the experiments reported, indicating good edge-probability estimation for some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Compared to VCN, DAG Bootstrap can be slower (significantly) and its support is restricted to graphs discovered during bootstrap, limiting full posterior coverage; performance varies with base algorithm and computational budget.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Bootstrapping helps identify unstable (spurious) edges by assigning them low empirical frequency, making the empirical posterior robust to some distractors; however, it is limited to the set of graphs found by the base algorithm and can be computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e987.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMAP MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimal I-map MCMC (structure MCMC for causal DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improved structure MCMC sampler designed for scalable discovery of DAG structure by sampling graph structures from a posterior-like energy landscape with specialized moves and heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minimal i-map mcmc for scalable structure discovery in causal dag models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>IMAP MCMC</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>A Markov Chain Monte Carlo method that samples DAG structures (or equivalence classes) using tailored proposal moves (e.g., edge reversals, additions, deletions) and heuristics to explore high-probability regions of the graph space; used to approximate the posterior over graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic ER graphs; Dream4 in-silico dataset (used as a baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Operates on observational datasets and provides sampled graphs that approximate the posterior; computational cost depends on chain length and thinning.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>VCN compares favourably to IMAP MCMC in many settings; IMAP MCMC is a competitive baseline but can be slower and requires careful tuning (chain length, burn-in).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MCMC-based approaches approximate the posterior but may be computationally slower; VCN provides a favourable runtime vs performance tradeoff compared to IMAP MCMC in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e987.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that identifies causal predictors by testing which conditional relationships remain invariant across multiple environments or interventions, thereby rejecting predictors that are spuriously correlated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal inference by using invariant prediction: identification and confidence intervals</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant Causal Prediction (ICP)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Assumes access to multiple environments (observational or interventional) and searches for sets of predictors whose conditional distribution for a target variable remains invariant across environments; uses conditional independence/invariance tests to accept/reject predictor sets, providing confidence intervals for causal effects.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Multiple environments (observational and interventional), applicable to interactive/virtual lab settings where interventions produce different environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>ICP explicitly requires or benefits from data collected across environments (e.g., interventions, perturbations) and is thus well-suited to virtual labs or interactive experiments where different interventions can be performed.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Invariant testing / conditional independence across environments to exclude predictors whose association is not stable — effectively detects and rejects spurious predictors that change under interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlates whose predictive relationship is not invariant across interventions/environments (e.g., confounding that is environment-dependent, selection bias manifesting differently across settings).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Statistical tests of invariance / conditional independence across environments (hypothesis tests for invariance of residual distributions when regressing target on candidate parents).</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Rejects non-invariant predictors (they are not included in the accepted parent set); no probabilistic downweighting in the original formulation but extensions provide confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Refutation occurs by failing invariance tests: predictors failing invariance are ruled out as causal parents.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>ICP is specifically designed to be robust to spurious correlations by exploiting variation across environments; the paper cites ICP as a related approach but does not evaluate it empirically in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ICP provides a principled way to detect and rule out spurious predictors by leveraging multiple environments or interventions — a natural fit for virtual labs/interactive settings where experiments can create environment variation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e987.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e987.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABCD strategy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ABCDStrategy: Budgeted experimental design for targeted causal structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach for selecting interventions under a budget to efficiently discover causal structure by optimising which experiments/perturbations to run to maximally reduce uncertainty about the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>abcdstrategy: Budgeted experimental design for targeted causal structure discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Budgeted experimental design (ABCD strategy)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Formulates intervention selection as a budgeted experimental design problem: given a budget on interventions, choose which variable interventions to perform to maximally reduce uncertainty about edges or to learn target parts of the graph; typically relies on approximate posteriors or scoring functions to evaluate the expected information gain from candidate interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Virtual/interactive experimental settings where interventions can be run under a cost/budget constraint (e.g., biological experiments, simulated labs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Designed for interactive experimental platforms and virtual labs where interventions are possible but costly, enabling targeted and efficient discovery of causal relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Active intervention selection to disambiguate spurious correlations by choosing experiments that directly test competing causal hypotheses, thus resolving distractors via interventions rather than purely observational criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Spurious correlations/confounding that are unresolved by observational data; the method aims to reduce such ambiguity by targeted interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Selection of interventions is driven by expected reduction in posterior uncertainty or other information-theoretic heuristics; detecting spuriousness arises from intervention outcomes that contradict observational associations.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Spurious hypotheses get reduced posterior mass after informative interventions are observed; the design strategy aims to rapidly downweight incorrect graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Interventional outcomes that conflict with a candidate causal graph refute it and reduce its posterior weight; the ABCD strategy chooses interventions expected to be maximally refuting for ambiguous hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td>Budgeted experimental design: choose interventions to maximise expected information gain about the graph under a constrained budget.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Not empirically evaluated within this paper (referenced as motivation); cited as a method that can leverage posterior uncertainty (such as from VCN) to robustly eliminate spurious correlations with few interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Active, budgeted intervention selection is a principled way to refute spurious observational correlations: the paper cites ABCD strategy as an example of how posterior uncertainty from Bayesian methods (like VCN) can be used to prioritise interventions to resolve distractors efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Variational Causal Networks: Approximate Bayesian Inference over Causal Structures', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dags with no tears: Continuous optimization for structure learning <em>(Rating: 2)</em></li>
                <li>Causal inference by using invariant prediction: identification and confidence intervals <em>(Rating: 2)</em></li>
                <li>abcdstrategy: Budgeted experimental design for targeted causal structure discovery <em>(Rating: 2)</em></li>
                <li>Learning neural causal models from unknown interventions <em>(Rating: 2)</em></li>
                <li>Minimal i-map mcmc for scalable structure discovery in causal dag models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-987",
    "paper_id": "paper-7fef0b38926fdb31cc8b8d50a746bd6ec8f19371",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "VCN",
            "name_full": "Variational Causal Networks",
            "brief_description": "A variational Bayesian method to approximate the posterior over DAG structures by parameterising a flexible autoregressive distribution (LSTM) over adjacency matrices and maximising an ELBO with a DAG-aware Gibbs prior and closed-form marginal likelihood (BGe score).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Variational Causal Networks (VCN)",
            "method_description": "Performs variational inference over discrete DAGs by modelling the variational posterior q_phi(G) as an autoregressive distribution over the non-diagonal entries of the adjacency matrix (implemented with an LSTM). The ELBO is E_q[log p(D | G)] - KL(q || p(G)); the marginal likelihood p(D | G) is computed in closed form for linear Gaussian SCMs via the BGe score. The prior p(G) is a Gibbs distribution that encodes a NOTEARS-style DAG constraint (trace(exp(A)) - d) and optional sparsity regulariser. Gradients of the ELBO use the score-function estimator (REINFORCE) with baselines; sampling from q_phi enables Monte Carlo ELBO estimates.",
            "environment_name": "Synthetic ER graphs; Dream4 in-silico gene-expression dataset (in-silico/virtual lab)",
            "environment_description": "Evaluations use synthetic Erdős–Rényi random DAGs (data generated by linear Gaussian SCMs) and the Dream4 in-silico multifactorial gene-expression dataset (10 nodes, in-silico experiments). The framework is designed to operate on purely observational data and to be used to plan interventions (active experimental design) to disambiguate causal structure.",
            "handles_distractors": true,
            "distractor_handling_technique": "Indirect handling via (1) Bayesian marginalisation which expresses uncertainty about edges and thus downweights spurious edges by low posterior mass, (2) explicit sparsity regulariser in the Gibbs prior (L1 penalty) to discourage fully connected (spurious) graphs, and (3) a DAG constraint prior to prevent cycle-induced spurious structures; additionally the posterior can be used to prioritise interventions to resolve spurious correlations.",
            "spurious_signal_types": "Irrelevant variables / spurious edges arising from finite-sample noise and model non-identifiability; cyclic/overconnected graphs (structures with spurious edges); general non-identifiability due to observational data.",
            "detection_method": "Implicit: spurious edges are identified via low marginal/ posterior probability under q_phi(G) (i.e., empirical edge beliefs from samples), and by the Bayesian posterior uncertainty concentrated away from incorrect edges; no explicit statistical test is proposed in the paper.",
            "downweighting_method": "KL regularisation to the Gibbs prior (which penalises cyclic and dense graphs) and the L1 sparsity term in the prior; marginalisation over graphs/parameters causes spurious edges to receive low posterior probability.",
            "refutation_method": "Refutation is enabled conceptually by (a) low posterior mass on suspect edges (allowing one to 'refute' with low confidence), and (b) using the posterior to select informative interventions/experiments (active design, as discussed and referenced) to disambiguate and empirically rule out spurious causal claims. The paper itself does not run an intervention-selection experiment but describes the capability.",
            "uses_active_learning": true,
            "inquiry_strategy": "Not implemented as a core algorithm in this paper, but the authors explicitly state that the posterior uncertainty from VCN can be leveraged to select informative interventions (referencing budgeted experimental design work). Thus the recommended strategy is posterior-based experimental design: pick interventions that maximally reduce posterior uncertainty about edges/effects.",
            "performance_with_robustness": "Qualitative: VCN (autoregressive variational posterior) achieves substantially lower Hellinger distance to the enumerated true posterior in small graphs (d ≤ 4) than a factorised posterior; in higher-dim tests VCN gives lower expected Structural Hamming Distance (E[SHD]) and higher AUROC than the factorised baseline, and compares favourably to non-differentiable baselines (IMAP MCMC, DAG bootstrap), particularly in low-sample (n=10) regimes where spurious correlations are more common. Exact numeric results are reported in the paper figures but not tabulated in text.",
            "performance_without_robustness": "The main non-robust comparator is the factorised Bernoulli variational posterior: it yields worse reconstruction of the true multimodal posterior (higher Hellinger distance), higher E[SHD], and lower AUROC than VCN across evaluated settings. Compared to IMAP MCMC and DAG Bootstrap, VCN is faster (runtime advantage) while achieving comparable or better metrics in many settings, especially n=10.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Using an autoregressive variational family (LSTM) substantially improves approximation of the multimodal posterior over DAGs versus a factorised Bernoulli posterior, which helps in finite-sample regimes where spurious correlations and non-identifiability create multiple plausible graphs. Incorporating a DAG-aware Gibbs prior (NOTEARS constraint) and sparsity regularisation steers posterior mass away from cyclic/overconnected (spurious) graphs. The posterior uncertainty produced by VCN can be exploited to select interventions to resolve spurious correlations, making it a practical tool for interactive/virtual-lab experimental design.",
            "uuid": "e987.0",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Factorised baseline",
            "name_full": "Factorised Bernoulli variational posterior (Ke et al.)",
            "brief_description": "A variational posterior that models each adjacency entry as an independent Bernoulli; easy to optimise but cannot capture dependencies between edges or multimodal posteriors arising from non-identifiability.",
            "citation_title": "Learning neural causal models from unknown interventions",
            "mention_or_use": "use",
            "method_name": "Factorised Bernoulli variational posterior",
            "method_description": "Represents the adjacency matrix entries as independent Bernoulli random variables (fully factorised q). Each edge probability is modelled independently and learned by optimising an ELBO-like objective; this ignores structural dependencies (e.g. mutual exclusion of opposite directed edges) and cannot model multimodality in the posterior.",
            "environment_name": "Synthetic ER graphs; Dream4 in-silico dataset (same experimental settings as VCN)",
            "environment_description": "Evaluated on the same synthetic and real (in-silico) datasets as VCN to compare posterior approximation quality and downstream metrics.",
            "handles_distractors": false,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": "Per paper: performs worse than VCN (higher Hellinger distance to true posterior, higher E[SHD], lower AUROC); fails to capture multimodal posteriors in non-identifiable settings.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "A factorised variational posterior is insufficient to capture multimodal posteriors caused by non-identifiability, leading to poorer robustness to spurious, sample-dependent edges compared to an autoregressive variational family.",
            "uuid": "e987.1",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "NOTEARS",
            "name_full": "DAGs with NO TEARS (continuous optimisation for structure learning)",
            "brief_description": "Converts combinatorial DAG search into a continuous constrained optimisation by relaxing adjacency to weighted matrix W with a smooth acyclicity constraint tr(exp(W)) - d = 0, enabling gradient-based optimisation.",
            "citation_title": "Dags with no tears: Continuous optimization for structure learning",
            "mention_or_use": "use",
            "method_name": "NOTEARS continuous optimisation",
            "method_description": "Reparameterises the adjacency as a real-valued weight matrix W and casts structure learning as continuous optimisation of a score F(W,D) subject to a smooth acyclicity constraint given by tr(exp(W)) - d = 0, solvable with constrained optimisation (e.g., augmented Lagrangian). In this paper, NOTEARS is used both as a baseline causal discovery algorithm and as the basis for a Gibbs prior that penalises cyclic graphs.",
            "environment_name": "Synthetic ER graphs; used within DAG Bootstrap baseline and as a prior term in VCN",
            "environment_description": "Applied to purely observational datasets; when used in bootstrap, NOTEARS is run on resampled datasets to generate candidate DAGs.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": "When used as a prior (Gibbs prior with DAG constraint) it downweights graphs that violate acyclicity and, together with sparsity penalty, penalises overconnected graphs that could reflect spurious edges.",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Used as baseline; bootstrapped NOTEARS (Boot Notears) performed well on AUROC in the paper's experiments, though DAG Bootstrap has limitations (support limited to graphs found by bootstrap).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "NOTEARS provides a differentiable DAG constraint used to (a) form a Gibbs prior that discourages cyclic and dense graphs (thereby reducing spurious cycle-induced structures) and (b) act as a competitive baseline when used in bootstrapping, performing well on AUROC though limited by support and runtime.",
            "uuid": "e987.2",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "DAG Bootstrap",
            "name_full": "DAG bootstrap (Boot Notears / Boot LiNGAM)",
            "brief_description": "Estimate posterior over DAGs by resampling the data many times, running an off-the-shelf causal discovery algorithm on each resample, and aggregating frequency counts over discovered graphs.",
            "citation_title": "abcdstrategy: Budgeted experimental design for targeted causal structure discovery",
            "mention_or_use": "use",
            "method_name": "DAG Bootstrap",
            "method_description": "Resample the dataset with replacement many times (bootstrap), run a causal discovery algorithm (e.g., NOTEARS or LiNGAM) on each resampled dataset, collect the set of discovered graphs, and form an empirical posterior by normalised frequency counts over these graphs. Different base algorithms give variants (Boot Notears, Boot LiNGAM).",
            "environment_name": "Synthetic ER graphs; Dream4 in-silico dataset (evaluated as a baseline)",
            "environment_description": "Non-interactive bootstrap procedure applied to observational datasets; can be computationally expensive because it runs the base discovery algorithm many times.",
            "handles_distractors": true,
            "distractor_handling_technique": "Empirical frequency-based robustness: spurious edges that appear only in a few bootstrap resamples receive low frequency/weight in the empirical posterior; thus bootstrapping downweights unstable/spurious edges.",
            "spurious_signal_types": "Finite-sample noise / unstable edges (spurious correlations that do not replicate across resamples).",
            "detection_method": "Empirical frequency across bootstrap resamples; edges/graphs with low occurrence are treated as less credible.",
            "downweighting_method": "Frequency weighting in the empirical posterior; rare (likely spurious) graphs/edges have low weight.",
            "refutation_method": "Edges with low bootstrap frequency can be considered refuted or flagged as unreliable; the method does not perform interventional refutation.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Boot Notears often performs well on AUROC in the experiments reported, indicating good edge-probability estimation for some datasets.",
            "performance_without_robustness": "Compared to VCN, DAG Bootstrap can be slower (significantly) and its support is restricted to graphs discovered during bootstrap, limiting full posterior coverage; performance varies with base algorithm and computational budget.",
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "Bootstrapping helps identify unstable (spurious) edges by assigning them low empirical frequency, making the empirical posterior robust to some distractors; however, it is limited to the set of graphs found by the base algorithm and can be computationally expensive.",
            "uuid": "e987.3",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "IMAP MCMC",
            "name_full": "Minimal I-map MCMC (structure MCMC for causal DAGs)",
            "brief_description": "An improved structure MCMC sampler designed for scalable discovery of DAG structure by sampling graph structures from a posterior-like energy landscape with specialized moves and heuristics.",
            "citation_title": "Minimal i-map mcmc for scalable structure discovery in causal dag models",
            "mention_or_use": "use",
            "method_name": "IMAP MCMC",
            "method_description": "A Markov Chain Monte Carlo method that samples DAG structures (or equivalence classes) using tailored proposal moves (e.g., edge reversals, additions, deletions) and heuristics to explore high-probability regions of the graph space; used to approximate the posterior over graphs.",
            "environment_name": "Synthetic ER graphs; Dream4 in-silico dataset (used as a baseline in experiments)",
            "environment_description": "Operates on observational datasets and provides sampled graphs that approximate the posterior; computational cost depends on chain length and thinning.",
            "handles_distractors": null,
            "distractor_handling_technique": null,
            "spurious_signal_types": null,
            "detection_method": null,
            "downweighting_method": null,
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "VCN compares favourably to IMAP MCMC in many settings; IMAP MCMC is a competitive baseline but can be slower and requires careful tuning (chain length, burn-in).",
            "performance_without_robustness": null,
            "has_ablation_study": false,
            "number_of_distractors": null,
            "key_findings": "MCMC-based approaches approximate the posterior but may be computationally slower; VCN provides a favourable runtime vs performance tradeoff compared to IMAP MCMC in the experiments.",
            "uuid": "e987.4",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "ICP",
            "name_full": "Invariant Causal Prediction (ICP)",
            "brief_description": "A method that identifies causal predictors by testing which conditional relationships remain invariant across multiple environments or interventions, thereby rejecting predictors that are spuriously correlated.",
            "citation_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "mention_or_use": "mention",
            "method_name": "Invariant Causal Prediction (ICP)",
            "method_description": "Assumes access to multiple environments (observational or interventional) and searches for sets of predictors whose conditional distribution for a target variable remains invariant across environments; uses conditional independence/invariance tests to accept/reject predictor sets, providing confidence intervals for causal effects.",
            "environment_name": "Multiple environments (observational and interventional), applicable to interactive/virtual lab settings where interventions produce different environments",
            "environment_description": "ICP explicitly requires or benefits from data collected across environments (e.g., interventions, perturbations) and is thus well-suited to virtual labs or interactive experiments where different interventions can be performed.",
            "handles_distractors": true,
            "distractor_handling_technique": "Invariant testing / conditional independence across environments to exclude predictors whose association is not stable — effectively detects and rejects spurious predictors that change under interventions.",
            "spurious_signal_types": "Spurious correlates whose predictive relationship is not invariant across interventions/environments (e.g., confounding that is environment-dependent, selection bias manifesting differently across settings).",
            "detection_method": "Statistical tests of invariance / conditional independence across environments (hypothesis tests for invariance of residual distributions when regressing target on candidate parents).",
            "downweighting_method": "Rejects non-invariant predictors (they are not included in the accepted parent set); no probabilistic downweighting in the original formulation but extensions provide confidence intervals.",
            "refutation_method": "Refutation occurs by failing invariance tests: predictors failing invariance are ruled out as causal parents.",
            "uses_active_learning": null,
            "inquiry_strategy": null,
            "performance_with_robustness": "ICP is specifically designed to be robust to spurious correlations by exploiting variation across environments; the paper cites ICP as a related approach but does not evaluate it empirically in their experiments.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "ICP provides a principled way to detect and rule out spurious predictors by leveraging multiple environments or interventions — a natural fit for virtual labs/interactive settings where experiments can create environment variation.",
            "uuid": "e987.5",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "ABCD strategy",
            "name_full": "ABCDStrategy: Budgeted experimental design for targeted causal structure discovery",
            "brief_description": "An approach for selecting interventions under a budget to efficiently discover causal structure by optimising which experiments/perturbations to run to maximally reduce uncertainty about the graph.",
            "citation_title": "abcdstrategy: Budgeted experimental design for targeted causal structure discovery",
            "mention_or_use": "mention",
            "method_name": "Budgeted experimental design (ABCD strategy)",
            "method_description": "Formulates intervention selection as a budgeted experimental design problem: given a budget on interventions, choose which variable interventions to perform to maximally reduce uncertainty about edges or to learn target parts of the graph; typically relies on approximate posteriors or scoring functions to evaluate the expected information gain from candidate interventions.",
            "environment_name": "Virtual/interactive experimental settings where interventions can be run under a cost/budget constraint (e.g., biological experiments, simulated labs)",
            "environment_description": "Designed for interactive experimental platforms and virtual labs where interventions are possible but costly, enabling targeted and efficient discovery of causal relationships.",
            "handles_distractors": true,
            "distractor_handling_technique": "Active intervention selection to disambiguate spurious correlations by choosing experiments that directly test competing causal hypotheses, thus resolving distractors via interventions rather than purely observational criteria.",
            "spurious_signal_types": "Spurious correlations/confounding that are unresolved by observational data; the method aims to reduce such ambiguity by targeted interventions.",
            "detection_method": "Selection of interventions is driven by expected reduction in posterior uncertainty or other information-theoretic heuristics; detecting spuriousness arises from intervention outcomes that contradict observational associations.",
            "downweighting_method": "Spurious hypotheses get reduced posterior mass after informative interventions are observed; the design strategy aims to rapidly downweight incorrect graphs.",
            "refutation_method": "Interventional outcomes that conflict with a candidate causal graph refute it and reduce its posterior weight; the ABCD strategy chooses interventions expected to be maximally refuting for ambiguous hypotheses.",
            "uses_active_learning": true,
            "inquiry_strategy": "Budgeted experimental design: choose interventions to maximise expected information gain about the graph under a constrained budget.",
            "performance_with_robustness": "Not empirically evaluated within this paper (referenced as motivation); cited as a method that can leverage posterior uncertainty (such as from VCN) to robustly eliminate spurious correlations with few interventions.",
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Active, budgeted intervention selection is a principled way to refute spurious observational correlations: the paper cites ABCD strategy as an example of how posterior uncertainty from Bayesian methods (like VCN) can be used to prioritise interventions to resolve distractors efficiently.",
            "uuid": "e987.6",
            "source_info": {
                "paper_title": "Variational Causal Networks: Approximate Bayesian Inference over Causal Structures",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dags with no tears: Continuous optimization for structure learning",
            "rating": 2
        },
        {
            "paper_title": "Causal inference by using invariant prediction: identification and confidence intervals",
            "rating": 2
        },
        {
            "paper_title": "abcdstrategy: Budgeted experimental design for targeted causal structure discovery",
            "rating": 2
        },
        {
            "paper_title": "Learning neural causal models from unknown interventions",
            "rating": 2
        },
        {
            "paper_title": "Minimal i-map mcmc for scalable structure discovery in causal dag models",
            "rating": 1
        }
    ],
    "cost": 0.017510249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Variational Causal Networks: Approximate Bayesian Inference over Causal Structures</h1>
<p>Yashas Annadani ${ }^{1}$, Jonas Rothfuss ${ }^{1}$, Alexandre Lacoste ${ }^{3}$, Nino Scherrer ${ }^{1}$, Anirudh Goyal ${ }^{2}$, Yoshua Bengio ${ }^{2}$, Stefan Bauer ${ }^{4,5}$<br>${ }^{1}$ ETH Zurich, ${ }^{2}$ Mila, Université de Montréal, ${ }^{3}$ ElementAI/ ServiceNow, ${ }^{4}$ MPI for Intelligent Systems ${ }^{5}$ CIFAR Azrieli Global Scholar</p>
<h4>Abstract</h4>
<p>Learning the causal structure that underlies data is a crucial step towards robust real-world decision making. The majority of existing work in causal inference focuses on determining a single directed acyclic graph (DAG) or a Markov equivalence class thereof. However, a crucial aspect to acting intelligently upon the knowledge about causal structure which has been inferred from finite data demands reasoning about its uncertainty. For instance, planning interventions to find out more about the causal mechanisms that govern our data requires quantifying epistemic uncertainty over DAGs. While Bayesian causal inference allows to do so, the posterior over DAGs becomes intractable even for a small number of variables. Aiming to overcome this issue, we propose a form of variational inference over the graphs of Structural Causal Models (SCMs). To this end, we introduce a parametric variational family modelled by an autoregressive distribution over the space of discrete DAGs. Its number of parameters does not grow exponentially with the number of variables and can be tractably learned by maximising an Evidence Lower Bound (ELBO). In our experiments, we demonstrate that the proposed variational posterior is able to provide a good approximation of the true posterior.</p>
<p>Moving from learning correlation and association in data to causation is a critical step towards increased robustness, interpretability and real-world decision-making [27, 34]. Doing so entails learning the causal structure underlying the data generating process. Causal inference is concerned with determining the causal structure of a set of random variables from data, commonly represented as a directed acycilc graph (DAG) [29]. While Structural Causal Models (SCMs) provide a generative model over the data, they are hard to learn from data due to the non-identifiability of the causal models without interventional data [7] and the combinatorial nature of the space of DAGs [15]. Even with infinite amount of data, recovering the causal structure is intrinsically hard since a DAG is only identifiable up to its Markov equivalence class (MEC) and the space of possible DAGs grows super-exponentially with the number of variables. While the majority of work on causal inference $[6,5,38]$ deals with getting a single underlying causal structure without a probabilistic treatment, quantifying the epistemic uncertainty in case of non-identifiability is crucial and is not possible in these approaches.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Schematic Diagram of Variational Inference on SCMs with VCN.</p>
<p>In this work, we take a Bayesian approach to causal structure learning. Given only finite observational data, a Bayesian approach allows us to quantify the uncertainty in the causal structure of the data generating process, even before performing interventions. Having such a framework over causal structures can help further downstream tasks on graph learning and causal inference. For example, we can leverage the model’s uncertainty to select informative interventions and discover the full graph with minimal amount of interventions [2]. Also, for estimating the causal effect of certain variables, it is often not required to know the full graph. Hence, by marginalising the posterior we can uncover the confidence we have about a specific causal effect, which may already fall below a specified tolerance level. Having such a model is very desirable as interventions are hard to perform, sometimes unethical and even impossible [29].</p>
<p>A key component in learning causal structures is the identifiability under the availability of observational data. While some assumptions are always required to say anything about the underlying causal process, additional assumptions are sometimes made to especially make the causal model identifiable from (observational) data. These additional assumptions are not necessarily part of the data generating process, and hence the recovered causal structure may be incorrect due to model misspecification. In addition, identifiability results are usually asymptotic in the number of samples. Causal discovery in a limited sample regime calls for active learning to perform interventions to improve identifiability. Such setups benefit from probabilistic reasoning about unknown causal structures. We are interested in such settings and we propose a Bayesian framework for linear SCM’s with additive noise which can quantify the uncertainty in the learned causal structure.</p>
<p>Our contributions are as follows:</p>
<ul>
<li>We perform Bayesian inference over the unknown causal structures. The posterior over this distribution is intractable. Therefore, we perform variational inference and demonstrate how we can model the variational family over this distribution.</li>
<li>The key contribution is to model distributions over adjacency matrices of the causal structures with an autoregressive distribution using an LSTM.</li>
<li>Empirically demonstrate the performance of our modeling choice.</li>
<li>Evaluating Bayesian causal inference techniques are hard in practice. We discuss the difficulty entailed in evaluation, as well as provide insights which alleviate this problem.</li>
</ul>
<h2>1 Problem Setting</h2>
<p>Causal modeling Consider a set of $d$ random variables $\mathbf{X}:=\left{X_{1}, \ldots, X_{d}\right}$. A Structural Causal Model (SCM) [29] over $\mathbf{X}$ is defined as set of structural assignments</p>
<p>$$
X_{i}:=f_{i}\left(\mathbf{x}<em _mathcal_G="\mathcal{G">{\pi</em>\right), i=1, \ldots, d
$$}}(i)}, \epsilon_{i</p>
<p>corresponding to a Directed Acyclic Graph (DAG) $\mathcal{G}$ with vertices $\mathbf{X}$. In here, $\pi_{\mathcal{G}}(i)$ are the parents of $X_{i}$ in $\mathcal{G}, \epsilon_{i}$ are independent noise variables with probability density $P_{\epsilon_{i}}$ and the $f_{i}$ 's are (potentially non-linear) functions. The SCM entails a joint probability distribution $P_{\mathbf{x}}$ over the random variables. In this work, we assume that all the endogenous variables are observed, that is, there are no hidden confounders.</p>
<p>A popular instantiation of this generic framework are linear SCMs with additive noise, given by:</p>
<p>$$
x_{i}:=\boldsymbol{\theta}<em _pi__mathcal_G="\pi_{\mathcal{G">{i}^{T} \mathbf{x}</em>}}(i)}+\epsilon_{i} \quad \boldsymbol{\theta<em _mathcal_G="\mathcal{G">{i} \in \mathbb{R}^{\left|\pi</em>
$$}}(i)\right|</p>
<p>wherein $\boldsymbol{\theta}<em i="i">{i}$ are parameters (edge weights) of the linear functions $f</em>$. Alternatively, this can be written as</p>
<p>$$
x_{i}:=\boldsymbol{\theta}<em _mathcal_G="\mathcal{G">{i}^{T}\left(\mathbf{X} \circ \mathbf{A}</em><em i="i">{i}}\right)+\epsilon</em>} \quad \boldsymbol{\theta<em _mathcal_G="\mathcal{G">{i}, \mathbf{A}</em>
$$}_{i}} \in \mathbb{R}^{d</p>
<p>where $\circ$ corresponds to elementwise product, $\mathbf{A}<em i="i">{\mathcal{G}</em>$. We restrict our further exposition to continuous random variables. However, the framework presented in the remainder of the paper applies to discrete random variables as well. We focus on linear SCMs with Gaussian variables, as they are non-identifiable in this setting and hence obtaining uncertainty estimates is useful. Non-linear additive noise SCM's with Gaussian variables are identifiable, and hence a single point estimate suffices.}}$ is the $i^{\text {th }}$ row of $\mathbf{A}_{\mathcal{G}} \in \mathbb{R}^{d \times d}$, the $(0,1)$-adjacency matrix of $\mathcal{G</p>
<p>NOTEARS - Causal inference as continuous optimization problem In practice, we often have data while the causal structure that underlies the data generating process is unknown to us. Causal discovery is concerned with recovering this causal structure, e.g. in the form of an SCM, from observational data or interventional data (or both). Assuming a linear SCM, this coincides with estimating the graph $\mathcal{G}$, i.e. its adjacency matrix $\mathbf{A}$, and its corresponding edge weights $\boldsymbol{\theta}=\left{\boldsymbol{\theta}<em d="d">{1}, \ldots, \boldsymbol{\theta}</em>}\right}$, given data $\mathcal{D}:=\left{\mathbf{x<em n="n">{1}, \ldots, \mathbf{x}</em>$.
In score-based causal discovery, we use a score function $F:{0,1}^{d \times d} \times \mathcal{X}^{n} \mapsto \mathbb{R}$ to find the graph that best corresponds to the data as follows:}\right} \in \mathcal{X}^{n</p>
<p>$$
\underset{\mathbf{A} \in{0,1}^{d \times d}}{\arg \min } F(\mathbf{A}, \mathcal{D}) \quad \text { s.t. } \mathcal{G}(\mathbf{A}) \in \operatorname{DAG}(d)
$$</p>
<p>where $\mathbf{A}$ is the ${0,1}$ adjacency matrix of graph $\mathcal{G}$.
However, since the search space of DAGs of this combinatorial optimization problem grows in the order of $\mathcal{O}\left(2^{d^{2}}\right)$, solving (4) becomes infeasible even for a relatively small number of variables $d$.</p>
<p>Addressing this issue, Zheng et al. [38] propose an alternative formulation that converts the combinatorial problem into the following continuous program:</p>
<p>$$
\underset{\boldsymbol{W} \in \mathbb{R}^{d \times d}}{\arg \min } F(\boldsymbol{W}, \mathcal{D}) \quad \text { s.t. } \operatorname{tr}\left(e^{\boldsymbol{W}}\right)-d=0
$$</p>
<p>that can be solved with standard tools of constrained optimization such as the Lagrange method. In here, we have converted the adjacency matrix $\mathbf{A} \in{0,1}^{d \times d}$ into a weighted adjacency matrix $\boldsymbol{W} \in \mathbb{R}^{d \times d}$ such that $a_{i, j}=1 \Leftrightarrow w_{i, j} \neq 0$.</p>
<p>Bayesian inference over DAGs An important aspect towards acting intelligently upon the knowledge about the causal structure which has been inferred from finite data demands reasoning about its uncertainty. For instance, in scientific inquiry, planning interventional experiments to find out more about the causal mechanisms that govern a data-generating system of interest requires quantifying epistemic uncertainty over DAGs. Bayesian inference gives us a principled framework for the treatment of such uncertainty (Figure 1).</p>
<p>In the Bayesian framework, we encode our prior structural knowledge in form of a prior $p(\mathcal{G})$ over DAGs. By combining the prior with the likelihood $p(\mathcal{D} \mid \mathcal{G})$ through Bayes' rule, we obtain a posterior distribution over DAGs:</p>
<p>$$
p(\mathcal{G} \mid \mathcal{D})=\frac{p(\mathcal{D} \mid \mathcal{G}) p(\mathcal{G})}{p(\mathcal{D})}
$$</p>
<p>where $p(\mathcal{D})=\sum_{\mathcal{G}} \int_{\theta} p(\mathbf{x} \mid \mathcal{G}, \boldsymbol{\theta}) p(\boldsymbol{\theta} \mid \mathcal{G}) p(\mathcal{G}) d \boldsymbol{\theta}$ is the model evidence. A key issue with (6) is the intractable evidence term in the denominator. Even if the integral over $\boldsymbol{\theta}$ can be solved analytically such as in the case of linear structural equations with Gaussian prior and likelihood with standard parameter priors [10], the sum over possible DAGs grows in the order of $\mathcal{O}\left(2^{d^{2}}\right)$, making its computation infeasible even for a small number of variables $d$.</p>
<h1>2 Variational Causal Networks</h1>
<p>In this section, we present our variational inference (VI) framework for approximating Bayesian posteriors over causal structures (Equation 6). First, we derive the evidence lower bound and sketch out how to perform VI on DAGs. Unlike the variational inference in a latent variable model [4], the evidence lower bound is still intractable in the case of causal models due to the superexponential number of graph configurations. Therefore, we introduce a novel parametric variational family which makes the intractable ELBO into a tractable one while still having the flexibility to model complex distributions over DAGs. Such a variational family can be modelled using autoregressive models like LSTM [17]. This particular instantiation of approximate Bayesian inference for SCMs with the presented variational family is called Variational Causal Networks (VCN).</p>
<h3>2.1 Variational Inference for Causal Structures</h3>
<p>In variational inference [4], we aim to approximate the Bayesian posterior $p(\mathcal{G} \mid \mathcal{D})$ by a variational distribution $q_{\phi}(\mathcal{G})$, parameterized by $\phi$, that has a tractable density. To learn the parameters of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Schematic Diagram of Variational Causal Networks
our variational distribution, we minimize the KL-Divergence between $q_{\phi}(\mathcal{G})$ and the true posterior $p(\mathcal{G} \mid \mathcal{D})$ which is equivalent to maximising the Evidence Lower Bound (ELBO), given by the following proposition.
Proposition 1. (ELBO) Let $q_{\phi}(\mathcal{G})$ be the variational posterior over causal structures. Then the evidence lower bound (ELBO) is given by:</p>
<p>$$
\log p(\mathcal{D}) \geq \mathcal{L}(\phi ; \mathcal{D})=\underset{q_{\phi}(\mathcal{G})}{\mathbb{E}}[\log p(\mathcal{D} \mid \mathcal{G})]-\mathrm{D}<em _phi="\phi">{\mathrm{KL}}\left(q</em>)\right)
$$}(\mathcal{G}) | p(\mathcal{G</p>
<p>To maximise the ELBO, we estimate the gradients $\nabla_{{\phi\rangle} \mathcal{L}(\phi ; \mathcal{D})$ to employ a form of gradient ascent on the objective.</p>
<h1>2.2 A Variational Family for Causal Structures</h1>
<p>A key challenge to variational inference over DAGs concerns the choice of the variational distribution $q_{\phi}(\mathcal{G})$ over discrete DAGs $\mathcal{G}$. For instance, if we choose $q_{\phi}(\mathcal{G})$ naively as categorical distribution over the space of DAGs, the dimensionality of $\phi$ grows super-exponentially in the number of variables $d$ (i.e. $\mathcal{O}\left(2^{d^{2}}\right)$ ), rendering this choice impractical. Hence, we need to find a way of representing $q_{\phi}(\mathcal{G})$ that is not combinatorial in its nature while ensuring a rich family of distributions over graphs. In addition, to be able to compute Monte Carlo gradient estimates of the ELBO in (7), $q_{\phi}(\mathcal{G})$ needs to have a tractable probability density.
Aiming to fulfil these requirements, we represent graphs by their adjacency matrix $\mathbf{A} \in{0,1}^{d \times d}$ and model $q_{\phi}$ as a discrete distribution over such ${0,1}$-matrices. While Ke et al. [19] represent each entry of the adjacency matrix as independent Bernoulli variable, such approach is insufficient to capture any dependencies between the entries of the adjacency matrix which are necessary to ensure that $q(\mathbf{A})$ only assigns positive probabilities to adjacency matrices corresponding to DAGs. For instance, to avoid cycles of length two, $(\mathbf{A})<em i="i" j_="j,">{i, j}$ needs to have a strong negative dependency $(\mathbf{A})</em>$, i.e., if there is a directed edge $i \rightarrow j$ there must be no edge $j \rightarrow i$, and reverse. In addition, a factorisable distribution can only capture unimodal distributions while the posterior over causal structures in the non-identifiable case could have exponential number of modes. Hence, the variational distribution needs to be parameterised such that multiple modes could be captured. In order to address these issues, we model this discrete distribution as an autoregressive distribution over entries of the adjacency matrix using an LSTM [17].
Let $q_{\phi}(\mathcal{G})=q_{\phi}\left(\mathbf{A}_{\mathcal{G}}\right)$ be defined in an autoregressive manner as follows:</p>
<p>$$
\begin{aligned}
q_{\phi}\left(\mathbf{A}<em i="1">{\mathcal{G}}\right) &amp; =\prod</em>}^{d(d-1)} q_{\phi}\left(a_{\mathcal{G<em _mathcal_G="\mathcal{G">{i}} \mid a</em><em _phi="\phi">{1: i-1}}\right) \
\text { s.t. } \quad q</em>}\left(a_{\mathcal{G<em _mathcal_G="\mathcal{G">{i}} \mid a</em><em _mathcal_G="\mathcal{G">{1: i-1}}\right) &amp; :=\text { Bernoulli }\left(a</em><em _phi="\phi">{i}} ; f</em>}\left(a_{\mathcal{G<em _mathcal_G="\mathcal{G">{1: i-1}}\right)\right), \quad \mathbf{A}</em>}}=\left{a_{\mathcal{G<em i="1">{i}}\right}</em>
\end{aligned}
$$}^{d(d-1)</p>
<p>where only the non-diagonal elements of the adjacency matrix are modelled and $f_{\phi}$ is a function which predicts the Bernoulli parameter based on the previous realisations of the entries of the</p>
<p>adjacency matrix, thus making the distribution autoregressive. We model this function using an LSTM [17].</p>
<p>Note that using an autoregressive distribution on the entries of the adjacency matrix helps to implicitly keep a distribution over super-exponential number of graphs, and also be able to sample from that distribution.</p>
<p>Prior over graph structures Choosing the appropriate prior $p(\mathcal{G})$ over graph structures is important to learn good approximation of the posterior. In causal discovery, it is usually a requirement that the search space of all graphs is a DAG. However, in the parameterisation described above, the discrete distribution is defined over all possible graphs, including the one which contains cycles. Having support over graphs with cycles and using maximum likelihood estimation results in high probability mass corresponding to a fully connected graph in the approximate posterior. Therefore, appropriate DAG regularizers are required. We employ the result of NOTEARS [38] to define a Gibbs distribution as the prior which helps us to limit the support of graphs to just DAGs. We can also encode additional assumptions about the data generating process such as sparsity. The Gibbs distribution in this case could be defined as:</p>
<p>$$
p(\mathcal{G}) \propto \exp \left(-\lambda_{t} g\left(\mathbf{A}<em s="s">{\mathcal{G}}\right)-\lambda</em>}\left|\mathbf{A<em 1="1">{\mathcal{G}}\right|</em>\right)
$$</p>
<p>where $g\left(\mathbf{A}<em _mathcal_G="\mathcal{G">{\mathcal{G}}\right)$ is the DAG constraint given by the matrix exponential [38], i.e $g\left(\mathbf{A}</em>}}\right)=\operatorname{tr}\left[e^{\mathbf{A<em t="t">{\mathcal{G}}}\right]-d$. Matrix binomial can also be used to define $g$, as given in [37]. The second term in the Gibbs distribution corresponds to the sparsity constraint. $\lambda</em> \rightarrow \infty$ if graphs with cycles should have zero probability mass under the posterior. However, any large value should still give a reasonable prior with very low probabilities for cyclic graphs.}$ and $\lambda_{s}$ are tunable hyperparameters. Typically, $\lambda_{t</p>
<p>Marginal Likelihood We use standard parameter priors which ensure marginal likelihood $\log p(\mathcal{D} \mid \mathcal{G})$ is "score-equivalent", i.e. all the DAGs which are in the same MEC have the same marginal likelihood. This can be ensured with a Gaussian-Wishart prior over the parameters $\boldsymbol{\theta}$ and the marginalisation can be done in closed form [10, 23] (called as the BGe score). Using this parameter prior to calculate marginal likelihood in the ELBO makes the ELBO score-equivalent. Details are given in Appendix B.3.</p>
<h1>2.3 Estimating Gradients</h1>
<p>Since the KL regulariser is defined on a discrete distribution with many parameters, obtaining a Monte Carlo estimate of this term (and hence the corresponding ELBO) requires unbiased gradients of $\phi$. Therefore, we use the score-function gradient estimator [36] with exponential moving average baseline for obtaining the gradients. As the score function estimator does not require the estimand function to be differentiable, we can perform closed form marginalisation of the parameters with the standard parameter priors like that of Gaussian-Wishart.</p>
<p>The detailed algorithm is given in Algorithm 1.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mh">1</span><span class="w"> </span><span class="n">VCN</span><span class="w"> </span><span class="n">Algorithm</span>
<span class="w">    </span><span class="n">\(\phi</span><span class="w"> </span><span class="n">\leftarrow\)</span><span class="w"> </span><span class="n">initialise</span><span class="w"> </span><span class="n">parameters</span>
<span class="w">    </span><span class="k">repeat</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">\(L\)</span><span class="w"> </span><span class="n">graphs</span><span class="w"> </span><span class="n">\(\left\{\mathcal{G}^{(i)}\right\}_{i=1}^{L}\)</span><span class="w"> </span><span class="n">autoregressively</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">\(q_{\phi}\left(\mathbf{A}_{\mathcal{G}}\right)\)</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">LSTM</span><span class="p">.</span>
<span class="w">        </span><span class="n">Calculate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">per</span><span class="o">-</span><span class="n">sample</span><span class="w"> </span><span class="n">marginal</span><span class="w"> </span><span class="n">log</span><span class="o">-</span><span class="n">likelihood</span><span class="w"> </span><span class="n">\(\log</span><span class="w"> </span><span class="n">p\left(\mathcal{D}</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">\mathcal{G}^{(i)}\right)\)</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="n">marginalising</span><span class="w"> </span><span class="n">\(\theta\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">obtain</span>
<span class="w">        </span><span class="n">the</span><span class="w"> </span><span class="n">BGe</span><span class="w"> </span><span class="n">Score</span><span class="w"> </span><span class="n">\([10,23]\).</span>
<span class="w">        </span><span class="n">\(g</span><span class="w"> </span><span class="n">\leftarrow</span><span class="w"> </span><span class="n">\frac{1}{L}</span><span class="w"> </span><span class="n">\sum_{i=1}^{L}</span><span class="w"> </span><span class="n">\nabla_{\phi}\left[\log</span><span class="w"> </span><span class="n">p\left(\mathcal{D}</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">\mathcal{G}^{(i)}\right)-\left[\log</span><span class="w"> </span><span class="n">q_</span><span class="p">{</span><span class="n">\phi}\left(\mathcal{G}^{(i)}\right)-\log</span><span class="w"> </span><span class="n">p\left(\mathcal{G}^{(i)}\right)\right]\right]\)</span>
<span class="w">        </span><span class="n">\(\phi</span><span class="w"> </span><span class="n">\leftarrow\)</span><span class="w"> </span><span class="n">Update</span><span class="w"> </span><span class="n">parameters</span><span class="w"> </span><span class="n">using</span><span class="w"> </span><span class="n">gradients</span><span class="w"> </span><span class="n">\(g\).</span>
<span class="w">    </span><span class="n">until</span><span class="w"> </span><span class="n">\(\phi</span><span class="w"> </span><span class="n">\leftarrow\)</span><span class="w"> </span><span class="n">converge</span><span class="p">.</span>
</code></pre></div>

<h2>3 Related Work</h2>
<p>Causal Discovery: Broadly, causal structure learning approaches can be mainly categorised to two different paradigms with regards to kind of data used: methods involving learning structure from</p>
<p>purely observational data and methods involving learning structure from both observational and interventional data. Both these approaches strongly rely on the identifiability of SCM. Zheng et al. [38], Yu et al. [37], Lachapelle et al. [24] use a score-based objective with global search over DAG space to learn the structure. Lachapelle et al. [24] extend the linear model of [38] to non-linear case using neural networks. GES [6, 31] greedily searches over the space of CPDAG's and maximize a score based on Bayesian Information Criterion (BIC). LiNGAM [33] uses an Independent Component Analysis (ICA) based approach by making the assumption that either the variables or one of the noise variables are non-Gaussian. PC [34] algorithm uses a constraint based approach for causal discovery.</p>
<p>Different from the above approaches, a few approaches use a combination of observational and interventional data to learn the causal structure [13]. ICP [28] assumes a linear SCM and resorts to conditional independence testing to test the hypothesis of invariance, a concept wherein the plausible causal predictors of a given variable are stable across different interventions. [16] extend ICP to nonlinear SCMs. These methods do not infer the complete graph but instead just the causal parents of a particular variable. The difficulty of these approaches rest in the fact that conditional independence tests are hard to perform. More recently, [3, 20] and [19] use a meta-learning objective to learn the causal structure using interventional data.</p>
<p>Bayesian Approach to Structure Learning: Existing approaches for Bayesian learning of causal graphs are mainly concerned with efficient sampling of graph structures from the posterior distribution. A popular choice for such an approach is the Markov Chain Monte Carlo (MCMC) with suitably chosen energy functions and heuristics [8, 26, 25, 22, 14, 9]. While Niinimäki et al. [26] is concerned with causal discovery by sampling graph structures from partial orders of edge orientation, Madigan et al. [25] proposes an approach for learning probabilistic graphical models with discrete variables called as structure MCMC. Kuipers and Moffa [22], Grzegorczyk and Husmeier [12] and Ellis and Wong [8] propose improved MCMC techniques for sampling graph structures by restricting graph space to certain topology and sampling from a restricted space. Heckerman et al. [14] use a BIC based scoring function to learn a Bayesian network by sampling from MetropolisHastings under different energy configurations. An estimate of MAP is obtained using Maximum Likelihood to get the final graph structure. Agrawal et al. [2] use DAG bootstrapping to estimate the posterior over graphs and use this for budgeted experimental design for causal discovery. While all these techniques are intended for a Bayesian approach to structure learning, they face the problem of efficiently approximating the posterior over graph structures and hence resort to either heuristics or restricted graph structures. However, we do not make any simplifying assumptions about the graph structures and we can efficiently approximate the posterior while being able to sample exactly from them.</p>
<h1>4 Experiments</h1>
<p>To validate the modelling choice presented in the previous section, we perform experiments mainly focusing on the following aspects: (1) since we approximate the posterior, we evaluate how close is the approximation to the true posterior in lower dimensional settings $(\leq 4)$ where enumeration of true posterior is possible. (2) We outline the difficulty involved in evaluating the Bayesian Causal inference models in higher dimensions where enumeration is not possible and suggest possible metrics which alleviate the problem and quantify how well the true posterior is approximated. We further evaluate our model on these metrics.</p>
<p>Key Findings We summarise the experimental results as follows: (1) The proposed autoregressive approach of VCN performs better than a factorised distribution [19] in all the considered settings. This is due to the ability of an autoregressive distribution to model multi-modal posteriors as compared to the unimodal factorised distribution. (2) In higher dimensions, the proposed approach compares favourably to competitive baselines on various metrics. (3) VCN achieves a good approximation vs run-time trade-off, thus making it a suitable approach for Bayesian causal inference.</p>
<p>Experimental Settings We evaluate our method on both synthetic and real datasets. For generating synthetic data, we follow the procedure of NOTEARS [38]. We sample a DAG at random from an Erdos-Renyi (ER) model with expected number of edges equal to $d$. Each reported result is over 20 different random graphs. The models were trained with a learning rate of $1 e-2$ using the Adam</p>
<p>optimiser [21] for 30k epochs. We consider the settings when we have $n=10,100$ samples. For taking the Monte Carlo estimate of the ELBO, we take $L=1000$ samples. $\lambda_{s}$ is fixed to 0.01 and $\lambda_{t}$ is annealed from 10 to 1000 with an exponential annealing schedule.</p>
<p>Baselines Parameterising the posterior with a factorised distribution [19] is our main baseline as it involves differentiable causal learning based techniques similar to our approach. We also compare with DAG Bootstrap [2] where the posterior is estimated by bootstrapping the data with any causal discovery algorithm and then forming an empirical estimate of the posterior based on frequency count. We use LiNGAM [33] and NOTEARS [38] as underlying causal discovery algorithms for DAG Bootstrap. They are denoted by Boot Lingam and Boot Notears respectively. In addition, we compare with the MCMC approach of Minimal IMAP MCMC [1]. Details of experimental settings of each of these techniques is given in Appendix B. 4 .</p>
<h1>4.1 Evaluation Metrics</h1>
<p>For low dimensional variables $(d \leq 4)$, we can enumerate the true posterior for all graphs. Therefore, in this setting we compute the distance between the variational posterior learned from data and the true posterior.</p>
<p>For higher dimensional variables, where enumerating the true posterior is not possible, evaluating Bayesian causal inference algorithms is not straightforward. Directly comparing the likelihoods do not necessarily ensure that the posterior learned is good, as graphs with more edges always have higher likelihoods. Instead, we employ the following metrics:</p>
<p>Expected Structural Hamming Distance ( $\mathbb{E}[\mathbf{S H D}]$ ) Given that the true posterior has modes over all the graphs inside the MEC corresponding to the ground truth graph, we can sample the graphs from the model and then compute the Structural Hamming Distance (SHD) between these samples and the ground truth. We can then compute the empirical mean of these SHDs as a metric. As all the graphs inside the MEC have the same number of edges, this metric indicates how well the approximated posterior is close to the ground truth on average. If $\mathcal{G}_{\mathrm{GT}}$ is the ground truth data generating graph, then $\mathbb{E}[\mathrm{SHD}]$ is given by</p>
<p>$$
\underset{q_{\phi}(\mathcal{G})}{\mathbb{E}}[\mathrm{SHD}] \approx \frac{1}{T} \sum_{i=1}^{T}\left[\operatorname{SHD}\left(\mathcal{G}^{(i)}, \mathcal{G}<em _phi="\phi">{\mathrm{GT}}\right)\right] \quad \mathcal{G}^{(i)} \sim q</em>)
$$}(\mathcal{G</p>
<p>Area Under Reciever Operating Curve (AUROC) If we care for just feature probabilities of quantities like the presence of an edge, we can compute edge beliefs and compute the Receiver Operating Curve (ROC). The area under this curve can be treated as a metric for evaluating the Bayesian model. However, it does not necessarily give a full picture of the multiple modes of the posterior and whether the sampled graph is far away /close to the MEC of the true graph. Nevertheless, it can still be informative of the edge beliefs of the learned approximation. Details of computing this is give in [30].</p>
<p>We would like to note that though these metrics capture to a reasonable extent how well the true posterior is approximated, they are still imperfect. Since the true posterior has mass over all the graphs corresponding to MEC of the ground truth graph, the true posterior does not necessarily evaluate to zero on $\mathbb{E}[\mathrm{SHD}]$ and one on AUROC. Hence, evaluating the learned model on these metrics only partially indicates how well the true posterior is approximated.</p>
<h3>4.2 Results</h3>
<h3>4.2.1 Estimation of True Posterior</h3>
<p>As indicated before, when the number of variables in the graph is small, we can enumerate the true posterior and compute the divergence/distance between the approximation and the true posterior. Figure 3 presents the Hellinger distance of the approximation up to four nodes. It can be seen that the autoregressive distribution of VCN reconstructs the true posterior much better than the factorised distribution, mainly due to the non-identifiability of graphs and the fact that the true posterior is multimodal.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Hellinger distance of the full posterior of the approximation with the true posterior.</p>
<p>Figure 4: Average runtime (in seconds, log scale) of different Bayesian causal inference approaches.</p>
<h3>4.2.2 Evaluation in higher dimensions</h3>
<p>Figure 5 reports the $\mathbb{E}[\mathrm{SHD}]$ of the approximation for different number of nodes. It can be seen that the autoregressive distribution of VCNs gives better performance than the factorised distribution in all the settings. In addition, the autoregressive distribution gives better results as compared to all the competitive baselines when there are 10 samples, a regime common in biological applications. When the number of samples are increased to 100, the proposed approach compares favourably to the baselines which do not involve differentiable learning based techniques, while being much better than the factorised distribution. Figure 6 presents the AUROC of these approaches. The autoregressive distribution of VCN performs better than the learning based method of factorised distribution, while comparing favourably to IMAP MCMC. We found that Boot Notears usually performs well on this metric. However, DAG Bootstrap in general has some limitations. Though DAG Bootstrap can capture multiple modes, its support is limited to the DAGs estimated using the bootstrap procedure and hence does not necessarily have full support. In addition, it can be significantly slower than VCN (Figure 4). Therefore, the proposed approach has a good tradeoff of runtime versus performance against the evaluated metrics and is strictly favourable in terms of purely learning based approaches.</p>
<h3>4.2.3 Real Dataset</h3>
<p>We evaluate our method in terms of AUROC on edge feature probabilities against all the baselines on the <em>Dream4</em> in-silico network challenge on gene regulation. As the typical metric for this challenge is the AUROC, we restrict our focus to this metric on this dataset. In particular, we examine the multifactorial dataset consisting of ten nodes and ten observations. Table 1 reports the AUROC of all the methods including the baselines. As the table suggests, our method achieves favourable performance while still being faster. In addition, we note that the underlying true graph corresponding to this dataset need not be a DAG, therefore introducing model misspacification.</p>
<p>Table 1: Results on the Dream4 gene expression dataset in terms of AUROC.</p>
<table>
<thead>
<tr>
<th>AUROC</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>IMAP MCMC</td>
<td></td>
</tr>
<tr>
<td>Boot Notears</td>
<td></td>
</tr>
<tr>
<td>Boot Lingam</td>
<td></td>
</tr>
<tr>
<td>Factorised</td>
<td></td>
</tr>
<tr>
<td>Autoregressive (VCN)</td>
<td></td>
</tr>
</tbody>
</table>
<h2>5 Conclusion</h2>
<p>We present a variational inference approach to perform Bayesian inference over the unknown causal structure in the structural causal model. Since the posterior over graph structures is multimodal in nature due to non-identifiability, we parameterise the variational family as an autoregressive distribution and model it using an LSTM. While we explore getting uncertainty estimates for linear models, it could be relevant to extend this to the case of non-linear models. One of the main limitations of this approach is the scalability of this method to larger dimensions. While a score function estimator might be limiting in higher dimensions due to higher variance, advanced variance reduction techniques could be used [11]. Further improvements to the proposed method can</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: $\mathbb{E}\left[\right.$ SHD] for $d=10$ and $d=20$ node ER random graphs (lower is better). Results obtained using 20 different random graphs.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: AUROC for $d=10$ and $d=20$ node ER random graphs (higher is better). Results obtained using 20 different random graphs.</p>
<p>be obtained by combining with normalising flows for discrete variables [35, 18]. Nevertheless, the uncertainty estimates obtained in this framework can be useful for selecting the most informative interventions, performing budgeted interventional experiments as well as representation learning of high-dimensional signals like natural images [3].</p>
<h1>References</h1>
<p>[1] Raj Agrawal, Caroline Uhler, and Tamara Broderick. Minimal i-map mcmc for scalable structure discovery in causal dag models. In International Conference on Machine Learning, pages 89-98. PMLR, 2018.
[2] Raj Agrawal, Chandler Squires, Karren Yang, Karthik Shanmugam, and Caroline Uhler. Abcdstrategy: Budgeted experimental design for targeted causal structure discovery. arXiv preprint arXiv:1902.10347, 2019.
[3] Yoshua Bengio, Tristan Deleu, Nasim Rahaman, Rosemary Ke, Sébastien Lachapelle, Olexa Bilaniuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle causal mechanisms. arXiv preprint arXiv:1901.10912, 2019.
[4] David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational Inference: A Review for Statisticians. Journal of the American Statistical Association, 112(518):859-877, Apr 2017. ISSN 1537-274X. doi: 10.1080/01621459.2017.1285773. URL http://dx.doi.org/ 10.1080/01621459.2017.1285773.
[5] Peter Bühlmann, Jonas Peters, Jan Ernest, et al. Cam: Causal additive models, highdimensional order search and penalized regression. The Annals of Statistics, 2014.
[6] David Maxwell Chickering. Optimal structure identification with greedy search. Journal of machine learning research, 2002.
[7] Frederick Eberhardt and Richard Scheines. Interventions and causal inference. Philosophy of science, 74(5):981-995, 2007.
[8] Byron Ellis and Wing Hung Wong. Learning causal bayesian network structures from experimental data. Journal of the American Statistical Association, 2008.
[9] Nir Friedman and Daphne Koller. Being bayesian about network structure. a bayesian approach to structure discovery in bayesian networks. Machine learning, 50(1):95-125, 2003.
[10] Dan Geiger, David Heckerman, et al. Parameter priors for directed acyclic graphical models and the characterization of several probability distributions. The Annals of Statistics, 2002.
[11] Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. arXiv preprint arXiv:1711.00123, 2017.
[12] Marco Grzegorczyk and Dirk Husmeier. Improving the structure mcmc sampler for bayesian networks by introducing a new edge reversal move. Machine Learning, 2008.
[13] Alain Hauser and Peter Bühlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. Journal of Machine Learning Research, 2012. URL http://jmir.org/papers/v13/hauser12a.html.
[14] David Heckerman, Christopher Meek, and Gregory Cooper. A bayesian approach to causal discovery. Computation, causation, and discovery, 1999.
[15] Christina Heinze-Deml, Marloes H Maathuis, and Nicolai Meinshausen. Causal structure learning. Annual Review of Statistics and Its Application, 5:371-391, 2018.
[16] Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen. Invariant causal prediction for nonlinear models. Journal of Causal Inference, 2018.
[17] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 1997.
[18] Emiel Hoogeboom, Jorn WT Peters, Rianne van den Berg, and Max Welling. Integer discrete flows and lossless compression. arXiv preprint arXiv:1905.07376, 2019.
[19] Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint arXiv:1910.01075, 2019.</p>
<p>[20] Nan Rosemary Ke, Jane Wang, Jovana Mitrovic, Martin Szummer, Danilo J Rezende, et al. Amortized learning of neural causal representations. arXiv preprint arXiv:2008.09301, 2020.
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[22] Jack Kuipers and Giusi Moffa. Partition mcmc for inference on acyclic digraphs. Journal of the American Statistical Association, 2017.
[23] Jack Kuipers, Giusi Moffa, David Heckerman, et al. Addendum on the scoring of gaussian directed acyclic graphical models. Annals of Statistics, 42(4):1689-1691, 2014.
[24] Sébastien Lachapelle, Philippe Brouillard, Tristan Deleu, and Simon Lacoste-Julien. Gradientbased neural dag learning. arXiv preprint arXiv:1906.02226, 2019.
[25] David Madigan, Jeremy York, and Denis Allard. Bayesian graphical models for discrete data. International Statistical Review/Revue Internationale de Statistique, 1995.
[26] Teppo Niinimäki, Pekka Parviainen, and Mikko Koivisto. Structure discovery in bayesian networks by sampling partial orders. The Journal of Machine Learning Research, 2016.
[27] Judea Pearl. Causality. Cambridge university press, 2009.
[28] Jonas Peters, Peter Bühlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 2016.
[29] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference. The MIT Press, 2017.
[30] Robert J Prill, Daniel Marbach, Julio Saez-Rodriguez, Peter K Sorger, Leonidas G Alexopoulos, Xiaowei Xue, Neil D Clarke, Gregoire Altan-Bonnet, and Gustavo Stolovitzky. Towards a rigorous assessment of systems biology models: the dream3 challenges. PloS one, 2010.
[31] Joseph Ramsey, Madelyn Glymour, Ruben Sanchez-Romero, and Clark Glymour. A million variables and more: the fast greedy equivalence search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images. International journal of data science and analytics, 2017.
[32] Thomas Schaffter, Daniel Marbach, and Dario Floreano. Genenetweaver: in silico benchmark generation and performance profiling of network inference methods. Bioinformatics, 2011.
[33] Shohei Shimizu. LiNGAM: Non-Gaussian methods for estimating causal structures. Behaviormetrika, 2014.
[34] Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction, and search. MIT press, 2000.
[35] Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete flows: Invertible generative models of discrete data. In Advances in Neural Information Processing Systems, 2019.
[36] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 1992.
[37] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. arXiv preprint arXiv:1904.10098, 2019.
[38] Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. In Advances in Neural Information Processing Systems, pages 9472-9483, 2018.</p>
<h1>A Proof of Proposition 1</h1>
<p>Proof.</p>
<p>$$
\begin{aligned}
&amp; \underset{\phi}{\arg \min } \mathrm{D}<em _phi="\phi">{\mathrm{KL}}\left(q</em>)\right) \
&amp; \stackrel{E q .6}{\equiv} \underset{\phi}{\arg \min } \underset{q_{\phi}(\mathcal{G})}{\mathbb{E}}[\log p(\mathcal{D} \mid \mathcal{G})+\log p(\mathcal{D})+\log q_{\phi}(\mathcal{G})-\log p(\mathcal{G})] \
&amp; =\underset{\phi}{\arg \min } \underset{q_{\phi}(\mathcal{G})}{\mathbb{E}}[-\log p(\mathcal{D} \mid \mathcal{G})]+\log p(\mathcal{D})+\mathrm{D}}(\mathcal{G}) | p(\mathcal{G} \mid \mathcal{D<em _phi="\phi">{\mathrm{KL}}\left(q</em>)\right)
\end{aligned}
$$}(\mathcal{G}) | p(\mathcal{G</p>
<p>which gives us a lower bound on the marginal log-likelihood of the data.</p>
<p>$$
\log p(\mathcal{D}) \geq \underset{q_{\phi}(\mathcal{G})}{\mathbb{E}}[\log p(\mathcal{D} \mid \mathcal{G})]-\mathrm{D}<em _phi="\phi">{\mathrm{KL}}\left(q</em>)\right)
$$}(\mathcal{G}) | p(\mathcal{G</p>
<h2>B Detailed Experimental Setup</h2>
<h2>B. 1 Synthetic Data Generation</h2>
<p>For generating synthetic data, we follow the procedure of NOTEARS [38]. We sample a DAG at random from an Erdos-Renyi model with expected number of edges equal to $d$. We only select the DAG if it has atleast two graphs inside the MEC. The edge weights are sampled independently at random from a Gaussian prior with a mean of 2 and variance of 1.The noise variables of the SCM are Gaussian mean 0 and variance of 1 .</p>
<h2>B. 2 Experimental Details</h2>
<p>Each model was then trained 20 times with different graphs (and hence datasets) for $n=10$ and $n=100$ data samples. For the plots of the true posterior comparison, we focus on $n=10$ samples. In addition, we also take the Hellinger distance instead of other divergences as the probabilities of many graph configurations are almost 0 and the log probabilities become numerically infeasible to tractably compute. For the factorised distribution and VCN, the models were trained with a learning rate of $1 e-2$ with Adam optimiser [21] for 30k epochs where the Gibbs temperature of the DAG constraint $\lambda_{t}$ is annealed from 10 to 1000 with the following annealing schedule:</p>
<p>$$
\lambda_{t}(i)=\min \text { temp }+10^{\frac{-2 \max (0, k-1,1 i)}{k}}(\max \text { temp }- \min \text { temp })
$$</p>
<p>where $k$ is the total number of epochs and $i$ is the current epoch. We set $\lambda_{s}$ to 0.01 . We also note that $Z_{d}$ is the partition function of the Gibbs distribution which in practice cannot be computed easily for data with dimensionality $d \geq 5$. However, we note that computing this is not required for the optimisation of the ELBO (Eq. 7) as it turns out to be a constant.</p>
<h2>B. 3 BGe score</h2>
<p>To compute the BGe score, the parameters $\theta$ have the following prior:</p>
<p>$$
\begin{aligned}
\boldsymbol{\theta} &amp; \sim \mathcal{N}\left(\boldsymbol{\theta} \mid \mu, W^{-1}\right) \
p\left(\mu \mid \gamma, W, \alpha_{w}\right) &amp; =\mathcal{N}\left(\mu \mid \gamma,\left(\alpha_{\mu} W\right)^{-1}\right) \
p\left(W \mid \alpha_{\lambda}, T\right) &amp; :=\operatorname{Wishart}\left(W \mid \alpha_{\lambda}, T\right)
\end{aligned}
$$</p>
<p>We set $\gamma$ to $2, \alpha_{\lambda}$ to 10 if $d \leq 5$ else we set it to 1000. $T$ is set to $\alpha_{\mu} \frac{\alpha_{\lambda}-d-1)}{\alpha_{\mu}+1} \cdot \mathbb{P}^{d \times d}$. The posterior for the above equation can be obtained with the same distribution family in close form and is given by:</p>
<p>$$
\begin{aligned}
p(\mu \mid W, \mathcal{D}) &amp; :=\mathcal{N}\left(\mu \mid \gamma^{\prime}, W^{\prime}\right) \
\gamma^{\prime} &amp; =\frac{\alpha_{\mu} \gamma+n \bar{x}<em _mu="\mu">{n}}{\alpha</em> \
W^{\prime} &amp; =\left(\alpha_{\mu}+n\right) W
\end{aligned}
$$}+n</p>
<p>$$
\begin{aligned}
p(W \mid \mathcal{D}) &amp; :=\operatorname{Wishart}\left(\alpha_{w}^{\prime}, T^{\prime}\right) \
\alpha_{w}^{\prime} &amp; =\alpha_{w}+n \
T^{\prime} &amp; =T+S_{n}+\frac{\alpha_{\mu} n}{\alpha_{\mu}+n}\left(\gamma-\bar{x}<em n="n">{n}\right)\left(\gamma-\bar{x}</em>
\end{aligned}
$$}\right)^{T</p>
<p>where $\bar{x}<em i="1">{n}=\sum</em>}^{n} x^{(i)}$ is the sample mean and $S_{n}=\sum_{i=1}^{n}\left(x^{(i)}-\bar{x<em n="n">{n}\right)\left(x^{(i)}-\bar{x}</em>$ is the sample covariance.}\right)^{T</p>
<h1>B. 4 Baselines</h1>
<p>Here we review the baselines which have been used for comparison and outline the hyperparameters and training procedures which have been used.</p>
<h2>B.4.1 DAG Bootstrap</h2>
<p>For the DAG bootstrap method, we ran Lingam [33] and NOTEARS [38] by resampling (with replacement) 50 points at random for 1000 different times for $n=100$ and 252 times with 5 points for $n=10$. An empirical estimate of the posterior $P(\mathcal{G} \mid \mathbf{x})=w_{\mathcal{G}, \mathbf{x}} \mathbb{1}\left(\mathcal{G} \in \mathcal{G}<em t="t">{t}\right)$ is obtained where $\mathcal{G}</em>$ is the empirical normalised frequency count.}$ is the set of all graphs predicted during the Bootstrap procedure and $w_{\mathcal{G}, \mathbf{x}</p>
<h2>B.4.2 IMAP MCMC</h2>
<p>We follow the default hyperparameters given in the IMAP MCMC paper [1]. In particular, we run the MCMC chain for $1 e 5$ iterations with a burn-in period of $2 e 4$ iterations. We use a thinning factor of 100 and use the same parameters for BGe score calculation as that for our approach.</p>
<h2>B.4.3 Open Source Codes for Baselines</h2>
<p>We use the following open source implementations for the baselines:</p>
<ul>
<li>NOTEARS: https://github.com/xunzheng/notears with Apache-2.0 License.</li>
<li>Lingam: https://github.com/cdt15/lingam with MIT License.</li>
<li>IMAP MCMC: https://github.com/miraep8/Minimal_IMAP_MCMC.</li>
</ul>
<h2>B. 5 Real Dataset</h2>
<p>For the Dream4 [32] gene-expression dataset, we use one of the gene expressions from the multifactorial dataset with ten nodes and ten observations. We also note that all the graphs in Dream4 do not necessarily correspond to DAGs, thereby introducing a crucial model misspacification in many of the models presented. We hypothesize that the inferior results of many of the techniques including the proposed approach suffer from this because of the non-DAGness.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Work done during an internship at Mila.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
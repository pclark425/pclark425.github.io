<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2248 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2248</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2248</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-62.html">extraction-schema-62</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <p><strong>Paper ID:</strong> paper-277595964</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.03024v1.pdf" target="_blank">Deep Reinforcement Learning via Object-Centric Attention</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. Contrary to deep agents, no single object-centric architecture can be applied to any environment. Inspired by principles of cognitive science and Occam’s Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information. Specifically, OC-CAM takes advantage of the object-centric inductive bias. Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2248.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2248.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCCAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-Centric Attention via Masking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of object-centric input abstractions that mask out background pixels and preserve only detected objects using simple bounding-box-based masking; supports multiple abstraction levels (object, binary, class, planes) and is evaluated end-to-end with PPO on Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OCCAM (Object-Centric Attention via Masking)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/object-level (multiple discrete abstraction levels: object-appearance, binary occupancy, class-coded, multi-plane per-class)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit masking via an external object detection/extraction method (bounding boxes); hard attention (predefined mask), not learned end-to-end</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari (12 games including Pong, MsPacman, Riverraid, Skiing, Freeway, Breakout, Frostbite, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — background clutter and visual distractors (color shifts, recoloring), plus gameplay perturbations (object displacement, stopped enemies, altered dynamics) derived from HackAtari</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Trained for 40M frames with PPO; OCCAM variants match or outperform pixel baselines on many games (example raw episodic scores from Table 2: MsPacman Object Masks 5880.00 vs DQN-like 3174.38; Riverraid Planes/variants ~7900-8200 vs DQN-like ~7849.38). Aggregate comparisons reported with IQM over Human Normalized Score (HNS) and Game Normalized Score (GNS) in Figures 4 and 5 (no single aggregate numeric HNS provided in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training: 40M frames per agent; reported wall-clock training times per agent per seed — DQN-like/Object Mask/Binary Mask/Class Mask: ~2–4 hours; Semantic Vector: ~1h 38min; Planes: ~6–10 hours per game per seed. No FLOPs/parameter counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>OCCAM variants generally match or outperform pixel (DQN-like) and symbolic/semantic-vector baselines on many games; e.g., MsPacman Object Masks 5880 vs DQN-like 3174 (≈+85%), Semantic Vector performs worse in spatial tasks. OCCAM improves robustness to visual perturbations (GNS closer to 1) compared to pixel baselines; still vulnerable to game-logic perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>No classic transfer-learning (retraining on new tasks) experiments reported; robustness evaluated as zero-shot transfer to perturbed variants (visual and logic changes) using Game Normalized Score (GNS). OCCAM maintains much higher GNS under visual perturbations but shows large drops under game-logic changes (e.g., Freeway vehicle-behavior changes cause significant degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated across 12 Atari games; OCCAM uses the same representation type per experiment (not a single multitask shared policy across games). Across games OCCAM variants perform comparably or better than baselines (paper reports per-game scores and IQM aggregates); no dedicated multi-task shared-representation training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Still sensitive to game-logic / dynamic perturbations (changes in object behavior). Over-abstraction can remove critical task details (e.g., Binary Masks drop color information like ghost color in MsPacman). Planes increase input size and compute. Semantic compression (Semantic Vector) removes spatial relationships and hurts spatial-reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Systematic comparison of four masking approaches (Object Masks, Binary Masks, Class Masks, Planes) vs two baselines (DQN-like pixels, Semantic Vector). Quantitative examples: MsPacman raw scores (Table 2): DQN-like 3174.38, Object Masks 5880.00, Binary Masks 4833.12, Class Masks 4549.38, Planes 7187.50. Visual-perturbation vs game-logic perturbation analyses presented (GNS comparisons in Figure 5 and Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Authors claim reduced sample complexity qualitatively; all agents trained for 40M frames for comparability. No explicit sample-efficiency curves or numeric sample-efficiency multipliers reported beyond training-time and wall-clock durations.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Zero-shot generalization evaluated via GNS on HackAtari perturbations: OCCAM robust to visual perturbations (color shifts, recoloring) — retains near-original performance; vulnerable to game-logic perturbations (e.g., Freeway altered vehicle behavior). Pong variants demonstrate OCCAM reduces shortcut reliance on opponent paddle in some cases (Planes helps), lowering performance degradation under Lazy/Hidden Enemy variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable — OCCAM does not perform pixel reconstruction; no MSE/SSIM metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Explicit: OCCAM is designed to remove task-irrelevant background pixels and preserve objects; empirical analysis shows removing background does not degrade, and in many cases improves, learning. Semantic Vector (excessive compression) shown to lose task-relevant spatial relations and hurt performance. Specific case studies: Pong (shortcut learning on enemy paddle vs ball) and MsPacman (ghost color matters).</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not implemented — OCCAM uses fixed masking strategies per experiment. Authors propose future work on adaptive/task-aware masking and temporal abstraction but report no results.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly analyzed. Paper does not provide specific experiments separating exploration vs exploitation benefits of abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>No information-theoretic metrics (mutual information, compression bounds, rate-distortion) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper identifies cases where retaining pixel-level object appearance is beneficial (Object Masks help when fine-grained features matter, e.g., ghost color in MsPacman); semantic/compressed representations can remove such useful cues and degrade performance (Semantic Vector).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2248.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Object Masks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OCCAM Object Masks (appearance-preserving object bounding-box mask)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OCCAM variant that keeps pixels inside detected object bounding boxes unchanged (preserving appearance and color) and sets background to zero, balancing abstraction and retained visual detail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Object Masks (OCCAM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>object-level while preserving within-object pixel appearance (object-appearance)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit masking via external bounding-box object extractor; keeps all object pixels</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (MsPacman, Pong, Riverraid, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — background clutter and visual distractors removed by the mask; experiments include color and visual perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: MsPacman raw episodic score 5880.00 (Table 2) vs DQN-like 3174.38; demonstrates substantial improvement on that game. Robust under visual perturbations (GNS close to baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time similar to DQN-like: ~2–4 hours per agent per seed for most games. No parameter/FLOP counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Outperforms DQN-like on several games (MsPacman example). Performs better than Semantic Vector on spatial tasks due to preserved appearance/spatial info.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Evaluated zero-shot on perturbed variants; maintains performance under visual changes but can be sensitive to game-logic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Included within per-game experiments across 12 games; no shared multi-task training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Can retain spurious within-object visual confounders (appearance cues) making it easier to find misalignments if object appearance correlates with irrelevant outcomes; may not remove all spurious correlations tied to object texture.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared directly (Table 2/Table 6) against Binary, Class, Planes, DQN-like and Semantic Vector; Object Masks often outperform DQN-like but not always best (Planes often best).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No explicit sample-efficiency numeric beyond training regime; training times comparable to DQN-like.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Robust to visual perturbations; still susceptible to game-logic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Preserves potential task-relevant visual cues (color, texture); empirical tests (MsPacman example) show that preserving appearance can be necessary for tasks where color encodes reward-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No — static mask per frame.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Yes — retains fine-grained appearance where beneficial (ghost color in MsPacman).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2248.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Binary Masks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OCCAM Binary Masks (occupancy-only bounding-box mask)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OCCAM variant that converts detected object bounding boxes into binary occupancy (1 inside box, 0 elsewhere), removing within-object appearance details to aggressively reduce visual complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Binary Masks (OCCAM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>binary/object occupancy (high abstraction, removes appearance and texture)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit masking via external bounding-box object extractor; binarizes object regions</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari (tested on same suite of games)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — removes background and within-object texture distractors; tests include recoloring and visual noise</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example: MsPacman raw episodic score 4833.12 (Table 2) vs DQN-like 3174.38; can outperform pixel-baseline in some games though lower than Planes or Object Masks in others.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time similar to DQN-like: ~2–4 hours per agent per seed for many games. No FLOPs/params reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Improves robustness to visual perturbations relative to DQN-like by forcing reliance on object positions; sometimes loses performance where appearance is important (e.g., ghost color).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Zero-shot evaluation to perturbed variants shows better visual robustness than pixel baselines but can fail when appearance encodes task-relevant info.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated per-game; no shared multitask training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Removes useful within-object signals (color/texture) causing failures in tasks that require this information (MsPacman ghost color).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Included in the core mask-variant comparisons (Table 2/Table 6); shows the trade-off of more abstraction (robustness) vs lost expressivity.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported numerically beyond training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Strong visual robustness, weaker to game-logic changes and to tasks needing fine-grained appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper documents that binary abstraction reduces spurious texture reliance but at cost of discarding some task-relevant cues.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Binary masks sacrifice pixel fidelity; authors note occasions where fidelity would help (color-coded cues).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2248.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Class Masks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OCCAM Class Masks (class-augmented object masks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OCCAM variant that replaces each detected object region with a uniform representation/color depending on its class, preserving semantic identity but removing appearance variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Class Masks (OCCAM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>semantic/object-class-level (task-dependent since it requires classification)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit masking plus object classification (requires labeled or classifier-based object classes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games where classes can be defined</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — removes background and within-class visual variation; experiments include color shifts and recoloring</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Per-table comparisons show Class Masks sometimes competitive but not uniformly superior (MsPacman Class Masks 4549.38 vs Object Masks 5880.00 and Planes 7187.50).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training time similar to other OCCAM masks (~2–4 hours per seed) but requires pre-processing/classifier to label objects; no FLOPs/params reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Class Masks trade off appearance for semantic identity; they do not always outperform simpler masks and require class-capable extractors making them task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Zero-shot to visual perturbations: stable if class identity preserved; vulnerable if class extraction fails under perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Tested per-game; no explicit multi-task/shared policy reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires object classifier which may need task-specific tuning; misclassification or missing classes harms downstream policy. Loses within-class appearance cues.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Included as one of the four mask strategies compared across games (Table 2/Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified beyond training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Robust to visual changes that don't affect class labels; less robust if classification fails under perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Class Masks explicitly preserve semantic (class) relevance at expense of appearance; the paper shows this can be beneficial but not universally.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not present.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Class Masks remove pixel fidelity; the paper notes this can hurt when subtle appearance cues carry task info.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2248.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Planes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OCCAM Planes (per-class binary planes representation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OCCAM structured encoding that represents each object class as a separate binary plane, providing a chess-like multi-plane spatial representation that preserves spatial layout while being semantically organized.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Planes (OCCAM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>structured semantic/plane-based (one-hot per-class binary spatial planes)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>explicit masking + class-based assignment of each object to a dedicated binary plane</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari (notably effective in Pong and other spatial tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — removes background and appearance distractors; preserves per-class spatial occupancy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Planes often perform best across many games; example MsPacman Planes 7187.50 (Table 2) higher than other mask types and DQN-like 3174.38. Training time heavier: 6–10 hours per game per seed.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Higher computational cost due to increased input dimensionality (scales with number of classes); reported training time ~6–10h per game per seed. No FLOPs/params reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Planes generally provide best policy stability and generalization across many tasks at cost of compute; outperforms simpler masks in some tasks (MsPacman example).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Zero-shot robustness to visual perturbations good; still vulnerable to game-logic changes. No cross-task transfer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Per-game evaluation only; Planes increase input dimensionality which could complicate large multi-task setups.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Higher compute and input size; requires class extraction; may not be efficient for many classes or compute-constrained scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Included among core mask-type comparisons and highlighted as often performing best (Table 2/Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>No numeric sample-efficiency claims beyond training regime; longer wall-clock training time indicates higher compute cost.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Planes mitigate shortcut learning in Pong more effectively than pixel or Semantic Vector input, reducing degradation under Lazy/Hidden Enemy variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Planes explicitly encode class-level task-relevant identity and spatial positions, shown empirically to improve generalization in several games.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No — static per-frame planes.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Planes discard within-object appearance but keep class identity; beneficial when spatial/class identity matters more than texture.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2248.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Vector (OCAtari)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semantic Vector representation (OCAtari structured vector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact symbolic representation from OCAtari that encodes object attributes (position, size, additional properties) into a single structured vector for symbolic reasoning and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OCAtari: Object-centric Atari 2600 reinforcement learning environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Semantic Vector (OCAtari)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>symbolic/vectorized (high compression of object attributes)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>hand-designed extraction into structured attributes (positions, sizes, properties) — task-specific object attribute encoding</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari environments (OCAtari wrapper used for experiments and perturbations)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — representation discards background and appearance, focusing only on encoded object attributes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Semantic Vector often underperforms in tasks requiring spatial detail; training time ~1h 38min per agent per seed (faster), but raw performance lower in some games (Table 2 shows Semantic Vector sometimes much worse; e.g., table indicates poor performance in certain settings and substantial negative impacts in Pong variants).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Faster training times (reported avg 1h 38min) because of low-dimensional input; no FLOPs/parameter counts provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Semantic Vector reduces input dimensionality but loses spatial relations leading to worse performance in spatially-demanding tasks (paper reports it struggles where spatial dependencies are crucial compared to OCCAM masks and pixel inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Zero-shot evaluation on perturbed variants shows mixed results; compression harms ability to generalize when spatial relationships are required.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Evaluated per-game; not shown to enable strong multi-task transfer due to loss of spatial information.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Excessive compression that removes spatial dependencies and relationships necessary for tasks requiring fine spatial or visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Compared to OCCAM masks and pixel baselines; shows limitations in Table 2 and discussion about compression vs expressivity trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training faster (lower wall-clock) due to compact input; no sample-efficiency numbers given beyond training frame budget.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Stable to some perturbations if encoded attributes preserved; poor when spatial or appearance cues are required for task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable (no reconstruction).</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper highlights that Semantic Vector removes spatial dependencies and can remove task-relevant dynamics; contrasts with OCCAM which preserves structure while reducing background noise.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not presented.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Paper reports scenarios where pixel or object-appearance fidelity matters (e.g., MsPacman ghost color) and where Semantic Vector's loss of appearance harms performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2248.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DQN-like</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DQN-like pixel-based representation (standard CNN pixel stack)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard pixel-based representation used in Atari (downsampled grayscale or RGB frames stacked) processed by CNNs, serving as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DQN-like pixel input (standard convolutional feature extractor)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>pixel-level (raw pixels / low-level visual features)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>no explicit feature selection beyond learned convolutional filters and implicit learned attention; susceptible to spurious correlations</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari benchmark (12 games used in study)</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>yes — includes full background and appearance; tested under visual and game-logic perturbations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Baseline comparisons reported across games; example MsPacman raw episodic score 3174.38 (Table 2). Performance degraded more under visual perturbations compared to OCCAM in many cases (figures and tables report IQM/HNS/GNS aggregates).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Training times comparable to most mask-based OCCAM variants (~2–4 hours per agent per seed); standard convolutional architectures used, but exact model sizes/FLOPs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Pixel-based agents often achieve high performance in standard environments but suffer larger performance drops under visual perturbations and are prone to shortcut learning (e.g., Pong relying on opponent paddle). OCCAM often matches or surpasses DQN-like performance while improving robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Zero-shot tests to perturbed variants show larger degradation than OCCAM for visual shifts; game-logic changes still problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Per-game performance reported; not a multi-task shared policy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Overfitting to spurious visual correlations and background artifacts; sensitive to simple visual changes (color shifts) and shortcut strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Included as a core baseline in all ablations comparing masking strategies and semantic vector representations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained with the same 40M-frame regime for comparability; no claims of superior sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Shows significant performance drops under slightly changed or simpler variants of environments (Figure 1).</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper documents that pixel-based agents often latch on to easily decodable but task-irrelevant features (e.g., opponent paddle in Pong) rather than truly task-relevant objects like the ball.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not explicitly discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Pixel fidelity supports learning fine-grained visual cues when needed, but these can also be spurious and hurt generalization; paper shows cases where pixel methods succeed in standard envs but fail under perturbations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2248.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Slot Attention</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-centric learning with Slot Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised object-centric representation learning method that decomposes scenes into a set of slot representations (object slots) via attention mechanisms; cited as prior work for object extraction in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Object-centric learning with slot attention</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Slot Attention</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>object-level (unsupervised slot-based decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned attention weights (slot attention) to group pixels into object slots</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>mentioned in context of Atari/object-centric RL (prior work); not used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>prior work used for object extraction in visual scenes (general), not specific to this paper's perturbation suite</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in this paper; cited as effective in prior work for object extraction applied to Atari in Delfosse et al. (2023).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper (refer to original Slot Attention paper for details).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Mentioned as a more complex object-extraction approach compared to OCCAM's simpler bounding-box masking; OCCAM preferred for simplicity and integration ease.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Cited literature notes slot methods can require careful tuning and may compress or mis-segment objects; paper states complex extraction can add noise or miss task-relevant details.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not part of this paper's ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not performed in this paper for Slot Attention; referenced for prior evidence of improved generalization when using object extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper contrasts learned unsupervised extractors (like Slot Attention) with OCCAM's simpler masking; argues OCCAM's simplicity avoids extra pretraining overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not discussed in this paper for slot methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not present in this paper regarding Slot Attention.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not discussed for Slot Attention in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2248.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPACE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised object extraction model that decomposes scenes into objects using spatial attention and generative modeling, cited as prior art enabling object-centric RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPACE</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>object-level unsupervised decomposition (generative)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>learned attention and generative modeling to infer object bounding boxes and latent factors</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>cited in context of object extraction applied to Atari and RL (prior work), not used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>General image scenes; not specifically evaluated on HackAtari perturbations in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated here (cited as successful in prior works for improving efficiency and generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Described as more complex and requiring pretraining versus OCCAM's simple masking; OCCAM avoids pretraining overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes complex extractors can introduce noise or require priors (max objects, categories), potentially harming downstream RL if mis-specified.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not part of this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Not performed here; referenced as prior evidence that object extraction can aid generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>SPACE represents an approach that extracts object-centric factors which can be used to focus on task-relevant entities; OCCAM positions itself as a simpler alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>pixel_fidelity_benefits</strong></td>
                            <td>Not discussed for SPACE in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2248.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2248.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models, representation learning methods, or predictive models that compare different levels of abstraction, discuss task-relevant vs task-irrelevant features, or evaluate transfer learning and multi-task performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Object-centric world models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-centric world models (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of predictive/world models that represent environment dynamics at the object level (objects and their interactions) rather than raw pixels, cited as part of related work and motivation for object-centric abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>object-centric world models (generic)</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_level</strong></td>
                            <td>object-level / relational (model dynamics between object entities)</td>
                        </tr>
                        <tr>
                            <td><strong>feature_selection_mechanism</strong></td>
                            <td>typically object decomposition plus learned dynamics (varies by method); in this paper referenced conceptually rather than implemented</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>discussed in RL/robustness context (related work); not experimentally used here</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_presence</strong></td>
                            <td>potentially robust to visual distractors by modeling object dynamics instead of pixels; paper advocates integration with OCCAM-like filtering</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in this paper; cited as promising approach in literature for robustness/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Paper positions OCCAM as complementary—structured abstraction can be combined with object-centric world models to improve focus on task-relevant elements while keeping computational cost lower than full symbolic extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td>Not evaluated here; object-centric world models in literature often aim to improve transfer via compositional dynamics, but no numeric results in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes object-centric models can require structured priors (max objects, categories), risk of excessive compression, and added complexity/overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Not applicable within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_analysis</strong></td>
                            <td>Mentioned as part of the motivation to focus on objects to improve generalization; OCCAM shown to improve visual robustness without full world-model training.</td>
                        </tr>
                        <tr>
                            <td><strong>reconstruction_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>task_relevance_analysis</strong></td>
                            <td>Paper argues that object-centric world models plus adaptive filtering could help focus on task-relevant entities; however this is proposed future work, not experimentally demonstrated.</td>
                        </tr>
                        <tr>
                            <td><strong>dynamic_abstraction</strong></td>
                            <td>Not implemented here; suggested as future direction (adaptive abstraction and task-aware masking).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_vs_exploitation</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_theoretic_analysis</strong></td>
                            <td>Not presented.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>OCAtari: Object-centric Atari 2600 reinforcement learning environments <em>(Rating: 2)</em></li>
                <li>Object-centric learning with slot attention <em>(Rating: 2)</em></li>
                <li>SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition <em>(Rating: 2)</em></li>
                <li>Boosting object representation learning via motion and object continuity <em>(Rating: 2)</em></li>
                <li>Generalization and robustness implications in object-centric learning <em>(Rating: 2)</em></li>
                <li>Contrastive abstraction for reinforcement learning <em>(Rating: 1)</em></li>
                <li>Interpretable concept bottlenecks to align reinforcement learning agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2248",
    "paper_id": "paper-277595964",
    "extraction_schema_id": "extraction-schema-62",
    "extracted_data": [
        {
            "name_short": "OCCAM",
            "name_full": "Object-Centric Attention via Masking",
            "brief_description": "A family of object-centric input abstractions that mask out background pixels and preserve only detected objects using simple bounding-box-based masking; supports multiple abstraction levels (object, binary, class, planes) and is evaluated end-to-end with PPO on Atari.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OCCAM (Object-Centric Attention via Masking)",
            "abstraction_level": "semantic/object-level (multiple discrete abstraction levels: object-appearance, binary occupancy, class-coded, multi-plane per-class)",
            "feature_selection_mechanism": "explicit masking via an external object detection/extraction method (bounding boxes); hard attention (predefined mask), not learned end-to-end",
            "task_domain": "Atari (12 games including Pong, MsPacman, Riverraid, Skiing, Freeway, Breakout, Frostbite, etc.)",
            "distractor_presence": "yes — background clutter and visual distractors (color shifts, recoloring), plus gameplay perturbations (object displacement, stopped enemies, altered dynamics) derived from HackAtari",
            "performance_metrics": "Trained for 40M frames with PPO; OCCAM variants match or outperform pixel baselines on many games (example raw episodic scores from Table 2: MsPacman Object Masks 5880.00 vs DQN-like 3174.38; Riverraid Planes/variants ~7900-8200 vs DQN-like ~7849.38). Aggregate comparisons reported with IQM over Human Normalized Score (HNS) and Game Normalized Score (GNS) in Figures 4 and 5 (no single aggregate numeric HNS provided in-text).",
            "computational_cost_details": "Training: 40M frames per agent; reported wall-clock training times per agent per seed — DQN-like/Object Mask/Binary Mask/Class Mask: ~2–4 hours; Semantic Vector: ~1h 38min; Planes: ~6–10 hours per game per seed. No FLOPs/parameter counts reported.",
            "comparison_to_baselines": "OCCAM variants generally match or outperform pixel (DQN-like) and symbolic/semantic-vector baselines on many games; e.g., MsPacman Object Masks 5880 vs DQN-like 3174 (≈+85%), Semantic Vector performs worse in spatial tasks. OCCAM improves robustness to visual perturbations (GNS closer to 1) compared to pixel baselines; still vulnerable to game-logic perturbations.",
            "transfer_learning_results": "No classic transfer-learning (retraining on new tasks) experiments reported; robustness evaluated as zero-shot transfer to perturbed variants (visual and logic changes) using Game Normalized Score (GNS). OCCAM maintains much higher GNS under visual perturbations but shows large drops under game-logic changes (e.g., Freeway vehicle-behavior changes cause significant degradation).",
            "multi_task_performance": "Evaluated across 12 Atari games; OCCAM uses the same representation type per experiment (not a single multitask shared policy across games). Across games OCCAM variants perform comparably or better than baselines (paper reports per-game scores and IQM aggregates); no dedicated multi-task shared-representation training reported.",
            "failure_modes": "Still sensitive to game-logic / dynamic perturbations (changes in object behavior). Over-abstraction can remove critical task details (e.g., Binary Masks drop color information like ghost color in MsPacman). Planes increase input size and compute. Semantic compression (Semantic Vector) removes spatial relationships and hurts spatial-reasoning tasks.",
            "ablation_studies": "Systematic comparison of four masking approaches (Object Masks, Binary Masks, Class Masks, Planes) vs two baselines (DQN-like pixels, Semantic Vector). Quantitative examples: MsPacman raw scores (Table 2): DQN-like 3174.38, Object Masks 5880.00, Binary Masks 4833.12, Class Masks 4549.38, Planes 7187.50. Visual-perturbation vs game-logic perturbation analyses presented (GNS comparisons in Figure 5 and Table 6).",
            "sample_efficiency": "Authors claim reduced sample complexity qualitatively; all agents trained for 40M frames for comparability. No explicit sample-efficiency curves or numeric sample-efficiency multipliers reported beyond training-time and wall-clock durations.",
            "generalization_analysis": "Zero-shot generalization evaluated via GNS on HackAtari perturbations: OCCAM robust to visual perturbations (color shifts, recoloring) — retains near-original performance; vulnerable to game-logic perturbations (e.g., Freeway altered vehicle behavior). Pong variants demonstrate OCCAM reduces shortcut reliance on opponent paddle in some cases (Planes helps), lowering performance degradation under Lazy/Hidden Enemy variants.",
            "reconstruction_quality": "Not applicable — OCCAM does not perform pixel reconstruction; no MSE/SSIM metrics reported.",
            "task_relevance_analysis": "Explicit: OCCAM is designed to remove task-irrelevant background pixels and preserve objects; empirical analysis shows removing background does not degrade, and in many cases improves, learning. Semantic Vector (excessive compression) shown to lose task-relevant spatial relations and hurt performance. Specific case studies: Pong (shortcut learning on enemy paddle vs ball) and MsPacman (ghost color matters).",
            "dynamic_abstraction": "Not implemented — OCCAM uses fixed masking strategies per experiment. Authors propose future work on adaptive/task-aware masking and temporal abstraction but report no results.",
            "exploration_vs_exploitation": "Not explicitly analyzed. Paper does not provide specific experiments separating exploration vs exploitation benefits of abstraction.",
            "information_theoretic_analysis": "No information-theoretic metrics (mutual information, compression bounds, rate-distortion) reported.",
            "pixel_fidelity_benefits": "Paper identifies cases where retaining pixel-level object appearance is beneficial (Object Masks help when fine-grained features matter, e.g., ghost color in MsPacman); semantic/compressed representations can remove such useful cues and degrade performance (Semantic Vector).",
            "uuid": "e2248.0"
        },
        {
            "name_short": "Object Masks",
            "name_full": "OCCAM Object Masks (appearance-preserving object bounding-box mask)",
            "brief_description": "An OCCAM variant that keeps pixels inside detected object bounding boxes unchanged (preserving appearance and color) and sets background to zero, balancing abstraction and retained visual detail.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Object Masks (OCCAM variant)",
            "abstraction_level": "object-level while preserving within-object pixel appearance (object-appearance)",
            "feature_selection_mechanism": "explicit masking via external bounding-box object extractor; keeps all object pixels",
            "task_domain": "Atari games (MsPacman, Pong, Riverraid, etc.)",
            "distractor_presence": "yes — background clutter and visual distractors removed by the mask; experiments include color and visual perturbations",
            "performance_metrics": "Example: MsPacman raw episodic score 5880.00 (Table 2) vs DQN-like 3174.38; demonstrates substantial improvement on that game. Robust under visual perturbations (GNS close to baseline).",
            "computational_cost_details": "Training time similar to DQN-like: ~2–4 hours per agent per seed for most games. No parameter/FLOP counts reported.",
            "comparison_to_baselines": "Outperforms DQN-like on several games (MsPacman example). Performs better than Semantic Vector on spatial tasks due to preserved appearance/spatial info.",
            "transfer_learning_results": "Evaluated zero-shot on perturbed variants; maintains performance under visual changes but can be sensitive to game-logic changes.",
            "multi_task_performance": "Included within per-game experiments across 12 games; no shared multi-task training reported.",
            "failure_modes": "Can retain spurious within-object visual confounders (appearance cues) making it easier to find misalignments if object appearance correlates with irrelevant outcomes; may not remove all spurious correlations tied to object texture.",
            "ablation_studies": "Compared directly (Table 2/Table 6) against Binary, Class, Planes, DQN-like and Semantic Vector; Object Masks often outperform DQN-like but not always best (Planes often best).",
            "sample_efficiency": "No explicit sample-efficiency numeric beyond training regime; training times comparable to DQN-like.",
            "generalization_analysis": "Robust to visual perturbations; still susceptible to game-logic changes.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Preserves potential task-relevant visual cues (color, texture); empirical tests (MsPacman example) show that preserving appearance can be necessary for tasks where color encodes reward-relevant information.",
            "dynamic_abstraction": "No — static mask per frame.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not present.",
            "pixel_fidelity_benefits": "Yes — retains fine-grained appearance where beneficial (ghost color in MsPacman).",
            "uuid": "e2248.1"
        },
        {
            "name_short": "Binary Masks",
            "name_full": "OCCAM Binary Masks (occupancy-only bounding-box mask)",
            "brief_description": "An OCCAM variant that converts detected object bounding boxes into binary occupancy (1 inside box, 0 elsewhere), removing within-object appearance details to aggressively reduce visual complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Binary Masks (OCCAM variant)",
            "abstraction_level": "binary/object occupancy (high abstraction, removes appearance and texture)",
            "feature_selection_mechanism": "explicit masking via external bounding-box object extractor; binarizes object regions",
            "task_domain": "Atari (tested on same suite of games)",
            "distractor_presence": "yes — removes background and within-object texture distractors; tests include recoloring and visual noise",
            "performance_metrics": "Example: MsPacman raw episodic score 4833.12 (Table 2) vs DQN-like 3174.38; can outperform pixel-baseline in some games though lower than Planes or Object Masks in others.",
            "computational_cost_details": "Training time similar to DQN-like: ~2–4 hours per agent per seed for many games. No FLOPs/params reported.",
            "comparison_to_baselines": "Improves robustness to visual perturbations relative to DQN-like by forcing reliance on object positions; sometimes loses performance where appearance is important (e.g., ghost color).",
            "transfer_learning_results": "Zero-shot evaluation to perturbed variants shows better visual robustness than pixel baselines but can fail when appearance encodes task-relevant info.",
            "multi_task_performance": "Evaluated per-game; no shared multitask training.",
            "failure_modes": "Removes useful within-object signals (color/texture) causing failures in tasks that require this information (MsPacman ghost color).",
            "ablation_studies": "Included in the core mask-variant comparisons (Table 2/Table 6); shows the trade-off of more abstraction (robustness) vs lost expressivity.",
            "sample_efficiency": "Not reported numerically beyond training regime.",
            "generalization_analysis": "Strong visual robustness, weaker to game-logic changes and to tasks needing fine-grained appearance.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Paper documents that binary abstraction reduces spurious texture reliance but at cost of discarding some task-relevant cues.",
            "dynamic_abstraction": "No.",
            "exploration_vs_exploitation": "Not analyzed.",
            "information_theoretic_analysis": "None.",
            "pixel_fidelity_benefits": "Binary masks sacrifice pixel fidelity; authors note occasions where fidelity would help (color-coded cues).",
            "uuid": "e2248.2"
        },
        {
            "name_short": "Class Masks",
            "name_full": "OCCAM Class Masks (class-augmented object masks)",
            "brief_description": "An OCCAM variant that replaces each detected object region with a uniform representation/color depending on its class, preserving semantic identity but removing appearance variation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Class Masks (OCCAM variant)",
            "abstraction_level": "semantic/object-class-level (task-dependent since it requires classification)",
            "feature_selection_mechanism": "explicit masking plus object classification (requires labeled or classifier-based object classes)",
            "task_domain": "Atari games where classes can be defined",
            "distractor_presence": "yes — removes background and within-class visual variation; experiments include color shifts and recoloring",
            "performance_metrics": "Per-table comparisons show Class Masks sometimes competitive but not uniformly superior (MsPacman Class Masks 4549.38 vs Object Masks 5880.00 and Planes 7187.50).",
            "computational_cost_details": "Training time similar to other OCCAM masks (~2–4 hours per seed) but requires pre-processing/classifier to label objects; no FLOPs/params reported.",
            "comparison_to_baselines": "Class Masks trade off appearance for semantic identity; they do not always outperform simpler masks and require class-capable extractors making them task-dependent.",
            "transfer_learning_results": "Zero-shot to visual perturbations: stable if class identity preserved; vulnerable if class extraction fails under perturbation.",
            "multi_task_performance": "Tested per-game; no explicit multi-task/shared policy reported.",
            "failure_modes": "Requires object classifier which may need task-specific tuning; misclassification or missing classes harms downstream policy. Loses within-class appearance cues.",
            "ablation_studies": "Included as one of the four mask strategies compared across games (Table 2/Table 6).",
            "sample_efficiency": "Not quantified beyond training regime.",
            "generalization_analysis": "Robust to visual changes that don't affect class labels; less robust if classification fails under perturbations.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Class Masks explicitly preserve semantic (class) relevance at expense of appearance; the paper shows this can be beneficial but not universally.",
            "dynamic_abstraction": "No.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not present.",
            "pixel_fidelity_benefits": "Class Masks remove pixel fidelity; the paper notes this can hurt when subtle appearance cues carry task info.",
            "uuid": "e2248.3"
        },
        {
            "name_short": "Planes",
            "name_full": "OCCAM Planes (per-class binary planes representation)",
            "brief_description": "An OCCAM structured encoding that represents each object class as a separate binary plane, providing a chess-like multi-plane spatial representation that preserves spatial layout while being semantically organized.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Planes (OCCAM variant)",
            "abstraction_level": "structured semantic/plane-based (one-hot per-class binary spatial planes)",
            "feature_selection_mechanism": "explicit masking + class-based assignment of each object to a dedicated binary plane",
            "task_domain": "Atari (notably effective in Pong and other spatial tasks)",
            "distractor_presence": "yes — removes background and appearance distractors; preserves per-class spatial occupancy",
            "performance_metrics": "Planes often perform best across many games; example MsPacman Planes 7187.50 (Table 2) higher than other mask types and DQN-like 3174.38. Training time heavier: 6–10 hours per game per seed.",
            "computational_cost_details": "Higher computational cost due to increased input dimensionality (scales with number of classes); reported training time ~6–10h per game per seed. No FLOPs/params reported.",
            "comparison_to_baselines": "Planes generally provide best policy stability and generalization across many tasks at cost of compute; outperforms simpler masks in some tasks (MsPacman example).",
            "transfer_learning_results": "Zero-shot robustness to visual perturbations good; still vulnerable to game-logic changes. No cross-task transfer experiments.",
            "multi_task_performance": "Per-game evaluation only; Planes increase input dimensionality which could complicate large multi-task setups.",
            "failure_modes": "Higher compute and input size; requires class extraction; may not be efficient for many classes or compute-constrained scenarios.",
            "ablation_studies": "Included among core mask-type comparisons and highlighted as often performing best (Table 2/Table 6).",
            "sample_efficiency": "No numeric sample-efficiency claims beyond training regime; longer wall-clock training time indicates higher compute cost.",
            "generalization_analysis": "Planes mitigate shortcut learning in Pong more effectively than pixel or Semantic Vector input, reducing degradation under Lazy/Hidden Enemy variants.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Planes explicitly encode class-level task-relevant identity and spatial positions, shown empirically to improve generalization in several games.",
            "dynamic_abstraction": "No — static per-frame planes.",
            "exploration_vs_exploitation": "Not reported.",
            "information_theoretic_analysis": "Not provided.",
            "pixel_fidelity_benefits": "Planes discard within-object appearance but keep class identity; beneficial when spatial/class identity matters more than texture.",
            "uuid": "e2248.4"
        },
        {
            "name_short": "Semantic Vector (OCAtari)",
            "name_full": "Semantic Vector representation (OCAtari structured vector)",
            "brief_description": "A compact symbolic representation from OCAtari that encodes object attributes (position, size, additional properties) into a single structured vector for symbolic reasoning and policy learning.",
            "citation_title": "OCAtari: Object-centric Atari 2600 reinforcement learning environments",
            "mention_or_use": "use",
            "model_name": "Semantic Vector (OCAtari)",
            "abstraction_level": "symbolic/vectorized (high compression of object attributes)",
            "feature_selection_mechanism": "hand-designed extraction into structured attributes (positions, sizes, properties) — task-specific object attribute encoding",
            "task_domain": "Atari environments (OCAtari wrapper used for experiments and perturbations)",
            "distractor_presence": "yes — representation discards background and appearance, focusing only on encoded object attributes",
            "performance_metrics": "Semantic Vector often underperforms in tasks requiring spatial detail; training time ~1h 38min per agent per seed (faster), but raw performance lower in some games (Table 2 shows Semantic Vector sometimes much worse; e.g., table indicates poor performance in certain settings and substantial negative impacts in Pong variants).",
            "computational_cost_details": "Faster training times (reported avg 1h 38min) because of low-dimensional input; no FLOPs/parameter counts provided.",
            "comparison_to_baselines": "Semantic Vector reduces input dimensionality but loses spatial relations leading to worse performance in spatially-demanding tasks (paper reports it struggles where spatial dependencies are crucial compared to OCCAM masks and pixel inputs).",
            "transfer_learning_results": "Zero-shot evaluation on perturbed variants shows mixed results; compression harms ability to generalize when spatial relationships are required.",
            "multi_task_performance": "Evaluated per-game; not shown to enable strong multi-task transfer due to loss of spatial information.",
            "failure_modes": "Excessive compression that removes spatial dependencies and relationships necessary for tasks requiring fine spatial or visual reasoning.",
            "ablation_studies": "Compared to OCCAM masks and pixel baselines; shows limitations in Table 2 and discussion about compression vs expressivity trade-off.",
            "sample_efficiency": "Training faster (lower wall-clock) due to compact input; no sample-efficiency numbers given beyond training frame budget.",
            "generalization_analysis": "Stable to some perturbations if encoded attributes preserved; poor when spatial or appearance cues are required for task performance.",
            "reconstruction_quality": "Not applicable (no reconstruction).",
            "task_relevance_analysis": "Paper highlights that Semantic Vector removes spatial dependencies and can remove task-relevant dynamics; contrasts with OCCAM which preserves structure while reducing background noise.",
            "dynamic_abstraction": "No.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not presented.",
            "pixel_fidelity_benefits": "Paper reports scenarios where pixel or object-appearance fidelity matters (e.g., MsPacman ghost color) and where Semantic Vector's loss of appearance harms performance.",
            "uuid": "e2248.5"
        },
        {
            "name_short": "DQN-like",
            "name_full": "DQN-like pixel-based representation (standard CNN pixel stack)",
            "brief_description": "Standard pixel-based representation used in Atari (downsampled grayscale or RGB frames stacked) processed by CNNs, serving as a baseline for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DQN-like pixel input (standard convolutional feature extractor)",
            "abstraction_level": "pixel-level (raw pixels / low-level visual features)",
            "feature_selection_mechanism": "no explicit feature selection beyond learned convolutional filters and implicit learned attention; susceptible to spurious correlations",
            "task_domain": "Atari benchmark (12 games used in study)",
            "distractor_presence": "yes — includes full background and appearance; tested under visual and game-logic perturbations",
            "performance_metrics": "Baseline comparisons reported across games; example MsPacman raw episodic score 3174.38 (Table 2). Performance degraded more under visual perturbations compared to OCCAM in many cases (figures and tables report IQM/HNS/GNS aggregates).",
            "computational_cost_details": "Training times comparable to most mask-based OCCAM variants (~2–4 hours per agent per seed); standard convolutional architectures used, but exact model sizes/FLOPs not reported.",
            "comparison_to_baselines": "Pixel-based agents often achieve high performance in standard environments but suffer larger performance drops under visual perturbations and are prone to shortcut learning (e.g., Pong relying on opponent paddle). OCCAM often matches or surpasses DQN-like performance while improving robustness.",
            "transfer_learning_results": "Zero-shot tests to perturbed variants show larger degradation than OCCAM for visual shifts; game-logic changes still problematic.",
            "multi_task_performance": "Per-game performance reported; not a multi-task shared policy.",
            "failure_modes": "Overfitting to spurious visual correlations and background artifacts; sensitive to simple visual changes (color shifts) and shortcut strategies.",
            "ablation_studies": "Included as a core baseline in all ablations comparing masking strategies and semantic vector representations.",
            "sample_efficiency": "Trained with the same 40M-frame regime for comparability; no claims of superior sample efficiency.",
            "generalization_analysis": "Shows significant performance drops under slightly changed or simpler variants of environments (Figure 1).",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Paper documents that pixel-based agents often latch on to easily decodable but task-irrelevant features (e.g., opponent paddle in Pong) rather than truly task-relevant objects like the ball.",
            "dynamic_abstraction": "Not applicable.",
            "exploration_vs_exploitation": "Not explicitly discussed.",
            "information_theoretic_analysis": "Not provided.",
            "pixel_fidelity_benefits": "Pixel fidelity supports learning fine-grained visual cues when needed, but these can also be spurious and hurt generalization; paper shows cases where pixel methods succeed in standard envs but fail under perturbations.",
            "uuid": "e2248.6"
        },
        {
            "name_short": "Slot Attention",
            "name_full": "Object-centric learning with Slot Attention",
            "brief_description": "An unsupervised object-centric representation learning method that decomposes scenes into a set of slot representations (object slots) via attention mechanisms; cited as prior work for object extraction in RL.",
            "citation_title": "Object-centric learning with slot attention",
            "mention_or_use": "mention",
            "model_name": "Slot Attention",
            "abstraction_level": "object-level (unsupervised slot-based decomposition)",
            "feature_selection_mechanism": "learned attention weights (slot attention) to group pixels into object slots",
            "task_domain": "mentioned in context of Atari/object-centric RL (prior work); not used in experiments",
            "distractor_presence": "prior work used for object extraction in visual scenes (general), not specific to this paper's perturbation suite",
            "performance_metrics": "Not evaluated in this paper; cited as effective in prior work for object extraction applied to Atari in Delfosse et al. (2023).",
            "computational_cost_details": "Not provided in this paper (refer to original Slot Attention paper for details).",
            "comparison_to_baselines": "Mentioned as a more complex object-extraction approach compared to OCCAM's simpler bounding-box masking; OCCAM preferred for simplicity and integration ease.",
            "transfer_learning_results": "Not evaluated here.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Cited literature notes slot methods can require careful tuning and may compress or mis-segment objects; paper states complex extraction can add noise or miss task-relevant details.",
            "ablation_studies": "Not part of this paper's ablations.",
            "sample_efficiency": "Not reported here.",
            "generalization_analysis": "Not performed in this paper for Slot Attention; referenced for prior evidence of improved generalization when using object extraction.",
            "reconstruction_quality": "Not applicable within this paper.",
            "task_relevance_analysis": "Paper contrasts learned unsupervised extractors (like Slot Attention) with OCCAM's simpler masking; argues OCCAM's simplicity avoids extra pretraining overhead.",
            "dynamic_abstraction": "Not discussed in this paper for slot methods.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not present in this paper regarding Slot Attention.",
            "pixel_fidelity_benefits": "Not discussed for Slot Attention in this paper.",
            "uuid": "e2248.7"
        },
        {
            "name_short": "SPACE",
            "name_full": "SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition",
            "brief_description": "An unsupervised object extraction model that decomposes scenes into objects using spatial attention and generative modeling, cited as prior art enabling object-centric RL.",
            "citation_title": "SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition",
            "mention_or_use": "mention",
            "model_name": "SPACE",
            "abstraction_level": "object-level unsupervised decomposition (generative)",
            "feature_selection_mechanism": "learned attention and generative modeling to infer object bounding boxes and latent factors",
            "task_domain": "cited in context of object extraction applied to Atari and RL (prior work), not used in experiments",
            "distractor_presence": "General image scenes; not specifically evaluated on HackAtari perturbations in this paper",
            "performance_metrics": "Not evaluated here (cited as successful in prior works for improving efficiency and generalization).",
            "computational_cost_details": "Not provided in this paper.",
            "comparison_to_baselines": "Described as more complex and requiring pretraining versus OCCAM's simple masking; OCCAM avoids pretraining overhead.",
            "transfer_learning_results": "Not reported in this paper.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Paper notes complex extractors can introduce noise or require priors (max objects, categories), potentially harming downstream RL if mis-specified.",
            "ablation_studies": "Not part of this paper's experiments.",
            "sample_efficiency": "Not reported here.",
            "generalization_analysis": "Not performed here; referenced as prior evidence that object extraction can aid generalization.",
            "reconstruction_quality": "Not applicable within this paper.",
            "task_relevance_analysis": "SPACE represents an approach that extracts object-centric factors which can be used to focus on task-relevant entities; OCCAM positions itself as a simpler alternative.",
            "dynamic_abstraction": "Not discussed.",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not provided here.",
            "pixel_fidelity_benefits": "Not discussed for SPACE in this paper.",
            "uuid": "e2248.8"
        },
        {
            "name_short": "Object-centric world models",
            "name_full": "Object-centric world models (general concept)",
            "brief_description": "A class of predictive/world models that represent environment dynamics at the object level (objects and their interactions) rather than raw pixels, cited as part of related work and motivation for object-centric abstractions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "object-centric world models (generic)",
            "abstraction_level": "object-level / relational (model dynamics between object entities)",
            "feature_selection_mechanism": "typically object decomposition plus learned dynamics (varies by method); in this paper referenced conceptually rather than implemented",
            "task_domain": "discussed in RL/robustness context (related work); not experimentally used here",
            "distractor_presence": "potentially robust to visual distractors by modeling object dynamics instead of pixels; paper advocates integration with OCCAM-like filtering",
            "performance_metrics": "Not evaluated in this paper; cited as promising approach in literature for robustness/generalization.",
            "computational_cost_details": "Not provided in this paper.",
            "comparison_to_baselines": "Paper positions OCCAM as complementary—structured abstraction can be combined with object-centric world models to improve focus on task-relevant elements while keeping computational cost lower than full symbolic extraction pipelines.",
            "transfer_learning_results": "Not evaluated here; object-centric world models in literature often aim to improve transfer via compositional dynamics, but no numeric results in this paper.",
            "multi_task_performance": "Not evaluated here.",
            "failure_modes": "Paper notes object-centric models can require structured priors (max objects, categories), risk of excessive compression, and added complexity/overhead.",
            "ablation_studies": "Not applicable within this paper.",
            "sample_efficiency": "Not quantified here.",
            "generalization_analysis": "Mentioned as part of the motivation to focus on objects to improve generalization; OCCAM shown to improve visual robustness without full world-model training.",
            "reconstruction_quality": "Not applicable.",
            "task_relevance_analysis": "Paper argues that object-centric world models plus adaptive filtering could help focus on task-relevant entities; however this is proposed future work, not experimentally demonstrated.",
            "dynamic_abstraction": "Not implemented here; suggested as future direction (adaptive abstraction and task-aware masking).",
            "exploration_vs_exploitation": "Not discussed.",
            "information_theoretic_analysis": "Not presented.",
            "uuid": "e2248.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "OCAtari: Object-centric Atari 2600 reinforcement learning environments",
            "rating": 2
        },
        {
            "paper_title": "Object-centric learning with slot attention",
            "rating": 2
        },
        {
            "paper_title": "SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition",
            "rating": 2
        },
        {
            "paper_title": "Boosting object representation learning via motion and object continuity",
            "rating": 2
        },
        {
            "paper_title": "Generalization and robustness implications in object-centric learning",
            "rating": 2
        },
        {
            "paper_title": "Contrastive abstraction for reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Interpretable concept bottlenecks to align reinforcement learning agents",
            "rating": 1
        }
    ],
    "cost": 0.022937,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Reinforcement Learning via Object-Centric Attention
3 Apr 2025</p>
<p>Jannis Blüml blueml@cs.tu-darmstadt.de 
Department of Computer Science
Technical University of Darmstadt
Germany</p>
<p>Hessian Center for Artificial Intelligence (hessian.AI)
Germany</p>
<p>indicating equal contribution</p>
<p>Cedric Derstroff cedric.derstroff@tu-darmstadt.de 
Department of Computer Science
Technical University of Darmstadt
Germany</p>
<p>Hessian Center for Artificial Intelligence (hessian.AI)
Germany</p>
<p>indicating equal contribution</p>
<p>Bjarne Gregori 
Department of Computer Science
Technical University of Darmstadt
Germany</p>
<p>Elisabeth Dillies 
Sorbonne Université
ParisFrance</p>
<p>Quentin Delfosse 
Department of Computer Science
Technical University of Darmstadt
Germany</p>
<p>Kristian Kersting 
Department of Computer Science
Technical University of Darmstadt
Germany</p>
<p>Hessian Center for Artificial Intelligence (hessian.AI)
Germany</p>
<p>German Research Center for Artificial Intelligence (DFKI)
Germany</p>
<p>Centre for Cognitive Science
DarmstadtGermany</p>
<p>Deep Reinforcement Learning via Object-Centric Attention
3 Apr 20256951E4A60F3A9F0A3A6CFADCB1BFC7EEarXiv:2504.03024v1[cs.LG]
Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details.To address this issue, object-centric agents have recently emerged.However, they require different representations tailored to the task specifications.Contrary to deep agents, no single object-centric architecture can be applied to any environment.Inspired by principles of cognitive science and Occam's Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information.Specifically, OC-CAM takes advantage of the object-centric inductive bias.Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL.These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines.</p>
<p>Introduction</p>
<p>Human visual processing, akin to the dual-system theory of fast and slow thinking (Kahneman, 2011), operates in two phases: a rapid, automatic process that scans the visual field to detect salient features (Treisman, 1985) and a sequential, focused attention mechanism that extracts complex representations from localized regions (Treisman &amp; Gelade, 1980).This hierarchical approach highlights the role of abstract representations-where key entities and their relationships serve as fundamental building blocks for human reasoning and planning (Baars, 1993;2002;Bengio, 2017;Goyal &amp; Bengio, 2022).</p>
<p>Reinforcement learning (RL) has predominantly relied on learning directly from raw pixel inputs without an explicit object extraction step, a paradigm introduced with Deep Q-Networks (DQN) (Mnih et al., 2015).While this task-agnostic architecture has enabled various algorithmic advancements, end-to-end convolutional neural network (CNN)-based approaches often exhibit sensitivity to noise and spurious correlations, leading to policies that fail under distribution shifts (Agnew &amp; Domingos, 2021;Yoon et al., 2023;Hermann et al., 2024).Farebrother et al. (2018) highlight that such agents frequently overfit to their training environments, struggling to generalize to task variations.A particularly striking example is the failure of RL agents in Pong, where they learn to base their actions primarily on the opponent's paddle position-a spurious shortcut-while largely ignoring the ball's trajectory (Delfosse et al., 2024a;c).As illustrated in Figure 1, CNN-based agents exhibit significant performance drops when evaluated outside their training environments.These limitations underscore the necessity of incorporating structured representations and inductive biases to improve generalization and robustness in RL.</p>
<p>&amp;FODVVLF</p>
<p>'41PRGHUQ 332
$JHQW +XPDQ1RUPDOL]HG6FRUH
,403HUIRUPDQFHZLWK&amp;RQILGHQFH,QWHUYDOV 2ULJLQDOYV0RGLILHG(QYLURQPHQWV</p>
<p>+XPDQ%DVHOLQH 2ULJLQDO 9DULDWLRQ</p>
<p>Figure 1: Deep agents cannot generalize to simpler scenarios.Testing deep agents in simpler or slightly changed versions of the environments reveals a significant performance drop, highlighting a critical limitation in their robustness and adaptability.The reported numbers are the interquartile mean (IQM) over 3 seeds, with 10 runs each on 11 games (C51 on six games).The exact performances are in Appendix F.</p>
<p>Therefore, inspired by the results from cognitive science, recent studies have proposed integrating abstraction processes into RL to enhance generalization (Zhao et al., 2021;Bertoin et al., 2022).The object-centric approaches offer a promising solution by disentangling scenes into object-level attributes and relationships.Advances like SPACE (Lin et al., 2020) and Slot Attention (Locatello et al., 2020), notably applied to Atari RL environments (Delfosse et al., 2023) demonstrate the potential of unsupervised object extraction methods that can extract object representations to improve efficiency and generalization (Patil et al., 2024).Thus, many symbolic policies have been developed to take advantage of object-centric extraction (Delfosse et al., 2024c;Kohler et al., 2024;Luo et al., 2024;Marton et al., 2024), with novel end-to-end object-centric neurosymbolic approaches that use limited (Luo et al., 2024) or even no supervision (Grandien et al., 2024).</p>
<p>However, integrating object extractors into RL agents presents several challenges.These methods often require expert-defined priors, such as the maximum number of objects, object categories, and relevant attributes (e.g., position, size, color, orientation, shape).Extracting excessive information introduces noise, increasing the risk of shortcut learning (Kohler et al., 2024), while excessive compression may discard critical task-relevant details, hindering learning a suitable policy.Moreover, object extraction models must be trained separately before RL can begin, adding computational overhead and limiting adaptability.</p>
<p>We introduce Object-Centric Attention via Masking (OCCAM), an object-oriented abstraction method inspired by the principle of Occam's Razor.OCCAM implements an object-centric attention bias by masking out non-object pixels from the input frames provided to CNN-based policies.This method relies on a simple object detection technique that only needs to extract object bounding boxes, avoiding complex pre-processing or fine-grained object analysis.By removing extraneous background information, OCCAM significantly reduces perceptual noise, limits reliance on spurious correlations, and enhances generalization across diverse environments.</p>
<p>Rethinking Input Representations</p>
<p>This section introduces our Object-Centric Attention via Masking (OCCAM) principle, a novel abstraction mechanism designed to enhance robustness and generalization.OCCAM's goal is to reduce the extreme dependency of classic CNN-based agents of irrelevant visual artifacts (Ma et al., 2020) while avoiding the necessity of building complex tasks-specific symbolic systems that rely on expert knowledge specific to each environment.</p>
<p>Object-Centric Attention via Masking</p>
<p>Inspired by Occam's Razor-which advocates for minimal yet effective explanations-OCCAM enhances RL by constructing structured visual representations.OCCAM systematically reduces visual complexity by eliminating background clutter while preserving object information, all without requiring domain-specific priors or external annotations.By applying a lightweight masking strategy (see Figure 2), OCCAM refines input representations, reducing perceptual noise and minimizing potential confounders.This ensures that CNN-based deep RL architectures (e.g., DQN, PPO) process only the most relevant features.</p>
<p>Object extraction is crucial in OCCAM's masking approach.Existing methods differ in their assumptions and design: motion-based approaches (e.g., Optical Flow (Farnebäck, 2003)) track dynamic elements but may struggle with static objects, while self-supervised feature learning (e.g., DINO (Zhang et al., 2022)), YOLO (Redmon et al., 2016) and Mask R-CNN (He et al., 2017) groups objects based on learned representations.While these methods effectively extract objectlike entities, symbolic extraction-which assigns objects high-level roles, relationships, or causal dependencies (Lake et al., 2017;Bengio, 2017)-introduces an additional layer of abstraction that typically relies on structured priors.While this is needed for symbolic approaches, it also adds another layer of complexity that can introduce biases, limit adaptability, and reduce scalability.</p>
<p>In this work, we evaluate different masking strategies that can either rely on task-agnostic or taskspecific representations, adapting to the available information.While fully task-agnostic masking enables generalization without predefined structures, when additional information-such as object categories or domain-specific knowledge-is accessible, OCCAM can integrate these details.This flexibility allows for scalable, adaptable, and efficient representations that improve RL generalization while maintaining computational efficiency.</p>
<p>A key advantage of OCCAM is its ease of integration, making it a practical alternative to conventional object-centric methods.Unlike symbolic approaches that rely on task-specific object extractors, OCCAM does not require pretraining or domain-specific priors.Delfosse et al. (2024c), however, argue that the color (of the ghosts) is a crucial feature to solve Pacman and MsPacman and that the orientation is necessary to solve Skiing.Thus, their symbolic method relies on task-specific fine-tuned object extractors to extract such concepts.In contrast, OCCAM allows the CNN to learn relevant object properties implicitly, reducing computational overhead while preserving a structured input.Moreover, most masking strategies maintain the same input dimensionality as DQN, ensuring compatibility with standard RL frameworks without requiring architectural modifications.OCCAM is not limited to a single masking approach but presents a general framework for scene abstraction, emphasizing essential information while filtering irrelevant details.To evaluate this, we analyze a range of abstraction levels-from binary masks, which aggressively reduce visual complexity, to structured, task-specific masks that retain the object classes as properties.</p>
<p>In summary, OCCAM provides robust and adaptable visual representations, balancing abstraction and expressivity to improve RL generalization in complex environments.</p>
<p>OCCAMs Abstraction Levels</p>
<p>Rather than presenting one new strategy, OCCAM supports multiple abstraction strategies that vary in visual detail and object-specific information.Below, we present four masking approaches that are concrete implementations of OCCAM's principles, as illustrated in Figure 2. All masks are visualized in Figure 3.</p>
<p>Object Masks: Retains object positions and appearances while removing background information.This representation preserves all object-specific features, including color, ensuring minimal loss of information while filtering irrelevant elements.</p>
<p>Binary Masks: Represents objects in a binary occupancy grid, where pixels inside object bounding boxes are set to 1, and all other pixels are 0.This removes object information, reducing potential spurious correlations but eliminating fine-grained details that may be useful for decision-making.</p>
<p>Class Masks: Encodes objects by category while discarding all other visual attributes.Each object is assigned a uniform representation based on its class, preserving semantic structure but requiring an object extraction method capable of classification, making it task-dependent.</p>
<p>Planes: Inspired by structured representations in chess AI (Browne, 2014;Silver et al., 2016;Czech et al., 2024) and the work by Davidson &amp; Lake (2020), this format represents object categories as separate binary planes, where each plane contains only one object type.This structured encoding improves policy stability and generalization but increases input dimensionality, scaling with the number of object classes.</p>
<p>Empirical Evaluation</p>
<p>To assess the effectiveness and limitations of OCCAM, we design a structured empirical analysis addressing the following research questions:</p>
<p>(Q1) How does OCCAM perform compared to raw pixel-based and symbolic OC approaches?</p>
<p>(Q2) Does OCCAM effectively reduce the tendency of RL agents to rely on irrelevant or spurious visual correlations?</p>
<p>(Q3) Can we find the optimal balance between representational simplicity and task performance across different abstraction levels?</p>
<p>We conduct experiments across multiple Atari environments to systematically evaluate OCCAM, focusing on three key aspects.First, we benchmark OCCAM's object-centric representations against conventional pixel-based reinforcement learning using Proximal Policy Optimization (PPO) (Schulman et al., 2017) and OCAtari (Delfosse et al., 2024b), a symbolic object-centric baseline.Second, we assess robustness to environmental variations by evaluating performance under standard and perturbed conditions.These perturbations, derived from HackAtari (Delfosse et al., 2024a), include visual modifications (e.g., color shifts, object recoloring) and gameplay variations (e.g., altered agent dynamics and enemy behavior).</p>
<p>A detailed description of the experimental setup, including environment selection, training procedures, and hyperparameter configurations, is provided in Appendix A. To evaluate how object-centric attention impacts RL performance compared to raw visual inputs and object-centric ones, we analyze aggregated human normalized scores (HNS) across multiple Atari environments (more about the metrics in Appendix C.This addresses the first research question by assessing whether OCCAM-based representations are comparable or superior to existing baselines.For this study, we compare six different input representations: the four masking approaches introduced in Section 2.2, alongside two baselines: DQN-like (Mnih et al., 2013): A standard deep RL representation, particularly in Atari (Fan, 2021), where agents process downsampled grayscale pixel inputs in a stack of four.This representation is computationally expensive and relies on convolutional feature extraction.In this study, we use PPO (Schulman et al., 2017) as architecture in combination with this representation.</p>
<p>Improved Performance with less Input Information</p>
<p>Semantic Vector (Delfosse et al., 2024b): A structured representation that encodes object attributes (e.g., position, size, and additional properties) for symbolic reasoning, independent of raw pixels.This format significantly reduces input dimensionality into a single vector.</p>
<p>The results in Figure 4 show that OCCAM matches or outperforms our baselines across six standard environments.These findings demonstrate that object-level information alone is sufficient for effective policy learning.Despite using a sparser input representation, OCCAM maintains or improves scores, indicating that removing task-irrelevant details does not degrade performance.</p>
<p>Generalization under Environmental Variations: How OCCAM Handles Perturbations</p>
<p>While OCCAM's object-centric representations improve robustness, they remain sensitive to certain environmental variations.To assess their generalization properties, we evaluate performance under two types of perturbations: visual modifications, which, e.g., alter object color while preserving game mechanics, and game logic alterations, which modify object behavior like movement patterns.Rather than relying solely on the HNS, we measure relative performance changes between the original and perturbed environments-the game normalized score (GNS).This approach provides a more detailed understanding of how different representations adapt across varying conditions.</p>
<p>Visual Robustness: OCCAM Representations Maintain Stability.As shown in Figure 5, OCCAM-based agents demonstrate robust performance under visual perturbations, validating the effectiveness of structured abstraction in RL.Representations such as Binary Masks and Planes, which remove background details while preserving object structures, allow agents to retain decisionmaking accuracy despite changes in object appearance.This result confirms that policies can generalize surface-level visual differences by focusing on underlying information such as position.</p>
<p>Game Logic Sensitivity: Abstraction Alone Is Not Enough.While OCCAM-based representations reduce reliance on visual distractions, they remain vulnerable to changes in game mechanics.In Freeway, for example, alterations to vehicle behavior cause a significant drop in performance, indicating that learned policies depend on predictable motion patterns.In Pong, structured representations-particularly the Planes-partially mitigate shortcut learning by enforcing object-centric reasoning rather than reliance on spurious correlations (cf.Table 1).It illustrates that the agents that use DQN-like or semantic vector as input, which are trained on Pong, suffer from immense performance degradations when evaluated in environments where the enemy stops moving when the ball is going to the agent (Lazy Enemy) or when the Enemy is hidden.Delfosse et al. (2024a) have shown that deep agents obtain the same performances when trained on each environment variation.The different OCCAM variations suffer from much lower performance degradations.Notably, OCCAM addresses this issue without requiring symbolic reasoning, contradicting the claim by Delfosse et al. (2024c) that symbolic extraction is necessary to resolve the Pong misalignment problem, suggesting that incorporating an object-centric and a locality bias reduces such shortcut dependence.Our conclusion can be seen from the perspective of Hermann et al. (2024), which explains that statistical (visual) agents learn to rely on the simplest decodable feature, not the most discriminative one.Here, the object-centric masking and locality importance prior (from the CNN) makes the agent rely on the ball's position to place its paddle instead of the correlated enemy's.As the ball is the object bouncing on the paddle, incorporating such a prior makes the agent robust to the decorrelated Lazy and Hidden Enemy variations.Nonetheless, this effect is inconsistent over multiple environments (cf. Figure 5).Performances can still be degraded for many types of task variations, and further work shall address these generalization problems.</p>
<p>The Trade-Off Between Abstraction and Expressivity in Visual Reasoning</p>
<p>A key challenge in designing effective RL representations is balancing abstraction and expressivity.While abstract representations enhance generalization and reduce reliance on spurious correlations, excessive simplification may remove critical task dynamics, limiting an agent's ability to learn optimal policies.Conversely, overly detailed representations risk encoding spurious patterns, leading to shortcut learning and overfitting.To examine this trade-off, we analyze the results in Table 2, which compare different visual abstraction strategies in MsPacman, Riverraid, and Skiing.These environments were chosen due to their differing spatial dependencies-MsPacman requires maze navigation, whereas Skiing relies on continuous movement based on the orientation of one player.</p>
<p>The results show that removing structural details-such as the maze layout in MsPacman, the river in Riverraid, or skier orientation in Skiing-does not significantly degrade performance.Agents trained with object masks outperform those using DQN-like representations in MsPacman, suggesting that explicit structural encoding is not always necessary for effective learning.Similar can be seen in Riverraid, where the river can be learned by playing the game.This, however, means changing the river layout can drastically impact the performance (cf.Table 2.In Skiing, performance remains unchanged mainly across representations, indicating again that explicit orientation encoding has little impact when task-relevant object information is retained, or information can be deduced implicitly using the temporal component in the stack of images provided.</p>
<p>The performance drop in the Semantic Vector representation highlights the limitations of excessive compression, as it removes spatial dependencies, impairing the agent's ability to model object relationships.This confirms that while symbolic representations improve interpretability, they struggle in tasks requiring fine-grained spatial or visual reasoning tasks.</p>
<p>Overall, these results indicate that abstraction remains effective as long as essential task dynamics are preserved.The optimal level of abstraction depends on the environment's structure and the agent's capacity to infer missing information from temporal or relational cues.</p>
<p>Choosing the Right Masking Strategy.</p>
<p>There is no universally optimal masking strategy-each has trade-offs depending on the environment and task requirements.Planes generally perform best, providing structured spatial representations that improve generalization.However, they increase input size and computational cost, which may not be ideal for efficiency-constrained applications.Among the other masking approaches, Class Masks require additional pre-processing to categorize objects but do not consistently offer a clear advantage over simpler alternatives.Object Masks retain more visual details, benefiting environments where fine-grained features matter.However, it also tends to be easier to find misaligned due to the additional potential confounders.Binary Masks enforce more substantial abstraction, reducing reliance on texture and color but potentially discarding helpful information, like the color of the ghosts in MsPacman.Ultimately, the choice depends on the balance between abstraction and expressivity and the specific demands of the task at hand.Also, this paper is not about the concrete mask and more about the idea.</p>
<p>4 Related Work  (Mnih et al., 2013) and AlphaGo (Schrittwieser et al., 2019), encode raw images into fixed-dimensional feature vectors.More structured approaches include feature-plane representations and scene graphs for modeling object relationships (Yang et al., 2018;Koner et al., 2021).While these methods offer advantages, they often incur high computational costs and require task-specific tuning (Locatello et al., 2020;Huang et al., 2022a;Farebrother et al., 2024), limiting their scalability.</p>
<p>Masking and Attention-Based Representations in RL.Selective attention mechanisms have been explored to enhance RL efficiency by focusing on task-relevant information (Mott et al., 2019;Manchin et al., 2019).Prior work has used attention-driven feature selection (Xu et al., 2015), spatial transformers (Jaderberg et al., 2015), and dynamic masking strategies (Manchin et al., 2019) to refine input representations.While these methods rely on learned attention weights, OCCAM provides a structurally defined abstraction that could be described as hard attention.Unlike learned attention, OCCAM applies a predefined masking strategy that explicitly filters task-irrelevant details without relying on gradient-based optimization.This structured approach avoids instability and computational overhead caused by attention-based feature selection.</p>
<p>Object-Centric RL.Object-centric RL focusing on objects and their interactions rather than raw pixels (Delfosse et al., 2024b;Patil et al., 2024).Techniques such as Slot Attention (Locatello et al., 2020) enable agents to decompose visual scenes into object-level representations, improving generalization and interpretability.Prior work has explored object-centric representations for RL generalization (Dittadi et al., 2022;Yoon et al., 2023) and applied them to Atari environments (Delfosse et al., 2024a;b;Davidson &amp; Lake, 2020).However, existing approaches often suffer from excessive compression, which can obscure spatial relationships crucial for decision-making.OCCAM addresses this trade-off by selectively preserving object structure while reducing unnecessary visual details, ensuring that abstraction does not come at the cost of expressivity.</p>
<p>Robustness and Generalization in RL.Ensuring robustness to environmental perturbations and domain shifts remains a key challenge in RL.Deep RL agents trained on raw pixels often overfit to spurious correlations, leading to catastrophic failures under minor changes in color or object placement, particularly in Atari benchmarks (Farebrother et al., 2018;Delfosse et al., 2024a).</p>
<p>Existing generalization techniques, such as data augmentation (Yarats et al., 2021), domain randomization (Tobin et al., 2017), and invariant feature learning (Zhang et al., 2021), require extensive tuning and struggle with out-of-distribution scenarios.In contrast, object-centric representations improve robustness by providing structured inputs that remain stable across visual variations.Our work extends these insights by integrating object-centric world models with adaptive filtering, ensuring RL agents focus on task-relevant elements and enhance robustness and interpretability.</p>
<p>Conclusion</p>
<p>This work examines how structured abstraction can improve generalization, robustness, and efficiency in reinforcement learning by reducing reliance on spurious correlations.Standard RL methods that process raw pixel inputs often capture irrelevant details, leading to overfitting and shortcut learning.In contrast, OCCAM applies object-centric attention, filtering out background noise while preserving task-relevant objects, allowing agents to focus on decision-critical elements.As shown in Figure 4 and 5, OCCAM-based representations match or exceed pixel-based approaches, particularly in robustness to visual perturbations.By enforcing structured abstraction, OCCAM enhances policy learning, demonstrating that removing task-irrelevant information does not degrade performance.</p>
<p>Limitations and Future Directions.While OCCAM reduces visual distractions, it remains sensitive to game logic perturbations, indicating that abstraction alone is insufficient for robust generalization.The effectiveness of different abstraction levels varies across tasks, suggesting that optimal representation strategies depend on task demands.Additionally, symbolic representations, such as OCAtari's vectorized encoding, struggle with spatial dependencies, leading to weaker performance in tasks requiring precise spatial reasoning.These findings highlight the trade-off between abstraction and expressivity, where preserving essential task dynamics is crucial for decision-making.</p>
<p>Future work should focus on adaptive abstraction mechanisms that dynamically adjust filtering based on task complexity and policy uncertainty.One approach is task-aware masking, which adapts to environmental conditions to improve generalization against game logic changes.This could involve temporal abstraction to track object interactions or causal reasoning to model dependencies.Further investigation into adaptive representation-policy learning could refine the balance between abstraction and expressivity, ensuring robust policy optimization across diverse environments.</p>
<p>This work contributes to developing more robust and generalizable RL agents that can operate effectively across varying environments by structuring input representations around task-relevant abstraction.</p>
<p>Broader Impact Statement</p>
<p>Object-centric representations in reinforcement learning improve generalization and interpretability, with potential applications in robotics, autonomous systems, and high-stakes decision-making.However, their effectiveness depends on accurate object extraction, which may introduce biases if objects are misidentified or omitted.Additionally, in dynamic environments, reliance on objectcentric features could lead to unexpected failures if critical task information is lost during abstraction.While these risks can be mitigated through careful design and evaluation, further research is required to enhance robustness across diverse settings and prevent unintended consequences.Investigating adaptive filtering mechanisms and more flexible abstraction strategies could improve reliability in real-world deployments.</p>
<p>A Experimental Setup</p>
<p>To evaluate the effectiveness of object-centric abstraction in RL, we conduct a series of controlled experiments using the Atari Learning Environment (ALE).We benchmark our approach against conventional pixel-based RL methods, including Deep Q-Networks (DQN) (Mnih et al., 2013), Proximal Policy Optimization (PPO) (Schulman et al., 2017), andC51 (Bellemare et al., 2017), as well as OCAtari (Delfosse et al., 2024b) as another object-centric representation with focus on symbolic representation.The DQN and C51 baseline models were taken from Gogianu et al. (2022), while the PPO and OCAtari ones were trained ourselves.To test robustness, we decided to test the trained agents not only in their original environment but also in their perturbations.These perturbations, derived from HackAtari (Delfosse et al., 2024a), include visual alterations (e.g., color changes, object recoloring), structural modifications (e.g., object displacement, swapped game elements), and gameplay variations (e.g., altered agent dynamics or enemy behavior).The metrics we were using are the human normalized score (HNS) and the game normalized score (GNS), both explained in Appendix C, over the aggregated game scores, using the interquartile mean (IQM).Calculations are done with rliable (Agarwal et al., 2021).</p>
<p>A.1 Environment Selection</p>
<p>We evaluate our framework across a diverse set of 12 Atari games selected to balance reactive and strategic decision-making tasks (Boxing vs Freeway), having environments where the background information or object features are crucial for the game (MsPacman and Skiing) as well as some classics, such as Pong or Breakout.This selection allows us to assess the adaptability of different representations across a spectrum of task complexities.To list all environments again, we used Amidar, BankHeist, Bowling, Boxing, Breakout, Freeway, Frostbite, MsPacman, Pong, Riverraid, Skiing, and SpaceInvaders.</p>
<p>A.2 Training PPO</p>
<p>PPO is a policy gradient algorithm widely used in deep RL that optimizes a clipped surrogate objective to balance exploration and stability (Schulman et al., 2017).Unlike value-based approaches such as DQN, PPO directly learns an optimal policy distribution and is known for its sample efficiency and stable convergence properties.It is one of the most common architectures used in RL and, as such, is a good baseline as well as a base for our experimental section.</p>
<p>We trained six PPO agents for the experiments using varying input representations.This allowed us to isolate the trade-offs between abstraction strength and spatial reasoning capacity and evaluate our visual reasoning with object-centric attention.All agents are trained on the unmodified versions of these environments for 40 million frames using PPO, with hyperparameters adapted from Huang et al. (2022b) and listed below.For PPO we adhere to standardized implementation guidelines (Huang et al., 2022a) to ensure comparability and used the aforementioned CleanRL framework (Huang et al., 2022b) for training.The object-centric representations, derived using OCAtari, selectively preserve task-relevant features while eliminating extraneous background information.Importantly, no fine-tuning is performed in perturbed environments, ensuring that generalization performance reflects the robustness of the learned representations rather than additional adaptation.</p>
<p>Performance is evaluated using average episodic rewards across 10 games per seed, with three fixed seeds per experiment.All experiments are conducted on NVIDIA A100 GPUs, with training times averaging around 2-4 hours per agent per seed for the DQN-like, Object Mask, Binary Mask, and Class Mask.Training the Semantic Vector representation took less than 2h (avg.1h 38min), while for Planes, the environment highly influenced the training time and took 6-10h per game per seed.</p>
<p>A.3 Hyperparameters</p>
<p>We detail the hyperparameters used in our Proximal Policy Optimization (PPO) training to ensure reproducibility and consistency in our experiments.These settings were chosen based on prior literature.</p>
<p>B Reproducibility Statement</p>
<p>We provide full experimental details to facilitate reproducibility, including hyperparameter configurations, random seeds, and training scripts.Each model is trained with three independent seeds (0, 1, 2) to ensure statistical robustness and account for variance in reinforcement learning training.Our implementation follows the CleanRL framework (Huang et al., 2022b), a well-established reinforcement learning library designed for transparency, simplicity, and ease of replication.</p>
<p>Code and Data.Our masking approaches are implemented as wrappers for the OCAtari/HackAtari environments (Delfosse et al., 2024a;b), as they provide a consistent object extraction that is easy to use for Atari games as well as an easy way to adapt and create perturbations.Our code and some models to test it can be found under https://github.com/VanillaWhey/OCAtariWrappers.</p>
<p>Test it yourself.To test our approach, we provide a small set of models that can be used with the provided run, print, and evaluation scripts (see scripts folder in the repository) to visualize the results.With the scripts, you can measure the performance reported in the paper and test other perturbations and games.Our training is based on a slight adaptation of the CleanRL framework.</p>
<p>To run the evaluation script with the correct perturbations, see the commands in Appendix D.</p>
<p>E Representations in MsPacman and Frostbite</p>
<p>Figure 3 displays the input representations used in our experimental section, illustrating their differences and highlighting which elements are retained or abstracted.This section presents two additional examples, applying the exact representations to different games to further examine their impact.By comparing these variations, we can better understand how various levels of abstraction influence learning across diverse environments.</p>
<p>F Additional/Extended Results</p>
<p>Evaluation Setup</p>
<p>We evaluate all agents using deterministic versions of the Arcade Learning Environment (ALE) to ensure a controlled comparison across input representations and perturbed variants.All agents are evaluated on three seeds with 10 episodes per seed per game.For the evaluation, we follow Machado et al. (2018).However, contrary to this, we disable sticky actions during evaluation by setting the repeat action probability to zero.While sticky actions typically introduce stochasticity for robustness assessments, our primary goal is to measure the agent's ability to generalize across structured, deterministic modifications-such as visual or behavioral changes-without confounding effects from input-level randomness.As such, our evaluation setup is designed to isolate the effects of representation and abstraction.Results with sticky actions are still reported to enable comparison with other work and to check the robustness of the created policies against noise.Still, they are not further discussed in this work.</p>
<p>Results</p>
<p>Table 5 presents the average performance of different RL agents-DQN, C51, and PPO-across a selection of Atari environments.The agents were trained exclusively in the standard environment (underlined) and evaluated on both their original settings and perturbed variations (gray-shaded).</p>
<p>The perturbations include modifications such as color shifts, altered object properties, and behavioral changes designed to assess each method's robustness and generalization capabilities.</p>
<p>The results show that all agents perform relatively well in the standard environments, but experience varied performance degradation in perturbed settings.DQN, for instance, shows significant instability in perturbed settings, especially in environments like Red Player, Blue Enemy (Boxing), and Player and Ball Red (Breakout), where performance drops substantially.C51, while more stable in some cases, also struggles with drastic changes, particularly in Stopped Ice (Frostbite).PPO is the most resilient, maintaining reasonable performance across some perturbed environments but still showing declines in all cases.These observations indicate that current pixel-based deep RL agents tend to overfit visual features during training, reducing adaptability when confronted with environment variations.The detailed performance results of PPO across different input representations are provided in Table 6.This table presents the raw episodic rewards obtained in both standard (underlined) and perturbed (gray-shaded) environments, averaged over three random seeds.These scores serve as the basis for the normalized performance visualizations in Figures Figure 4 and   we also report the Median and Mean.The main message that OCCAM can mitigate performance loss in visually altered environments can also be seen in these metrics.Numbers are from Table 6.Next to the interquartile mean (IQM), we also report the Median and Mean.These metrics also show that OCCAM approaches still remain vulnerable to game logic changes.Numbers are from Table 6.The standard reward function in Skiing is known to be malformed and sparse, making it difficult for reinforcement learning agents to optimize their behavior effectively (see Delfosse et al. (2024c)).</p>
<p>To enable training, we modified the reward function to incorporate orientation, speed, and score changes, allowing agents to learn meaningful policies.However, all final evaluations use the original reward function to ensure comparability.</p>
<p>The reward function is</p>
<p>Figure 2 :
2
Figure 2: Object-Centric Attention via Masking.An object detection method detects moving objects, allowing it to mask out the irrelevant background details.Optionally, objects can be classified to obtain a class-augmented mask.This representation is then passed to the CNN-based agent.</p>
<p>Figure 3 :
3
Figure 3: The different extracted representations compared in this paper.For a given frame (left), (a) gray scaling and resizing are applied to reduce computational complexity.On top of it, (b) uses an object extractor to mask out the background information, (c) whitens the bounding boxes, (d) relies on a classifier to assign each object box a given class color, while (e) separates the masks in different planes for each class.Agents are provided with stacks of the latest 4 extracted representations, except for the semantic vector (f), which extracts symbolic properties of the depicted objects from the last 2 frames.More examples are provided in Appendix E.</p>
<p>Figure 4:OCCAM-based representations match or surpass pixel inputs, showing that abstraction improves performance.This figure compares PPO agents using different input types.OCCAM preserves effectiveness despite filtering details, proving structured inputs can replace raw pixels.Full results are in Appendix F.</p>
<p>Figure 5 :
5
Figure 5: OCCAM-based representations improve robustness to visual perturbations but remain vulnerable to game logic changes.This figure illustrates the relative performance (game normalized score) of PPO agents utilizing different input representations under both (a) visual and (b) game logic perturbations.While OCCAM-based representations significantly mitigate performance degradation due to visual modifications, they remain sensitive to changes in game mechanics, highlighting the limits of abstraction in RL.The game-specific scores are in Appendix F.</p>
<p>Figure 6 :
6
Figure 6: Illustration of different game variants used in our study.Games are grouped on a blueshaded background, starting with the original game on the left.These variants introduce modifications such as color changes and (opponent) behavior variations to study their impact on learning and generalization.</p>
<p>Figure 7 :
7
Figure 7: Alternative to Figure 3, showcasing the input representations using Frostbite as an example.This visualization highlights how different representations preserve or abstract various elements within the game environment</p>
<p>Figure 8 :
8
Figure 8: Alternative to Figure3, showcasing the input representations using MsPacman as an example.This visualization highlights how the masking approach can visualize and take in small objects incorrectly.</p>
<p>Figure 5.The results highlight the differences in robustness across input representations, particularly under visual and structural perturbations, offering deeper insight into the trends observed in the main text.</p>
<p>Figure 9 :Figure 10 :
910
Figure9: Extended Version of Figure4.Next to the interquartile mean (IQM), we also report the Median and Mean.The main message that OCCAM can keep up with PPO also can be seen in these metrics.Numbers are from Table6.</p>
<p>Figure 11 :
11
Figure11: Extended Version of Figure5b: Game logic changes.Next to the interquartile mean (IQM), we also report the Median and Mean.These metrics also show that OCCAM approaches still remain vulnerable to game logic changes.Numbers are from Table6.</p>
<p>Table 1 :
1
Having the correct inductive bias can reduce misalignment.Performance comparison of different representations in the Pong environment with and without a lazy enemy.The drop in performance (in percentage) highlights the impact of different input representations on robustness.The color gradient emphasizes the severity of the drop, with stronger red indicating a greater decline in performance.
GameDQN-likeObject Masks Binary Masks Class MasksPlanesSemantic VectorPong21.00 [21,21]21.00 [21,21] 21.00 [21,21] 21.00 [21,21] 21.00 [21,21]21.00 [21,21]Lazy Enemy−4.38 [−11,−0] 20.00 [20,20] 11.25 [−2,20]8.25 [3,14]12.44 [−1,20] −21.00 [−21,−21]Hidden Enemy-21.00 [21,21] 21.00 [21,21] 21.00 [21,21] 21.00 [21,21] −20.88 [−21,−21]</p>
<p>Table 2 :
2
Removing the maze structure (MsPacman), river (Riverraid), or skier orientation (Skiing) does not degrade performance.Despite reducing structural information, agents trained with Object and Binary Masks perform comparably, suggesting that task-relevant features remain sufficient for effective learning.Blue marks all values within 5% of the best performance.* Skiing has an ill-defined reward function, which needed adaptation in training (cf.Section F.1). −3203.62 [−3233,−3175] −3235.25 [−3261,−3206] −3263.50[−3323,−3229] −3201.25 [−3229,−3176] −3206.31[−3237,−3178]
GameDQN-likeObject MasksBinary MasksClass MasksPlanesMsPacman3174.38 [2792,3637]5880.00 [5531,6068]4833.12 [4583,5221]4549.38 [3810,5188]7187.50 [6779,7415]Skiing  Riverraid7849.38 [7434,8181]7938.75 [7780,8068]8076.88 [7872,8351]8160.00 [7998,8274]7953.75 [7841,8188]Linear River6689.38 [5086,7879]2903.75 [2750,3179]2705.00 [2690,2724]2832.50 [2680,3080]2931.88 [2761,3172]
*</p>
<p>Table 3 :
3
Key hyperparameters used for PPO training.
HyperparameterValue HyperparameterValueSeed{0, 1, 2} Learning Rate (α)2.5 × 10 −4Total Timesteps10 7Number of Environments10Batch Size (B)1280Minibatch Size (b)320Update Epochs4GAE Lambda (λ)0.95Discount Factor (γ)0.99Value Function Coefficient (c v )0.5Entropy Coefficient (c e ) 0.01Clipping Coefficient (ϵ)0.1Clip Value LossTrue Max Gradient Norm (∥g∥ max )0.5</p>
<p>Table 4 :
4
Key hyperparameters regarding the used environments.We are using Gymnasium and the ALE, following the best practices byMachado et al. (2018).
HyperparameterValueHyperparameterValueALE version0.8.1Gymnasium version 0.29.1Environment versionv5Frameskip4Buffer Window Size4 (PPO), 2 (Sem. Vector) Observation ModeRGBRepeat Action Probability 0.25 (Training), 0 (Testing) Full Action Space FalseContinuousFalse</p>
<p>Table 5 :
5
This table presents the average performance (interquartile mean + 95% confidence interval) of different RL agents evaluated in standard (underlined) and perturbed (gray-shaded) environments.All agents were trained only in the standard setting for 40M frames and evaluated on three seeds, playing 30 games per environment.A visualized version of this can be seen in Figure1.
Game (Variant)DQNC51PPOAmidar389.25 [258,565]-564.75 [540,575]Enemy to Pig314.25 [207,503]-555.00 [543,562]Player to Roller70.81 [58,82]-</p>
<p>Table 6 :
6
This table compares the average episodic rewards (interquartile mean + 95% confidence interval) achieved by PPO using different input representations: the standard DQN-like representation, OCAtari's Semantic Vector, and our structured masking approaches, including Object Masks, Binary Masks, Class Masks, and Planes.Results are averaged over three random seeds, with standard deviations indicating variability.All agents were trained in the original game environment and evaluated in the standard setting (underlined) and visually or behaviorally perturbed variations (gray-shaded).The table highlights how different levels of abstraction in input representations impact the agents' performance and robustness across diverse environments.These results are then normalized and visualized in Figure4and Figure5.
Game (Variant)DQN-likeObject MasksBinary MasksClass MasksPlanesSemantic Vector</p>
<p>Table 7 :
7
Evaluation with Repeat Action Probability = 0.25.These results extend Tables5 and 6by adding a robustness test.Enabling "Sticky Actions", as proposed byMachado et al. (2018), is a common practice in evaluating Atari agents and is provided to make comparison to related work easier.
Semantic VectorPlanesClass MasksBinary MasksObject MasksPPOMDQNC51DQN431.44 [318,575]Game (Variant)Amidar
-F.1 Reward Function Adjustment in Skiing</p>
<p>AcknowledgmentsWe would like to express our gratitude for funding our project to the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts (HMWK).This project was enabled due to their joint support of the National Research Center for Applied Cybersecurity ATHENE, via the "SenPai: XReLeaS" project and the cluster project within the Hessian Center for AI (hessian.AI) "The Third Wave of Artificial Intelligence -3AI".Supplementary MaterialsThe following content was not necessarily subject to peer review.C Human Normalized and Game Normalized ScoresIn this work, we used two widely used metrics comparing the performance of agents in Atari: the IQM over human normalized scores (HNSs) and the game normalized score (GNS) over the game scores, aggregated with IQM, using rliable(Agarwal et al., 2021).Both provide standardized performance evaluations across different environments.C.1 Human Normalized Score (HNS)The HNS is a metric that compares an RL agent's performance to that of a human expert and a random agent.It is computed as follows:where:• Score agent is the episodic score achieved by the RL agent.• Score random is the score obtained by a random policy.• Score human is the score achieved by a human expert.An HNS value of 1.0 means the agent matches human performance.Values close to 0 suggest the agent performs at a level similar to random play.In this work, we assumed that our perturbations only slightly change the difficulty and performance of the human expert.Hence, we assumed the same human score for all perturbations in the original game.We agree that evaluating humans on perturbations would be the correct way, but it requires that the possibility exists.C.2 Game Normalized Score (GNS)Unlike HNS, which compares performance to a human baseline, the GNS measures an agent's performance relative to its performance in the original (unperturbed) environment.This ensures that an agent playing in the original environment always receives a score of 1.0, while agents evaluated in modified environments are measured relative to this baseline:where:• Score agent is the episodic score achieved by the RL agent in the modified environment.• Score original is the agent's performance in the original, unperturbed environment.With a GNS of 1.0, the agent performs identically to its original environment performance.Scores above 1.0 indicate improved performance in the new environment, while scores below 1.0 signify a performance drop.C.3 Interpretation and Application• The HNS metric provides a way to evaluate whether an RL agent is reaching or surpassing humanlevel play.It is widely accepted and used in a broad range of works, making the results of this work comparable.• The GNS metric ensures a fair comparison of robustness across different environmental conditions by using the original performance as a reference.D Variants OverviewBelow, we briefly describe each variant used in our study.Descriptions are done by us or taken from the ALE Documentation (?).These variants are visualized in Figure6and created using the HackAtari Environment(Delfosse et al., 2024a).The command needed to start the eval or run script with this variant is given below.This can be used to visualize or evaluate the models' performances.python scripts/eval.py-g $GAME -a $MODEL_PATH -m $MODIFICATION_LIST ,e.g., python scripts/eval.py-g Boxing -a models/Boxing/0/model.py\ -m color_player_red color_enemy_blueD.1 Amidar VariantsAmidar: Amidar is similar to Pac-Man: You are trying to visit all places on a 2-dimensional grid while simultaneously avoiding your enemies.Enemy to Pig: Change the enemy warriors into Pigs.The game logic stays the same.Player to Roller: Changing the sprite of the player figure from a human to a paint roller.The game logic stays the same.D.2 BankHeist VariantsBankHeist: You are a bank robber and (naturally) want to rob as many banks as possible.You control your getaway car and must navigate maze-like cities.The police chase you and will appear whenever you rob a bank.You may destroy police cars by dropping sticks of dynamite.You can fill up your gas tank by entering a new city.This is your BankHeist.Random City: The starting city and the city you enter are randomly selected and do not follow a pattern.D.3 Bowling VariantsBowling: Your goal is to score as many points as possible in the game of Bowling.A game consists of 10 frames, and you have two tries per frame.Shift Player:The player starts closer to the pins.D.4 Boxing VariantsBoxing: The standard Boxing environment where two players compete to land punches.Red Player, Blue Enemy:A modified version where one player is red and the other is blue, potentially influencing object perception.D.5 Breakout VariantsBreakout: The original Breakout environment where the player controls a paddle to break bricks.All Blocks Red: All blocks are red, removing the color of blocks which does not hold game-relevant information.Player and Ball Red: The paddle is a little redder than in normal, potentially changing agent perception and behavior.D.6 Freeway VariantsFreeway: The standard Freeway environment where a chicken crosses a highway with moving cars.All Black Cars: All vehicles are black, reducing visual diversity between the cars.There are no additional changes.Stop All Cars: All cars are stopped on the edge of the frame, making it trivial to pass the street.D.7 Frostbite VariantFrostbite: The original Frostbite environment where the player builds an igloo while avoiding hazards.To collect ice, the player has to jump between moving ice shelves.Static Ice: Ice platforms remain fixed instead of moving, altering difficulty (making it much easier).D.8 MsPacman VariantsMsPacman The standard MsPacman environment that is very similar to the Pacman environment.Level X: A later level with a changed maze structure.No agent ever reached this level in training.As such, it is out of distribution for all agents.D.9 Pong VariantsPong: The standard Pong environment where two paddles hit a ball back and forth.Lazy Enemy: The opponent stays still while the ball is flying away from it and only starts moving after the player hits the ball.No visual changes are visible.This was presented as one of two examples of misalignment inDelfosse et al. (2024c).Hidden Enemy: The opponent is hidden for the player.The only observable objects are the paddle and the Ball.This follows the experiment byDelfosse et al. (2024c)and is used to compare to their work directly.D.10 Riverraid VariantsRiverraid: In Riverraid, you control a jet that flies over a river: you can move it sideways and fire missiles to destroy enemy objects.Each time an enemy object is destroyed, you score points (i.e., rewards).Color Set X: Change the color of all objects to a different preset color-no change in the game logic.Linear River: Change the river always to have the same shape, a single line, and always the same width-no splits, etc.D.11 SpaceInvaders VariantsSpaceInvaders: In SpaceInvaders, your objective is to destroy the space invaders by shooting your laser cannon at them before they reach the Earth.The game ends when all your lives are lost after taking enemy fire or when they reach the earth.Shields off by X: The shields are moved X pixel(s) to the right.No further changes.return reward
Deep reinforcement learning at the edge of the statistical precipice. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G Bellemare, Advances in Neural Information Processing Systems. 2021</p>
<p>Relevance-guided modeling of object dynamics for reinforcement learning. William Agnew, Pedro Domingos, arXiv:2003.013842021arXiv preprint</p>
<p>An attentive approach for building partial reasoning agents from pixels. Safa Alver, Doina Precup, Transactions on Machine Learning Research. 2024</p>
<p>A cognitive theory of consciousness. J Bernard, Baars, 1993Cambridge University Press</p>
<p>The conscious access hypothesis: origins and recent evidence. J Bernard, Baars, Trends in cognitive sciences. 2002</p>
<p>A distributional perspective on reinforcement learning. Will Marc G Bellemare, Rémi Dabney, Munos, International conference on machine learning. 2017</p>
<p>The consciousness prior. Yoshua Bengio, 2017</p>
<p>Look where you look! saliency-guided q-networks for generalization in visual reinforcement learning. David Bertoin, Adil Zouitine, Mehdi Zouitine, Emmanuel Rachelson, Advances in Neural Information Processing Systems. 2022</p>
<p>Representation matters for mastering chess: Improved feature representation in alphazero outperforms changing to transformers. Cameron Browne, Proceedings of the 42th Annual Meeting of the Cognitive Science Society -Developing a Mind: Learning in Humans, Animals, and Machines, CogSci 2020, virtual. the 42th Annual Meeting of the Cognitive Science Society -Developing a Mind: Learning in Humans, Animals, and Machines, CogSci 2020, virtual2014. July 29 -August 1, 2020, 2020Proceedings of the 27th European Conference on Artificial Intelligence (ECAI), 2024. Guy Davidson and Brenden M. Lake</p>
<p>Boosting object representation learning via motion and object continuity. Quentin Delfosse, Wolfgang Stammer, Thomas Rothenbächer, Dwarak Vittal, Kristian Kersting, Machine Learning and Knowledge Discovery in Databases: Research Track. 2023</p>
<p>Hackatari: Atari learning environments for robust and continual reinforcement learning. Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Kristian Kersting, Interpretable Policies Workshop @ The Reinforcement Learning Conference. 2024a</p>
<p>OCAtari: Object-centric Atari 2600 reinforcement learning environments. Quentin Delfosse, Jannis Blüml, Bjarne Gregori, Sebastian Sztwiertnia, Kristian Kersting, Reinforcement Learning Journal. 2024b</p>
<p>Interpretable concept bottlenecks to align reinforcement learning agents. Quentin Delfosse, Sebastian Sztwiertnia, Mark Rothermel, Wolfgang Stammer, Kristian Kersting, The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024c</p>
<p>Generalization and robustness implications in object-centric learning. Andrea Dittadi, Samuele S Papa, Michele De Vita, Bernhard Schölkopf, Ole Winther, Francesco Locatello, arXiv:2112.04145Jiajun Fan. A review for deep reinforcement learning in atari: Benchmarks, challenges, and solutions. Baltimore, Maryland, USA17-23 July 2022. 20212022arXiv preprintInternational Conference on Machine Learning, ICML 2022</p>
<p>Jesse Farebrother, Marlos C Machado, Michael Bowling, arXiv:1810.00123Generalization and regularization in dqn. 2018arXiv preprint</p>
<p>Aviral Kumar, and Rishabh Agarwal. Stop regressing: Training value functions via classification for scalable deep RL. Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao, Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad. Sweden2024. June 29-July 2, 2003 Proceedings 13, 2003ICML</p>
<p>Atari agents. Florin Gogianu, Tudor Berariu, Lucian Bus, Elena Burceanu, 2022</p>
<p>Inductive biases for deep learning of higher-level cognition. Anirudh Goyal, Yoshua Bengio, Proceedings of the Royal Society A. 2022</p>
<p>Interpretable end-to-end neurosymbolic reinforcement learning agents. Nils Grandien, Quentin Delfosse, Kristian Kersting, 2024arXiv preprint</p>
<p>Mask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>On the foundations of shortcut learning. Katherine L Hermann, Hossein Mobahi, Thomas Fel, Michael Curtis Mozer, The Twelfth International Conference on Learning Representations, ICLR 2024. Vienna, AustriaMay 7-11, 2024, 2024</p>
<p>The 37 implementation details of proximal policy optimization. Shengyi Huang, Rousslan Fernand, Julien Dossa, Antonin Raffin, Anssi Kanervisto, Weixun Wang, ICLR Blog Track. 2022a</p>
<p>Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Shengyi Huang, Rousslan Fernand, Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, G M João, Araújo, Journal of Machine Learning Research. 2022b</p>
<p>Spatial transformer networks. Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems. Montreal, Quebec, CanadaDaniel Kahneman. Thinking, fast and slow2015. December 7-12, 2015. 2015. 2011</p>
<p>Interpretable and editable programmatic tree policies for reinforcement learning. Hector Kohler, Quentin Delfosse, Riad Akrour, Kristian Kersting, Philippe Preux, arXiv:2405.149562024arXiv preprint</p>
<p>Graphhopper: Multi-hop scene graph reasoning for visual question answering. Rajat Koner, Hang Li, Marcel Hildebrandt, Deepan Das, Stephan Volker Tresp, Günnemann, ISWC. 2021</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 2017</p>
<p>SPACE: unsupervised object-oriented scene representation via spatial attention and decomposition. Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, Sungjin Ahn, International Conference on Learning Representations, ICLR. 2020</p>
<p>Object-centric learning with slot attention. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, Advances in Neural Information Processing Systems. 2020</p>
<p>End-to-end neurosymbolic reinforcement learning with textual explanations. Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, Qing Li, arXiv:2403.12451Discriminative particle filter reinforcement learning for complex partial observations. Xiao Ma, Peter Karkus, David Hsu, Wee Sun Lee, Nan Ye, 2024. 2020arXiv</p>
<p>Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents (extended abstract). C Marlos, Marc G Machado, Erik Bellemare, Joel Talvitie, Matthew J Veness, Michael Hausknecht, Bowling, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence. the Twenty-Seventh International Joint Conference on Artificial Intelligence2018</p>
<p>Reinforcement learning with attention that works: A self-supervised approach. Anthony Manchin, Ehsan Abbasnejad, Anton Van Den, Hengel, Neural Information Processing -26th International Conference, ICONIP 2019. Sydney, NSW, AustraliaDecember 12-15, 2019. 2019Proceedings, Part V</p>
<p>Sascha Marton, Tim Grams, Florian Vogt, Stefan Lüdtke, Christian Bartelt, Heiner Stuckenschmidt, Sympol: Symbolic tree-based on-policy reinforcement learning. 2024arXiv preprint</p>
<p>Playing atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin A Riedmiller, arXiv:1312.56022013arXiv preprint</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 2015</p>
<p>Towards interpretable reinforcement learning using attention augmented agents. Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, Danilo Jimenez Rezende, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2019. 2019. December 8-14, 2019. 2019</p>
<p>Vihang Patil, Markus Hofmarcher, Elisabeth Rumetshofer, Sepp Hochreiter, arXiv:2410.00704Contrastive abstraction for reinforcement learning. 2024arXiv preprint</p>
<p>You only look once: Unified, real-time object detection. Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, L Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P Lillicrap, David Silver, Nature. 2019</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 2016</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). 2017</p>
<p>Computer vision, graphics, and image processing. Anne Treisman ; Anne, M Treisman, Garry Gelade, Cognitive Psychology. 1985. 1980A feature-integration theory of attention</p>
<p>Show, attend and tell: Neural image caption generation with visual attention. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C Courville, Ruslan Salakhutdinov, Richard S Zemel, Yoshua Bengio, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015. the 32nd International Conference on Machine Learning, ICML 2015Lille, France6-11 July 2015. 2015</p>
<p>Graph R-CNN for scene graph generation. Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, Devi Parikh, 10.1007/978-3-030-01246-5_41Computer Vision -ECCV 2018 -15th European Conference. Munich, GermanySeptember 8-14. 2018. 2018Proceedings, Part I</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Denis Yarats, Ilya Kostrikov, Rob Fergus, International Conference on Learning Representations. 2021</p>
<p>An investigation into pre-training objectcentric representations for reinforcement learning. Jaesik Yoon, Yi-Fu Wu, Heechul Bae, Sungjin Ahn, arXiv:2302.044192023arXiv preprint</p>
<p>Learning invariant representations for reinforcement learning without reconstruction. Amy Zhang, Thomas Rowan, Roberto Mcallister, Yarin Calandra, Sergey Gal, Levine, International Conference on Learning Representations. 2021</p>
<p>Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, Heung-Yeung Shum, arXiv:2203.03605Dino: Detr with improved denoising anchor boxes for end-to-end object detection. 2022arXiv preprint</p>
<p>A consciousness-inspired planning agent for model-based reinforcement learning. Mingde Zhao, Zhen Liu, Sitao Luan, Shuyuan Zhang, Doina Precup, Yoshua Bengio, 2021. 149.75 [124,178] BankHeist 1281.25 [1215,1326] - 1176.88 [1139,1212]Advances in neural information processing systems</p>
<p>Random City. 1228.75 [1158,1286] - 1165.001131,1198</p>
<p>. Red Player, Blue Enemy −2.50 [−7,1] −9.19 [−13,−5] −2.38 [−10,5</p>
<p>All Cars. Black 12.38 [9,14] 22.00 [22,22] 23.0622,24</p>
<p>8.56 [1,21] 33.12Stop All Cars. 21,41] 7.25 [0,20] Start and Stop Cars 13.50 [8,17] 20.50 [20,21] 8.25 [2,19] Frostbite 2409.38 [1668,3321] 3613.12 [3363,3931] 46.88 [40,83</p>
<p>Static Ice. 41.25 [20,73] 0.00 [0,0] 10.00 [10,10] MsPacman 2365.00 [2248,2418] - 3174.38 [2792,3637]</p>
<p>Red Player, Blue Enemy −2.38. −10,5] 76.75 [67,84] 96.19 [95,98] 96.94 [95,98] 98.50 [97,100] 100.00 [100,100</p>
<p>All Blocks Red. 306.31 [277,332] 314.56 [268,345] 347.19 [317,367] 279.25 [254,307] 374.12 [361,388] 46.9444,56] Player and Ball Red 52.19 [27,89] 170.50 [105,251] 348.62 [314,374] 289.19 [262,320] 373.81 [360,388] 47.19 [43,51</p>
<p>All Cars. Black 23.06 [22,24] 22.44 [20,24] 33.38 [33,34] 32.56 [32,33] 33.31 [33,34] 30.25 [30,31]</p>
<p>Start and Stop Cars. 46.88 [40,83] 46.88 [40,79] 46.88 [40,79] 51.88 [40,74] 48.75 [40,81] 62.50Stop All Cars. 0,17] 7.81 [0,20] 0.00 [0,0. 11,14] 7.06 [5,9. 47,82</p>
<p>Static Ice. 10.00 [10,10] 0.00 [0,1] 5.00 [2,9] 0.000,0] 2.50 [0,17] 2.50 [0,19</p>            </div>
        </div>

    </div>
</body>
</html>
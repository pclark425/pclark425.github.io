<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-e7ad08848d5d7c5c47673ffe0da06af443643bda</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e7ad08848d5d7c5c47673ffe0da06af443643bda" target="_blank">Large Language Models are Zero-Shot Reasoners</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples.</p>
                <p><strong>Paper Abstract:</strong> Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding"Let's think step by step"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3537.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3537.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (text-davinci-002) / MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on the MultiArith arithmetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applies a single zero-shot trigger prompt (e.g., "Let's think step by step") in a two-stage prompting pipeline to elicit step-by-step reasoning from InstructGPT (text-davinci-002) and then extract the final numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of GPT-3 (decoder-only Transformer) trained with reinforcement learning from human feedback to follow instructions; exact parameter count not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step arithmetic word problems requiring multi-step symbolic and numeric calculation and reasoning (from MAWPS repository).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT: prepend a short trigger (e.g., "Let's think step by step.") in a two-stage prompting pipeline (1. generate chain-of-thought reasoning; 2. answer extraction prompt to parse final answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot-CoT accuracy 78.7% (text-davinci-002, greedy decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard zero-shot prompting accuracy 17.7%; Few-shot 8-shot prompting accuracy ~33.8%; Few-shot-CoT (8 exemplars) accuracy 93.0% (text-davinci-002).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improved by +61.0 percentage points over standard zero-shot (17.7% -> 78.7%); substantially better than standard few-shot (~33.8%), but lower than Few-shot-CoT (93.0%), i.e., a remaining gap of ~14.3 pp versus the best few-shot CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although accuracy increased greatly, Zero-shot-CoT still underperforms Few-shot-CoT; reported failure modes include (a) producing unnecessary extra reasoning steps after initially correct intermediate work that lead to an incorrect final answer, (b) occasionally failing to start a reasoning chain and merely rephrasing the question, and (c) sensitivity to prompt wording (some templates yield much lower accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Prompt-template ablation (Table 4) shows best template "Let's think step by step." (78.7%) vs many others (range ~9.3% to 77.3%); model-scaling analysis shows chain-of-thought gains rise with model size (small models show little benefit), indicating reliance on large-scale models for elicited multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3537.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (text-davinci-002) / GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on GSM8K arithmetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same two-stage Zero-shot-CoT pipeline applied to GSM8K grade-school math word problems; elicits step-by-step reasoning with the single trigger prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant (decoder-only Transformer); exact parameter count unspecified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Grade-school math word problems requiring multi-step arithmetic and reasoning; dataset of diverse multi-step numeric problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT two-stage prompting with trigger "Let's think step by step." (generate chain-of-thought then extract answer).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Zero-shot-CoT accuracy 40.7% (text-davinci-002).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard zero-shot prompting accuracy 10.4%; Few-shot (8-shot) accuracy 15.6%; Few-shot-CoT (8-shot) accuracy 48.7%; finetuned GPT-3 (reported by Wei et al.) 33% and finetuned + verifier 55% (for GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improved by +30.3 percentage points over standard zero-shot (10.4% -> 40.7%); outperforms standard few-shot and finetuned GPT-3 baseline in some reported comparisons but remains below Few-shot-CoT (48.7%) and far below PaLM + self-consistency best results.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Still substantially below the best few-shot CoT and PaLM+self-consistency results; quality of final numeric answers can be harmed by extraneous reasoning steps; method requires two-stage prompting and answer-extraction heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Scaling and model comparisons show magnitude of gains depends on model size; PaLM exhibits larger absolute gains (see separate PaLM entries).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3537.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (PaLM 540B) / MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to PaLM (540B) on MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the Zero-shot-CoT prompting approach to Google's PaLM 540B model; shows large improvements on MultiArith and strong further gains with sampling-based self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pathways Language Model (PaLM), a large decoder-only Transformer pretrained at Google; 540 billion parameter variant evaluated alongside smaller PaLM variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step arithmetic word problems requiring multi-step symbolic and numeric calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT (single-chain-of-thought trigger) and also Zero-shot-CoT combined with self-consistency (sampling multiple chain-of-thought samples and majority/consensus over answers).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PaLM 540B zero-shot 25.5% -> Zero-shot-CoT 66.1% -> Zero-shot-CoT + self-consistency 89.0% (reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot 25.5%; Few-shot-CoT results for MultiArith not directly reported for PaLM in this table, but trend shows Few-shot-CoT helps as well on large models.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Zero-shot-CoT improved by +40.6 pp over PaLM zero-shot (25.5% -> 66.1%); self-consistency further improved to 89.0% (additional +22.9 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires very large model scale to obtain these magnitudes of improvement; self-consistency requires sampling and extra compute. Authors report smaller models do not gain as much from CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Model-scale study (Figure 3) and comparisons across PaLM sizes show large models benefit substantially from CoT; combining Zero-shot-CoT with self-consistency (sampling diverse reasoning paths and voting) yields very large additional gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3537.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (PaLM 540B) / GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to PaLM (540B) on GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot-CoT and self-consistency applied to PaLM 540B on the GSM8K benchmark, showing strong performance gains relative to zero-shot and competitive performance versus few-shot baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pathways Language Model (PaLM), decoder-only Transformer; 540B parameter variant used for experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>540B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Grade-school math word problems requiring multi-step numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT with the "Let's think step by step" trigger, and self-consistency (sample multiple reasoning chains and aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>PaLM 540B zero-shot 12.5% -> Zero-shot-CoT 43.0% -> Zero-shot-CoT + self-consistency 70.1% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot 12.5%; PaLM Few-shot-CoT reported 56.9%, Few-shot-CoT + self-consistency 74.4% (from prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Zero-shot-CoT improved by +30.5 pp over zero-shot (12.5% -> 43.0%); combining with self-consistency further narrows the gap to few-shot-CoT results (70.1% vs PaLM Few-shot-CoT+self-consistency 74.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance still sensitive to sampling/aggregation choices and to model scale; lower absolute zero-shot baseline means gains depend on strong underlying capabilities of the large model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors show self-consistency sampling dramatically improves accuracy for PaLM 540B, indicating that sampling diverse reasoning chains and aggregating answers reduces sensitivity to individual chain errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3537.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (text-davinci-002) / Symbolic reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on symbolic tasks (Last Letter Concatenation, Coin Flip)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot-CoT applied to symbolic reasoning tasks where models must perform deterministic symbol manipulations (e.g., concatenate last letters, track coin flips), showing dramatic improvements from near-random to high accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant used with greedy decoding; parameter count not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Last Letter Concatenation; Coin Flip</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Symbolic reasoning tasks: (1) Last Letter Concatenation — concatenate the last letters of given words; (2) Coin Flip — track coin state through a sequence of flips; both require exact symbolic step-by-step state tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT ("Let's think step by step") two-stage prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Last Letter: zero-shot 0.2% -> Zero-shot-CoT 57.6%; Coin Flip (4 flips): zero-shot 12.8% -> Zero-shot-CoT 91.4% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot very low (Last Letter 0.2%, Coin Flip 12.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Dramatic improvements: Last Letter +57.4 pp (0.2% -> 57.6%), Coin Flip +78.6 pp (12.8% -> 91.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although large gains occurred, symbolic tasks still exhibited a flat scaling curve without CoT, indicating that eliciting explicit step-by-step reasoning is crucial; some errors remain (e.g., occasional reasoning lapses) and performance depends on prompt phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors note these tasks are easy for humans but elicit flat performance without CoT; Zero-shot-CoT recovers multi-step symbolic computation ability without task-specific exemplars, suggesting the prompt unlocks latent symbolic reasoning mechanisms in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3537.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (text-davinci-002) / Date & Tracking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting on logical tasks from BIG-bench (Date Understanding, Tracking Shuffled Objects) using InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of Zero-shot-CoT to non-arithmetic logical tasks involving date reasoning and object-tracking to test multi-step deterministic logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant; model size not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Date Understanding; Tracking Shuffled Objects</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Date Understanding: infer a date from context (logical manipulation of dates). Tracking Shuffled Objects: track final state/position of objects after a sequence of shuffles (requires precise sequential-state reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT (single consistent trigger across tasks) with two-stage prompting and tailored answer-extraction prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Date Understanding: zero-shot 49.3% -> Zero-shot-CoT 67.5%; Tracking Shuffled Objects (three objects): zero-shot 31.3% -> Zero-shot-CoT 52.4% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot 49.3% (Date) and 31.3% (Tracking).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Improvements of +18.2 pp (Date) and +21.1 pp (Tracking) versus standard zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although improved, these tasks still show remaining errors; models sometimes output multiple candidate answers or produce understandable but ultimately incorrect reasoning traces. Sensitivity to prompt template and answer-extraction format was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Authors evaluated answer-extraction prompt variants (specialized vs generic triggers) and found specialized answer prompts improve parsing; robustness to different templates was assessed (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3537.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT (text-davinci-002) / Commonsense reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting applied to CommonsenseQA and StrategyQA using InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluates Zero-shot-CoT on commonsense multi-choice and implicit multi-hop reasoning tasks; elicited chains of thought are often reasonable but do not always increase end-task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructGPT (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3 variant; parameter count unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>CommonsenseQA; StrategyQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>CommonsenseQA: multiple-choice questions requiring commonsense knowledge; StrategyQA: yes/no questions that require multi-hop implicit reasoning (strategy inference).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Zero-shot-CoT ("Let's think step by step") two-stage prompting with multiple-choice answer extraction templates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CommonsenseQA: zero-shot 68.8% -> Zero-shot-CoT 64.6% (decrease); StrategyQA: zero-shot 12.7% -> Zero-shot-CoT 54.8% (large increase) (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline zero-shot 68.8% (CommonsenseQA) and 12.7% (StrategyQA); Few-shot-CoT did not consistently improve CommonsenseQA in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>No improvement (slight degradation) on CommonsenseQA; substantial improvement on StrategyQA (+42.1 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>On CommonsenseQA, Zero-shot-CoT often produced plausible chain-of-thoughts but frequently produced multiple candidate answers or could not narrow to a single correct choice; the task metric did not reflect improvements in reasoning quality. This indicates CoT does not uniformly improve all types of reasoning (esp. knowledge-heavy multiple-choice).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Manual error analysis showed CoT chains are often logically coherent even when final answer is wrong; model sometimes lists multiple choices when uncertain (Table 3). Authors note that larger models may be required to observe improvements for certain commonsense tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3537.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3537.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablations: Prompt Templates & Scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation studies in the paper: robustness to prompt template phrasing and sensitivity to model scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic analyses showing (1) choice of trigger template greatly affects Zero-shot-CoT accuracy (instructive phrasing works best), and (2) chain-of-thought benefits scale with model size—small models show little improvement while large models benefit strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 and other models (GPT-3 family, PaLM variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various decoder-only Transformer LMs evaluated by the paper, including InstructGPT family (text-ada/babbage/curie/davinci-001 and text-davinci-002), original GPT-3 variants (ada, babbage, curie, davinci), PaLM (8B, 62B, 540B), and others (GPT-2, GPT-Neo, GPT-J, T0, OPT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MultiArith (primary ablation) and GSM8K (scaling studies)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Arithmetic multi-step reasoning benchmarks used as primary probes for template and scale ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Prompt-template ablation comparing 16 templates (instructive, misleading, irrelevant); model scaling experiments across many model sizes and families; Few-shot-CoT exemplar-domain mismatch ablation (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Template results on MultiArith (text-davinci-002): e.g., "Let's think step by step." 78.7%, "First," 77.3%, "Let's think about this logically." 74.5%, down to misleading/irrelevant templates that give poor performance (e.g., "Don't think. Just feel." 18.8%). Scaling study: without CoT performance mostly flat as size increases; with CoT performance increases sharply for larger models (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Standard zero-shot baseline on MultiArith 17.7% (no CoT trigger).</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Best prompt templates yield large gains vs baseline (up to ~61 pp on MultiArith); appropriate template selection is critical. Model scale determines whether CoT has effect — small models show little to no improvement with CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Prompt design remains brittle: some reasonable-seeming templates perform much worse; Few-shot-CoT is sensitive to exemplar-task mismatch (examples from different domains degrade performance), indicating prompting is brittle with respect to exemplar content and prompt wording. The paper leaves open how to automatically generate robust templates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Detailed template ablation (Table 4) and exemplar-domain ablation (Table 5) demonstrate (1) instructive wording matters, (2) exemplar/domain mismatch degrades Few-shot-CoT, and (3) CoT effectiveness increases with model scale; authors also discuss two-stage prompting and answer-extraction parsing as important implementation details.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models are Zero-Shot Reasoners', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling language modeling with pathways <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3537",
    "paper_id": "paper-e7ad08848d5d7c5c47673ffe0da06af443643bda",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Zero-shot-CoT (text-davinci-002) / MultiArith",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on the MultiArith arithmetic benchmark",
            "brief_description": "Applies a single zero-shot trigger prompt (e.g., \"Let's think step by step\") in a two-stage prompting pipeline to elicit step-by-step reasoning from InstructGPT (text-davinci-002) and then extract the final numeric answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned variant of GPT-3 (decoder-only Transformer) trained with reinforcement learning from human feedback to follow instructions; exact parameter count not specified in the paper.",
            "model_size": null,
            "reasoning_task_name": "MultiArith",
            "reasoning_task_description": "Multi-step arithmetic word problems requiring multi-step symbolic and numeric calculation and reasoning (from MAWPS repository).",
            "method_or_intervention": "Zero-shot-CoT: prepend a short trigger (e.g., \"Let's think step by step.\") in a two-stage prompting pipeline (1. generate chain-of-thought reasoning; 2. answer extraction prompt to parse final answer).",
            "performance": "Zero-shot-CoT accuracy 78.7% (text-davinci-002, greedy decoding).",
            "baseline_performance": "Standard zero-shot prompting accuracy 17.7%; Few-shot 8-shot prompting accuracy ~33.8%; Few-shot-CoT (8 exemplars) accuracy 93.0% (text-davinci-002).",
            "improvement_over_baseline": "Improved by +61.0 percentage points over standard zero-shot (17.7% -&gt; 78.7%); substantially better than standard few-shot (~33.8%), but lower than Few-shot-CoT (93.0%), i.e., a remaining gap of ~14.3 pp versus the best few-shot CoT.",
            "limitations_or_failures": "Although accuracy increased greatly, Zero-shot-CoT still underperforms Few-shot-CoT; reported failure modes include (a) producing unnecessary extra reasoning steps after initially correct intermediate work that lead to an incorrect final answer, (b) occasionally failing to start a reasoning chain and merely rephrasing the question, and (c) sensitivity to prompt wording (some templates yield much lower accuracy).",
            "ablation_or_analysis": "Prompt-template ablation (Table 4) shows best template \"Let's think step by step.\" (78.7%) vs many others (range ~9.3% to 77.3%); model-scaling analysis shows chain-of-thought gains rise with model size (small models show little benefit), indicating reliance on large-scale models for elicited multi-step reasoning.",
            "uuid": "e3537.0",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (text-davinci-002) / GSM8K",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on GSM8K arithmetic benchmark",
            "brief_description": "Same two-stage Zero-shot-CoT pipeline applied to GSM8K grade-school math word problems; elicits step-by-step reasoning with the single trigger prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned GPT-3 variant (decoder-only Transformer); exact parameter count unspecified in the paper.",
            "model_size": null,
            "reasoning_task_name": "GSM8K",
            "reasoning_task_description": "Grade-school math word problems requiring multi-step arithmetic and reasoning; dataset of diverse multi-step numeric problems.",
            "method_or_intervention": "Zero-shot-CoT two-stage prompting with trigger \"Let's think step by step.\" (generate chain-of-thought then extract answer).",
            "performance": "Zero-shot-CoT accuracy 40.7% (text-davinci-002).",
            "baseline_performance": "Standard zero-shot prompting accuracy 10.4%; Few-shot (8-shot) accuracy 15.6%; Few-shot-CoT (8-shot) accuracy 48.7%; finetuned GPT-3 (reported by Wei et al.) 33% and finetuned + verifier 55% (for GSM8K).",
            "improvement_over_baseline": "Improved by +30.3 percentage points over standard zero-shot (10.4% -&gt; 40.7%); outperforms standard few-shot and finetuned GPT-3 baseline in some reported comparisons but remains below Few-shot-CoT (48.7%) and far below PaLM + self-consistency best results.",
            "limitations_or_failures": "Still substantially below the best few-shot CoT and PaLM+self-consistency results; quality of final numeric answers can be harmed by extraneous reasoning steps; method requires two-stage prompting and answer-extraction heuristics.",
            "ablation_or_analysis": "Scaling and model comparisons show magnitude of gains depends on model size; PaLM exhibits larger absolute gains (see separate PaLM entries).",
            "uuid": "e3537.1",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (PaLM 540B) / MultiArith",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to PaLM (540B) on MultiArith",
            "brief_description": "Application of the Zero-shot-CoT prompting approach to Google's PaLM 540B model; shows large improvements on MultiArith and strong further gains with sampling-based self-consistency.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_description": "Pathways Language Model (PaLM), a large decoder-only Transformer pretrained at Google; 540 billion parameter variant evaluated alongside smaller PaLM variants.",
            "model_size": "540B",
            "reasoning_task_name": "MultiArith",
            "reasoning_task_description": "Multi-step arithmetic word problems requiring multi-step symbolic and numeric calculation.",
            "method_or_intervention": "Zero-shot-CoT (single-chain-of-thought trigger) and also Zero-shot-CoT combined with self-consistency (sampling multiple chain-of-thought samples and majority/consensus over answers).",
            "performance": "PaLM 540B zero-shot 25.5% -&gt; Zero-shot-CoT 66.1% -&gt; Zero-shot-CoT + self-consistency 89.0% (reported in Table 2).",
            "baseline_performance": "Baseline zero-shot 25.5%; Few-shot-CoT results for MultiArith not directly reported for PaLM in this table, but trend shows Few-shot-CoT helps as well on large models.",
            "improvement_over_baseline": "Zero-shot-CoT improved by +40.6 pp over PaLM zero-shot (25.5% -&gt; 66.1%); self-consistency further improved to 89.0% (additional +22.9 pp).",
            "limitations_or_failures": "Requires very large model scale to obtain these magnitudes of improvement; self-consistency requires sampling and extra compute. Authors report smaller models do not gain as much from CoT prompting.",
            "ablation_or_analysis": "Model-scale study (Figure 3) and comparisons across PaLM sizes show large models benefit substantially from CoT; combining Zero-shot-CoT with self-consistency (sampling diverse reasoning paths and voting) yields very large additional gains.",
            "uuid": "e3537.2",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (PaLM 540B) / GSM8K",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to PaLM (540B) on GSM8K",
            "brief_description": "Zero-shot-CoT and self-consistency applied to PaLM 540B on the GSM8K benchmark, showing strong performance gains relative to zero-shot and competitive performance versus few-shot baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_description": "Pathways Language Model (PaLM), decoder-only Transformer; 540B parameter variant used for experiments in this paper.",
            "model_size": "540B",
            "reasoning_task_name": "GSM8K",
            "reasoning_task_description": "Grade-school math word problems requiring multi-step numerical reasoning.",
            "method_or_intervention": "Zero-shot-CoT with the \"Let's think step by step\" trigger, and self-consistency (sample multiple reasoning chains and aggregate).",
            "performance": "PaLM 540B zero-shot 12.5% -&gt; Zero-shot-CoT 43.0% -&gt; Zero-shot-CoT + self-consistency 70.1% (Table 2).",
            "baseline_performance": "Baseline zero-shot 12.5%; PaLM Few-shot-CoT reported 56.9%, Few-shot-CoT + self-consistency 74.4% (from prior work).",
            "improvement_over_baseline": "Zero-shot-CoT improved by +30.5 pp over zero-shot (12.5% -&gt; 43.0%); combining with self-consistency further narrows the gap to few-shot-CoT results (70.1% vs PaLM Few-shot-CoT+self-consistency 74.4%).",
            "limitations_or_failures": "Performance still sensitive to sampling/aggregation choices and to model scale; lower absolute zero-shot baseline means gains depend on strong underlying capabilities of the large model.",
            "ablation_or_analysis": "Authors show self-consistency sampling dramatically improves accuracy for PaLM 540B, indicating that sampling diverse reasoning chains and aggregating answers reduces sensitivity to individual chain errors.",
            "uuid": "e3537.3",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (text-davinci-002) / Symbolic reasoning",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to InstructGPT (text-davinci-002) on symbolic tasks (Last Letter Concatenation, Coin Flip)",
            "brief_description": "Zero-shot-CoT applied to symbolic reasoning tasks where models must perform deterministic symbol manipulations (e.g., concatenate last letters, track coin flips), showing dramatic improvements from near-random to high accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned GPT-3 variant used with greedy decoding; parameter count not provided in paper.",
            "model_size": null,
            "reasoning_task_name": "Last Letter Concatenation; Coin Flip",
            "reasoning_task_description": "Symbolic reasoning tasks: (1) Last Letter Concatenation — concatenate the last letters of given words; (2) Coin Flip — track coin state through a sequence of flips; both require exact symbolic step-by-step state tracking.",
            "method_or_intervention": "Zero-shot-CoT (\"Let's think step by step\") two-stage prompting.",
            "performance": "Last Letter: zero-shot 0.2% -&gt; Zero-shot-CoT 57.6%; Coin Flip (4 flips): zero-shot 12.8% -&gt; Zero-shot-CoT 91.4% (Table 1).",
            "baseline_performance": "Baseline zero-shot very low (Last Letter 0.2%, Coin Flip 12.8%).",
            "improvement_over_baseline": "Dramatic improvements: Last Letter +57.4 pp (0.2% -&gt; 57.6%), Coin Flip +78.6 pp (12.8% -&gt; 91.4%).",
            "limitations_or_failures": "Although large gains occurred, symbolic tasks still exhibited a flat scaling curve without CoT, indicating that eliciting explicit step-by-step reasoning is crucial; some errors remain (e.g., occasional reasoning lapses) and performance depends on prompt phrasing.",
            "ablation_or_analysis": "Authors note these tasks are easy for humans but elicit flat performance without CoT; Zero-shot-CoT recovers multi-step symbolic computation ability without task-specific exemplars, suggesting the prompt unlocks latent symbolic reasoning mechanisms in LLMs.",
            "uuid": "e3537.4",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (text-davinci-002) / Date & Tracking",
            "name_full": "Zero-shot Chain-of-Thought prompting on logical tasks from BIG-bench (Date Understanding, Tracking Shuffled Objects) using InstructGPT (text-davinci-002)",
            "brief_description": "Application of Zero-shot-CoT to non-arithmetic logical tasks involving date reasoning and object-tracking to test multi-step deterministic logical inference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned GPT-3 variant; model size not specified in paper.",
            "model_size": null,
            "reasoning_task_name": "Date Understanding; Tracking Shuffled Objects",
            "reasoning_task_description": "Date Understanding: infer a date from context (logical manipulation of dates). Tracking Shuffled Objects: track final state/position of objects after a sequence of shuffles (requires precise sequential-state reasoning).",
            "method_or_intervention": "Zero-shot-CoT (single consistent trigger across tasks) with two-stage prompting and tailored answer-extraction prompts.",
            "performance": "Date Understanding: zero-shot 49.3% -&gt; Zero-shot-CoT 67.5%; Tracking Shuffled Objects (three objects): zero-shot 31.3% -&gt; Zero-shot-CoT 52.4% (Table 1).",
            "baseline_performance": "Baseline zero-shot 49.3% (Date) and 31.3% (Tracking).",
            "improvement_over_baseline": "Improvements of +18.2 pp (Date) and +21.1 pp (Tracking) versus standard zero-shot prompting.",
            "limitations_or_failures": "Although improved, these tasks still show remaining errors; models sometimes output multiple candidate answers or produce understandable but ultimately incorrect reasoning traces. Sensitivity to prompt template and answer-extraction format was reported.",
            "ablation_or_analysis": "Authors evaluated answer-extraction prompt variants (specialized vs generic triggers) and found specialized answer prompts improve parsing; robustness to different templates was assessed (Table 4).",
            "uuid": "e3537.5",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Zero-shot-CoT (text-davinci-002) / Commonsense reasoning",
            "name_full": "Zero-shot Chain-of-Thought prompting applied to CommonsenseQA and StrategyQA using InstructGPT (text-davinci-002)",
            "brief_description": "Evaluates Zero-shot-CoT on commonsense multi-choice and implicit multi-hop reasoning tasks; elicited chains of thought are often reasonable but do not always increase end-task accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InstructGPT (text-davinci-002)",
            "model_description": "Instruction-tuned GPT-3 variant; parameter count unspecified.",
            "model_size": null,
            "reasoning_task_name": "CommonsenseQA; StrategyQA",
            "reasoning_task_description": "CommonsenseQA: multiple-choice questions requiring commonsense knowledge; StrategyQA: yes/no questions that require multi-hop implicit reasoning (strategy inference).",
            "method_or_intervention": "Zero-shot-CoT (\"Let's think step by step\") two-stage prompting with multiple-choice answer extraction templates.",
            "performance": "CommonsenseQA: zero-shot 68.8% -&gt; Zero-shot-CoT 64.6% (decrease); StrategyQA: zero-shot 12.7% -&gt; Zero-shot-CoT 54.8% (large increase) (Table 1).",
            "baseline_performance": "Baseline zero-shot 68.8% (CommonsenseQA) and 12.7% (StrategyQA); Few-shot-CoT did not consistently improve CommonsenseQA in prior work.",
            "improvement_over_baseline": "No improvement (slight degradation) on CommonsenseQA; substantial improvement on StrategyQA (+42.1 pp).",
            "limitations_or_failures": "On CommonsenseQA, Zero-shot-CoT often produced plausible chain-of-thoughts but frequently produced multiple candidate answers or could not narrow to a single correct choice; the task metric did not reflect improvements in reasoning quality. This indicates CoT does not uniformly improve all types of reasoning (esp. knowledge-heavy multiple-choice).",
            "ablation_or_analysis": "Manual error analysis showed CoT chains are often logically coherent even when final answer is wrong; model sometimes lists multiple choices when uncertain (Table 3). Authors note that larger models may be required to observe improvements for certain commonsense tasks.",
            "uuid": "e3537.6",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Ablations: Prompt Templates & Scaling",
            "name_full": "Ablation studies in the paper: robustness to prompt template phrasing and sensitivity to model scale",
            "brief_description": "Systematic analyses showing (1) choice of trigger template greatly affects Zero-shot-CoT accuracy (instructive phrasing works best), and (2) chain-of-thought benefits scale with model size—small models show little improvement while large models benefit strongly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 and other models (GPT-3 family, PaLM variants)",
            "model_description": "Various decoder-only Transformer LMs evaluated by the paper, including InstructGPT family (text-ada/babbage/curie/davinci-001 and text-davinci-002), original GPT-3 variants (ada, babbage, curie, davinci), PaLM (8B, 62B, 540B), and others (GPT-2, GPT-Neo, GPT-J, T0, OPT).",
            "model_size": null,
            "reasoning_task_name": "MultiArith (primary ablation) and GSM8K (scaling studies)",
            "reasoning_task_description": "Arithmetic multi-step reasoning benchmarks used as primary probes for template and scale ablations.",
            "method_or_intervention": "Prompt-template ablation comparing 16 templates (instructive, misleading, irrelevant); model scaling experiments across many model sizes and families; Few-shot-CoT exemplar-domain mismatch ablation (Table 5).",
            "performance": "Template results on MultiArith (text-davinci-002): e.g., \"Let's think step by step.\" 78.7%, \"First,\" 77.3%, \"Let's think about this logically.\" 74.5%, down to misleading/irrelevant templates that give poor performance (e.g., \"Don't think. Just feel.\" 18.8%). Scaling study: without CoT performance mostly flat as size increases; with CoT performance increases sharply for larger models (Figure 3).",
            "baseline_performance": "Standard zero-shot baseline on MultiArith 17.7% (no CoT trigger).",
            "improvement_over_baseline": "Best prompt templates yield large gains vs baseline (up to ~61 pp on MultiArith); appropriate template selection is critical. Model scale determines whether CoT has effect — small models show little to no improvement with CoT.",
            "limitations_or_failures": "Prompt design remains brittle: some reasonable-seeming templates perform much worse; Few-shot-CoT is sensitive to exemplar-task mismatch (examples from different domains degrade performance), indicating prompting is brittle with respect to exemplar content and prompt wording. The paper leaves open how to automatically generate robust templates.",
            "ablation_or_analysis": "Detailed template ablation (Table 4) and exemplar-domain ablation (Table 5) demonstrate (1) instructive wording matters, (2) exemplar/domain mismatch degrades Few-shot-CoT, and (3) CoT effectiveness increases with model scale; authors also discuss two-stage prompting and answer-extraction parsing as important implementation details.",
            "uuid": "e3537.7",
            "source_info": {
                "paper_title": "Large Language Models are Zero-Shot Reasoners",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "PaLM: Scaling language modeling with pathways",
            "rating": 2
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 1
        }
    ],
    "cost": 0.022533749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models are Zero-Shot Reasoners</h1>
<p>Takeshi Kojima<br>The University of Tokyo<br>t.kojima@weblab.t.u-tokyo.ac.jp<br>Machel Reid<br>Yutaka Matsuo<br>The University of Tokyo<br>Shixiang Shane Gu<br>Google Research, Brain Team<br>Yusuke Iwasawa<br>The University of Tokyo</p>
<h2>Abstract</h2>
<p>Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from $17.7 \%$ to $78.7 \%$ and GSM8K from $10.4 \%$ to $40.7 \%$ with large-scale InstructGPT model (text-davinci002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.</p>
<h2>1 Introduction</h2>
<p>Scaling up the size of language models has been key ingredients of recent revolutions in natural language processing (NLP) [Vaswani et al., 2017, Devlin et al., 2019, Raffel et al., 2020, Brown et al., 2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022]. The success of large language models (LLMs) is often attributed to (in-context) few-shot or zero-shot learning. It can solve various tasks by simply conditioning the models on a few examples (few-shot) or instructions describing the task (zero-shot). The method of conditioning the language model is called "prompting" [Liu et al., 2021b], and designing prompts either manually [Schick and Schütze, 2021, Reynolds and McDonell, 2021] or automatically [Gao et al., 2021, Shin et al., 2020] has become a hot topic in NLP.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Example inputs and outputs of GPT-3 with (a) standard Few-shot ([Brown et al., 2020]), (b) Few-shot-CoT ([Wei et al., 2022]), (c) standard Zero-shot, and (d) ours (Zero-shot-CoT). Similar to Few-shot-CoT, Zero-shot-CoT facilitates multi-step reasoning (blue text) and reach correct answer where standard prompting fails. Unlike Few-shot-CoT using step-by-step reasoning examples per task, ours does not need any examples and just uses the same prompt "Let’s think step by step" across all tasks (arithmetic, symbolic, commonsense, and other logical reasoning tasks).</p>
<p>In contrast to the excellent performance of LLMs in intuitive and single-step system-1 [Stanovich and West, 2000] tasks with task-specific few-shot or zero-shot prompting [Liu et al., 2021b], even language models at the scale of 100B or more parameters had struggled on system-2 tasks requiring slow and multi-step reasoning [Rae et al., 2021]. To address this shortcoming, Wei et al. [2022], Wang et al. [2022] have proposed chain of thought prompting (CoT), which feed LLMs with the step-by-step reasoning examples rather than standard question and answer examples (see Fig. 1-a). Such chain of thought demonstrations facilitate models to generate a reasoning path that decomposes the complex reasoning into multiple easier steps. Notably with CoT, the reasoning performance then satisfies the scaling laws better and jumps up with the size of the language models. For example, when combined with the 540B parameter PaLM model [Chowdhery et al., 2022], chain of thought prompting significantly increases the performance over standard few-shot prompting across several benchmark reasoning tasks, e.g., GSM8K (17.9% $\rightarrow$ 58.1%).</p>
<p>While the successes of CoT prompting [Wei et al., 2022], along those of many other task-specific prompting work [Gao et al., 2021, Schick and Schütze, 2021, Liu et al., 2021b], are often attributed to LLMs’ ability for few-shot learning [Brown et al., 2020], we show that LLMs are decent zero-shot reasoners by adding a simple prompt, Let’s think step by step, to facilitate step-by-step thinking before answering each question (see Figure 1). Despite the simplicity, our Zero-shot-CoT successfully generates a plausible reasoning path in a zero-shot manner and reaches the correct answer in a problem where the standard zero-shot approach fails. Importantly, our Zero-shot-CoT is versatile and task-agnostic, unlike most prior task-specific prompt engineering in the forms of examples (few-shot) or templates (zero-shot) [Liu et al., 2021b]: it can facilitate step-by-step answers across various reasoning tasks, including arithmetic (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021], AQUA-RAT [Ling et al., 2017], and SVAMP [Patel et al., 2021]), symbolic reasoning (Last letter and Coin flip), commonsense reasoning (CommonSenseQA [Talmor et al., 2019] and Strategy QA [Geva et al., 2021]), and other logical reasoning tasks (Date understanding and Tracking Shuffled Objects from BIG-bench [Srivastava et al., 2022]) without modifying the prompt per task.</p>
<p>We empirically evaluate Zero-shot-CoT against other prompting baselines in Table 2. While our Zero-shot-CoT underperforms Few-shot-CoT with carefully-crafted and task-specific step-by-step examples, Zero-shot-CoT achieves enormous score gains compared to the zero-shot baseline, e.g. from 17.7% to 78.7% on MultiArith and from 10.4% to 40.7% on GSM8K with large-scale InstructGPT</p>
<p>model (text-davinci-002). We also evaluate Zero-shot-CoT with another off-the-shelf large model, 540B parameter PaLM, showing similar magnitudes of improvements on MultiArith and GSM8K. Importantly, with our single fixed prompt, zero-shot LLMs have a significantly better scaling curve comparable to that of the few-shot CoT baseline. We also show that besides Few-shot-CoT requiring human engineering of multi-step reasoning prompts, their performance deteriorates if prompt example question types and task question type are unmatched, suggesting high sensitivity to per-task prompt designs. In contrast, the versatility of this single prompt across diverse reasoning tasks hints at untapped and understudied zero-shot fundamental capabilities of LLMs, such as higher-level broad cognitive capabilities like generic logical reasoning [Chollet, 2019]. While the vibrant field of LLMs started out from the premise of excellent few-shot learners [Brown et al., 2020], we hope our work encourages more research into uncovering high-level and multi-task zero-shot capabilities hidden inside those models.</p>
<h1>2 Background</h1>
<p>We briefly review the two core preliminary concepts that form the basis of this work: the advent of large language models (LLMs) and prompting, and chain of thought (CoT) prompting for multi-step reasoning.</p>
<p>Large language models and prompting A language model (LM), is a model that looks to estimate the probability distribution over text. Recently, scaling improvements through larger model sizes (from a few million [Merity et al., 2016] to hundreds of millions [Devlin et al., 2019] to hundreds of billions [Brown et al., 2020] parameters) and larger data (e.g. webtext corpora [Gao et al., 2020]) have enabled pre-trained large language models (LLMs) to be incredibly adept at many downstream NLP tasks. Besides the classic "pre-train and fine-tune" paradigm [Liu et al., 2021b], models scaled to 100B+ parameters exhibit properties conducive to few-shot learning [Brown et al., 2020], by way of in context learning, where one can use a text or template known as a prompt to strongly guide the generation to output answers for desired tasks, thus beginning an era of "pre-train and prompt" [Liu et al., 2021a]. In work, we call such prompts with explicit conditioning on few task examples as few-shot prompts, and other template-only prompts as zero-shot prompts.</p>
<p>Chain of thought prompting Multi-step arithmetic and logical reasoning benchmarks have particularly challenged the scaling laws of large language models [Rae et al., 2021]. Chain of thought (CoT) prompting [Wei et al., 2022], an instance of few-shot prompting, proposed a simple solution by modifying the answers in few-shot examples to step-by-step answers, and achieved significant boosts in performance across these difficult benchmarks, especially when combined with very large language models like PaLM [Chowdhery et al., 2022]. The top row of Figure 1 shows standard few-shot prompting against (few-shot) CoT prompting. Notably, few-shot learning was taken as a given for tackling such difficult tasks, and the zero-shot baseline performances were not even reported in the original work [Wei et al., 2022]. To differentiate it from our method, we call Wei et al. [2022] as Few-shot-CoT in this work.</p>
<h2>3 Zero-shot Chain of Thought</h2>
<p>We propose Zero-shot-CoT, a zero-shot template-based prompting for chain of thought reasoning. It differs from the original chain of thought prompting [Wei et al., 2022] as it does not require step-by-step few-shot examples, and it differs from most of the prior template prompting [Liu et al., 2021b] as it is inherently task-agnostic and elicits multi-hop reasoning across a wide range of tasks with a single template. The core idea of our method is simple, as described in Figure 1: add Let's think step by step, or a a similar text (see Table 4), to extract step-by-step reasoning.</p>
<h3>3.1 Two-stage prompting</h3>
<p>While Zero-shot-CoT is conceptually simple, it uses prompting twice to extract both reasoning and answer, as explained in Figure 2. In contrast, the zero-shot baseline (see the bottom-left in Figure 1) already uses prompting in the form of "The answer is", to extract the answers in correct formats. Few-shot prompting, standard or CoT, avoids needing such answer-extraction prompting by explicitly designing the few-shot example answers to end in such formats (see the top-right and top-left</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Full pipeline of Zero-shot-CoT as described in $\S 3$ : we first use the first "reasoning" prompt to extract a full reasoning path from a language model, and then use the second "answer" prompt to extract the answer in the correct format from the reasoning text.
in Figure 1). In summary, Few-shot-CoT [Wei et al., 2022] requires careful human engineering of a few prompt examples with specific answer formats per task, while Zero-shot-CoT requires less engineering but requires prompting LLMs twice.</p>
<p>1st prompt: reasoning extraction In this step we first modify the input question $\mathbf{x}$ into a prompt $\mathbf{x}^{\prime}$ using a simple template "Q: [X]. A: [T]", where [X] is an input slot for $\mathbf{x}$ and [T] is an slot for hand-crafted trigger sentence $\mathbf{t}$ that would extract chain of though to answer the question $\mathbf{x}$. For example, if we use "Let's think step by step" as a trigger sentence, the prompt $\mathbf{x}^{\prime}$ would be "Q: [X]. A: Let's think step by step.". See Table 4 for more trigger examples. Prompted text $\mathbf{x}^{\prime}$ is then fed into a language model and generate subsequent sentence $\mathbf{z}$. We can use any decoding strategy, but we used greedy decoding throughout the paper for the simplicity.</p>
<p>2nd prompt: answer extraction In the second step, we use generated sentence $\mathbf{z}$ along with prompted sentence $\mathbf{x}^{\prime}$ to extract the final answer from the language model. To be concrete, we simply concatenate three elements as with " $\left[\mathrm{X}^{\prime}\right][\mathrm{Z}][\mathrm{A}]$ ": $\left[\mathrm{X}^{\prime}\right]$ for 1st prompt $\mathrm{x}^{\prime}$, [Z] for sentence z generated at the first step, and [A] for a trigger sentence to extract answer. The prompt for this step is self-augmented, since the prompt contains the sentence z generated by the same language model. In experiment, we use slightly different answer trigger depending on the answer format. For example, we use "Therefore, among A through E, the answer is" for multi-choice QA, and "Therefore, the answer (arabic numerals) is" for math problem requiring numerical answer. See Appendix A. 5 for the lists of answer trigger sentences. Finally, the language model is fed the prompted text as input to generate sentences $\hat{\mathbf{y}}$ and parse the final answer. See "Answer Cleansing" at $\S 4$ for the parser details.</p>
<h1>4 Experiment</h1>
<p>Tasks and datasets We evaluate our proposal on 12 datasets from four categories of reasoning tasks: arithmetic, commonsense, symbolic, and other logical reasoning tasks. See Appendix A. 2 for the detailed description of each datasets.</p>
<p>For arithmetic reasoning, we consider the following six datasets: (1) SingleEq [Koncel-Kedziorski et al., 2015], (2) AddSub [Hosseini et al., 2014], (3) MultiArith [Roy and Roth, 2015], (4) AQUARAT [Ling et al., 2017], (5) GSM8K [Cobbe et al., 2021], and (6) SVAMP [Patel et al., 2021]. The first three are from the classic Math World Problem Repository [Koncel-Kedziorski et al., 2016], and the last three are from more recent benchmarks. SingleEq and AddSub contain easier problems, which do not require multi-step calculation to solve the tasks. MultiArith, AQUA-RAT, GSM8k, and SVAMP are more challenging datasets that require multi-step reasoning to solve.</p>
<p>For commonsense reasoning, we use CommonsenseQA [Talmor et al., 2019] and StrategyQA [Geva et al., 2021]. CommonsenseQA asks questions with complex semantics that often require reasoning</p>
<p>based on prior knowledge [Talmor et al., 2019]. StrategyQA requires models to infer an implicit multi-hop reasoning to answer questions [Geva et al., 2021].
For symbolic reasoning, we use Last Letter Concatenation and Coin Flip [Wei et al., 2022]. Last letter Concatenation asks the model to concatenate the last letters of each word. We used randomly selected four names for each sample. Coin Flip asks the model to answer whether a coin is still heads up after people either flip or do not flip the coin. We created samples of four times flip or not flip trials. Although these tasks are easy for humans, LMs typically exhibit a flat scaling curve.
For other logical reasoning tasks, we choose two evaluation sets from the BIG-bench effort [Srivastava et al., 2022]: Date Understanding ${ }^{2}$ and Tracking Shuffled Objects. Date Understanding asks models to infer the date from a context. Tracking Shuffled Objects tests a model's ability to infer the final state of objects given its initial state and a sequence of object shuffling. We used a dataset of tracking three shuffled objects for our experiment.</p>
<p>Models We experiment with 17 models in total. Main experiments are conducted with InstructGPT3 [Ouyang et al., 2022] (text-ada/babbage/curie/davinci-001 and text-davinci-002) ${ }^{3}$, original GPT3 [Brown et al., 2020] (ada, babbage, curie, and davinci) ${ }^{4}$, and PaLM [Chowdhery et al., 2022] (8B, 62B, and 540B). In addition, we used GPT-2[Radford et al., 2019], GPT-Neo[Black et al., 2021], GPT-J[Wang and Komatsuzaki, 2021], T0 [Sanh et al., 2022], and OPT [Zhang et al., 2022] for model scaling study. The size of LMs ranges from 0.3B to 540B. We include both standard (e.g. GPT-3 and OPT), and instruction following variants (e.g. Instruct-GPT3 and T0). See Appendix A. 3 for model description details. Unless otherwise stated, we use text-davinci-002 throughout the experiments.</p>
<p>Baselines We compare our Zero-shot-CoT mainly to standard Zero-shot prompting to verify the effectiveness of its chain of thought reasoning. For Zero-shot experiments, similar answer prompts as Zero-shot-CoT are used as default. See Appendix A. 5 for detail. To better evaluate the zero-shot ability of LLMs on reasoning tasks, we also compare our method to Few-shot and Few-shot-CoT baselines from [Wei et al., 2022], using the same in-context examples. Throughout the experiments, we use greedy decoding across all the methods. For the zero-shot approaches, the results are therefore deterministic. For the few-shot approaches, since the order of in-context examples could affect the results [Lu et al., 2022], we run each experiment only once with a fixed seed across all methods and datasets, for fair comparisons with the zero-shot methods. Wei et al. [2022] showed that the order of examples did not cause large variance in CoT experiments.</p>
<p>Answer cleansing After the model outputs a text by answer extraction (see § 3 and Figure 2), our method picks up only the part of the answer text that first satisfies the answer format. For example, if the answer prompting outputs "probably 375 and 376 " on arithmetic tasks, we extract the first number " 375 " and set it as the model prediction. In the case of multiple-choice, the first large letter we encounter is set as the prediction. See Appendix A. 6 for more detail. Standard Zero-shot method follows the same idea. For Few-shot and Few-shot-CoT methods, we follow [Wang et al., 2022] and first extract the answer text after "The answer is " from the model output, and apply the same answer cleansing to parse the answer text. If "The answer is" is not found in the model output, we search from the back of the text and set the first text that satisfies the answer format as the prediction.</p>
<h1>4.1 Results</h1>
<p>Zero-shot-CoT vs. Zero-shot Table 1 summarize accuracy of our method (Zero-shot-CoT) and standard zero-shot prompting (Zero-shot) for each dataset. Zero-shot-CoT substantially outperforms four out of six arithmetic reasoning tasks (MultiArith, GSM8K, AQUA, SVAMP), all symbolic reasoning, and all other logical reasoning tasks (from BIG-bench [Srivastava et al., 2022]). For</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Accuracy comparison of Zero-shot-CoT with Zero-shot on each tasks. The values on the left side of each task are the results of using answer extraction prompts depending on answer format as described at $\S 3$. The values on the right side are the result of additional experiment where standard answer prompt "The answer is" is used for answer extraction. See Appendix A. 5 for detail setups.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Arithmetic</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SingleEq</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">AQUA</td>
<td style="text-align: center;">SVAMP</td>
</tr>
<tr>
<td style="text-align: left;">zero-shot</td>
<td style="text-align: center;">$74.6 / \mathbf{7 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 2 / 7 7 . 0}$</td>
<td style="text-align: center;">$17.7 / 22.7$</td>
<td style="text-align: center;">$10.4 / 12.5$</td>
<td style="text-align: center;">$22.4 / 22.4$</td>
<td style="text-align: center;">$58.8 / 58.7$</td>
</tr>
<tr>
<td style="text-align: left;">zero-shot-cot</td>
<td style="text-align: center;">$\mathbf{7 8 . 0 / 7 8 . 7}$</td>
<td style="text-align: center;">$69.6 / 74.7$</td>
<td style="text-align: center;">$\mathbf{7 8 . 7 / 7 9 . 3}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 7 / 4 0 . 5}$</td>
<td style="text-align: center;">$\mathbf{3 3 . 5 / 3 1 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 1 / 6 3 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Common Sense</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Other Reasoning Tasks</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Symbolic Reasoning</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Common <br> SenseQA</td>
<td style="text-align: center;">Strategy <br> QA</td>
<td style="text-align: center;">Date <br> Understand</td>
<td style="text-align: center;">Shuffled <br> Objects</td>
<td style="text-align: center;">Last Letter <br> (4 words)</td>
<td style="text-align: center;">Coin Flip <br> (4 times)</td>
</tr>
<tr>
<td style="text-align: left;">zero-shot</td>
<td style="text-align: center;">$\mathbf{6 8 . 8 / 7 2 . 6}$</td>
<td style="text-align: center;">$12.7 / \mathbf{5 4 . 3}$</td>
<td style="text-align: center;">$49.3 / 33.6$</td>
<td style="text-align: center;">$31.3 / 29.7$</td>
<td style="text-align: center;">$0.2 /-$</td>
<td style="text-align: center;">$12.8 / 53.8$</td>
</tr>
<tr>
<td style="text-align: left;">zero-shot-cot</td>
<td style="text-align: center;">$64.6 / 64.0$</td>
<td style="text-align: center;">$\mathbf{5 4 . 8 / 5 2 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 7 . 5 / 6 1 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 2 . 4 / 5 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 6 / -}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 4 / 8 7 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison with baseline methods using accuracies on MultiArith and GSM8K. text-davinci002 is used as the model if not specified. We used the same 8 examples as described in [Wei et al., 2022] for Few-shot and Few-shot-CoT settings. (<em>1) To verify the variance of changing examples, we report two results for 4 -shot-cot by splitting the eight examples into two groups. (</em>2) We insert "Let's think step by step." at the beginning of answer part of each exemplars for Few-shot-CoT to test performance gains. Further experiment results with PaLM are found at Appendix D</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">GSM8K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: center;">$\mathbf{1 7 . 7}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot (2 samples)</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot (8 samples)</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Shot-CoT</td>
<td style="text-align: center;">$\mathbf{7 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT (2 samples)</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">41.3</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT (4 samples : First) (*1)</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT (4 samples : Second) (*1)</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">Few-Shot-CoT (8 samples)</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: left;">Zero-Plus-Few-Shot-CoT (8 samples) (*2)</td>
<td style="text-align: center;">$\mathbf{9 2 . 8}$</td>
<td style="text-align: center;">$\mathbf{5 1 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Finetuned GPT-3 175B [Wei et al., 2022]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33</td>
</tr>
<tr>
<td style="text-align: left;">Finetuned GPT-3 175B + verifier [Wei et al., 2022]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Zero-Shot</td>
<td style="text-align: center;">$\mathbf{2 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 2 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Zero-Shot-CoT</td>
<td style="text-align: center;">$\mathbf{6 6 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 3 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Zero-Shot-CoT + self consistency</td>
<td style="text-align: center;">$\mathbf{8 9 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Few-Shot [Wei et al., 2022]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Few-Shot-CoT [Wei et al., 2022]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">56.9</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B: Few-Shot-CoT + self consistency [Wang et al., 2022]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">74.4</td>
</tr>
</tbody>
</table>
<p>example, Zero-shot-CoT achieves score gains from $17.7 \%$ to $78.7 \%$ on MultiArith and from $10.4 \%$ to $40.7 \%$ on GSM8K. Our method gives on-par performances for the remaining two arithmetic reasoning tasks (SingleEq and AddSub), which is expected since they do not require multi-step reasoning.
In commonsense reasoning tasks, Zero-shot-CoT does not provide performance gains. It is expected as Wei et al. [2022] also reports that even Few-shot-CoT does not provide performance gains on Lambda (135B), but does improve StrategyQA when combined with substantially larger PaLM (540B) model, which may also apply for ours. More importantly, we observe that many generated chain of thought themselves are surprisingly logically correct or only contains human-understandable mistakes (See Table 3), suggesting that Zero-shot-CoT does elicit for better commonsense reasoning even when the task metrics do not directly reflect it. We provide samples generated by Zero-shot-CoT for each dataset in Appendix B.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Model scale study with various types of models. S: text-ada-001, M: text-babbage-001, L: text-curie-001, XL: text-davinci-002. See Appendix A. 3 and E for the detail.</p>
<p>Table 3: Examples generated by Zero-Shot-CoT on CommonsenseQA for Error Analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Example 1 (Reasoning is correct.)</th>
<th style="text-align: left;">Example 2 (Model cannot choose only one.)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Q: Where is a well used toy car likely to be found?</td>
<td style="text-align: left;">Q: What would be necessary for getting in shape?</td>
</tr>
<tr>
<td style="text-align: left;">Answer Choices: (A) child's room (B) boy's bed-</td>
<td style="text-align: left;">Answer Choices: (A) good health (B) exercise (C)</td>
</tr>
<tr>
<td style="text-align: left;">room (C) own home (D) toy store (E) house</td>
<td style="text-align: left;">muscle tone (D) sweat (E) feel better</td>
</tr>
<tr>
<td style="text-align: left;">A: Let's think step by step. A toy car is likely</td>
<td style="text-align: left;">A: Let's think step by step. In order to get in shape,</td>
</tr>
<tr>
<td style="text-align: left;">to be found in a child's room. A child's room is</td>
<td style="text-align: left;">you need to be able to do three things:</td>
</tr>
<tr>
<td style="text-align: left;">likely to be found in a house. Therefore, a toy</td>
<td style="text-align: left;">1. Exercise regularly</td>
</tr>
<tr>
<td style="text-align: left;">car is likely to be found in a house.</td>
<td style="text-align: left;">2. Eat a healthy diet</td>
</tr>
<tr>
<td style="text-align: left;">Therefore, among A through E, the answer is E.</td>
<td style="text-align: left;">3. Get enough sleep</td>
</tr>
<tr>
<td style="text-align: left;">$\boldsymbol{X}$ (GT : A, Zero-shot: A)</td>
<td style="text-align: left;">If you can do all three of those things, then you will</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">be well on your way to getting in shape! Therefore,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">among A through E, the answer is B, C, and D.</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">(GT : B, Zero-shot: B)</td>
</tr>
</tbody>
</table>
<p>Comparison with other baselines Table 2 compares the performances on two arithmetic reasoning benchmarks (MultiArith and GSM8K) across Zero-shot-CoT and baselines. The large gap between standard prompting (1st block) and chain of thought prompting (2nd block) suggests that these tasks are difficult without eliciting multi-step reasoning. Major improvements are confirmed on both Instruct GPT-3 (text-davinci-002) and PaLM (540B) models (4th block). While Zero-shot-CoT naturally underperforms Few-shot-CoT, it substantially outperforms standard Few-shot prompting with even 8 examples per task. For GSM8K, Zero-shot-CoT with Instruct GPT-3 (text-davinci-002) also outperforms finetuned GPT-3 and standard few-shot prompting with large models (PaLM, 540B), reported in Wei et al. [2022] (3rd and 4th block). See App. D for more experiment results with PaLM.</p>
<p>Does model size matter for zero-shot reasoning? Figure 3 compares performance of various language models on MultiArith / GSM8K. Without chain of thought reasoning, the performance does not increase or increases slowly as the model scale is increased, i.e., the curve is mostly flat. In contrast, the performance drastically increases with chain of thought reasoning, as the model size gets bigger, for Original/Instruct GPT-3 and PaLM. When the model size is smaller, chain of thought reasoning is not effective. This result aligns with the few-shot experiment results in Wei et al. [2022]. Appendix E shows extensive experiment results using wider variety of language models, including GPT-2, GPT-Neo, GPT-J, T0, and OPT. We also manually investigated the quality of generated chain of thought, and large-scale models clearly demonstrate better reasoning (See Appendix B for the sampled outputs for each model).</p>
<p>Error Analysis To better understand the behavior of Zero-shot-CoT, we manually investigated randomly selected examples generated by Instruct-GPT3 with Zero-shot-CoT prompting. See Appendix C for examples, where some of the observations include: (1) In commonsense reasoning (CommonsenseQA), Zero-shot-CoT often produces flexible and reasonable chain of thought even when the final prediction is not correct. Zero-shot-CoT often output multiple answer choices when the model find it is difficult to narrow it down to one (see Table 3 for examples). (2) In arithmetic</p>
<p>Table 4: Robustness study against template measured on the MultiArith dataset with text-davinci-002. (<em>1) This template is used in </em>Ahn et al. [2022]<em> where a language model is prompted to generate step-by-step actions given a high-level instruction for controlling robotic actions. (</em>2) This template is used in Reynolds and McDonell [2021] but is not quantitatively evaluated.</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Category</th>
<th>Template</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>instructive</td>
<td>Let's think step by step.</td>
<td>78.7</td>
</tr>
<tr>
<td>2</td>
<td></td>
<td>First, (*1)</td>
<td>77.3</td>
</tr>
<tr>
<td>3</td>
<td></td>
<td>Let's think about this logically.</td>
<td>74.5</td>
</tr>
<tr>
<td>4</td>
<td></td>
<td>Let's solve this problem by splitting it into steps. (*2)</td>
<td>72.2</td>
</tr>
<tr>
<td>5</td>
<td></td>
<td>Let's be realistic and think step by step.</td>
<td>70.8</td>
</tr>
<tr>
<td>6</td>
<td></td>
<td>Let's think like a detective step by step.</td>
<td>70.3</td>
</tr>
<tr>
<td>7</td>
<td></td>
<td>Let's think</td>
<td>57.5</td>
</tr>
<tr>
<td>8</td>
<td></td>
<td>Before we dive into the answer,</td>
<td>55.7</td>
</tr>
<tr>
<td>9</td>
<td></td>
<td>The answer is after the proof.</td>
<td>45.7</td>
</tr>
<tr>
<td>10</td>
<td>misleading</td>
<td>Don't think. Just feel.</td>
<td>18.8</td>
</tr>
<tr>
<td>11</td>
<td></td>
<td>Let's think step by step but reach an incorrect answer.</td>
<td>18.7</td>
</tr>
<tr>
<td>12</td>
<td></td>
<td>Let's count the number of "a" in the question.</td>
<td>16.7</td>
</tr>
<tr>
<td>13</td>
<td></td>
<td>By using the fact that the earth is round,</td>
<td>9.3</td>
</tr>
<tr>
<td>14</td>
<td>irrelevant</td>
<td>By the way, I found a good restaurant nearby.</td>
<td>17.5</td>
</tr>
<tr>
<td>15</td>
<td></td>
<td>Abrakadabra!</td>
<td>15.5</td>
</tr>
<tr>
<td>16</td>
<td></td>
<td>It's a beautiful day.</td>
<td>13.1</td>
</tr>
<tr>
<td>-</td>
<td></td>
<td>(Zero-shot)</td>
<td>17.7</td>
</tr>
</tbody>
</table>
<p>Table 5: Robustness study of Few-shot-CoT against examples. When the examples are from entirely different tasks, the performance generally becomes worse, but when the answer formats are matched (i.e. CommonsenseQA to AQUA-RAT, multiple-choice), the performance loss is less severe. ${ }^{\dagger}$ CommonsenseQA samples are used in this variation</p>
<table>
<thead>
<tr>
<th></th>
<th>Zero-shot</th>
<th>Few-shot-CoT ${ }^{\dagger}$</th>
<th>Zero-shot-CoT</th>
<th>Few-shot-CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td>AQUA-RAT</td>
<td>22.4</td>
<td>$\underline{31.9}$</td>
<td>33.5</td>
<td>39.0</td>
</tr>
<tr>
<td>MultiArith</td>
<td>17.7</td>
<td>$\underline{27.0}$</td>
<td>78.7</td>
<td>88.2</td>
</tr>
</tbody>
</table>
<p>reasoning (MultiArith), Zero-shot-CoT and Few-shot-CoT show substantial differences regarding the error patterns. First, Zero-shot-CoT tends to output unnecessary steps of reasoning after getting the correct prediction, which results in changing the prediction to incorrect one. Zero-shot-CoT also sometimes does not start reasoning, just rephrasing the input question. In contrast, Few-shot-CoT tend to fail when generated chain of thought include ternary operation, e.g. $(3+2) * 4$.</p>
<p>How does prompt selection affect Zero-shot-CoT? We validate the robustness of Zero-shot-CoT against input prompts. Table 4 summarizes performance using 16 different templates with three categories. Specifically, following <em>Webson and Pavlick [2022]</em>, the categories include instructive (encourage reasoning), misleading (discourage reasoning or encouraging reasoning but in a wrong way), and irrelevant (nothing to do with reasoning). The results indicate that the performance is improved if the text is written in a way that encourages chain of thought reasoning, i.e., the templates are within "instructive" category. However, the difference in accuracy is significant depending on the sentence. In this experiment, "Let's think step by step." achieves the best results. Interestingly, it is found that different templates encourage the model to express reasoning quite differently (see Appendix B for sample outputs by each template). In contrast, when we use misleading or irrelevant templates, the performance does not improve. It remains an open question how to automatically create better templates for Zero-shot-CoT.</p>
<p>How does prompt selection affect Few-shot-CoT? Table 5 shows the performance of Few-shot-CoT when using examples from different datasets: CommonsenseQA to AQUA-RAT and CommonsenseQA to MultiArith. The domains are different in both cases, but the answer format</p>
<p>is the same in the former. Surprisingly, the chain of thought examples from different domains (common sense to arithmetic) but with the same answer (multiple-choice) format provide substantial performance gain over Zero-shot (to AQUA-RAT), measured relative to the possible improvements from Zero-shot-CoT or Few-shot-CoT. In contrast, the performance gain becomes much less when using examples with different answer types (to MultiArith), confirming prior work [Min et al., 2022] that suggests LLMs mostly leverage the few-shot examples to infer the repeated format rather than the task itself in-context. Nevertheless, for both cases the results are worse than Zero-shot-CoT, affirming the importance of task-specific sample engineering in Few-shot-CoT.</p>
<h1>5 Discussion and Related Work</h1>
<p>Table 6: Summary of related work on arithmetic/commonsense reasoning tasks. Category denotes the training strategy. CoT denotes whether to output chain of thought. Task column lists the tasks that are performed in corresponding papers. AR: Arithmetic Reasoning, CR: Commonsense Reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">CoT</th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Model</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rajani et al. [2019]</td>
<td style="text-align: left;">Fine-Tuning</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">CR</td>
<td style="text-align: left;">GPT</td>
</tr>
<tr>
<td style="text-align: left;">Cobbe et al. [2021]</td>
<td style="text-align: left;">Fine-Tuning</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR</td>
<td style="text-align: left;">GPT-3</td>
</tr>
<tr>
<td style="text-align: left;">Zelikman et al. [2022]</td>
<td style="text-align: left;">Fine-Tuning</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">GPT-3, etc</td>
</tr>
<tr>
<td style="text-align: left;">Nye et al. [2022]</td>
<td style="text-align: left;">Fine-Tuning</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR</td>
<td style="text-align: left;">Transformer(Decoder)</td>
</tr>
<tr>
<td style="text-align: left;">Brown et al. [2020]</td>
<td style="text-align: left;">Few/Zero-Shot</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">CR</td>
<td style="text-align: left;">GPT-3</td>
</tr>
<tr>
<td style="text-align: left;">Smith et al. [2022]</td>
<td style="text-align: left;">Few/Zero-Shot</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">MT-NLG</td>
</tr>
<tr>
<td style="text-align: left;">Rae et al. [2021]</td>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">Gopher</td>
</tr>
<tr>
<td style="text-align: left;">Wei et al. [2022]</td>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">PaLM, LaMBDA, GPT-3</td>
</tr>
<tr>
<td style="text-align: left;">Wang et al. [2022]</td>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">PaLM, etc</td>
</tr>
<tr>
<td style="text-align: left;">Chowdhery et al. [2022]</td>
<td style="text-align: left;">Few-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">PaLM</td>
</tr>
<tr>
<td style="text-align: left;">Shwartz et al. [2020]</td>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">CR</td>
<td style="text-align: left;">GPT-2, etc</td>
</tr>
<tr>
<td style="text-align: left;">Reynolds and McDonell [2021]</td>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR</td>
<td style="text-align: left;">GPT-3</td>
</tr>
<tr>
<td style="text-align: left;">Zero-shot-CoT (Ours)</td>
<td style="text-align: left;">Zero-Shot</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">AR,CR</td>
<td style="text-align: left;">PaLM, Instruct-GPT3, GPT-3, etc</td>
</tr>
</tbody>
</table>
<p>Reasoning Ability of LLMs Several studies have shown that pre-trained models usually are not good at reasoning [Brown et al., 2020, Smith et al., 2022, Rae et al., 2021], but its ability can be substantially increased by making them produce step-by-step reasoning, either by fine-tuning [Rajani et al., 2019, Cobbe et al., 2021, Zelikman et al., 2022, Nye et al., 2022] or few-shot prompting [Wei et al., 2022, Wang et al., 2022, Chowdhery et al., 2022] (See Table 6 for summary of related work). Unlike most prior work, we focus on zero-shot prompting and show that a single fixed trigger prompt substantially increases the zero-shot reasoning ability of LLMs across a variety of tasks requiring complex multi-hop thinking (Table 1), especially when the model is scaled up (Figure 3). It also generates reasonable and understandable chain of thought across diverse tasks (Appendix B), even when the final prediction is wrong (Appendix C). Similar to our work, Reynolds and McDonell [2021] demonstrate a prompt, "Let's solve this problem by splitting it into steps", would facilitate the multi-step reasoning in a simple arithmetic problem. However, they treated it as a task-specific example and did not evaluate quantitatively on diverse reasoning tasks against baselines. Shwartz et al. [2020] propose to decompose a commonsense question into a series of information seeking question, such as "what is the definition of [X]". It does not require demonstrations but requires substantial manual prompt engineering per each reasoning task. Our results strongly suggest that LLMs are decent zero-shot reasoners, while prior work [Wei et al., 2022] often emphasize only few-shot learning and task-specific in-context learning, e.g. no zero-shot baselines were reported. Our method does not require time-consuming fine-tuning or expensive sample engineering, and can be combined with any pre-trained LLM, serving as the strongest zero-shot baseline for all reasoning tasks.</p>
<p>Zero-shot Abilities of LLMs Radford et al. [2019] show that LLMs have excellent zero-shot abilities in many system-1 tasks, including reading comprehension, translation, and summarization.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Sanh et al. [2022], Ouyang et al. [2022] show that such zero-shot abilities of LLMs can be increased by explicitly fine-tuning models to follow instructions. Although these work focus on the zero-shot performances of LLMs, we focus on many system-2 tasks beyond system-1 tasks, considered a grand challenge for LLMs given flat scaling curves. In addition, Zero-shot-CoT is orthogonal to instruction tuning; it increases zero-shot performance for Instruct GPT3, vanilla GPT3, and PaLM (See Figure 3).</p>
<p>From Narrow (task-specific) to Broad (multi-task) Prompting Most prompts are task-specific. While few-shot prompts are naturally so due to task-specific in-context samples [Brown et al., 2020, Wei et al., 2022], majority of zero-shot prompts have also focused on per-task engineering (of templates) [Liu et al., 2021b, Reynolds and McDonell, 2021]. Borrowing terminologies from Chollet [2019] which builds on hierarchical models of intelligence [McGrew, 2005, Johnson and Bouchard Jr, 2005], these prompts are arguably eliciting "narrow generalization" or task-specific skills from LLMs. On the other hand, our method is a multi-task prompt and elicits "broad generalization" or broad cognitive abilities in LLMs, such as logical reasoning or system-2 itself. We hope our work can serve as a reference for accelerating not just logical reasoning research with LLMs, but also discovery of other broad cognitive capabilities within LLMs.</p>
<p>Training Dataset Details A limitation of the work is the lack of public information on the details of training datasets used for LLMs, e.g. 001 vs 002 for GPT models, original GPT3 vs InstructGPT [Ouyang et al., 2022], and data for PaLM models [Chowdhery et al., 2022]. However, big performance increases from Zero-shot to Zero-shot-CoT in all recent large models (InstructGPT 001 or 002, Original GPT3, and PaLM) and consistent improvements in both arithmetic and nonarithmetic tasks suggest that the models are unlikely simply memorizing, but instead capturing a task-agnostic multi-step reasoning capability for generic problem solving. While most results are based on InstructGPT since it is the best performing open-access LLM, key results are reproduced on PaLM, and dataset details in InstructGPT (Appendix A, B, and F in Ouyang et al. [2022]) also confirm that it is not specially engineered for multi-step reasoning.</p>
<p>Limitation and Social Impact Our work is based on prompting methods for large language models. LLMs have been trained on large corpora from various sources on the web (also see "Training Dataset Details"), and have shown to capture and amplify biases found in the training data. Prompting is a method that looks to take advantage of the patterns captured by language models conducive to various tasks, and therefore it has the same shortcomings. This being said, our approach is a more direct way to probe complex reasoning inside pre-trained LLMs, removing the confounding factor of in-context learning in prior few-shot approaches, and can lead to more unbiased study of biases in LLMs.</p>
<h1>6 Conclusion</h1>
<p>We have proposed Zero-shot-CoT, a single zero-shot prompt that elicits chain of thought from large language models across a variety of reasoning tasks, in contrast to the few-shot (in-context) approach in previous work that requires hand-crafting few-shot examples per task. Our simple method not only is the minimalist and strongest zero-shot baseline for difficult multi-step system-2 reasoning tasks that long evaded the scaling laws of LLMs, but also encourages the community to further discover similar multi-task prompts that elicit broad cognitive abilities instead of narrow task-specific skills.</p>
<h2>Acknowledgements</h2>
<p>This work has been supported by the Mohammed bin Salman Center for Future Science and Technology for Saudi-Japan Vision 2030 at The University of Tokyo (MbSC2030). Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used for experiments other than PaLM. We also thank Jason Wei and Denny Zhou for discussions and support on running PaLM experiments, and Sharan Narang and Aakanksha Chowdhery for generic support on PaLM infrastructures.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian</p>
<p>Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/ 2204.01691 .</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi . org/10.5281/zenodo. 5297715 .</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in NeurIPS, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. URL https://arxiv.org/abs/1911.01547.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, pages 4171-4186, 2019. URL https://aclanthology.org/N19-1423.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv: Arxiv-2101.00027, 2020.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Proceedings of ACL-IJCNLP, pages 3816-3830, 2021. URL https://aclanthology. org/2021.acl-long. 295.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. TACL, 9:346-361, 2021. URL https://aclanthology.org/2021.tacl-1.21/.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In EMNLP, volume 523533. Citeseer, 2014. URL https://aclanthology.org/D14-1058/.</p>
<p>Wendy Johnson and Thomas J Bouchard Jr. The structure of human intelligence: It is verbal, perceptual, and image rotation (vpr), not fluid and crystallized. Intelligence, 33(4):393-416, 2005.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. TACL, 3:585-597, 2015. URL https : //aclanthology.org/Q15-1042.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of NAACL, pages 1152-1157, 2016. URL https://aclanthology.org/N16-1136.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of ACL, pages 158-167, 2017. URL https://aclanthology.org/P17-1015.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021a. URL https://arxiv.org/abs/2101.06804.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021b. URL https://arxiv.org/abs/2107. 13586 .</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of $A C L$, pages 8086-8098, 2022. URL https://aclanthology.org/2022.acl-long. 556.</p>
<p>Kevin S McGrew. The cattell-horn-carroll theory of cognitive abilities: Past, present, and future. 2005 .</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv: Arxiv-1609.07843, 2016. URL https://arxiv.org/abs/1609. 07843 .</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022. URL https://arxiv.org/pdf/2202.12837.pdf.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022. URL https://openreview.net/forum? id=HBlx2idbkbq.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in NeurIPS, 32:8026-8037, 2019. URL https://papers.nips.cc/paper/2019/hash/ bdbca288fee7f92f2bfa9f7012727740-Abstract.html.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of NAACL, pages 2080-2094, 2021. URL https:// aclanthology.org/2021.naacl-main. 168.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, page 9, 2019. URL http://www. persagen.com/files/misc/radford2019language.pdf.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis \&amp; insights from training gopher, 2021. URL https://arxiv.org/abs/2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of ACL, pages 4932-4942, 2019. URL https://aclanthology.org/P19-1487.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7, 2021. URL https://arxiv.org/pdf/2102.07350.pdf.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of EMNLP, pages 1743-1752, 2015. URL https://aclanthology.org/D15-1202.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In ICLR, 2022. URL https://openreview.net/forum?id=9Vrb9D0W14.</p>
<p>Timo Schick and Hinrich Schütze. It's not just size that matters: Small language models are also fewshot learners. In Proceedings of NAACL, pages 2339-2352, 2021. URL https://aclanthology . org/2021.naacl-main. 185 .</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. In Proceedings of EMNLP, pages 4222-4235, 2020. URL https://aclanthology.org/2020. emnlp-main. 346 .</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In Proceedings of EMNLP, pages 4615-4629, 2020. URL https://aclanthology.org/2020.emnlp-main. 373.</p>
<p>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model, 2022. URL https: //arxiv.org/abs/2201.11990.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/2206.04615.</p>
<p>Keith E Stanovich and Richard F West. Individual differences in reasoning: Implications for the rationality debate? Behavioral and brain sciences, 23(5):645-665, 2000.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of NAACL-HLT, pages 4149-4158, 2019. URL https://aclanthology.org/N19-1421/.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. URL https: //arxiv.org/abs/2201.08239.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in NeurIPS, 2017. URL https://proceedings.neurips.cc/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. URL https://arxiv.org/abs/2203.11171.</p>
<p>Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300-2344. Association for Computational Linguistics, July 2022. URL https://aclanthology.org/2022.naacl-main. 167 .</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022. URL https: //arxiv.org/abs/2201.11903.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of EMNLP, 2020. URL https://aclanthology.org/ 2020.emnlp-demos.6.</p>
<p>Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. URL https://arxiv.org/abs/2205.01068.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes]
(c) Did you discuss any potential negative societal impacts of your work? [Yes]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes]
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes]
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Our paper mainly used GPT-3 API with greedy decoding, and there are no randomness for the experiments.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes]
(b) Did you mention the license of the assets? [Yes]
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes]
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? $[\mathrm{N} / \mathrm{A}]$
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? $[\mathrm{N} / \mathrm{A}]$</li>
</ol>
<h1>A Details of Experimental Setup</h1>
<h2>A. 1 Code</h2>
<p>Code is available at https://github.com/kojima-takeshi188/zero_shot_cot.</p>
<h2>A. 2 Datasets</h2>
<h2>A.2.1 Dataset Description</h2>
<p>Table 7 summarizes the description of each dataset used in our experiment.</p>
<p>Table 7: Dataset Description. Our experiments used publicly available datasets except for "Last Letters" and "Coin Flip" datasets. We created these two datasets. See Appendix A.2.2 for the details. (<em>1) N : Number, M : Pick up one from multiple choices, Y : Answer Yes or No, F : Free Format. (</em>2) Average number of words in questions texts.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Answer <br> Format <br> (*1)</th>
<th style="text-align: left;"># of <br> samples</th>
<th style="text-align: left;">Avg # <br> words <br> (*2)</th>
<th style="text-align: left;">Data split (filename) <br> used for our experiment</th>
<th style="text-align: left;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">508</td>
<td style="text-align: left;">27.4</td>
<td style="text-align: left;">questions.json</td>
<td style="text-align: left;">No License</td>
</tr>
<tr>
<td style="text-align: left;">AddSub</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">395</td>
<td style="text-align: left;">31.5</td>
<td style="text-align: left;">AddSub.json</td>
<td style="text-align: left;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">600</td>
<td style="text-align: left;">31.8</td>
<td style="text-align: left;">MultiArith.json</td>
<td style="text-align: left;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">1319</td>
<td style="text-align: left;">46.9</td>
<td style="text-align: left;">test.jsonl</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">254</td>
<td style="text-align: left;">51.9</td>
<td style="text-align: left;">test.jsonl</td>
<td style="text-align: left;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: left;">N</td>
<td style="text-align: left;">1000</td>
<td style="text-align: left;">31.8</td>
<td style="text-align: left;">SVAMP.json</td>
<td style="text-align: left;">MIT License</td>
</tr>
<tr>
<td style="text-align: left;">CommonsenseQA</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">1221</td>
<td style="text-align: left;">27.8</td>
<td style="text-align: left;">dev_rand_split.jsonl</td>
<td style="text-align: left;">Unspecified</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">2290</td>
<td style="text-align: left;">9.6</td>
<td style="text-align: left;">task.json</td>
<td style="text-align: left;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Date Understanding</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">369</td>
<td style="text-align: left;">35.0</td>
<td style="text-align: left;">task.json</td>
<td style="text-align: left;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Shuffled Objects</td>
<td style="text-align: left;">M</td>
<td style="text-align: left;">750</td>
<td style="text-align: left;">91.1</td>
<td style="text-align: left;">three_objects/task.json</td>
<td style="text-align: left;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: left;">Last Letters</td>
<td style="text-align: left;">F</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">15.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Coin Flip</td>
<td style="text-align: left;">Y</td>
<td style="text-align: left;">500</td>
<td style="text-align: left;">37.0</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<h2>A.2.2 Dataset creation</h2>
<p>Regarding "Last Letter Concatenation" and "Coin Flip", datasets are not publicly available so we created the datasets following Wei et al. [2022] with a minor rephrasing of the question template. Specifically, as for Last Letter Concatenation, we use the following template. We randomly select human names from names-dataset library (https://pypi.org/project/names-dataset/) and insert them into {Name1} through {Name4}.</p>
<ul>
<li>'Take the last letters of each words in "{Name1} {Name2} {Name3} {Name4}" and concatenate them.'</li>
</ul>
<p>As for Coin Flip, we use the following template. We randomly select human names from namesdataset library and insert them into {Name1} through {Name4}. We also randomly pick up "flips" or "does not flip" and insert the phrase into each {flips I does not flip} part, respectively.</p>
<ul>
<li>'A coin is heads up. {Name1} {flips I does not flip} the coin. {Name2} {flips I does not flip} the coin. {Name3} {flips I does not flip} the coin. {Name4} {flips I does not flip} the coin. Is the coin still heads up? Note that "flip" here means "reverse".</li>
</ul>
<h2>A. 3 Language Models</h2>
<p>Our experiment uses multiple language models as described at Table 8</p>
<h2>A. 4 Implementation details</h2>
<p>For Original GPT-3 and Instruct-GPT3, we used OpenAI API. For OPT, T0, GPT-J, GPT-Neo, and GPT-2, we used Hugging Face Transformer Library [Wolf et al., 2020]. We set max_tokens $=128$ and</p>
<p>Table 8: Description of language models. (<em>1) As for Original GPT3 models, we assign model size information to each model by referring to https://blog.eleuther.ai/gpt3-model-sizes/ and https://beta.openai.com/docs/model-index-for-researchers. (</em>2) There is no official information about the model size of Instruct GPT3. We infer from the API name that the order of model size of Instruct GPT3 matches that of Original GPT3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Language Model</th>
<th style="text-align: center;"># of params</th>
<th style="text-align: center;">Library / API Name</th>
<th style="text-align: center;">Model Name in Library / API</th>
<th style="text-align: center;">License</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">62B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">8B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Original GPT3</td>
<td style="text-align: center;">175B (*1)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">davinci</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Original GPT3</td>
<td style="text-align: center;">6.7B (*1)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">curie</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Original GPT3</td>
<td style="text-align: center;">1.3B (*1)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">babbage</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Original GPT3</td>
<td style="text-align: center;">0.3B (*1)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">ada</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Instruct GPT3</td>
<td style="text-align: center;">- (*2)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">text-davinci-002</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Instruct GPT3</td>
<td style="text-align: center;">- (*2)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">text-davinci-001</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Instruct GPT3</td>
<td style="text-align: center;">- (*2)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">text-curie-001</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Instruct GPT3</td>
<td style="text-align: center;">- (*2)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">text-babbage-001</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">Instruct GPT3</td>
<td style="text-align: center;">- (*2)</td>
<td style="text-align: center;">OpenAI API</td>
<td style="text-align: center;">text-ada-001</td>
<td style="text-align: center;">unspecified</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Hugging Face Library</td>
<td style="text-align: center;">opt-13b</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: center;">T0</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">Hugging Face Library</td>
<td style="text-align: center;">T0pp</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-J</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">Hugging Face Library</td>
<td style="text-align: center;">gptj</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">2.7B</td>
<td style="text-align: center;">Hugging Face Library</td>
<td style="text-align: center;">gpt-neo</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-2</td>
<td style="text-align: center;">1.5B</td>
<td style="text-align: center;">Hugging Face Library</td>
<td style="text-align: center;">gpt2-xl</td>
<td style="text-align: center;">Apache-2.0</td>
</tr>
</tbody>
</table>
<p>used greedy decoding (temperature $=0$ in the case of OpenAI API) across all the methods and models except PaLM. For PaLM, we used 'TopK=1' for greedy deterministic decoding and max_tokens $=$ 256. "Q:" is set as a customized stop sequence for all the models except for Instruct-GPT3 to stop the models from repeating questions and answers by themselves. We run our experiments on cloud V100 instances without GPU for GPT-3 models, on cloud A100x8 GPU(60GB) instances for T0 and OTP, and on cloud A100x1 GPU(60GB) instances for GPT-J, GPT-Neo, and GPT-2. Our implementation is in PyTorch [Paszke et al., 2019].</p>
<h1>A. 5 Prompts For Answer Extraction</h1>
<p>Table 9 and Table 10 summarizes a list of answer extraction prompts used for the experiments at Table 1. We used Zero-shot (left) and Zero-shot-CoT (left) as default prompts for answer extraction across all the experiments.</p>
<p>Table 9: Answer extraction prompts used for Zero-shot experiments in Table 1. C.S.QA : CommonsenseQA, D.U. : Date Understanding, S.O. : Tracking Shuffled Objects</p>
<table>
<thead>
<tr>
<th style="text-align: left;">No</th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Zero-Shot (left)</th>
<th style="text-align: left;">Zero-Shot (right)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: left;">The answer (arabic numerals) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">AddSub</td>
<td style="text-align: left;">The answer (arabic numerals) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: left;">The answer (arabic numerals) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: left;">The answer (arabic numerals) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: left;">Among A through E, the answer is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: left;">The answer (arabic numerals) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">C.S.QA</td>
<td style="text-align: left;">Among A through E, the answer is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">The answer (Yes or No) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">D.U.</td>
<td style="text-align: left;">Among A through F, the answer is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: left;">S.O.</td>
<td style="text-align: left;">Among A through C, the answer is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">11</td>
<td style="text-align: left;">Last Letters</td>
<td style="text-align: left;">The answer is</td>
<td style="text-align: left;">The answer is</td>
</tr>
<tr>
<td style="text-align: left;">12</td>
<td style="text-align: left;">Coin Flip</td>
<td style="text-align: left;">The answer (Yes or No) is</td>
<td style="text-align: left;">The answer is</td>
</tr>
</tbody>
</table>
<p>Table 10: Answer extraction prompts used for Zero-shot-CoT experiments in Table 1. C.S.QA : CommonsenseQA, D.U. : Date Understanding, S.O. : Tracking Shuffled Objects</p>
<table>
<thead>
<tr>
<th style="text-align: left;">No</th>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Zero-Shot-CoT (left)</th>
<th style="text-align: left;">Zero-Shot-CoT (right)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">SingleEq</td>
<td style="text-align: left;">Therefore, the answer (arabic numerals) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">AddSub</td>
<td style="text-align: left;">Therefore, the answer (arabic numerals) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: left;">MultiArith</td>
<td style="text-align: left;">Therefore, the answer (arabic numerals) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: left;">GSM8K</td>
<td style="text-align: left;">Therefore, the answer (arabic numerals) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">AQUA-RAT</td>
<td style="text-align: left;">Therefore, among A through E, the answer is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">SVAMP</td>
<td style="text-align: left;">Therefore, the answer (arabic numerals) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">C.S.QA</td>
<td style="text-align: left;">Therefore, among A through E, the answer is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: left;">Therefore, the answer (Yes or No) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">D.U.</td>
<td style="text-align: left;">Therefore, among A through F, the answer is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">10</td>
<td style="text-align: left;">S.O.</td>
<td style="text-align: left;">Therefore, among A through C, the answer is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">11</td>
<td style="text-align: left;">Last Letters</td>
<td style="text-align: left;">Therefore, the answer is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
<tr>
<td style="text-align: left;">12</td>
<td style="text-align: left;">Coin Flip</td>
<td style="text-align: left;">Therefore, the answer (Yes or No) is</td>
<td style="text-align: left;">Therefore, the answer is</td>
</tr>
</tbody>
</table>
<h1>A. 6 Answer Cleansing</h1>
<p>Table 11 summarizes a list of answer cleansing approaches used across all the experiments.</p>
<p>Table 11: Detail description of answer cleansing. See Table 7 for the mapping between each datasets and the corresponding answer formats.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Answer <br> Format</th>
<th style="text-align: center;">Answer Cleansing Approach</th>
<th style="text-align: center;">Pseudo Code <br> (Example in Pytorch 3.8)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Number</td>
<td style="text-align: center;">Pick up the first number encountered in the text.</td>
<td style="text-align: center;"><code>pred = pred.replace(",", "") pred = [s for s in re.findall(r'-7\ d+\.7\d+', pred)] pred = pred[0]</code></td>
</tr>
<tr>
<td style="text-align: center;">Multiple- <br> Choice</td>
<td style="text-align: center;">Pick up the first large letter encountered in the text.</td>
<td style="text-align: center;"><code>pred = re.findall(r'\A|B|C|D|E', pred) pred = pred[0]</code></td>
</tr>
<tr>
<td style="text-align: center;">Yes or No</td>
<td style="text-align: center;">Pick up the first "yes" or "no" encountered in the text after removing unnecessary letters.</td>
<td style="text-align: center;"><code>pred = pred.lower() pred = re.sub("\"|\'\|\n|\.\|s|\:\|,\, ",",", pred) pred = pred.split(",") pred = [i for i in pred if i in (" yes", "no")) pred = pred[0]</code></td>
</tr>
<tr>
<td style="text-align: center;">Free Format</td>
<td style="text-align: center;">Just remove unnecessary letters, such as ".".</td>
<td style="text-align: center;"><code>pred = re.sub("\"|\'\|\n|\.\|s","", pred)</code></td>
</tr>
</tbody>
</table>
<h2>B Additional Experiment Results</h2>
<p>This section summarizes more example texts generated by models in our experiments. Note that for readability all texts are modified from the original ones by omitting or inserting some linebreaks. Without mentioning otherwise, we use Instruct-GPT3 (text-davinci-002) model.</p>
<ul>
<li>Table 12 lists example texts generated by Zero-shot-CoT for each dataset (See Table 1).</li>
<li>Table 13 lists example texts generated by Zero-shot-CoT for each reasoning extraction template (See Table 4).</li>
<li>
<p>Table 14 and Table 15 lists example texts generated by Zero-shot-CoT for each langugage model (See Table 26).</p>
</li>
<li>
<p>Table 16 has an example text generated by Few-shot.</p>
</li>
<li>Table 17 has an example text generated by Few-shot-CoT.</li>
<li>Table 18 has an example text generated by Few-shot-CoT with exemplars from a different task (Exemplars from CommonsenseQA, and a task is from MultiArith).</li>
<li>Table 19 has an example text generated by Zero-Plus-Few-Shot-CoT.</li>
<li>Table 20 compares different outcome scenarios on results generated by Zero-shot and Zero-shot-CoT using PaLM (540B) model.</li>
</ul>
<p>Table 12: Example outputs by Zero-shot-CoT for each datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SingleEq</td>
<td style="text-align: center;">Q: A spaceship traveled 0.5 of a light-year from Earth to Planet X and 0.1 of a lightyear from Planet X to Planet Y. Then it traveled 0.1 of a lightyear from Planet Y back to Earth. How many light-years did the spaceship travel in all?</td>
<td style="text-align: center;">A: Let's think step by step. The spaceship traveled 0.5 light-years from Earth to Planet X. Then it traveled 0.1 light-years from Planet X to Planet Y. Finally, it traveled 0.1 light-years from Planet Y back to Earth. So the total distance the spaceship traveled is $0.5+0.1+0.1=0.7$ light-years. Therefore, the answer (arabic numerals) is: 0.7 light-years $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: While making desserts for a bake sale, Victor used 0.625 of a scoop of brown sugar as well as 0.25 of a scoop of white sugar. How much more brown sugar did Victor use?</td>
<td style="text-align: center;">A: Let's think step by step. 0.625 of a scoop of brown sugar is the same as 0.25 of a scoop of white sugar. This means that Victor used the same amount of brown sugar as he did white sugar. Therefore, the answer (arabic numerals) is $0 . \mathcal{X}(\mathbf{G T}$ : 0.375 )</td>
</tr>
<tr>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">Sandy grew 6 carrots . Sam grew 3 carrots . How many carrots did they grow in total?</td>
<td style="text-align: center;">A: Let's think step by step. Sandy grew 6 carrots. Sam grew 3 carrots. That means they grew a total of 9 carrots. Therefore, the answer (arabic numerals) is $9 . \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Melanie bought a Batman game for \$ 6.95 , a strategy game for \$ 7.90 , and a Superman game for \$ 7.73 . Melanie already owns 4 games . How much did Melanie spend on video games?</td>
<td style="text-align: center;">A: Let's think step by step. Melanie bought a Batman game for \$ 6.95. So she spent \$ 6.95 on video games. Then she bought a strategy game for $\$ 7.90$. So she spent $\$ 14.85$ on video games. Then she bought a Superman game for $\$ 7.73$. So she spent $\$ 22.58$ on video games. Melanie already owns 4 games. So she spent $\$ 18.58$ on video games. Therefore, Melanie spent a total of $\$ 18.58$ on video games. Therefore, the answer (arabic numerals) is: $18.58 \mathcal{X}(\mathbf{G T}: \mathbf{2 2 . 5 8})$</td>
</tr>
<tr>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">Q: Adam bought 2 boxes of chocolate candy and 5 boxes of caramel candy. If each box has 4 pieces inside it, how much candy did he have total?</td>
<td style="text-align: center;">A: Let's think step by step. Adam bought 2 boxes of chocolate candy and 5 boxes of caramel candy. We know that each box has 4 pieces inside it. So, we can multiply 2 by 4 to find out how many pieces of chocolate candy Adam bought. This will give us 8 . We can also multiply 5 by 4 to find out how many pieces of caramel candy Adam bought. This will give us 20 . Now, we can add 8 and 20 together to find out how much candy Adam bought in total. $8+20=28$ Adam bought 28 pieces of candy in Therefore, the answer (arabic numerals) is $28 . \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Continued on next page</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 12 - Continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Billy was organizing his baseball cards in a binder with 5 on each page. If he had 3 new cards and 42 old cards to put in the binder, how many pages would he use?</td>
<td style="text-align: center;">A: Let's think step by step. Billy has 42 old cards. He can put 5 old cards on each page, which means he will use 9 pages for old cards. Billy has 3 new cards. He can put 5 new cards on each page, which means he will use 1 page for new cards. So in total, Billy will use 10 pages. Therefore, the answer (arabic numerals) is 10 . $\mathcal{X}(\mathrm{GT}: 9)$</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Q: James buys a plane. The plane cost $\$ 150,000$. He pays $\$ 5000$ a month to rent a hanger to keep it in. He also spends twice as much as that on fuel per month. How much did it cost him to get and maintain the plane for the first year?</td>
<td style="text-align: center;">A: Let's think step by step. James buys a plane. This costs him $\$ 150,000$. He pays $\$ 5000$ a month to rent a hanger. This costs him $\$ 60,000$ for the year. He spends twice as much as that on fuel per month. This costs him $\$ 120,000$ for the year. So the total cost for the first year is $\$ 150,000+$ $\$ 60,000+\$ 120,000=\$ 330,000$. Therefore, the answer (arabic numerals) is $330,000 . \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: James is counting his Pokemon cards. He has 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type and buys 14 grass type, what's the percentage chance (rounded to the nearest integer) that a randomly picked card will be a water type?</td>
<td style="text-align: center;">A: Let's think step by step. There are 30 fire type, 20 grass type, and 40 water type. If he loses 8 of the water type, then he has 32 cards in total. If he buys 14 grass type, then he has 46 cards in total, and the number of water type cards becomes 32. The percentage chance that a randomly picked card will be a water type is $\$ \backslash f r a c{32}{46} \backslash t i m e s 100 \backslash \%=$ $\backslash$ fbox ${70} \backslash \% \$$. Therefore, the answer (arabic numerals) is 70 . $\mathcal{X}(\mathrm{GT}: 33)$</td>
</tr>
<tr>
<td style="text-align: center;">AQUA-RAT</td>
<td style="text-align: center;">Q: The capacity of a tank of dimensions $(8 \mathrm{~m} \times 6 \mathrm{~m} \times 2.5$ m) is Answer Choices: (A) 120 litres (B) 1200 litres (C) 12000 litres (D) 120000 litres (E) None of these</td>
<td style="text-align: center;">A: Let's think step by step. <br> The first thing we need to do is calculate the volume of the tank. The volume of the tank is 8 m x $6 \mathrm{~m} \times 2.5 \mathrm{~m}$. <br> $8 \mathrm{~m} \times 6 \mathrm{~m} \times 2.5 \mathrm{~m}=120 \mathrm{~m}^{\sim} 3$ <br> Now that we have the volume of the tank, we need to convert it to litres. <br> $1 \mathrm{~m}^{\sim} 3=1000$ litres <br> $120 \mathrm{~m}^{\sim} 3=120,000$ litres <br> The capacity of the tank is 120,000 litres. <br> Therefore, among A through E, the answer is D. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: In a row of children Neha is 12th from left end and Radha is 6th from right end. When Radha is shifted to left by 2 places and Neha is shifted to right by 2 places there 6 children between Radha and Neha. How many children are there in the row? Answer Choices: (A) 23 (B) 27 (C) 26 (D) 28 (E) 29</td>
<td style="text-align: center;">A: Let's think step by step. <br> There are 12 children to the left of Neha, so there must be at least 12 children in the row. <br> There are 6 children to the right of Radha, so there must be at least 18 children in the row. <br> There are 6 children between Radha and Neha, so there must be at least 24 children in the row. <br> The answer must be between 24 and 27, and the only answer that satisfies this is 26 . <br> Therefore, the answer is 26 . Therefore, among A through E, the answer is C. $\mathcal{X}(\mathrm{GT}: \mathrm{D})$</td>
</tr>
</tbody>
</table>
<p>Continued on next page</p>
<p>Table 12 - Continued from previous page</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">Q: There are 7 baskets of peaches. Each basket has 10 red peaches and 2 green peaches. How many green peaches are in the baskets altogether?</td>
<td style="text-align: center;">A: Let's think step by step. There are 7 baskets. Each basket has 10 red peaches and 2 green peaches. So, there are 2 green peaches in each basket. Now, we need to multiply because there are 7 baskets. 2 multiplied by 7 is 14 . So, there are 14 green peaches in the baskets altogether. <br> Therefore, the answer (arabic numerals) is $14 . \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: Mary is baking a cake. The recipe calls for 12 cups of flour 14 cups of sugar and 7 cups of salt. She already put in 2 cups of flour. How many more cups of flour than cups of salt does she need to add now?</td>
<td style="text-align: center;">A: Let's think step by step. Mary has already put in 2 cups of flour. The recipe calls for a total of 12 cups of flour. That means Mary needs to add 10 more cups of flour. The recipe calls for a total of 7 cups of salt. That means Mary needs to add 5 more cups of salt. So, Mary needs to add 10 more cups of flour than she does salt. Therefore, the answer (arabic numerals) is $10 . \mathcal{X}(\mathbf{G T}: 3)$</td>
</tr>
<tr>
<td style="text-align: center;">CommonsenseQA</td>
<td style="text-align: center;">Q: If there is a pond with trees around it, where is it likely located? Answer Choices: (A) ground (B) bathroom (C) forest (D) countryside (E) rural area</td>
<td style="text-align: center;">A: Let's think step by step. A pond is likely to be located near trees because trees need water to survive. Therefore, the most likely location for a pond with trees around it is in a forest. Therefore, among A through E, the answer is C. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Q: David watched some nesting birds using his binoculars while on vacation. Where might David be?. Answer Choices: (A) sky (B) vaccation (C) forest (D) countryside (E) roof</td>
<td style="text-align: center;">A: Let's think step by step. <br> First, we need to understand what the word "binoculars" means. Binoculars are two telescopes that are joined together and allow a person to see things that are far away. <br> Now, we need to think about where a person might use binoculars. A person might use binoculars to see things that are far away and in the sky, or to see things that are far away and on the ground. <br> So, the answer could be either (A) sky or (D) countryside. <br> Therefore, among A through E, the answer is (A) or (D). $\mathcal{X}(\mathbf{G T}: \mathbf{C})$</td>
</tr>
<tr>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Q: Would JPEG be a good format for saving an image of Da Vinci's Vitruvian Man?</td>
<td style="text-align: center;">A: Let's think step by step. The JPEG format is a lossy compression format, which means that some of the image data will be lost when the image is compressed. The Vitruvian Man is a very detailed image, with lots of small details. So, when we compress the image using JPEG, some of those small details will be lost, and the image will not be as sharp and clear as the original. So, in conclusion, JPEG is not the best format for saving an image of the Vitruvian Man. Therefore, the answer (Yes or No) is No. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Continued on next page</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Nye et al. [2022] also evaluates few-shot settings, but the few-shot performances on their domains are worse than the fine-tuning results.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6624 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6624</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6624</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-832fff14d2ed50eb7969c4c4b976c35776548f56</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/832fff14d2ed50eb7969c4c4b976c35776548f56" target="_blank">REALM: Retrieval-Augmented Language Model Pre-Training</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The effectiveness of Retrieval-Augmented Language Model pre-training (REALM) is demonstrated by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA) and is found to outperform all previous methods by a significant margin, while also providing qualitative benefits such as interpretability and modularity.</p>
                <p><strong>Paper Abstract:</strong> Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. 
To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. 
We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6624.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6624.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Language Model Pre-Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-model pre-training method that jointly trains a dense neural retriever and a knowledge-augmented encoder so the LM retrieves and conditions on textual documents (a large corpus such as Wikipedia) at pre-training, fine-tuning, and inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REALM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieve-then-predict language model: an input encoder produces a query embedding, a dense document encoder pre-computes embeddings for a large corpus, top-k documents are retrieved by maximum inner-product search (MIPS) and fed (joined) into a separate knowledge-augmented Transformer encoder which attends over the retrieved text to predict masked tokens or extract answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈330M parameters (configuration reported in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external vector store (dense retrieval / MIPS index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages (Wikipedia chunks) stored together with their pre-computed dense embeddings (document embeddings); there is also a special null document</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Similarity search via inner-product between input embedding and document embeddings (dense inner-product scoring) implemented with a sub-linear MIPS index; index refreshed asynchronously (documents are re-embedded and re-indexed periodically during pre-training); gradient flows through top-k retrieved candidates (retriever trained end-to-end)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain Question Answering (NaturalQuestions-Open, WebQuestions, CuratedTrec) and Masked Language Modeling (pre-training task)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / open-domain QA; masked language modeling (pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test set (Table 1): NaturalQuestions-Open (NQ) 39.2% Exact Match, WebQuestions (WQ) 40.2% Exact Match, CuratedTrec (CT) 46.8% Exact Match (setting: pretraining X=Wikipedia, Z=Wikipedia); alternate pretraining X=CC-News, Z=Wikipedia: NQ 40.4% EM, WQ 40.7% EM, CT 42.9% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>No direct ablation that entirely disables retrieval is reported; paper compares to large param-only generative model T5 (11B) which has no explicit retrieval: T5 (11B) achieves 34.5% EM on NQ (reported in Table 1) — used as a representative strong no-explicit-retrieval baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for Open-QA; MLM log-likelihood / masked-token probability for pretraining analyses</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Training complexity and infrastructure: must build and maintain a large MIPS index (≈13M chunk embeddings), asynchronous index refresh overhead (documents re-embedded every ~500 training steps in experiments); stale index harms training (ablation shows 30× staler MIPS degrades performance); need extra TPU/compute to re-embed corpus; inference requires retrieving top-k (paper uses k=5 at finetuning/inference) which adds retrieval latency and memory footprint for the index; however retrieval yields interpretability and modularity and allows corpus updates without re-training model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cold-start problem if retriever not warmed up (requires ICT warmstart); if pre-training corpus X equals knowledge corpus Z there are trivial exact-match retrievals that must be excluded; some knowledge remains baked into encoder so replacing/updating corpus does not always change predictions (paper gives example where model still predicts obsolete fact despite corpus update); retrieval utility varies and negative utility can occur (null document models cases where no retrieval is helpful); optimization can be unstable if index refreshes are too infrequent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. REALM: Retrieval-Augmented Language Model Pre-Training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6624.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ORQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-Retrieval Question Answering (ORQA / latent retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent variable open-domain QA system that uses a dense retriever (learned via the Inverse Cloze Task) and a reading model; retrieval treated as latent during training but in ORQA the index is fixed (not backpropagated into the index).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Latent retrieval for weakly supervised open domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ORQA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Dense-retriever + Transformer reader: a learned dense retriever (initialized via ICT) retrieves documents by dense similarity (MIPS) and a Transformer-based reader extracts answers; ORQA trains retrieval as a latent selection but does not backpropagate into the prebuilt MIPS index.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈330M parameters (as reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented external vector store (dense retrieval / MIPS index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Text passages (Wikipedia chunks) with pre-computed dense embeddings (document keys)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Maximum inner product search over pre-computed document embeddings (MIPS); index treated as fixed during fine-tuning in comparisons here (no asynchronous backprop into index used in ORQA baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain Question Answering (NaturalQuestions-Open, WebQuestions, CuratedTrec as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Test set (Table 1): NaturalQuestions-Open 33.3% EM, WebQuestions 36.4% EM, CuratedTrec 30.1% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>ORQA uses a fixed (non-updated) MIPS index which simplifies infrastructure but prevents end-to-end updates of document embeddings; REALM reports better performance by backpropagating into retriever and using pre-training strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower accuracy than REALM in these experiments; being based on a fixed index prevents the retriever from adapting during pre-training/fine-tuning (as discussed in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Kenton Lee, Sebastian Salant, Tom Kwiatkowski, Ankur Parikh, Daniel Das, Jonathan Berant. Latent retrieval for weakly supervised open domain question answering. (Lee et al., 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6624.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-Nearest Neighbor Language Model (kNN-LM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language model that augments prediction with a k-nearest-neighbor lookup into a datastore of cached past LM representations to improve memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization through memorization: Nearest neighbor language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>kNN-LM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Augments a parametric LM with a non-parametric datastore of past context–next-token examples; at prediction time performs k-NN lookup in representation space and interpolates retrieved next-token distributions with the LM's distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>non-parametric datastore / k-NN cache</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored LM examples: key = context representation (e.g., hidden states), value = next-token occurrences</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>k-nearest-neighbor search in representation space (similarity lookup) and interpolation of retrieved token distributions with LM output</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language modeling and memorization tasks (discussed in related work; not evaluated in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>language modeling / memorization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes kNN-LM improves memorization but was not fine-tuned for downstream tasks in cited work; such approaches rely on storing large datastores and nearest-neighbor lookup at inference time (memory and latency overhead).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Cited work did not fine-tune retrieval for downstream tasks; k-NN requires datastore of labeled examples to be effective for task-specific fine-tuning which can limit adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., & Lewis, M. Generalization through memorization: Nearest neighbor language models. (2019).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6624.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProductKeyMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large memory layers with product keys (Product Key Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory layer enabling sub-linear memory access by using product-key hashing to index large-scale key-value memories, permitting integration of scalable memory into neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large memory layers with product keys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Product-key large memory layer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A differentiable memory mechanism where a large number of memory slots are indexed via compact product keys to permit efficient sub-linear retrieval; keys are used to route queries to subsets of memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>scalable key-value memory layer with product-key indexing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Unnamed value vectors keyed by product-key indices (learned keys)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Product-key lookup that reduces candidate set for attention/reading (sub-linear access); retrieval is based on learned keys and routing</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General large-memory usage in neural networks (discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>scalable memory / representation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper mentions this approach as a way to obtain sub-linear memory access; tradeoffs include complexity of key design and integration compared to grounded document memories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Different from REALM in that memories are unnamed vectors rather than grounded documents (less interpretable); no experimental numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., & Jégou, H. Large memory layers with product keys. (2019).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6624.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KeyValueMemoryNets</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Key-Value Memory Networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-augmented neural architecture that stores (key, value) pairs and performs attention to read relevant values given a query, useful for question answering and retrieval-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Key-value memory networks for directly reading documents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Key-Value Memory Network</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Memory network storing document passages as (key, value) pairs where queries attend to keys and read out values; used for retrieval-style QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit key-value memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Keys: document/context representations; Values: corresponding content or representations to be read</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention over keys to retrieve values (soft or hard addressing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Document reading / QA (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / reading-comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Provides a way to store and read documents, but typically scales linearly unless combined with scalable indexing; REALM contrasts by grounding memories to documents and using MIPS for sub-linear retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scalability to millions of documents requires extra engineering (indexing) not inherent to vanilla key-value memory networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., & Weston, J. Key-value memory networks for directly reading documents. (2016).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6624.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrQA (sparse retrieval + Document Reader)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-domain QA system that uses sparse lexical retrieval (e.g., TF-IDF / BM25) to fetch candidate documents and a neural reader to extract answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reading wikipedia to answer open-domain questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DrQA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-stage retrieval pipeline: a sparse retriever (bag-of-words matching / BM25-style) fetches candidate documents from the corpus, followed by a neural Document Reader that extracts the answer span.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈34M parameters (reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit text corpus accessed via sparse retrieval (inverted index / bag-of-words)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages / documents (Wikipedia chunks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Sparse lexical matching (TF-IDF / BM25) to retrieve candidate passages which are then read by a neural reader</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (WebQuestions, CuratedTrec as reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1: WebQuestions 20.7% EM, CuratedTrec 25.7% EM (no NQ result listed in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sparse retrieval can be fast and memory-efficient but may miss semantically relevant documents (coverage limitations); typical pipelines also re-rank retrieved set which may be limited by initial sparse retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Coverage limited by lexical overlap; may perform worse on questions requiring semantic matching beyond bag-of-words.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes. Reading wikipedia to answer open-domain questions. (2017).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6624.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HardEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hard Expectation-Maximization (HardEM) for weakly supervised QA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A weakly-supervised open-domain QA approach that uses heuristic sparse retrieval followed by a Transformer reader trained with a discrete hard-EM objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A discrete hard em approach for weakly supervised question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HardEM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-based Open-QA system that combines sparse retrieval of candidate documents and a Transformer reader; training uses a hard-EM style optimization to select supporting documents under weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110M parameters (reporting standard Transformer size used in paper comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit text corpus accessed by sparse retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages / documents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Sparse lexical retrieval (bag-of-words) followed by learned reader; uses EM-style selection of latent supporting document</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NaturalQuestions-Open reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1: NaturalQuestions-Open 28.1% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Uses heuristic sparse retrieval and EM-style training; may be limited by initial retrieval coverage and complexity of latent selection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not as effective as learned dense retrieval approaches with better pre-training according to comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Min, S., Chen, D., Hajishirzi, H., & Zettlemoyer, L. A discrete hard em approach for weakly supervised question answering. (2019a).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6624.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRetriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRetriever (knowledge-guided text retrieval + reader)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based Open-QA approach that incorporates knowledge-guided retrieval signals (graph-based) combined with a Transformer reader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge guided text retrieval and reading for open domain question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphRetriever</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval module that leverages graph-based signals (e.g., entity/knowledge graph) to improve retrieval of supporting passages, followed by a Transformer reader for answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110M parameters (as reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit text corpus accessed via graph-enhanced retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text passages with graph/entity link information</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Graph-aware retrieval heuristics and re-ranking, then neural reader extracts answer</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NaturalQuestions-Open and WebQuestions as reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>retrieval / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1: NaturalQuestions-Open 31.8% EM, WebQuestions 31.6% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Graph-based retrieval can improve semantic coverage but adds complexity and dependencies on entity linking/graph construction; may require extra engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance lower than REALM in these comparisons; graph retrieval may be brittle where entity linking fails.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Min, S., Chen, D., Zettlemoyer, L., & Hajishirzi, H. Knowledge guided text retrieval and reading for open domain question answering. (2019b).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6624.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PathRetriever</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PathRetriever (retrieval of reasoning paths over Wikipedia graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-based method that learns to retrieve reasoning paths (multi-hop) across Wikipedia graph to support question answering, combined with a reader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to retrieve reasoning paths over wikipedia graph for question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PathRetriever</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns to retrieve multi-hop reasoning paths (sequence of documents/pages) over a Wikipedia graph as supporting evidence; retrieved textual paths are provided to a downstream reader.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110M parameters (as shown in Table 1 comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>explicit corpus memory with graph-structured retrieval (multi-hop path retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Text passages (Wikipedia) plus inter-page links forming paths</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Learned retrieval over graph paths (retrieval of sequences of documents), then reading/extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain QA (NaturalQuestions-Open reported in Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop retrieval / open-domain QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 1: NaturalQuestions-Open 32.6% EM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Retrieving multi-hop paths can improve reasoning ability but increases retrieval complexity and search space (engineering and latency costs).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Compared to REALM, lower performance on the reported benchmarks in this paper; complexity of path retrieval may be sensitive to graph coverage and link quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., & Xiong, C. Learning to retrieve reasoning paths over wikipedia graph for question answering. (2019).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6624.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6624.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryNetworks_family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory Networks / End-to-end Memory Networks / Neural Turing Machines (classical neural memory models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical memory-augmented neural architectures that provide soft attention over stored memory slots (Memory Networks / End-to-end Memory Networks) or differentiable read/write controllers over external memory (Neural Turing Machines).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory networks family (Memory Networks, End-to-end Memory Networks, Neural Turing Machines)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A family of neural architectures that augment networks with an explicit memory: Memory Networks provide attention-based reads over a memory of facts; End-to-end Memory Networks enable differentiable multi-hop attention; Neural Turing Machines add read/write controllers to an external differentiable memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>differentiable external memory (key-value / slot-based / read-write controllers)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory slots or key-value pairs (task-dependent contents)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Attention-based soft reads, multi-hop attention, or learned differentiable read/write operations (controller networks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General sequence / QA / reasoning tasks (cited as related ideas to REALM's memory view)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>memory-augmented reasoning / QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>These approaches provide differentiable memory access but can scale poorly to very large (millions) of items without specialized indexing; REALM contrasts by grounding memories to documents and using MIPS to achieve scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scalability to massive corpora is a primary limitation; memories often unnamed and less interpretable than grounded document memories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Referenced classics: Weston et al., 2014 (Memory networks); Sukhbaatar et al., 2015 (End-to-end memory networks); Graves et al., 2014 (Neural Turing Machines).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REALM: Retrieval-Augmented Language Model Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Latent retrieval for weakly supervised open domain question answering <em>(Rating: 2)</em></li>
                <li>Generalization through memorization: Nearest neighbor language models. <em>(Rating: 2)</em></li>
                <li>Large memory layers with product keys <em>(Rating: 2)</em></li>
                <li>Key-value memory networks for directly reading documents <em>(Rating: 2)</em></li>
                <li>Memory networks <em>(Rating: 2)</em></li>
                <li>End-to-end memory networks <em>(Rating: 1)</em></li>
                <li>Neural turing machines <em>(Rating: 1)</em></li>
                <li>Reading wikipedia to answer open-domain questions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6624",
    "paper_id": "paper-832fff14d2ed50eb7969c4c4b976c35776548f56",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "REALM",
            "name_full": "Retrieval-Augmented Language Model Pre-Training",
            "brief_description": "A language-model pre-training method that jointly trains a dense neural retriever and a knowledge-augmented encoder so the LM retrieves and conditions on textual documents (a large corpus such as Wikipedia) at pre-training, fine-tuning, and inference time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REALM",
            "agent_description": "A retrieve-then-predict language model: an input encoder produces a query embedding, a dense document encoder pre-computes embeddings for a large corpus, top-k documents are retrieved by maximum inner-product search (MIPS) and fed (joined) into a separate knowledge-augmented Transformer encoder which attends over the retrieved text to predict masked tokens or extract answers.",
            "model_size": "≈330M parameters (configuration reported in paper)",
            "memory_used": true,
            "memory_type": "retrieval-augmented external vector store (dense retrieval / MIPS index)",
            "memory_representation": "Raw text passages (Wikipedia chunks) stored together with their pre-computed dense embeddings (document embeddings); there is also a special null document",
            "memory_access_mechanism": "Similarity search via inner-product between input embedding and document embeddings (dense inner-product scoring) implemented with a sub-linear MIPS index; index refreshed asynchronously (documents are re-embedded and re-indexed periodically during pre-training); gradient flows through top-k retrieved candidates (retriever trained end-to-end)",
            "task_name": "Open-domain Question Answering (NaturalQuestions-Open, WebQuestions, CuratedTrec) and Masked Language Modeling (pre-training task)",
            "task_category": "retrieval / open-domain QA; masked language modeling (pretraining)",
            "performance_with_memory": "Test set (Table 1): NaturalQuestions-Open (NQ) 39.2% Exact Match, WebQuestions (WQ) 40.2% Exact Match, CuratedTrec (CT) 46.8% Exact Match (setting: pretraining X=Wikipedia, Z=Wikipedia); alternate pretraining X=CC-News, Z=Wikipedia: NQ 40.4% EM, WQ 40.7% EM, CT 42.9% EM.",
            "performance_without_memory": "No direct ablation that entirely disables retrieval is reported; paper compares to large param-only generative model T5 (11B) which has no explicit retrieval: T5 (11B) achieves 34.5% EM on NQ (reported in Table 1) — used as a representative strong no-explicit-retrieval baseline.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for Open-QA; MLM log-likelihood / masked-token probability for pretraining analyses",
            "tradeoffs_reported": "Training complexity and infrastructure: must build and maintain a large MIPS index (≈13M chunk embeddings), asynchronous index refresh overhead (documents re-embedded every ~500 training steps in experiments); stale index harms training (ablation shows 30× staler MIPS degrades performance); need extra TPU/compute to re-embed corpus; inference requires retrieving top-k (paper uses k=5 at finetuning/inference) which adds retrieval latency and memory footprint for the index; however retrieval yields interpretability and modularity and allows corpus updates without re-training model weights.",
            "limitations_or_failure_cases": "Cold-start problem if retriever not warmed up (requires ICT warmstart); if pre-training corpus X equals knowledge corpus Z there are trivial exact-match retrievals that must be excluded; some knowledge remains baked into encoder so replacing/updating corpus does not always change predictions (paper gives example where model still predicts obsolete fact despite corpus update); retrieval utility varies and negative utility can occur (null document models cases where no retrieval is helpful); optimization can be unstable if index refreshes are too infrequent.",
            "citation": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. REALM: Retrieval-Augmented Language Model Pre-Training.",
            "uuid": "e6624.0",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ORQA",
            "name_full": "Open-Retrieval Question Answering (ORQA / latent retrieval)",
            "brief_description": "A latent variable open-domain QA system that uses a dense retriever (learned via the Inverse Cloze Task) and a reading model; retrieval treated as latent during training but in ORQA the index is fixed (not backpropagated into the index).",
            "citation_title": "Latent retrieval for weakly supervised open domain question answering",
            "mention_or_use": "use",
            "agent_name": "ORQA",
            "agent_description": "Dense-retriever + Transformer reader: a learned dense retriever (initialized via ICT) retrieves documents by dense similarity (MIPS) and a Transformer-based reader extracts answers; ORQA trains retrieval as a latent selection but does not backpropagate into the prebuilt MIPS index.",
            "model_size": "≈330M parameters (as reported in Table 1)",
            "memory_used": true,
            "memory_type": "retrieval-augmented external vector store (dense retrieval / MIPS index)",
            "memory_representation": "Text passages (Wikipedia chunks) with pre-computed dense embeddings (document keys)",
            "memory_access_mechanism": "Maximum inner product search over pre-computed document embeddings (MIPS); index treated as fixed during fine-tuning in comparisons here (no asynchronous backprop into index used in ORQA baseline in this paper)",
            "task_name": "Open-domain Question Answering (NaturalQuestions-Open, WebQuestions, CuratedTrec as evaluated in this paper)",
            "task_category": "retrieval / open-domain QA",
            "performance_with_memory": "Test set (Table 1): NaturalQuestions-Open 33.3% EM, WebQuestions 36.4% EM, CuratedTrec 30.1% EM.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "ORQA uses a fixed (non-updated) MIPS index which simplifies infrastructure but prevents end-to-end updates of document embeddings; REALM reports better performance by backpropagating into retriever and using pre-training strategies.",
            "limitations_or_failure_cases": "Lower accuracy than REALM in these experiments; being based on a fixed index prevents the retriever from adapting during pre-training/fine-tuning (as discussed in paper).",
            "citation": "Kenton Lee, Sebastian Salant, Tom Kwiatkowski, Ankur Parikh, Daniel Das, Jonathan Berant. Latent retrieval for weakly supervised open domain question answering. (Lee et al., 2019).",
            "uuid": "e6624.1",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "kNN-LM",
            "name_full": "k-Nearest Neighbor Language Model (kNN-LM)",
            "brief_description": "A language model that augments prediction with a k-nearest-neighbor lookup into a datastore of cached past LM representations to improve memorization.",
            "citation_title": "Generalization through memorization: Nearest neighbor language models.",
            "mention_or_use": "mention",
            "agent_name": "kNN-LM",
            "agent_description": "Augments a parametric LM with a non-parametric datastore of past context–next-token examples; at prediction time performs k-NN lookup in representation space and interpolates retrieved next-token distributions with the LM's distribution.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "non-parametric datastore / k-NN cache",
            "memory_representation": "Stored LM examples: key = context representation (e.g., hidden states), value = next-token occurrences",
            "memory_access_mechanism": "k-nearest-neighbor search in representation space (similarity lookup) and interpolation of retrieved token distributions with LM output",
            "task_name": "Language modeling and memorization tasks (discussed in related work; not evaluated in this paper's experiments)",
            "task_category": "language modeling / memorization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes kNN-LM improves memorization but was not fine-tuned for downstream tasks in cited work; such approaches rely on storing large datastores and nearest-neighbor lookup at inference time (memory and latency overhead).",
            "limitations_or_failure_cases": "Cited work did not fine-tune retrieval for downstream tasks; k-NN requires datastore of labeled examples to be effective for task-specific fine-tuning which can limit adaptability.",
            "citation": "Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., & Lewis, M. Generalization through memorization: Nearest neighbor language models. (2019).",
            "uuid": "e6624.2",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "ProductKeyMemory",
            "name_full": "Large memory layers with product keys (Product Key Memory)",
            "brief_description": "A memory layer enabling sub-linear memory access by using product-key hashing to index large-scale key-value memories, permitting integration of scalable memory into neural networks.",
            "citation_title": "Large memory layers with product keys",
            "mention_or_use": "mention",
            "agent_name": "Product-key large memory layer",
            "agent_description": "A differentiable memory mechanism where a large number of memory slots are indexed via compact product keys to permit efficient sub-linear retrieval; keys are used to route queries to subsets of memory.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "scalable key-value memory layer with product-key indexing",
            "memory_representation": "Unnamed value vectors keyed by product-key indices (learned keys)",
            "memory_access_mechanism": "Product-key lookup that reduces candidate set for attention/reading (sub-linear access); retrieval is based on learned keys and routing",
            "task_name": "General large-memory usage in neural networks (discussed in related work)",
            "task_category": "scalable memory / representation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper mentions this approach as a way to obtain sub-linear memory access; tradeoffs include complexity of key design and integration compared to grounded document memories.",
            "limitations_or_failure_cases": "Different from REALM in that memories are unnamed vectors rather than grounded documents (less interpretable); no experimental numbers provided in this paper.",
            "citation": "Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., & Jégou, H. Large memory layers with product keys. (2019).",
            "uuid": "e6624.3",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "KeyValueMemoryNets",
            "name_full": "Key-Value Memory Networks",
            "brief_description": "A memory-augmented neural architecture that stores (key, value) pairs and performs attention to read relevant values given a query, useful for question answering and retrieval-style tasks.",
            "citation_title": "Key-value memory networks for directly reading documents",
            "mention_or_use": "mention",
            "agent_name": "Key-Value Memory Network",
            "agent_description": "Memory network storing document passages as (key, value) pairs where queries attend to keys and read out values; used for retrieval-style QA tasks.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "explicit key-value memory",
            "memory_representation": "Keys: document/context representations; Values: corresponding content or representations to be read",
            "memory_access_mechanism": "Attention over keys to retrieve values (soft or hard addressing)",
            "task_name": "Document reading / QA (mentioned in related work)",
            "task_category": "retrieval / reading-comprehension",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Provides a way to store and read documents, but typically scales linearly unless combined with scalable indexing; REALM contrasts by grounding memories to documents and using MIPS for sub-linear retrieval.",
            "limitations_or_failure_cases": "Scalability to millions of documents requires extra engineering (indexing) not inherent to vanilla key-value memory networks.",
            "citation": "Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., & Weston, J. Key-value memory networks for directly reading documents. (2016).",
            "uuid": "e6624.4",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "DrQA",
            "name_full": "DrQA (sparse retrieval + Document Reader)",
            "brief_description": "An open-domain QA system that uses sparse lexical retrieval (e.g., TF-IDF / BM25) to fetch candidate documents and a neural reader to extract answers.",
            "citation_title": "Reading wikipedia to answer open-domain questions",
            "mention_or_use": "use",
            "agent_name": "DrQA",
            "agent_description": "Two-stage retrieval pipeline: a sparse retriever (bag-of-words matching / BM25-style) fetches candidate documents from the corpus, followed by a neural Document Reader that extracts the answer span.",
            "model_size": "≈34M parameters (reported in Table 1)",
            "memory_used": true,
            "memory_type": "explicit text corpus accessed via sparse retrieval (inverted index / bag-of-words)",
            "memory_representation": "Raw text passages / documents (Wikipedia chunks)",
            "memory_access_mechanism": "Sparse lexical matching (TF-IDF / BM25) to retrieve candidate passages which are then read by a neural reader",
            "task_name": "Open-domain QA (WebQuestions, CuratedTrec as reported in Table 1)",
            "task_category": "retrieval / open-domain QA",
            "performance_with_memory": "Table 1: WebQuestions 20.7% EM, CuratedTrec 25.7% EM (no NQ result listed in Table 1).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Sparse retrieval can be fast and memory-efficient but may miss semantically relevant documents (coverage limitations); typical pipelines also re-rank retrieved set which may be limited by initial sparse retrieval.",
            "limitations_or_failure_cases": "Coverage limited by lexical overlap; may perform worse on questions requiring semantic matching beyond bag-of-words.",
            "citation": "Danqi Chen, Adam Fisch, Jason Weston, Antoine Bordes. Reading wikipedia to answer open-domain questions. (2017).",
            "uuid": "e6624.5",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "HardEM",
            "name_full": "Hard Expectation-Maximization (HardEM) for weakly supervised QA",
            "brief_description": "A weakly-supervised open-domain QA approach that uses heuristic sparse retrieval followed by a Transformer reader trained with a discrete hard-EM objective.",
            "citation_title": "A discrete hard em approach for weakly supervised question answering",
            "mention_or_use": "use",
            "agent_name": "HardEM",
            "agent_description": "Retrieval-based Open-QA system that combines sparse retrieval of candidate documents and a Transformer reader; training uses a hard-EM style optimization to select supporting documents under weak supervision.",
            "model_size": "≈110M parameters (reporting standard Transformer size used in paper comparisons)",
            "memory_used": true,
            "memory_type": "explicit text corpus accessed by sparse retrieval",
            "memory_representation": "Raw text passages / documents",
            "memory_access_mechanism": "Sparse lexical retrieval (bag-of-words) followed by learned reader; uses EM-style selection of latent supporting document",
            "task_name": "Open-domain QA (NaturalQuestions-Open reported in Table 1)",
            "task_category": "retrieval / open-domain QA",
            "performance_with_memory": "Table 1: NaturalQuestions-Open 28.1% EM.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Uses heuristic sparse retrieval and EM-style training; may be limited by initial retrieval coverage and complexity of latent selection.",
            "limitations_or_failure_cases": "Not as effective as learned dense retrieval approaches with better pre-training according to comparisons in this paper.",
            "citation": "Min, S., Chen, D., Hajishirzi, H., & Zettlemoyer, L. A discrete hard em approach for weakly supervised question answering. (2019a).",
            "uuid": "e6624.6",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "GraphRetriever",
            "name_full": "GraphRetriever (knowledge-guided text retrieval + reader)",
            "brief_description": "A retrieval-based Open-QA approach that incorporates knowledge-guided retrieval signals (graph-based) combined with a Transformer reader.",
            "citation_title": "Knowledge guided text retrieval and reading for open domain question answering",
            "mention_or_use": "use",
            "agent_name": "GraphRetriever",
            "agent_description": "Retrieval module that leverages graph-based signals (e.g., entity/knowledge graph) to improve retrieval of supporting passages, followed by a Transformer reader for answer extraction.",
            "model_size": "≈110M parameters (as reported in Table 1)",
            "memory_used": true,
            "memory_type": "explicit text corpus accessed via graph-enhanced retrieval",
            "memory_representation": "Raw text passages with graph/entity link information",
            "memory_access_mechanism": "Graph-aware retrieval heuristics and re-ranking, then neural reader extracts answer",
            "task_name": "Open-domain QA (NaturalQuestions-Open and WebQuestions as reported in Table 1)",
            "task_category": "retrieval / open-domain QA",
            "performance_with_memory": "Table 1: NaturalQuestions-Open 31.8% EM, WebQuestions 31.6% EM.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Graph-based retrieval can improve semantic coverage but adds complexity and dependencies on entity linking/graph construction; may require extra engineering.",
            "limitations_or_failure_cases": "Performance lower than REALM in these comparisons; graph retrieval may be brittle where entity linking fails.",
            "citation": "Min, S., Chen, D., Zettlemoyer, L., & Hajishirzi, H. Knowledge guided text retrieval and reading for open domain question answering. (2019b).",
            "uuid": "e6624.7",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "PathRetriever",
            "name_full": "PathRetriever (retrieval of reasoning paths over Wikipedia graph)",
            "brief_description": "A retrieval-based method that learns to retrieve reasoning paths (multi-hop) across Wikipedia graph to support question answering, combined with a reader.",
            "citation_title": "Learning to retrieve reasoning paths over wikipedia graph for question answering",
            "mention_or_use": "use",
            "agent_name": "PathRetriever",
            "agent_description": "Learns to retrieve multi-hop reasoning paths (sequence of documents/pages) over a Wikipedia graph as supporting evidence; retrieved textual paths are provided to a downstream reader.",
            "model_size": "≈110M parameters (as shown in Table 1 comparisons)",
            "memory_used": true,
            "memory_type": "explicit corpus memory with graph-structured retrieval (multi-hop path retrieval)",
            "memory_representation": "Text passages (Wikipedia) plus inter-page links forming paths",
            "memory_access_mechanism": "Learned retrieval over graph paths (retrieval of sequences of documents), then reading/extraction",
            "task_name": "Open-domain QA (NaturalQuestions-Open reported in Table 1)",
            "task_category": "multi-hop retrieval / open-domain QA",
            "performance_with_memory": "Table 1: NaturalQuestions-Open 32.6% EM.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Retrieving multi-hop paths can improve reasoning ability but increases retrieval complexity and search space (engineering and latency costs).",
            "limitations_or_failure_cases": "Compared to REALM, lower performance on the reported benchmarks in this paper; complexity of path retrieval may be sensitive to graph coverage and link quality.",
            "citation": "Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., & Xiong, C. Learning to retrieve reasoning paths over wikipedia graph for question answering. (2019).",
            "uuid": "e6624.8",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "MemoryNetworks_family",
            "name_full": "Memory Networks / End-to-end Memory Networks / Neural Turing Machines (classical neural memory models)",
            "brief_description": "Classical memory-augmented neural architectures that provide soft attention over stored memory slots (Memory Networks / End-to-end Memory Networks) or differentiable read/write controllers over external memory (Neural Turing Machines).",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Memory networks family (Memory Networks, End-to-end Memory Networks, Neural Turing Machines)",
            "agent_description": "A family of neural architectures that augment networks with an explicit memory: Memory Networks provide attention-based reads over a memory of facts; End-to-end Memory Networks enable differentiable multi-hop attention; Neural Turing Machines add read/write controllers to an external differentiable memory.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "differentiable external memory (key-value / slot-based / read-write controllers)",
            "memory_representation": "Memory slots or key-value pairs (task-dependent contents)",
            "memory_access_mechanism": "Attention-based soft reads, multi-hop attention, or learned differentiable read/write operations (controller networks)",
            "task_name": "General sequence / QA / reasoning tasks (cited as related ideas to REALM's memory view)",
            "task_category": "memory-augmented reasoning / QA",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "These approaches provide differentiable memory access but can scale poorly to very large (millions) of items without specialized indexing; REALM contrasts by grounding memories to documents and using MIPS to achieve scalability.",
            "limitations_or_failure_cases": "Scalability to massive corpora is a primary limitation; memories often unnamed and less interpretable than grounded document memories.",
            "citation": "Referenced classics: Weston et al., 2014 (Memory networks); Sukhbaatar et al., 2015 (End-to-end memory networks); Graves et al., 2014 (Neural Turing Machines).",
            "uuid": "e6624.9",
            "source_info": {
                "paper_title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Latent retrieval for weakly supervised open domain question answering",
            "rating": 2
        },
        {
            "paper_title": "Generalization through memorization: Nearest neighbor language models.",
            "rating": 2
        },
        {
            "paper_title": "Large memory layers with product keys",
            "rating": 2
        },
        {
            "paper_title": "Key-value memory networks for directly reading documents",
            "rating": 2
        },
        {
            "paper_title": "Memory networks",
            "rating": 2
        },
        {
            "paper_title": "End-to-end memory networks",
            "rating": 1
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 1
        },
        {
            "paper_title": "Reading wikipedia to answer open-domain questions",
            "rating": 2
        }
    ],
    "cost": 0.02245175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REALM: Retrieval-Augmented Language Model Pre-Training</h1>
<p>Kelvin Guu<em> ${ }^{1}$ Kenton Lee ${ }^{</em> 1}$ Zora Tung ${ }^{1}$ Panupong Pasupat ${ }^{1}$ Ming-Wei Chang ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pretrain such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-theart models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.</p>
<h2>1. Introduction</h2>
<p>Recent advances in language model pre-training have shown that models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2019) store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Petroni et al., 2019). For example, BERT is able to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. REALM augments language model pre-training with a neural knowledge retriever that retrieves knowledge from a textual knowledge corpus, $\mathcal{Z}$ (e.g., all of Wikipedia). Signal from the language modeling objective backpropagates all the way through the retriever, which must consider millions of documents in $\mathcal{Z}$-a significant computational challenge that we address.
correctly predict the missing word in the following sentence: "The is the currency of the United Kingdom" (answer: "pound").</p>
<p>In these language models, the learned world knowledge is stored implicitly in the parameters of the underlying neural network. This makes it difficult to determine what knowledge is stored in the network and where. Furthermore, storage space is limited by the size of the network-to capture more world knowledge, one must train ever-larger networks, which can be prohibitively slow or expensive.</p>
<p>To capture knowledge in a more interpretable and modular way, we propose a novel framework, Retrieval-Augmented Language Model (REALM) pre-training, which augments language model pre-training algorithms with a learned textual knowledge retriever. In contrast to models that store knowledge in their parameters, this approach explicitly exposes the role of world knowledge by asking the model to</p>
<p>decide what knowledge to retrieve and use during inference. Before making each prediction, the language model uses the retriever to retrieve documents ${ }^{1}$ from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction. Learning this model end-toend requires backpropagating through a retrieval step that considers an entire corpus of textual knowledge, as shown in Figure 1.</p>
<p>The key intuition of REALM is to train the retriever using a performance-based signal from unsupervised text: a retrieval that improves the language model's perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized. For example, in Figure 1, if the model needs to fill the blank in "the __ at the top of the pyramid", the retriever should be rewarded for selecting a document containing "The pyramidion on top allows for less material higher up the pyramid". We achieve this behavior by modeling our retrieve-then-predict approach as a latent variable language model and optimizing the marginal likelihood.</p>
<p>Incorporating a large-scale neural retrieval module during pre-training constitutes a significant computational challenge, since the retriever must consider millions of candidate documents for each pre-training step, and we must backpropagate through its decisions. To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS).</p>
<p>Numerous prior works have demonstrated the benefit of adding a discrete retrieval step to neural networks (Miller et al., 2016; Chen et al., 2017), but did not apply the framework to language model pre-training and employed non-learned retrievers to handle large-scale document collections. In the language modeling literature, the $k$-Nearest Neighbor Language Model (Khandelwal et al., 2019) ( $k \mathrm{NN}-\mathrm{LM}$ ) retrieves similar LM examples to improve memorization. However, $k \mathrm{NN}-\mathrm{LM}$ was not finetuned for downstream tasks, perhaps because it is unclear how to adapt the retrieval mechanism: a $k \mathrm{NN}$ can only use examples labeled for the target task-during fine-tuning, this precludes LM examples, which contain the desired world knowledge. In contrast, REALM's retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.</p>
<p>We evaluate our approach by fine-tuning the models pre-trained with REALM on the task of Opendomain Question Answering (Open-QA), one of the most knowledge-intensive tasks in natural language processing. We evaluate on three popular Open-QA benchmarks (NaturalQuestions-Open, WebQuestions, and</p>
<p>CuratedTrec) and compare to state-of-the-art Open-QA models, including both extremely large models that store knowledge implicitly (such as T5) as well as previous approaches that also use a knowledge retriever to access external knowledge, but implement retrieval in a more heuristic fashion (Lee et al., 2019; Min et al., 2019a; Asai et al., 2019). REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16\% absolute accuracy. We also demonstrate qualitative benefits of REALM, including interpretability and modularity.</p>
<h2>2. Background</h2>
<p>Language model pre-training The goal of language model pre-training is to learn useful representations of language, usually from unlabeled text corpora. The resulting pre-trained model can then be further trained (fine-tuned) for a downstream task of primary interest (in our case, Open-QA), often leading to better generalization than training from scratch (Dai \&amp; Le, 2015; Radford et al., 2019).</p>
<p>We focus on the masked language model ${ }^{2}$ (MLM) variant of pre-training popularized by BERT (Devlin et al., 2018). In its basic form, an MLM is trained to predict the missing tokens in an input text passage. Given an unlabeled pre-training corpus $\mathcal{X}$ (e.g., Wikipedia text), a training example $(x, y)$ can be generated by randomly masking tokens in a sampled piece of text (e.g., $x=$ "The [MASK] is the currency [MASK] the UK"; $y=$ ("pound", "of")). The model uses its representation of the masked input $x$ to predict the token that should go in each mask. A good MLM must learn to encode syntactic and semantic information (e.g., to predict "of") as well as some world knowledge (e.g., to predict "pound").</p>
<p>Open-domain question answering (Open-QA) To measure a model's ability to incorporate world knowledge, we need a downstream task where world knowledge is critical. Perhaps one of the most knowledge-intensive tasks in natural language processing is open-domain question answering (Open-QA): given a question $x$ such as "What is the currency of the UK?", a model must output the correct answer string $y$, "pound". The "open" part of OpenQA refers to the fact that the model does not receive a preidentified document that is known to contain the answer, unlike traditional reading comprehension (RC) tasks such as SQuAD (Rajpurkar et al., 2016; 2018). While RC mod-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>els comprehend a single document, Open-QA models must retain knowledge from millions of documents, since a question could be about any of them.</p>
<p>We focus on Open-QA systems that utilize a textual knowledge corpus $\mathcal{Z}$ as the knowledge source. Many of these systems employ a retrieval-based approach: given a question $x$, retrieve potentially relevant documents $z$ from the corpus $\mathcal{Z}$, and then extract an answer $y$ from the documents (Brill et al., 2002; Chen et al., 2017; Lee et al., 2019). Our approach, REALM, is inspired by this paradigm and extends it to language model pre-training. Alternatively, some recent work has proposed generationbased systems that apply a sequence-to-sequence model on $x$ to directly generate $y$ token-by-token (Lewis et al., 2019; Raffel et al., 2019). We will compare against state-of-theart systems from both paradigms in our experiments.</p>
<h2>3. Approach</h2>
<p>We start by formalizing REALM's pre-training and finetuning tasks as a retrieve-then-predict generative process in Section 3.1. Then in Section 3.2, we describe the model architectures for each component of that process. In Section 3.3, we show how to implement REALM pre-training and fine-tuning by maximizing the likelihood of REALM's generative process. En route, we address important computational challenges, explain why training works, and also discuss strategies for injecting useful inductive biases. The overall framework is illustrated in Figure 2.</p>
<h3>3.1. REALM's generative process</h3>
<p>For both pre-training and fine-tuning, REALM takes some input $x$ and learns a distribution $p(y \mid x)$ over possible outputs $y$. For pre-training, the task is masked language modeling: $x$ is a sentence from a pre-training corpus $\mathcal{X}$ with some tokens masked out, and the model must predict the value of those missing tokens, $y$. For fine-tuning, the task is Open-QA: $x$ is a question, and $y$ is the answer.</p>
<p>REALM decomposes $p(y \mid x)$ into two steps: retrieve, then predict. Given an input $x$, we first retrieve possibly helpful documents $z$ from a knowledge corpus $\mathcal{Z}$. We model this as a sample from the distribution $p(z \mid x)$. Then, we condition on both the retrieved $z$ and the original input $x$ to generate the output $y$-modeled as $p(y \mid z, x)$. To obtain the overall likelihood of generating $y$, we treat $z$ as a latent variable and marginalize over all possible documents $z$, yielding</p>
<p>$$
p(y \mid x)=\sum_{z \in \mathcal{Z}} p(y \mid z, x) p(z \mid x)
$$</p>
<h3>3.2. Model architecture</h3>
<p>We now describe the two key components: the neural knowledge retriever, which models $p(z \mid x)$, and the knowledge-augmented encoder, which models $p(y \mid z, x)$.</p>
<p>Knowledge Retriever The retriever is defined using a dense inner product model:</p>
<p>$$
\begin{aligned}
&amp; p(z \mid x)=\frac{\exp f(x, z)}{\sum_{z^{\prime}} \exp f\left(x, z^{\prime}\right)} \
&amp; f(x, z)=\operatorname{Embed}<em _doc="{doc" _text="\text">{\text {input }}(x)^{\top} \operatorname{Embed}</em>(z)
\end{aligned}
$$}</p>
<p>where Embed $<em _doc="{doc" _text="\text">{\text {input }}$ and Embed $</em>$ are embedding functions that map $x$ and $z$ respectively to $d$-dimensional vectors. The relevance score $f(x, z)$ between $x$ and $z$ is defined as the inner product of the vector embeddings. The retrieval distribution is the softmax over all relevance scores.}</p>
<p>We implement the embedding functions using BERT-style Transformers (Devlin et al., 2018). Following standard practices, we join spans of text by applying wordpiece tokenization, separating them with [SEP] tokens, prefixing a [CLS] token, and appending a final [SEP] token.</p>
<p>$$
\begin{aligned}
\operatorname{join}<em _BERT="{BERT" _text="\text">{\text {BERT }}(x) &amp; =[\mathrm{CLS}] x[\mathrm{SEP}] \
\operatorname{join}</em>]
\end{aligned}
$$}}\left(x_{1}, x_{2}\right) &amp; =[\mathrm{CLS}] x_{1}[\mathrm{SEP}] x_{2}[\mathrm{SEP</p>
<p>As in Devlin et al. (2018), we pass this into a Transformer, which produces one vector for each token, including the vector corresponding to [CLS] which is used as a "pooled" representation of the sequence (denoted BERT $_{\text {CLS }}$ ). Finally, we perform a linear projection to reduce the dimensionality of the vector, denoted as a projection matrix $\mathbf{W}$ :</p>
<p>$$
\begin{aligned}
\operatorname{Embed}<em _input="{input" _text="\text">{\text {input }}(x) &amp; =\mathbf{W}</em>}} \operatorname{BERT<em _BERT="{BERT" _text="\text">{\mathrm{CLS}}\left(\text { join }</em>(x)\right) \
\operatorname{Embed}}<em _doc="{doc" _text="\text">{\text {doc }}(z) &amp; =\mathbf{W}</em>}} \operatorname{BERT<em _BERT="{BERT" _text="\text">{\mathrm{CLS}}\left(\text { join }</em>\right)\right)
\end{aligned}
$$}}\left(z_{\text {title }}, z_{\text {body }</p>
<p>where $z_{\text {title }}$ is the document's title and $z_{\text {body }}$ is its body. We let $\theta$ denote all parameters associated with the retriever, which include the Transformer and projection matrices.</p>
<p>Knowledge-Augmented Encoder Given an input $x$ and a retrieved document $z$, the knowledge-augmented encoder defines $p(y \mid z, x)$. We join $x$ and $z$ into a single sequence that we feed into a Transformer (distinct from the one used in the retriever). This allows us to perform rich crossattention between $x$ and $z$ before predicting $y$. See Figure 1 for a concrete example.</p>
<p>At this stage, the architectures for pre-training and finetuning differ slightly. For the masked language model pretraining task, we must predict the original value of each [MASK] token in $x$. To do so, we use the same masked</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. The overall framework of REALM. Left: Unsupervised pre-training. The knowledge retriever and knowledge-augmented encoder are jointly pre-trained on the unsupervised language modeling task. Right: Supervised fine-tuning. After the parameters of the retriever $(\theta)$ and encoder $(\phi)$ have been pre-trained, they are then fine-tuned on a task of primary interest, using supervised examples.
language modeling (MLM) loss as in Devlin et al. (2018):</p>
<p>$$
\begin{aligned}
&amp; p(y \mid z, x)=\prod_{j=1}^{J_{x}} p\left(y_{j} \mid z, x\right) \
&amp; p\left(y_{j} \mid z, x\right) \propto \exp \left(w_{j}^{\top} \operatorname{BERT}<em _operatorname_BERT="\operatorname{BERT">{\operatorname{MASK}(j)}\left(\operatorname{join}</em>\right)\right)\right)
\end{aligned}
$$}}\left(x, z_{\text {body }</p>
<p>where $\operatorname{BERT}<em x="x">{\operatorname{MASK}(j)}$ denotes the Transformer output vector corresponding to the $j^{\text {th }}$ masked token, $J</em>$.}$ is the total number of [MASK] tokens in $x$, and $w_{j}$ is a learned word embedding for token $y_{j</p>
<p>For Open-QA fine-tuning, we wish to produce the answer string $y$. Following previous reading comprehension work (Rajpurkar et al., 2016; Seo et al., 2016; Lee et al., 2016; Clark \&amp; Gardner, 2017), we will assume that the answer $y$ can be found as a contiguous sequence of tokens in some document $z$. Let $S(z, y)$ be the set of spans matching $y$ in $z$. Then we can define $p(y \mid z, x)$ as:</p>
<p>$$
\begin{aligned}
p(y \mid z, x) &amp; \propto \sum_{s \in S(z, y)} \exp \left(\operatorname{MLP}\left(\left\lceil h_{\operatorname{START}(s)} ; h_{\operatorname{END}(s)}\right\rceil\right)\right) \
h_{\operatorname{START}(s)} &amp; =\operatorname{BERT}<em _operatorname_BERT="\operatorname{BERT">{\operatorname{START}(s)}\left(\operatorname{join}</em>\right)\right) \
h_{\operatorname{END}(s)} &amp; =\operatorname{BERT}}}\left(x, z_{\text {body }<em _operatorname_BERT="\operatorname{BERT">{\operatorname{END}(s)}\left(\operatorname{join}</em>\right)\right)
\end{aligned}
$$}}\left(x, z_{\text {body }</p>
<p>where $\operatorname{BERT}<em _operatorname_END="\operatorname{END">{\operatorname{START}(s)}$ and $\operatorname{BERT}</em>$ denote the Transformer output vectors corresponding to the start and end tokens of span $s$, respectively, while MLP denotes a feed-forward neural network. We will let $\phi$ denote all parameters associated with the knowledge-augmented encoder.}(s)</p>
<h3>3.3. Training</h3>
<p>For both pre-training and fine-tuning, we train by maximizing the log-likelihood $\log p(y \mid x)$ of the correct output $y$. Since both the knowledge retriever and knowledgeaugmented encoder are differentiable neural networks, we can compute the gradient of $\log p(y \mid x)$ (defined in Equation 1) with respect to the model parameters $\theta$ and $\phi$, and optimize using stochastic gradient descent.</p>
<p>The key computational challenge is that the marginal probability $p(y \mid x)=\sum_{z \in \mathcal{Z}} p(y \mid x, z) p(z \mid x)$ involves a summation over all documents $z$ in the knowledge corpus $\mathcal{Z}$. We approximate this by instead summing over the top $k$ documents with highest probability under $p(z \mid x)$-this is reasonable if most documents have near zero probability.</p>
<p>Even with this approximation, we still need an efficient way to find the top $k$ documents. Note that the ordering of documents under $p(z \mid x)$ is the same as under the relevance score $f(x, z)=\operatorname{Embed}<em _doc="{doc" _text="\text">{\text {input }}(x)^{\top} \operatorname{Embed}</em>(z)$, which is an inner product. Thus, we can employ Maximum Inner Product Search (MIPS) algorithms to find the approximate top $k$ documents, using running time and storage space that scale sub-linearly with the number of documents (Ram \&amp; Gray, 2012; Shrivastava \&amp; Li, 2014; Shen et al., 2015).}</p>
<p>To employ MIPS, we must pre-compute $\operatorname{Embed}<em _doc="{doc" _text="\text">{\text {doc }}(z)$ for every $z \in \mathcal{Z}$ and construct an efficient search index over these embeddings. However, this data structure will no longer be consistent with $p(z \mid x)$ if the parameters $\theta$ of $\operatorname{Embed}</em>$ are later updated. Hence, the search index goes "stale" after every gradient update on $\theta$.}</p>
<p>Our solution is to "refresh" the index by asynchronously re-embedding and re-indexing all documents every several hundred training steps. The MIPS index is slightly stale between refreshes, but note that it is only used to select the top $k$ documents. We recompute $p(z \mid x)$ and its gradient, using the fresh $\theta$, for these top $k$ documents after retrieving them. In Section 4.5, we empirically demonstrate that this procedure results in stable optimization, provided that refreshes happen at a sufficiently frequent rate.</p>
<p>Implementing asynchronous MIPS refreshes We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents. As shown</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. REALM pre-training with asynchronous MIPS refreshes.
below, the trainer sends the index builder a snapshot of its parameters, $\theta^{\prime}$. The trainer then continues to train while the index builder uses $\theta^{\prime}$ to construct a new index in the background. As soon as the index builder is done, it sends the new index back to the trainer, and the process repeats.</p>
<p>While asynchronous refreshes can be used for both pretraining and fine-tuning, in our experiments we only use it for pre-training. For fine-tuning, we just build the MIPS index once (using the pre-trained $\theta$ ) for simplicity and do not update Embed $<em _input="{input" _text="\text">{\text {doc }}{ }^{3}$ Note that we still fine-tune Embed $</em>$, so the retrieval function is still updated from the query side.}</p>
<p>What does the retriever learn? Since the knowledge retrieval of REALM is latent, it is not obvious how the training objective encourages meaningful retrievals. Here, we show how it rewards retrievals that improve prediction accuracy.</p>
<p>For a given query $x$ and document $z$, recall that $f(x, z)$ is the "relevance score" that the knowledge retriever assigns to document $z$. We can see how a single step of gradient descent during REALM pre-training alters this score by analyzing the gradient with respect to the parameters of the knowledge retriever, $\theta$ :</p>
<p>$$
\begin{aligned}
\nabla \log p(y \mid x) &amp; =\sum_{z \in \mathcal{Z}} r(z) \nabla f(x, z) \
r(z) &amp; =\left[\frac{p(y \mid z, x)}{p(y \mid x)}-1\right] p(z \mid x)
\end{aligned}
$$</p>
<p>For each document $z$, the gradient encourages the retriever to change the score $f(x, z)$ by $r(z)$ - increasing if $r(z)$ is positive, and decreasing if negative. The multiplier $r(z)$ is positive if and only if $p(y \mid z, x)&gt;p(y \mid x)$. The term $p(y \mid z, x)$ is the probability of predicting the correct output $y$ when using document $z$. The term $p(y \mid x)$ is the expected value of $p(y \mid x, z)$ when randomly sampling a document from $p(z \mid x)$. Hence, document $z$ receives a positive update whenever it performs better than expected.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.4. Injecting inductive biases into pre-training</h3>
<p>In the process of developing REALM, we discovered several additional strategies that further guide the model towards meaningful retrievals, described below.</p>
<p>Salient span masking During REALM pre-training, we want to focus on examples $x$ that require world knowledge to predict the masked tokens. As explained in Section 2, some MLM spans only require local context. To focus on problems that require world knowledge, we mask salient spans such as "United Kingdom" or "July 1969". We use a BERT-based tagger trained on CoNLL-2003 data (Sang \&amp; De Meulder, 2003) to identify named entities, and a regular expression to identify dates. We select and mask one of these salient spans within a sentence for the masked language modeling task. We show that this significantly outperforms other masking strategies in Section 4.5.</p>
<p>Null document Even with salient span masking, not all masked tokens require world knowledge to predict. We model this by adding an empty null document $\varnothing$ to the top $k$ retrieved documents, allowing appropriate credit to be assigned to a consistent sink when no retrieval is necessary.</p>
<p>Prohibiting trivial retrievals If the pre-training corpus $\mathcal{X}$ and the knowledge corpus $\mathcal{Z}$ are the same, there exists a trivial retrieval candidate $z$ that is too informative: if the masked sentence $x$ comes from document $z$, the knowledge augmented encoder can trivially predict $y$ by looking at the unmasked version of $x$ in $z$. This results in a large positive gradient for $p(z \mid x)$. If this occurs too often, the knowledge retriever ends up learning to look for exact string matches between $x$ and $z$, which does not capture other forms of relevance. For this reason, we exclude this trivial candidate during pre-training.</p>
<p>Initialization At the beginning of training, if the retriever does not have good embeddings for Embed $<em _doc="{doc" _text="\text">{\text {input }}(x)$ and $\operatorname{Embed}</em>(z)$, the retrieved documents $z$ will likely be unrelated to $x$. This causes the knowledge augmented encoder to learn to ignore the retrieved documents. Once this occurs, the knowledge retriever does not receive a meaningful gradient and cannot improve, creating a vicious cycle. To avoid this cold-start problem, we warm-start Embed $}<em _doc="{doc" _text="\text">{\text {input }}$ and Embed $</em>$ using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from. We defer to Lee et al. (2019) for details. For the knowledge-augmented encoder, we warmstart it with BERT pre-training-specifically, the uncased BERT-base model (12 layers, 768 hidden units, 12 attention heads).}</p>
<h2>4. Experiments</h2>
<p>We now evaluate our approach on the Open-QA task. In this section, we describe in detail the benchmarks used and the different approaches to which we compare empirically.</p>
<h3>4.1. Open-QA Benchmarks</h3>
<p>A number of benchmarks have been proposed for OpenQA. In this work, we focus on datasets where the question writers did not already know the answer. This yields questions that reflect more realistic information-seeking needs, and also avoids artifacts that can arise if the question is formulated with a particular answer in mind. A deeper justification is given in Lee et al. (2019). In all cases, the predicted answer is evaluated via exact match with any reference answer, following previous Open-QA work (Chen et al., 2017).</p>
<p>NaturalQuestions-Open The NaturalQuestions dataset (Kwiatkowski et al., 2019) consists of naturally occurring Google queries and their answers. Each answer also comes with an "answer type": following Lee et al. (2019), we only keep questions that are categorized as "short answer type" with at most five tokens. The dataset also provides a suggested Wikipedia document to retrieve; like all models we compare against, we do not provide this to our model.</p>
<p>WebQuestions The WebQuestions dataset (Berant et al., 2013) was collected from the Google Suggest API, using one seed question and expanding the set to related questions. We follow the setting defined by Chen et al. (2017).</p>
<p>CuratedTrec The CuratedTrec dataset is a collection of question-answer pairs drawn from real user queries issued on sites such as MSNSearch and AskJeeves. To account for multiple correct answers or different spelling variations, the answers in this dataset are defined as regular expressions that match all correct answers. It is unclear how to train generation-based models with this type of supervision, so we do not evaluate them on this dataset.</p>
<h3>4.2. Approaches compared</h3>
<p>Retrieval-based Open-QA Most existing Open-QA systems answer the input question by first retrieving potentially relevant documents from a knowledge corpus, and then using a reading comprehension system to extract an answer from the documents. In this paradigm, the knowledge is stored explicitly in the corpus. We wish to compare different methods for implementing retrieval.</p>
<p>Many approaches use non-learned heuristic retrieval such as sparse bag-of-words matching (Robertson et al., 2009) or entity linking on the question to select a small set of rel-
evant documents (e.g., 20). These documents are typically then re-ranked using a learned model, but coverage may be limited by the initial heuristic retrieval step. Approaches such as DrQA (Chen et al., 2017), HardEM (Min et al., 2019a), GraphRetriever (Min et al., 2019b), and PathRetriever (Asai et al., 2019) in Table 1 are in this category.</p>
<p>Some recent approaches have proposed to implement learnable retrieval using a MIPS index. ORQA (Lee et al., 2019) formulates Open-QA using a similar latent variable model as REALM, and also trains by maximizing the marginal likelihood. However, REALM adds a novel language model pre-training step, and backpropagates into the MIPS index, rather than using a fixed index. In Table 1, we directly compare the two. It is also important to note that the retrievers for both REALM pretraining and ORQA are initialized using the Inverse Cloze Task, described in Section 3.4.</p>
<p>Generation-based Open-QA An emerging alternative approach to Open-QA is to model it as a sequence prediction task: simply encode the question, and then decode the answer token-by-token based on the encoding. While it was initially unclear how large amounts of knowledge could be injected into the model, GPT-2 (Radford et al., 2019) hinted at the possibility of directly generating answers without using any given context via sequence-tosequence. However, their performance was not competitive possibly due to the lack of fine-tuning. Orthogonally, T5 (Raffel et al., 2019) showed that directly generating answers without explicit extraction from the given context is viable approach, but they only experimented on the reading comprehension task, where a context document is provided.</p>
<p>For the most competitive and comparable generation-based baseline, we compare to concurrent work which fine-tunes T5 for Open-QA (Roberts et al., 2020). ${ }^{4}$ We compare against the Base, Large, and even larger 11-billion parameter model to measure the effect of model size.</p>
<h3>4.3. Implementation Details</h3>
<p>Fine-tuning We reuse all hyperparameters from Lee et al. (2019), to enable direct comparison. Our knowledge corpus is derived from the December 20, 2018 snapshot of English Wikipedia. Documents are greedily split into chunks of up to 288 BERT wordpieces, resulting in just over 13 million retrieval candidates. During finetuning inference, we consider the top-5 candidates, and the</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1. Test results on Open-QA benchmarks. The number of train/test examples are shown in paretheses below each benchmark. Predictions are evaluated with exact match against any reference answer. Sparse retrieval denotes methods that use sparse features such as TF-IDF and BM25. Our model, REALM, outperforms all existing systems.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Architectures</th>
<th style="text-align: center;">Pre-training</th>
<th style="text-align: center;">$\begin{gathered} \text { NQ } \ (79 \mathrm{k} / 4 \mathrm{k}) \end{gathered}$</th>
<th style="text-align: center;">$\begin{gathered} \text { WQ } \ (3 \mathrm{k} / 2 \mathrm{k}) \end{gathered}$</th>
<th style="text-align: center;">$\begin{gathered} \text { CT } \ (1 \mathrm{k} / 1 \mathrm{k}) \end{gathered}$</th>
<th style="text-align: center;"># params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BERT-Baseline (Lee et al., 2019)</td>
<td style="text-align: center;">Sparse Retr.+Transformer</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">110 m</td>
</tr>
<tr>
<td style="text-align: center;">T5 (base) (Roberts et al., 2020)</td>
<td style="text-align: center;">Transformer Seq2Seq</td>
<td style="text-align: center;">T5 (Multitask)</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">223 m</td>
</tr>
<tr>
<td style="text-align: center;">T5 (large) (Roberts et al., 2020)</td>
<td style="text-align: center;">Transformer Seq2Seq</td>
<td style="text-align: center;">T5 (Multitask)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">738 m</td>
</tr>
<tr>
<td style="text-align: center;">T5 (11b) (Roberts et al., 2020)</td>
<td style="text-align: center;">Transformer Seq2Seq</td>
<td style="text-align: center;">T5 (Multitask)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11318 m</td>
</tr>
<tr>
<td style="text-align: center;">DrQA (Chen et al., 2017)</td>
<td style="text-align: center;">Sparse Retr.+DocReader</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">34 m</td>
</tr>
<tr>
<td style="text-align: center;">HardEM (Min et al., 2019a)</td>
<td style="text-align: center;">Sparse Retr.+Transformer</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">110 m</td>
</tr>
<tr>
<td style="text-align: center;">GraphRetriever (Min et al., 2019b)</td>
<td style="text-align: center;">GraphRetriever+Transformer</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">110 m</td>
</tr>
<tr>
<td style="text-align: center;">PathRetriever (Asai et al., 2019)</td>
<td style="text-align: center;">PathRetriever+Transformer</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">110 m</td>
</tr>
<tr>
<td style="text-align: center;">ORQA (Lee et al., 2019)</td>
<td style="text-align: center;">Dense Retr.+Transformer</td>
<td style="text-align: center;">ICT+BERT</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">330 m</td>
</tr>
<tr>
<td style="text-align: center;">Ours ( $\mathcal{X}=$ Wikipedia, $\mathcal{Z}=$ Wikipedia)</td>
<td style="text-align: center;">Dense Retr.+Transformer</td>
<td style="text-align: center;">REALM</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">330 m</td>
</tr>
<tr>
<td style="text-align: center;">Ours ( $\mathcal{X}=$ CC-News, $\mathcal{Z}=$ Wikipedia)</td>
<td style="text-align: center;">Dense Retr.+Transformer</td>
<td style="text-align: center;">REALM</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">330 m</td>
</tr>
</tbody>
</table>
<p>Table 2. Ablation experiments on NQ's development set.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Ablation</th>
<th style="text-align: center;">Exact <br> Match</th>
<th style="text-align: center;">Zero-shot <br> Retrieval <br> Recall@5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">REALM</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: left;">REALM retriever+Baseline encoder</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: left;">Baseline retriever+REALM encoder</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">13.9</td>
</tr>
<tr>
<td style="text-align: left;">Baseline (ORQA)</td>
<td style="text-align: center;">31.3</td>
<td style="text-align: center;">13.9</td>
</tr>
<tr>
<td style="text-align: left;">REALM with random uniform masks</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">24.2</td>
</tr>
<tr>
<td style="text-align: left;">REALM with random span masks</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">26.1</td>
</tr>
<tr>
<td style="text-align: left;">$30 \times$ stale MIPS</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">15.1</td>
</tr>
</tbody>
</table>
<p>entire model can be run on a single machine with a 12GB GPU.</p>
<p>Pre-training We pre-train for 200k steps on 64 Google Cloud TPUs, with a batch size of 512 and a learning rate of $3 \mathrm{e}-5$, using BERT's default optimizer. The document embedding step for the MIPS index is parallelized over 16 TPUs. For each example, we retrieve and marginalize over 8 candidate documents, including the null document $\varnothing$.</p>
<p>We experiment with two choices of the pre-training corpus $\mathcal{X}$ : (1) Wikipedia, which is identical to the knowledge corpus $\mathcal{Z}$, and (2) CC-News, our reproduction of the corpus of English news proposed by Liu et al. (2019).</p>
<h3>4.4. Main results</h3>
<p>Table 1 shows the accuracy of different approaches on the three Open-QA datasets. REALM outperform all previous approaches by a significant margin. Table 1 also shows the number of parameters for each model.</p>
<p>As reported in the concurrent work of Roberts et al. (2020), the generative Open-QA systems based on T5 are surprisingly powerful, with the largest T5-11B model outperforming the previous best Open-QA system. Increasing the size of T5 yields consistent improvement, but comes at significant computational cost (from Base to 11B, the model is 50 times larger, and gains roughly 5 points in accuracy). In contrast, REALM outperforms the largest T5-11B model while being 30 times smaller. It is also important to note that T5 accesses additional reading comprehension data from SQuAD during its pre-training (100,000+ examples). Access to such data could also benefit REALM, but was not used in our experiments.</p>
<p>Among all systems, the most direct comparison with REALM is ORQA (Lee et al., 2019), where the fine-tuning setup, hyperparameters and training data are identical. The improvement of REALM over ORQA is purely due to better pre-training methods. The results also indicate that our method of pre-training can be applied both on (1) the singlecorpus setting ( $\mathcal{X}=$ Wikipedia, $\mathcal{Z}=$ Wikipedia), or (2) the separate-corpus setting ( $\mathcal{X}=$ CC-News, $\mathcal{Z}=$ Wikipedia).</p>
<p>Compared to other retrieval-based systems (Asai et al., 2019; Min et al., 2019a;b) which often retrieve from 20 to 80 documents, our system gets the overall best performance while only retrieving 5 documents.</p>
<h3>4.5. Analysis</h3>
<p>In Table 2 we present results for NaturalQuestions-Open after ablating critical components of REALM. In addition to the end-to-end results, we also report how often the gold answer appears in the top-5 retrievals before applying any fine-tuning. The latter metric more significantly isolates the contribution of improving the retriever during pre-training.</p>
<p>Table 3. An example where REALM utilizes retrieved documents to better predict masked tokens. It assigns much higher probability (0.129) to the correct term, "Fermat", compared to BERT. (Note that the blank corresponds to 3 BERT wordpieces.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$x:$</th>
<th style="text-align: left;">An equilateral triangle is easily constructed using a straightedge and compass, because 3 is a ___ prime.</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">(a) BERT</td>
<td style="text-align: left;">$p(y=$ "Fermat" $] x)=1.1 \times 10^{-14}$</td>
<td style="text-align: left;">(No retrieval.)</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(b) REALM</td>
<td style="text-align: left;">$p(y=$ "Fermat" $] x, z)=1.0$</td>
<td style="text-align: left;">(Conditional probability with document $z=$ "257 is $\ldots$ a Fermat prime.</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">(c) REALM</td>
<td style="text-align: left;">$p(y=$ "Fermat" $] x)=0.129$</td>
<td style="text-align: left;">(Thus a regular polygon with 257 sides is constructible with compass ...")</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Encoder or Retriever We first aim to determine whether REALM pre-training improves the retriever or the encoder, or both. To do so, we can reset the parameters of either the retriever or the encoder to their baseline state before REALM pre-training, and feed that into fine-tuning. Resetting both the retriever and encoder reduces the system to our main baseline, ORQA. We find that both the encoder and retriever benefit from REALM training separately, but the best result requires both components acting in unison.</p>
<p>Masking scheme We compare our salient span masking scheme (Section 3.4) with (1) random token masking introduced in BERT (Devlin et al., 2018) and (2) random span masking proposed by SpanBERT (Joshi et al., 2019). While such salient span masking has not been shown to be impactful in previous work with standard BERT training (Joshi et al., 2019), it is crucial for REALM. Intuitively, the latent variable learning relies heavily on the utility of retrieval and is therefore more sensitive to a consistent learning signal.</p>
<p>MIPS index refresh rate During pre-training, we run a parallel process to re-embed corpus documents and rebuild the MIPS index. This results in one index refresh per approximately 500 training steps. To demonstrate the importance of frequent index refreshes, we compare against using a slower refresh rate. The results in Table 2 suggests that a stale index can hurt model training, and further reducing this staleness could offer better optimization.</p>
<p>Examples of retrieved documents Table 3 shows an example of the REALM masked language model prediction. In this example, "Fermat" is the correct word, and REALM (row (c)) gives the word a much high probability compared to the BERT model (row (a)). Since REALM manages to retrieve some documents with a related fact (row (b)), the marginalized probability of the correct answer dramatically increases. This shows that REALM is able to retrieve document to fill in the masked word even though it is trained with unsupervised text only.</p>
<h2>5 Discussion and Related Work</h2>
<p>We previously discussed related methods for Open-QA. Here we present several alternate ways of viewing REALM that connect it to a broader set of ideas beyond Open-QA:</p>
<p>Language modeling with corpus as context Language representation models have been incorporating contexts of increasingly large scope when making predictions. Examples of this progression include models that condition on surrounding words (Mikolov et al., 2013a;b), sentences (Kiros et al., 2015; Peters et al., 2018), and paragraphs (Radford et al., 2018; Devlin et al., 2018). We can view REALM as a generalization of the above work to the next level of scope: the entire text corpus.</p>
<p>Retrieve-and-edit with learned retrieval In order to better explain the variance in the input text and enable controllable generation, Guu et al. (2018) proposed a language model with the retrieve-and-edit framework (Hashimoto et al., 2018) that conditions on text with high lexical overlap. REALM has a similar approach, except that the model learns for itself which texts are most useful for reducing perplexity. By jointly learning the retriever, REALM has the capacity to depend on information beyond lexical overlap.</p>
<p>Scalable grounded neural memory The document index can be viewed as a memory where the keys are the document embeddings. From this view, our work share motivations with works such as product key memory (Lample et al., 2019), which enables sub-linear memory access in a memory network (Weston et al., 2014; Graves et al., 2014; Sukhbaatar et al., 2015), allowing these scalable memory layers to be integrated into large language models. One main difference is that our memories are grounded-each memory is associated with a document rather than unnamed value vectors. This level of interpretability is crucial for applications like Open-QA, where users would require provenance for a predicted answer to be trustworthy.</p>
<p>Unsupervised Corpus Alignment In sequence-tosequence models with attention (Bahdanau et al., 2014),</p>
<p>text is generated with latent selection of relevant tokens. This results in a set of model-centric unsupervised alignments between target and source tokens. Analogously, REALM also generates text with latent selection of relevant documents. A by-product of our method is that we offer a set of model-centric unsupervised alignments between text in the pre-training corpus $\mathcal{X}$ and knowledge corpus $\mathcal{Z}$.</p>
<h2>6. Future Work</h2>
<p>The work presented here is the minimal instantiation of a family of REALM-like approaches where a representation is pre-trained to perform reasoning over a large corpus of knowledge on-the-fly during inference. We are particularly optimistic about generalizations of this work to (1) structured knowledge, which would result in a generalization of Peters et al. (2019) where we would also learn the decision of which entities are informative, (2) the multi-lingual setting, e.g., retrieving knowledge in a high-resource language to better represent text in a low-resource language, and (3) the multi-modal setting, e.g., retrieving images or videos that can provide knowledge rarely observed in text.</p>
<h2>References</h2>
<p>Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and Xiong, C. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470, 2019.</p>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533-1544, 2013.</p>
<p>Brill, E., Dumais, S., and Banko, M. An analysis of the askmsr question-answering system. In Empirical Methods in Natural Language Processing, 2002.</p>
<p>Chen, D., Fisch, A., Weston, J., and Bordes, A. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 1870-1879, 2017.</p>
<p>Clark, C. and Gardner, M. Simple and effective multiparagraph reading comprehension. In Annual Meeting of the Association for Computational Linguistics, 2017.</p>
<p>Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Advances in neural information processing systems, pp. 3079-3087, 2015.</p>
<p>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Graves, A., Wayne, G., and Danihelka, I. Neural turing machines. ArXiv, abs/1410.5401, 2014.</p>
<p>Guu, K., Hashimoto, T. B., Oren, Y., and Liang, P. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437450, 2018.</p>
<p>Hashimoto, T. B., Guu, K., Oren, Y., and Liang, P. S. A retrieve-and-edit framework for predicting structured outputs. In Advances in Neural Information Processing Systems, pp. 10052-10062, 2018.</p>
<p>Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., and Levy, O. SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint arXiv:1907.10529, 2019.</p>
<p>Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. Generalization through memorization: Nearest neighbor language models. ArXiv, abs/1911.00172, 2019.</p>
<p>Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. In Advances in neural information processing systems, pp. 3294-3302, 2015.</p>
<p>Kwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019.</p>
<p>Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and Jégou, H. Large memory layers with product keys. In Advances in Neural Information Processing Systems, pp. 8546-8557, 2019.</p>
<p>Lee, K., Salant, S., Kwiatkowski, T., Parikh, A., Das, D., and Berant, J. Learning recurrent span representations for extractive question answering. arXiv preprint arXiv:1611.01436, 2016.</p>
<p>Lee, K., Chang, M.-W., and Toutanova, K. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the Conference of Association for Computational Linguistics, 2019.</p>
<p>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ArXiv, abs/1910.13461, 2019.</p>
<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013a.</p>
<p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pp. 3111-3119, 2013b.</p>
<p>Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A., and Weston, J. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126, 2016.</p>
<p>Min, S., Chen, D., Hajishirzi, H., and Zettlemoyer, L. A discrete hard em approach for weakly supervised question answering. arXiv preprint arXiv:1909.04849, 2019a.</p>
<p>Min, S., Chen, D., Zettlemoyer, L., and Hajishirzi, H. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868, 2019b.</p>
<p>Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In Proc. of NAACL, 2018.</p>
<p>Peters, M. E., Neumann, M., IV, R. L. L., Schwartz, R., Joshi, V., Singh, S., and Smith, N. A. Knowledge enhanced contextual word representations, 2019.</p>
<p>Petroni, F., Rocktäschel, T., Lewis, P., Bakhtin, A., Wu, Y., Miller, A. H., and Riedel, S. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.</p>
<p>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding with unsupervised learning. Technical report, OpenAI, 2018.</p>
<p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. OpenAI Blog, 2019.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.</p>
<p>Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, 2016.</p>
<p>Rajpurkar, P., Jia, R., and Liang, P. Know what you don't know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.</p>
<p>Ram, P. and Gray, A. G. Maximum inner-product search using cone trees. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 931-939, 2012.</p>
<p>Roberts, A., Raffel, C., and Shazeer, N. How much knowledge can you pack into the parameters of a language model? arXiv preprint arXiv:TBD, 2020.</p>
<p>Robertson, S., Zaragoza, H., et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333-389, 2009.</p>
<p>Sang, E. T. K. and De Meulder, F. Introduction to the conll2003 shared task: Language-independent named entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pp. 142-147, 2003.</p>
<p>Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H. Bidirectional attention flow for machine comprehension. In International Conference on Learning Representations, 2016.</p>
<p>Shen, F., Liu, W., Zhang, S., Yang, Y., and Tao Shen, H. Learning binary codes for maximum inner product search. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4148-4156, 2015.</p>
<p>Shrivastava, A. and Li, P. Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips). In Advances in Neural Information Processing Systems, pp. 2321-2329, 2014.</p>
<p>Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end memory networks. In Advances in neural information processing systems, 2015.</p>
<p>Weston, J., Chopra, S., and Bordes, A. Memory networks. arXiv preprint arXiv:1410.3916, 2014.</p>
<h2>A. Derivation of the gradient with respect to the knowledge retriever</h2>
<p>We compute the gradient of the REALM pre-training objective (a log-likelihood) with respect to the parameters of the knowledge retriever, $\theta$ :</p>
<p>$$
\begin{aligned}
\nabla \log p(y \mid x) &amp; =p(y \mid x)^{-1} \nabla p(y \mid x) \
&amp; =p(y \mid x)^{-1} \sum_{z} p(y \mid z, x) \nabla p(z \mid x) \
&amp; =p(y \mid x)^{-1} \sum_{z} p(y \mid z, x) p(z \mid x) \nabla \log p(z \mid x) \
&amp; =\sum_{z} p(z \mid y, x) \nabla \log p(z \mid x)
\end{aligned}
$$</p>
<p>where the last line follows from applying conditional Bayes' rule. We can then expand $\nabla \log p(z \mid x)$ as:</p>
<p>$$
\begin{aligned}
\nabla \log p(z \mid x) &amp; =\nabla \log \frac{\exp f(x, z)}{\sum_{z^{\prime}} \exp f\left(x, z^{\prime}\right)} \
&amp; =\nabla\left[f(x, z)-\log \sum_{z^{\prime}} \exp f\left(x, z^{\prime}\right)\right] \
&amp; =\nabla f(x, z)-\sum_{z^{\prime}} p\left(z^{\prime} \mid x\right) \nabla f\left(x, z^{\prime}\right)
\end{aligned}
$$</p>
<p>Plugging this back into the first set of equations yields:</p>
<p>$$
\begin{aligned}
\nabla \log p(y \mid x) &amp; =\sum_{z} p(z \mid y, x)\left[\nabla f(x, z)-\sum_{z^{\prime}} p\left(z^{\prime} \mid x\right) \nabla f\left(x, z^{\prime}\right)\right] \
&amp; =\sum_{z} p(z \mid y, x) \nabla f(x, z)-\sum_{z^{\prime}} p\left(z^{\prime} \mid x\right) \nabla f\left(x, z^{\prime}\right) \
&amp; =\sum_{z}[p(z \mid y, x)-p(z \mid x)] \nabla f(x, z) \
&amp; =\sum_{z}\left[\frac{p(y \mid z, x) p(z \mid x)}{p(y \mid x)}-p(z \mid x)\right] \nabla f(x, z) \
&amp; =\sum_{z}\left[\frac{p(y \mid z, x)}{p(y \mid x)}-1\right] p(z \mid x) \nabla f(x, z)
\end{aligned}
$$</p>
<p>In the second line, we used the fact that the overall expression is an expectation with respect to $p(z \mid y, x)$, and the terms which depend on $z^{\prime}$ but not $z$ can be moved out of that expectation.</p>
<h2>B. Connection between REALM and supervised learning</h2>
<p>From the equations in Appendix A, we saw that</p>
<p>$$
\nabla \log p(y \mid x)=\sum_{z}[p(z \mid y, x)-p(z \mid x)] \nabla f(x, z)
$$</p>
<p>Suppose that there exists one document $z^{<em>}$ which causes the model to achieve perfect prediction accuracy (i.e., $p\left(y \mid z^{</em>}, x\right)=1$ ), while all other documents $z^{\prime}$ result in
zero accuracy (i.e., $p\left(y \mid z^{\prime}, x\right)=0$ ). Under this setting, $p\left(z^{<em>} \mid y, x\right)=1$ (provided that $p\left(z^{</em>} \mid x\right)$ is non-zero), which causes the gradient to become</p>
<p>$$
\begin{aligned}
\nabla \log p(y \mid x) &amp; =\nabla f\left(x, z^{<em>}\right)-\sum_{z} p(z \mid x) \nabla f(x, z) \
&amp; =\nabla \log p\left(z^{</em>} \mid x\right)
\end{aligned}
$$</p>
<p>From this, we see that gradient descent on the REALM objective is equivalent to gradient descent on $\log p\left(z^{<em>} \mid x\right)$. This is none other than the typical maximum likelihood training objective used in supervised learning, where $z^{</em>}$ is the "gold" document.</p>
<h2>C. Adapting to new knowledge</h2>
<p>An explicit retrieval system allows us to adapt to new world knowledge simply by modifying the corpus documents. To demonstrate this ability, we replace the knowledge corpus with a more recent version of Wikipedia corpus after pre-training is done. When the input query is about a fact where the two corpora disagree, REALM can change the prediction to reflect the updated information, as exemplified in Table 4. However, even with an explicit retrieval mechanism, the knowledge-augmented encoder will still end up remembering some world knowledge, making the prediction of some input sentences not updated with the new corpus. (For instance, the model predicts "Thatcher" for " $\qquad$ is the prime minister of United Kingdom." on both corpora, perhaps due to the frequent mention of her name in Wikipedia articles.)</p>
<h2>D. Retrieval Utility</h2>
<p>The null document $\varnothing$ described in Section 3.4 provides a way to measure the importance of a retrieved document $z$ : we define the retrieval utility $(\mathrm{RU})$ of $z$ for the masked input $x$ as the difference between the log-likelihood of the knowledge-augmented encoder when conditioning on $z$ versus on $\varnothing$ :</p>
<p>$$
\mathrm{RU}(z \mid x)=\log p(y \mid z, x)-\log p(y \mid \varnothing, x)
$$</p>
<p>A negative RU shows that $z$ is less useful for predicting $y$ than the null document. This could mean that $z$ is irrelevant to $x$, but could also mean that the masked tokens in $x$ do not require world knowledge to predict, or that the world knowledge is sufficiently commonplace it has been baked into the model's parameters. In practice, we find that RU increases steadily over the course of pre-training, and is more predictive of good performance on the downstream task of Open-QA than even the overall log-likelihood. An example of how RU behaves over time and across different settings is in Figure 4.</p>
<div class="codehilite"><pre><span></span><code>\F: &quot;Jennifer __ formed the production company Excellent Cadaver.&quot;
BERT
REALM ( \(\mathcal{Z}=20\) Dec 2018 corpus) also ( 0.13 ), then ( 0.08 ), later ( 0.05 ), ...
REALM ( \(\mathcal{Z}=20\) Jan 2020 corpus) \(\quad\) smith ( 0.01 ), brown ( 0.01 ), jones ( 0.01 )
lawrence ( 0.13 ), brown ( 0.01 ), smith ( 0.01 ), ...
</code></pre></div>

<p>Table 4. An example where REALM adapts to the updated knowledge corpus. The Wikipedia page "Excellent Cadaver" was added in 2019, so the model was not about to recover the word when the knowledge corpus is outdated (2018). Interestingly, the same REALM model pre-trained on the 2018 corpus is able to retrieve the document in the updated corpus (2020) and generate the correct token, "Lawrence".
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. The Retrieval Utility (RU, described in Eq. 2) vs the number of pre-training steps. RU roughly estimates the "usefulness" of retrieval. RU is impacted by the choice of masking and the number of pre-training steps.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We initially conducted our own T5 experiments using the code from https://tinyurl.com/t5-openqa-colab (Raffel et al., 2019). We now report results from the concurrent work of Roberts et al. (2020), which has an improved fine-tuning procedure.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
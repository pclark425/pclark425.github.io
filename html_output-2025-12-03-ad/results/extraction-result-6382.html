<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6382 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6382</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6382</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-268264158</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.04652v3.pdf" target="_blank">Yi: Open Foundation Models by 01.AI</a></p>
                <p><strong>Paper Abstract:</strong> We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6382.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6382.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi number tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi model tokenizer: digit-splitting for numbers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Yi uses a SentencePiece BPE tokenizer with a 64k vocabulary and explicitly splits numbers into individual digits to facilitate better numeric understanding and robustness to rare characters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi (6B, 34B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 34B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>3.1T bilingual (English & Chinese) web-derived corpus with heavy cleaning; pretraining intentionally included limited math/coding content (authors note they refrained from extensive mathematical/coding content in initial design).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>numeric representation / downstream arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>tokenized text (numbers split into digit tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>n/a (tokenization-level design choice applied during pretraining/inference)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>n/a (mechanism described rather than benchmarked independently)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Design choice: numbers are split into individual digits in BPE to 'facilitate a better understanding of numeric data'; rare characters fall back to unicode-byte encoding. This is presented as an explicit inductive bias to improve numeric handling rather than as an analyzed emergent internal representation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No explicit failure taxonomy for digit-splitting is given; potential failure modes implied include token-level lengthening of numeric expressions (more tokens for numbers) and possible sensitivity to multi-digit semantics (digit-level representation may impair holistic numeric patterns unless model learns composition).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Not directly analyzed; tokenizer is used across Yi-6B and Yi-34B and thus treated as a fixed preprocessing inductive bias independent of model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Yi: Open Foundation Models by 01.AI', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6382.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6382.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context linear coefficient inference</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning study: inferring linear coefficients of weighted sums</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A controlled few-shot in-context learning task where models are given demonstrations (x1..xn, y) for y = w1 x1 + ... + wn xn and are asked to predict y for new inputs; authors measure both continuous error and exact-match behavior to probe arithmetic and functional inference capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-6B, Yi-34B; comparisons with LLaMA-2 70B, Mixtral (other open models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B, 34B (Yi); 70B (LLaMA-2), Mixtral (multi‑checkpoint ensemble style described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same Yi pretraining mixture (3.1T tokens bilingual, heavy cleaning); task is an evaluation / probing protocol run post-training (not additional training data for the probe).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Custom in-context linear coefficients task (described in paper, Figure 3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>in-context functional inference / arithmetic (weighted linear combination prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>few-shot natural-language-like examples: sequences of numeric input-output demonstrations (x1,...,xn,y) presented as text prompting; model must output y for new x</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Varied: small experiments with 2 coefficients (e.g., [1, -1]) up to 5 coefficients (e.g., [1,1,1,1,1]); difficulty increases with number of coefficients and dimensionality of input</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot in-context examples (standard in-context learning); authors explicitly state they avoid arithmetic confound by noting addition/subtraction is easy for most models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Two complementary metrics: continuous error |y - y*| (absolute difference) and discontinuous exact match (y == y*).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively: for two-coefficient tasks ([1,-1]) Yi-34B and LLaMA-2 70B performed best by exact match and by difference-to-target; when coefficients increased to 5, only very large models (LLaMA-2 70B and Mixtral variants) achieve meaningful exact-match success, while others show continuous differences to target.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors use the continuous vs discontinuous distinction to separate smooth approximation (difference-to-target) from exact symbolic-like recovery (exact match). They interpret success on exact match as an emergent ability for inferring complex functions in-context and argue arithmetic per se is not the confound because addition/subtraction are already solved by most models. No neuron-level probing, attention analysis, or logit-lens traces are provided; analysis is at behavioral metric level and comparative scaling observation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades as number of linear coefficients increases; smaller models (e.g., Yi-6B) fail to achieve exact-match inference for higher-dimensional linear functions, indicating inability to fully infer parameter vectors from few-shot examples. The paper frames this as an emergence with model scale rather than as classic digit-errors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Clear scaling trend: Yi-34B outperforms Yi-6B; emergent exact-match behavior appears only in much larger models (e.g., LLaMA-2 70B, Mixtral), indicating non-linear (emergent) improvement in the ability to infer more complex functions with scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Yi: Open Foundation Models by 01.AI', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6382.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6382.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8K / MATH evaluation (Yi)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic and mathematical problem evaluations using GSM8K and MATH benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Yi models were evaluated on standard math benchmarks (GSM8K and MATH). For chat-model zero-shot evaluations on GSM8K and BBH the authors used Chain-of-Thought prompting to improve deliberation; reported numeric pass@1 scores indicate substantial improvement from 6B to 34B but a remaining gap to GPT-4 on harder math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-6B, Yi-34B; Yi-Chat-6B, Yi-Chat-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only Transformer (Yi family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B and 34B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretraining on 3.1T cleaned bilingual web data; authors explicitly note they refrained from adding extensive math and coding content during initial pretraining which contributes to a performance gap on math/coding relative to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (grade-school math word problems), MATH (competition-level math problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / mathematical problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language math word problems (GSM8K: grade-school-level, MATH: competition-style), evaluated with and without Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school, multi-step; MATH: harder competition problems (higher difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>For chat models, Chain-of-Thought (CoT) prompting was used for zero-shot evaluations on GSM8K and BBH; base-model evaluations used few-shot settings described in benchmark protocols (e.g., 8-shot for GSM8K in base model runs in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1 accuracy (percentage exact-correct answers) for GSM8K and MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported values (from paper tables): Yi-6B GSM8K pass@1 ≈ 32.5%; Yi-34B GSM8K pass@1 ≈ 67.2% (Table 3). MATH performance for Yi-34B is reported as low (≈ 4.6% in Table 3), indicating strong disparity between grade-school and competition-level math. For chat models, Yi-Chat-34B shows substantial GSM8K improvement in Table 4 (chat 34B rows) and authors note Yi-34B-Chat performs well on GSM8K and the Hungarian exam; exact chat numbers are given in Table 4 per metric.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors attribute better GSM8K performance to model scale and SFT strategy; they also caution prompt sensitivity and evaluation pipeline differences across models. They note CoT is used to guide deliberation for GSM8K in chat-mode evaluation. No internal neuron-level mechanistic analysis of arithmetic processing is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Large gap on MATH (hard competition problems) despite good GSM8K results; smaller models fail to show mathematical capabilities without additional data or SFT; prompt sensitivity — different prompts and post-processing can materially affect scores; the paper notes general gaps vs GPT-4 on reasoning-related and code/math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Strong positive scaling: Yi-34B substantially outperforms Yi-6B on GSM8K; authors link mathematical and coding performance increases more strongly to model size than to some other tasks (commonsense, reading). They also report that finetuning and continued pretraining can further improve math ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Yi: Open Foundation Models by 01.AI', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6382.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6382.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hungarian exam eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation on 2023 Hungarian high-school mathematics final exam</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors evaluated Yi-34B-Chat on the 2023 Hungarian high‑school mathematics final exam (used previously in community evaluations) as a test of real-world mathematical ability and overfitting; Yi-34B-Chat is described as performing 'inspiringly' while Yi-6B-Chat performs poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-Chat-6B, Yi-Chat-34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only Transformer chat models (SFT fine-tuned Yi variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B (chat), 34B (chat)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same Yi pretraining mixture with <10K high-quality instruction SFT examples; authors emphasize SFT data quality and small size rather than huge-scale instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>2023 Hungarian high-school mathematics final exam (reproduced by Paster / xAI Grok community)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>real-world multi-step math exam questions (mixture of algebra, geometry, problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language exam items (some require multi-step derivations and proofs or numerical solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>High-school final exam (multi-step, moderate to high difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chat-mode instruction-following; specific prompting not exhaustively detailed, but as a chat model evaluation authors likely used instruction prompts and possibly CoT-style prompting for deliberation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exam score (percent or raw score) — paper gives qualitative descriptor and figure (Figure 4) rather than detailed per-question numeric table in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: Yi-34B-Chat 'performs inspiringly' per authors' description and Figure 4; Yi-6B-Chat does not exhibit strong mathematical capabilities. No single numeric aggregate reported in-text besides the figure visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Presented as an external stress-test to check math overfitting; authors interpret the good Yi-34B-Chat result as evidence of genuine mathematical capability rather than dataset memorization. No internal mechanistic probing of how the model computes solutions is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Smaller chat model (6B) fails to perform well; authors imply methods like more SFT, larger model size, or continued pretraining are required. No detailed failure-mode breakdown (e.g., algebraic symbol errors) provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Evidence that SFT plus larger model scale (34B) improves performance on a challenging real-world exam; smaller models underperform, supporting the paper's broader claim that reasoning capability correlates with model scale when pretraining data is held fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Yi: Open Foundation Models by 01.AI', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Are emergent abilities of large language models a mirage? <em>(Rating: 2)</em></li>
                <li>Data engineering for scaling language models to 128k context <em>(Rating: 2)</em></li>
                <li>Training Verifiers to Solve Math Word Problems <em>(Rating: 2)</em></li>
                <li>Take a step back: Evoking reasoning via abstraction in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6382",
    "paper_id": "paper-268264158",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Yi number tokenization",
            "name_full": "Yi model tokenizer: digit-splitting for numbers",
            "brief_description": "Yi uses a SentencePiece BPE tokenizer with a 64k vocabulary and explicitly splits numbers into individual digits to facilitate better numeric understanding and robustness to rare characters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Yi (6B, 34B)",
            "model_family": "decoder-only Transformer",
            "model_size": "6B, 34B",
            "training_data_description": "3.1T bilingual (English & Chinese) web-derived corpus with heavy cleaning; pretraining intentionally included limited math/coding content (authors note they refrained from extensive mathematical/coding content in initial design).",
            "benchmark_name": "n/a",
            "task_type": "numeric representation / downstream arithmetic",
            "problem_format": "tokenized text (numbers split into digit tokens)",
            "difficulty_level": "n/a",
            "prompting_method": "n/a (tokenization-level design choice applied during pretraining/inference)",
            "performance_metric": "n/a (mechanism described rather than benchmarked independently)",
            "performance_value": null,
            "internal_analysis": "Design choice: numbers are split into individual digits in BPE to 'facilitate a better understanding of numeric data'; rare characters fall back to unicode-byte encoding. This is presented as an explicit inductive bias to improve numeric handling rather than as an analyzed emergent internal representation.",
            "failure_modes": "No explicit failure taxonomy for digit-splitting is given; potential failure modes implied include token-level lengthening of numeric expressions (more tokens for numbers) and possible sensitivity to multi-digit semantics (digit-level representation may impair holistic numeric patterns unless model learns composition).",
            "scaling_trend": "Not directly analyzed; tokenizer is used across Yi-6B and Yi-34B and thus treated as a fixed preprocessing inductive bias independent of model scale.",
            "uuid": "e6382.0",
            "source_info": {
                "paper_title": "Yi: Open Foundation Models by 01.AI",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "In-context linear coefficient inference",
            "name_full": "In-context learning study: inferring linear coefficients of weighted sums",
            "brief_description": "A controlled few-shot in-context learning task where models are given demonstrations (x1..xn, y) for y = w1 x1 + ... + wn xn and are asked to predict y for new inputs; authors measure both continuous error and exact-match behavior to probe arithmetic and functional inference capabilities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Yi-6B, Yi-34B; comparisons with LLaMA-2 70B, Mixtral (other open models)",
            "model_family": "decoder-only Transformer",
            "model_size": "6B, 34B (Yi); 70B (LLaMA-2), Mixtral (multi‑checkpoint ensemble style described in paper)",
            "training_data_description": "Same Yi pretraining mixture (3.1T tokens bilingual, heavy cleaning); task is an evaluation / probing protocol run post-training (not additional training data for the probe).",
            "benchmark_name": "Custom in-context linear coefficients task (described in paper, Figure 3)",
            "task_type": "in-context functional inference / arithmetic (weighted linear combination prediction)",
            "problem_format": "few-shot natural-language-like examples: sequences of numeric input-output demonstrations (x1,...,xn,y) presented as text prompting; model must output y for new x",
            "difficulty_level": "Varied: small experiments with 2 coefficients (e.g., [1, -1]) up to 5 coefficients (e.g., [1,1,1,1,1]); difficulty increases with number of coefficients and dimensionality of input",
            "prompting_method": "few-shot in-context examples (standard in-context learning); authors explicitly state they avoid arithmetic confound by noting addition/subtraction is easy for most models",
            "performance_metric": "Two complementary metrics: continuous error |y - y*| (absolute difference) and discontinuous exact match (y == y*).",
            "performance_value": "Reported qualitatively: for two-coefficient tasks ([1,-1]) Yi-34B and LLaMA-2 70B performed best by exact match and by difference-to-target; when coefficients increased to 5, only very large models (LLaMA-2 70B and Mixtral variants) achieve meaningful exact-match success, while others show continuous differences to target.",
            "internal_analysis": "Authors use the continuous vs discontinuous distinction to separate smooth approximation (difference-to-target) from exact symbolic-like recovery (exact match). They interpret success on exact match as an emergent ability for inferring complex functions in-context and argue arithmetic per se is not the confound because addition/subtraction are already solved by most models. No neuron-level probing, attention analysis, or logit-lens traces are provided; analysis is at behavioral metric level and comparative scaling observation.",
            "failure_modes": "Performance degrades as number of linear coefficients increases; smaller models (e.g., Yi-6B) fail to achieve exact-match inference for higher-dimensional linear functions, indicating inability to fully infer parameter vectors from few-shot examples. The paper frames this as an emergence with model scale rather than as classic digit-errors.",
            "scaling_trend": "Clear scaling trend: Yi-34B outperforms Yi-6B; emergent exact-match behavior appears only in much larger models (e.g., LLaMA-2 70B, Mixtral), indicating non-linear (emergent) improvement in the ability to infer more complex functions with scale.",
            "uuid": "e6382.1",
            "source_info": {
                "paper_title": "Yi: Open Foundation Models by 01.AI",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GSM8K / MATH evaluation (Yi)",
            "name_full": "Arithmetic and mathematical problem evaluations using GSM8K and MATH benchmarks",
            "brief_description": "Yi models were evaluated on standard math benchmarks (GSM8K and MATH). For chat-model zero-shot evaluations on GSM8K and BBH the authors used Chain-of-Thought prompting to improve deliberation; reported numeric pass@1 scores indicate substantial improvement from 6B to 34B but a remaining gap to GPT-4 on harder math tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-6B, Yi-34B; Yi-Chat-6B, Yi-Chat-34B",
            "model_family": "decoder-only Transformer (Yi family)",
            "model_size": "6B and 34B",
            "training_data_description": "Pretraining on 3.1T cleaned bilingual web data; authors explicitly note they refrained from adding extensive math and coding content during initial pretraining which contributes to a performance gap on math/coding relative to GPT-4.",
            "benchmark_name": "GSM8K (grade-school math word problems), MATH (competition-level math problems)",
            "task_type": "multi-step arithmetic word problems / mathematical problem solving",
            "problem_format": "natural-language math word problems (GSM8K: grade-school-level, MATH: competition-style), evaluated with and without Chain-of-Thought prompting",
            "difficulty_level": "GSM8K: grade-school, multi-step; MATH: harder competition problems (higher difficulty)",
            "prompting_method": "For chat models, Chain-of-Thought (CoT) prompting was used for zero-shot evaluations on GSM8K and BBH; base-model evaluations used few-shot settings described in benchmark protocols (e.g., 8-shot for GSM8K in base model runs in the paper).",
            "performance_metric": "pass@1 accuracy (percentage exact-correct answers) for GSM8K and MATH.",
            "performance_value": "Reported values (from paper tables): Yi-6B GSM8K pass@1 ≈ 32.5%; Yi-34B GSM8K pass@1 ≈ 67.2% (Table 3). MATH performance for Yi-34B is reported as low (≈ 4.6% in Table 3), indicating strong disparity between grade-school and competition-level math. For chat models, Yi-Chat-34B shows substantial GSM8K improvement in Table 4 (chat 34B rows) and authors note Yi-34B-Chat performs well on GSM8K and the Hungarian exam; exact chat numbers are given in Table 4 per metric.",
            "internal_analysis": "Authors attribute better GSM8K performance to model scale and SFT strategy; they also caution prompt sensitivity and evaluation pipeline differences across models. They note CoT is used to guide deliberation for GSM8K in chat-mode evaluation. No internal neuron-level mechanistic analysis of arithmetic processing is provided.",
            "failure_modes": "Large gap on MATH (hard competition problems) despite good GSM8K results; smaller models fail to show mathematical capabilities without additional data or SFT; prompt sensitivity — different prompts and post-processing can materially affect scores; the paper notes general gaps vs GPT-4 on reasoning-related and code/math tasks.",
            "scaling_trend": "Strong positive scaling: Yi-34B substantially outperforms Yi-6B on GSM8K; authors link mathematical and coding performance increases more strongly to model size than to some other tasks (commonsense, reading). They also report that finetuning and continued pretraining can further improve math ability.",
            "uuid": "e6382.2",
            "source_info": {
                "paper_title": "Yi: Open Foundation Models by 01.AI",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Hungarian exam eval",
            "name_full": "Evaluation on 2023 Hungarian high-school mathematics final exam",
            "brief_description": "The authors evaluated Yi-34B-Chat on the 2023 Hungarian high‑school mathematics final exam (used previously in community evaluations) as a test of real-world mathematical ability and overfitting; Yi-34B-Chat is described as performing 'inspiringly' while Yi-6B-Chat performs poorly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Yi-Chat-6B, Yi-Chat-34B",
            "model_family": "decoder-only Transformer chat models (SFT fine-tuned Yi variants)",
            "model_size": "6B (chat), 34B (chat)",
            "training_data_description": "Same Yi pretraining mixture with &lt;10K high-quality instruction SFT examples; authors emphasize SFT data quality and small size rather than huge-scale instruction tuning.",
            "benchmark_name": "2023 Hungarian high-school mathematics final exam (reproduced by Paster / xAI Grok community)",
            "task_type": "real-world multi-step math exam questions (mixture of algebra, geometry, problem solving)",
            "problem_format": "natural-language exam items (some require multi-step derivations and proofs or numerical solutions)",
            "difficulty_level": "High-school final exam (multi-step, moderate to high difficulty)",
            "prompting_method": "Chat-mode instruction-following; specific prompting not exhaustively detailed, but as a chat model evaluation authors likely used instruction prompts and possibly CoT-style prompting for deliberation.",
            "performance_metric": "Exam score (percent or raw score) — paper gives qualitative descriptor and figure (Figure 4) rather than detailed per-question numeric table in the main text.",
            "performance_value": "Qualitative: Yi-34B-Chat 'performs inspiringly' per authors' description and Figure 4; Yi-6B-Chat does not exhibit strong mathematical capabilities. No single numeric aggregate reported in-text besides the figure visualization.",
            "internal_analysis": "Presented as an external stress-test to check math overfitting; authors interpret the good Yi-34B-Chat result as evidence of genuine mathematical capability rather than dataset memorization. No internal mechanistic probing of how the model computes solutions is provided.",
            "failure_modes": "Smaller chat model (6B) fails to perform well; authors imply methods like more SFT, larger model size, or continued pretraining are required. No detailed failure-mode breakdown (e.g., algebraic symbol errors) provided.",
            "scaling_trend": "Evidence that SFT plus larger model scale (34B) improves performance on a challenging real-world exam; smaller models underperform, supporting the paper's broader claim that reasoning capability correlates with model scale when pretraining data is held fixed.",
            "uuid": "e6382.3",
            "source_info": {
                "paper_title": "Yi: Open Foundation Models by 01.AI",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Are emergent abilities of large language models a mirage?",
            "rating": 2,
            "sanitized_title": "are_emergent_abilities_of_large_language_models_a_mirage"
        },
        {
            "paper_title": "Data engineering for scaling language models to 128k context",
            "rating": 2,
            "sanitized_title": "data_engineering_for_scaling_language_models_to_128k_context"
        },
        {
            "paper_title": "Training Verifiers to Solve Math Word Problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Take a step back: Evoking reasoning via abstraction in large language models",
            "rating": 2,
            "sanitized_title": "take_a_step_back_evoking_reasoning_via_abstraction_in_large_language_models"
        }
    ],
    "cost": 0.01593375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>21 Jan 2025
21 Jan 20257101AECA4723501CCE1B5B34FA10D146arXiv:2403.04652v3[cs.CL]
We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities.The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models.Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena.Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts.For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline.For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers.For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model.We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance.We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance.We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.</p>
<p>Introduction</p>
<p>Recent breakthroughs in large language models have revolutionized the whole field of artificial intelligence and potentially radiate across the entire human society.Our vision for large language models is to make them the next generation computational platform and empower the whole community with significantly amplified intelligence.As a step towards this mission, we present the Yi model series, 6B and 34B language models pretrained from scratch on 3.1T highly-engineered large amount of data, and finetuned on a small but meticulously polished alignment data.Due to the data quality resulting from our substantial engineering efforts, which we will detail in the upcoming sections, Yi achieves near GPT-3.5 benchmark scores and human preferences.</p>
<p>In designing the Yi model series, we are mostly concerned on the following dimensions regarding model scale, data scale, and data quality: (1).when choosing model scale, the desiderata is to have small enough model that is feasible for inference on consumer-grade hardware like the RTX 4090 where the bounding factor is its limited 24G memory, yet still large enough with complex reasoning and emergent abilities.This is why we found 34B gives a nice performance-cost balance; (2).since 34B is smaller than the conventional 70B used by Chinchilla [30] and LLaMA [77], we increase the pretrain data scale to 3.1T tokens to compensate for the decreased compute flops.This makes the model-data scale combination fall into the post Chinchilla optimal regime [64], i.e., we overtrain the model on more tokens (3T) than the compute optimal (around 1T).The benefit is from the inference side, as we achieve stronger performance with reduced serving cost: after int4 [81] quantization, one can serve the 34B chat model on 24G GPU memory with almost no performance drop; (3).our data engineering principle is to promote quality over quantity for both pretraining and finetuning.The pretraining data quality is guaranteed by a sophisticated data cleaning pipeline with cascaded filtering methods and intentionally increased deduplication strength; (4).for finetuning data we heavily emphasize quality by handcrafting less than 10K instructions over multiple iterations based on user feedback.This approach significantly deviates from the quantity-scaling styled instruction tuning works like FLAN [9] and UltraChat [19], but aligns more with the handcrafting styled works like LIMA [94].</p>
<p>Our pretraining data cleaning system features a sophisticated filtering pipeline based on language, heuristic textual features, perplexity, semantics, topic, and safety, as well as a cascaded deduplication process based on paragraph, MinHash, and exact matching.This thorough pipeline leads to a much higher removal ratio than existing pipelines like CCNet [80], RefinedWeb [56] and RedPajama [13], which we believe is key to the success of data engineering.The underlying principle is although pretraining requires data scaling, one would like to make sure the data used are of high quality, rather than training the model on large raw data, i.e., we prefer 3T tokens over sophasticated engineering over 10T tokens without extensive filtering.Regarding the model architecture, we use standard implementation of the Transformer architecture with Grouped-Query Attention (GQA) [1], SwiGLU [68] activation, and RoPE with an adjusted base frequency (RoPE ABF) [82].This design choice is the standard approach rooted from the Transformer original paper [78], later modified by GPT-3 and Chinchilla [30], then followed by LLaMA [77], Baichuan [84], Qwen [3] and many related works.</p>
<p>To approach GPT-3.5-matchinghuman preferences, our finetuning dataset is curated from carefully selected multi-turn instruction-response pairs, annotated directly by our team of machine learning engineers then polished over multiple iterations of user feedback.As mentioned above, the size of our finetuning dataset is less than 10K, but improved over and over again across the model development timeline.Benefiting from the dataset's manageable size, we employed an extensive grid search to identify the optimal data composition, promote diversity, and discover effective hyperparameters.After 8-bit and 4-bit quantization, the final chat model can be deployed on consumer-grade GPUs nearly without performance degradation compared to the bf16 format.</p>
<p>We further extend the Yi model capability from three dimensions: context scaling, vision-language adaptation, and depth-upscaling.To achive 200K context length, we continue pretrain the model on about 5B length-upsampled data, similar to the concurrent work in Fu et al. [22].To adapt the model to vision-language tasks, we integrate a vision encoder and develop a multi-stage training method, following and improving the practice of Liu et al. [47].We also study the effectiveness of depth-upscailng [38], i.e., making the model deeper by continual pretraining, and confirming its effectiveness to further improve model performance.Our infrastructure provides strong support for the full-stack development of the Yi model series, from pretraining to finetuning to serving.To support pretraining, we develop cross-cloud elastic task scheduling, automatic failure recovery, and topology-aware resource allocation which collectively enable us to run tasks according to the real-time available GPU nodes cross clusters with limited switching overhead.To support finetuning, we build a hierarchical scheduling framework supporting different distributed backends for different models (e.g., Megatron [70] for the policy model and DeepSpeed [60] for the reward model).For efficient inference, we use 4-bit model and 8-bit KV cache quantization, combining with PagedAttention [41] and Dynamic Batching.</p>
<p>Extensive experiments demonstrate that Yi-34B can match GPT-3.5 in both performance and efficiency.On most standard benchmarks like MMLU [27] (for the base model) and LMSys ELO Rating [93] (for the chat model), Yi-34B generally achieves scores on par with GPT-3.5.After model parameter and KV cache quantization, the inference cost is also controlled such that a wide range of the community can deploy the model on cost effective devices.We further report a detailed performance comparison between Yi and major LLMs on commonsense reasoning, college exams, math, coding, reading comprehension, and human preference win-rate on multiple evaluation benchmarks.</p>
<p>Since its release, the Yi model series has benefited the community from the following perspectives: (1). it provides GPT-3.5-matchingquality yet cost-effective models to researchers, and enables developers to build AI-native applications like language model based agents; (2). it empowers end users with locally runnable chatbots, which consequently helps protecting user data privacy; (3). it sheds light on the direction on further data and model scaling to achieve even stronger frontier models.for both research and commercial use.</p>
<p>Pretraining</p>
<p>Our approach to pretraining is to train a standard dense transformer architecture on a heavily engineered large pretraining corpora, where our underlying assumption is that when trained on extensive data of high-enough quality, a standard architecture can exhibit advanced capability.This is to say, we may not need much architectural modification, although we have indeed conducted extensive preliminary architectural experiments.In the following subsections, we first detail our data engineering pipeline, then briefly discuss the model architecture.</p>
<p>Data Processing</p>
<p>The Yi data mixture is shown in Fig. 2. To produce a high-quality bilingual pretraining data, we meticulously designed a cascaded data-processing pipeline, as illustrated in Fig 1 .This pipeline features a series of data-cleaning strategies targeting quality and diversity.We start with web documents from Common Crawl, use the CCNet pipeline [79] for language identification and perplexity scoring.Then we use a combination of filtering and deduplication process, as detailed below.</p>
<p>Heuristic Rule Filters This part of filter aims for removing text of low quality.We filter out text based on: (1).URL, domain, word blocklists and garbled text filters; (2).document length, the ratio of special symbols, and the ratio of short, consecutive, or incomplete lines; (3).repeated words, n-grams, or paragraphs [58]; The filtering thresholds are based on a statistical analysis of large document samples, as described in Nguyen et al. [52].Furthermore, we identify and anonymize Personal Identifiable Information (PII), such as email addresses and phone numbers.Overall our data consist of 3.1T high-quality tokens in Both English and Chinese, and come from various sources.Our major differences from existing known mixtures like LLaMA [76] and Falcon [56] are that we are bilingual, and of higher quality due to our more rigorous cleaning pipeline.</p>
<p>Learned Filters We use learned filters to address nuanced cases that exceed the capabilities of standard heuristic rules.Notably, the Chinese content extracted from Common Crawl present unique challenges, particularly with a higher ratio of inappropriate content like pornography and gambling.Traditional heuristic-rule-based filters struggle to effectively identify and eliminate all harmful content.To enhance our filtering process, we have integrated a suite of learned scorers for filtering, namely the perplexity scorer, quality scorer, safety scorer, and document coherence scorer: (1). the Perplexity Scorer, utilizing the KenLM library as per CCNet [80], evaluates a vast array of web documents, discarding those with perplexity scores largely above average; (2). the Quality Scorer is a classifier trained to recognize and favor pages similar to Wikipedia in quality and assign scores accordingly.Documents that fail to meet the quality standard are subsequently removed; (3). the Document Coherence Scorer identifies low-quality web documents that consist of disparate sentences or paragraphs, thus being incoherence.Such documents are either segmented for further analysis or removed entirely.(4). the Safety Scorer identifies and removes web documents containing toxic content, such as violence, pornography, and political propaganda.</p>
<p>Cluster-based Filters We further use unsupervised semantic clustering to group web documents.This clustering process enables efficient identification and analysis of documents sharing similar semantic features.The clustered data are subsequently annotated with quality labels, providing essential references for the optimization of Yi's data mixture strategy.Documents identified as low-quality through automatic and manual verification are excluded from the dataset.</p>
<p>Deduplication After filtering, we implement a comprehensive deduplication pipeline following the procedure in Penedo et al. (2023) [56].This pipeline integrates document-level MinHash deduplication and sub-document exact-match deduplication, effectively identifying and removing duplicate content within and across documents.We further categorize web documents into specific themes using a topic model predicting labels like as news, ads, and knowledge-based content.In the final pretraining dataset, we down-sample less helpful content, mostly advertisements, to ensure information density.The final composition of Yi's pretraining data is shown in Fig. 2.</p>
<p>Tokenization</p>
<p>We use byte-pair encoding (BPE) [69] implemented in the SentencePiece framework [40], to tokenize the pretraining data.The vocabulary size of Yi is set to 64,000 to balance computational efficiency and word comprehension.Specifically, we split numbers into individual digits to facilitate a better understanding of numeric data.We allow rare characters to fall back to the unicode-byte encoding to ensure fault tolerance.We employ the identity tokenizer to avoid transferring all punctuations to the half-width format.LLMs prioritizing English usually utilize dummy prefix (whitespace at the beginning of text) in their tokenizers to generalize the same words at different positions of sentences.We do not use this approach because the assumption does not always hold even in the English context, especially for sentences that begin with quotation marks, also it does not show positive effect in Chinese context.</p>
<p>Model Architecture</p>
<p>Yi uses a modified version of the classical decoder-only Transformer architecture [78] where the code is based on LLaMA's [77] implementation.The main parameter setting is summarized in Table 1.</p>
<p>The modifications from LLaMA to Yi are further summarized below:</p>
<p>Attention Mechanism LLaMA 2 uses Grouped-Query Attention(GQA) [1] only on its largest 70B model, and its 7B and 13B uses full attention.We incorporate GQA in both Yi-6B and Yi-34B.GQA splits query-heads into G groups, sharing a single key and value head within each group of query [1].This approach offers substantial reductions of training and inference costs, compared to the original Multi-Head Attention (MHA) [16,57,67].We do not observe performance degradation after applying GQA to our 6B smaller model.</p>
<p>Activation Function We use SwiGLU [68] as Yi's post-attention layer, reducing its activation size from 4h to 8/3h (h denotes hidden size) to be consistent with the normal post-attention layer.This adjustment also compensates for the reduction in parameter resulted from GQA, making the overall parameter count comparible of existing 7B and 34B models.</p>
<p>Positional Embedding and Long Context</p>
<p>We use Rotary Position Embedding (RoPE) [73] following the standard implementation.We adjust the base frequency (RoPE ABF), introduced in Xiong et al. [82], to support long context windows up to 200K where the base model itself is trained on 4K context length.To adapt the base model to longer context, we continue pretrain the model on 10B tokens from our pretraining data mixture with slightly upsampled long sequences, mostly from book.We observe that only 1-2B tokens is enough for the model to converge to low loss on 4K-200K length, and a lightweight finetuning further induces near-perfect long-context retrieval performance.Based on this observation, we tend to view that the capability of modeling longer dependency than the pretrained length (4K) is a intrinsic capability (rather than an being injected by post-train).This is to say, the base model already has the capability to model longer than 4K dependency even the model is trained shorter, and the post-train / finetuning procedure simply release this capability.</p>
<p>Finetuning</p>
<p>Our finetuning method significantly emphasizes data quality over quantity.Our approach does not follow existing data-intensive approaches like FLAN [9] and UltraChat [19], which scales the SFT data to millions of entries but each of the entries may not been examined carefully because the scale is too large.In contrast, our method aligns with the LIMA [94] and DEITA [48] approach, which focus on data selection rather than scaling.With the scale being less than 10K, we are able to examine and optimize every single data point.Below we discuss our data construction and training details.</p>
<p>Data Preprocessing</p>
<p>Quality is All You Need Our finetuning dataset consists of less than 10K multi-turn instructionresponse dialog pairs, with each and every one of the entry constructed and polished over multiple iterations and from user feedback.We take this approach because in our preliminary experiments, we observe that compared to the open-source data of several hundred thousand entries, the results from a smaller, manually annotated dataset are superior.These observations align with those reported in Gemini Team et al. [23], Touvron et al. [77], Zhou et al. [94].</p>
<p>We use the following techniques to improve prompt distribution selection, response formatting, and chain-of-thought formatting: (1).for prompt distribution selection, drawing inspiration from WizardLM [83], we develope compound instructions and progressively evolved them to increase their complexity.This approach has significantly reduced the size of SFT data in our experiments; (2).for response formatting, we generally use a default style extended from LIMA [94].Overall, the responses are structured in an introduction-body-conclusion format where the body is usually a list of bullet point; (3).for CoT data formatting, we have use a "Step-Back" pattern, inspired by Zheng et al. [92], by performing abstraction to formulate higher-level solutions before delving into reasoning about the original, more concrete questions.</p>
<p>We spend extra efforts on reducing hallucination and repetition: (1). to reduce hallucinations, we examine and ensure that the knowledge in the responses is not contained within the model, and eliminate responses that might lead to memorization; (2). to reduce repetition, we rewrite the repetitive turns of the responses that usually exist but may be overlooked in the finetuning data.</p>
<p>Diversity and Mixture To ensure the coverage of different capabilities, we have included a wide spectrum of open-source prompt, encompassing areas such as question answering, creative writing, dialogue, reasoning, mathematics, coding, safety, bilingual capabilities, and others.</p>
<p>To obtain a fine-grained control of different directions of capabilities, inspired by InsTag [49], we develop a instruction tagging system.By designing a diversity-focused sampling algorithm, we carefully balanced the distribution of instructions across various tags.This approach ensures a diverse finetuning dataset, aiming to achieve enhanced cross-task robustness.</p>
<p>To achieve the optimal data ratio for balancing different directions of the capability, we use an approximate grid search to determine our data mixture.Motivated by Dong et al. [20], this process involved experimenting with {1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64} proportions for each ability.The search process was guided by validation results and our in-house human evaluation sets.</p>
<p>ChatML Format Beyond the focus on data quality and diversity, our observations revealed that the format of the data substantially influences the model's ultimate performance.To this end, we implemented the ChatML-style format [53].This structured approach empowers the model to differentiate among various information types, such as system configurations, user inputs, and assistant responses.</p>
<p>Training Method</p>
<p>We use next-word prediction loss for finetuning, and only compute loss on the responses, but not system and user instructions.We use AdamW optimizer with β 1 set to 0.9, β 2 set to 0.999, and ϵ set to 10 −8 .We use a sequence length of 4096, alongside a batch size of 64.We set training step to 300 with a constant 1 × 10 −5 learning rate, a weight decay of 0.1, gradient clipping with a maximum threshold of 1.0, and NEFTune [34] with a noise scale of 45 for Yi-34B-Chat and 5 for Yi-6B-Chat.</p>
<p>Infrastructure</p>
<p>We build the infrastructure supporting the full-stack data processing, pretraining, finetuning, and serving.Our infrastructure feasures: (1).automated managing and monitoring the computing resource; (2).improved the training speed from optimized parallel strategies, kernel efficiency, and long-context support; (3).unified finetuning framework supporting heterogeneous distributed training backend, such as simultaneously using Megatron and DeepSpeed for multiple models in Direct Preference Optimization (DPO) [59]; (4).reducing the deployment cost by various LLM serving accelerations such as quantization, continuous batching, and paged attention.Below we explain these techniques one by one.</p>
<p>Computing Resources Management To efficient schedule large-scale language model development, particularly pretraining, which may take months on thousands of GPUs , we build a highly efficient multi-cloud task scheduling algorithm to manage pre-training, SFT, and RLHF tasks of different priorities.We also build a high-performance in-house training framework that allows us to automatically elastic scale the pre-train jobs to different node sizes based on the GPU availability.</p>
<p>More importantly, all the training-related hyper-parameters will be scaled at the same time seamlessly.During the large language model training stage, a wide range of failures regularly occur, ranging from GPU crashes to communication fabric errors to loss spikes.We use the following strategies to address these reliability challenges: (1) we apply automated inspection, prediction, and labeling of nodes for different kind of software/hardware error categories.Nodes marked as tainted will be temporarily removed from the resource pool until the errors got cleared.( 2) we implement a task queuing system with pre-checks and the capability for fast, automatic recovery in the event of failures during training tasks.</p>
<p>(3) we develop of a user-friendly multi-task submission and management console, enabling developers to seamlessly manage and track their training tasks and hyper-parameters.</p>
<p>Performance and Cost Efficiency Memory and communication restrictions are the two major technical challenges of large scale model training requiring integrated solutions beyond adding more GPUs.We use and improve upon the following techniques to tackle the memory and communication restrictions: (1) ZeRO-1 [60] to remove the memory consumption by partitioning optimizer states cross data-parallel processes; (2) tensor parallel combined with pipeline parallel [70] within each compute node to avoid inter-node communication bottleneck, and the 3D parallel strategy is well designed and optimized to avoid using activation checkpointing and minimize the pipeline bubbles;</p>
<p>(3) kernel fusion techniques like flash attention [15][14] and JIT kernels to reduce redundant global memory access and consumption; (4) topology-aware resource allocation (ranking strategy) to minimize the communication across different layers of switches, which is the limitation of a typical fat-tree-topology.</p>
<p>Finetuning Framework Different from pretraining, finetuning LLMs may require the orchestration of multiple models, as is the practice of DPO [59] and PPO [54].In such training jobs, a typical process is to use reference/reward model to predict a batch of data (which also requires nontrivial time), then let the target model use this data to calculate loss and update parameters.To this end, we build a multi-model scheduling framework to support multiple backends for different LLMs in a single job.For example, when finetuning a language model with DPO, the intermediate results from the reference model can be cached and reused, improving the training speed and resource cost to be close to the supervised finetuning counterparts.</p>
<p>Fast and Efficient Inference</p>
<p>We primarily use quantization, dynamic batching, and Paged Attention for improving decoding speed and memory usage.We use quantization to decrease both the memory footprint and computation demand.By 4-bit model quantization [81] and 8-bit KV cache quantization [18], we are able to achieve significant GPU memory saving with near-zero performance degradation (e.g., less than 1% accuracy drop in MMLU/CMMLU benchmark).We use dynamic batching [86] to minimize the response time and improve batching efficiency.We use PagedAttention [41] to improve memory utilization and improve decoding.</p>
<p>Long-context Window Support</p>
<p>We implement and improve computation-communication overlapping, sequence parallelism, and communication compression to support up to 200K context length continue pretraining and finetuning.Our method to scale the context length to 200K is solely based on engineering, that is to say, we do not modify the model architecture like sparse, local, or sliding window attention -the model remains using the full attention even the input is 200K.</p>
<p>Safety</p>
<p>To enhance the model's trustworthiness and safety, we develop a full-stack Responsible AI Safety Engine (RAISE).RAISE ensures safe pretraining, alignment, and deployment.This section discusses our safety measures in the pretraining and alignment stages.</p>
<p>Safety in Pretraining</p>
<p>Aligning with standard pretraining data safety practices [5,58,77], we build a set of filters based on heuristic rules, keyword matching, and learned classifiers to remove text containing personal identifiers and private data, and reduce sexual, violent, and extremist content.</p>
<p>Safety in Alignment</p>
<p>Informed by existing research in [24,35], we first build a comprehensive safety taxonomy.This taxonomy covers a broad spectrum of potential concerns, including environmental disharmony, superstitious, religious sensitivities, discriminatory practices, substance abuse, violent behavior, illegal activities, hate speech, ethical violations, privacy breaches, self-harm, sexually explicit content, mental health issues, and cybersecurity threats.We curated datasets reflecting these categories for a robust alignment, and mix them with our dialog SFT data.We also include a targeted set of prompts simulating attack scenarios in the alignment phase, which effectively improved the model's resilience against malicious use.</p>
<p>Evaluations</p>
<p>Our evaluation demonstrates that the Yi model family achieves inspiring performance on a wide range of tasks and delivers close to GPT-3.5 user preference rate.We first report the base model performance on standard benchmarks, then we discuss the chat model performance and its user preference rate.</p>
<p>6.1 Base Model Performance</p>
<p>Main Results</p>
<p>Here we present the results for our base models and several other well-known base models across standard academic benchmarks.While benchmarking open-source models, we observed a disparity between the results generated by our pipeline and those reported in public sources.Upon conducting a more in-depth investigation of this difference, mostly because different models use different prompts, post-processing strategies, and sampling techniques.These differences may potentially induce significant variations in the outcomes.Our prompt and post-processing strategy remains consistent with the default settings of the original benchmarks [2, 4, 7, 8, 10-12, 27, 28, 42, 50, 61-63, 72, 74, 75, 89, 90].We use greedy decoding without any post-processing for the generated content.For scores that were not reported publicly (or scores reported with different settings), we try to get results with our pipeline.For scores that can be found publicly, we directly report the existing numbers.We use the following benchmarks, largely following the practice of LLaMA 2 [77]:</p>
<p>Commonsense Reasoning: We included PIQA [4], SIQA [63], HellaSwag [89], WinoGrande [62], ARC [11], OpenBookQA(OBQA) [50], and CommonsenseQA(CSQA) [75] to assess common sense reasoning.CSQA was exclusively tested using a 7-shot setup, while all other tests were conducted with a 0-shot configuration.Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD [61],</p>
<p>QuAC [8], and BoolQ [10].Math: We report the average of the GSM8K [12] (8 shot), and MATH [28] (4 shot) benchmarks with pass@1 accuracy without any specific prompting strategy (e.g.Chain-of-Thought prompting) and other ensemble technique (e.g., majority voting).</p>
<p>Model</p>
<p>Size GSM8k MATH Human-Eval pass@1 MBPP pass@1 GPT- Popular Aggregated Benchmark: We report the overall results for MMLU <a href="5-shot">27</a>, CMMLU [42] (5-shot), Gaokao-Bench [90] (5-shot), and BigBench [72] Hard (BBH [74]) (3-shot).</p>
<p>By training on a significantly larger number of tokens (3.1T) compared to prior work (usually ≤ 2T), we have observed a substantial performance gain across benchmarks, as shown in Table 2.However, it is important to note that there are still discernible disparities between our model and existing open-source and close-source models, particularly in tasks related to mathematics and coding.As performance in these domains can be significantly improved by continual pretraining and instruction fine-tuning, we have refrained from incorporating extensive mathematical and coding content in the pretraining corpus when making the initial design choices.We do plan to release models with enhanced math and coding capabilities in the future.</p>
<p>Discussions</p>
<p>Gain from Model Scale.We observe that Yi-34B has substantial performance improvement compared to Yi-6B, though they utilized the same pretrain corpora.Larger model size leads to higher performance gain on Code and Math benchmarks, referring to Tab. 3, compared to benchmarks focusing on Commonsense Reasoning, Reading Comprehension, or Knowledge.</p>
<p>Data Quality.Smaller models of higher quality pretrain data, like Yi-34B or Qwen-14B, usually demonstrate better performance than models of larger size but (presumably) lower quality data, such as Falcon-180B (though the focus of Falcon-180B might be more on the scaling side, which is definitely of important value on its own).</p>
<p>Gap between GPT-4 and Open-source LLMs.Based on Tab. 2, we note that open-source LLMs still lag behind the performance of GPT-4 and GPT-3.5 on various benchmarks.Yet representative bilingual LLMs, e.g.Qwen-14B and Yi-34B, can match or even surpass the performance of GPT-4 on Chinese knowledge related benchmarks, including C-Eval [31], CMMLU [42], and Gaokao [90].However, there is still a huge gap between GPT-4 and open-source models on reasoning-related benchmarks like BBH [72], code (HumanEval), and math (MATH).Considering the discussions of whether emergent ability is an artifact of measurement [65], we use difference to the target (target number -model prediction) as a continuous measure, and exact match (target number == model prediction) as a discontinuous measure.A: when there is two linear coefficients, Yi-34B performs the best when measuring by the difference to the target number.B: increasing the number of linear coefficients to 5, only models that are large enough (LLaMA2 70B and Mixtral 8x7B) can achieve meaningful exact match, showing that in-context learning complex functions is an emergent ability.</p>
<p>In-Context Learning Study</p>
<p>We further investigate the in-context learning capability, i.e., the capability of inferring the underlying function given the few-show input-output demonstrations.We consider the task of inferring the linear coefficient of a weighted sum.Specifically, define y = w 1 x 1 + w2x 2 + ... + w n x n , our few-shot demonstration is x 1 , x 2 , ..., x n , y, and we ask the model to (implicitly) infer w 1 , w 2 , ..., w n by predicting the y given a new set of input x.We use (a). the absolute difference between model prediction y and the ground truth y * , i.e., |y − y * | as a continuous measure, and use (b). the exact match y == y * as a discontinuous measure.We further note that most of the models perform reasonably well on addition and subtraction, so the ability to do arithmetic, as a confounding factor, can be ruled out.</p>
<p>The results are shown in Figure 3.When setting the linear coefficients of be [1, -1], we see that Yi 34B and LLaMA-2 70B performs the best in-terms of answer exact match.If we increase the number of the linear coefficients to be [1, 1, 1, 1, 1], we observe the emergent behavior that only large models (LLaMA-2 70B and Mixtral) can achieve good scores on exact match, although the differences to target is more continuous.These observations give side evidence for Yi-34B's performance on in-context learning and indicates that further scaling may allow the model to infer more complicated functions by in-context learning.</p>
<p>Chat Model Performance</p>
<p>In this section, we report the automatic and human preference evaluation of the Chat Model.We use greedy decoding to generate responses.For the automatic evaluation benchmarks, we extract answers from the model's generated outputs and calculate accuracy.During the evaluation process, we observed that different prompts have varying influence on results.Therefore, for the same set of questions, we use identical prompts to evaluate all models, aiming to ensure as fair and unbiased results as possible.</p>
<p>Automatic Evaluations</p>
<p>For automatic evaluation, we use the same benchmarks as is for the base model, detailed in Sec.6.1.1.</p>
<p>We use both zero-shot and few-shot methods but generally, zero-shot is more suitable for chat models.</p>
<p>Our evaluation involves generating responses while following instructions explicitly or implicitly (such as the format in the few-shot examples).We then isolate relevant answers from the generated text.Unlike the base model, for the zero-shot evaluations on the GSM8K and BBH datasets, we employ the Chain-of-Thought (CoT) approach to guide the model in deliberation before reaching an answer.</p>
<p>The results shown in Tab. 4 demonstrate the effectiveness of our chat models in understanding human instructions and generating appropriate instruction-following responses.We particularly highlight the 4-bit quantization results, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop.This observation serve as the foundation of serving the model on consumer-grade devices.</p>
<p>In line with Goodhart's principle, when a measurement metric becomes the target of our pursuit, it ceases to serve as a reliable standard of assessment.Consequently, the outcomes of our evaluations on benchmarks are exclusively employed for ensuring that our alignment training does not detrimentally impact the foundational knowledge and capabilities of the base model.We do not engage in targeted optimization of our chat model with the objective of enhancing benchmark performance.</p>
<p>To further evaluate the generalizability of our model's capabilities, we conducted assessments of its mathematical computation proficiency by subjecting it to the 2023 Hungarian high school mathematics final exam questions, first proposed by the xAI Grok team then reproduced by Paster [55].This evaluation was undertaken with the aim of determining whether our model exhibited signs of overfitting to training datasets that are mathematically oriented.The results in Fig. 4 show that Yi-34B-Chat performs inspiringly on both the GSM8K and the Hungarian mathematics exam.However, note that Yi-6B-Chat does not exhibit strong mathematical capabilities (on both GSM8K and the Hungarian mathematics exam).We speculate that smaller models may require more data to activate their corresponding abilities during the SFT stage.</p>
<p>Human Evaluations</p>
<p>In this section we conducted an assessment of the model's conversational abilities, considering aspects to ensure its effectiveness and safety.We have compiled a collection of open-source evaluation datasets from the community, such as alpaca-eval [21], Belle-eval [88], and MT-bench [93].Additionally, we have established our own helpful and harmless evaluation dataset by gathering and constructing data of varying difficulty levels, for the purpose of comprehensively assessing the conversational abilities of chat models.</p>
<p>However, whether it is a public evaluation set or a self-built evaluation set, the evaluation results are strongly influenced by the assessment criteria and the design of the prompt.Our internal evaluation results may be unfair to other models, making it difficult to accurately represent the true capability level of our model.Therefore, here we only present external evaluation results to demonstrate the current conversational abilities of our chat model.We consider: (1).AlapcaEval1 [44], which is designed to assess the English conversation capabilities of models by comparing the responses of a specified model to reference replies from Davinci003 [21] in order to calculate a win-rate; (2).LMSys2 [93] Chatbot Arena, which showcases the responses of different models through a dialogue platform, then asks users to make selections based on their preferences, then computes the Elo score;   (3).SuperClue3 , on the other hand, is a leaderboard aimed at comprehensively evaluating the Chinese language capabilities of models.</p>
<p>Tab. 5 presents the performance results of Yi-34B-Chat in the three third-party evaluations we consider, with the cutoff date for the results being December 21, 2023.The data demonstrates that, although there is still a gap compared to GPT-4, our model exhibits proficient bilingual (Chinese and English) dialogue capabilities and aligns well with user preferences.Additional comparative results of various models are accessible for review on the official website.We further demonstrate the data quality by comparing the speed of preference increase during data scaling.As is shown in Fig. 5, when compared with UltraChat [19] and its cleaned version UltraChat 200K, we see a clear tendency of performance improvements when scaling up the Yi data.</p>
<p>Capability Extension</p>
<p>In this section, we discuss our post-training methods to extend the Yi base model to 200K long-context, equip it with visual understanding capability, and enhance the 6B model by depth upsacaling.</p>
<p>Long Context Modeling</p>
<p>Our long-context solution consists of a continual pretraining and a finetuning phase, both are lightweight.We hold the basic hypothesis that the potential of utilizing information anywhere within the 200K input context is already exist in the base model (same as Fu et al. 22), the continue pretraining phase "unlocks" such capability, evidenced by a strong performance on Needle-in-a-Haystack test, then the finetuning phase further adapt the style of response to follow human instruction and preference.</p>
<p>Continue Pretraining</p>
<p>We continue pretrain the full-attention model using sequence parallelism [43] and distributed attention.This is to say, we do not use any sparse or linear attention, but use a brute force implementation of the full attention.We continue pretrain the Yi 6B/ 34B base model on the data mixture of (1).original pretraining data, as is introduced in section 2;</p>
<p>(2). length-upsampled long-context data, where the long documents are mostly from books; (3).multi-document question-answering synthetic data, where we construct QA pairs where the answer contains a recitation of the related paragraph before the answer.Our data approach mostly follows the data engineering practice in Fu et al. [22] and Yu et al. [87].We continue pretrain the model on 5B tokens with 4M batch size, which translate to 100 optimization steps.Aligning with the concurrent work from Fu et al. [22], we observe that such light-weight continue pretraining is already able to enable a strong performance on Needle-in-a-Haystack test, as we will show in Figure 6.</p>
<p>Supervised Finetuning We mix our short-context SFT data with long-context document questionanswering data.We use model-assisted automated methods (i.e., synthetic data) to construct document QA.Specifically, we randomly concatenate multiple documents into a sequence, sample one or more paragraphs from the long sequence, and ask a chat model to construct question and answer pairs  based on the sampled paragraph.One important detail is recitation and rephrasing: before giving the answer, we ask the model to recite or paraphrase the original paragraph.This data format encourages the model's retrieval behavior and consequently discourages the hallucination behavior: given a question, the model is more likely to use the information within the input to construct the answer, rather than use its internal knowledge, which may be related but inaccurate.Our finetuned model is deployed at www.wanzhi01.com,and we encourage the readers to try it out.</p>
<p>The performance of the 200K models is shown in figure.6 and table 6.Specifically, Figure 6 shows the famous Needle-in-a-Haystack test of Yi-34B-200K, though we tend to view that this level of retrieval is relatively easy for long-context LLMs.Table 6 shows that our context scaling does not significantly influence the short-context generic capability.</p>
<p>Vision-Language</p>
<p>In the burgeoning field of multimodal research, the integration of image understanding capabilities into large language models has become increasingly viable.Drawing inspiration from the opensourced LLaVA [46,47], we present Yi Vision Language (Yi-VL) models, i.e., Yi-VL-6B and Yi-VL-34B, based on Yi-6B-Chat and Yi-34B-Chat language models.The architecture of Yi-VL models, as illustrated in Figure 7, comprises three primary modules.The Vision Transformer (ViT), used for image encoding, is initialized with CLIP ViT-H/14 model [33].A Projection Module, designed to align image features with text feature spcae, consists of a two-layer Multilayer Perceptron (MLP) with layer normalizations.Finally, the large language model, initialized with the Yi-Chat models, demonstrating exceptional proficiency in understanding and generating both English and Chinese.To enhance the performance of Yi-VL models in bilingual multimodal understanding and generation, we leverage a rich dataset of bilingual image-text pairs.In the image, there are three cats situated on a stone floor.The first cat, with a mix of black, orange, and white fur, is actively eating from a metal bowl.The second cat, which is entirely black, is also engaged in eating from a separate metal bowl.The third cat, a mix of gray and white, is not eating but is instead looking off to the side, seemingly distracted from the food.The bowls are positioned close to each other, and the cats are all within a similar proximity to the bowls.The scene captures a typical moment of feline behavior, with some cats enjoying their meal while others appear indifferent or distracted.</p>
<p>Image Features Text</p>
<p>Features million image-text pairsn from diverse sources, e.g., CLLaVA [45], LLaVAR [91], Flickr [85], VQAv2 [25], RefCOCO [37], Visual7w [95] and so on.</p>
<p>Stage 3: the parameters of the entire model are trained.The primary goal is to enhance the model's proficiency in multimodal chat interactions, thereby endowing it with the ability to seamlessly integrate and interpret visual and linguistic inputs.To this end, the training dataset encompasses a diverse range of sources, totalling approximately 1 million image-text pairs, including GQA [32],</p>
<p>VizWiz VQA [26], TextCaps [71], OCR-VQA [51], Visual Genome [39], ShareGPT4V [6] and so on.To ensure data balancing, we impose a cap on the maximum data contribution from any single source, restricting it to no more than 50, 000 pairs.</p>
<p>In Stage 1 and 2, we set the global batch size, the learning rate, the gradient clip and the number of epoch to 4096, 1e−4, 0.5 and 1, respectively.In Stage 3, these parameters are adjusted to 256, 2e−5, 1.0 and 2. The training consumes 128 NVIDIA A100 GPUs.The total training time amounted to approximately 3 days for Yi-VL-6B and 10 days for Yi-VL-34B.</p>
<p>Table 7 shows the MMMU test set leaderboard by Yi-VL's release.We note that this area is currently actively under research, aligning with the community's advances, we will continuously improve the update Yi-VL's performance.</p>
<p>Depth Upscaling</p>
<p>Recent studies on scaling laws [29,30,36] have underscored the predictable improvement in model performance with increases in computational budget, model size, and data size.Yet, identifying the most effective distribution of resources between model and data sizes upon expanding the computational budget remains a formidable challenge in the field of scaling laws.Additionally, research conducted by DeepSeek-AI et al. [17] has highlighted that the allocation of an increased computational budget towards model scaling should be proportional to the quality of the data available.In light of these insights, we propose a novel approach aimed at dynamically adjusting the resource allocation between data and model sizes through a series of staged training processes.This strategy iteratively fine-tunes the balance between data characteristics and model size according to scaling laws, enhancing both model training efficiency and performance.Method Following the methodology outlined by Kim et al. [38], our goal is to upscale our Yi-6B base model, which has 32 layers, to a 9B model named the Yi-9B base model, featuring 48 layers, by duplicating the original 16 middle layers 12-28.Depth up-scaling involves expanding the base model's depth and subsequently continuing the pretraining phase for the enhanced model.</p>
<p>Our investigations reveal that the decision on which layers to replicate could be informed by evaluating the cosine similarity scores between the inputs and outputs of each layer.Such an approach allows for targeted model scaling without necessitating additional pretraining, leading only to minimal performance impacts.This minimal impact on performance is attributed to the high cosine similarity, approaching one, between the inputs and outputs of the duplicated layers, as evidenced in Figure 8.This observation suggests that the replication of these layers does not significantly alter the output Table 8: Performance between Yi-6B and Yi-9B: Arc Challenge (25-shot), HellaSwag (10-shot) MMLU (5-shot), Winogrande (5-shot), GSM8K (5-shot), MATH (4-shot), HumanEval pass@1, MBPP pass@1(3-shot).Yi-9B Init is just depthwise upscaling from Yi-6B by duplicating layers 12-28 without further training.logits produced by the original model.This method ensures the efficient scaling of the model by optimizing its architecture based on the internal processing dynamics of its layers.</p>
<p>Continual Training</p>
<p>The dataset is composed of approximately 800 billion tokens across two stages, with around 70% having been recently collected and carefully selected.We have enhanced the code coverage in the final stage to improve code performance.</p>
<p>To optimize the training process, we maintain a constant learning rate of 3e-5, and adopt a strategic approach to gradually increase the batch size from 4M tokens whenever the model's loss plateaued.This incremental adjustment of the batch size, alongside maintaining all other parameters in alignment with the established Yi-6B base model configuration, was instrumental in navigating the challenges of training at scale.</p>
<p>The effectiveness of these strategies is demonstrated in Table 8, which details the Yi-9B base model's performance across a variety of benchmarks, including common sense, reasoning, knowledge, coding, and mathematics.It underscores the competitive advantages of Yi-9B base model in specific domains, illustrating the efficacy of our methodology in enhancing model performance by optimally adjusting the interplay between data characteristics and model size.</p>
<p>Final Discussions</p>
<p>In this report, we discuss the full-stack development of the Yi language model family.Yi-34B achieves GPT-3.5 matching performance and is deployable (thank to the 4/8-bit quantization) on consumer-grade devices, making it an ideal model for local deployment.</p>
<p>The key takeaways from the Yi pretraining procedure are about data quantity and quality: (1).training the model on a larger amount of data than the Chinchilla optimal delivers clear and consistent performance gain, which we highly recommend for all pretraining teams.Our model is trained on 3.1T tokens, yet we belive with larger amount of data, we can continue improve the model performance (i.e., the model have not saturated at 3.1T); (2).when it comes to the pretraining data quality, we believe the most critical two factors are the source of the data (e.g., whether the text is produced for professional usage or for casual social media posting) and the details of the data cleaning (e.g., the strength of filtering and deduplication).Since data cleaning is a very complicated pipeline and it is extremely difficult to conduct extensive grid-search styled optimizations, our current solution may still have room for improvements.</p>
<p>The key takeaways from the Yi finetuning procedure is to heavily iterate on a small amount of data (≤ 10K), case by case, over multiple iterations, directly by the machine learning engineer, and improved from real user feedback.This approach clearly deviates from the instruction-scaling approach, initially introduced by the FLAN series [9] then followed by the UltraChat series [19].</p>
<p>As is demonstrated by our current results, the reasoning capability, which we view as the core capability for real-world deployment of language models, is strongly correlated with model scale when the amount of pretraining data is fixed.We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models in our upcoming next versions.</p>
<p>Figure 1 :
1
Figure 1: Yi's pretraining data cleaning pipeline.</p>
<p>Figure 2 :
2
Figure 2: Yi's pre-training data mixture.Overall our data consist of 3.1T high-quality tokens in Both English and Chinese, and come from various sources.Our major differences from existing known mixtures like LLaMA[76] and Falcon[56] are that we are bilingual, and of higher quality due to our more rigorous cleaning pipeline.</p>
<p>BFigure 3 :
3
Figure3: Evaluating language model's in-context learning capability by inferring the linear coefficients of a weighted sum.Considering the discussions of whether emergent ability is an artifact of measurement[65], we use difference to the target (target number -model prediction) as a continuous measure, and exact match (target number == model prediction) as a discontinuous measure.A: when there is two linear coefficients, Yi-34B performs the best when measuring by the difference to the target number.B: increasing the number of linear coefficients to 5, only models that are large enough (LLaMA2 70B and Mixtral 8x7B) can achieve meaningful exact match, showing that in-context learning complex functions is an emergent ability.</p>
<p>Exam Score vs GSM8k Performance of Various Models</p>
<p>Figure 4 :
4
Figure 4: Yi's result of Hungarian mathematics exam.</p>
<p>Figure 5 :
5
Figure 5: SFT data scaling curve.Compared with UltraChat and its cleaned version UltraChat 200K, our SFT data demonstrates clear scaling advantages.We attribute its steep slope to the data quality.</p>
<p>Figure 6 :
6
Figure 6: Needle-in-a-Haystack performance of Yi-34B-200K.X-axis means length of the document, and Y-axis means the depth of the needle sentence within the document.We continue pretrain the model on 5B tokens long-context data mixture and demonstrates a near-all-green performance.</p>
<p>and what they are doing in detail.</p>
<p>Figure 7 :Stage 2 :
72
Figure 7: Architecture of Yi-VL models.Symbols are used to denote the training status of various modules at three training stages: a fire icon ( ) indicates the parameters of the module are trainable, while a snowflake icon ( ) signifies that parameters are frozen.The image resolution used in ViT at each stage, either 224 2 or 448 2 , is also marked.</p>
<p>Figure 8 :
8
Figure 8: Input/output cosine similarity score of each token per layer for text "Write a quiz about bits".The cosine similarity scores of the 16 newly added layers(layers 28-44), as depicted in the lower figure, are observed to be nearly 1.</p>
<p>Table 1 :
1
Model configs of Yi-6B and Yi-34B.LR stands for learning rate.
Models Hidden Size Q-heads KV-heads Layers Pretrain Seq. LenMax LR6B40963243240963 × 10 −434B71685686040961.5 × 10 −4</p>
<p>Table 2 :
2
Overall performance on grouped academic benchmarks compared to open-source base models.CR stands for Commonsense Reasoning.RC stands for Reading Comprehension.
Size MMLU BBH C-Eval CMMLU Gaokao CRRC Code MathGPT-4-83.086.769.971.072.389.3-65.366.1GPT-3.5-69.170.152.555.551.183.1-54.835.6Qwen14B66.753.472.171.062.574.2 72.5 40.643.1Llama234B 70B62.6 69.744.1 64.9-50.1-53.3-23.371.1 68.9 27.8 72.7 72.3 38.424.2 35.2Baichuan-2 13B55.049.059.061.9745.666.3 62.4 23.416.1InternLM20B62.152.558.859.045.578.3-34.8 30.26Skywork13B62.141.760.661.868.172.4 61.4 64.918.1Falcon180B70.454.057.858.059.074.4---Yi6B 34B63.2 76.342.8 54.372.0 81.475.5 83.772.2 82.872.2 68.7 21.1 80.7 76.5 32.118.6 40.8</p>
<p>Table 3 :
3
Comparison of models on GSM8k, MATH, Human-Eval, and MBPP.
3.5-57.114.048.161.4GPT-4-92.040.267.063.6Falcon180B54.4-0.6147.0Qwen7B 14B51.7 61.311.6 24.829.9 32.334.0 48.9Baichuan 27B 13B24.5 22.15.6 10.118.3 20.728.3 26.17B16.73.312.814.8LLaMA 234B42.26.222.633.070B56.813.531.745.0Mistral7B47.511.330.547.5InternLM20B62.910.928.141.4Skywork7B55.87.813.422.8Yi6B 34B32.5 67.24.6 14.415.9 23.226.3 41.0
[7]e: We report the average pass@1 scores of our models on HumanEval<a href="Chen et al., 2021">7</a> and MBPP[2] (Austin et al., 2021).</p>
<p>Table 4 :
4
Overall performance on automatic benchmarks compared to open-source chat models.We highlight the 4-bit quantization results, as 4-bit quantization substantially reduces the memory requirement while the model performance nearly does not drop.This observation serve as the foundation of serving the model on consumer-grade devices, e.g., RTX4090.
ModelSizeMMLUCMMLUC-Eval(val) TruthfulQABBHGSM8K0-shot / 5-shot 0-shot / 5-shot 0-shot / 5-shot0-shot0-shot / 3-shot 0-shot / 4-shotLLaMA2-Chat13B 50.9 / 47.327.5 / 35.127.9 / 35.936.832.9 / 58.236.9 / 2.770B 59.4 / 59.936.1 / 41.035.0 / 41.354.042.4 / 58.547.1 / 58.7Baichuan2-Chat13B 55.1 / 50.158.6 / 59.556.0 / 54.849.038.8 / 47.245.7 / 23.3Qwen-Chat14B 64.0 / 65.067.7 / 70.666.1 / 70.152.549.7 / 55.059.5 / 61.2InternLM-Chat20B 55.6 / 57.453.6 / 53.851.2 / 53.651.842.4 / 36.715.7 / 43.4AquilaChat234B 65.2 / 66.767.5 / 70.083.0 / 89.464.320.1 / 34.311.5 / 48.5Yi-Chat6B58.2 / 61.069.4 / 74.768.8 / 74.250.639.7 / 47.238.4 / 44.9Yi-Chat-8bits(GPTQ) 6B58.3 / 61.069.2 / 74.769.2 / 73.949.940.4 / 47.339.4 / 44.9Yi-Chat-4bits(AWQ) 6B56.8 / 59.967.7 / 73.367.5 / 72.350.337.7 / 43.635.7 / 38.4Yi-Chat34B 67.6 / 73.579.1 / 81.377.0 / 78.562.451.4 / 71.771.7 / 76.0Yi-Chat-8bits(GPTQ) 34B 66.2 / 73.779.1 / 81.276.8 / 79.061.852.1 / 71.070.7 / 75.7Yi-Chat-4bits(AWQ) 34B 65.8 / 72.478.2 / 80.575.7 / 77.361.848.3 / 69.470.5 / 74.0</p>
<p>Table 5 :
5
Human evaluation comparison with other open-source chat models.</p>
<p>Table 6 :
6
Performance on MMLU after 200K adaptation.Extending the context length to 200K does not significantly change the short context capability.
ModelAverage Humanity STEM Social Science OtherYi-6B 4K63.2459.1053.1573.8369.26Yi-6B 200K61.7356.1752.3672.5468.94Yi-34B 4K76.3273.1768.0385.1180.78Yi-34B 200K75.5672.2066.8384.7680.40</p>
<p>Table 7 :
7
MMMU test set performance by the time of Yi-VL's release.
ModelOverall Art Business Science Health Society EngineeringGPT-4V55.765.364.348.463.576.341.7Yi-VL-34B41.656.133.332.945.966.536.0Qwen-VL-PLUS40.859.934.532.843.765.532.9Marco-VL40.456.531.031.046.966.533.8Yi-VL-6B37.853.430.330.039.358.534.1InfMIM-Zephyr-7B35.550.029.628.237.554.631.1SVIT34.148.928.026.835.550.930.7Emu2-Chat34.150.627.728.032.450.331.3BLIP-2 FLAN-T5-XXL34.049.228.627.333.751.530.4InstructBLIP-T5-XXL33.848.530.627.633.649.829.4LLaVA-1.5-13B33.649.828.225.934.954.728.3Qwen-VL-7B-Chat32.947.729.825.633.645.330.2SPHINX*32.950.927.225.334.151.227.8mPLUG-OWL232.148.525.624.932.846.729.6BLIP-2 FLAN-T5-XL31.043.025.625.131.848.027.8InstructBLIP-T5-XL30.643.325.225.229.345.828.6CogVLM30.138.025.625.131.241.528.9
https://tatsu-lab.github.io/alpaca_eval/
https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
https://www.superclueai.com/
A Author List and ContributionsOur team members contribute to the development of Yi from the following perspectives: We list our team members in alphabetical order.All authors contributed equally to this work.
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai, arXiv:2305.132452023arXiv preprint</p>
<p>Program Synthesis With lLarge Language Models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.077322021arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, . 09 2023Xiaohuan Zhou, and Tianhang ZhuQwen Technical Report</p>
<p>PIQA: Reasoning about Physical Commonsense in Natural Language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, ArXiv, abs/1911.116412019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin, arXiv:2311.12793Sharegpt4v: Improving large multi-modal models with better captions. 2023arXiv preprint</p>
<p>Evaluating Large Language Models Trained on Code. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé De Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, CoRR, abs/2107.03374Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr</p>
<p>. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, QuAC : Question Answering in Context. 2018</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 2019</p>
<p>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, 2018</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021Training Verifiers to Solve Math Word Problems. arXiv preprint</p>
<p>Redpajama: an open dataset for training large language models. 2023Together Computer</p>
<p>FlashAttention-2: Faster attention with better parallelism and work partitioning. Tri Dao, 2023</p>
<p>FlashAttention: Fast and memory-efficient exact attention with IO-awareness. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. 2022</p>
<p>Jong Michiel De, Yury Zemlyanskiy, Joshua Ainslie, Nicholas Fitzgerald, Sumit Sanghai, Fei Sha, William Cohen, Fido, arXiv:2212.08153Fusion-in-Decoder Optimized for Stronger Performance and Faster Inference. 2022arXiv preprint</p>
<p>. Deepseek-Ai , : , Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y K Li, Wenfeng Liang, Fangyun Lin, A X Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R X Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, 2024Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.07339-bit matrix multiplication for transformers at scale. 20228arXiv preprintint8 (</p>
<p>Enhancing chat language models by scaling high-quality instructional conversations. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, Bowen Zhou, arXiv:2305.142332023arXiv preprint</p>
<p>How abilities in large language models are affected by supervised fine-tuning data composition. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou, 2023</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>Data engineering for scaling language models to 128k context. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, Hao Peng, arXiv:2402.101712024arXiv preprint</p>
<p>Gemini Gemini, Rohan Team, Sebastian Anil, Yonghui Borgeaud, Jean-Baptiste Wu, Jiahui Alayrac, Radu Yu, Johan Soricut, Andrew M Schalkwyk, Anja Dai, Hauth, arXiv:2312.11805A family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Improving alignment of dialogue agents via targeted human judgements. Amelia Glaese, Nat Mcaleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, arXiv:2209.143752022arXiv preprint</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Vizwiz grand challenge: Answering visual questions from blind people. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P Bigham, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Measuring Massive Multitask Language Understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, CoRR, abs/2009.033002020</p>
<p>. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021Measuring Mathematical Problem Solving With the MATH Dataset. arXiv preprint</p>
<p>Scaling laws for autoregressive generative modeling. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M Ziegler, John Schulman, Dario Amodei, Sam Mccandlish, 2020</p>
<p>Training compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, arXiv:2305.083222023arXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, Ludwig Schmidt, Openclip, 10.5281/zenodo.5143773July 2021</p>
<p>Neel Jain, Ping-Yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Bhavya Brian R Bartoldson, Avi Kailkhura, Aniruddha Schwarzschild, Saha, arXiv:2310.05914Noisy embeddings improve instruction finetuning. 2023arXiv preprint</p>
<p>Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang, 2023</p>
<p>Scaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, 2020</p>
<p>Referitgame: Referring to objects in photographs of natural scenes. Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). the 2014 conference on empirical methods in natural language processing (EMNLP)2014</p>
<p>Solar 10.7b: Scaling large language models with simple yet effective depth up-scaling. Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, Sunghun Kim, 2023</p>
<p>Visual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1232017</p>
<p>Taku Kudo, John Richardson, arXiv:1808.06226SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing. 2018arXiv preprint</p>
<p>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, arXiv:2309.06180Efficient Memory Management for Large Language Model Serving with PagedAttention. 2023arXiv preprint</p>
<p>Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, Timothy Baldwin, arXiv:2306.09212CMMLU: Measuring Massive Multitask Language Understanding in Chinese. 2023arXiv preprint</p>
<p>Sequence parallelism: Long sequence training from system perspective. Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, Yang You, arXiv:2105.131202021arXiv preprint</p>
<p>Alpacaeval: An automatic evaluator of instruction-following models. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, 2023</p>
<p>. Linksoul-Ai. Chinese Llava, 2023</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv:2310.03744Improved baselines with visual instruction tuning. 2023arXiv preprint</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, arXiv:2304.08485Visual instruction tuning. 2023arXiv preprint</p>
<p>What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, Junxian He, arXiv:2312.156852023arXiv preprint</p>
<h1>instag: Instruction tagging for analyzing supervised fine-tuning of large language models. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, Jingren Zhou, 2023</h1>
<p>Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal, 2018</p>
<p>Ocr-vqa: Visual question answering by reading text in images. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, Anirban Chakraborty, 2019 international conference on document analysis and recognition (ICDAR). IEEE2019</p>
<p>Thuat Nguyen, Chien Van Nguyen, Dac Viet, Hieu Lai, Man, Trung Nghia, Franck Ngo, Ryan A Dernoncourt, Thien Huu Rossi, Nguyen, arXiv:2309.09400CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. 2023arXiv preprint</p>
<p>. Openai, Chatml, 2022</p>
<p>Training Language Models to Follow Instructions with Human Feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Testing language models on a held-out high school national finals exam. Keiran Paster, 2023</p>
<p>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, 2023</p>
<p>Efficiently Scaling Transformer Inference. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean, Proceedings of Machine Learning and Systems. Machine Learning and Systems20235</p>
<p>Sebastian Jack W Rae, Trevor Borgeaud, Katie Cai, Jordan Millican, Francis Hoffmann, John Song, Sarah Aslanides, Roman Henderson, Susannah Ring, Young, arXiv:2112.11446Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher. 2021arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, Chelsea Finn, arXiv:2305.182902023arXiv preprint</p>
<p>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>SQuAD: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 2016</p>
<p>WinoGrande: An Adversarial Winograd Schema Challenge at Scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, 2019</p>
<p>. Maarten Sap, Derek Hannah Rashkin, Ronan Chen, Yejin Lebras, Choi, Socialiqa, 2019Commonsense Reasoning about Social Interactions</p>
<p>Beyond chinchilla-optimal: Accounting for inference in language model scaling laws. Nikhil Sardana, Jonathan Frankle, arXiv:2401.004482023arXiv preprint</p>
<p>Are emergent abilities of large language models a mirage?. Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, Advances in Neural Information Processing Systems. 362024</p>
<p>Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXiv:2111.02114Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. 2021arXiv preprint</p>
<p>Fast Transformer Decoding: One Write-Head is All You Need. Noam Shazeer, arXiv:1911.021502019arXiv preprint</p>
<p>Noam Shazeer, arXiv:2002.05202GLU Variants Improve Transformer. 2020arXiv preprint</p>
<p>Byte Pair Encoding: A Text Compression Scheme That Accelerates Pattern Matching. Yusuxke Shibata, Takuya Kida, Shuichi Fukamachi, Masayuki Takeda, Ayumi Shinohara, Takeshi Shinohara, Setsuo Arikawa, DOI-TR-1611999Department of Informatics, Kyushu UniversityTechnical Report</p>
<p>Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, arXiv:1909.08053Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. 2019arXiv preprint</p>
<p>Textcaps: a dataset for image captioning with reading comprehension. Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 2020Proceedings, Part II 16</p>
<p>. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Danny Hernandez, Danqi Chen, Daphne Ippolito, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft2023Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson; Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk,351 additional authors not shown). Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models</p>
<p>Roformer: Enhanced Transformer with Rotary Position Embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, arXiv:2104.098642021arXiv preprint</p>
<p>Challenging Big-Bench Tasks and Whether Chain-of-Thought can Solve Them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 2019</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>Attention Is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems, 06 2017. </p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave, Ccnet, arXiv:1911.00359Extracting High Quality Monolingual Datasets from Web Crawl Data. 201911arXiv preprint</p>
<p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave, arXiv:1911.00359CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. 2019arXiv preprint</p>
<p>Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He, arXiv:2301.120172023arXiv preprint</p>
<p>Effective long-context scaling of foundation models. Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, arXiv:2309.160392023arXiv preprint</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>. Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chenxu Chao Yin, Lv, Dian Da Pan, Dong Wang, Fan Yan, Fei Yang, Feng Deng, Feng Wang, Guangwei Liu, Guosheng Ai, Haizhou Dong, Hang Zhao, Haoze Xu, Hongda Sun, Hui Zhang, Jiaming Liu, Jian Ji, Juntao Xie, Kun Dai, Lei Fang, Liang Su, Lifeng Song, Liyun Liu, Luyao Ru, Mang Ma, Mickel Wang, Mingan Liu, Nuolan Lin, Peidong Nie, Ruiyang Guo, Tao Sun, Tianpeng Zhang, Tianyu Li, Wei Li, Weipeng Cheng, Xiangrong Chen, Xiaochuan Zeng, Xiaoxi Wang, Xin Chen, Xin Men, Xuehai Yu, Yanjun Pan, Yiding Shen, Yiyu Wang, Youxin Li, Yuchen Jiang, Yupeng Gao, Zenan Zhang, Zhiying Zhou, Wu, Baichuan. 2Open Large-scale Language Models. 09 2023</p>
<p>From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, Transactions of the Association for Computational Linguistics. 22014</p>
<p>Orca: A Distributed Serving System for Transformer-Based Generative Models. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, Byung-Gon Chun, 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 2022</p>
<p>Paraphrasing the original text makes high accuracy long-context qa. Yijiong Yu, Zhe Zhou, Zhixiao Qi, Yongfeng Huang, arXiv:2312.111932023arXiv preprint</p>
<p>Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. Ji Yunjie, Deng Yong, Gong Yan, Peng Yiping, Niu Qiang, Zhang Lei, Ma Baochang, Li Xiangang, arXiv:2303.147422023arXiv preprint</p>
<p>HellaSwag: Can a Machine Really Finish Your Sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 2019</p>
<p>Evaluating the Performance of Large Language Models on GAOKAO Benchmark. Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, Xipeng Qiu, arXiv:2305.124742023arXiv preprint</p>
<p>Llavar: Enhanced visual instruction tuning for text-rich image understanding. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, Tong Sun, arXiv:2306.171072023arXiv preprint</p>
<p>Take a step back: Evoking reasoning via abstraction in large language models. Swaroop Huaixiu Steven Zheng, Xinyun Mishra, Heng-Tze Chen, Ed H Cheng, Chi, Denny Quoc V Le, Zhou, 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 2023</p>
<p>. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, Lima, 2023Less is more for alignment</p>
<p>Visual7w: Grounded question answering in images. Yuke Zhu, Oliver Groth, Michael Bernstein, Li Fei-Fei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7974 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7974</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7974</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-0b063955bb5cbf6c2e89630206a921de82fafa05</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0b063955bb5cbf6c2e89630206a921de82fafa05" target="_blank">PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses, and is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses.</p>
                <p><strong>Paper Abstract:</strong> PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7974.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7974.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Article citation precision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Manual article citation precision (proportion of cited articles supporting LLM claims)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-evaluation metric used to measure how many of the articles cited by an LLM- generated answer actually contain verifiable evidence supporting the claimed relationship; computed as correct_cited_articles / total_cited_articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (version 2023-06-13) via Azure OpenAI Services (2023-07-01-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature / clinical & biological sciences</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Claims/hypotheses expressed as relational answers (e.g., treatment/cause/interact statements)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual citation verification (article citation precision)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Human reviewers manually read each PubMed article cited by the model and judged whether the article provided evidence that supports the specific relationship claimed in the model's answer; the metric is the fraction of cited articles that were judged to contain valid supporting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Citation precision (correct_cited_articles / total_cited_articles)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Fraction in [0,1] (often reported as count ratio) representing the proportion of cited PubMed articles that contain valid supporting evidence for the claimed relation (examples reported as e.g., 49/50).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Eight-question benchmark sampled from PubMed query logs (questions listed in Supplementary Table 5 of the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual review of each cited PubMed article for eight adapter questions. For each cited article, reviewers judged whether it supported the stated relationship. The number of cited articles per question varied; Supplementary Table 5 reports per-question correct/total counts.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Per Supplementary Table 5: For the PubTator-augmented GPT-4 condition the per-question citation precisions were (examples): 'What can be caused by tocilizumab?' 49/50; 'Causes of memory deficits' 15/15; 'In what situations can cocaine be used?' 20/25; 'What can be treated by doxorubicin?' 45/45; 'Are there any genes that interact with cocaine?' 16/17; 'What drugs can treat breast cancer?' 45/45; 'What drugs can treat Scleroderma?' 50/50; 'What can be treated by finasteride?' 39/45. In contrast, GPT-4 only and PubMed-augmented GPT-4 produced much lower citation-precision counts (see Supplementary Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small qualitative evaluation (n=8 questions); questions sampled from PubMed query logs (not a comprehensive benchmark); manual review is labor-intensive and subject to reviewer interpretation; only measures whether cited articles support claims, not overall factual completeness or correctness of un-cited claims; evaluation limited to PubMed/PubTator-sourced articles; deterministic decoding (temperature=0) was used so results are not representative of stochastic generation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7974.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7974.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Top-20 precision (precision@20)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-20 article precision for entity-pair retrieval (precision@20)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manual retrieval-evaluation criterion measuring the fraction of the top-20 search results that are relevant (here: mention both query entities and support a relationship) used to compare PubTator 3.0 retrieval versus PubMed and Google Scholar.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (as downstream consumer of retrieved evidence)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical information retrieval supporting claim generation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Evidence retrieval to support model-produced claims/hypotheses</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Manual top-20 relevance evaluation (precision@20)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>For a collection of entity-pair queries, human annotators manually reviewed the top 20 results returned by each search system and marked an article as relevant if it mentioned both query entities and supported a relationship between them; precision@20 computed accordingly; total retrieved counts were also compared.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision@20 (number of relevant articles in top-20 / number of articles examined, up to 20) and total article count retrieved</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision@20 is a fraction (0–1) or percentage (0–100%) computed as (# relevant in top 20) / min(20, #returned). Total result counts are raw integer counts of matched articles returned by the system.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Twelve entity-pair case studies (two pairs each of Disease/Gene, Chemical/Disease, Chemical/Gene, Chemical/Chemical, Gene/Gene, Disease/Variant), search snapshot dated May 19, 2023 (detailed in Supplementary Table 4)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual evaluation of the top 20 results for each query and each system; relevance required both entities present and an apparent supporting relation; judgments performed for PubTator 3.0, PubMed, and Google Scholar results.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Across the 12 case studies PubTator 3.0 achieved precision@20 = 90.0% (216/240), PubMed achieved 81.6% (84/103), and Google Scholar achieved 48.5% (98/202). PubTator also returned larger total article counts for many queries (examples in Supplementary Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Query set limited (12 pairs) and biased toward relations first discussed in 2022+; Google Scholar results were filtered for non-PubMed items for parity; manual relevance judgments may be subjective; precision@20 does not measure recall beyond top results nor the quality of evidence within relevant articles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7974.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7974.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMed-derived 8-question benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eight-question qualitative benchmark sampled from PubMed query logs for evaluating relational question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small, human-curated set of eight relational biomedical questions adapted from real PubMed user queries, used to evaluate citation accuracy of LLMs (GPT-4) with and without retrieval augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4 (version 2023-06-13)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical question answering / evidence-based claims</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>Relational question answering (treatment, causation, interactions, situational uses)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Question-based manual citation accuracy benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Questions were chosen by sampling entities from PubMed query logs (both frequent and rare) and adapting common relation-seeking queries into natural-language questions; models generated answers with citations which were manually validated against the cited PubMed articles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-question article citation precision (correctly supporting cited articles / total cited articles)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>For each question, the metric is a ratio (count) of cited PubMed articles that were judged to support the stated relationships in the model answer; reported as count pairs (e.g., 45/45).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>8 natural-language questions derived from PubMed query logs; questions listed in Supplementary Table 5</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Manual verification of each PubMed article cited in model responses for the 8 questions; reviewers judged whether each cited article contained evidence for the claimed relation; counts and proportions reported in Supplementary Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PubTator-augmented GPT-4 substantially outperformed PubMed-augmented GPT-4 and GPT-4-only on citation precision across these eight questions (see Supplementary Table 5 for per-question counts; many PubTator-augmented results were near-perfect, e.g., multiple 45/45 or 50/50 outcomes).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Very small sample (n=8); qualitative and not a standardized benchmark; questions were adapted from logs which may bias toward common queries; manual review may vary by annotator; does not measure downstream scientific validity beyond whether cited articles support stated relations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? <em>(Rating: 2)</em></li>
                <li>Opportunities and challenges for ChatGPT and large language models in biomedicine and health <em>(Rating: 2)</em></li>
                <li>BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets <em>(Rating: 2)</em></li>
                <li>BioRED: A Rich Biomedical Relation Extraction Dataset <em>(Rating: 2)</em></li>
                <li>BioCreative V CDR task corpus: a resource for chemical disease relation extraction <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7974",
    "paper_id": "paper-0b063955bb5cbf6c2e89630206a921de82fafa05",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "Article citation precision",
            "name_full": "Manual article citation precision (proportion of cited articles supporting LLM claims)",
            "brief_description": "A human-evaluation metric used to measure how many of the articles cited by an LLM- generated answer actually contain verifiable evidence supporting the claimed relationship; computed as correct_cited_articles / total_cited_articles.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "GPT-4 (version 2023-06-13) via Azure OpenAI Services (2023-07-01-preview)",
            "scientific_domain": "Biomedical literature / clinical & biological sciences",
            "theory_type": "Claims/hypotheses expressed as relational answers (e.g., treatment/cause/interact statements)",
            "evaluation_method_name": "Manual citation verification (article citation precision)",
            "evaluation_method_description": "Human reviewers manually read each PubMed article cited by the model and judged whether the article provided evidence that supports the specific relationship claimed in the model's answer; the metric is the fraction of cited articles that were judged to contain valid supporting evidence.",
            "evaluation_metric": "Citation precision (correct_cited_articles / total_cited_articles)",
            "metric_definition": "Fraction in [0,1] (often reported as count ratio) representing the proportion of cited PubMed articles that contain valid supporting evidence for the claimed relation (examples reported as e.g., 49/50).",
            "dataset_or_benchmark": "Eight-question benchmark sampled from PubMed query logs (questions listed in Supplementary Table 5 of the paper)",
            "human_evaluation_details": "Manual review of each cited PubMed article for eight adapter questions. For each cited article, reviewers judged whether it supported the stated relationship. The number of cited articles per question varied; Supplementary Table 5 reports per-question correct/total counts.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Per Supplementary Table 5: For the PubTator-augmented GPT-4 condition the per-question citation precisions were (examples): 'What can be caused by tocilizumab?' 49/50; 'Causes of memory deficits' 15/15; 'In what situations can cocaine be used?' 20/25; 'What can be treated by doxorubicin?' 45/45; 'Are there any genes that interact with cocaine?' 16/17; 'What drugs can treat breast cancer?' 45/45; 'What drugs can treat Scleroderma?' 50/50; 'What can be treated by finasteride?' 39/45. In contrast, GPT-4 only and PubMed-augmented GPT-4 produced much lower citation-precision counts (see Supplementary Table 5).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Small qualitative evaluation (n=8 questions); questions sampled from PubMed query logs (not a comprehensive benchmark); manual review is labor-intensive and subject to reviewer interpretation; only measures whether cited articles support claims, not overall factual completeness or correctness of un-cited claims; evaluation limited to PubMed/PubTator-sourced articles; deterministic decoding (temperature=0) was used so results are not representative of stochastic generation settings.",
            "uuid": "e7974.0",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Top-20 precision (precision@20)",
            "name_full": "Top-20 article precision for entity-pair retrieval (precision@20)",
            "brief_description": "A manual retrieval-evaluation criterion measuring the fraction of the top-20 search results that are relevant (here: mention both query entities and support a relationship) used to compare PubTator 3.0 retrieval versus PubMed and Google Scholar.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "model_name": "GPT-4 (as downstream consumer of retrieved evidence)",
            "model_size": "GPT-4",
            "scientific_domain": "Biomedical information retrieval supporting claim generation",
            "theory_type": "Evidence retrieval to support model-produced claims/hypotheses",
            "evaluation_method_name": "Manual top-20 relevance evaluation (precision@20)",
            "evaluation_method_description": "For a collection of entity-pair queries, human annotators manually reviewed the top 20 results returned by each search system and marked an article as relevant if it mentioned both query entities and supported a relationship between them; precision@20 computed accordingly; total retrieved counts were also compared.",
            "evaluation_metric": "Precision@20 (number of relevant articles in top-20 / number of articles examined, up to 20) and total article count retrieved",
            "metric_definition": "Precision@20 is a fraction (0–1) or percentage (0–100%) computed as (# relevant in top 20) / min(20, #returned). Total result counts are raw integer counts of matched articles returned by the system.",
            "dataset_or_benchmark": "Twelve entity-pair case studies (two pairs each of Disease/Gene, Chemical/Disease, Chemical/Gene, Chemical/Chemical, Gene/Gene, Disease/Variant), search snapshot dated May 19, 2023 (detailed in Supplementary Table 4)",
            "human_evaluation_details": "Manual evaluation of the top 20 results for each query and each system; relevance required both entities present and an apparent supporting relation; judgments performed for PubTator 3.0, PubMed, and Google Scholar results.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Across the 12 case studies PubTator 3.0 achieved precision@20 = 90.0% (216/240), PubMed achieved 81.6% (84/103), and Google Scholar achieved 48.5% (98/202). PubTator also returned larger total article counts for many queries (examples in Supplementary Table 4).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Query set limited (12 pairs) and biased toward relations first discussed in 2022+; Google Scholar results were filtered for non-PubMed items for parity; manual relevance judgments may be subjective; precision@20 does not measure recall beyond top results nor the quality of evidence within relevant articles.",
            "uuid": "e7974.1",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PubMed-derived 8-question benchmark",
            "name_full": "Eight-question qualitative benchmark sampled from PubMed query logs for evaluating relational question answering",
            "brief_description": "A small, human-curated set of eight relational biomedical questions adapted from real PubMed user queries, used to evaluate citation accuracy of LLMs (GPT-4) with and without retrieval augmentations.",
            "citation_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "GPT-4 (version 2023-06-13)",
            "scientific_domain": "Biomedical question answering / evidence-based claims",
            "theory_type": "Relational question answering (treatment, causation, interactions, situational uses)",
            "evaluation_method_name": "Question-based manual citation accuracy benchmark",
            "evaluation_method_description": "Questions were chosen by sampling entities from PubMed query logs (both frequent and rare) and adapting common relation-seeking queries into natural-language questions; models generated answers with citations which were manually validated against the cited PubMed articles.",
            "evaluation_metric": "Per-question article citation precision (correctly supporting cited articles / total cited articles)",
            "metric_definition": "For each question, the metric is a ratio (count) of cited PubMed articles that were judged to support the stated relationships in the model answer; reported as count pairs (e.g., 45/45).",
            "dataset_or_benchmark": "8 natural-language questions derived from PubMed query logs; questions listed in Supplementary Table 5",
            "human_evaluation_details": "Manual verification of each PubMed article cited in model responses for the 8 questions; reviewers judged whether each cited article contained evidence for the claimed relation; counts and proportions reported in Supplementary Table 5.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "PubTator-augmented GPT-4 substantially outperformed PubMed-augmented GPT-4 and GPT-4-only on citation precision across these eight questions (see Supplementary Table 5 for per-question counts; many PubTator-augmented results were near-perfect, e.g., multiple 45/45 or 50/50 outcomes).",
            "comparison_to_human_generated": false,
            "comparison_results": "",
            "limitations_noted": "Very small sample (n=8); qualitative and not a standardized benchmark; questions were adapted from logs which may bias toward common queries; manual review may vary by annotator; does not measure downstream scientific validity beyond whether cited articles support stated relations.",
            "uuid": "e7974.2",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature?",
            "rating": 2
        },
        {
            "paper_title": "Opportunities and challenges for ChatGPT and large language models in biomedicine and health",
            "rating": 2
        },
        {
            "paper_title": "BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets",
            "rating": 2
        },
        {
            "paper_title": "BioRED: A Rich Biomedical Relation Extraction Dataset",
            "rating": 2
        },
        {
            "paper_title": "BioCreative V CDR task corpus: a resource for chemical disease relation extraction",
            "rating": 2
        }
    ],
    "cost": 0.015571749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</h1>
<p>Chih-Hsuan Wei ${ }^{1,1}$, Alexis Allot ${ }^{1,2}$, Po-Ting Lai ${ }^{1}$, Robert Leaman ${ }^{1}$, Shubo Tian ${ }^{1}$, Ling Luo ${ }^{1}$, Qiao Jin ${ }^{1}$, Zhizheng Wang ${ }^{1}$, Qingyu Chen ${ }^{1}$ and Zhiyong Lu ${ }^{1, <em>}$<br>${ }^{1}$ National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH), MD, 20894, Bethesda, USA<br></em> To whom correspondence should be addressed. Tel: +1 301594 7089; Email: zhiyong.lu@nih.gov<br>Present Address: Alexis Allot, The Neuro (Montreal Neurological Institute-Hospital), McGill University, Montreal, Quebec H3A 2B4, Canada<br>Present Address: Ling Luo, School of Computer Science and Technology, Dalian University of Technology, 116024, Dalian, China<br>Present Address: Qingyu Chen, Biomedical Informatics and Data Science, Yale School of Medicine, CT, 06510, New Haven, USA</p>
<h4>Abstract</h4>
<p>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art Al techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
<h1>INTRODUCTION</h1>
<p>The biomedical literature is a primary resource to address information needs across the biological and clinical sciences (1), however the requirements for literature search vary widely. Activities such as formulating a research hypothesis require an exploratory approach, whereas tasks like interpreting the clinical significance of genetic variants are more focused.</p>
<p>Traditional keyword-based search methods have long formed the foundation of biomedical literature search (2). While generally effective for basic search, these methods also have significant limitations, such as missing relevant articles due to differing terminology or including irrelevant articles because surface-level term matches cannot adequately represent the required association between query terms. These limitations cost time and risk information needs remaining unmet.</p>
<p>Natural language processing (NLP) methods provide substantial value for creating bioinformatics resources (3-5), and may improve literature search by enabling semantic and relation search. In semantic search, users indicate specific concepts of interest (entities) for which the system has precomputed matches regardless of the terminology used. Relation search increases precision by allowing users to specify the type of relationship desired between entities, such as whether a chemical enhances or reduces expression of a gene. In this regard, we present PubTator 3.0, a novel resource engineered to support semantic and relation search in the biomedical literature. Its search capabilities allow users to explore automated entity annotations for six key biomedical entities: genes, diseases, chemicals, genetic variants, species, and cell lines. PubTator 3.0 also identifies and makes searchable 12 common types of relations between entities, enhancing its utility for both targeted and exploratory searches. Focusing on relations and entity types of interest across the biomedical sciences allows PubTator 3.0 to retrieve information precisely while providing broad utility (see detailed comparisons with its predecessor in Supplementary Table 1).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
ure 1. PubTator 3.0 system overview and search results page: 1. Query auto-complete enhances search accuracy and synonym matching. 2. Natural language processing (NLP)-enhanced relevance: Search results are prioritized according to the depth of the relationship between the entities queried. 3. Users can further refine results with facet filters-section, journal, and type. 4. Search results include highlighted entity snippets explaining relevance. 5. Histogram visualizes number of results by publication year. 6. Entity highlighting can be switched on or off according to user preference.</p>
<h1>SYSTEM OVERVIEW</h1>
<p>The PubTator 3.0 online interface, illustrated in Fig. 1 and Supplementary Fig. 1, is designed for interactive literature exploration, supporting semantic, relation, keyword, and Boolean queries. An auto-complete function provides semantic search suggestions to assist users with query</p>
<p>formulation. For example, it automatically suggests replacing either "COVID-19" or "SARS-CoV-2 infection" with the semantic term "@DISEASE_COVID_19". Relation queries - new to PubTator 3.0 - provide increased precision, allowing users to target articles which discuss specific relationships between entities.</p>
<p>PubTator 3.0 offers unified search results, simultaneously searching approximately 36 million PubMed abstracts and over 6 million full-text articles from the PMC Open Access Subset (PMCOA), improving access to the substantial amount of relevant information present in the article full text (6). Search results are prioritized based on the depth of the relationship between the query terms: articles containing identifiable relations between semantic terms receive the highest priority, while articles where semantic or keyword terms co-occur nearby (e.g., within the same sentence) receive secondary priority. Search results are also prioritized based on the article section where the match appears (e.g., matches within the title receive higher priority). Users can further refine results by employing filters, narrowing articles returned to specific publication types, journals, or article sections.</p>
<p>PubTator 3.0 is supported by an NLP pipeline, depicted in Fig. 2A. This pipeline, run weekly, first identifies articles newly added to PubMed and PMC-OA. Articles are then processed through three major steps: 1. named entity recognition, provided by the recently developed deeplearning transformer model AIONER (7), 2. identifier mapping, and 3. relation extraction, performed by BioREx (8) of 12 common types of relations (described in Supplementary Table 2).</p>
<p>In total, PubTator 3.0 contains over 1.6 billion entity annotations ( 4.6 million unique identifiers) and 33 million relations ( 8.8 million unique pairs). It provides enhanced entity recognition and normalization performance over its previous version, PubTator 2 (9), also known as PubTator Central (Fig. 2B and Supplementary Table 3). We show the relation extraction performance of PubTator 3.0 in Fig. 2C and its comparison results to the previous state-of-the-art systems (1012) on the BioCreative V Chemical-Disease Relation (13) corpus, finding that PubTator 3.0 provided substantially higher accuracy. Moreover, when evaluating a randomized sample of entity pair queries compared to PubMed and Google Scholar, PubTator 3.0 consistently returns a</p>
<p>greater number of articles with higher precision in the top 20 results (Fig. 2D and Supplementary Table 4).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A. The PubTator 3.0 processing pipeline: AlONER (7) identifies six types of entities in PubMed abstracts and PMC-OA full-text articles. Entity annotations are associated with database identifiers by specialized mappers and BioREx (8) identifies relations between entities. Extracted data is stored in MongoDB and made searchable using Solr. B. Entity recognition performance for each entity type compared with PubTator2 (also known as PubTatorCentral) (13) on the BioRED corpus (14). C. Relation</p>
<p>extraction performance compared with SemRep (10) and notable previous best systems $(11,12)$ on the BioCreative V Chemical-Disease Relation (13) corpus. D. Comparison of information retrieval for PubTator 3.0, PubMed, and Google Scholar for entity pair queries, with respect to total article count and top-20 article precision.</p>
<h1>MATERIAL AND METHODS</h1>
<h2>Data Sources and Article Processing</h2>
<p>PubTator 3.0 downloads new articles weekly from the BioC PubMed API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/) and the BioC PMC API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/) in BioC-XML format (15). Local abbreviations are identified using Ab3P (16). Article text and extracted data are stored internally using MongoDB and indexed for search with Solr, ensuring robust and scalable accessibility unconstrained by external dependencies such as the NCBI eUtils API.</p>
<h2>Entity Recognition and Normalization / Linking</h2>
<p>PubTator 3.0 uses AIONER (7), a recently developed named entity recognition (NER) model, to recognize entities of six types: genes/proteins, chemicals, diseases, species, genetic variants, and cell lines. AIONER utilizes a flexible tagging scheme to integrate training data created separately into a single resource. These training datasets include NLM-Gene (17), NLM-Chem (18), NCBIDisease (19), BC5CDR (13), tmVar3 (20), Species-800 (21), BioID (22), and BioRED (14). This consolidation creates a larger training set, improving the model's ability to generalize to unseen data. Furthermore, it enables recognizing multiple entity types simultaneously, enhancing efficiency and simplifying the challenge of distinguishing boundaries between entities that reference others, such as the disorder "Alpha-1 antitrypsin deficiency" and the protein "Alpha-1 antitrypsin." We previously evaluated the performance of AIONER on 14 benchmark datasets (7), including the test sets for the aforementioned training sets. This evaluation demonstrated that AIONER's performance surpasses or matches previous state-of-the-art methods.</p>
<p>Entity mentions found by AIONER are normalized (linked) to a unique identifier in an appropriate entity database. Normalization is performed by a module designed for (or adapted to) each entity type, using the latest version. The recently-upgraded GNorm2 system (23) normalizes genes to NCBI Gene identifiers and species mentions to NCBI Taxonomy. tmVar3 (20), also recently upgraded, normalizes genetic variants; it uses dbSNP identifiers for variants listed in dbSNP and HGNV format otherwise. Chemicals are normalized by the NLM-Chem tagger (18) to MeSH identifiers (24). TaggerOne (25) normalizes diseases to MeSH and cell lines to Cellosaurus (26) using an improved normalization-only mode. These enhancements provide a significant overall improvement in entity normalization performance (Supplementary Table 2).</p>
<h1>Relation Extraction</h1>
<p>Relations for PubTator 3.0 are extracted by the unified relation extraction model BioREx (8), designed to simultaneously extract 12 types of relations across eight entity type pairs: chemicalchemical, chemical-disease, chemical-gene, chemical-variant, disease-gene, disease-variant, gene-gene, and variant-variant. Detailed definitions of these relation types and their corresponding entity pairs are presented in Supplementary Table 2. Deep-learning methods for relation extraction, such as BioREx, require ample training data. However, training data for relation extraction is fragmented into many datasets, often tailored to specific entity pairs. BioREx overcomes this limitation with a data-centric approach, reconciling discrepancies between disparate training datasets to construct a comprehensive, unified dataset.</p>
<p>We evaluated the relations extracted by BioREx using performance on manually annotated relation extraction datasets as well as a comparative analysis between BioREx and notable comparable systems. BioREx established a new performance benchmark on the BioRED corpus test set (14), elevating the performance from $74.4 \%$ (F-score) to $79.6 \%$, and demonstrating higher performance than alternative models such as transfer learning (TL), multi-task learning (MTL), and state-of-the-art models trained on isolated datasets (8). For PubTator 3.0, we replaced its deep learning module, PubMedBERT (27), with LinkBERT (28), further increasing the performance to $82.0 \%$. Furthermore, we conducted a comparative analysis between BioREx and SemRep (10), a widely used rule-based method for extracting diverse relations, the CD-REST (12)</p>
<p>system, and the previous state-of-the-art system (11), using the BioCreative V Chemical Disease Relation corpus test set (13). Our evaluation demonstrated that PubTator 3.0 provided substantially higher F-score than previous methods.</p>
<h1>Programmatic Access and Data Formats</h1>
<p>PubTator 3.0 offers programmatic access through its API and bulk download. The API (https://www.ncbi.nlm.nih.gov/research/pubtator3/) supports keyword, entity and relation search, and also supports exporting annotations in XML and JSON-based BioC (15) formats and tabdelimited free text. The PubTator 3.0 FTP site (https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3) provides bulk downloads of annotated articles and extraction summaries for entities and relations. Programmatic access supports more flexible query options; for example, the information need "what chemicals reduce expression of JAK1?" can be answered directly via API (e.g., https://www.ncbi.nlm.nih.gov/research/pubtator3-
api/relations?e1=@GENE_JAK1\&amp;type=negative_correlate\&amp;e2=Chemical) or by filtering the bulk relations file. Additionally, the PubTator 3.0 API supports annotation of user-defined free text.</p>
<h2>Case Study I: Entity Relation Queries</h2>
<p>We analyzed the retrieval quality of PubTator 3.0 by preparing a series of 12 entity pairs to serve as case studies for comparison between PubTator 3.0, PubMed, and Google Scholar. To provide an equal comparison, we filtered Google Scholar results for articles not in PubMed. To ensure that the number of results would remain low enough to allow filtering Google Scholar results for articles not in PubMed, we identified entity pairs first discussed together in the literature in 2022 or later. We then randomly selected two entity pairs of each of the following types: Disease/Gene, Chemical/Disease, Chemical/Gene, Chemical/Chemical, Gene/Gene and Disease/Variant. The comparison was performed with respect to a snapshot of the search results returned by all search engines on May 19, 2023. We manually evaluated the top 20 results for each system and each query; articles were judged to be relevant if they mentioned both entities in the query and supported a relationship between them.</p>
<p>Our analysis is summarized in Fig. 2D, and Supplementary Table 4 presents a detailed comparison of the quality of retrieved results between PubTator 3.0, PubMed, and Google Scholar. Our results demonstrate that PubTator 3.0 retrieves a greater number of articles than the comparison systems and its precision is higher for the top 20 results. For instance, PubTator 3.0 returned 346 articles for the query "GLPG0634 + Ulcerative Colitis," and manual review of the top 20 articles showed that all contained statements about an association between GLPG0634 and ulcerative colitis. In contrast, PubMed only returned a total of 18 articles, with only 12 mentioning an association. Moreover, when searching for "COVID19 + PON1," PubTator 3.0 returns 212 articles in PubMed, surpassing the 43 articles obtained from Google Scholar, only 29 of which are sourced from PubMed. These disparities can be attributed to several factors: 1. PubTator 3.0's search includes full texts available in PMC-OA, resulting in significantly broader coverage of articles, 2. Entity normalization improves recall, for example, by matching "paraoxonase 1" to "PON1," 3. PubTator 3.0 prioritizes articles containing relations between the query entities, 4. Pubtator 3.0 prioritizes articles where the entities appear nearby, rather than distant paragraphs. Across the 12 information retrieval case studies, PubTator 3.0 demonstrated an overall precision of $90.0 \%$ for the top 20 articles ( 216 out of 240), which is significantly higher than PubMed's precision of $81.6 \%$ ( 84 out of 103) and Google Scholar's precision of $48.5 \%$ ( 98 out of 202).</p>
<h1>Case Study II: Retrieval-Augmented Generation</h1>
<p>In the era of large language models (LLMs), PubTator 3.0 can also enhance their factual accuracy via retrieval augmented generation. Despite their strong language ability, LLMs are prone to generating incorrect assertions, sometimes known as hallucinations $(29,30)$. For example, when requested to cite sources for questions such as "which diseases can doxorubicin treat," GPT-4 frequently provides seemingly plausible but nonexistent references. Augmenting GPT-4 with PubTator 3.0 APIs can anchor the model's response to verifiable references via the extracted relations, significantly reducing hallucinations.</p>
<p>We assessed the citation accuracy of responses from three GPT-4 variations: PubTatoraugmented GPT-4, PubMed-augmented GPT-4 and standard GPT-4. We performed a qualitative</p>
<p>evaluation based on eight questions selected as follows. We identified entities mentioned in the PubMed query logs and randomly selected from entities searched both frequently and rarely. We then identified the common queries for each entity that request relational information and adapted one into a natural language question. Each question is therefore grounded on common information needs of real PubMed users. For example, the questions "What can be caused by tocilizumab?" and "What can be treated by doxorubicin?" are adapted from the user queries "tocilizumab side effects" and "doxorubicin treatment" respectively. Such questions typically require extracting information from multiple articles and an understanding of biomedical entities and relationship descriptions. Supplementary Table 5 lists the questions chosen.</p>
<p>We augmented the GPT-4 large language model (LLM) with PubTator 3.0 via the function calling mechanism of the OpenAI ChatCompletion API. This integration involved prompting GPT-4 with descriptions of three PubTator APIs: 1. Find Entity ID, which retrieves PubTator entity identifiers; 2. Find Related Entities, which identifies related entities based on an input entity and specified relations; and 3. Export Relevant Search Results, which returns PubMed article identifiers containing textual evidence for specific entity relationships. Our instructions prompted GPT-4 to decompose user questions into sub-questions addressable by these APIs, execute the function calls, and synthesize the responses into a coherent final answer. Our prompt promoted a summarized response by instructing GPT-4 to start its message with "Summary:" and requested the response include citations to the articles providing evidence. The PubMed augmentation experiments provided GPT-4 with access to PubMed database search via the National Center for Biotechnology Information (NCBI) E-utils APIs (31). We used Azure OpenAI Services (version 2023-07-01-preview) and GPT-4 (version 2023-06-13) and set the decoding temperature to zero to obtain deterministic outputs. The full prompts are provided in Supplementary Table 6.</p>
<p>PubTator-augmented GPT-4 generally processed the questions in three steps: 1. finding the standard entity identifiers, 2. finding its related entity identifiers, and 3. searching PubMed articles. For example, to answer "What drugs can treat breast cancer?", GPT-4 first found the PubTator entity identifier for breast cancer (@DISEASE_Breast_Cancer) using the Find Entity ID API. It then used the Find Related Entities API to identify entities related to</p>
<p>@DISEASE_Breast_Cancer through a "treat" relation. For demonstration purposes, we limited the maximum number of output entities to five. Finally, GPT-4 called the Export Relevant Search Results API for the PubMed article identifiers containing evidence for these relationships. The raw responses to each prompt for each method are provided in Supplementary Table 6.</p>
<p>We manually evaluated the accuracy of the citations in the responses by reviewing each PubMed article and verifying whether each PubMed article cited supported the stated relationship (e.g., Tamoxifen treating breast cancer). Supplementary Table 5 reports the proportion of the cited articles with valid supporting evidence for each method. GPT-4 frequently generated fabricated citations, widely known as the hallucination issue. While PubMed-augmented GPT-4 showed a higher proportion of accurate citations, some articles cited did not support the relation claims. This is likely because PubMed is based on keyword and Boolean search and does not support queries for specific relationships. Responses generated by PubTator-augmented GPT-4 demonstrated the highest level of citation accuracy, underscoring the potential of PubTator 3.0 as a high-quality knowledge source for addressing biomedical information needs through retrieval-augmented generation with LLMs such as GPT-4.</p>
<h1>DISCUSSION</h1>
<p>Previous versions of PubTator have fulfilled over one billion API requests since 2015, supporting a wide range of research applications. Numerous studies have harnessed PubTator annotations for disease-specific gene research, including efforts to prioritize candidate genes (32), determine gene-phenotype associations (33), and identify the genetic underpinnings of disease comorbidities (34). Several projects have used PubTator to create gene and genetic variant resources $(35,36)$ or to enrich disease knowledge graphs $(37,38)$. Moreover, PubTator has supported biocuration efforts $(39,40)$ and the creation of NLP benchmarks (41). With enhanced accuracy, PubTator 3.0 will better support these use cases.</p>
<p>Introducing relation annotations to PubTator 3.0 opens novel avenues for expanded use scenarios. With relations precomputed from the literature, complex research questions can often</p>
<p>be answered directly. Drug repurposing, for example, can be formulated as identifying chemicals which target specific genes. Conversely, determining the genetic targets of a chemical can be achieved by querying the same chemical/gene relations. Clinicians evaluating genetic variants, e.g. for rare diseases or personalized medicine, may explore the relationships between specific genetic variants and disease. Biologists, on the other hand, may utilize interactions between multiple genes to assemble complex molecular pathways.</p>
<p>There are several notable limitations for PubTator 3.0. Although it is capable of extracting relations from full-text articles, this feature is currently restricted to abstracts due to computational constraints. However, the system has been designed to support full-text relation extraction in a future enhancement. The current system only extracts 12 relation types, though these represent common uses. Finally, entity annotation and relation extraction are automated; though these systems exhibit high performance, their accuracy remains imperfect.</p>
<h1>CONCLUSION</h1>
<p>PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery. The PubTator 3.0 interface, API, and bulk file downloads are available at https://www.ncbi.nlm.nih.gov/research/pubtator3/.</p>
<h2>DATA AVAILABILITY</h2>
<p>Data is available through the online interface at https://www.ncbi.nlm.nih.gov/research/pubtator3/, through the API at https://www.ncbi.nlm.nih.gov/research/pubtator3/api or bulk FTP download at https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3/.</p>
<p>The source code for each component of PubTator 3.0 is openly accessible. The AIONER named entity recognizer is available at https://github.com/ncbi/AIONER. GNorm2, for gene name normalization, is available at https://github.com/ncbi/GNorm2. The tmVar3 variant name normalizer is available at https://github.com/ncbi/tmVar3. The NLM-Chem Tagger, for chemical name normalization, is available at https://ftp.ncbi.nlm.nih.gov/pub/lu/NLMChem. The TaggerOne system, for disease and cell line normalization, is available at https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/taggerone. The BioREx relation extraction system is available at https://github.com/ncbi/BioREx. The code for customizing ChatGPT with the PubTator 3.0 API is available at https://github.com/ncbi-nlp/pubtator-gpt.</p>
<h1>FUNDING</h1>
<p>This research was supported by the Intramural Research Program of the National Library of Medicine (NLM), National Institutes of Health. Funding for open access charge: National Institutes of Health.</p>
<h2>CONFLICT OF INTEREST</h2>
<p>None declared.</p>
<h2>REFERENCES</h2>
<ol>
<li>Lindberg, D.A. and Humphreys, B.L. (2008) Rising expectations: access to biomedical information. Yearb. Med. Inform., 3, 165-172.</li>
<li>Jin, Q., Leaman, R. and Lu, Z. (2023) PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence. arXiv.</li>
<li>Rzhetsky, A., Seringhaus, M. and Gerstein, M. (2008) Seeking a new biology through text mining. Cell, 134, 9-13.</li>
<li>Mayers, M., Li, T.S., Queralt-Rosinach, N. and Su, A.I. (2019) Time-resolved evaluation of compound repositioning predictions on a text-mined knowledge network. BMC Bioinf., 20, 653.</li>
<li>Zhao, S., Su, C., Lu, Z. and Wang, F. (2021) Recent advances in biomedical literature mining. Brief Bioinform, 22.</li>
<li>
<p>Westergaard, D., Staerfeldt, H.H., Tonsberg, C., Jensen, L.J. and Brunak, S. (2018) A comprehensive and quantitative comparison of text-mining in 15 million full-text articles versus their corresponding abstracts. PLoS Comput. Biol., 14, e1005962.</p>
</li>
<li>
<p>Luo, L., Wei, C.-H., Lai, P.-T., Leaman, R., Chen, Q. and Lu, Z. (2023) AIONER: all-in-one schemebased biomedical named entity recognition using deep learning. Bioinformatics, 39.</p>
</li>
<li>Lai, P.T., Wei, C.H., Luo, L., Chen, Q. and Lu, Z. (2023) BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets. J. Biomed. Inf., 146, 104487.</li>
<li>Wei, C.-H., Allot, A., Leaman, R. and Lu, Z. (2019) PubTator central: automated concept annotation for biomedical full text articles. Nucleic Acids Res., 47, W587-W593.</li>
<li>Kilicoglu, H., Rosemblat, G., Fiszman, M. and Shin, D. (2020) Broad-coverage biomedical relation extraction with SemRep. BMC Bioinf., 21, 188.</li>
<li>Peng, Y., Wei, C.-H. and Lu, Z. (2016) Improving chemical disease relation extraction with rich features and weakly labeled data. J. Cheminf., 8, 1-12.</li>
<li>Xu, J., Wu, Y., Zhang, Y., Wang, J., Lee, H.J. and Xu, H. (2016) CD-REST: a system for extracting chemical-induced disease relation in literature. Database, 2016.</li>
<li>Li, J., Sun, Y., Johnson, R.J., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A.P., Mattingly, C.J., Wiegers, T.C. and Lu, Z. (2016) BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings Bioinf., 23, bbac282.</li>
<li>Comeau, D.C., Islamaj Doğan, R., Ciccarese, P., Cohen, K.B., Krallinger, M., Leitner, F., Lu, Z., Peng, Y., Rinaldi, F. and Torii, M.J.D. (2013) BioC: a minimalist approach to interoperability for biomedical text processing. Database, 2013, bat064.</li>
<li>Sohn, S., Comeau, D.C., Kim, W. and Wilbur, W.J. (2008) Abbreviation definition identification based on automatic precision estimates. BMC Bioinf., 9, 1-10.</li>
<li>Islamaj, R., Wei, C.-H., Cissel, D., Miliaras, N., Printseva, O., Rodionov, O., Sekiya, K., Ward, J. and Lu, Z.J.J.o.b.i. (2021) NLM-Gene, a richly annotated gold standard dataset for gene entities that addresses ambiguity and multi-species gene recognition. J. Biomed. Inf., 118, 103779.</li>
<li>Islamaj, R., Leaman, R., Kim, S., Kwon, D., Wei, C.-H., Comeau, D.C., Peng, Y., Cissel, D., Coss, C. and Fisher, C. (2021) NLM-Chem, a new resource for chemical entity recognition in PubMed full text literature. Scientific Data, 8, 91.</li>
<li>Doğan, R.I., Leaman, R. and Lu, Z. (2014) NCBI disease corpus: a resource for disease name recognition and concept normalization. J. Biomed. Inf., 47, 1-10.</li>
<li>Wei, C.-H., Allot, A., Riehle, K., Milosavljevic, A. and Lu, Z. (2022) tmVar 3.0: an improved variant concept recognition and normalization tool. Bioinformatics, 38, 4449-4451.</li>
<li>Pafilis, E., Frankild, S.P., Fanini, L., Faulwetter, S., Pavloudi, C., Vasileiadou, A., Arvanitidis, C. and Jensen, L.J. (2013) The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text. PLoS One, 8, e65390.</li>
<li>Arighi, C., Hirschman, L., Lemberger, T., Bayer, S., Liechti, R., Comeau, D. and Wu, C. (2017), Proc. BioCreative Workshop, Vol. 482, pp. 376.</li>
<li>Wei, C.H., Luo, L., Islamaj, R., Lai, P.T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, 39.</li>
<li>Lipscomb, C.E. (2000) Medical subject headings (MeSH). Bull. Med. Libr. Assoc., 88, 265.</li>
<li>Leaman, R. and Lu, Z. (2016) TaggerOne: joint named entity recognition and normalization with semi-Markov Models. Bioinformatics, 32, 2839-2846.</li>
<li>Bairoch, A. (2018) The Cellosaurus, a Cell-Line Knowledge Resource. J. Biomol Tech., 29, 25-38.</li>
<li>Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J. and Poon, H. (2021) Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3, 1-23.</li>
<li>
<p>Yasunaga, M., Leskovec, J. and Liang, P. (2022), Association for Computational Linguistics, pp. 8003-8016.</p>
</li>
<li>
<p>Jin, Q., Leaman, R. and Lu, Z. (2023) Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? J. Am. Soc. Nephrol., 34, 1302-1304.</p>
</li>
<li>Tian, S., Jin, Q., Yeganova, L., Lai, P.T., Zhu, Q., Chen, X., Yang, Y., Chen, Q., Kim, W., Comeau, D.C. et al. (2023) Opportunities and challenges for ChatGPT and large language models in biomedicine and health. Brief Bioinform, 25.</li>
<li>National Center for Biotechnology Information (US). (2010) Entrez Programming Utilities Help. National Center for Biotechnology Information (US), Bethesda (MD).</li>
<li>Lieberwirth, J.K., Buttner, B., Klockner, C., Platzer, K., Popp, B. and Abou Jamra, R. (2022) AutoCaSc: Prioritizing candidate genes for neurodevelopmental disorders. Hum. Mutat., 43, 1795-1807.</li>
<li>Buch, A.M., Vertes, P.E., Seidlitz, J., Kim, S.H., Grosenick, L. and Liston, C. (2023) Molecular and network-level mechanisms explaining individual differences in autism spectrum disorder. Nat. Neurosci., 26, 650-663.</li>
<li>Pinto, B.G.G., Oliveira, A.E.R., Singh, Y., Jimenez, L., Goncalves, A.N.A., Ogava, R.L.T., Creighton, R., Schatzmann Peron, J.P. and Nakaya, H.I. (2020) ACE2 Expression Is Increased in the Lungs of Patients With Comorbidities Associated With Severe COVID-19. J. Infect. Dis., 222, 556-563.</li>
<li>Mitsuhashi, N., Toyo-Oka, L., Katayama, T., Kawashima, M., Kawashima, S., Miyazaki, K. and Takagi, T. (2022) TogoVar: A comprehensive Japanese genetic variation database. Hum. Genome Var., 9, 44.</li>
<li>Jiang, J., Yuan, J., Hu, Z., Zhang, Y., Zhang, T., Xu, M., Long, M., Fan, Y., Tanyi, J.L., Montone, K.T. et al. (2022) Systematic illumination of druggable genes in cancer genomes. Cell Rep., 38, 110400 .</li>
<li>Pu, Y., Beck, D. and Verspoor, K. (2023) Graph embedding-based link prediction for literaturebased discovery in Alzheimer's Disease. J. Biomed. Inf., 145, 104464.</li>
<li>Chen, C., Ross, K.E., Gavali, S., Cowart, J.E. and Wu, C.H. (2021) COVID-19 Knowledge Graph from semantic integration of biomedical literature and databases. Bioinformatics, 37, 4597-4598.</li>
<li>Lou, P., Jimeno Yepes, A., Zhang, Z., Zheng, Q., Zhang, X. and Li, C. (2020) BioNorm: deep learning-based event normalization for the curation of reaction databases. Bioinformatics, 36, 611-620.</li>
<li>Percha, B. and Altman, R.B. (2018) A global network of biomedical relationships derived from text. Bioinformatics, 34, 2614-2624.</li>
<li>Legrand, J., Gogdemir, R., Bousquet, C., Dalleau, K., Devignes, M.-D., Digan, W., Lee, C.-J., Ndiaye, N.-C., Petitpain, N. and Ringot, P. (2020) PGxCorpus, a manually annotated corpus for pharmacogenomics. Scientific Data, 7, 3.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings in Bioinformatics.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2015) GNormPlus: an integrative approach for tagging genes, gene families, and protein domains. BioMed research international, 2015, 918710.</li>
<li>Wei, C.-H., Luo, L., Islamaj, R., Lai, P.-T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, in press.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2012) SR4GN: a species recognition software tool for gene normalization. PloS one, 7, e38460.</li>
<li>Wei, C.-H., Phan, L., Feltz, J., Maiti, R., Hefferon, T. and Lu, Z. (2018) tmVar 2.0: integrating genomic variant information from literature with dbSNP and ClinVar for precision medicine. Bioinformatics, 34, 80-87.</li>
</ol>
<p>Supplementary Table 1. Feature comparison between PubTator 3.0 and its previous version, PubTator 2 (also known as PubTator Central).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">PubTator 2</th>
<th style="text-align: left;">PubTator 3.0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity annotations</td>
<td style="text-align: left;">Genes, diseases, chemicals, <br> genetic variants, species, <br> and cell lines in abstracts <br> and full text</td>
<td style="text-align: left;">Same types and scope; higher accuracy</td>
</tr>
<tr>
<td style="text-align: left;">Relation annotations</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">12 relation types across eight entity type pairs; <br> scope: abstracts only</td>
</tr>
<tr>
<td style="text-align: left;">Search scope</td>
<td style="text-align: left;">Abstracts only, via NCBI <br> eUtils</td>
<td style="text-align: left;">Unified search in abstracts \&amp; full text via <br> Apache Solr, no external dependencies</td>
</tr>
<tr>
<td style="text-align: left;">Query types</td>
<td style="text-align: left;">Keyword, Boolean</td>
<td style="text-align: left;">Also: semantic, relation</td>
</tr>
<tr>
<td style="text-align: left;">Search support</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Semantic autocomplete, facet filters (section, <br> journal, article type)</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval relevance</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Results prioritized by entity relationships, co- <br> occurrence \&amp; matching sections; highlighted <br> snippets (explains relevance); temporal <br> visualization</td>
</tr>
<tr>
<td style="text-align: left;">Literature management</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">User-defined collections</td>
</tr>
<tr>
<td style="text-align: left;">API</td>
<td style="text-align: left;">Retrieve articles and <br> annotations by PMID</td>
<td style="text-align: left;">Also: query relevant articles (semantic, <br> relation, keyword, Boolean), and query related <br> entities</td>
</tr>
<tr>
<td style="text-align: left;">Advanced natural- <br> language search</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Retrieval-augmented generation with GPT-4 <br> large language model</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Supplementary Figure 1. PubTator 3.0 article display page. (1) List of entities and relations identified by PubTator 3.0, providing a quick content overview. (2) Extracted entities highlighted in article text. (3) Display highlighting for query entities or all entities; display article abstract or full text. (4) Add article to custom collection for convenient access later.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">PubTator 3.0 Relations</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Entity types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ASSOCIATE</td>
<td style="text-align: left;">Complex or unclear relationships</td>
<td style="text-align: left;">Chemical / Disease <br> Chemical / Gene <br> Chemical / Variant <br> Disease / Gene <br> Disease / Variant <br> Variant / Variant</td>
</tr>
<tr>
<td style="text-align: left;">CAUSE</td>
<td style="text-align: left;">Triggering a disease by a specific agent</td>
<td style="text-align: left;">Chemical / Disease <br> Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">COMPARE</td>
<td style="text-align: left;">Comparing the effects of two chemicals or drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">COTREAT</td>
<td style="text-align: left;">Simultaneous administration of multiple drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">DRUG_INTERACT</td>
<td style="text-align: left;">Pharmacodynamic interactions between two <br> chemicals</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">INHIBIT</td>
<td style="text-align: left;">Reduction in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">INTERACT</td>
<td style="text-align: left;">Physical interactions, such as protein-binding</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">NEGATIVE_CORRELATE</td>
<td style="text-align: left;">Increases in the amount or degree of one entity <br> decreases the amount or degree of the other entity</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">POSITIVE_CORRELATE</td>
<td style="text-align: left;">The amount or degree of two entities increase or <br> decrease together</td>
<td style="text-align: left;">Chemical / Chemical <br> Chemical / Gene <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">PREVENT</td>
<td style="text-align: left;">Prevention of a disease by a genetic variant</td>
<td style="text-align: left;">Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">STIMULATE</td>
<td style="text-align: left;">Increase in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">TREAT</td>
<td style="text-align: left;">Treatment of a disease using a chemical or drug</td>
<td style="text-align: left;">Chemical / Disease</td>
</tr>
</tbody>
</table>
<h1>Supplementary Table 3. Normalization performance enhancements from PubTator Central to PubTator 3.0. Measurements reflect document-level normalization performance on the BioRED test set (42).</h1>
<table>
<thead>
<tr>
<th></th>
<th>PubTator Central (2.0)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PubTator 3.0</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NER/Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
<td>NER</td>
<td>Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
</tr>
<tr>
<td>Gene</td>
<td>GNormPlus (43)</td>
<td>86.92%</td>
<td>73.00%</td>
<td>79.35%</td>
<td>AIONER (7)</td>
<td>GNorm2 (44)</td>
<td>90.60%</td>
<td>79.41%</td>
<td>84.63%</td>
</tr>
<tr>
<td>Disease</td>
<td>TaggerOne (25)</td>
<td>77.13%</td>
<td>76.45%</td>
<td>76.79%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>75.33%</td>
<td>83.43%</td>
<td>79.17%</td>
</tr>
<tr>
<td>Chemical</td>
<td>TaggerOne (25)</td>
<td>73.42%</td>
<td>78.38%</td>
<td>75.82%</td>
<td></td>
<td>NLM-Chem (18)</td>
<td>83.26%</td>
<td>80.63%</td>
<td>81.92%</td>
</tr>
<tr>
<td>Species</td>
<td>SR4GN (45)</td>
<td>94.69%</td>
<td>94.69%</td>
<td>94.69%</td>
<td></td>
<td>GNorm2 (44)</td>
<td>93.97%</td>
<td>96.46%</td>
<td>95.20%</td>
</tr>
<tr>
<td>CellLine</td>
<td>TaggerOne (25)</td>
<td>42.42%</td>
<td>63.64%</td>
<td>50.91%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>76.00%</td>
<td>86.36%</td>
<td>80.85%</td>
</tr>
<tr>
<td>Variant</td>
<td>tmVar2 (46)</td>
<td>94.92%</td>
<td>84.85%</td>
<td>89.60%</td>
<td></td>
<td>tmVar3 (20)</td>
<td>98.48%</td>
<td>98.48%</td>
<td>98.48%</td>
</tr>
<tr>
<td>Micro-average</td>
<td></td>
<td>77.30%</td>
<td>77.49%</td>
<td>77.40%</td>
<td></td>
<td></td>
<td>84.04%</td>
<td>83.55%</td>
<td>83.80%</td>
</tr>
<tr>
<td>Macro-average</td>
<td></td>
<td>78.25%</td>
<td>78.50%</td>
<td>77.86%</td>
<td></td>
<td></td>
<td>86.27%</td>
<td>87.46%</td>
<td>86.71%</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 4. Comparison of PubTator 3.0, PubMed, and Google Scholar search results for various recently discussed relation pairs. D: Disease, G: Gene, C: Chemical, and V: Variant. The '#' column lists the number of results; for Google Scholar the number in parentheses indicates the number of articles that appear in PubMed. The 'Top 20' column indicates the number of articles in the top 20 results which discuss a relation between the specified entities; some queries return fewer than 20 articles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity <br> pair</th>
<th style="text-align: center;">Entities</th>
<th style="text-align: center;">PubTator 3.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubMed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Google Scholar</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;"># (in <br> PubMed)</td>
<td style="text-align: center;">Top20</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;$ D,G&gt;</td>
<td style="text-align: center;">COVID19 + PON1</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$9 / 11$</td>
<td style="text-align: center;">43 (29)</td>
<td style="text-align: center;">$10 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coronary Artery <br> Disease + SESN2</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">151 (104)</td>
<td style="text-align: center;">$14 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{D}&gt;$</td>
<td style="text-align: center;">GLPG0634 + Ulcerative <br> Colitis</td>
<td style="text-align: center;">346</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$12 / 18$</td>
<td style="text-align: center;">362 (281)</td>
<td style="text-align: center;">$8 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">brolucizumab <br> Glycogen <br> Storage <br> Disease Type II</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0 / 0$</td>
<td style="text-align: center;">1 (1)</td>
<td style="text-align: center;">$0 / 0$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">Gallium-68 + FAP alpha</td>
<td style="text-align: center;">261</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">11 (4)</td>
<td style="text-align: center;">$9 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lipopolysaccharides + <br> PVT1</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$18 / 20$</td>
<td style="text-align: center;">128 (95)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{C}&gt;$</td>
<td style="text-align: center;">N -dimethylnitrosamine <br> + Metformin</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">11 (8)</td>
<td style="text-align: center;">$1 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2'-fucosyllactose <br> Volatile fatty acids</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$3 / 6$</td>
<td style="text-align: center;">71 (40)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{G}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">interleukin 17 + cell <br> division cycle 42</td>
<td style="text-align: center;">599</td>
<td style="text-align: center;">$12 / 20$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">85 (39)</td>
<td style="text-align: center;">$1 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HAVCR2 + TOX</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">$11 / 20$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$1 / 4$</td>
<td style="text-align: center;">701 (479)</td>
<td style="text-align: center;">$3 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{D}, \mathrm{V}&gt;$</td>
<td style="text-align: center;">COVID19 + rs12329760</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">87 (63)</td>
<td style="text-align: center;">20/20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COVID19 + rs4646994</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$15 / 16$</td>
<td style="text-align: center;">30 (24)</td>
<td style="text-align: center;">20/20</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 5. Article citation precision for all 8 queries tested using GPT-4 only, GPT-4 augmented with PubMed, and GPT-4 augmented with PubTator 3.0. Results are summarized as "number of articles correctly referenced / total number of articles referenced." Full responses provided in Supplemental Data.</p>
<table>
<thead>
<tr>
<th>Questions</th>
<th>GPT4 <br> Only</th>
<th>GPT4 with PubMed <br> Augmentation</th>
<th>GPT4 with PubTator <br> Augmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td>What can be caused by tocilizumab? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 1$</td>
<td>$1 / 5$</td>
<td>$49 / 50$</td>
</tr>
<tr>
<td>Can you tell me what the causes of memory <br> deficits are? For each cause in your answer, <br> please cite and summarized the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 5$</td>
<td>$4 / 5$</td>
<td>$15 / 15$</td>
</tr>
<tr>
<td>In what situations can cocaine be used? For <br> each situation in your answer, please cite <br> and summarize the article PMIDs that <br> contain the evidence.</td>
<td>$0 / 3$</td>
<td>$1 / 3$</td>
<td>$20 / 25$</td>
</tr>
<tr>
<td>What can be treated by doxorubicin? For <br> each disease in your answer, please cite and <br> summarize the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 7$</td>
<td>$3 / 5$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>Are there any genes that interact with <br> cocaine? For each drug in your answer, <br> please cite the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 5$</td>
<td>$5 / 5$</td>
<td>$16 / 17$</td>
</tr>
<tr>
<td>What drugs can treat breast cancer? For <br> each drug in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$3 / 4$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>What drugs can treat Scleroderma? For each <br> drug in your answer, please cite the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$1 / 2$</td>
<td>$50 / 50$</td>
</tr>
<tr>
<td>What can be treated by finasteride? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$2 / 3$</td>
<td>$4 / 5$</td>
<td>$39 / 45$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Joint Authors&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
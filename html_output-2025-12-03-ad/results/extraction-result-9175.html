<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9175 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9175</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9175</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-275458111</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.06184v1.pdf" target="_blank">PEACE: Empowering Geologic Map Holistic Understanding with MLLMs</a></p>
                <p><strong>Paper Abstract:</strong> Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth’s subsurface and surface. These maps are indispensable in various fields, including disaster assessment, resource exploration, and civil engineering. Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding. This gap is primarily due to the challenging nature of cartographic generalization, which involves handling high-resolution map, managing multiple associated components, and requiring domain-specific knowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA). Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions. Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o. Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations. The code and data are available at https://github.com/microsoft/PEACE.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9175.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9175.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (base in GeoMap-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI multimodal large language model, used as the base model in GeoMap-Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is used in this work as the base multimodal LLM for hierarchical extraction, prompt-enhanced QA, and as an answer-judging agent; it is evaluated both standalone and as the core model inside the GeoMap-Agent pipeline on geologic-map understanding tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal large language model from OpenAI (GPT-4 family variant 'GPT-4o'); exact parameter count and training corpus not specified in this paper — used here in structured/JSON mode with temperature 0 and max tokens 2048.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Geoscience / Geologic map understanding</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of geologic-map understanding tasks including extracting metadata (title, scale, lon/lat), grounding (locating components by name/intention), referring (mapping legend colors to rock names), reasoning (area comparisons, fault existence, lithology composition, lon/lat localization), and analyzing (essay-style earthquake risk analysis); implemented as question-answering over digitized map metadata and cropped subimages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Composite ability scores on GeoMap-Bench (per-ability ability scores and overall S_all); task-level metrics include S_mcq (exact-match for MCQ), S_fitb (IoU_det for grounding bounding boxes, IoU_set for set extraction), S_eq (essay judged by a GPT-4o-powered judging agent J), and overall aggregated score S_all.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Standalone GPT-4o on GeoMap-Bench (All subsets): extracting 0.219, grounding 0.128, referring 0.378, reasoning 0.507, analyzing 0.612, overall 0.369. When used as the base model inside GeoMap-Agent, GeoMap-Agent achieves overall 0.811 (extracting 0.832, grounding 0.920, referring 0.886, reasoning 0.588, analyzing 0.831). (Scores are unitless normalized ability scores reported in the paper's GeoMap-Bench.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>High image resolution and many interrelated components (makes extraction/higher-level tasks hard); presence/absence of domain knowledge; prompt design (context, Chain-of-Thought, few-shot examples, JSON formatting); hierarchical 'divide-and-conquer' extraction (HIE) improves basic extraction/grounding; Domain Knowledge Injection (DKI) improves reasoning/analyzing; base-model variant (GPT-4o vs GPT-4o-mini) affects performance; resolution downscaling alone does not improve performance (improvements come from HIE structure), legend color/pattern similarity and complex rock patterns reduce recognition accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against multiple public MLLMs (QWen-chat, GLM-4v-9b, CogVLM2-llama3-chat, Monkey-chat, GPT-4o-mini) and a random baseline; GPT-4o standalone was the best public MLLM baseline (overall 0.369) but was substantially outperformed by GeoMap-Agent (0.811) which still uses GPT-4o as base but adds HIE, DKI, PEQA and tool-pool/expert group.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even with domain injection, reasoning tasks (e.g., fault detection and detailed lithology composition inference) remain challenging; difficulty recognizing rocks with complex patterns or very similar legend colors (notably in CGS subset); grounding can be poor for public MLLMs without HIE; reduced image resolution does not rescue performance if hierarchical extraction is not used.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Improve performance by extending the expert group and tool pool (more domain experts and data sources), apply supervised finetuning of MLLMs or use more powerful future base models, keep hierarchical information extraction (HIE) rather than crude downsampling, and enhance prompts (context, CoT, few-shot, JSON formatting, and cropped-context inclusion) to boost accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9175.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K2 (scientific model for geoscience knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>K2 is a geoscience-specialist language model included in the GeoMap-Agent tool pool to provide detailed geologic-specific knowledge lists that augment extraction prompts and metadata generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A geoscience-focused scientific model (referenced as K2) used here to supply domain knowledge about geologic maps and to enrich extraction prompts; specific model size and training details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Geoscience / Geologic map knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Provide detailed domain knowledge (tables of stratigraphic ages, lithology mappings, component information schemas) to improve extraction prompts and support reasoning in question answering about geologic maps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>No direct standalone evaluation of K2 is reported in this paper; its contribution is measured indirectly via ablation of the DKI module in GeoMap-Agent (impact on reasoning/analyzing scores).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Quality and coverage of geoscience knowledge K2 supplies; integration method (DKI) and how well the base model leverages injected knowledge; mapping between spatial coordinates and external DBs (latitude/longitude linking).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct baseline for K2 alone; contribution inferred from GeoMap-Agent ablation: removing DKI reduces advanced-ability scores (reasoning/analyzing) relative to full system.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper does not present direct failure modes for K2 itself, but indicates overall system still struggles with complex reasoning tasks despite DKI.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Expand domain knowledge sources and experts in the DKI step to further improve advanced reasoning and analysis tasks; K2-like specialized models are valuable as knowledge providers within an agent pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9175.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (answer-judging agent J)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o used as evaluation/judging agent J for essay scoring</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o is employed as the automated judging agent J to compare essay-style answers and compute S_eq scores, used inside the benchmark evaluation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (judging agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o configured with a specific answer-judging prompt to compare two essay answers according to diversity, specificity, and professionalism; used to produce ternary judgments (better/worse/comparable).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Evaluation of scientific question answering (geologic map analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Automated pairwise judging of essay answers to compute essay-type score S_eq (average of judgments with answer order switched to avoid bias).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>S_eq computed as 0.5*(1 - J(q,l,a) + J(q,a,l)), where J is the GPT-4o judging agent output (1.0 if a1 better than a2, 0.0 if worse, 0.5 if comparable).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Prompting of judging agent, criteria provided (diversity, specificity, professionalism), and potential order/bias mitigated by double evaluation and score averaging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No human-judge baseline reported; automated judging used for scalable essay evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reliance on another LLM for evaluation can inherit model biases; paper acknowledges evaluation tasks (essay scoring) could be improved further.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use careful prompt design for the judging agent and mitigate order bias by double-evaluation; consider improving essay evaluation methodology for effectiveness and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9175.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoGalactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoGalactica (geological text-modeling effort)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a prior effort that constructs a geological text corpus and continues training on Galactica-30B for text-based geo-question answering; cited in related work but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GeoGalactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A geological-domain LLM built by continuing training on Galactica-30B using geological text corpora (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Geoscience / Text-based geo-question answering</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-only geo-question answering (no multimodal map tasks described here).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9175.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned prior work that enhances instruction prompts and was fine-tuned for geospatial prediction problems (e.g., population density prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GeoLLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A geospatially-focused LLM fine-tuned to handle geospatial prediction problems (paper gives this as context in related work; specifics not provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Geoscience / Geospatial prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Geospatial prediction tasks such as population density estimation (textual/structured outputs), per related-work description.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9175.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OceanGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OceanGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a domain LLM trained on an extensive oceanic corpus for ocean science tasks; cited in related work but not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OceanGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM trained on a large oceanic corpus to target ocean-science tasks; no further details or metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Ocean science</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Various ocean-science tasks (textual prediction/QA) as per the OceanGPT project description.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9175.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an LLM applied to Electronic Health Records (EHR) data to predict future medical events; referenced in related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MedGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A clinical LLM trained/used on EHR data for prediction tasks in healthcare (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Clinical medicine / Electronic Health Records prediction</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predicting future medical events from EHRs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9175.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a biomedical LLM pre-trained on biomedical literature to assist text generation and mining in biomedicine.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BioGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Biomedical-domain LLM pre-trained on biomedical literature; referenced as part of related scientific LLM efforts.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Biomedical / Biomedical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Biomedical text generation and information mining.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9175.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GeoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as an autonomous GPT using GIS tools (GeoGPT) to tackle geospatial tasks; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GeoGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An agent-style GPT applied to geospatial tasks with tool use (GIS tool integration); details not included in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Geospatial analysis / GIS</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Autonomous geospatial analysis using a combination of LLM reasoning and GIS tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9175.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9175.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an LLM agent augmenting LLMs with chemistry tools to accomplish tasks in synthesis, drug discovery, and materials design; included in related work only.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A chemistry-focused LLM agent that leverages external tools for chemistry tasks; no experiments in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Chemistry / Organic synthesis and materials design</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Chemistry task automation (synthesis planning, discovery), combining LLM reasoning with domain tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PEACE: Empowering Geologic Map Holistic Understanding with MLLMs', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>K2: A foundation language model for geoscience knowledge understanding and utilization <em>(Rating: 2)</em></li>
                <li>GeoGalactica <em>(Rating: 2)</em></li>
                <li>A scientific large language model in geoscience <em>(Rating: 1)</em></li>
                <li>OceanGPT: A large language model for ocean science tasks <em>(Rating: 2)</em></li>
                <li>MedGPT <em>(Rating: 1)</em></li>
                <li>BioGPT: generative pretrained transformer for biomedical text generation and mining <em>(Rating: 1)</em></li>
                <li>GeoGPT: leveraging a GPT agent and GIS tools for geospatial tasks <em>(Rating: 1)</em></li>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9175",
    "paper_id": "paper-275458111",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4o (base in GeoMap-Agent)",
            "name_full": "GPT-4o (OpenAI multimodal large language model, used as the base model in GeoMap-Agent)",
            "brief_description": "GPT-4o is used in this work as the base multimodal LLM for hierarchical extraction, prompt-enhanced QA, and as an answer-judging agent; it is evaluated both standalone and as the core model inside the GeoMap-Agent pipeline on geologic-map understanding tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Multimodal large language model from OpenAI (GPT-4 family variant 'GPT-4o'); exact parameter count and training corpus not specified in this paper — used here in structured/JSON mode with temperature 0 and max tokens 2048.",
            "scientific_subdomain": "Geoscience / Geologic map understanding",
            "simulation_task": "Text-based simulation of geologic-map understanding tasks including extracting metadata (title, scale, lon/lat), grounding (locating components by name/intention), referring (mapping legend colors to rock names), reasoning (area comparisons, fault existence, lithology composition, lon/lat localization), and analyzing (essay-style earthquake risk analysis); implemented as question-answering over digitized map metadata and cropped subimages.",
            "evaluation_metric": "Composite ability scores on GeoMap-Bench (per-ability ability scores and overall S_all); task-level metrics include S_mcq (exact-match for MCQ), S_fitb (IoU_det for grounding bounding boxes, IoU_set for set extraction), S_eq (essay judged by a GPT-4o-powered judging agent J), and overall aggregated score S_all.",
            "simulation_accuracy": "Standalone GPT-4o on GeoMap-Bench (All subsets): extracting 0.219, grounding 0.128, referring 0.378, reasoning 0.507, analyzing 0.612, overall 0.369. When used as the base model inside GeoMap-Agent, GeoMap-Agent achieves overall 0.811 (extracting 0.832, grounding 0.920, referring 0.886, reasoning 0.588, analyzing 0.831). (Scores are unitless normalized ability scores reported in the paper's GeoMap-Bench.)",
            "factors_affecting_accuracy": "High image resolution and many interrelated components (makes extraction/higher-level tasks hard); presence/absence of domain knowledge; prompt design (context, Chain-of-Thought, few-shot examples, JSON formatting); hierarchical 'divide-and-conquer' extraction (HIE) improves basic extraction/grounding; Domain Knowledge Injection (DKI) improves reasoning/analyzing; base-model variant (GPT-4o vs GPT-4o-mini) affects performance; resolution downscaling alone does not improve performance (improvements come from HIE structure), legend color/pattern similarity and complex rock patterns reduce recognition accuracy.",
            "comparison_baseline": "Compared against multiple public MLLMs (QWen-chat, GLM-4v-9b, CogVLM2-llama3-chat, Monkey-chat, GPT-4o-mini) and a random baseline; GPT-4o standalone was the best public MLLM baseline (overall 0.369) but was substantially outperformed by GeoMap-Agent (0.811) which still uses GPT-4o as base but adds HIE, DKI, PEQA and tool-pool/expert group.",
            "limitations_or_failure_cases": "Even with domain injection, reasoning tasks (e.g., fault detection and detailed lithology composition inference) remain challenging; difficulty recognizing rocks with complex patterns or very similar legend colors (notably in CGS subset); grounding can be poor for public MLLMs without HIE; reduced image resolution does not rescue performance if hierarchical extraction is not used.",
            "author_recommendations_or_insights": "Improve performance by extending the expert group and tool pool (more domain experts and data sources), apply supervised finetuning of MLLMs or use more powerful future base models, keep hierarchical information extraction (HIE) rather than crude downsampling, and enhance prompts (context, CoT, few-shot, JSON formatting, and cropped-context inclusion) to boost accuracy.",
            "uuid": "e9175.0",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "K2",
            "name_full": "K2 (scientific model for geoscience knowledge)",
            "brief_description": "K2 is a geoscience-specialist language model included in the GeoMap-Agent tool pool to provide detailed geologic-specific knowledge lists that augment extraction prompts and metadata generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "K2",
            "model_description": "A geoscience-focused scientific model (referenced as K2) used here to supply domain knowledge about geologic maps and to enrich extraction prompts; specific model size and training details are not provided in this paper.",
            "scientific_subdomain": "Geoscience / Geologic map knowledge",
            "simulation_task": "Provide detailed domain knowledge (tables of stratigraphic ages, lithology mappings, component information schemas) to improve extraction prompts and support reasoning in question answering about geologic maps.",
            "evaluation_metric": "No direct standalone evaluation of K2 is reported in this paper; its contribution is measured indirectly via ablation of the DKI module in GeoMap-Agent (impact on reasoning/analyzing scores).",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Quality and coverage of geoscience knowledge K2 supplies; integration method (DKI) and how well the base model leverages injected knowledge; mapping between spatial coordinates and external DBs (latitude/longitude linking).",
            "comparison_baseline": "No direct baseline for K2 alone; contribution inferred from GeoMap-Agent ablation: removing DKI reduces advanced-ability scores (reasoning/analyzing) relative to full system.",
            "limitations_or_failure_cases": "Paper does not present direct failure modes for K2 itself, but indicates overall system still struggles with complex reasoning tasks despite DKI.",
            "author_recommendations_or_insights": "Expand domain knowledge sources and experts in the DKI step to further improve advanced reasoning and analysis tasks; K2-like specialized models are valuable as knowledge providers within an agent pipeline.",
            "uuid": "e9175.1",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GPT-4o (answer-judging agent J)",
            "name_full": "GPT-4o used as evaluation/judging agent J for essay scoring",
            "brief_description": "GPT-4o is employed as the automated judging agent J to compare essay-style answers and compute S_eq scores, used inside the benchmark evaluation pipeline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o (judging agent)",
            "model_description": "GPT-4o configured with a specific answer-judging prompt to compare two essay answers according to diversity, specificity, and professionalism; used to produce ternary judgments (better/worse/comparable).",
            "scientific_subdomain": "Evaluation of scientific question answering (geologic map analysis)",
            "simulation_task": "Automated pairwise judging of essay answers to compute essay-type score S_eq (average of judgments with answer order switched to avoid bias).",
            "evaluation_metric": "S_eq computed as 0.5*(1 - J(q,l,a) + J(q,a,l)), where J is the GPT-4o judging agent output (1.0 if a1 better than a2, 0.0 if worse, 0.5 if comparable).",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "Prompting of judging agent, criteria provided (diversity, specificity, professionalism), and potential order/bias mitigated by double evaluation and score averaging.",
            "comparison_baseline": "No human-judge baseline reported; automated judging used for scalable essay evaluation.",
            "limitations_or_failure_cases": "Reliance on another LLM for evaluation can inherit model biases; paper acknowledges evaluation tasks (essay scoring) could be improved further.",
            "author_recommendations_or_insights": "Use careful prompt design for the judging agent and mitigate order bias by double-evaluation; consider improving essay evaluation methodology for effectiveness and efficiency.",
            "uuid": "e9175.2",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GeoGalactica",
            "name_full": "GeoGalactica (geological text-modeling effort)",
            "brief_description": "Mentioned as a prior effort that constructs a geological text corpus and continues training on Galactica-30B for text-based geo-question answering; cited in related work but not used in experiments here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GeoGalactica",
            "model_description": "A geological-domain LLM built by continuing training on Galactica-30B using geological text corpora (details not provided in this paper).",
            "scientific_subdomain": "Geoscience / Text-based geo-question answering",
            "simulation_task": "Text-only geo-question answering (no multimodal map tasks described here).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.3",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GeoLLM",
            "name_full": "GeoLLM",
            "brief_description": "Mentioned prior work that enhances instruction prompts and was fine-tuned for geospatial prediction problems (e.g., population density prediction).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GeoLLM",
            "model_description": "A geospatially-focused LLM fine-tuned to handle geospatial prediction problems (paper gives this as context in related work; specifics not provided here).",
            "scientific_subdomain": "Geoscience / Geospatial prediction",
            "simulation_task": "Geospatial prediction tasks such as population density estimation (textual/structured outputs), per related-work description.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.4",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OceanGPT",
            "name_full": "OceanGPT",
            "brief_description": "Mentioned as a domain LLM trained on an extensive oceanic corpus for ocean science tasks; cited in related work but not used in this paper's experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "OceanGPT",
            "model_description": "An LLM trained on a large oceanic corpus to target ocean-science tasks; no further details or metrics provided in this paper.",
            "scientific_subdomain": "Ocean science",
            "simulation_task": "Various ocean-science tasks (textual prediction/QA) as per the OceanGPT project description.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.5",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "MedGPT",
            "name_full": "MedGPT",
            "brief_description": "Mentioned as an LLM applied to Electronic Health Records (EHR) data to predict future medical events; referenced in related work only.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MedGPT",
            "model_description": "A clinical LLM trained/used on EHR data for prediction tasks in healthcare (details not provided in this paper).",
            "scientific_subdomain": "Clinical medicine / Electronic Health Records prediction",
            "simulation_task": "Predicting future medical events from EHRs.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.6",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BioGPT",
            "name_full": "BioGPT",
            "brief_description": "Mentioned as a biomedical LLM pre-trained on biomedical literature to assist text generation and mining in biomedicine.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "BioGPT",
            "model_description": "Biomedical-domain LLM pre-trained on biomedical literature; referenced as part of related scientific LLM efforts.",
            "scientific_subdomain": "Biomedical / Biomedical text mining",
            "simulation_task": "Biomedical text generation and information mining.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.7",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "GeoGPT",
            "name_full": "GeoGPT",
            "brief_description": "Mentioned in related work as an autonomous GPT using GIS tools (GeoGPT) to tackle geospatial tasks; not used in experiments here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GeoGPT",
            "model_description": "An agent-style GPT applied to geospatial tasks with tool use (GIS tool integration); details not included in this paper.",
            "scientific_subdomain": "Geospatial analysis / GIS",
            "simulation_task": "Autonomous geospatial analysis using a combination of LLM reasoning and GIS tool calls.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.8",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow",
            "brief_description": "Mentioned as an LLM agent augmenting LLMs with chemistry tools to accomplish tasks in synthesis, drug discovery, and materials design; included in related work only.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemCrow",
            "model_description": "A chemistry-focused LLM agent that leverages external tools for chemistry tasks; no experiments in this paper.",
            "scientific_subdomain": "Chemistry / Organic synthesis and materials design",
            "simulation_task": "Chemistry task automation (synthesis planning, discovery), combining LLM reasoning with domain tools.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9175.9",
            "source_info": {
                "paper_title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "K2: A foundation language model for geoscience knowledge understanding and utilization",
            "rating": 2,
            "sanitized_title": "k2_a_foundation_language_model_for_geoscience_knowledge_understanding_and_utilization"
        },
        {
            "paper_title": "GeoGalactica",
            "rating": 2,
            "sanitized_title": "geogalactica"
        },
        {
            "paper_title": "A scientific large language model in geoscience",
            "rating": 1,
            "sanitized_title": "a_scientific_large_language_model_in_geoscience"
        },
        {
            "paper_title": "OceanGPT: A large language model for ocean science tasks",
            "rating": 2,
            "sanitized_title": "oceangpt_a_large_language_model_for_ocean_science_tasks"
        },
        {
            "paper_title": "MedGPT",
            "rating": 1
        },
        {
            "paper_title": "BioGPT: generative pretrained transformer for biomedical text generation and mining",
            "rating": 1,
            "sanitized_title": "biogpt_generative_pretrained_transformer_for_biomedical_text_generation_and_mining"
        },
        {
            "paper_title": "GeoGPT: leveraging a GPT agent and GIS tools for geospatial tasks",
            "rating": 1,
            "sanitized_title": "geogpt_leveraging_a_gpt_agent_and_gis_tools_for_geospatial_tasks"
        },
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 1,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        }
    ],
    "cost": 0.01572075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PEACE: Empowering Geologic Map Holistic Understanding with MLLMs
10 Jan 2025</p>
<p>Yangyu Huang yanghuan@microsoft.com 
Tianyi Gao v-tianyigao@microsoft.com 
Haoran Xu 
Qihao Zhao v-qihaozhao@microsoft.com 
Yang Song song.yang@cags.ac.cn 
Chinese Academy of Geological Sciences</p>
<p>Zhipeng Gui zhipeng.gui@whu.edu.cn 
Wuhan University</p>
<p>Tengchao Lv tengchaolv@microsoft.com 
Hao Chen 
Lei Cui 
Scarlett Li 
Furu Wei fuwei@microsoft.com 
Microsoft Research 
PEACE: Empowering Geologic Map Holistic Understanding with MLLMs
10 Jan 202540A4D2D3D74B7C093D2F22D6C7E757DFarXiv:2501.06184v1[cs.CV]
Geologic map, as a fundamental diagram in geology science, provides critical insights into the structure and composition of Earth's subsurface and surface.These maps are indispensable in various fields, including disaster detection, resource exploration, and civil engineering.Despite their significance, current Multimodal Large Language Models (MLLMs) often fall short in geologic map understanding.This gap is primarily due to the challenging nature of cartographic generalization, which involves handling highresolution map, managing multiple associated components, and requiring domain-specific knowledge.To quantify this gap, we construct GeoMap-Bench, the first-ever benchmark for evaluating MLLMs in geologic map understanding, which assesses the full-scale abilities in extracting, referring, grounding, reasoning, and analyzing.To bridge this gap, we introduce GeoMap-Agent, the inaugural agent designed for geologic map understanding, which features three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA).Inspired by the interdisciplinary collaboration among human scientists, an AI expert group acts as consultants, utilizing a diverse tool pool to comprehensively analyze questions.Through comprehensive experiments, GeoMap-Agent achieves an overall score of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.Our work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs, paves the way for advanced AI applications in geology, enhancing the efficiency and accuracy of geological investigations.</p>
<p>Introduction</p>
<p>Geology science plays a pivotal role in comprehensively understanding the Earth, offering insights into the processes that have shaped our planet over 4.5 billion years.The Earth's history is divided into several eons, eras, periods, and epochs, each characterized by significant geological and biological events.Geologic map is crucial in deciphering this history, as it documents the distribution and rela-Figure 1.The sampled questions of GeoMap-Bench and the corresponding answers by GeoMap-Agent and GPT-4o respectively.Green chatboxes indicate correct answers, while red ones denote incorrect or poor.GeoMap-Agent provides accurate answers for basic questions and comprehensive responses for advanced ones.GPT-4o, although able to correctly answer some basic questions, struggles with advanced ones.For instance, the response to the last question is too vague to provide concrete information.</p>
<p>tionships of rock units and geological features.</p>
<p>As for application, geologic map plays a key part in the following scenarios: (1) Disaster Detection: assessing risks of natural hazards like landslides, earthquakes, and groundwater contamination.(2) Resource Exploration: identifying potential locations for natural resources such as mining minerals, oil, and gas.(3) Civil Engineering: planning infrastructure projects by understanding the subsurface conditions.For instance, seismic activity is closely linked to active fault systems in the current tectonic environment.In an anticline trap, tectonic uplift causes the reservoir's top surface to rise, with impermeable layers sealing the upper part and water bodies or impermeable rocks isolating the lower part.This configuration creates ideal conditions for petroleum accumulation.</p>
<p>Despite its importance, the specialized and massive knowledge required to understand geologic map poses a high threshold.Even geologists often struggle to quickly link and retrieve knowledge from external sources, such as geological, geographical, and seismological data.This difficulty is amplified when dealing with private sources or large quantities of data, making efficient and systematic analy-sis of geologic map more challenging.When considering leveraging AI for this, MLLMs have demonstrated strengths in general image understanding.However, several challenges remain in geologic map understanding, which has the nature of cartographic generalization.Specifically: (1) High Resolution: it has extremely high resolutions, e.g., sometimes even reaching up to 10,000 2 pixels.(2) Multiple Associated Components: it contains numerous components, many of which are interrelated.(3) Domain-specific Knowledge: it consists of complex and symbolized geographical objects with diverse visual representations.Additionally, a range of diverse AI capabilities is required, including detection, classification, segmentation, optical character recognition (OCR), cross-region understanding, and reasoning.Consequently, the potential of MLLMs in understanding geologic map remains under-explored.</p>
<p>To address these challenges, we propose GeoMap-Bench, a benchmark for evaluating the performance of LLMs in geologic map understanding from 5 aspects: extracting, referring, grounding, reasoning, and analyzing.Alongside GeoMap-Bench, we introduce GeoMap-Agent, an AI system specifically designed for geologic map understanding and analysis.Its innovative design facilitates map digitization and enables question answering, extending even to downstream applications.Specifically, GeoMap-Agent comprises three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA).In summary, the main contributions are as follows:</p>
<p>• We construct the GeoMap-Bench, which is the first-ever benchmark for evaluating the performance of MLLMs on geologic map understanding comprehensively.• We propose the GeoMap-Agent, which is the inaugural AI agent for question answering of geologic map.Among it, several modules are designed for information structuring and deep thinking with domain-specific knowledge.• The GeoMap-Agent achieves superior performance compared with the existing MLLMs, showcasing its potential to significantly enhance the efficiency and accuracy of geologic map understanding.</p>
<p>Related Work</p>
<p>Science Benchmark.GeoBench [9] is specifically designed to test the geoscientific understanding of LLMs, focusing exclusively on text-based evaluations.Charting New Territories benchmark [32] includes visual tasks aims at evaluating the geographic and geospatial capabilities of MLLMs.OceanBench [6] is proposed to assess the capabilities of LLMs in performing tasks related to ocean science.ScienceQA [23], a multimodal benchmark, consists of multiple-choice questions covering a wide range of science topics.However, geology science is rarely considered in science QA.LHRS-Bench [26] is a comprehensive benchmark for thoroughly evaluating MLLMs in understanding remote sensing (RS) images.Currently, despite the significance of geologic map, none of the existing benchmarks are designed to comprehensively understand them.</p>
<p>Science Agent.GeoGPT [45] leverages Chain-of-Thought (COT) [42] reasoning and a suite of GIS tools to tackle diverse geospatial tasks.ChemCrow [7] is an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design.SocialSimulacra [30] is a prototyping technique that generates a wide range of realistic social interactions that may emerge when a social computing system is populated.DS-Agent [14] is a novel automatic framework that harnesses a LLM agent and case-based reasoning (CBR).which can flexibly capitalize on expert knowledge from Kaggle [1] and facilitate consistent performance improvement through the feedback mechanism.Nevertheless, there is no work focusing on geologic map understanding.Meanwhile, due to its challenges, other methods are difficult to apply effectively.</p>
<p>Scientific LLM.Scientific models are being increasingly utilized to empower various scientific fields.In geoscience domain, GeoLLM [25] enhances instruction prompt and is fine-tuned on GPT-3.5 turbo [29] to address geospatial prediction problems, such as population density prediction.GeoGalactica [21] constructs a geological text corpus, and continues training on Galactica-30B [37] for merely textbased geo-question answering.K2 [9], the first-ever LLM in geoscience, aims to align LLM responses to geosciencerelated user queries, which only support text query.In other scientific domains, Clinical LLMs [34] introduces a human evaluation framework and instruction prompt tuning.</p>
<p>MedGPT [17] applies Electronic Health Records (EHR) data and Named Entity Recognition tools to predict future medical events.BioGPT [24] is pre-trained on biomedical literature to facilitate biomedical text generation and mining.OceanGPT [6] collects an extensive oceanic corpus and trains the first LLM specifically designed for ocean science tasks.However, when considering applications in the geological field, there is a notable lack of related work focused on multimodal understanding, especially geologic map.</p>
<p>General LLM.Large Language Models (LLMs) [3,4,8,10,16,43] have rapidly developed recently, achieving significant breakthroughs across both general and specific domains.To support cross-modality understanding, research has extended beyond pure text to include other modalities, such as CLIP [31], LLaVA [22], Qwen-VL [5], and GPT-4V [3] for image, as well as specific works like Gemini [38] and Video-llava [20] in video, and AudioPaLM [33] and Vi-oLA [41] in speech.Among them, GPT-4o [27] stands out as one of the most powerful MLLMs.Although, it's hard to be directly applied in geologic map understanding, we utilize it as the base model in GeoMap-Agent.</p>
<p>GeoMap-Bench</p>
<p>We construct a geologic map benchmark, GeoMap-Bench, to evaluate the performance of MLLMs on geologic map understanding across different abilities, the overview of it is as shown in</p>
<p>Data Sources</p>
<p>There are lots of reliable data sources for geologic maps, including United States Geological Survey (USGS), China Geological Survey (CGS), British Geological Survey (BGS), Geological Survey of Canada (GSC), European Geological Data Infrastructure (EGDI), OneGeology.To construct a high-quality benchmark for geologic map, it is essential to utilize data that is standardized in format and diverse in content.Specifically, the geologic maps from the target data sources should contain standardized components, as mentioned in the Appendix.Additionally, the geologic maps should cover different physical regions with various geological features and be available in multiple languages.Consequently, we select publicly available geologic maps from USGS and CGS as benchmark data sources.USGS Source.Geologic maps from the USGS source exhibit significant variation in drawing style and geologic components.These maps are in rasterized format or Ar-cGIS [28] format, with nearly 10,000 instances accessible.They were published over a broad time span, from 1911 to 2024, and cover regions ranging from ∼67°W to ∼125°W longitude and ∼24°N to ∼49°N latitude.Additionally, the scales of these maps range from 1:24,000 to 1:500,000.CGS Source.Geologic maps from the CGS source are standardized in drawing style and geological components.These maps are in MapGIS [2] format, with several thou- sands instances.There are over 158 datasets, most of which were published after the 1980s.They cover regions ranging from ∼72°E to ∼138°E longitude and ∼16°N to ∼56°N latitude, with scales mostly ranging from 1:50,000 to 1:250,000.</p>
<p>Dataset Construction</p>
<p>Rasterization</p>
<p>Since the CGS map data is in the proprietary MapGIS [2] format, unlike the ArcGIS format, we use MapGIS software [2] to render them to rasterized images.To expedite this process, a simulation program is developed that automates keyboard and mouse operations within the software.We select the "National 1:200,000 Digital Geologic Map (Public Version) Spatial Database" from the CGS maps for rasterization since their standard format in content, which includes 1,163 maps.For the USGS maps, we choose those published after the 1990s in rasterized image format, totaling 6,415 maps.After filtering for image quality, we obtain ∼5,000 rasterized images for further processing.</p>
<p>Annotation</p>
<p>After obtaining the rasterized maps, we manually annotate the metadata for each map, including the bounding box of each component, the basic information of each component (names, longitudes, scale, etc.), as well as the details of each legend unit (including bounding boxes, colors, texts, lithology, and stratigraphic age).Additionally, we also record statistical information, such as the lithology composition, the area of each rock unit, and the presence of faults in different regions.Following an accuracy cross-check of metadata, we have ∼2,000 images with corresponding metadata.</p>
<p>Definition</p>
<p>To thoroughly measure the performance of MLLMs on geologic map understanding, we collaborate with senior geologists to define measuring abilities across five aspects: extracting, grounding, referring, reasoning, and analyzing.The details are as follows:</p>
<p>Extracting.It evaluates the ability to accurately extract basic information from the map, such as the title, scale, and latitude coordinates.</p>
<p>Grounding.It measures the capability to precisely locate components on the map based on their names or intentions.2. Defined tasks in GeoMap-Bench.The question format is randomly selected from a format pool for each task.For example, both "What's the scale of this map?" and "Can you provide the scale of this map?" are variations of the same task of extracting the map scale.Meanwhile, each component corresponds to an intention pool, from which an intention is randomly chosen per question in a grounding-byintention task.The question types "FITB," "MCQ," and "EQ" represent fill-in-the-blank, multiple-choice, and essay questions, respectively.</p>
<p>Referring.It assesses the skill in linking names to their corresponding properties, such as identifying the rock name by its legend color.</p>
<p>Reasoning.It evaluates the ability to perform high-level logical tasks that require connecting information across components or incorporating external knowledge.</p>
<p>Analyzing.It assesses the capability to comprehensively interpret a given topic on the map and provide detailed and meaningful insights from various perspectives.</p>
<p>To concretize these abilities, we delineate specific tasks for each aspect, totaling 25 tasks, which is defined in Table 2.After this process, we compile a dataset comprising ∼5,000 questions, ensuring the completeness of each map and a balanced task distribution of questions.</p>
<p>Generation</p>
<p>Based on the annotated metadata of geologic maps, we generate the ground-truth answer for each question through retrieving, calculating, and statistics.To ensure the quality of ground-truth answers, we enlist senior geologists to review and correct all the map-question pairs.Eventually, this process results in a dataset comprising 124 maps and 3,864 questions with ground-truth answers.The distribution of questions in GeoMap-Bench across different abilities and tasks is illustrated in Figure 3.</p>
<p>GeoMap-Agent</p>
<p>Scientific diagrams in certain scenarios often exhibit attributes such as high resolution, multiple associated components, and the need for domain knowledge.These attributes pose significant challenges for image understanding of MLLMs.For instance, geologic map is poorly understood even by the powerful GPT-4o, as demonstrated in Table 3.</p>
<p>To tackle these challenges, we propose an MLLM-based paradigm for image understanding.GeoMap-Agent, illustrated in Figure 4, serves as an example of this paradigm.</p>
<p>Modules</p>
<p>The framework of GeoMap-Agent consits of three modules, Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompted-enhanced Question Answering (PEQA).</p>
<p>HIE</p>
<p>Extracting information from a high-resolution image often leads to poor performance for MLLMs.The Hierarchical Information Extraction (HIE) module mitigates this by employing a "divide and conquer" strategy.In the "divide" stage, the entire image is treated as the root of a tree, and it is hierarchically divided into sub-images.To ensure each subimage represents a semantically independent component, a map component detector and legend unit detector from the tool pool are progressively applied.During the "conquer" stage, a base model, currently GPT-4o, is applied to extract local information from each sub-image.This model can be replaced with a more powerful one as it becomes available.To ensure comprehensive information extraction, the K2 [9], geology specialist model, from the tool pool is utilized to provide a detailed information list for each component, enhancing the extraction prompt.Finally, the extracted information from each sub-image is aggregated into the global metadata.This process effectively forms a structured representation of geologic maps.</p>
<p>DKI</p>
<p>The Domain Knowledge Injection (DKI) module supplies the essential field knowledge for question answering, particularly for questions that require reasoning and analyzing.The DKI module operates in two steps: First, for the given question, the base model is confirmed with to determine whether specific types of knowledge from each expert in the group are needed.Second, for the required knowledge types, the relevant experts are consulted one by one  to acquire the corresponding knowledge, some of which is linked through the latitude and longitude range extracted in HIE.Currently, our expert group includes a geologist, a geographer, and a seismologist, each of whom can leverage related tools from the tool pool.The expert group can be easily extended based on specific requirements.</p>
<p>PEQA</p>
<p>In the previous two modules, the global metadata (internal information) of geologic map and the required knowledge (external knowledge) specific to the question about this map are obtained.Based on them, the prompt-enhanced question answering (PEQA) module enhances the prompt from different aspects: context, chain of thought (CoT) [42], fewshot learning, and attention-like design.(1).We provide the digitized metadata and required knowledge as context in the prompt.(2).We require the base model to respond with not only answer but also reasoning, which encouraging deeper thinking.(3).We supply an example answer in the prompt and instruct the base model to respond in JSON format to ensure proper formatting.(4).We crop the relevant components of geologic map to the given question and include the cropped images in the prompt.By leveraging these prompt designs, the performance is further improved.</p>
<p>Expert Group</p>
<p>Inspired by the working models of human scientists, interdisciplinary group collaboration is employed.The expert group comprises AI experts from various scientific fields, which provides domain-specific knowledge for GeoMap-Agent, as illustrated in Figure 4.Each expert specializes in a distinct field mastering unique types of knowledge.Both the knowledge types of each expert and the composition of the expert group are scalable.</p>
<p>Geologist.The geologist specializes in geologic map knowledge, including the composition of geologic map, the information schema of each geologic component, the table of stratigraphic age, and the table of lithology.</p>
<p>Geographer.The geographer focuses on distributions of land cover and population density covering global regions.</p>
<p>Seismologist.The seismologist concentrates on both history earthquake data and active faults data worldwide.</p>
<p>Tool Pool</p>
<p>The tool pool provides various functionalities for modules of GeoMap-Agent.It currently contains more than 8 tools and is scalable to accommodate additional tools as needed.</p>
<p>Vision Detectors.Two vision detectors [40] are developed that assist in analyzing the layout of geologic map.The map component detector identifies 7 components as outlined in Appendix, while the legend unit detector identifies all text units and color units within the legend component, aiding in rock name recognition and rock color detection.GEE APIs.Two APIs in Google Earth Engine (GEE) [13] are included to retrieve the population density [11,35] and land cover [44] distributions of an area based on the specified latitude and longitude range in geologic map.Scientific DBs.Two scientific databases are integrated: one for accessing historical earthquake records from the USGS database [39] and the other for active fault records from the GEM database [36].Both of them can be retrieved based on the specified latitude and longitude coordinates.AI Models.Two models are involved: the K2 [9] scientific model offers geologic-specific knowledge, particularly concerning geologic map; the GPT-4o [27] base model is utilized for information extraction, question answering, etc.</p>
<p>Others.Other tools are described in the Appendix.</p>
<p>Experiment</p>
<p>Performance on Benchmark</p>
<p>We conducted comparison experiments on GeoMap-Bench using various methods, including the publicly available API, like GPT-4o [27], and open-source models, such as QWen-chat [5].As shown in Specifically, in our designed benchmark, public MLLMs exhibit significant weaknesses, particularly in extracting, grounding, and reasoning.Although GPT-4o performs the best among public MLLMs, its grounding ability remains subpar, resulting in an overall benchmark score of less than 0.5.In contrast, GeoMap-Agent demonstrates outstanding performance in basic abilities, such as 0.832 in extracting, 0.920 in grounding, and 0.886 in referring.It also shows relative high performance in advanced abilities, like 0.588 in reasoning, and 0.831 in analyzing.In summary, GeoMap-Bench comprehensively evaluates different methods for geologic map understanding, and GeoMap-Agent enables a thorough understanding of geologic map, as verified by its</p>
<p>Ablation Study</p>
<p>To thoroughly analyze the GeoMap-Agent, more experiments are conducted for different purposes, such as the contribution of each module, the adaptability to other base models, and the performance under lower resolutions.Additional experiments can be found in the Appendix.</p>
<p>Contribution on Different Modules</p>
<p>General LLMs encounter several challenges in understanding geologic map,and the three modules proposed in Section 4.1 mitigate these challenges.To verify their effectiveness, we conduct experiments to assess the contribution of each module by applying them partly.As shown in Table 4, each module enhances the abilities of GeoMap-Agent from different perspectives.For example, HIE significantly improves basic abilities by addressing challenges related to high resolution and multiple associated components, DKI further enhances advanced abilities by incorporating domain knowledge, while PEQA boosts overall performance.GeoMap-Agent achieves optimal results when all these modules are applied.</p>
<p>Adaptability to Different Base Models</p>
<p>According to the GeoMap-Agent framework, the base model can theoretically be any MLLM.To verify the adaptability of different base models, we test the GeoMap-Agent framework using GPT-4o [27] or GPT-4o mini [27].The results, shown in Table 5, indicate varying degrees of improvement by different base model.Inferred from this, the base model is not limited to GPT-4o [27] and can be upgraded when more powerful MLLMs become available.</p>
<p>Performance across Different Resolutions</p>
<p>High resolution presents a significant challenge for MLLMs in understanding geologic map.To tackle this, the HIE in GeoMap-Agent crops the semantic regions of map into subimages for information extraction.As illustrated in Table 6, reducing the resolution of geologic map does not improve performance.Therefore, we conclude that the improvement provided by HIE is not due to the direct reduction in resolution, but rather from the "divide and conquer" strategy.</p>
<p>Discussion</p>
<p>GeoMap-Bench.Although GeoMap-Bench comprehensively evaluates geologic map understanding from various aspects, there is room for extending more abilities and tasks, especially those requiring analysis and external domainspecific knowledge, such as natural resource exploration.Additionally, for essay-type questions, there is potential to improve evaluation methods to enhance both effectiveness and efficiency from different levels.GeoMap-Agent.While GeoMap-Agent performs well in GeoMap-Bench, several limitations are evident based on the framework and specific task evaluations: (1) Despite incorporating domain knowledge from the expert group, conducting reasoning for question answering, such as fault detection and lithology composition, remains challenging.</p>
<p>(2) It struggles to recognize rocks with complex patterns or similar colors in the legend, especially in CGS subset.</p>
<p>To further improve its performance, we could either extend the expert group and tool pool or explore supervised finetuning on MLLMs.Moreover, the framework of our agent could be treated as a paradigm for scenarios facing similar challenges, such as high resolution, multiple associated components, and the need for domain-specific knowledge.</p>
<p>Conclusion</p>
<p>Geologic map is a crucial scientific diagram in geology field with numerous applications.This work leverages MLLMs to investigate and promote the performance of its understanding.To elaborate, the GeoMap-Bench aims to quantify the ability of geologic map understanding across different aspects, and the GeoMap-Agent is a comprehensive framework designed to understand geologic map by question answering, such as lithology composition and earthquake risk assessment.The experiments demonstrate that GeoMap-Agent significantly overperforms publicly available MLLMs on GeoMap-Bench.These findings highlight that GeoMap-Agent effectively addresses the challenges faced by MLLMs, such as handling high resolution, multiple components with intricate interrelations, and the need for domain knowledge.On the one hand, it is expected to help geologists efficiently and comprehensively analyze the vast number of geologic maps worldwide.On the other hand, we believe that the GeoMap-Agent paradigm could also be applied to scenarios with similar challenges.
S f itb (q, a, l) =     
IoU det (a, l), all grounding tasks IoU set (a, l), set extracting tasks S mcq (q, a, l), otherwise (A4) where all grounding tasks encompass tasks of both grounding by name and grounding by intention, and set extracting tasks include tasks of index map extracting and longitude-latitude extracting.</p>
<p>IoU det is the intersection over union metric to evaluate the accuracy of a predicted bounding box against the ground-truth bounding box.
IoU det (b 1 , b 2 ) = I(b 1 , b 2 ) U (b 1 , b 2 ) (A5)
where b 1 and b 2 are two bounding boxes.I and U are functions to calculate the intersection area and union area of two bounding boxes respectively.</p>
<p>IoU set is the intersection over union metric to evaluate the overlap of two sets.
IoU set (A, B) = |A ∩ B| |A ∪ B| (A6)
where A and B are two sets, which could be either discrete, such as neighboring regions, or continuous, like longitude and latitude range.Essay Question.S eq is the type score of an essay question.To avoid any order-related bias of answer-judging agent, it is measured twice per question by keeping and switching the order of two answers.</p>
<p>S eq (q, a, l) = 1 2 (1 − J(q, l, a) + J(q, a, l)) (A7)</p>
<p>J is a answer-judging agent powered by GPT-4o [27], with its prompt detailed in Section C.2.For the given essay question, it is designed to determine which of the two answers is better based on principles of diversity, specificity, and professionalism.
J(q, a 1 , a 2 ) =     
1.0, a1 is better than a2 0.0, a1 is worse than a2 0.5, a1 and a2 are comparable (A8)</p>
<p>where a 1 and a 2 are two input answers of judging agent.</p>
<p>C. Evaluation Prompt</p>
<p>There are two types of prompts used in the evaluation process, the question answering (QA) prompt and the answer judging (AJ) prompt of essay question.We introduce them in the following subsections, where variables are represented in the format ${var name}.</p>
<p>D. Evaluation Setting</p>
<p>Base Model.We set all the random seeds to 42, the temperature to 0, and the maximum tokens to 2048 for base models.Among them, we enable structured mode to enforce responses in JSON format for GPT-4o and GPT-4o-mini.This functionality is not applied to other open-source MLLMs as they do not support it.The system prompt is set to "You are an expert in geology and cartography with a focus on geologic map.".Detection Model.We use YOLOV10 [40] as the detection framework to train the map component detector and legend unit detector models.The training settings are as follows: input images are resized to 640×640 for both detectors, SGD is employed as optimizer with initial learning rate of 0.01 and finnally linear decay to 0.0001.The weight decay is set to 0.0005, and the total number of epochs is 500.The models are trained on single GPU (80GB NVIDIA Ampere A100), where the batch size is 32.We select and annotate approximately 1k original geologic maps as training dataset, ensuring no overlap with the GeoMap-Bench dataset.During the inference stage, the Intersection over Union (IoU) threshold for Non-Maximum Suppression (NMS) is set to 0.8.GEE APIs.In Google Earth Engine (GEE) [13], we use API of "WorldPop/GP/100m/pop" [11,35] image collection to retrieve population density data and API of "ESA/World-Cover/v200" [44] image collection for land cover data.The scale for both collections is set to 100.Scientific DBs.We use USGS earthquake database [39] to retrieve records of historical earthquake data with magnitudes greater than 2.5 occurring since the 1970s.For active faults database, we use GEM DB [36], which currently encompasses most of the deforming continental regions on Earth, with the exceptions of the Malay Archipelago, Madagascar, Canada, and a few other areas.</p>
<p>E. Additional Experiment E.1. Overall Performance Comparison</p>
<p>We compare the performance of different methods evaluated on the entire GeoMap-Bench dataset at both the ability and the task levels.To visually present these results, we use radar charts, as shown in Figure A2 and Figure A3.The results demonstrate that (1) GPT-4o is the best publicly available MLLMs on GeoMap-Bench across various abilities and tasks.(2) Our method, GeoMap-Agent, significantly outperforms all the public MLLMs using GPT-4o as the base model.</p>
<p>E.2. Improvement from Prompt Enhancement</p>
<p>In the last PEQA module, we further improve GeoMap-Agent by enhancing its prompt from 4 aspects.Aside from the first context enhancement, which relies on the global metadata and external knowledge from the initial two modules, the other three can be applied independently.To evaluate the effectiveness, we conduct experiments with and without the last 3 enhancements in the PEQA module.As shown in Table A1, these enhancements led to additional</p>
<p>F. Other Tools F.1. Lithological Mapping Table</p>
<p>To incorporate lithological knowledge into GeoMap-Agent, our professional geologists compile a 3-level lithological table (rock type, rock category, and lithology), containing 335 items in English and 256 items in Chinese, which is scalable as well.A sample of the English lithological table is presented in Table A2.</p>
<p>F.2. Legend Unit Extractor</p>
<p>The legend unit is a standardized component across different geologic map sources.We develop a tool for information extraction within each legend unit, encompassing both text and color extraction.This process is based on the bounding box pairs of text unit and color unit detected by legend unit detector D. For text extraction, we employ the base model to process each cropped legend text unit, using the prompt "Only output the OCR result of the given image.".For color extraction, we calculate the median color in each cropped legend color unit.</p>
<p>Class</p>
<p>Figure 2 .
2
Figure 2. The pipeline of GeoMap-Bench construction.</p>
<p>Figure 3 .
3
Figure3.The distribution of questions in GeoMap-Bench.It consists of 25 task types, which measure MLLMs abilities across 5 aspects.The questions within these different task types are relatively evenly distributed, as indicated by the area of each task.</p>
<p>…FrameworkFigure 4 .
4
Figure 4.The framework of GeoMap-Agent.It consists of three modules: Hierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI), and Prompt-enhanced Question Answering (PEQA).HIE digitizes the map by extracting the global information hierarchically.DKI involves expert group as consultants to provide required domain-specific knowledge for each question.PEQA enhances the QA prompt using the extracted metadata and injected knowledge, as detailed in Section 4.1.3.All the modules are empowered by a tool pool.Both the expert group and tool pool are scalable.</p>
<p>Figure 5 .
5
Figure 5. Example of geologic map and its question answering.The geologic map contains numerous semantic components with specific information.Through HIE, the vision detectors identify these regions, and the base model extracts information region by region, which is then merged into global metadata.In the DKI module, each question acquires specific knowledge by consulting the expert group.Finally, the PEQA module utilizes metadata and knowledge to perform prompt-enhanced QA.</p>
<p>Figure A3 .
A3
Figure A2.Overall performance comparison on different abilities.</p>
<p>Table 2 .
2
A detailed introduction is provided in the following subsections, and the evaluation metrics are defined in the Appendix.
PropertyDescriptionSourceUSGS (English) CGS (Chinese)ContentImage-question pair with annotated answerScale124 images and 3,864 questionsResolution6,146 2 pixels on averageQuestion Type1. Multiple-choice question 2. Fill-in-the-blank question 3. Essay question1. ExtractingCovering Ability2. Grounding 3. Referring 4. Reasoning5. AnalyzingDefined Task 25 tasks</p>
<p>Table 1 .
1
The overview of GeoMap-Bench composition.</p>
<p>What is the location (bounding box) of the title component on the geologic map?FITB scale by name What is the location (bounding box) of the scale component on the geologic map?FITB legend by name What is the location (bounding box) of the legend component on the geologic map?FITB main map by name What is the location (bounding box) of the main map component on the geologic map?FITB index map by name What is the location (bounding box) of the index map component on the geologic map?FITB cross section by name What is the location (bounding box) of the cross section component on the geologic map?FITB stratigraphic column by name What is the location (bounding box) of the stratigraphic column component on the geologic map?FITB
AbilityTaskSampled QuestionTypesheet nameWhat is the name of this map?FITBExtractingscale lonlatWhat is the scale of this map? What is the latitude and longitude ranges of this map?FITB FITBindex mapWhat are the neighboring areas of this region?FITBtitle by nameGrounding(by name)title by intentionWhat location (bounding box) of the geologic map should I focus on to categorize, archive, and retrieve the geologic map?FITBscale by intentionWhat location (bounding box) of the geologic map should I focus on to measure distances and understand the terrain scale?FITBlegend by intentionWhat location (bounding box) of the geologic map should I focus on to identify different geologic units and phenomena through the markings?FITBGrounding (by intention)main map by intention index map by intentionWhat location (bounding box) of the geologic map should I focus on to identify the distribution of specific geologic resources? What location (bounding box) of the geologic map should I focus on to identify the names of adjacent geologic map sheets in different directions?FITB FITBcross section by intentionWhat location (bounding box) of the geologic map should I focus on to understand geologic structures from a three-dimensional perspective?FITB
stratigraphic column by intention What location (bounding box) of the geologic map should I focus on to understand the deposition or formation time of different strata to help determine their age?FITB Referring color by rock In this geologic map, what legend color is used to represent the 'Quartz-sericite phyllite and schist' rock name?MCQ rock by color In this geologic map, what is the rock name whose legend color is closest to #5D1C1C?MCQ Reasoning area comparison Regarding the rock name in main map, which one ranks third in area among 4 choices?MCQ fault existence If the area represented by the geologic map is equally divided into a 3x3 grid, is there a fault in the grid located in the Northeast direction?MCQ lithology composition In this geologic map, which type of primary lithology has the largest proportion?MCQ lonlat localization Can you infer the most likely title of the map in which (longitude:-81.5, latitude:35.25) is located?MCQ Analyzing earthquake risk Based on this geologic map, please analyze the seismic risk in this area from possibility and societal impact?EQ Table</p>
<p>Table 3
3Dataset MethodExtracting Grounding Referring Reasoning Analyzing OverallRandom000.2500.25000.100QWen-chat [5]0.0500.0030.2530.4420.2500.199GLM-4v-9b [12]0.0500.0100.2580.2120.6000.226USGS SetIdefics-9b-instruct [18] Cogvlm2-llama3-chat-19B [15] Monkey-chat [19]0.025 0.033 0.0420.000 0.000 0.0100.247 0.189 0.2130.260 0.177 0.3490.333 0.067 0.2670.173 0.093 0.176GPT-4o-mini [27]0.1830.0500.2780.4560.5120.295GPT-4o [27]0.2080.1000.3980.4940.6830.376GeoMap-Agent (Ours)0.8870.9350.9490.5810.8170.833Random000.2500.25000.100QWen-chat [5]0.0000.0030.2640.3340.4570.211GLM-4v-9b [12]0.2950.0760.2340.3660.4680.287CGS SetIdefics-9b-instruct [18] Cogvlm2-llama3-chat-19B [15] Monkey-chat [19]0.000 0.205 0.0310.000 0.000 0.0020.235 0.236 0.2480.134 0.156 0.1450.457 0.415 0.4570.165 0.202 0.176GPT-4o-mini [27]0.2040.1020.2870.4740.4910.311GPT-4o [27]0.2300.1570.3590.5210.5420.361GeoMap-Agent (Ours)0.7770.9060.8240.5950.8460.789AllRandom000.2500.25000.100SetsGPT-4o0.2190.1280.3780.5070.6120.369GeoMap-Agent0.8320.9200.8860.5880.8310.811
, GeoMap-Agent consistently achieves the best performance across different subsets (USGS and CGS) on all aspects of ability, including extracting, grounding, referring, reasoning, and analyzing.Table3.Evaluation of different methods on GeoMap-Bench.The publicly available MLLMs perform poorly across all metrics.In contrast, our GeoMap-Agent demonstrates superior performance on both subsets, significantly surpassing all other methods in every aspect.</p>
<p>Table 4 .
4
Contributions of each module in GeoMap-Agent on GeoMap-Bench.All other settings remain the same, including the use of GPT-4o as base model.The symbols of "Ext.","Gro.","Ref.","Rea.","Ana.", and "Ove."represent extracting, grounding, referring, reasoning, analyzing, and overall respectively in this and following tables.
Dataset AbilityHIE, DKI, PEQAHIE, DKIHIENoneExt.0.887 0.745 0.741 0.208Gro.0.935 0.760 0.747 0.100USGSRef.0.949 0.835 0.818 0.398SetRea.0.581 0.573 0.526 0.494Ana.0.817 0.786 0.691 0.683Ove.0.833 0.739 0.704 0.376Ext.0.777 0.627 0.596 0.230Gro.0.906 0.810 0.796 0.157CGSRef.0.824 0.755 0.755 0.359SetRea.0.595 0.596 0.550 0.521Ana.0.846 0.783 0.572 0.542Ove.0.789 0.714 0.653 0.361Dataset Ability GPT-4o [27] GPT-4o-mini [27]Ext.0.8870.737Gro.0.9350.841USGSRef.0.9490.813SetRea.0.5810.522Ana.0.8170.791Ove.0.8330.740Ext.0.7770.561Gro.0.9060.735CGSRef.0.8240.637SetRea.0.5950.539Ana.0.8460.825Ove.0.7890.659</p>
<p>Table 5 .
5
Performance comparison of different base model in GeoMap-Agent on GeoMap-Bench.All other settings remain the same, including the use of all 3 modules.</p>
<p>Table 6 .
6
Performance comparison under different resolution scales.All other settings remain the same, including the use of GPT-4o as base model and the application of all 3 modules.Resolution scale 1/2 indicates resizing each original geologic map to 1/2 its original size.
Dataset AbilityResolution Scale11/21/4Ext.0.208 0.183 0.183Gro.0.100 0.083 0.083USGSRef.0.398 0.415 0.393SetRea.0.494 0.479 0.468Ana.0.683 0.350 0.383Ove.0.376 0.302 0.302Ext.0.230 0.230 0.220Gro.0.157 0.176 0.165CGSRef.0.359 0.355 0.366SetRea.0.521 0.519 0.513Ana.0.542 0.534 0.527Ove.0.361 0.362 0.358</p>
<p>Table A1 .
A1
Performance comparison of GeoMap-Agent on GeoMap-Bench with and without prompt enhancement.All other settings remain the same, including the use of GPT-4o as base model and excluding the HIE and DKI modules.
Dataset Abilityenhance prompt w/enhance prompt w/oExt.0.3790.208Gro.0.1230.100USGSRef.0.4150.398SetRea.0.4910.494Ana.0.7330.683Ove.0.4280.376Ext.0.3260.230Gro.0.2580.157CGSRef.0.3310.359SetRea.0.5470.521Ana.0.5840.542Ove.0.4090.361
improvements on GeoMap-Bench.</p>
<p>Table A2 .
A2
Sampled lithological mapping table.
SubclassLithologyconglomerateClastictilliteSedimentarybreccia limestoneCarbonatemarl......trachydaciteAcid volcanickeratophyreVolcanicquartz keratophyre analcimiteAlkali volcanicleucitite......Acid intrusivetonalite plagiograniteIntrusivefoid dioriteAlkaline intrusivefoid gabbro......siliceous slateSlatecharcoal slatesandy slateMetamorphicgraphitic schistSchistactionlite schist amphibole schist......
PEACE: Empowering Geologic Map Holistic Understanding with MLLMsSupplementary MaterialA. Geologic MapGeologic map is a specialized type of map that depicts the distribution, characteristics, and chronological relationships of rock units as well as the occurrence of structural features such as faults and folds.These maps are essential tools for geologists and earth scientists as they provide a visual representation of the geological characteristics of a specific area.Typically, as shown in FigureA1, a geologic map comprises several key elements, including the title, scale, legend, main map, index map, cross section, stratigraphic column, and other components.These elements collectively contribute to the coherence and utility of a geologic map.Specifically, please refer to the following content.Title indicates the physical region, map type, author, and other pertinent information.Scale demonstrates the relationship between distances on the map and physical distances on the ground.Legend explains the symbols and colors used to represent different rock types, ages, and geological features.For detailed information on the legend units, refer to the legend component in FigureA1.Main Map depicts the geological characteristics of the mapped area, including distributions of rock types, ages, folds, and faults.Index Map illustrates the relationship and connection to neighboring regions.Cross Section provides a vertical slice through the Earth, showing the arrangement of rock units below the surface.Stratigraphic Column displays the sequence, thickness, and types of rock layers present in a particular area.Other Components Besides the above 7 key components frequently found in geologic maps, there are additional supplementary components that provide further geological explanation for the region.B. Evaluation MetricsThe metrics are designed to measure the quality of answers generated by AI-based methods for each question in GeoMap-Bench.B.1. Overall ScoreS all is the overall score of an AI-based method on GeoMap-Bench, where M denotes the number of abilities to be measured in it, including extracting, grounding, referring, reasoning, and analyzing.B.2. Ability ScoreS i is the ability score of an AI-based method measured for i-th ability in GeoMap-Bench, where N represents the number of questions pertaining to that ability.T , Q, A, and L indicate the sets of question types, questions, AI-responded answers, and expert-labeled answers respectively.The j-th instance of these sets are denoted as t j , q j , a j , and l j .S i,tj (q j , a j , l j ) (A2)B.3. Type ScoreS i,tj is the type score for the j-th question type within the ith ability.This score can correspond to one of the following types: S mcq for multiple-choice questions, S f itb for fill-inthe-blank questions, and S eq for essay questions.Multiple-choice Question.S mcq is the type score of a multiple-choice question, where q, a, l are a element of sets Q, A, L respectively.S mcq (q, a, l) = 1.0, a = l 0.0, otherwise (A3)Fill-in-the-blank Question.S f itb is the type score of a fillin-the-blank question.
Mapgis. </p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Claude 3 haiku: our fastest model yet. Anthropic, 2024</p>
<p>Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.1296617arXiv preprint3, 2023. 2, 6</p>
<p>Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen, arXiv:2310.02031Oceangpt: A large language model for ocean science tasks. 2023arXiv preprint</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>K2: A foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Yi Xu, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Proceedings of the 17th ACM International Conference on Web Search and Data Mining. the 17th ACM International Conference on Web Search and Data Mining202456</p>
<p>The llama 3 herd of models. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.217832024arXiv preprint</p>
<p>High resolution population distribution maps for southeast asia in 2010 and. Andrea E Gaughan, Forrest R Stevens, Catherine Linard, Peng Jia, Andrew J Tatem, PloS one. 82112015. 2013</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, arXiv:2406.12793A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Google earth engine: Planetary-scale geospatial analysis for everyone. Remote Sensing of Environment. Noel Gorelick, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, Rebecca Moore, 2017611</p>
<p>Ds-agent: Automated data science by empowering large language models with case-based reasoning. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang, arXiv:2402.174532024arXiv preprint</p>
<p>Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, arXiv:2408.16500Visual language models for image and video understanding. 2024arXiv preprint</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Zeljko Kraljevic, Anthony Shek, Daniel Bean, Rebecca Bendayan, James Teo, Richard Dobson, Medgpt, arXiv:2107.03134Medical concept prediction from clinical narratives. 2021arXiv preprint</p>
<p>Obelics: An open web-scale filtered dataset of interleaved image-text documents. Lucile Hugo Laurenc ¸on, Léo Saulnier, Stas Tronchon, Amanpreet Bekman, Anton Singh, Thomas Lozhkov, Siddharth Wang, Alexander Karamcheti, Douwe Rush, Kiela, Advances in Neural Information Processing Systems. 202436</p>
<p>Monkey: Image resolution and text label are important things for large multi-modal models. Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, Xiang Bai, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Video-llava: Learning united visual representation by alignment before projection. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Jin Peng, Li Yuan, arXiv:2311.101222023arXiv preprint</p>
<p>Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, arXiv:2401.00434A scientific large language model in geoscience. 2023arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Biogpt: generative pretrained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in bioinformatics. 2364092022</p>
<p>Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, Stefano Ermon, arXiv:2310.06213Geollm: Extracting geospatial knowledge from large language models. 2023arXiv preprint</p>
<p>Lhrs-bot: Empowering remote sensing with vgi-enhanced large multimodal language model. Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, Pengfeng Xiao, arXiv:2402.025442024arXiv preprint</p>
<p>hello-gpt-4o. 20242, 6, 7, 8, 10OpenAI</p>
<p>Getting to know ArcGIS desktop: basics of ArcView, ArcEditor, and ArcInfo. Tim Ormsby, 2004ESRI, Inc3</p>
<p>Training language models to follow instructions with human feedback. Advances in neural information processing systems. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 202235</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Charting new territories: Exploring the geographic and geospatial capabilities of multimodal llms. Jonathan Roberts, Timo Lüddecke, Rehan Sheikh, Kai Han, Samuel Albanie, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Chulayuth Paul K Rubenstein, Asawaroengchai, Dung Duc, Ankur Nguyen, Zalán Bapna, Félix Borsos, De Chaumont, Peter Quitry, Dalia El Chen, Wei Badawy, Eugene Han, Kharitonov, arXiv:2306.12925A large language model that can speak and listen. 2023arXiv preprint</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Alessandro Sorichetta, Graeme M Hornby, Forrest R Stevens, Andrea E Gaughan, Catherine Linard, Andrew J Tatem, High-resolution gridded population datasets for latin america and the caribbean in 2010, 2015, and 2020. Scientific data. 2015211</p>
<p>The gem global active faults database. Richard Styron, Marco Pagani, Earthquake Spectra. 361112020suppl</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, arXiv:2211.09085Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Earthquake hazards program. USGS. 6111977</p>
<p>Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, arXiv:2405.14458Jungong Han, and Guiguang Ding. Yolov10: Real-time endto-end object detection. 2024611arXiv preprint</p>
<p>Viola: Unified codec language models for speech recognition, synthesis, and translation. Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, Furu Wei, arXiv:2305.161072023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022356</p>
<p>Paml 4: phylogenetic analysis by maximum likelihood. Ziheng Yang, Molecular biology and evolution. 2482007</p>
<p>. Daniele Zanaga, Ruben Van De Kerchove, Dirk Daems, Wanda De Keersmaecker, Carsten Brockmann, Grit Kirches, Jan Wevers, Oliver Cartus, Maurizio Santoro, Steffen Fritz, Esa worldcover 10 m 2021 v200. 2022611</p>
<p>Geogpt: understanding and processing geospatial tasks through an autonomous gpt. Yifan Zhang, Cheng Wei, Shangyou Wu, Zhengting He, Wenhao Yu, arXiv:2307.079302023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
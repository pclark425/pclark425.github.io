<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7615 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7615</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7615</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-272753704</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.12740v1.pdf" target="_blank">HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have achieved remarkable success in various fields, prompting several studies to explore their potential in recommendation systems. However, these attempts have so far resulted in only modest improvements over traditional recommendation models. Moreover, three critical questions remain under-explored: firstly, the real value of LLMs' pre-trained weights, often considered to encapsulate world knowledge; secondly, the necessity of fine-tuning for recommendation tasks; lastly, whether LLMs can exhibit the same scalability benefits in recommendation systems as they do in other domains. In this paper, we propose a novel Hierarchical Large Language Model (HLLM) architecture designed to enhance sequential recommendation systems. Our approach employs a two-tier model: the first Item LLM extracts rich content features from the detailed text description of the item, while the second User LLM utilizes these features to predict users' future interests based on their interaction history. Extensive experiments demonstrate that our method effectively leverages the pre-trained capabilities of open-source LLMs, and further fine-tuning leads to significant performance boosts. Additionally, HLLM achieves excellent scalability, with the largest configuration utilizing 7B parameters for both item feature extraction and user interest modeling. Moreover, HLLM offers excellent training and serving efficiency, making it practical in real-world applications. Evaluations on two large-scale datasets, PixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results, outperforming traditional ID-based models by a wide margin. In online A/B testing, HLLM showcases notable gains, validating its practical impact in real-world recommendation scenarios. Codes are available at https://github.com/bytedance/HLLM.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7615",
    "paper_id": "paper-272753704",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0036745,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling</p>
<p>Junyi Chen chenjunyi.s@bytedance.com 
Lu Chi chilu@bytedance.com 
Bingyue Peng bingyue.peng@bytedance.com 
Zehuan Yuan yuanzehuan@bytedance.com 
HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling
D968173BBC13F4FE4113E0910B8643B8
Large Language Models (LLMs) have achieved remarkable success in various fields, prompting several studies to explore their potential in recommendation systems.However, these attempts have so far resulted in only modest improvements over traditional recommendation models.Moreover, three critical questions remain under-explored: firstly, the real value of LLMs' pre-trained weights, often considered to encapsulate world knowledge; secondly, the necessity of finetuning for recommendation tasks; lastly, whether LLMs can exhibit the same scalability benefits in recommendation systems as they do in other domains.In this paper, we propose a novel Hierarchical Large Language Model (HLLM) architecture designed to enhance sequential recommendation systems.Our approach employs a two-tier model: the first Item LLM extracts rich content features from the detailed text description of the item, while the second User LLM utilizes these features to predict users' future interests based on their interaction history.Extensive experiments demonstrate that our method effectively leverages the pre-trained capabilities of open-source LLMs, and further fine-tuning leads to significant performance boosts.Additionally, HLLM achieves excellent scalability, with the largest configuration utilizing 7B parameters for both item feature extraction and user interest modeling.Moreover, HLLM offers excellent training and serving efficiency, making it practical in real-world applications.Evaluations on two large-scale datasets, PixelRec and Amazon Reviews, show that HLLM achieves state-ofthe-art results, outperforming traditional ID-based models by a wide margin.In online A/B testing, HLLM showcases notable gains, validating its practical impact in real-world recommendation scenarios.Codes are available at https://github.com/bytedance/HLLM.</p>
<p>Introduction</p>
<p>The recommendation algorithm is a classic yet complex problem that requires understanding user interests to predict future behaviors across various items.The key to effective recommendation lies in accurately modeling both item and user features.Currently, mainstream approaches are predominantly ID-based, converting items and users into IDs and creating corresponding embedding tables for encoding (Goldberg et al. 1992;Koren, Bell, and Volinsky 2009;Sarwar et al. 2001).To capture diverse and temporally varying user interests, several sequential modeling methods have been developed, demonstrating notable success in sequential recommendations (Hidasi et al. 2015;Zhou et al. 2018;Kang and McAuley 2018;Sun et al. 2019).However, these methods are typically dominated by embedding parameters and have relatively small model sizes, leading to two major drawbacks: a heavy reliance on ID features which results in poor performance in cold-start scenarios, and relatively shallow neural networks find it difficult to model complex and diverse user interests.</p>
<p>With the advent of ChatGPT (OpenAI 2022), large language models (LLMs) have achieved significant breakthroughs across various domains, showcasing impressive world knowledge and reasoning capabilities (Touvron et al. 2023;Achiam et al. 2023;Team et al. 2023).This success has spurred interest among researchers in exploring the integration of LLMs into recommendation systems (Wu et al. 2023;Li et al. 2023b).These explorations can be broadly categorized into three approaches: (1).Utilizing LLMs to provide refined or supplementary information for recommendation systems (Zhang et al. 2024a;Ren et al. 2024;Xi et al. 2023), such as summary of user behavior and item information expansion.(2).Transforming the recommendation system into a dialogue-driven format compatible with LLMs (Bao et al. 2023;Friedman et al. 2023;Zhang et al. 2023;Yang et al. 2023;Zhai et al. 2023).(3).Modifying LLMs to handle recommendation tasks beyond just text input and output.This includes approaches that input ID features into LLMs (Ning et al. 2024;Zhai et al. 2024;Liao et al. 2024) and those that replace existing models with LLMs, optimizing directly for objectives like Click-Through Rate (CTR) (Cui et al. 2022;Kang et al. 2023).</p>
<p>Despite these advancements, integrating LLMs with recommendation systems presents notable challenges in complexity and effectiveness.One issue is that inputting user behavior history as text to LLMs results in very long input sequences.Consequently, LLMs need longer sequences to represent the same time span of user behavior than ID-based methods, while the complexity of the self-attention module in LLMs scales quadratically with the sequence length.Additionally, recommending a single item requires generating several text tokens, leading to multiple forwards and resulting in lower efficiency.In terms of effectiveness, the arXiv:2409.12740v1[cs.IR] 19 Sep 2024 performance improvements of existing LLM-based methods over traditional methods are not significant, raising questions about whether the potential of LLMs has been fully realized.</p>
<p>Moreover, some critical issues remain underexplored.Firstly, the actual value of pre-trained LLM weights, often regarded as encapsulating world knowledge, needs further investigation.While LLMs offer impressive zero-shot and few-shot capabilities, their value when training on largescale recommendation data is unclear.Secondly, the necessity of fine-tuning for recommendation tasks is in question.LLMs pre-trained on massive corpora exhibit strong world knowledge, but whether further fine-tuning on recommendation tasks enhances or diminishes performance remains to be seen.Lastly, the scalability of LLMs, a hallmark characteristic with proven scaling laws in other domains, requires validation in the context of recommendation systems.While some studies have successfully validated the scaling laws in the recommendation domain (Shin et al. 2023;Zhai et al. 2024), these models have considerably fewer parameters compared to LLMs.Whether models exceeding 1 billion parameters exhibit good scalability in the recommendation domain remains an open question.</p>
<p>To address these challenges, this paper proposes the Hierarchical Large Language Model (HLLM) architecture.The approach begins by using an LLM to extract item features.To empower the LLM to effectively extract these features, a special token is appended to the end of the detailed textual description of each item.This augmented description is then input into the LLM (referred to as the Item LLM), and the output corresponding to the special token is used as the item feature.These item features are then input into a second LLM (referred to as the User LLM) to model user interest and predict future behaviors.By transforming extensive item descriptions into concise embeddings, the length of behavior sequences is reduced to that of ID-based models, significantly lowering computational complexity compared to other text-based LLM recommendation models.We also verified that HLLM has a significant training efficiency advantage compared to ID-based models, as it can surpass IDbased models with only a small amount of training data.</p>
<p>Extensive experiments are conducted to explore the value of pre-training.Although the HLLM does not employ text interaction in the conventional manner of standard LLMs, such as the Item LLM being designed as a feature extractor, and both input and output of the User LLM being item embeddings, the pre-trained weights have proven beneficial for both types of LLMs.This demonstrates that the world knowledge embedded in LLMs is indeed valuable for recommendations.Nevertheless, this does not obviate the need for fine-tuning towards recommendation objectives.Conversely, our experiments indicate that such fine-tuning is crucial for surpassing traditional methods.To verify scalability, experiments on large academic datasets confirm that LLMs exhibit excellent scalability with performance improving as model parameters increase.Within the limited resources, models up to 7 billion parameters show consistent performance gains with increasing size.</p>
<p>Ultimately, the proposed HLLM architecture outperforms existing methods across multiple academic datasets, achieving state-of-the-art results.More importantly, the effectiveness of HLLM is also validated through real-world online A/B testing, confirming its practical applicability.</p>
<p>Our main contributions can be summarized as follows: 1) A novel hierarchical LLM (HLLM) framework is introduced for sequential recommendations.This approach significantly outperforms classical ID-based models on largescale academic datasets and has been validated to yield tangible benefits in real-world industrial settings.Additionally, this method demonstrates excellent training and serving efficiency.</p>
<p>2) HLLM effectively transfers the world knowledge encoded during the LLM pre-training stage into the recommendation model, encompassing both item feature extraction and user interest modeling.Nevertheless, task-specific fine-tuning with recommendation objectives is essential.</p>
<p>3) HLLM exhibits excellent scalability, with performance continuously improving as the data volume and model parameters increase.This scalability highlights the potential of the proposed approach when applied to even larger datasets and model sizes.</p>
<p>Related Work Traditional Recommender Systems</p>
<p>Traditional Recommender Systems predominantly rely on ID-based embeddings, and how to design feature interactions is an important topic.DeepFM (Guo et al. 2017) models low-order feature interactions with FM and models highorder feature interactions with DNN.DCN (Wang et al. 2017(Wang et al. , 2021) ) can model higher-order interactions by explicitly applying feature crossing at each layer.Besides, some researchers make efforts to model user interests from their historical behavior.For instance, DIN (Zhou et al. 2018) and DIEN (Zhou et al. 2019) introduce attention mechanisms to capture user's diverse interests from historical behaviors.Inspired by transformer, SASRec (Kang and McAuley 2018) applies self-attention mechanisms to sequential recommendation.CLUE (Shin et al. 2023) and HSTU (Zhai et al. 2024) demonstrate that models with parameter counts within hundreds of millions adhere to the scaling law.Some works have also introduced content features into recommendation models, showing certain advantages in generalization (Baltescu et al. 2022;Li et al. 2023a;Cheng et al. 2024).</p>
<p>Recommendation with Language Models</p>
<p>The success of LLMs has attracted many researchers to explore their applications in recommendation systems.These explorations can be categorized into three types.Firstly, LLMs are used for summarizing or supplementing information about users or items (Zhang et al. 2024a;Ren et al. 2024;Xi et al. 2023).For example, RLMRec (Ren et al. 2024) develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework.LLMs are also employed to generate augmented training signals for coldstart items (Wang et al. 2024).Secondly, some works Compress the following sentence into embedding: Title: Arctic Terns: Animals that live in the light of day.Tag: Short Film Description: Arctic terns make a round trip to the North and South Poles once a year, a distance of kilometers, which is equivalent to a full circle of the Earth, and live only in the polar daytime, so they are known as the creatures that always live in the lfight ...
••• ••• t 1 t 2 t 3 t 4 t m-2 t m-1 t m [ITEM]
Item Large Language Model
••• ••• E 1 E 2 E 3 E 4 E n-3 E n-2 E n-1 E n E ' 2 E ' 3 E ' 4 E ' 5 E ' n-2 E ' n-1 E ' n E ' n+1
User Large Language Model adapt the recommendation domain data into conversational formats (Bao et al. 2023;Friedman et al. 2023;Zhang et al. 2023;Yang et al. 2023;Zhai et al. 2023).Some approaches treat the recommendation task as a special form of instruction-following, inputting user historical behaviors in text form to the LLM to predict subsequent actions (Li et al. 2024).Lastly, there are also some works that have adapted LLMs for recommendation tasks, allowing their inputs or outputs to go beyond just textual forms.LLaRA (Liao et al. 2024) proposed a novel hybrid prompting method that integrates ID-based item embeddings with textual item features.LEARN (Jia et al. 2024) utilizes pre-trained LLMs to extract item features.LLMs are also adapted to multi-class classification or regression for rating prediction (Kang et al. 2023).However, these methods offer limited improvements compared to traditional recommendation models.</p>
<p>Method</p>
<p>In this section, we first introduce the problem formulation, and then propose Hierarchical Large Language Model (HLLM) with a detailed explanation of how to adapt pretrained large language models to recommendation systems, including item feature extraction and user interest modeling.</p>
<p>Finally we discuss how to align HLLM with the objectives of recommendation systems, thereby significantly enhancing its performance on recommendation tasks.</p>
<p>Problem Formulation</p>
<p>We study the task of sequential recommendations, formulated as: Given a user u ∈ U, a sequence of user u's historical interactions U = {I 1 , I 2 , . . ., I n } in chronological order, predict the next item I n+1 , where n is the length of U and I ∈ I.Each item I has its corresponding ID and text information (e.g.title, tag, etc.), but the method proposed in this paper uses only the text information.</p>
<p>Hierarchical Large Language Model Architecture</p>
<p>Currently, a considerable number of LLM-based recommendation models flatten users' historical behaviors into plain text inputs for the LLM (Kang et al. 2023;Yang et al. 2023;Li et al. 2024).This results in very long input sequences, and due to the self-attention module in LLMs, the complexity grows quadratically with the length of the input sequence.To reduce the burden of user sequence modeling, we adopt a hierarchical modeling approach called the Hierarchical Large Language Model (HLLM) that decouples item modeling from user modeling, as shown in Figure 1.Specifically, we first extract item features using the Item LLM, compressing the complex text descriptions into an embedding representation.Then, we model the user profile based on these item features with the User LLM.Additionally, to ensure better compatibility with pre-trained LLMs and to enhance scalability, we introduce minimal structural changes and design simple yet efficient training objectives.The following is a detailed introduction to item and user modeling.</p>
<p>Item LLM is proposed to extract item features.It takes as input the text description of an item and outputs an embedding representation.LLMs have demonstrated excellent performance in text comprehension, but their use has mostly been limited to text generation scenarios, with few works using them as feature extractors.Inspired by previous works (Devlin 2018;Neelakantan et al. 2022), a special token [ITEM] is added at the end of the item's text description to extract features.Specifically, as shown in Figure 1, for Item I we first flatten its corresponding textual attributes into the sentence T , and prepend it with a fixed prompt.After passing through the LLM tokenizer, we additionally append a special token [ITEM] at the end, thus the input token sequence for the Item LLM can be formulated as {t 1 , t 2 , . . ., t m , [ITEM]} where m represents the length of text tokens.The hidden state from the last layer corresponding to the special token [ITEM] is considered as the item embedding.</p>
<p>User LLM is designed to model user interests which is another key aspect of recommendation systems.The original user history sequence U = {I 1 , I 2 , . . ., I n } can be transformed into a historical feature sequence {E 1 , E 2 , . . ., E n } through the Item LLM, where E i represents the item embedding of I i .The User LLM takes this historical feature sequence as input and predict next item embedding based on a sequence of previous interactions.As shown in Figure 1, the output of the User LLM corresponding to E i is E ′ i+1 , which is expected to be the embedding of I i+1 .</p>
<p>Unlike traditional LLMs with text-in and text-out formats, here both the input and output of the User LLM are item embeddings.Therefore, we discard the word embeddings from the pre-trained LLM but retain all other pre-trained weights.Experiments show that these pre-trained weights are very helpful for reasoning user interests.</p>
<p>Training for Recommendation Objectives</p>
<p>Existing LLMs are all pre-trained using general natural language corpora.Although they possess a wealth of world knowledge and strong reasoning abilities, there remains a considerable gap between their capabilities and those required by recommendation systems.Following the best practices of other works (Zhou et al. 2024;Touvron et al. 2023), we adopt supervised fine-tuning on top of the pretrained LLM.</p>
<p>Recommendation systems can be divided into two categories, generative and discriminative recommendation.It is noteworthy that the proposed HLLM architecture is applicable to both types, requiring only appropriate adjustments to the training objectives.The following sections provide a detailed introduction to the training objectives for both categories.</p>
<p>Generative Recommendation Recent work (Zhai et al. 2024) has provided a successful generative recommendation solution, including both retrieval and ranking.Our approach differs from it in two major ways: the model architecture is upgraded to large language models with pre-trained weights, and the input features are changed from IDs to text-input LLM features.The above differences have minimal impact on the training and serving strategies, therefore, we largely follow approaches proposed in (Zhai et al. 2024).</p>
<p>For the training objective of generative recommendation, next item prediction is adopted, which aims to generate the embedding of the next item given the embeddings of the previous items in the user's history.Specifically, the InfoNCE loss (Oord, Li, and Vinyals 2018) is used during training.For any prediction E ′ i in the output sequence of the User LLM, the positive sample is E i , and the negative samples are randomly sampled from the dataset excluding the current user sequence.The loss function can be formulated as:
L gen = − b j=1 n i=2 log e s(E ′ j,i ,Ej,i) e s(E ′ j,i ,E j,i) + N k e s(E ′ j,i ,E j,i,k )
(1) where s is the similarity function with a learnable temperature parameter, E j,i denotes the i-th item embedding produced by the Item LLM in the j-th user's history interaction and E ′ j,i denotes the i-th item embedding predicted by the User LLM for the j-th user.N is the number of negative samples, E j,i,k represents the k-th negative embedding of
User LLM ••• E 1 E 2 E n [USER] E tgt logit Prediction Head User LLM ••• E 1 E 2 E n logit Prediction Head E tgt</p>
<p>(a). Early Fusion (b). Late Fusion</p>
<p>Figure 2: Two User LLM variants for discriminative recommendations.</p>
<p>E ′ j,i .b represents the total number of users within the batch, n is the length of user history interactions.</p>
<p>Discriminative Recommendation Since discriminative recommendation models still dominate in the industry, we also present an application scheme for HLLM under discriminative recommendation models.The optimization objective of discriminative models is to judge, given a user sequence U and a target item I tgt , whether the user is interested in the target item (e.g., by clicking, liking, purchasing, etc.).</p>
<p>As shown in Figure 2, there are two User LLM variants for discriminative recommendation, while keeping the Item LLM unchanged.Early fusion appends the target item embedding E tgt to the end of the user's historical sequence, then produces a high-order cross feature through User LLM, and finally inputs this cross feature into the prediction head to generate the final logits.Late fusion, on the other hand, first uses the User LLM to extract user features, which are independent of the target item, in a manner similar to the Item LLM feature extraction.A special token [USER] is added to the end of the user sequence to extract user representation.The user embedding and the target item embedding are then input together into the prediction head to predict the final logits.Early fusion, due to its deep integration of user interests and the target item, tends to perform better but is challenging to apply simultaneously across numerous candidates; conversely, late fusion is more efficient since different candidates share the same user features, but typically sees a performance decline.</p>
<p>The training objective of discriminative recommendation is usually a classification task, such as predicting whether a user will click, etc.For the binary classification example, the training loss is as follows:
L cls = − (y • log(x) + (1 − y) • log(1 − x))(2)
where y denotes the label of the training sample and x denotes the predicted logit.</p>
<p>Empirically, next item prediction can also be used as an auxiliary loss in discriminative models to further enhance performance.Hence, the final loss can be formulated as follows:
L dis = λL gen + L cls
(3) where λ controls the weight of the auxiliary loss.</p>
<p>Dataset</p>
<p>Experiments</p>
<p>In this section, we first introduce the basic experimental settings, and then numerous experiments are conducted to address the following research questions: RQ1: Does the general pre-training of the LLM and the fine-tuning with recommendation objectives improve the final recommendation performance?</p>
<p>RQ2: Does HLLM have good scalability?RQ3: Are the advantages of HLLM significant compared with other state-of-the-art models?RQ4: How does the training and serving efficiency compare with ID-based models?</p>
<p>Finally, we demonstrate how to deploy HLLM in online scenarios and achieve real-world benefits.</p>
<p>Datasets and Evaluation Setup</p>
<p>For offline experiments, we evaluate HLLM on two largescale datasets: PixelRec (including three subsets: 200K, 1M, and 8M) (Cheng et al. 2024), and Amazon Book Reviews (Books) (McAuley et al. 2015).Consistent with previous works (Cheng et al. 2024;Zhai et al. 2024), we adopt the same data preprocessing and evaluation protocols to ensure a fair comparison.A more detailed analysis of these datasets after preprocessing is presented in Table 1 and Figure 5.We utilize a leave-one-out approach to split the data into training, validation, and testing sets.Performance is measured using the metrics Recall@K (R@K) and NDCG@K (N@K).All open-source datasets are employed solely for training and evaluating in offline experiments.</p>
<p>Baselines and Training</p>
<p>For baselines, we use two ID-based sequential recommenders SASRec (Kang and McAuley 2018), and HSTU (Zhai et al. 2024).They are all aimed at industrial applications and boast state-of-the-art performance.</p>
<p>For offline experiments, the generative recommendation is used to stay consistent with other methods.For the online A/B test, discriminative recommendation is used to better #Tokens R@5 R@10 N@5 N@10 CSR ↑ 0T</p>
<p>3 align with the online system1 .</p>
<p>In HLLM-1B, we use TinyLlama-1.1B(Zhang et al. 2024b) for both Item LLM and User LLM.Correspondingly, in HLLM-7B, we utilize Baichuan2-7B (Baichuan 2023) for both.Due to resource constraints, HLLMs are trained only 5 epochs on PixelRec and Amazon Reviews while other models are trained 50 and 200 epochs, respectively.The learning rate is set to 1e-4.Each item's text length is truncated to a maximum of 256.On PixelRec, following PixelNet (Cheng et al. 2024), we utilize a batch size of 512.The maximum sequence length is set to 10, and the ratio of positive to negative samples is 1:5632.On Books, we utilize a batch size of 128, set a maximum sequence length of 50, and the number of negative samples is 512.</p>
<p>For a fair comparison, we also implemented SASRec-1B (replacing its network structure with TinyLlama-1.1B)and HSTU-1B, which uses the same hidden size and number of layers as TinyLlama-1.1Bbut has only 462M parameters due to the elimination of the traditional FFN.</p>
<p>Pre-training and Fine-tuning (RQ1)</p>
<p>As clearly seen from Table 2, pre-trained weights are beneficial for HLLM, including both item feature extraction and user interest modeling.Furthermore, as shown in Table 3, the performance is positively correlated with the number of pre-trained tokens, indicating that the quality of pre-trained weights also impacts the recommendation task.However, supervised fine-tuning (SFT) on conversation data can result in slight negative effects, probably because world knowledge is primarily acquired during the pre-training stage, and SFT mainly enhances instruction-following abilities, which do not aid in recommendation tasks (Zhou et al. 2024).</p>
<p>Item Model #Params R@5 R@10 N@5 N@10 It is also evident that fine-tuning both the Item LLM and User LLM is crucial for outperforming ID-based models, as shown in Table 4.When we freeze the Item LLM and only fine-tune the User LLM, using mean pooling of all token outputs in the last layer of TinyLlama-1.1Bas item features, we find that the performance is very poor.This indicates that LLMs trained on predicting the next token are not directly suitable as feature extractors.Similarly, when we use an Item LLM that has been fine-tuned on Pixel200K and freeze the pre-trained User LLM, the performance remains critically low.</p>
<p>Scaling Up (RQ2)</p>
<p>The experimental results for increasing the model's parameter count are shown in Table 5 and Table 6.It can be observed that the growth in the number of parameters for both Item LLM and User LLM consistently leads to performance improvements.Finally, we scale up both the Item LLM and User LLM from 1 billion parameters to 7 billion parameters on the Amazon Books.As shown in Table 7, this leads to further performance improvements, demonstrating that HLLM has excellent scalability.</p>
<p>To explore scalability of data volume, we sampled multiple different scales of data from Pixel8M for training, ranging from 0.1M to 8M in size.From Figure 3, it is evident that HLLM demonstrates remarkable scalability across various data volumes.With increasing data, significant enhancements in performance are observed, and no performance bottlenecks are observed at the current data scale.</p>
<p>We also conducted more comprehensive ablation experiments related to scaling up on a large-scale industrial recommendation dataset to demonstrate the scalability of the HLLM architecture, with detailed experimental results presented in the appendix.</p>
<p>HLLM vs. SOTA Methods (RQ3)</p>
<p>In Table 7, we compare the performance of HLLM with the current state-of-the-art models, including ID-based models such as SASRec (Kang and McAuley 2018)   HSTU (Zhai et al. 2024), as well as the text-based model LEARN (Jia et al. 2024) on the Pixel8M and Amazon Book Reviews datasets.They all exhibit excellent performance and are dedicated to industrial practice.</p>
<p>It's clear that HLLM holds a significant performance advantage, decisively outperforming other models on all metrics across all datasets.Under the same experimental settings, compared to the lowest-performing baseline, HLLM-1B shows an average improvement of 22.93% on Pixel8M, and an even more significant average improvement of 108.68% on Books.In contrast, ID-based models only show a maximum improvement of 5.37% on Pixel8M and 64.96% on Books.</p>
<p>Furthermore, it is notable that when ID-based models increase the number of negative samples and batch size, the performance improvements are relatively modest, especially in R@200 where HSTU-large only increases by 0.76, while HLLM-1B increases by 2.44 under the same setting.By further increasing the model's parameters, HLLM-7B achieves a significant improvement of 169.58% compared to the baseline, which is highly impressive.</p>
<p>The table also shows that even with fully converged IDbased models, the gains from increasing parameters are minimal.On Pixel8M, both SASRec-1B and HSTU-1B show relatively modest improvements compared to smaller sizes, while on Books, SASRec-1B even experiences a decline in all metrics.In contrast, for HLLM, scaling up from HLLM-1B to HLLM-7B still results in corresponding performance improvements on recommendation tasks, demonstrating the superiority of the HLLM architecture.</p>
<p>Training and Serving Effeciency (RQ4)</p>
<p>Firstly, HLLM shows better training data efficiency than IDbased models.As shown in Figure 3, HLLM requires only one-sixth to one-fourth of the data volume to achieve performance on par with ID-based methods.</p>
<p>Previous extensive experiments have shown that fully fine-tuning the entire HLLM significantly improves performance but requires real-time encoding of all items during inference, which is inefficient.Thanks to the decoupling of item and user encoding in HLLM, our architecture can reduce computational complexity by caching item embeddings in advance.To demonstrate the feasibility of item caching, we pre-trained HLLM on sequences longer than 10 from the Pixel8M dataset, truncating sequences at the tenth position to avoid data leakage, covering 3 million users.Based on this pre-trained HLLM, we freeze the Item LLM and fine-tune only the User LLM on Pixel8M.Results in Ta-Dataset Method R@10 R@50 R@200 N@10 N@50 N@200 Impv.(avg)</p>
<p>Pixel8M</p>
<p>SASRec vit (2024) 3  Method R@5 R@10 N@5 N@10 HSTU-1B 3.501 5.120 2.358 2.879 HLLM-1B cache 3.585 5.218 2.432 2.958 HLLM-1B</p>
<p>4.278 6.106 2.935 3.524</p>
<p>Table 8: Experiments on the effectiveness of item caching.HLLM-1B cache utilizes a pre-trained item HLLM to extract item features, but the parameters are frozen.</p>
<p>ble 8 show that while freezing the Item LLM leads to some metric decreases, performance still exceeds ID-based models, proving item caching is more effective.Given that user behaviors in industrial scenarios far exceed the number of items, HLLM's training and serving costs can match those of ID-based models.Notably, our pre-training data constitutes less than half of Pixel8M, with some items not appearing in the pre-training data, yet we still achieve respectable performance.Experiments on industrial data show that as the amount of pre-training data increases, the gap between the item caching and the full fine-tuning is largely narrowed.</p>
<p>Online A/B Test</p>
<p>Apart from offline experiments, HLLM is also targeted at and successfully applied in real-world industrial practices.For simplicity, flexibility, and to better align with the online system, we adopted HLLM-1B, using the discriminative recommendation approach with the late fusion variant for optimization.Considering the balance between performance and efficiency, our training process is divided into the following three stages: Regarding serving, as shown in Figure 4, item embeddings are extracted when they are created, and user embeddings are updated on a daily basis only for users who had activity the previous day.Embeddings of items and users are stored for online model training and serving.Under this approach, the inference time of the online recommendation system is virtually unchanged.</p>
<p>Finally, we test HLLM in online A/B experiments of the ranking task.Key metrics have shown a significant increase of 0.705%.</p>
<p>In this paper, we propose a novel Hierarchical Large Language Model (HLLM) architecture designed to enhance sequential recommendations.HLLM leverages LLMs to extract item features and model user interests, effectively integrating pre-training knowledge into the recommendation system, and it is proved that fine-tuning with recommendation objectives is essential.HLLM exhibited excellent scalability with larger model parameters.Experiments demonstrated that HLLM outperforms traditional ID-based models, achieving state-of-the-art results on academic datasets.Real-world online A/B testing further validated HLLM's practical efficiency and applicability, marking a significant advancement in the field of recommendation systems.9 shows that the text content has a significant impact on the final performance.Richer text content and longer text lengths allow the Item LLM to extract more detailed item features, better differentiate between items, and more effectively aid the User LLM in modeling user interests.</p>
<p>Method of Item LLM Feature Extraction</p>
<p>To enable LLMs trained on next token prediction to have feature extraction capabilities, we add a special token</p>
<p>[ITEM] at the end of the text input.Another feasible feature extraction approach is to take the average of the hidden states from the final layer of the LLM to represent the features of the entire sentence.Table 10 shows the comparison results of these two methods.As can be seen, using the [ITEM] token is better than mean pooling.</p>
<p>Sequence Length of User LLM</p>
<p>Compatibility with ID-based Features</p>
<p>In the previous sections, we primarily modeled item and user features based on the textual descriptions of items.Most current recommendation systems, however, still rely on ID features, including not only Item IDs but also features like actions, timestamps, and item categories in ID form.Here, we present a compatibility solution for integrating HLLM with ID features, and demonstrate that complementary ID features, when combined with item descriptions, can indeed bring significant improvements to HLLM, further highlighting its application value in industrial environments.</p>
<p>Here, we choose the raw item IDs and timestamps as ID features for validation.The item IDs are transformed into id embeddings through an embedding lookup table.The behavior's timestamp is first split into specific year, month, day, hour, minute, and second components, obtaining the timestamp embedding as Algorithm 1.We perform sum pooling with the ID features and item LLM embeddings before inputting them into the User LLM.The prediction target during training remains the item embedding extracted by the Item LLM, and the experimental results are shown in Table 12.The introduction of item IDs actually results in a slight decrease in performance, likely because the item IDs do not provide incremental information beyond what is already captured by the textual descriptions, which comprehensively describe the item's characteristics and are sufficiently extracted by the Item LLM.However, the improvement resulting from the introduction of timestamps is very pronounced, as timestamps complement the textual descriptions.This also demonstrates that our method can be compatible with ID-based features.</p>
<p>B Scaling Up of HLLM on Industrial Dataset</p>
<p>More extensive experiments are conducted on a large-scale industrial dataset to evaluate the scalability of HLLM.Douyin has a vast number of users and recommendation candidates, with extensive records of user behavior.We construct a dataset comprising 30 million samples from the past 3 years' logs.Each sample includes only the user's historical click sequence, the target item, and a label indicating whether the item was clicked or not.We validate the effectiveness of HLLM in a discriminative recommendation system, using AUC as the evaluation metric, and verifying scalability from two aspects: the sequence length of User LLM, and the parameters of both Item LLM and User LLM.</p>
<p>Sequence Length of User LLM</p>
<p>The length of user behavior sequences in the industrial dataset is shown in Figure 5.And table 13 shows the impact of user sequence length, with HLLM's performance steadily increasing as the sequence length This illustrates HLLM's substantial in modeling users with longer sequences.Since Pixel200K is randomly sampled from Pixel1M, their distributions are consistent.We truncate the sequence length to a maximum of 2,000 for industrial data, hence p90 is exactly 2,000.</p>
<p>Parameters of Item LLM and User LLM</p>
<p>Algorithm 1: Pseudo code of timestamp processing in a PyTorch-like style.</p>
<p>and</p>
<p>Figure 3 :
3
Figure3: Experiments of HLLM's performance at various data scales.Recall@5 and NDCG@5 are reported.</p>
<p>Figure 4 :
4
Figure 4: An overview of the online system.celerate training.Stage II: We first use the Item LLM trained in Stage I to encode and store the embeddings of all items in the recommendation system.We then continue to train only the User LLM by retrieving the necessary item embeddings from storage.Since this stage only trains the User LLM, it significantly reduces the training demand, allowing us to extend the user sequence length from 150 to 1000, further enhancing the effectiveness of the User LLM.Stage III: After extensive data training in the first two stages, the HLLM model parameters are no longer updated.We extract features for all users which are then combined with item LLM embeddings and other existing features and fed into the online recommendation model for training.Regarding serving, as shown in Figure4, item embeddings are extracted when they are created, and user embeddings are updated on a daily basis only for users who had activity the previous day.Embeddings of items and users are stored for online model training and serving.Under this approach, the inference time of the online recommendation system is virtually unchanged.Finally, we test HLLM in online A/B experiments of the ranking task.Key metrics have shown a significant increase of 0.705%.</p>
<p>Figure 5 :
5
Figure5: Distribution of textual descriptions (flattening all attributes) and sequence lengths in Pixel200K, Pixel1M, Pixel8M, Amazon Book Reviews and industrial scenario.Since Pixel200K is randomly sampled from Pixel1M, their distributions are consistent.We truncate the sequence length to a maximum of 2,000 for industrial data, hence p90 is exactly 2,000.</p>
<h2></h2>
<p>Control the precision of time, such as 4 to the hour, and 6 to the second.5self.time_num= time_num 6 self.time_embeddings= nn.ModuleList(nn.Embedding(x, time_dim) for x in[2100, 13, 32, 24, 60, 60])7 Projection from time_dim to user_dim 8 self.merge_time= MLP(time_dim * time_num, user_dim) .datetime.fromtimestamp(time)16 split_time.append([dt.year,dt.month, dt.day, dt.hour, dt.minute, dt.second]) self, timestamps: List) -&gt; torch.tensor:20 # Times: timestamp of each item in List format (bs, seq) 21 # (bs, seq) -&gt; (bs, seq, 6) 22 time_seq = torch.tensor([self.split_time(x) for x in timestamps]) 23 # (bs, seq, 6) -&gt; [(bs, seq, time_dim)] * time_num 24 time_emb = [self.time_embeddings<a href="time_seq[...,i]">i</a> for i in range(self.time_num)] 25 # [(bs, seq, time_dim)] * time_num -&gt; (bs, seq, time_dim * time_num) 26 time_emb = torch.cat(time_emb,dim=-1) 27 # (bs, seq, time_dim * time_num) -&gt; (bs, seq, user_dim) 28 time_emb = self.merge_time(time_emb)29 return time_emb</p>
<p>Table 1 :
1
Statics of PixelRec and Amazon Book Reviews.</p>
<h1>User#Item #InteractionPixel200K200,00096,2823,965,656Pixel1M1,001,822 100,54119,886,579Pixel8M8,886,078 407,082 158,488,652Books694,898 686,62410,053,086Item LLMUser LLMR@5 R@10 N@5 N@10ScratchScratch3.330 5.0632.199 2.755ScratchPre-trained 3.556 5.4162.371 2.969Pre-trained Scratch3.521 5.3312.358 2.940Pre-trained Pre-trained 3.755 5.5812.513 3.100</h1>
<p>Table 2 :
2
Ablation studies of pre-training on Pixel200K with HLLM-1B.</p>
<p>Table 5 :
5
Experiments with different sizes of the item model on Pixel200K.SASRec is used as the user model for all.
BERT-Base110M2.576 4.0201.694 2.158BERT-Large 340M3.032 4.6351.993 2.508TinyLlama1.1B3.484 5.2392.319 2.883User Model #Params R@5 R@10 N@5 N@10SASRec4M3.484 5.2392.319 2.883Llama-2L0.1B3.494 5.2332.338 2.898TinyLlama1.1B3.521 5.3312.358 2.940Table 6: Experiments with different sizes of the user modelon Pixel200K. Llama-2L maintains the same architecture asLlama but uses only 2 decoder layers. TinyLlama-1.1B isused as the item model for all. All user models are trainedfrom scratch.</p>
<p>Table 7 :
7
Performance comparison of HLLM with SOTA models.SASRec vit means SASRec uses the ViT as an image encoder for item encoding and trained by BCE loss from (Cheng et al. 2024).* indicates the result is reproduced by us.† indicates the number of negative samples and the batch size are increased from 512 and 128 to 28k and 512, respectively."Scratch" indicates both Item LLM and User LLM are trained from scratch.</p>
<p>Table 10 :
10
Ablation studies of Item LLM feature extraction method on Pixel200K.Mean pooling refers to using the mean pooling of hidden states from the last layer of the Item LLM as the item features.SASRec is used as the user model.AMore Experiments on Academic DatasetsTextual Input Length and Richness of Item LLM By default, we input all types of text information with a length of 256.Here, we conduct ablation experiments on text length and richness.Table
Tag Title Description Length R@5 R@10640.082 0.196643.520 5.317643.610 5.439643.647 5.5022563.755 5.581Tag Title Description Length N@5 N@10640.049 0.086642.348 2.926642.415 3.003642.430 3.0262562.513 3.099Table 9: Ablation studies of text length and richness onPixel200K.MethodR@5 R@10 N@5 N@10Mean Pooling3.386 5.1592.257 2.826[ITEM] Token 3.484 5.2392.319 2.883</p>
<p>Table 12 :
12
Ablation studies of input features of User LLM on Pixel1M.LLM Emb represents the item features extracted using the Item LLM based on textual descriptions.
We explore the impact of input sequence length of UserLLM on HLLM's recommendation performance in Ta-ble 11. Similar to other sequential recommenders, HLLMcan also benefit from expanding the length of the input se-quence. Although the table shows only modest performancegains with increasing sequence length, we suspect this islikely because user sequence lengths are generally quiteshort in the academic dataset as shown in Figure 5. As shownin Appendix B, in the real-world industrial systems, where</p>
<p>Table 13 :
13
Experiments on the sequence length of User LLM on the industrial dataset.
Sequence Length AUC2000.74295000.74461,0000.7458Item LLM User LLM AUC1B1B0.74581B7B0.74987B1B0.75177B7B0.7533</p>
<p>Table 14 :
14
Experiments with different sizes of Item LLM and User LLM on the industrial dataset.</p>
<p>Table14illustrates the impact of the parameters of HLLM in industrial scenario.For both Item LLM and User LLM, AUC consistently increases with the growth in the number of parameters.</p>
<p>Experiments demonstrated that most conclusions drawn from the academic dataset still hold true on large-scale industrial benchmarks.</p>
<p>J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023arXiv preprint</p>
<p>Baichuan 2: Open Large-scale Language Models. Baichuan, arXiv:2309.103052023arXiv preprint</p>
<p>Itemsage: Learning product embeddings for shopping recommendations at pinterest. P Baltescu, H Chen, N Pancha, A Zhai, J Leskovec, C Rosenberg, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2022</p>
<p>Tallrec: An effective and efficient tuning framework to align large language model with recommendation. K Bao, J Zhang, Y Zhang, W Wang, F Feng, X He, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>. Y Cheng, Y Pan, J Zhang, Y Ni, A Sun, F Yuan, </p>
<p>M6-rec: Generative pretrained language models are open-ended recommender systems. Siam, Z Cui, J Ma, C Zhou, J Zhou, H Yang, arXiv:2205.08084Proceedings of the 2024 SIAM International Conference on Data Mining (SDM). the 2024 SIAM International Conference on Data Mining (SDM)2022arXiv preprintAn Image Dataset for Benchmarking Recommender Systems with Raw Pixels</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, arXiv:1810.048052018arXiv preprint</p>
<p>Leveraging large language models in conversational recommender systems. L Friedman, S Ahuja, D Allen, Z Tan, H Sidahmed, C Long, J Xie, G Schubiner, A Patel, H Lara, arXiv:2305.07961Communications of the ACM. 35122023. 1992arXiv preprintUsing collaborative filtering to weave an information tapestry</p>
<p>DeepFM: a factorization-machine based neural network for CTR prediction. H Guo, R Tang, Y Ye, Z Li, X He, arXiv:1703.042472017arXiv preprint</p>
<p>Session-based recommendations with recurrent neural networks. B Hidasi, A Karatzoglou, L Baltrunas, D Tikk, J Jia, Y Wang, Y Li, H Chen, X Bai, Z Liu, J Liang, Q Chen, H Li, P Jiang, arXiv:1511.06939arXiv:2405.03988Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application. 2015. 2024arXiv preprint</p>
<p>Self-attentive sequential recommendation. W.-C Kang, J Mcauley, 2018 IEEE international conference on data mining (ICDM). IEEE2018</p>
<p>W.-C Kang, J Ni, N Mehta, M Sathiamoorthy, L Hong, E Chi, D Z Cheng, arXiv:2305.06474Do llms understand user preferences? evaluating llms on user rating prediction. 2023arXiv preprint</p>
<p>Matrix factorization techniques for recommender systems. Y Koren, R Bell, C Volinsky, Computer. 4282009</p>
<p>Text is all you need: Learning language representations for sequential recommendation. J Li, M Wang, J Li, J Fu, X Shen, J Shang, J Mcauley, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023a</p>
<p>Large language models for generative recommendation: A survey and visionary discussions. L Li, Y Zhang, D Liu, L Chen, arXiv:2309.011572023barXiv preprint</p>
<p>Y Li, X Zhai, M Alzantot, K Yu, I Vulić, A Korhonen, M Hammad, arXiv:2405.02429CALRec: Contrastive Alignment of Generative LLMs For Sequential Recommendation. 2024arXiv preprint</p>
<p>Llara: Large language-recommendation assistant. J Liao, S Li, Z Yang, J Wu, Y Yuan, X Wang, X He, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval2024</p>
<p>Image-based recommendations on styles and substitutes. J Mcauley, C Targett, Q Shi, Van Den, A Hengel, Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval. the 38th international ACM SIGIR conference on research and development in information retrieval2015</p>
<p>A Neelakantan, T Xu, R Puri, A Radford, J M Han, J Tworek, Q Yuan, N Tezak, J W Kim, C Hallacy, arXiv:2201.10005Text and code embeddings by contrastive pretraining. 2022arXiv preprint</p>
<p>L Ning, L Liu, J Wu, N Wu, D Berlowitz, S Prakash, B Green, S O'banion, J Xie, arXiv:2402.13598User-LLM: Efficient LLM Contextualization with User Embeddings. 2024arXiv preprint</p>
<p>Representation learning with contrastive predictive coding. A V D Oord, Y Li, O Vinyals, arXiv:1807.03748OpenAI. 2022. Introducing ChatGPT. 2018arXiv preprint</p>
<p>Representation learning with large language models for recommendation. X Ren, W Wei, L Xia, L Su, S Cheng, J Wang, D Yin, C Huang, Proceedings of the ACM on Web Conference 2024. the ACM on Web Conference 20242024</p>
<p>Item-based collaborative filtering recommendation algorithms. B Sarwar, G Karypis, J Konstan, J Riedl, Proceedings of the 10th international conference on World Wide Web. the 10th international conference on World Wide Web2001</p>
<p>Scaling law for recommendation models: Towards general-purpose user representations. K Shin, H Kwak, S Y Kim, M N Ramström, J Jeong, J.-W Ha, K.-M Kim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. F Sun, J Liu, J Wu, C Pei, X Lin, W Ou, P Jiang, Proceedings of the 28th ACM international conference on information and knowledge management. the 28th ACM international conference on information and knowledge management2019</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Large Language Models as Data Augmenters for Cold-Start Item Recommendation. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2024arXiv preprintIn Companion Proceedings of the ACM on Web Conference 2024</p>
<p>Deep &amp; cross network for ad click predictions. R Wang, B Fu, G Fu, M Wang, Proceedings of the AD-KDD. the AD-KDD201717</p>
<p>Dcn v2: Improved deep &amp; cross network and practical lessons for web-scale learning to rank systems. R Wang, R Shivanna, D Cheng, S Jain, D Lin, L Hong, E Chi, Proceedings of the web conference 2021. the web conference 20212021</p>
<p>L Wu, Z Zheng, Z Qiu, H Wang, H Gu, T Shen, C Qin, C Zhu, H Zhu, Q Liu, arXiv:2305.19860A survey on large language models for recommendation. 2023arXiv preprint</p>
<p>Y Xi, W Liu, J Lin, X Cai, H Zhu, J Zhu, B Chen, R Tang, W Zhang, R Zhang, arXiv:2306.10933Towards openworld recommendation with knowledge augmentation from large language models. 2023arXiv preprint</p>
<p>. F Yang, Z Chen, Z Jiang, E Cho, X Huang, Y Lu, </p>
<p>Palr: Personalization aware llms for recommendation. arXiv:2305.07622arXiv preprint</p>
<p>Actions speak louder than words: Trillion-parameter sequential transducers for generative recommendations. J Zhai, L Liao, X Liu, Y Wang, R Li, X Cao, L Gao, Z Gong, F Gu, M He, arXiv:2402.171522024arXiv preprint</p>
<p>Knowledge prompt-tuning for sequential recommendation. J Zhai, X Zheng, C.-D Wang, H Li, Y Tian, Proceedings of the 31st ACM International Conference on Multimedia. the 31st ACM International Conference on Multimedia2023</p>
<p>C Zhang, Y Sun, J Chen, J Lei, M Abdul-Mageed, S Wang, R Jin, S Park, N Yao, B Long, arXiv:2402.10555SPAR: Personalized Content-Based Recommendation via Long Engagement Attention. 2024aarXiv preprint</p>
<p>J Zhang, R Xie, Y Hou, W X Zhao, L Lin, J.-R Wen, arXiv:2305.07001Recommendation as instruction following: A large language model empowered recommendation approach. 2023arXiv preprint</p>
<p>P Zhang, G Zeng, T Wang, W Lu, arXiv:2401.02385Tinyllama: An open-source small language model. 2024barXiv preprint</p>
<p>Lima: Less is more for alignment. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, Advances in Neural Information Processing Systems. 202436</p>
<p>Deep interest evolution network for click-through rate prediction. G Zhou, N Mou, Y Fan, Q Pi, W Bian, C Zhou, X Zhu, K Gai, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>Deep interest network for click-through rate prediction. G Zhou, X Zhu, C Song, Y Fan, H Zhu, X Ma, Y Yan, J Jin, H Li, K Gai, Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining2018</p>            </div>
        </div>

    </div>
</body>
</html>
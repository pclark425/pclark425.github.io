<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7870 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7870</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7870</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-19f1ad5c233e8e17b8defa02dd9cc750af16509a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/19f1ad5c233e8e17b8defa02dd9cc750af16509a" target="_blank">Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Experiments on three MRC tasks demonstrate the effectiveness of the proposed recurrent chunking mechanisms: they can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.</p>
                <p><strong>Paper Abstract:</strong> In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer. State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document. However, these transformer models can only take as input a fixed-length (e.g., 512) text. To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering. Moreover, they are less capable of answering questions that need cross-segment information. We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also apply recurrent mechanisms to enable information to flow across segments. Experiments on three MRC tasks – CoQA, QuAC, and TriviaQA – demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7870",
    "paper_id": "paper-19f1ad5c233e8e17b8defa02dd9cc750af16509a",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0036405,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Recurrent Chunking Mechanisms for Long-Text Machine Reading Comprehension</h1>
<p>Hongyu Gong ${ }^{1 *}$ Yelong Shen ${ }^{2 \dagger}$ Dian Yu ${ }^{3}$ Jianshu Chen ${ }^{3}$ Dong Yu ${ }^{3}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign, IL, USA<br>${ }^{2}$ Microsoft Dynamics 365 AI, Redmond, WA, USA<br>${ }^{3}$ Tencent AI Lab, Bellevue, WA, USA<br>hgong6@illinois.edu<br>yeshe@microsoft.com<br>{yudian, jianshuchen, dyu}@tencent.com</p>
<h4>Abstract</h4>
<p>In this paper, we study machine reading comprehension (MRC) on long texts, where a model takes as inputs a lengthy document and a question and then extracts a text span from the document as an answer. State-of-the-art models tend to use a pretrained transformer model (e.g., BERT) to encode the joint contextual information of document and question. However, these transformer-based models can only take a fixed-length (e.g., 512) text as its input. To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments. As a result, they may form segments that fail to cover the correct answer span or retain insufficient contexts around it, which significantly degrades the performance. Moreover, they are less capable of answering questions that need cross-segment information. We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction. We also employ recurrent mechanisms to enable information to flow across segments. Experiments on three MRC datasets - CoQA, QuAC, and TriviaQA - demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.</p>
<h2>1 Introduction</h2>
<p>Teaching machines to read, process, and comprehend natural language is a coveted goal of machine reading comprehension (MRC) problems</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016; Trischler et al., 2017; Zhang et al., 2018; Kočiskỳ et al., 2018). Many existing MRC datasets have a similar task definition: given a document and a question, the goal is to extract a span from the document (in most cases) or instead generate an abstractive answer to answer the question.</p>
<p>There is a growing trend of building MRC readers (Hu et al., 2019; Xu et al., 2019; Yang et al., 2019a; Keskar et al., 2019) based on pre-trained language models (Baker et al., 2019; Yang et al., 2019b), such as GPT (Radford et al., 2018) and BERT (Devlin et al., 2019). These models typically consist of a stack of transformer layers that only allow fixed-length (e.g., 512) inputs. However, it is often the case that input sequences exceed the length constraint, e.g., documents in the TriviaQA dataset (Joshi et al., 2017) contain 2,622 tokens on average. Some conversational MRC datasets such as CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) often go beyond the length limit as we may need to incorporate previous questions as well as relatively long documents into the input to answer the current question.</p>
<p>To deal with long text inputs, a commonly used approach firstly chunks the input text into equallyspaced segments, secondly predicts the answer for each individual segment, and finally ensembles the answers from multiple segments (Devlin et al., 2019). However, there are two major limitations of this approach: first, a predetermined large stride size for chunking may result in incomplete answers, and we observe that models are more likely to fail when the answer is near the boundaries of a segment, compared to the cases when an answer is in the center of a segment surrounded by richer context (Figure 1); second, we empirically observe that chunking with a smaller stride size contributes little to (sometimes even hurts) the model performance. A possible explanation is that predicting answer</p>
<p>for each segment independently may cause incomparable answer scores across segments. A similar phenomenon is also observed in open-domain question answering tasks (Clark and Gardner, 2017).</p>
<p>Considering the limitations mentioned above, we propose recurrent chunking mechanisms (RCM) on top of the transformer-based models for MRC tasks. There are two main characteristics of RCM. First, it could let the machine reader learn how to choose the stride size intelligently when reading a lengthy document via reinforcement learning, so it helps prevent extracting incomplete answers from a segment and retain sufficient contexts around the answer. Second, we apply recurrent mechanisms to allow the information to flow across segments. As a result, the model can have access to the global contextual information beyond the current segment.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The influence of the distance between the center of the answer span and the center of the segment. The test performance (in F1 score) is evaluated on CoQA using a BERT-Large reader. The best performance is achieved when the chunk center coincides with the answer span center. Within the distance of $\pm 80$ (in tokens), while $99 \%$ answers are completely covered, the performance degrades as the segment center moves away from the answer center, and the segment contains fewer relevant contexts. When the distance reaches 96 , more than half of the predicted spans are incomplete.</p>
<p>In the experiments, we evaluate the proposed RCM $^{1}$ on three MRC datasets: CoQA, QuAC, and TriviaQA. Experimental results demonstrate that RCM leads to consistent performance gains on these benchmarks. Furthermore, it also generates segments that are more likely to cover the entire answer spans and provide richer contextual information around the ground truth answers.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>The primary contributions of this work are:</p>
<ul>
<li>We propose a chunking mechanism for machine reading comprehension to let a model learn to chunk lengthy documents in a more flexible way via reinforcement learning.</li>
<li>We also apply recurrence to allow information transfer between segments so that the model can have knowledge beyond the current segment when selecting answers.</li>
<li>We have performed extensive experiments on three machine reading comprehension datasets: CoQA, QuAC, and TriviaQA. Our approach outperforms two state-of-the-art BERT-based models on different datasets.</li>
</ul>
<h2>2 Method</h2>
<p>The proposed recurrent chunking mechanisms (RCM) are built upon the pre-trained BERT models. We will briefly introduce the basic model in Section 2.1, and then the RCM approach in Section 2.2 and 2.3. More details of our model in training and testing are presented in Sections 2.4 and 2.5.</p>
<h3>2.1 Baseline Model</h3>
<p>Pre-trained BERT model has been shown to achieve new state-of-the-art performance on many MRC datasets (Devlin et al., 2019). Here, we introduce this basic BERT model, which is used as our baseline. As the maximum input length in BERT is restricted to be 512 , a widely adopted strategy is to chunk a long document into multiple segments with a fixed stride size (i.e., 128). Following the input format of BERT, the input for each document segment starts with "CLS" token, which is followed by question tokens "Q" and document segment tokens. We use "SEP" token as a separator between the question and the segment. We also append a special "UNK" token at the end of the segment to handle unanswerable questions. If a given question is annotated as unanswerable, we mark the "UNK" token as the ground truth answer during training. Accordingly in evaluation, if "UNK" token is selected by the model from the input segment, we output the answer as "unanswerable".
Answer Extraction. Following previous work on extractive machine reading comprehension, we predict the start and the end positions of the answer span in the given document segment. BERT first generates a vector representation $\mathbf{h}_{c, i}$ for each $i$-th</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: BERT generates representations for each input sequence, and recurrence accumulates information over segments. Based on these representations, the answer extractor extracts answers from the current segment, and the policy network takes chunking action and moves to the next segment. Chunking scorer scores each segment by estimating its likelihood of containing an answer and selects answers among predictions from multiple segments.</p>
<p>token in the c-th segment. Given <strong>h</strong>_{c,i}, the model scores each token in terms of its likelihood of being the start token of the answer span.</p>
<p>$$l_{c,i}^{\text{start}} = \mathbf{w}<em c_i="c,i">{s^T}\mathbf{h}</em>$$},\tag{1</p>
<p>where <strong>w</strong><em c_i="c,i">{s} is the model parameter. The probability <em>p</em></em> that the answer starts at the i-th token is computed by applying the softmax to }^{start<em>l</em>_{c,i}^{start}.</p>
<p>$$p_{c,i}^{\text{start}} = \text{softmax}(l_{c,i}^{\text{start}}).\tag{2}$$</p>
<p>Likewise, the model scores how likely the answer ends at the j-th token in segment c using</p>
<p>$$l_{c,j}^{\text{end}} = \mathbf{w}<em c_j="c,j">{c^T}\mathbf{h}</em>$$},\tag{3</p>
<p>where <strong>w</strong><em c_j="c,j">{e} is the model parameter. The probability of the j-th token being the end of the answer (denoted as <em>p</em></em>) is calculated in a similar manner as Eq. (2).}^{end</p>
<p><strong>Answer Ensemble</strong>. The baseline model adopts a max-pooling approach to ensemble candidate answers from multiple segments. The answer with the highest probability is selected.</p>
<h3>2.2 Recurrent Mechanisms</h3>
<p>The baseline model makes the answer prediction for each document segment independently, which may cause incomparable answer scores across segments due to the lack of document-level information. We propose to use a recurrent layer to propagate the information across different segments and a chunking scorer model to estimate the probability that a segment contains the answer.</p>
<p>For an input sequence containing the segment c, BERT's representation for its first token "CLS" is taken as the local representation v_{c} of the segment. The segment representation is further enriched with the representations of previously generated segments via recurrence. We denote the enriched segment representation as <strong>v̂</strong>_{c}:</p>
<p>$$\tilde{\mathbf{v}}<em c-1="c-1">c = f(\mathbf{v}_c, \tilde{\mathbf{v}}</em>$$}),\tag{4</p>
<p>where <em>f</em>(·) is the recurrent function. We consider two recurrent mechanisms here: gated recurrence and Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrence.</p>
<p>Gated recurrence is simply a weighted sum of its inputs:</p>
<p>$$f_{\text{gated}}(\mathbf{v}<em c-1="c-1">c, \tilde{\mathbf{v}}</em>}) = \alpha \mathbf{v<em c-1="c-1">c + \beta \tilde{\mathbf{v}}</em>$$},\tag{5</p>
<p>where α and β are coefficients depending on the inputs. We have α, β = softmax(w_{r}^{T}[v_{c}, \tilde{\mathbf{v}}<em r="r">{c-1}]), where <strong>w</strong></em> is a model parameter.</p>
<p>The LSTM recurrence, which uses LSTM unit as the recurrence function, takes <strong>v</strong><em c-1="c-1">{c} as the current input and <strong>v̂</strong></em> as the previous hidden state.</p>
<p>$$f_{\text{LSTM}}(\mathbf{v}<em c-1="c-1">c, \tilde{\mathbf{v}}</em>}) = \text{LSTM}(\mathbf{v<em c-1="c-1">c, \tilde{\mathbf{v}}</em>$$}).\tag{6</p>
<p><strong>Chunking Scorer</strong>. Given the enriched segment representation <strong>v̂</strong><em c="c">{c} as input, the chunking scorer produces an scalar q</em> by:</p>
<p>$$q_c = \sigma(\mathbf{W}_c \tilde{\mathbf{v}}_c + \mathbf{b}_c),\tag{7}$$</p>
<p>where <strong>W</strong><em c="c">{c} and <strong>b</strong></em> is an estimation of the probability that an answer is included in} are model parameters, and σ(·) is the sigmoid function. The scalar q_{c</p>
<p>segment $c$. Then, the chunking scorer uses $q_{c}$ to further refine the likelihood of the candidate answers from different segments (see Sections 2.4 and 2.5 for more details on this part of chunking scorer).</p>
<h3>2.3 Learning to Chunk</h3>
<p>The baseline approach divides a long document into multiple segments with a fixed stride size, from left to right. We will present an approach that could allow the model to choose the stride size flexibly by itself when reading the document. Our motivation, as mentioned in Section 1, is to prevent the answer span from being too close to the segment boundary and covering incomplete answers.</p>
<p>We formulate the problem of learning-to-chunk under the framework of reinforcement learning. We define the state $s$ of the model to be the segments that a model has processed up to the current segment $c$, i.e., $s={1,2, \ldots, c}$. The action $a$ is the stride size and direction (forward or backward) the model chooses to move to the next document segment. We define the action space $A$ as a set of strides, e.g., $A={-16,16,32}$, where 32 indicates moving forward with stride size 32 and -16 indicates moving backward with stride size 16 . In this work, we represent the state $s$ with the enriched segment representation $\tilde{\mathbf{v}}_{c}$.
Chunking Policy. The chunking policy gives the probability $p^{\text {act }}(a \mid s)$ of taking an action $a$ at the current state $s$, which is modeled by a one-layer feedforward neural network:</p>
<p>$$
p^{\text {act }}(a \mid s)=\operatorname{softmax}\left(\mathbf{W}<em c="c">{a} \tilde{\mathbf{v}}</em>\right)
$$}+\mathbf{b}_{a</p>
<p>where $\mathbf{W}<em a="a">{a}$ and $\mathbf{b}</em>$ are trainable parameters.
Fig. 2 gives an overview of the proposed recurrent chunking mechanisms built upon the BERT model: the chunking policy network takes the enriched segment representation as the input to generate the chunking action, which decides the next segment to be processed.</p>
<h3>2.4 Training</h3>
<p>In the training phase of the recurrent chunking mechanisms, the stride actions of moving to the next segment are sampled according to the probability given by the chunking policy (Sutton and Barto, 2018). Our model generates a sequence of document segments for each question. We train the answer extractor and chunking scorer network with supervised learning, and we train the chunking policy network via reinforcement learning.</p>
<p>Supervised Learning for Answer Extraction. Just as the baseline model, we train the answer extraction network via supervised learning. Given a question, the answer extractor classifies whether a word from a document segment is the start or the end of the answer. The cross-entropy loss can be computed given the ground-truth answer and the predictions of the answer extractor. Suppose that the $i^{<em>}$-th and $j^{</em>}$-th tokens are the answer start and end, respectively. The training objective to minimize the following cross-entropy loss, $L_{\text {ans }}$ :</p>
<p>$$
L_{\mathrm{ans}}=-\sum_{c} \log p_{c, i <em>}^{\text {start }}-\sum_{c} \log p_{c, j </em>}^{\mathrm{end}}
$$</p>
<p>Supervised Learning for Chunking Scorer. A binary variable $y_{c}$ indicates whether the segment $c$ contains an answer or not. Chunking scorer estimates the probability $q_{c}$ that the segment contains an answer. Similarly, the chunking scorer network can be trained in a supervised manner by minimizing the cross-entropy loss, $L_{\mathrm{cs}}$ :</p>
<p>$$
L_{\mathrm{cs}}=-\sum_{c} y_{c} \log q_{c}-\sum_{c}\left(1-y_{c}\right) \log \left(1-q_{c}\right)
$$</p>
<p>where the chunking score $q_{c}$ is given in Eq. (7).
Reinforcement Learning for Chunking Policy. Since the selection of the stride actions is a sequential decision-making process, it is natural to train the chunking policy via reinforcement learning.</p>
<p>First of all, the accumulated reward for taking action $a$ at state $s$ is denoted as $R(s, a)$, which is derived in a recursive manner:</p>
<p>$$
R(s, a)=q_{c} r_{c}+\left(1-q_{c}\right) R\left(s^{\prime}, a^{\prime}\right)
$$</p>
<p>where $q_{c}$ is the probability that segment $c$ contains an answer as given in Eq. (7), and $\left(s^{\prime}, a^{\prime}\right)$ denotes the next state-action pair. The value of $r_{c}$ indicates the probability of the correct answer being extracted from the current segment $c$. The mathematical definition of $r_{c}$ is given as:</p>
<p>$$
r_{c}= \begin{cases}p_{c, j^{<em>}}^{\text {start }} \cdot p_{c, j^{</em>}}^{\text {end }} &amp; \text { if answer included } \ 0, &amp; \text { else }\end{cases}
$$</p>
<p>The first term in Eq. (11) is the reward of the answer being correctly extracted from the current segment. The answer is included in the current segment $c$ with probability $q_{c}$, and thus the first term is weighted by $q_{c}$ in reward $R(s, a)$. The second term in Eq. (11) indicates that $R(s, a)$ also</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Train</th>
<th></th>
<th></th>
<th>Validation</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Question #</td>
<td>Avg tokens #</td>
<td>Max token #</td>
<td>Question #</td>
<td>Avg tokens #</td>
<td>Max token #</td>
</tr>
<tr>
<td>CoQA</td>
<td>108,647</td>
<td>352</td>
<td>1,323</td>
<td>7,983</td>
<td>341</td>
<td>1,037</td>
</tr>
<tr>
<td>QuAC</td>
<td>83,568</td>
<td>516</td>
<td>2,310</td>
<td>7,354</td>
<td>576</td>
<td>2,146</td>
</tr>
<tr>
<td>TriviaQA (wiki)</td>
<td>61,888</td>
<td>2,622</td>
<td>5,839</td>
<td>7,993</td>
<td>2,630</td>
<td>6,690</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of the CoQA, QuAC and TriviaQA datasets. We report the number of sub-tokens generated by the BERT tokenizer.
relies on the accumulated reward $R\left(s^{\prime}, a^{\prime}\right)$ of the next state when the answer is not available in the current segment.</p>
<p>The chunking policy network can be trained by maximizing the expected accumulated reward (as shown in Eq. (13)) through the policy gradient algorithm (Williams, 1992; Sutton et al., 2000; Gong et al., 2019).</p>
<p>$$
J=\mathbb{E}_{p^{\text {act }}(a \mid s)}[R(s, a)]
$$</p>
<p>To be consistent with the notations in answer extraction and chunking scorer modules, we denote the loss function of chunking policy as $L_{\mathrm{cp}}$, which is the negative expected accumulated reward $J$ in Eq. (13): $L_{\mathrm{cp}}=-J$. Thus, the stochastic gradient of $L_{\mathrm{cp}}$ over a mini-batch of data $\mathcal{B}$ is given by:</p>
<p>$$
\nabla L_{\mathrm{cp}}=-\sum_{(s, a) \in \mathcal{B}} \nabla \log p^{\mathrm{act}}(a \mid s) R(s, a)
$$</p>
<p>where $p^{\text {act }}(a \mid s)$ is the chunking policy in Eq. (8). Training procedure. The overall training loss $L$ is an sum of all three losses: $L=L_{\text {ans }}+L_{\text {ex }}+L_{\text {cp }}$. In addition, we initialize the bottom representation layers with a pre-trained BERT model and initialize other model parameters randomly. We use the Adam optimizer with peak learning rate $3 \times 10^{-5}$ and a linear warming-up schedule.</p>
<h3>2.5 Testing</h3>
<p>In the testing phase, the model starts from the beginning of the document as its first segment. Later on in state $s$, the model takes the best stride action $a^{*}$ according to the chunking policy:</p>
<p>$$
a^{*}=\underset{a \in A}{\operatorname{argmax}} p^{a c t}(a \mid s)
$$</p>
<p>After the stride action $a^{*}$ is taken, a new segment is taken from the given document, and so on untill the maximum number of segments $C$ is reached. Now for a document segment $c$, we score its candidate answer spanning from the $i$-th to the $j$-th token by $p_{i, j, c}^{\mathrm{A}}$ :</p>
<p>$$
p_{i, j, c}^{\mathrm{A}}=p_{c, i}^{\text {start }} \cdot p_{c, j}^{\text {end }} \cdot q_{c}
$$</p>
<p>The best answer span $(\bar{i}, \bar{j})$ across multiple segments can be obtained by selecting the one with the highest score $p_{i, j, c}^{\mathrm{A}}$.</p>
<p>$$
\bar{i}, \bar{j}=\underset{i \leq j, 1 \leq c \leq C}{\operatorname{argmax}} p_{i, j, c}^{\mathrm{A}}
$$</p>
<p>where dynamic programming is used to find $(\bar{i}, \bar{j})$ efficiently in linear time.</p>
<h2>3 Experiment</h2>
<h3>3.1 Datasets</h3>
<p>We use three MRC datasets, CoQA (Reddy et al., 2018), QuAC (Choi et al., 2018) and TriviaQA (Joshi et al., 2017)) in our experiments.
(1) CoQA. Answers in the CoQA dataset can be abstractive texts written by annotators. It is reported that an extractive MRC approach can achieve an upper bound as high as $97.8 \%$ in F1 score (Yatskar, 2019). Therefore, We preprocess the CoQA training data and select a text span from the document as the extractive answer that achieves the highest F1 score compared with the given ground truth.
(2) QuAC. All the answers in the QuAC dataset are text spans, which are highlighted by annotators in the given document.
(3) TriviaQA. TriviaQA is a large-scale MRC dataset, containing data from Wikipedia and Web domains. We use its Wikipedia subset in this work. It is reported to be challenging in its variability between questions and documents as well as its requirement of cross-sentence reasoning. Documents in TriviaQA contain more than 2,000 words on average, which is suitable for evaluating the capability of a model to deal with long documents.</p>
<p>The dataset statistics are summarized in Table 1, including the data sizes, the average and maximum number of sub-tokens in documents.</p>
<h3>3.2 Baselines and Evaluation Metric</h3>
<p>Baselines. We have two strong baselines based on the pre-trained BERT, which has achieved state-of-the-art performance in a wide range of NLP tasks</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>CoQA</th>
<th></th>
<th></th>
<th></th>
<th>QuAC</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Max sequence length</td>
<td>192</td>
<td>256</td>
<td>384</td>
<td>512</td>
<td>192</td>
<td>256</td>
<td>384</td>
<td>512</td>
</tr>
<tr>
<td>BERT-Large (Devlin et al., 2019)</td>
<td>72.8</td>
<td>76.2</td>
<td>81.0</td>
<td>81.4</td>
<td>34.5</td>
<td>50.6</td>
<td>56.7</td>
<td>61.5</td>
</tr>
<tr>
<td>Sent-Selector (with previous questions)</td>
<td>54.5</td>
<td>63.8</td>
<td>75.3</td>
<td>79.4</td>
<td>33.9</td>
<td>38.8</td>
<td>47.6</td>
<td>55.4</td>
</tr>
<tr>
<td>Sent-Selector (only current questions)</td>
<td>57.5</td>
<td>66.5</td>
<td>76.5</td>
<td>79.5</td>
<td>34.3</td>
<td>39.1</td>
<td>47.6</td>
<td>56.4</td>
</tr>
<tr>
<td>BERT-RCM</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Gated recurrence (no RL chunking)</td>
<td>74.5</td>
<td>78.6</td>
<td>81.0</td>
<td>81.4</td>
<td>48.8</td>
<td>51.4</td>
<td>56.2</td>
<td>61.4</td>
</tr>
<tr>
<td>- Gated recurrence</td>
<td>$\mathbf{7 6 . 0}$</td>
<td>79.2</td>
<td>$\mathbf{8 1 . 3}$</td>
<td>$\mathbf{8 1 . 8}$</td>
<td>51.6</td>
<td>55.2</td>
<td>59.9</td>
<td>$\mathbf{6 2 . 0}$</td>
</tr>
<tr>
<td>- LSTM recurrence (no RL chunking)</td>
<td>74.1</td>
<td>78.5</td>
<td>81.0</td>
<td>81.3</td>
<td>49.2</td>
<td>51.5</td>
<td>56.4</td>
<td>61.6</td>
</tr>
<tr>
<td>- LSTM recurrence</td>
<td>75.4</td>
<td>$\mathbf{7 9 . 5}$</td>
<td>$\mathbf{8 1 . 3}$</td>
<td>$\mathbf{8 1 . 8}$</td>
<td>$\mathbf{5 3 . 9}$</td>
<td>$\mathbf{5 5 . 6}$</td>
<td>$\mathbf{6 0 . 4}$</td>
<td>61.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of F1 scores (\%) achieved by different algorithms.
including machine reading comprehension.
(1) BERT-LARGE MODEL. It achieves competitive performance on extractive MRC tasks such as SQuAD (Rajpurkar et al., 2016, 2018). It adopts a simple sliding window chunking policy - moving to the next document segment with a fixed stride size from left to right. We also analyze the performance of the Large BERT model with different stride sizes in training and testing (see Section 4.1 for details). The best performance is obtained by setting stride size as 64 in CoQA and QuAC, and 128 in TriviaQA.
(2) SENTENCE SELECTOR. Given a question, the sentence selector chooses a subset of sentences that are likely to contain an answer. The selected sentences are then concatenated and fed to the BERTLarge model for answer extraction. For conversational datasets CoQA and QuAC, since a question is correlated with its previous questions within the same conversation, we apply the sentence selector to select sentences based on the current question alone or the concatenation of the previous questions and the current question. We only use the current question as the input to the sentence selector for TriviaQA, which does not involve any conversational history. The sentence selector we used in experiments is released by Htut et al. (2018).</p>
<p>Evaluation Metric. The main evaluation metric is macro-average word-level F1 score. We compare each prediction with the reference answer. Precision is defined by the percentage of predicted answer tokens that appear in the reference answer, and recall is the percentage of reference answer tokens captured in the prediction. F1 score is the harmonic mean of the precision and recall. When multiple reference answers are provided, the maximum F1 score is used for evaluation.</p>
<h3>3.3 Results on CoQA and QuAC</h3>
<p>We first perform experiments on two conversational MRC datasets, CoQA and QuAC.
Setting. We perform a set of experiments with different maximum sequence lengths of 192, 256, 384 , and 512 . Our model fixes the number of segments read from a document for each question. It generates $4,3,3$, and 2 segments under the length limit of $192,256,384$, and 512 , respectively.</p>
<p>Considering that questions are highly correlated due to the existence of coreferential mentions across questions, we concatenate each question with as many of its previous questions as possible up to the length limit of 64 question tokens. The action space of the model strides is set as $[-16,16,32,64,128]$ for CoQA and $[-16,32,64,128,256]$ for QuAC considering that documents in CoQA documents are shorter than those in QuAC. The first segment always starts with the first token of the document, and the model will take stride action after the first segment.
Results. In Table 2, we present F1 scores achieved by our methods and the baselines. The performance of the BERT-Large model drops drastically as the maximum sequence length decreases. We see a drop of $8.6 \%$ in F1 score on the CoQA dataset and a drop of $27.0 \%$ on the QuAC dataset when the maximum input length decreases from 512 to 192.</p>
<p>Followed by the same BERT-Large reader, the sentence selector baseline that only considers the current question achieves better performance than the selector fed with the concatenation of the current question and its previous questions. The selector with the current question performs well in selecting sentences containing answers from documents. For $90.4 \%$ of questions in CoQA and $81.2 \%$ of questions in QuAC, the top-ranked 12 sentences in the documents can include complete</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">CoQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QuAC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># of Doc Tokens</td>
<td style="text-align: center;">$&lt;=200$</td>
<td style="text-align: center;">$(200,300]$</td>
<td style="text-align: center;">$(300,400]$</td>
<td style="text-align: center;">$&gt;400$</td>
<td style="text-align: center;">$&lt;=300$</td>
<td style="text-align: center;">$(300,450]$</td>
<td style="text-align: center;">$(450,600]$</td>
<td style="text-align: center;">$&gt;600$</td>
</tr>
<tr>
<td style="text-align: left;">Percentage (\%)</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">18.9</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: left;">BERT-Large</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: left;">BERT-RCM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Gated recurrence</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">$\mathbf{4 3 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">- LSTM recurrence</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">$\mathbf{7 4 . 7}$</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">41.3</td>
</tr>
</tbody>
</table>
<p>Table 3: F1 Score (\%) on documents with different numbers of tokens (max sequence length is 512).
answers. However, the selector does not improve upon BERT-Large despite its high precision in sentence selection. This might be because selected sentences do not provide sufficient contexts for a model to identify answers accurately.</p>
<p>Our model with recurrent chunking mechanisms BERT-RCM performs consistently better than both BERT-Large and BERT-Sent-Selector. On the CoQA dataset, BERT-RCM with gated recurrence improves upon the BERT-Large model by $3.2 \%$, $3 \%, 0.3 \%$, and $0.4 \%$ with maximum sequence length of $192,256,284$, and 512 , respectively. The improvement brought by LSTM recurrence and RL chunking is $2.6 \%, 3.3 \%, 0.3 \%, 0.4 \%$ on CoQA. As for the QuAC dataset, gated recurrence combined with RL chunking leads to improvements of $17.1 \%$, $4.6 \%, 3.2 \%, 0.5 \%$, and LSTM recurrence has gains of $19.4 \%, 5.0 \%, 3.7 \%, 0.3 \%$ under different maximum sequence lengths. On the two datasets, the gains of BERT-RCM over BERT-Large are statistically significant at $p=0.05$ with both gated and LSTM recurrence. We notice that our model is less sensitive to the maximum sequence length, and LSTM recurrence has comparable performance to the gated recurrence.</p>
<p>The gain is more obvious with maximum sequence length $(192,256,384)$, and relatively small under the length of 512 . This is perhaps because most document lengths are smaller than 512 in CoQA and QuAC. Therefore, we report the performance of our proposed method on documents of different lengths in Table 3, where the maximum sequence length is set as 512 . We observe that the gain is more obvious on longer documents. For documents with more than 400 words in the CoQA dataset, RL chunking with gated recurrence has an improvement of $7.3 \%$ over BERT-Large, and RL chunking with LSTM recurrence improves F1 score by $7.5 \%$. As for QuAC, the improvement of gated recurrence with RL chunking is $4.5 \%$, and the improvement of LSTM recurrence is $2.6 \%$.</p>
<p>Ablation Analysis. We further study the effect of recurrence alone without RL chunking here. As shown in rows BERT-Large and Gated recurrence (no RL chunking) in Table 2, gated recurrence alone can improve F1 score by $2.4 \%$, and LSTM recurrence leads to an improvement of $2.3 \%$ without RL chunking when the maximum sequence length is 256. However, we do not observe any improvement when the maximum sequence length is set to 384 or 512 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Algorithms</th>
<th style="text-align: left;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-Large</td>
<td style="text-align: left;">61.3</td>
</tr>
<tr>
<td style="text-align: left;">Sent-Selector</td>
<td style="text-align: left;">59.8</td>
</tr>
<tr>
<td style="text-align: left;">BERT-RCM</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">- Gated recurrence</td>
<td style="text-align: left;">$\mathbf{6 2 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">- LSTM recurrence</td>
<td style="text-align: left;">62.3</td>
</tr>
</tbody>
</table>
<p>Table 4: F1 score (\%) of different algorithms on the TriviaQA dataset.</p>
<h3>3.4 Results on TriviaQA</h3>
<p>We further evaluate the ability of our model in dealing with extremely long documents on the TriviaQA Wikipedia dataset.
Setting. We set the maximum sequence length as 512 for all models. The action space of our BERTRCM model is set to $[-64,128,256,512,1024]$. The stride sizes are larger than those in CoQA and QuAC, since TriviaQA provides much longer documents. During training, the maximum number of segments our model can extract from a document is set to three in the TriviaQA dataset. Note that our model reads no more than $512 \cdot 3=1536$ tokens from these three segments, which are much fewer than the average document length.
Results. We filter a small number of questions whose answers cannot be extracted from documents and keep 7,251 questions from a total of 7,993 questions. In Table 4, we present the F1 scores of different algorithms. Compared with</p>
<p>| Dataset | CoQA | | | | QuAC | | | |
| BERT-Large (Devlin et al., 2019) | Prediction Stride Size | | | | Prediction Stride Size | | | |
| Training Stride Size | 16 | 32 | 64 | 128 | 16 | 32 | 64 | 128 |
| 16 | 80.8 | 80.9 | 80.8 | 80.7 | 60.6 | 60.7 | 60.7 | 60.8 |
| 32 | 81.1 | 81.1 | 81.1 | 81.1 | 60.7 | 60.7 | 60.9 | 61.0 |
| 64 | $\mathbf{8 1 . 4}$ | $\mathbf{8 1 . 4}$ | $\mathbf{8 1 . 4}$ | 81.3 | 61.0 | 61.0 | $\mathbf{6 1 . 4}$ | $\mathbf{6 1 . 4}$ |
| 128 | 81.0 | 81.1 | 81.1 | 81.1 | 60.8 | 60.8 | 60.8 | 61.2 |</p>
<p>Table 5: F1 score (\%) of the BERT-Large model with different training/prediction stride sizes on the CoQA and QuAC datasets.</p>
<p>BERT-Large, the BERT-RCM model achieves $1.6 \%$ gain with gated recurrence and $1 \%$ gain with LSTM recurrence. Also, both BERT-RCM and BERT-Large models beat the Sent-Selector model.</p>
<h2>4 Discussion</h2>
<p>In this section, we will analyze the performance of the baseline BERT-Large model and our proposed recurrent chunking mechanisms.</p>
<h3>4.1 Analysis of different Stride Sizes in BERT-Large</h3>
<p>In Table 5, we give an analysis of how the performance varies with different stride sizes in BERTLarge model (the baseline) training and prediction. An interesting observation is that smaller stride size in prediction does not always improve the performance, sometimes even hurts as can be seen on the QuAC dataset. It suggests that BERT-Large performs badly on selecting good answers from multiple chunks. Smaller stride size in model training also leads to worse performance. A possible explanation is that smaller stride size would cause the significant distortion of training data distribution, since the longer question-document pairs produces more training samples than short ones.</p>
<h3>4.2 Discussions of Recurrent Chunking Mechanisms</h3>
<p>We now provide an insight into the recurrent mechanisms and chunking policy learned by our proposed model using quantitative analysis. For the clarity of our discussions, we use the following setting on the CoQA and QuAC datasets: the maximum chunk length is set to 256 , and the stride size of BERT-Large model is 128.</p>
<p>Segment-Hit Rate. With the ability of chunking policy, BERT-RCM is expected to focus on those document segments that contain an answer.</p>
<p>To evaluate how well a model can capture good segments, we use hit rate, i.e., the percentage of segments that contain a complete answer among all extracted segments, as evaluation metric.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hit rate</th>
<th style="text-align: center;">CoQA</th>
<th style="text-align: center;">QuAC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT-Large</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">34.1</td>
</tr>
<tr>
<td style="text-align: left;">BERT-RCM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">- Gated recurrence</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">44.9</td>
</tr>
<tr>
<td style="text-align: left;">- LSTM recurrence</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">42.8</td>
</tr>
</tbody>
</table>
<p>Table 6: Comparison of BERT-Large and BERT-RCM on segment-hit rate (\%).</p>
<p>As shown in Table 6, BERT-RCM significantly outperforms BERT-Large, which indicates that the learned chunking policy is more focused on informative segments.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The answer-segment center distance.</p>
<p>Answer-Chunk Center Distance. As discussed in Fig. 1, the answer's position with respect to a document segment is important for answer prediction. When an answer is centered within the document segment, sufficient contexts on both sides help a model make better predictions. In</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Example of generated document segments by BERT-RCM from a CoQA document.</p>
<p>Fig. 3, it presents the averaged center distances of the first three segments generated by BERT-Large and BERT-RCMs on the CoQA validation dataset. Since all models start from the beginning of a document in the first segment, their first answer-chunk center distances are the same: 96 tokens. But for the second and third segments generated by BERTRCMs, the answer-chunk center distances are much smaller than BERT-Large.</p>
<p>In this section, we also illustrate the working flow of BERT-RCM with a case study.</p>
<p>Case Study. We show an example from a CoQA document in Figure 4 to illustrate the chunking mechanism of our BERT-RCM model with LSTM recurrence. The model starts with the beginning of the document as the first segment, where the answer span is close to its right boundary. The model moves forwards 128 tokens to include more right contexts and generates the second chunk. The stride size is a bit large since the answer is close to the left boundary of the second segment. The model then moves back to the left by 16 tokens and obtains its third segment. The chunking scorer assigns the three segments with the scores $0.24,0.87$, and 0.90 , respectively. It suggests that the model considers the third segment as the most informative chunk in answer selection.</p>
<h2>5 Related Work</h2>
<p>There is a growing interest in MRC tasks that require the understanding of both questions and reference documents (Trischler et al., 2017; Rajpurkar et al., 2018; Saeidi et al., 2018; Choi et al., 2018; Reddy et al., 2018; Xu et al., 2019). Recent studies on pre-trained language models (Radford et al., 2018; Devlin et al., 2019; Baker et al., 2019; Yang
et al., 2019b) have demonstrated their great success in fine-tuning on MRC tasks. However these pre-trained NLP models (e.g., BERT) only take as input a fixed-length text. Variants of BERT are proposed to process long documents in tasks such as text classification (Chalkidis et al., 2019). To deal with lengthy documents in machine reading comprehension tasks, some previous studies skip certain tokens (Yu et al., 2017; Seo et al., 2018) or select a set of sentences as input based on the given questions (Hewlett et al., 2017; Min et al., 2018; Lin et al., 2018). However, they mainly focus on tasks in which most of the answers to given questions are formed by a single informative sentence. These previous approaches are less applicable to deal with those complicated questions that demand cross-sentences reasoning or have much lexical variability from their lengthy documents.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we propose a chunking policy network for machine reading comprehension, which enables a model learn to chunk lengthy documents in a more flexible way via reinforcement learning. We also add a recurrent mechanism to allow the information to flow across segments so that the model could have knowledge beyond the current segment when selecting answers. We have performed extensive experiments on three public datasets of machine reading comprehension: CoQA, QuAC, and TriviaQA. Our approach outperforms benchmark models across different datasets.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank the anonymous reviewers for their constructive comments and suggestions.</p>
<h2>References</h2>
<p>Henrietta Baker, Matthew R. Hallowell, and Antoine J.P. Tixier. 2019. Automatically learning construction injury precursors from text. arXiv preprint, cs.CL/1907.11769v3.</p>
<p>Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in english. In Proceedings of the ACL, pages 43174323, Florence, Italy.</p>
<p>Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question answering in context. In Proceedings of the EMNLP, pages 21742184, Brussels, Belgium.</p>
<p>Christopher Clark and Matt Gardner. 2017. Simple and effective multi-paragraph reading comprehension. arXiv preprint, cs.CL/1710.10723v2.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proeedings of the NAACL-HLT, pages 4171-4186, Minneapolis, MN.</p>
<p>Hongyu Gong, Suma Bhat, Lingfei Wu, JinJun Xiong, and Wen-mei Hwu. 2019. Reinforcement learning based text style transfer without parallel training corpus. In Proceedings of the NAACL-HLT, pages 3168-3180, Minneapolis, MN.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of the NeurIPS, pages 1693-1701, Montreal, Canada.</p>
<p>Daniel Hewlett, Llion Jones, Alexandre Lacoste, et al. 2017. Accurate supervised and semi-supervised machine reading for long documents. In Proceedings of the EMNLP, pages 2011-2020, Copenhagen, Denmark.</p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2016. The goldilocks principle: Reading children's books with explicit memory representations. In Proceedings of the ICLR, Caribe Hilton, Puerto Rico.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Phu Mon Htut, Samuel R Bowman, and Kyunghyun Cho. 2018. Training a ranking function for opendomain question answering. In Proceedings of the NAACL-HLT, pages 120-127, New Orleans, LA.</p>
<p>Minghao Hu, Yuxing Peng, Zhen Huang, Nan Yang, Ming Zhou, et al. 2019. Read+ verify: Machine reading comprehension with unanswerable questions. In Proceedings of the AAAI, pages 6529-6537, Honolulu, HI.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the ACL, pages 16011611, Vancouver, Canada.</p>
<p>Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Unifying question answering and text classification via span extraction. arXiv preprint, cs.CL/1904.09286v2.</p>
<p>Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gáabor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. TACL, 6:317328.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the ACL, pages 1736-1745, Melbourne, Australia.</p>
<p>Sewon Min, Victor Zhong, Richard Socher, and Caiming Xiong. 2018. Efficient and robust question answering from minimal context over documents. In Proceedings of the ACL, pages 1725-1735, Melbourne, Australia.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. In Preprint.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In Proceedings of the ACL, pages 784-789, Melbourne, Australia.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the EMNLP, pages 2383-2392, Austin, TX.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. CoQA: A conversational question answering challenge. TACL, 7:249-266.</p>
<p>Marzieh Saeidi, Max Bartolo, Patrick Lewis, Sameer Singh, Tim Rocktäschel, Mike Sheldon, Guillaume Bouchard, and Sebastian Riedel. 2018. Interpretation of natural language rules in conversational machine reading. In Proceedings of the EMNLP, pages 2087-2097, Brussels, Belgium.</p>
<p>Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi. 2018. Neural speed reading via SkimRNN. In Proceedings of the ICLR, New Orleans, LA.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction.</p>
<p>Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the NeurIPS, pages 1057-1063, Denver, CO.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A machine comprehension dataset. In Proceedings of the RepL4NLP, pages 191-200, Vancouver, Canada.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. Review conversational reading comprehension. arXiv preprint, cs.CL/1902.00821v2.</p>
<p>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a. End-to-end open-domain question answering with bertserini. In Proceedings of the NAACL-HLT, pages 72-77, Minneapolis, MN.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019b. XLNet: Generalized autoregressive pretraining for language understanding. In Proceedings of the NeurIPS, pages 5754-5764, Vancouver, Canada.</p>
<p>Mark Yatskar. 2019. A qualitative comparison of CoQA, SQuAD 2.0 and QuAC. In Proceedings of the NAACL-HLT, pages 2318-2323, Minneapolis, MN.</p>
<p>Adams Wei Yu, Hongrae Lee, and Quoc Le. 2017. Learning to skim text. In Proceedings of the ACL, pages 1880-1890, Vancouver, Canada.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension. arXiv preprint, cs.CL/1810.12885v1.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The code is available at https://github.com/ HongyuGong/RCM-Question-Answering.git.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
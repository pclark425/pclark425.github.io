<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9001 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9001</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9001</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-265610018</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.01276v2.pdf" target="_blank">Toward best research practices in AI Psychology</a></p>
                <p><strong>Paper Abstract:</strong> Language models have become an essential part of the burgeoning field of AI Psychology. I discuss 14 methodological considerations that can help design more robust, generalizable studies evaluating the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9001.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9001.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WSC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Winograd Schema Challenge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark of minimally differing sentence pairs that require commonsense/world knowledge to resolve ambiguous pronouns; designed to probe commonsense reasoning rather than surface statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Winograd Schema Challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language models (unspecified; state-of-the-art models by 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic LLMs trained on large web/text corpora (architectures and training runs not specified in this paper); reported to have achieved high accuracy on the WSC by 2020.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Winograd Schema Challenge (WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Pairs of minimally different sentences with ambiguous pronouns whose correct referent must be determined using commonsense/world knowledge (assesses commonsense reasoning and world knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>over 90% accuracy (reported as of 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs achieved very high accuracy on the original WSC (reported >90%), suggesting parity with or superiority to many baselines on that dataset, but later work showed this was likely inflated by heuristics and dataset issues.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Original WSC is small (<300 hand-crafted items) and publicly available; items are vulnerable to memorization/contamination from training corpora; common shortcut heuristics include selectional restrictions and frequency/co-occurrence cues; authors recommend 'Google-proof' items and control conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High reported accuracy was later found to reflect shortcuts (selectional restrictions, frequency associations), potential training-data contamination/memorization, and small public dataset size — thus success on WSC does not reliably indicate deep commonsense understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9001.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9001.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale, crowd-sourced augmentation of the Winograd Schema Challenge that used automatic filtering to reduce items solvable by simple co-occurrence/association heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An adversarial winograd schema challenge at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language models (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic LLMs evaluated on WinoGrande (specific architectures/sizes not reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>WinoGrande</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Thousands of crowd-sourced Winograd-style examples filtered (via language-model-embedding-based metrics) to reduce co-occurrence associations; assesses commonsense pronoun resolution at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>substantially reduced relative to original WSC after filtering (no numeric accuracy reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Model performance dropped on WinoGrande, indicating that earlier high WSC accuracy was partly due to association-based shortcuts exploited by models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Items were produced by crowdworkers and then automatically filtered to reduce co-occurrence-based associations as estimated through LM embeddings; designed to be less solvable by simple lexical/statistical cues.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Scaling introduced quality-control problems (typos, grammatical errors, unwarranted assumptions) and automatic filtering may still be imperfect; no specific human baselines or exact numeric results are provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9001.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9001.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elazar baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline analyses from Elazar et al. (WSC artifact detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Control-baseline sentence variants designed to detect frequency/association artifacts in Winograd-style items; revealed that LLMs sometimes show above-chance preferences even on reduced stimuli, indicating association bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language models (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Unspecified LLMs evaluated on WSC baseline controls; showed above-chance behavior on reduced-context sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>WSC baseline controls</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Two reduced-sentence baselines: (a) sentences with candidate referents excluded, and (b) sentences with the first half excluded; intended to reveal raw frequency/preference biases rather than reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>above-chance performance on baseline conditions (no numeric values reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs performing above chance on these controls indicates reliance on statistical associations rather than genuine commonsense inference; thus apparent WSC success can be artifactual.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Baselines consisted of removing candidate referents or removing the first half of the sentence; the expectation is no consistent referent preference in these reduced forms unless association biases exist.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Shows previously undetected association biases in LLMs; highlights need for control conditions and that passing original WSC items may not reflect true reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9001.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9001.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM cognitive tests (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models evaluated on diverse cognitive psychology tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that many contemporary studies evaluate LLMs on a broad set of cognitive assessments (working memory, logical reasoning, planning, social reasoning, creativity, personality), often via accessible chat-based interfaces like ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT and other chat-based LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based large language models and LLM-containing systems accessed via public interfaces; specific architectures, training data, and sizes are not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Various (working memory capacity, logical reasoning, planning, social reasoning, creativity, personality assessments, vision-language tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A range of cognitive assessments adapted from human psychology to probe corresponding cognitive domains (e.g., working memory, reasoning, planning, social cognition, creativity).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>The paper emphasizes recreating human testing conditions for models, checking for shortcuts, controlling for training-data contamination, and examining cultural/prompting effects; it warns against using well-known items or trusting automatic item scoring without human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No specific numeric LLM vs human comparisons are provided in this paper for these domains; the paper highlights pervasive methodological caveats (shortcut heuristics, data contamination, prompt/cultural effects, evaluator-model circularity) that complicate interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Toward best research practices in AI Psychology', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The Winograd Schema Challenge. <em>(Rating: 2)</em></li>
                <li>An adversarial winograd schema challenge at scale. <em>(Rating: 2)</em></li>
                <li>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. <em>(Rating: 2)</em></li>
                <li>The defeat of the winograd schema challenge. <em>(Rating: 2)</em></li>
                <li>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. <em>(Rating: 1)</em></li>
                <li>Baby steps in evaluating the capacities of large language models. <em>(Rating: 1)</em></li>
                <li>How do we know how smart AI systems are? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9001",
    "paper_id": "paper-265610018",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "WSC",
            "name_full": "Winograd Schema Challenge",
            "brief_description": "A benchmark of minimally differing sentence pairs that require commonsense/world knowledge to resolve ambiguous pronouns; designed to probe commonsense reasoning rather than surface statistics.",
            "citation_title": "The Winograd Schema Challenge.",
            "mention_or_use": "mention",
            "model_name": "large language models (unspecified; state-of-the-art models by 2020)",
            "model_description": "Generic LLMs trained on large web/text corpora (architectures and training runs not specified in this paper); reported to have achieved high accuracy on the WSC by 2020.",
            "model_size": null,
            "test_battery_name": "Winograd Schema Challenge (WSC)",
            "test_description": "Pairs of minimally different sentences with ambiguous pronouns whose correct referent must be determined using commonsense/world knowledge (assesses commonsense reasoning and world knowledge).",
            "llm_performance": "over 90% accuracy (reported as of 2020)",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs achieved very high accuracy on the original WSC (reported &gt;90%), suggesting parity with or superiority to many baselines on that dataset, but later work showed this was likely inflated by heuristics and dataset issues.",
            "experimental_details": "Original WSC is small (&lt;300 hand-crafted items) and publicly available; items are vulnerable to memorization/contamination from training corpora; common shortcut heuristics include selectional restrictions and frequency/co-occurrence cues; authors recommend 'Google-proof' items and control conditions.",
            "limitations_or_caveats": "High reported accuracy was later found to reflect shortcuts (selectional restrictions, frequency associations), potential training-data contamination/memorization, and small public dataset size — thus success on WSC does not reliably indicate deep commonsense understanding.",
            "uuid": "e9001.0",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "WinoGrande",
            "name_full": "WinoGrande",
            "brief_description": "A large-scale, crowd-sourced augmentation of the Winograd Schema Challenge that used automatic filtering to reduce items solvable by simple co-occurrence/association heuristics.",
            "citation_title": "An adversarial winograd schema challenge at scale.",
            "mention_or_use": "mention",
            "model_name": "large language models (unspecified)",
            "model_description": "Generic LLMs evaluated on WinoGrande (specific architectures/sizes not reported in this paper).",
            "model_size": null,
            "test_battery_name": "WinoGrande",
            "test_description": "Thousands of crowd-sourced Winograd-style examples filtered (via language-model-embedding-based metrics) to reduce co-occurrence associations; assesses commonsense pronoun resolution at scale.",
            "llm_performance": "substantially reduced relative to original WSC after filtering (no numeric accuracy reported in this paper)",
            "human_baseline_performance": null,
            "performance_comparison": "Model performance dropped on WinoGrande, indicating that earlier high WSC accuracy was partly due to association-based shortcuts exploited by models.",
            "experimental_details": "Items were produced by crowdworkers and then automatically filtered to reduce co-occurrence-based associations as estimated through LM embeddings; designed to be less solvable by simple lexical/statistical cues.",
            "limitations_or_caveats": "Scaling introduced quality-control problems (typos, grammatical errors, unwarranted assumptions) and automatic filtering may still be imperfect; no specific human baselines or exact numeric results are provided in this paper.",
            "uuid": "e9001.1",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Elazar baselines",
            "name_full": "Baseline analyses from Elazar et al. (WSC artifact detection)",
            "brief_description": "Control-baseline sentence variants designed to detect frequency/association artifacts in Winograd-style items; revealed that LLMs sometimes show above-chance preferences even on reduced stimuli, indicating association bias.",
            "citation_title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.",
            "mention_or_use": "mention",
            "model_name": "large language models (unspecified)",
            "model_description": "Unspecified LLMs evaluated on WSC baseline controls; showed above-chance behavior on reduced-context sentences.",
            "model_size": null,
            "test_battery_name": "WSC baseline controls",
            "test_description": "Two reduced-sentence baselines: (a) sentences with candidate referents excluded, and (b) sentences with the first half excluded; intended to reveal raw frequency/preference biases rather than reasoning.",
            "llm_performance": "above-chance performance on baseline conditions (no numeric values reported in this paper)",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs performing above chance on these controls indicates reliance on statistical associations rather than genuine commonsense inference; thus apparent WSC success can be artifactual.",
            "experimental_details": "Baselines consisted of removing candidate referents or removing the first half of the sentence; the expectation is no consistent referent preference in these reduced forms unless association biases exist.",
            "limitations_or_caveats": "Shows previously undetected association biases in LLMs; highlights need for control conditions and that passing original WSC items may not reflect true reasoning.",
            "uuid": "e9001.2",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "LLM cognitive tests (general)",
            "name_full": "Large language models evaluated on diverse cognitive psychology tests",
            "brief_description": "The paper notes that many contemporary studies evaluate LLMs on a broad set of cognitive assessments (working memory, logical reasoning, planning, social reasoning, creativity, personality), often via accessible chat-based interfaces like ChatGPT.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT and other chat-based LLMs (unspecified)",
            "model_description": "Chat-based large language models and LLM-containing systems accessed via public interfaces; specific architectures, training data, and sizes are not detailed in this paper.",
            "model_size": null,
            "test_battery_name": "Various (working memory capacity, logical reasoning, planning, social reasoning, creativity, personality assessments, vision-language tasks)",
            "test_description": "A range of cognitive assessments adapted from human psychology to probe corresponding cognitive domains (e.g., working memory, reasoning, planning, social cognition, creativity).",
            "llm_performance": null,
            "human_baseline_performance": null,
            "performance_comparison": null,
            "experimental_details": "The paper emphasizes recreating human testing conditions for models, checking for shortcuts, controlling for training-data contamination, and examining cultural/prompting effects; it warns against using well-known items or trusting automatic item scoring without human checks.",
            "limitations_or_caveats": "No specific numeric LLM vs human comparisons are provided in this paper for these domains; the paper highlights pervasive methodological caveats (shortcut heuristics, data contamination, prompt/cultural effects, evaluator-model circularity) that complicate interpretation.",
            "uuid": "e9001.3",
            "source_info": {
                "paper_title": "Toward best research practices in AI Psychology",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The Winograd Schema Challenge.",
            "rating": 2,
            "sanitized_title": "the_winograd_schema_challenge"
        },
        {
            "paper_title": "An adversarial winograd schema challenge at scale.",
            "rating": 2,
            "sanitized_title": "an_adversarial_winograd_schema_challenge_at_scale"
        },
        {
            "paper_title": "Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema.",
            "rating": 2,
            "sanitized_title": "back_to_square_one_artifact_detection_training_and_commonsense_disentanglement_in_the_winograd_schema"
        },
        {
            "paper_title": "The defeat of the winograd schema challenge.",
            "rating": 2,
            "sanitized_title": "the_defeat_of_the_winograd_schema_challenge"
        },
        {
            "paper_title": "Can language models handle recursively nested grammatical structures? a case study on comparing models and humans.",
            "rating": 1,
            "sanitized_title": "can_language_models_handle_recursively_nested_grammatical_structures_a_case_study_on_comparing_models_and_humans"
        },
        {
            "paper_title": "Baby steps in evaluating the capacities of large language models.",
            "rating": 1,
            "sanitized_title": "baby_steps_in_evaluating_the_capacities_of_large_language_models"
        },
        {
            "paper_title": "How do we know how smart AI systems are?",
            "rating": 1,
            "sanitized_title": "how_do_we_know_how_smart_ai_systems_are"
        }
    ],
    "cost": 0.010633,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOWARD BEST RESEARCH PRACTICES IN AI PSYCHOLOGY A PREPRINT
October 30, 2024</p>
<p>Anna Ivanova a.ivanova@gatech.edu 
Georgia Institute of Technology</p>
<p>TOWARD BEST RESEARCH PRACTICES IN AI PSYCHOLOGY A PREPRINT
October 30, 2024F4B28D4374D0E031FF12D2E28C3E2246arXiv:2312.01276v2[cs.AI]
Language models have become an essential part of the burgeoning field of AI Psychology.I discuss 14 methodological considerations that can help design more robust, generalizable studies evaluating the cognitive abilities of language-based AI systems, as well as to accurately interpret the results of these studies.</p>
<p>Cognitive evaluations of AI models</p>
<p>Ever since the Turing test, the idea of having a dialogue with a machine to probe its cognitive abilities ("thought") has been inextricably associated with the field of artificial intelligence (AI).In addition to its intuitive simplicity, this idea naturally aligns with everyday practice in psychology: language-based assessments are the bread and butter of many psychologists' toolkits.If researchers want to know what is happening in the mind of a human, the easiest approach is to ask.Today, advances in linguistic abilities of large language models (LLMs)-and AI systems that might incorporate LLMs as one of their components-make it possible to seamlessly test these models on languagebased assessments originally designed for people.This is an unprecedented advance: to date, the only entities who could flexibly use human language were, well, human.Now, however, we are faced with artificial systems that can process linguistic information, generate novel texts, and respond to questions.How do we assess the cognitive capabilities of these systems?Easy access to chat-based LLM interfaces (such as ChatGPT) makes it possible for anyone to run a "cognitive test" on an AI system.This advance has led to an explosive growth of AI psychology, with papers assessing LLMs' working memory capacity, logical reasoning, planning abilities, social reasoning, creativity, and even personality traits.Advances in vision-language models now also make it possible to test AI systems on cognitive assessments that incorporate pictures and videos.</p>
<p>Although running such assessments can be fairly straightforward, interpreting the results is not.In fact, AI Psychology today is faced with a plethora of methodological questions.What factors should we consider when assessing model performance?How can we adapt our stimuli to reduce the prevalence of "hacks", i.e., heuristics that the model might employ to achieve high task performance without using the cognitive skill being assessed?If a model passes a human test for a cognitive ability X, does it mean that it indeed possesses X?</p>
<p>To begin answering these questions, I provide a list of 14 do's and don'ts to consider when designing, conducting, and interpreting the results of an AI Psychology study (see also 1 and 2</p>
<p>Experimental Design</p>
<p>When designing or adapting a cognitive test for evaluating an AI model: for LLMs include word associations based on their statistical co-occurrence ("racecar"/"fast"), although there might be many others, such as grammatical cues or more abstract structural patterns in text.Humans, too, routinely use task strategies not initially considered by the experimenters, but the kinds of alternative strategies available to humans and to AI systems are not necessarily the same.Considering possible shortcut paths to solutions can help both design shortcut-proof items and identify edge cases where shortcut-based models are likely to fail 3 .</p>
<p>DO incorporate careful control conditions.</p>
<p>Once you identify alternative strategies by which a model might solve the task, the next step is to design control conditions to help distinguish these possible strategies.For instance: did the model choose answer a over b simply because a is more frequent?Will the model show the same preference for answer a over b even when the question is omitted?</p>
<ol>
<li>
<p>DO NOT rely on well-known (or minimally changed) test items.When tested on a famous test item (such as "The trophy did not fit the suitcase because it was too small"; Box 1), the model has likely seen it multiple times during training and therefore has a high chance of relying on answer memorization; thus, its performance may not generalize to new, non-famous test items.Even if the experimenter makes a minor change to the original item, such as replacing "trophy" with "prize", a model might still be able to complete the test based on simple association between the old and the new item.</p>
</li>
<li>
<p>DO NOT overly trust crowd-sourced / automatically generated items.The ability of AI models to process large amounts of queries-way more than any human-creates a temptation of evaluating their performance on thousands of test items, obtained from online human workers, created by automatic item generation scripts, or even generated by AI models themselves.However, if the quality of these items is not rigorously evaluated, such tests may be meaningless.AI-generated items present their own set of challenges: a model might (a) generate test items that resemble samples from the training data or, more broadly, (b) generate patterns that have a high likelihood under the model and might therefore facilitate its performance.The systematic assessment of such biases in AI-generated materials remains to be done.</p>
</li>
</ol>
<p>DO evaluate models under conditions similar to humans when comparing their performance.</p>
<p>When conducting direct model-to-human comparisons, the default should be to recreate the same testing conditions for humans and models 4 .If a human needs instructions to perform well on a test, the model should also receive those instructions.If a human performs well on a task with no in-the-moment training, a model with human-level performance would also need to perform the task with no in-context training.If divergences during evaluation occur (for either theoretical or practical reasons), they need to be justified and highlighted when presenting the results.</p>
<ol>
<li>DO examine the effect of culture-specific aspects of the prompt on model behavior.The language in which the test is presented, the dialect, the vocabulary used, and even the use of capitalization and punctuation might all affect AI model performance as it leverages these cues to mimic specific users online.</li>
</ol>
<p>Interpreting the results</p>
<p>When interpreting the results of a cognitive assessment:</p>
<ol>
<li>
<p>DO compare model and human performance.We often assume humans will perform well on a specific test even when we don't have direct evidence for it.If these exact test items and question/prompt wordings have not been tested on humans before, they should be.9. DO be explicit about the experimental settings when reporting the results.Computational work should, ideally, have perfect replicability.For that to be achievable, all the materials and code needed to run the experiment should be made available online.For models offered by commercial third parties, there is no guarantee that they will continue to offer access to the model, so some backup arrangements may need to be made.Finally, a third party may roll out a model update that would change experimental results; in such cases, the date of the experiment needs to be reported as well.</p>
</li>
<li>
<p>DO NOT overly trust LLM item scoring.The flip side of testing a model on thousands of items is the need to evaluate responses to all these items.In some cases, determining correct answers is straightforward (e.g., a-d for a multiple-choice question); however, scoring free response items is hard work.Many studies are opting for LLM-based item evaluation; however, such approaches might be circular because the items that are hard for the model being evaluated might also be hard for the model doing the evaluation.At a minimum, the model being tested and the model doing the evaluation should belong to different classes; and even then, evaluator model performance needs to be rigorously checked by comparing it with human evaluations across a range of item difficulties.</p>
</li>
<li>
<p>DO NOT assume that model responses reflect "universal" human behavior.Even when directly comparing AI models and humans, it is important to be specific about which human population serves as a reference.Human psychology suffers from over-reliance on WEIRD (white, educated, industrialized, rich, democratic) participant pools; AI Psychology suffers from similar issues 5 because WEIRD individuals disproportionately contribute to the training data.Thus, calls to replace human data in psychology studies with AI-generated responses need to account for the strong demographic and cultural biases that these models bring.Similarly, model performance in English (the most represented language on the web) might not be reflective of model performance in other languages.</p>
</li>
<li>
<p>DO NOT assume that models solve the task in the same way as humans.Even if both humans and models do well on a particular test, it does not mean that they solve it in the same way.Some approaches to evaluating similarities and differences in the mechanisms underlying human and model task performance include comparing their error patterns, generalization to new tasks, and the internal (neural) representations required to accomplish the task.</p>
</li>
<li>
<p>DO check whether a model generalizes beyond a single test.Even when we control for possible shortcuts, a model might still find a loophole that allows it to perform well on a particular test.However, the more diverse tests we include (and the more we control for the shortcuts in each), the harder it will be to attribute model performance to serendipitous factors.If a model does well on 20 logical reasoning tasks that vary in their content and format, it is more plausible that it possesses logical reasoning than if it performs well on one such task.</p>
</li>
<li>
<p>DO NOT jump to conclusions.The discourse around AI models has become very polarized, with both extreme enthusiasm based on a few isolated examples and extreme skepticism based on isolated failures.What the field needs is careful evaluation of specific model capabilities with a clear acknowledgement of advances and limitations.Moreover, results reported for one model may not generalize to other models, so a single study's conclusions should be qualified accordingly.</p>
</li>
</ol>
<p>Case study: Winograd Schema Challenge</p>
<p>In 2012, Levesque and colleagues proposed a test for diverse kinds of basic world knowledge 6 (named after an initial example by Terry Winograd).The test includes minimally different item pairs with pronouns whose referent can be inferred based on general world knowledge:</p>
<p>(1) Q: The trophy doesn't fit into the brown suitcase because it's too small.What is too small?A: The suitcase</p>
<p>(2) Q: The trophy doesn't fit into the brown suitcase because it's too large.What is too large?A: The trophy A set of such minimally differing sentence pairs, tackling diverse world knowledge phenomenaphysical properties, biology, social situations, etc., -was then compiled into the Winograd Schema Challenge (WSC).Although initially challenging, the WSC was largely solved by 2020, with LLMs achieving over 90% accuracy 7 .However, it turned out that this success did not reflect models' deep knowledge of the world but rather the flaws in the test itself.</p>
<p>In the original paper introducing the challenge 6 , the authors cautioned against examples that could be solved via simple heuristics.They cite two such heuristics: selectional restrictions and frequency effects.The selectional restrictions heuristics can be illustrated with the example: "The women stopped taking the pills because they were pregnant/carcinogenic".Here, only an animate entity can be pregnant and only an inanimate entity can be carcinogenic, leading to an unambiguous association between the adjectives and the corresponding nouns without the need to tap into deeper world knowledge.The frequency heuristics applies to cases like: "The racecar zoomed by the school bus because it was going so fast/slow": a simple association between "racecar" and "fast" will suffice to determine the referent.Finally, the authors note that the examples need to be, in their words, Google-proof, i.e., not present in online text corpora used to train the models.</p>
<p>Despite the field's awareness of these heuristics, constructing a heuristics-free dataset was hard.In the original WSC dataset, over 10% of the items turned out to have simple association-based solutions 8 .To address this issue, Sakaguchi et al 7 constructed a large dataset called WinoGrande, where the examples were first crowd-sourced online and then automatically filtered to reduce cooccurrence-based associations as estimated through language model embeddings (rather than human intuition).This filtering substantially reduced model performance, suggesting that the association heuristic indeed inflates model accuracy.</p>
<p>Soon after, Elazar et al 9 showed that adding even more stringent quality controls leads to significant decreases in model performance on the WSC.The authors introduced two baseline cases to account for raw frequency of possible continuations: sentences with candidate referents excluded ("doesn't fit into because it's too large/small") and sentences with the first half excluded ("because it's too large/small").The assumption is that for these reduced sentences, there should be no consistent preference for "suitcase" vs. "trophy", and if there is, it reflects an association bias.It turned out that LLMs performed above chance on the WSC even in those baseline conditions, indicating previously undetected association biases.</p>
<p>The final issue raised by the WSC story is the quality vs. quantity tradeoff in test design.The initial WSC set includes less than 300 examples, all hand-crafted by scholars.Now that these examples are freely available online, performance on this set is no longer reflective of the models' general capabilities.The authors of WinoGrande used a different approach: to obtain thousands of novel items, they asked human workers to come up with many different examples and then used automatic filtering techniques.This approach is more scalable but suffers from numerous quality issues, e.g.typos ("wit" instead of "with"), grammatical errors ("more brighter color"), and unwarranted assumptions ("good at math" means "likely to be a professor").Overall, the tradeoff between result generalizability (which is harder for small datasets) and quality control (which is harder for large datasets) remains an important issue to consider in test design.</p>
<p>Kocijan et al 10 formulate several lessons from the WSC saga, the most important of which is perhaps: "We need to be careful not to rely on a perceived connection between tasks and methods".Just because we think that a certain task requires a cognitive ability X, doesn't mean that it actually does.</p>
<p>Testing open vs. closed models</p>
<p>Some of the DO'S discussed above -checking the training data for contamination with test items, verifying that the model has not been fine-tuned on the exact task being tested, creating conditions that will allow the study to be replicated in the future on the exact same model -are essentially impossible in the case of closed models.Thus, there is an argument to stop running AI Psychology assessments on closed models altogether given that, from a scientific perspective, the results are potentially non-reproducible and difficult to interpret.</p>
<p>Will researchers indeed stop running cognitive tests on closed AI models?Probably not.Although scientifically questionable, evaluating the behavior of closed, industry-standard models has important practical implications, including understanding which real-life tasks they can (and cannot) be reliably used for, what AI models are in principle capable of achieving (given that closed models often exhibit state-of-the-art performance), and what safety risks they may present.Thus, going forward, cognitive evaluations of AI systems might be used in two separate settings: (a) basic scientific inquiry of cognitive capacities of AI systems -which should prioritize open models, and (b) applied studies tackling questions related to model performance, safety, and user impact -which can be applied both to open and to closed models with the caveat that closed model results might not replicate or generalize.</p>
<p>As with any rapidly growing research area, the scientific practices and norms in AI Psychology get defined on the fly, often through trial and error.I hope that this discussion of the DO'S and DON'TS of AI Psychology will help distill some of the lessons learned and improve the robustness and validity of future work aiming to probe the cognitive capacities of AI systems.</p>
<p>determine what the model might have learned about your test during training consider alternative strategies a model might use to arrive at the correct answer incorporate careful control conditions evaluate models under conditions similar to humans examine the effect of culture- specific aspects of the prompt on model behavior</p>
<p>).I do not touch on the philosophical question of whether it is at all appropriate to ascribe mental capacities to a machine; my goal here is simply to clarify the methodological criteria that determine the inferences we can(not) make based on a model's responses to a questionnaire or a cognitive test.
y'allyoucompare model and humanperformance85% 87%</p>
<p>be explicit about the experimental settings when reporting the results check whether a model generalizes beyond a single test rely on well-known (or minimally changed) test items
The trophy is…The prize is…</p>
<p>overly trust crowd- sourced / automatically generated items My nam is Mark overly trust LLM item scoring assume that model responses reflect "universal" human behavior jump to conclusions A+ F- DO DON'T assume that models solve the task in the same way as humans
Figure 1: The do's and don'ts of AI model evaluation on cognitive tests.</p>
<ol>
<li>DO determine what the model might have learned about your test during training. The</li>
<li>DOtwo mostimportant issues to consider are: (a) was the model directly trained on your task? (b) did the model"see" examples from your test during its training? In the worst case, a and b might occur together,i.e., the model was trained on the task of interest using the same items as those in the current study.</li>
</ol>
<p>consider alternative strategies a model might use to arrive at the correct answer.</p>
<p>Even if the model has not directly learned your task during training or finetuning, it might still use a different strategy from the strategy you, the experimenter, have presupposed.The most prominent shortcuts</p>
<p>AcknowledgementsI gratefully acknowledge the funding support from the University System of Georgia.Many thanks to Aalok Sathe, Ben Lipkin, Carina Kauf, Greta Tuckute, Kyle Mahowald, and the reviewers for their constructive comments.Competing interestsThe author declares no competing interests.
Baby steps in evaluating the capacities of large language models. M C Frank, Nature Reviews Psychology. 22023</p>
<p>How do we know how smart AI systems are?. M Mitchell, 2023</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, Proceedings of the National Academy of Sciences. 121e23224201212024</p>
<p>Can language models handle recursively nested grammatical structures? a case study on comparing models and humans. A K Lampinen, arXiv:2210.153032022arXiv preprint</p>
<p>. M Atari, M J Xue, P S Park, D Blasi, Henrich, J. Which humans? PsyArXiv. 2023</p>
<p>The Winograd Schema Challenge. H Levesque, E Davis, L Morgenstern, Thirteenth international conference on the principles of knowledge representation and reasoning. 2012</p>
<p>An adversarial winograd schema challenge at scale. K Sakaguchi, R L Bras, C Bhagavatula, Y Choi, Winogrande, Communications of the ACM. 642021</p>
<p>How reasonable are commonsense reasoning tasks: A case-study on the Winograd schema challenge and SWAG. P Trichelair, A Emami, A Trischler, K Suleman, J C K Cheung, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong; ChinaAssociation for Computational Linguistics2019</p>
<p>Back to square one: Artifact detection, training and commonsense disentanglement in the Winograd schema. Y Elazar, H Zhang, Y Goldberg, D Roth, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>The defeat of the winograd schema challenge. V Kocijan, E Davis, T Lukasiewicz, G Marcus, L Morgenstern, Artificial Intelligence. 1039712023</p>            </div>
        </div>

    </div>
</body>
</html>
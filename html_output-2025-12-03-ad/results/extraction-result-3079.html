<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3079 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3079</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3079</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264426101</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.14628v2.pdf" target="_blank">Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts</a></p>
                <p><strong>Paper Abstract:</strong> As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks. In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts. For each question, XoT always begins with selecting the most suitable method then executes each method iteratively. Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods. Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module. Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain. By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework. The code is available at https://github.com/tengxiaoliu/XoT.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3079.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3079.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>X-of-Thoughts (XoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated iterative problem-solving framework that plans among, executes, verifies, and switches between diverse prompting/reasoning methods (CoT, PoT, EoT) to solve math and logical reasoning problems using LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A conversational large language model accessed via OpenAI API; experiments used few-shot prompting (8 examples), greedy decoding, and averaged results across 3 runs. Appendix reports experiments with Llama-2 series as additional base models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'Program-of-Thought / Program-Aided Language Model (PoT / PAL)', 'Equation-of-Thought (EoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>XoT uses a planning module to choose a reasoning method (CoT: step-by-step natural language rationale; PoT/PAL: generate Python programs executed by interpreters; EoT: build linear equation systems and solve deterministically). After a reasoning attempt, a verification module (passive via executor errors and active via assertion checks) decides whether to accept the answer or trigger planning to switch to another method; iterations continue until success or methods exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse set of reasoning methods. XoT explicitly integrates multiple distinct reasoning styles (natural-language CoT, code-based PoT, equation-based EoT) and dynamically switches between them via verification and planning rather than repeating the same style.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple math reasoning datasets (GSM8K, SVAMP, AQuA, Algebra, GSM-hard, AddSub, SingleOP, SingleEQ, MultiArith, MATH) and logical reasoning (FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Standard math word-problem benchmarks covering arithmetic, algebra, multi-step reasoning, and a logical reasoning benchmark (FOLIO) for first-order logic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported example metrics (accuracy) on GSM8K: CoT 80.2%, PoT 77.2%, EoT 63.8%; XoT: 83.3% on GSM8K. Across the 10 math datasets the paper reports an average XoT accuracy ≈ 84.63% and average majority-vote baseline ≈ 82.59%. Oracle upper-bound (if any method succeeds) ≈ 92.7% (paper cites 92.72% potential). XoT yields an average improvement of +5.49% over single-method baselines across datasets. On FOLIO (logical domain) XoT = 62.75% (validation), CoT = 58.82%, formal-Language FOL method = 42.65%, oracle = 77.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>XoT vs single methods: XoT consistently improves average performance compared to any single method by combining them and switching when verification fails. XoT vs majority voting: XoT (84.63% avg) outperforms majority-vote ensemble (82.59% avg) while consuming fewer tokens (~16.7% lower token cost reported). Oracle analysis shows combining methods gives substantial headroom (~+10% or more) compared to best single method. Ablations show the planning module improves efficiency (fewer iterations) and average accuracy relative to fixed method orders. Repeating the same method (PoT repeated 3 times) achieves substantially less complementarity than switching among different methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Distinct prompting styles (CoT, PoT, EoT) are complementary: oracle combination yields ~92% potential coverage while the best single method is substantially lower. 2) Integrating planning, passive+active verification, and method switching (XoT) improves average performance (reported +5.49% across math datasets) and is more token-efficient than naive majority voting. 3) Active verification reduces false positive verification rates and enables safer method switching. 4) Gains are larger on harder datasets with longer/multi-step problems; limited on very simple datasets where methods already individually perform well.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>1) On easy/single-step datasets (SingleEQ, MultiArith) there is minimal room for improvement; XoT can be negatively affected by verification errors, and in some datasets the delta is slightly negative. 2) PoT and EoT sometimes fail to produce executable/valid outputs on multiple-choice AQuA: PoT had ~24.4% non-executable answers and EoT ~30.3% across runs, so passive verification often forced method switching. 3) Passive-only verification has very high false-positive rates (paper reports alarmingly high rates e.g., 89.5% and 41.0% in some settings); adding active verification reduced false positives but increased false negatives slightly. 4) Smaller base models show weaker active verification capability and less effective switching; XoT benefits more from larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3079.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits step-by-step natural-language rationales from LLMs to perform multi-step reasoning, with intermediate textual steps guiding the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Few-shot prompted LLM used to generate natural-language rationales; used the examples from prior CoT literature and greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) - natural language step-by-step']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Implement by few-shot prompts containing exemplars with stepwise natural-language rationales; model produces a textual reasoning chain and computes on-the-fly.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single method (natural-language reasoning). In XoT CoT is one of multiple diverse methods and is used when planning selects it.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same math and logic datasets as main experiments (e.g., GSM8K, MATH, FOLIO)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-step math word-problem and logical reasoning tasks requiring intermediate reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Examples: GSM8K CoT ≈ 80.2% (reported). On some datasets CoT is the strongest single baseline; on MATH subtopics CoT achieved higher scores than PoT in several categories (paper reports many subtopic breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT is flexible in format but can lose arithmetic precision compared to code- or equation-based methods; EoT and PoT complement CoT by providing more deterministic computation. XoT can choose CoT but often prefers PoT/EoT early because those allow passive verification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT is a strong baseline for natural-language reasoning but is less amenable to deterministic verification and precise arithmetic than code/equation methods; complementarity with PoT and EoT yields gains when integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The verification module currently focuses more on PoT/EoT since CoT intermediate values are harder to extract for deterministic checking; authors leave stronger CoT verification to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3079.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-of-Thought / Program-Aided Language Model (PoT / PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm where the LLM generates executable code (Python) to perform computations and logic, and an external interpreter executes the code for deterministic results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM generates Python programs via few-shot prompts (examples from prior PAL/PoT papers), executed with external interpreter (passive verification).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Program generation and execution (PoT/PAL)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompt the model with examples that produce Python code implementing the solution; rely on external execution to obtain exact numeric results and detect runtime errors (passive verification).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single method style (code-based), used as one distinct style among others in XoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Math benchmarks (GSM8K, SVAMP, GSM-hard, etc.) and some MATH subtopics</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Math word problems requiring precise arithmetic and procedural computations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Example: GSM8K PoT ≈ 77.2% (reported). PoT often produces precise arithmetic when code runs correctly but can hallucinate assignments for unknown variables (problematic for algebraic unknowns). Across datasets PoT is sometimes best or competitive with CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PoT is more precise for numeric computation due to interpreter execution (passive verification) but struggles with unknown variables that require symbolic equation reasoning; EoT complements this by handling unknowns. Repeating PoT across iterations (PoT3) shows limited complementarity compared to switching methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PoT provides reliable computation when programs are executable; passive-executor feedback is a useful verification signal enabling robust switching. Integration into XoT yields better performance than PoT alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PoT can fail on problems involving undefined symbolic unknowns (requires defining values in code) and may produce non-executable outputs (paper reports ~24.4% of PoT answers on AQuA being non-executable across runs). Repeating PoT alone across iterations yields less oracle coverage than switching to different reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3079.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Equation-of-Thought (EoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method introduced in this paper that expresses word problems as systems of linear equations (symbolic), solves them with a deterministic equation solver (sympy), and is particularly effective for problems with unknown variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM is prompted with few-shot examples to produce linear-equation representations of problems; equations are solved externally using sympy (deterministic solver).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Equation-system construction and symbolic solving (EoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM translates natural language into algebraic equations with unknown variables; external symbolic solver (sympy) deterministically solves the system and provides intermediate variable values for active verification.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>A distinct style (symbolic/equation-based) complementing natural-language and program-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Algebraic word problems, GSM8K, GSM-hard, and others where unknown variables are central</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Problems that can be naturally modeled with unknowns and linear equations, where backward reasoning or symbolic manipulation is appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Example: GSM8K EoT ≈ 63.8% (reported) — lower than CoT/PoT on GSM8K but excels on problems with unknown variables and algebraic structure; EoT contributes substantially to oracle coverage. On some datasets (e.g., Algebra) EoT is a key complementary method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>EoT can express unknowns directly, avoiding PoT's need to assign concrete values; CoT may set up equations but make calculation errors. Combining EoT with PoT and CoT improves oracle coverage and overall XoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Introducing EoT increases complementarity across methods, particularly improving the model's ability to solve algebraic problems that PoT (code) struggles with due to requirement to define variables; EoT with symbolic solver gives deterministic correctness for solvable linear-equation formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>EoT occasionally produces non-executable outputs or formulations not suitable for sympy (paper reports ~30.3% of EoT answers non-executable on AQuA across runs), and on datasets requiring heavy numeric calculation (rather than symbolic reasoning) EoT alone lags behind PoT/CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3079.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PoT repeated (PoT3) - repeated same-method attempts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental ablation where the framework repeatedly uses the same prompting method (PoT) across multiple iterations (with either identical or different few-shot examples) instead of switching methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three sequential attempts of PoT on each problem (same examples or different examples across attempts) to test whether repetition of the same reasoning style can approximate complementarity from diverse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Repeated Program-of-Thought (PoT repeated)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Rather than switching to other methods on failure, the framework re-queries the same PoT prompt multiple times (PoT3) possibly with varied in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Similar style only (no diversity) — designed to compare the benefit of repeating a single method vs switching between diverse methods.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same math benchmarks (e.g., GSM8K, SVAMP, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Tests whether reattempting the same method yields similar complementarity as switching methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports PoT3 oracle complementarity under oracle setting of ~84.08% (i.e., repetitively exploiting a single method reaches limited complementarity), final PoT3 performance ≈ 78.39% on GSM8K compared to XoT ≈ 82.71% (example numbers in paper context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PoT3 shows substantially less oracle coverage and final performance than XoT which switches among methods; repeated attempts of the same style cannot fully substitute diverse reasoning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Repetition of the same reasoning style gives limited gains; diversity of reasoning styles is important for covering a broader set of solvable problems.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Even with different few-shot examples between repeats (PoT3-d), oracle complementarity remains significantly lower (~8-9% less) compared to XoT's use of multiple distinct methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3079.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Majority Vote</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority Voting Ensemble of Methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline ensemble approach that runs multiple prompting methods (CoT, PoT, EoT), collects their answers, and returns the majority answer as the final prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each method is executed once to produce an answer; majority voting picks the consensus answer. This requires running all methods for every question (highest token/execution cost).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Ensemble majority vote over CoT, PoT, EoT outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Execute each method once per question and apply majority vote to final answers (no iterative verification/switching).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse in methods (uses all three), but static (no iterative switching or verification).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Same math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Baseline ensemble to compare with XoT's adaptive switching-and-verification approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported average accuracy ~82.59% across datasets (paper Table 7) versus XoT ~84.63%; majority voting consumed more tokens (~95.4k vs XoT ~84.5k in reported token accounting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Majority vote underperforms XoT in average accuracy and is less token-efficient because it always runs all methods rather than stopping when verification passes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptive verification + early stopping in XoT yields better accuracy and lower compute/token cost than naive majority voting.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Majority vote can perform poorly on datasets where one method is especially well-suited (e.g., Algebra): majority vote diluted the stronger method signal and performed worse than XoT that can select the best method earlier.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3079.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3079.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verification (passive + active)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Passive and Active Verification Modules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary verification techniques: passive verification uses external executor feedback (e.g., runtime errors) to detect failures; active verification asks the model to assert that computed intermediate/final values satisfy problem constraints and executes those assertions for deterministic checks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Verification implemented via prompts and external execution: passive checks executor exceptions; active builds assertion statements from intermediate values returned by executors (without exposing model's full rationale) and runs assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Passive verification (executor errors)', 'Active verification (model-generated assertions checked by executor)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Passive: if the generated program/equations fail to execute, treat as verification failure. Active: the model receives intermediate numeric results and is prompted to assert whether the results meet the question's conditions; executor runs assertions to yield pass/fail.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Verification is orthogonal to reasoning styles; it enables method-switching and supports diverse methods by providing deterministic checks for PoT/EoT (less developed for CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Math benchmarks and PoT/EoT outputs primarily</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to decide whether a reasoning attempt is accepted or prompts switching; critical to iterative XoT behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Passive-only verification reported to have high false-positive rates (paper reports alarming false-positive rates e.g., 89.5% and 41.0% in some settings). Adding active verification reduced false positives substantially (paper reports reductions of 56.8% and 24.3%) and improved overall XoT accuracy by ~2.3%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Passive verification is cheap and reliable for catching executor errors but allows many incorrect solutions to pass (high false positives). Active verification trades some increased false negatives for much lower false positives, enabling more reliable switching and overall better XoT performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining passive and active verification is necessary: passive verification filters execution errors; active verification enables deterministic checks that materially improve switching decisions and final accuracy. Verification design strongly affects XoT's ability to explore alternative methods.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Passive-only verification is insufficient (very high false-positive rates). Active verification increases false negatives (correct answers being rejected), but this is mitigated since other methods can still be tried; verification adds token/compute overhead and can introduce accumulated errors hurting very simple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>PAL: program-aided language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Solving math word problems by combining language models with symbolic solvers <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3079",
    "paper_id": "paper-264426101",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "XoT",
            "name_full": "X-of-Thoughts (XoT)",
            "brief_description": "An integrated iterative problem-solving framework that plans among, executes, verifies, and switches between diverse prompting/reasoning methods (CoT, PoT, EoT) to solve math and logical reasoning problems using LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "A conversational large language model accessed via OpenAI API; experiments used few-shot prompting (8 examples), greedy decoding, and averaged results across 3 runs. Appendix reports experiments with Llama-2 series as additional base models.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "Program-of-Thought / Program-Aided Language Model (PoT / PAL)",
                "Equation-of-Thought (EoT)"
            ],
            "reasoning_methods_description": "XoT uses a planning module to choose a reasoning method (CoT: step-by-step natural language rationale; PoT/PAL: generate Python programs executed by interpreters; EoT: build linear equation systems and solve deterministically). After a reasoning attempt, a verification module (passive via executor errors and active via assertion checks) decides whether to accept the answer or trigger planning to switch to another method; iterations continue until success or methods exhausted.",
            "diversity_of_methods": "Diverse set of reasoning methods. XoT explicitly integrates multiple distinct reasoning styles (natural-language CoT, code-based PoT, equation-based EoT) and dynamically switches between them via verification and planning rather than repeating the same style.",
            "reasoning_task_name": "Multiple math reasoning datasets (GSM8K, SVAMP, AQuA, Algebra, GSM-hard, AddSub, SingleOP, SingleEQ, MultiArith, MATH) and logical reasoning (FOLIO)",
            "reasoning_task_description": "Standard math word-problem benchmarks covering arithmetic, algebra, multi-step reasoning, and a logical reasoning benchmark (FOLIO) for first-order logic problems.",
            "performance_by_method": "Reported example metrics (accuracy) on GSM8K: CoT 80.2%, PoT 77.2%, EoT 63.8%; XoT: 83.3% on GSM8K. Across the 10 math datasets the paper reports an average XoT accuracy ≈ 84.63% and average majority-vote baseline ≈ 82.59%. Oracle upper-bound (if any method succeeds) ≈ 92.7% (paper cites 92.72% potential). XoT yields an average improvement of +5.49% over single-method baselines across datasets. On FOLIO (logical domain) XoT = 62.75% (validation), CoT = 58.82%, formal-Language FOL method = 42.65%, oracle = 77.45%.",
            "comparison_of_methods": "XoT vs single methods: XoT consistently improves average performance compared to any single method by combining them and switching when verification fails. XoT vs majority voting: XoT (84.63% avg) outperforms majority-vote ensemble (82.59% avg) while consuming fewer tokens (~16.7% lower token cost reported). Oracle analysis shows combining methods gives substantial headroom (~+10% or more) compared to best single method. Ablations show the planning module improves efficiency (fewer iterations) and average accuracy relative to fixed method orders. Repeating the same method (PoT repeated 3 times) achieves substantially less complementarity than switching among different methods.",
            "key_findings": "1) Distinct prompting styles (CoT, PoT, EoT) are complementary: oracle combination yields ~92% potential coverage while the best single method is substantially lower. 2) Integrating planning, passive+active verification, and method switching (XoT) improves average performance (reported +5.49% across math datasets) and is more token-efficient than naive majority voting. 3) Active verification reduces false positive verification rates and enables safer method switching. 4) Gains are larger on harder datasets with longer/multi-step problems; limited on very simple datasets where methods already individually perform well.",
            "counter_examples_or_negative_results": "1) On easy/single-step datasets (SingleEQ, MultiArith) there is minimal room for improvement; XoT can be negatively affected by verification errors, and in some datasets the delta is slightly negative. 2) PoT and EoT sometimes fail to produce executable/valid outputs on multiple-choice AQuA: PoT had ~24.4% non-executable answers and EoT ~30.3% across runs, so passive verification often forced method switching. 3) Passive-only verification has very high false-positive rates (paper reports alarmingly high rates e.g., 89.5% and 41.0% in some settings); adding active verification reduced false positives but increased false negatives slightly. 4) Smaller base models show weaker active verification capability and less effective switching; XoT benefits more from larger models.",
            "uuid": "e3079.0",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting method that elicits step-by-step natural-language rationales from LLMs to perform multi-step reasoning, with intermediate textual steps guiding the final answer.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "Few-shot prompted LLM used to generate natural-language rationales; used the examples from prior CoT literature and greedy decoding.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT) - natural language step-by-step"
            ],
            "reasoning_methods_description": "Implement by few-shot prompts containing exemplars with stepwise natural-language rationales; model produces a textual reasoning chain and computes on-the-fly.",
            "diversity_of_methods": "Single method (natural-language reasoning). In XoT CoT is one of multiple diverse methods and is used when planning selects it.",
            "reasoning_task_name": "Same math and logic datasets as main experiments (e.g., GSM8K, MATH, FOLIO)",
            "reasoning_task_description": "Multi-step math word-problem and logical reasoning tasks requiring intermediate reasoning steps.",
            "performance_by_method": "Examples: GSM8K CoT ≈ 80.2% (reported). On some datasets CoT is the strongest single baseline; on MATH subtopics CoT achieved higher scores than PoT in several categories (paper reports many subtopic breakdowns).",
            "comparison_of_methods": "CoT is flexible in format but can lose arithmetic precision compared to code- or equation-based methods; EoT and PoT complement CoT by providing more deterministic computation. XoT can choose CoT but often prefers PoT/EoT early because those allow passive verification.",
            "key_findings": "CoT is a strong baseline for natural-language reasoning but is less amenable to deterministic verification and precise arithmetic than code/equation methods; complementarity with PoT and EoT yields gains when integrated.",
            "counter_examples_or_negative_results": "The verification module currently focuses more on PoT/EoT since CoT intermediate values are harder to extract for deterministic checking; authors leave stronger CoT verification to future work.",
            "uuid": "e3079.1",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PoT",
            "name_full": "Program-of-Thought / Program-Aided Language Model (PoT / PAL)",
            "brief_description": "A prompting paradigm where the LLM generates executable code (Python) to perform computations and logic, and an external interpreter executes the code for deterministic results.",
            "citation_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "LLM generates Python programs via few-shot prompts (examples from prior PAL/PoT papers), executed with external interpreter (passive verification).",
            "model_size": null,
            "reasoning_methods": [
                "Program generation and execution (PoT/PAL)"
            ],
            "reasoning_methods_description": "Prompt the model with examples that produce Python code implementing the solution; rely on external execution to obtain exact numeric results and detect runtime errors (passive verification).",
            "diversity_of_methods": "Single method style (code-based), used as one distinct style among others in XoT.",
            "reasoning_task_name": "Math benchmarks (GSM8K, SVAMP, GSM-hard, etc.) and some MATH subtopics",
            "reasoning_task_description": "Math word problems requiring precise arithmetic and procedural computations.",
            "performance_by_method": "Example: GSM8K PoT ≈ 77.2% (reported). PoT often produces precise arithmetic when code runs correctly but can hallucinate assignments for unknown variables (problematic for algebraic unknowns). Across datasets PoT is sometimes best or competitive with CoT.",
            "comparison_of_methods": "PoT is more precise for numeric computation due to interpreter execution (passive verification) but struggles with unknown variables that require symbolic equation reasoning; EoT complements this by handling unknowns. Repeating PoT across iterations (PoT3) shows limited complementarity compared to switching methods.",
            "key_findings": "PoT provides reliable computation when programs are executable; passive-executor feedback is a useful verification signal enabling robust switching. Integration into XoT yields better performance than PoT alone.",
            "counter_examples_or_negative_results": "PoT can fail on problems involving undefined symbolic unknowns (requires defining values in code) and may produce non-executable outputs (paper reports ~24.4% of PoT answers on AQuA being non-executable across runs). Repeating PoT alone across iterations yields less oracle coverage than switching to different reasoning styles.",
            "uuid": "e3079.2",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "EoT",
            "name_full": "Equation-of-Thought (EoT)",
            "brief_description": "A prompting method introduced in this paper that expresses word problems as systems of linear equations (symbolic), solves them with a deterministic equation solver (sympy), and is particularly effective for problems with unknown variables.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "LLM is prompted with few-shot examples to produce linear-equation representations of problems; equations are solved externally using sympy (deterministic solver).",
            "model_size": null,
            "reasoning_methods": [
                "Equation-system construction and symbolic solving (EoT)"
            ],
            "reasoning_methods_description": "LLM translates natural language into algebraic equations with unknown variables; external symbolic solver (sympy) deterministically solves the system and provides intermediate variable values for active verification.",
            "diversity_of_methods": "A distinct style (symbolic/equation-based) complementing natural-language and program-based approaches.",
            "reasoning_task_name": "Algebraic word problems, GSM8K, GSM-hard, and others where unknown variables are central",
            "reasoning_task_description": "Problems that can be naturally modeled with unknowns and linear equations, where backward reasoning or symbolic manipulation is appropriate.",
            "performance_by_method": "Example: GSM8K EoT ≈ 63.8% (reported) — lower than CoT/PoT on GSM8K but excels on problems with unknown variables and algebraic structure; EoT contributes substantially to oracle coverage. On some datasets (e.g., Algebra) EoT is a key complementary method.",
            "comparison_of_methods": "EoT can express unknowns directly, avoiding PoT's need to assign concrete values; CoT may set up equations but make calculation errors. Combining EoT with PoT and CoT improves oracle coverage and overall XoT performance.",
            "key_findings": "Introducing EoT increases complementarity across methods, particularly improving the model's ability to solve algebraic problems that PoT (code) struggles with due to requirement to define variables; EoT with symbolic solver gives deterministic correctness for solvable linear-equation formulations.",
            "counter_examples_or_negative_results": "EoT occasionally produces non-executable outputs or formulations not suitable for sympy (paper reports ~30.3% of EoT answers non-executable on AQuA across runs), and on datasets requiring heavy numeric calculation (rather than symbolic reasoning) EoT alone lags behind PoT/CoT.",
            "uuid": "e3079.3",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PoT3",
            "name_full": "PoT repeated (PoT3) - repeated same-method attempts",
            "brief_description": "An experimental ablation where the framework repeatedly uses the same prompting method (PoT) across multiple iterations (with either identical or different few-shot examples) instead of switching methods.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "Three sequential attempts of PoT on each problem (same examples or different examples across attempts) to test whether repetition of the same reasoning style can approximate complementarity from diverse methods.",
            "model_size": null,
            "reasoning_methods": [
                "Repeated Program-of-Thought (PoT repeated)"
            ],
            "reasoning_methods_description": "Rather than switching to other methods on failure, the framework re-queries the same PoT prompt multiple times (PoT3) possibly with varied in-context examples.",
            "diversity_of_methods": "Similar style only (no diversity) — designed to compare the benefit of repeating a single method vs switching between diverse methods.",
            "reasoning_task_name": "Same math benchmarks (e.g., GSM8K, SVAMP, etc.)",
            "reasoning_task_description": "Tests whether reattempting the same method yields similar complementarity as switching methods.",
            "performance_by_method": "Paper reports PoT3 oracle complementarity under oracle setting of ~84.08% (i.e., repetitively exploiting a single method reaches limited complementarity), final PoT3 performance ≈ 78.39% on GSM8K compared to XoT ≈ 82.71% (example numbers in paper context).",
            "comparison_of_methods": "PoT3 shows substantially less oracle coverage and final performance than XoT which switches among methods; repeated attempts of the same style cannot fully substitute diverse reasoning methods.",
            "key_findings": "Repetition of the same reasoning style gives limited gains; diversity of reasoning styles is important for covering a broader set of solvable problems.",
            "counter_examples_or_negative_results": "Even with different few-shot examples between repeats (PoT3-d), oracle complementarity remains significantly lower (~8-9% less) compared to XoT's use of multiple distinct methods.",
            "uuid": "e3079.4",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Majority Vote",
            "name_full": "Majority Voting Ensemble of Methods",
            "brief_description": "A baseline ensemble approach that runs multiple prompting methods (CoT, PoT, EoT), collects their answers, and returns the majority answer as the final prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "Each method is executed once to produce an answer; majority voting picks the consensus answer. This requires running all methods for every question (highest token/execution cost).",
            "model_size": null,
            "reasoning_methods": [
                "Ensemble majority vote over CoT, PoT, EoT outputs"
            ],
            "reasoning_methods_description": "Execute each method once per question and apply majority vote to final answers (no iterative verification/switching).",
            "diversity_of_methods": "Diverse in methods (uses all three), but static (no iterative switching or verification).",
            "reasoning_task_name": "Same math benchmarks",
            "reasoning_task_description": "Baseline ensemble to compare with XoT's adaptive switching-and-verification approach.",
            "performance_by_method": "Reported average accuracy ~82.59% across datasets (paper Table 7) versus XoT ~84.63%; majority voting consumed more tokens (~95.4k vs XoT ~84.5k in reported token accounting).",
            "comparison_of_methods": "Majority vote underperforms XoT in average accuracy and is less token-efficient because it always runs all methods rather than stopping when verification passes.",
            "key_findings": "Adaptive verification + early stopping in XoT yields better accuracy and lower compute/token cost than naive majority voting.",
            "counter_examples_or_negative_results": "Majority vote can perform poorly on datasets where one method is especially well-suited (e.g., Algebra): majority vote diluted the stronger method signal and performed worse than XoT that can select the best method earlier.",
            "uuid": "e3079.5",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Verification (passive + active)",
            "name_full": "Passive and Active Verification Modules",
            "brief_description": "Two complementary verification techniques: passive verification uses external executor feedback (e.g., runtime errors) to detect failures; active verification asks the model to assert that computed intermediate/final values satisfy problem constraints and executes those assertions for deterministic checks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (OpenAI API)",
            "model_description": "Verification implemented via prompts and external execution: passive checks executor exceptions; active builds assertion statements from intermediate values returned by executors (without exposing model's full rationale) and runs assertions.",
            "model_size": null,
            "reasoning_methods": [
                "Passive verification (executor errors)",
                "Active verification (model-generated assertions checked by executor)"
            ],
            "reasoning_methods_description": "Passive: if the generated program/equations fail to execute, treat as verification failure. Active: the model receives intermediate numeric results and is prompted to assert whether the results meet the question's conditions; executor runs assertions to yield pass/fail.",
            "diversity_of_methods": "Verification is orthogonal to reasoning styles; it enables method-switching and supports diverse methods by providing deterministic checks for PoT/EoT (less developed for CoT).",
            "reasoning_task_name": "Math benchmarks and PoT/EoT outputs primarily",
            "reasoning_task_description": "Used to decide whether a reasoning attempt is accepted or prompts switching; critical to iterative XoT behavior.",
            "performance_by_method": "Passive-only verification reported to have high false-positive rates (paper reports alarming false-positive rates e.g., 89.5% and 41.0% in some settings). Adding active verification reduced false positives substantially (paper reports reductions of 56.8% and 24.3%) and improved overall XoT accuracy by ~2.3%.",
            "comparison_of_methods": "Passive verification is cheap and reliable for catching executor errors but allows many incorrect solutions to pass (high false positives). Active verification trades some increased false negatives for much lower false positives, enabling more reliable switching and overall better XoT performance.",
            "key_findings": "Combining passive and active verification is necessary: passive verification filters execution errors; active verification enables deterministic checks that materially improve switching decisions and final accuracy. Verification design strongly affects XoT's ability to explore alternative methods.",
            "counter_examples_or_negative_results": "Passive-only verification is insufficient (very high false-positive rates). Active verification increases false negatives (correct answers being rejected), but this is mitigated since other methods can still be tried; verification adds token/compute overhead and can introduce accumulated errors hurting very simple datasets.",
            "uuid": "e3079.6",
            "source_info": {
                "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "PAL: program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Solving math word problems by combining language models with symbolic solvers",
            "rating": 2,
            "sanitized_title": "solving_math_word_problems_by_combining_language_models_with_symbolic_solvers"
        }
    ],
    "cost": 0.0169175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts</p>
<p>Tengxiao Liu 
School of Computer Science
Fudan University</p>
<p>Qipeng Guo 
Amazon AWS AI</p>
<p>Yuqing Yang yuqingyang21@m.fudan.edu.cn 
School of Computer Science
Fudan University</p>
<p>Xiangkun Hu 
Amazon AWS AI</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
School of Engineering
Westlake University</p>
<p>Xipeng Qiu xpqiu@fudan.edu.cn 
School of Computer Science
Fudan University</p>
<p>Zheng Zhang 
Amazon AWS AI</p>
<p>Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts
9F5FFB673C99D80E9129432AC7909E70
As large language models (LLMs) have shown effectiveness with different prompting methods, such as Chain of Thought, Program of Thought, we find that these methods have formed a great complementarity to each other on math reasoning tasks.In this work, we propose XoT, an integrated problem solving framework by prompting LLMs with diverse reasoning thoughts.For each question, XoT always begins with selecting the most suitable method then executes each method iteratively.Within each iteration, XoT actively checks the validity of the generated answer and incorporates the feedback from external executors, allowing it to dynamically switch among different prompting methods.Through extensive experiments on 10 popular math reasoning datasets, we demonstrate the effectiveness of our proposed approach and thoroughly analyze the strengths of each module.Moreover, empirical results suggest that our framework is orthogonal to recent work that makes improvements on single reasoning methods and can further generalise to logical reasoning domain.By allowing method switching, XoT provides a fresh perspective on the collaborative integration of diverse reasoning thoughts in a unified framework.</p>
<p>Introduction</p>
<p>The AI community has long sought to achieve automated reasoning (Hewitt, 1969), which is an important component of Artificial General Intelligence (Steunebrink et al., 2016).Mathematical reasoning, as a cognitive skill essential for humans yet challenging for language models, attracts increasing interests and commitment from researchers (Feigenbaum and Feldman, 1963;Wang et al., 2017;Lu et al., 2022).</p>
<p>With the abilities endowed by in-context learning (ICL), Large Language Models (LLMs) Figure 1: CoT only reasons in a single pass, while selfrefine involves refinement using the same method.XoT integrates a verification module that makes a difference in method planning, enabling the attempts of diverse reasoning thoughts within an iterative framework.(Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;OpenAI, 2023) are able to solve mathematical problems through textual rationales with Chain-of-Thought prompting (Wei et al., 2022) (CoT) or through Python functions with Program-Aided Language Model (Gao et al., 2022) and Program-of-Thought prompting (Chen et al., 2022) (PAL or PoT).These prompting methods exhibit unique strengths and limitations.CoT generates a step-by-step reasoning flow in natural language and performs calculations on the fly.This approach enables a more flexible solution format, but may result in a loss of precision since language models often struggle with arithmetic calculations (Lewkowycz et al., 2022;Wei et al., 2022).On the other hand, PoT or PAL resolves problems through Python statements, relying on Python interpreters to ensure calculation accuracy.Another noteworthy and intriguing prompting method is to form math problems as linear equation systems (He-Yueya et al., 2023).Similarly, inspired by Linear Algebra, we propose Equation-of-Thought (EoT), which performs math reasoning in a more direct way.The diversity inherent in each method does not arXiv:2310.14628v2[cs.CL] 27 Dec 2023 render them as competing or mutually exclusive alternatives.On the contrary, in practical problem solving scenarios, possessing multiple methods can always yield a range of complementary advantages.</p>
<p>The distinct problem-solving approaches can contribute to synergistic benefits that surpass the outcomes of any single approach.We find that this intuition also applies to the realm of math reasoning.With the availability of CoT, PoT and EoT, we hold the hypothesis that a model has the potential to solve a problem if it reaches the correct answer using any one of the prompting methods.As illustrated in Figure 2, our analysis shows that the model exhibits the potential to solve 92.72% of the problems, surpassing the best performing single method by over 10%.</p>
<p>Motivated by this observation, we propose XoT, an integrated math problem solving framework, which improves the LLM's reasoning ability by switching among diverse reasoning thoughts.Since there is no guarantee that LLMs can always solve the problem in a single attempt, we follow the human intuition and allow the model to rethink and switch to a different method when encountering difficulties or obstacles.We apply two complementary verification methods to facilitate the model to decide whether it is time to switch to another method: passive and active verification.Passive verification relies on the external executors to provide determinable results based on the generated programs (Chen et al., 2023;Le et al., 2022).It offers shallow inspections, such as program syntax issues or the runtime errors.For active verification, we ask the model to verify the solution by checking whether the answer adheres to the conditions outlined in the original question.</p>
<p>As shown in Figure 1, XoT consists of three modules that work in an iterative framework: planning, reasoning and verification.Given a problem as input, the planning module first proposes the most appropriate method.The reasoning module then generates one solution using the planned prompting method.With the outputs and the results from external executors, the model is asked to assess the answers in the context of the questions.If the answer fails the verification, we will go back to the planning module for another round of iteration and attempt alternative methods.The iterative process concludes when the verification confirms the correctness of the answer or after exhausting all available methods.To demonstrate the effectiveness of XoT, we conduct extensive experiments on 10 popular mathematical reasoning datasets and achieve consistent improvement.Empirical results suggest that XoT can accommodate recent work that focuses on improving single reasoning methods.Additional experiments also indicate that XoT can generalise to other domains such as logical reasoning tasks.</p>
<p>We summarize the main contributions as follows.First, we propose an integrated problem solving framework XoT, utilising the complementarity of different reasoning thoughts.Second, we introduce EoT which solves math problems with a system of linear equations, serving as a complementary method to existing approaches.Third, we incorporate passive and active verification to facilitate the framework to switch among diverse reasoning thoughts, empowering the framework to make informed decisions regarding the subsequent steps to be taken.More generally, XoT sheds lights on a new direction of interacting with diverse reasoning methods and tools.As shown in Figure 1, instead of sticking to one determined method, LLMs can benefit from the verification and the flexible switching among available reasoning thoughts.1 2 Related Work</p>
<p>Math Reasoning with LLMs</p>
<p>As the field of large language models continues to prosper, many prompting techniques have emerged to unlock the reasoning abilities of LLMs (Qiao et al., 2022).Early success includes reasoning with step-by-step chain of thought (Wei et al., 2022), decomposing questions into sub-questions in a least-to-most fashion (Zhou et al., 2022), zero-shot prompting LLMs with simply one sentence (Kojima et al., 2022), writing programs to solve procedural tasks (Gao et al., 2022;Chen et al., 2022).Despite generating solutions in single forward pass, one line of work employs multiple reasoning results and ensembles them by majority vote (Wang et al., 2022), and stepwise verifier (Li et al., 2022).Additionally, Tree-of-Thoughts (Yao et al., 2023) deliberately explores multiple reasoning paths and searches over a tree-structured reasoning states.Imani et al. (2023) propose to vote over multiple solutions generated with algebraic and program prompts.One concurrent work (Zhao et al., 2023) considers the difference of CoT and PoT and asks the LLM to choose one better reasoning rationale.In contrast to their work, XoT involves more reliable verification modules and switches methods when necessary.</p>
<p>Iterative Refinement</p>
<p>One stream of work is dedicated to iteratively enhancing LLMs by continuously reevaluating and refining outputs until the desired quality is achieved.Madaan et al. (2023) prompts the model to write feedback based on previously generated drafts and leverages the feedback to generate high-quality outputs.Similarly, Chen et al. (2023) iteratively debugs the code by utilizing external program execution results and code explanations generated by the model itself.In order to avoid repetitive mistakes, Shinn et al. (2023) builds a memory of previous errors, while Wang and Li (2023) collects all mistakes during the training phase to provide a global insight.When considering sources of hints to guide rethinking, Paul et al. (2023) focuses on intermediate reasoning steps, while Zheng et al. (2023) directly utilizes the previously generated answers.Qi et al. (2023) propose to emulate the divideand-conquer fashion of human thinking strategy and involve self-questioning and recursive thinking processes in the problem solving framework.Although these approaches contribute to improving the reasoning quality of LLMs, they are limited in retrying without looking around for other possible thoughts.In contrast, our proposed method aims to explore alternative solutions, and it is orthogonal to iterative refinement, as we have the flexibility to switch solutions when refining no longer leads to further improvement.</p>
<p>Preliminary</p>
<p>Prompting methods</p>
<p>For math reasoning tasks, we use three reasoning thoughts in this work, namely Chain-of-Thought (CoT), Program-of-Thought (PoT) and Equation-of-Thought (EoT).Despite the wellknown strengths of CoT and PoT methods, our proposed EoT excels particularly in reasoning with unknown variables.For each problem, EoT attempts to model the questions as linear equations and involves unknown values in the description.</p>
<p>A detailed formulation of EoT prompting can be found in Table 12 of Appendix C. As illustrated in Figure 3, while CoT correctly sets up the equations, it fails in accurately performing the calculations.PoT falls short in dealing with unknown variables, as Python requires that every variable is defined with a value.Assigning a value to an unknown variable (david_insects) hallucinates PoT to generate a misleading step (the highlighted line).In comparison, EoT manages to express the question context in straightforward equations and solves them with a deterministic equation solver.</p>
<p>Complementarity</p>
<p>Given a question q, we denote the correctness of the reasoning answers using each method as RX (q), where X ∈ {CoT, P oT, EoT } denotes the diverse reasoning methods.RX (q) = {0, 1} represents whether the generated answer is correct according to the gold label.We define the accuracy under the oracle setting as:
ACC oracle = q RCoT (q)∨ RP oT (q)∨ REoT (q).
(1) The oracle setting represents that the model has the potential for solving one given problem if any of the methods accurately generates the answer.It also implies that in cases where the generated answer does not match the gold answers, XoT will make further attempts using alternative methods to answer the question.Under oracle setting, the model can potentially achieve more than 10% gains on various datasets.In Figure 2, the bar at the bottom represents the highest performance achieved by employing a single method, followed by the optimal performance achieved through the use of two methods.The overall stacked bar shows the utilization of all three methods, which indicates the upper bound that can be reached through the combined collaboration of various methods.</p>
<p>XoT</p>
<p>Our goal is to develop a generalized problem solving framework that can automatically select the appropriate method for different problems and has the capability to switch among reasoning thoughts using both active and passive verification.We first describe the overall framework and introduce each module in detail.</p>
<p>Overall Framework</p>
<p>The overall pipeline is described in Algorithm 1.The inputs of our framework include a question q and a predefined set of methods M .With the user input, XoT employs its three built-in modules to output the final solution, namely planning module P , reasoning module R and verification module V .</p>
<p>These three modules collaborate in an iterative manner.Suppose at iteration t, the planning module P first chooses the most appropriate method available: m t = P (M ).The chosen method is subsequently excluded from the set of methods.The reasoning module is then tasked to generate Algorithm 1 XoT Reasoning Algorithm Require: input question q, method set M , planning module P , reasoning module R, verifica-
tion module V 1: t ← 0 2: while |M | &gt; 0 do 3: m t ← P (M ) ▷ Choose method 4: M ← M \ {m t } 5: y ← R mt (q) 6: if V (y) then 7: break ▷ Verification passed 8:
else 9:
t ← t + 1 ▷ Continue next iteration 10:
end if 11: end while 12: return y ▷ Return the solution one solution y using the proposed method m t .Following this, the verification module evaluates the solution by rethinking the answer within the given conditions.If the answer successfully passes the verification, we proceed to return the current solution.Otherwise, XoT will move forward to the next iteration.Every module is implemented with a LLM through inference under few-shot setting.</p>
<p>We will elaborate each module with details.</p>
<p>Planning and Reasoning</p>
<p>The planning module is responsible for selecting the appropriate method at the beginning of each round of iteration.Recent work shows the necessity to equip reasoning framework with the ability to plan ahead (Lu et al., 2023).As elaborated in Section 3, it is evident that each method possesses distinct strengths.Our intuition is to consistently initiate the process with the optimal method to enhance reasoning efficiency.</p>
<p>The reasoning module performs few-shot reasoning with the planned prompting method.Each round of reasoning operates independently, meaning that subsequent iterations do not rely on the failed reasoning attempts of previous iterations.</p>
<p>Verification module</p>
<p>The verification module assesses the effectiveness of the reasoning solution through two approaches: passive verification and active verification.</p>
<p>When solutions involve offloading computation to external tools, the execution results naturally serve as a passive verification.Any occurrence of errors or exceptions during the execution directly 222 ≥ 2 ⋆ GSM-hard (Gao et al., 2022) 1,313 3.25 MATH (Hendrycks et al., 2021) 5,000 ≥ 3 ⋆ AddSub (Hosseini et al., 2014) 395 1 SingleOP (Roy et al., 2015) 562 1 SingleEQ (Koncel-Kedziorski et al., 2015) 508 1.31 MultiArith (Roy and Roth, 2015) 600 2 results in a failure in the verification process.Solutions that pass the passive verification stage then proceed to active verification.</p>
<p>In the case of active verification, the module rethinks the answer within the context of the given question.It first acquires all intermediate values associated with each variable mentioned in the solution.These values are computed by external executors.We intentionally exclude the reasoning process (expressions) leading to the results to prevent the verification module from emulating the solution's thinking process.With the intermediate results and final answer in hand, the module is expected to recheck whether the answer satisfies the conditions specified in the question.The desired format for this evaluation is an assertion statement, as shown in Figure 4.This assertion is subsequently combined with the original solution for external tools to execute.If no issues arise during this execution phase, it means the solution successfully passes the verification.A detailed illustration of the prompts we use can be found in Appendix C. The verification module is specially designed for PoT and EoT as the intermediate values can be easily obtained.We leave the exploration of a more effective verification for CoT as future work.</p>
<p>Experiments</p>
<p>Experimental Setting</p>
<p>Datasets Our experiments are conducted on a comprehensive set of 10 math reasoning datasets, encompassing various challenging math reasoning scenarios.Some widely used datasets include GSM8K, SVAMP, AQuA, MATH and MAWPS (AddSub, SingleOP, SingleEQ, Multi-Arith) (Koncel-Kedziorski et al., 2016).Besides, we also incorporate several recently introduced datasets, namely Algebra, GSM-hard.Algebra comprises a collection of solely algebraic word problems that can be resolved through the use of equations.To increase the complexity of calculations, GSM-hard replaced small numerical values with larger ones.The details of the statistics of the datasets can be found in Table 1.</p>
<p>Model We query OpenAI API for experiments 2 .Specifically we use gpt-3.5-turboas the inference engine.If not further explained, we manually construct the prompts with 8 examples sampled from the training set.For CoT and PoT, we directly use the examples released by published paper (Fu et al., 2022;Gao et al., 2022;Chen et al., 2022).For model generation strategy, we employ greedy decoding in all runs.Due to the non-deterministic APIs, we report the average performance and the standard deviation across 3 runs.We also evaluate XoT with various base models in Appendix A.2.</p>
<p>Main Results</p>
<p>The main results are shown in Table 2.We consider three prompting methods as baselines, namely CoT, PoT and EoT.On average, XoT achieves a significant improvement of 5.49% across the datasets.For MATH dataset, we show the breakdown results of different question subtopics in Table 3.We also represent the performance enhancement over the strongest baseline as ∆.As questions in MATH are too complex for equation systems to solve, we only consider CoT and PoT with passive verification.Specifically, on the AQuA dataset, which consists of multiple-choice questions, we observe that PoT or EoT often fails to generate a valid answer due to the diverse answer formats.Across the three runs, 24.4% of the PoT answers and 30.3% of the EoT answers cannot be executed.Therefore, applying passive verification is adequate to ensure the explortion of other method options.When post processing the generated results, we further enforce a restriction that the model cannot make a random guess if it fails to extract an answer from the generated output.Such instances should be proceeded to the next iteration to guarantee a fair evaluation of the performance.</p>
<p>Notably, we observe that the enhancements are more pronounced for the challenging datasets compared to the easier ones.Difficult datasets usually contain longer questions and more than 3 reasoning steps while easier datasets such as SingleEQ require only one equation to solve the problem.We find that the improvement directly correlates with the complementary nature of the three methods employed across different datasets.On easier datasets, each method performs well individually, resulting in only minor complementarity.Figure 5 reveals that XoT demonstrates superior performance on 2 https://openai.comdatasets that exhibit stronger enhancement under oracle setting.The bars in the figure represent the improvement under XoT, while the line indicates the upper bound of the improvement under oracle setting.The comparison indicates that MultiArith and SingleEQ allow minimal room for improvement, therefore the overall XoT performance is negatively impacted by the accumulated errors introduced by the verification module.</p>
<p>Additionally, we conduct experiments on logical reasoning task to evaluate the generalisability of XoT.Details can be found in Appendix A.1.</p>
<p>Analysis</p>
<p>In this section, we first analyze the effectiveness and necessity of each module within XoT.Then we provide comparison with majority voting and describe how model's self refinement can be integrated in our framework.</p>
<p>Ablation Study</p>
<p>Planning The planning module decides which method to attempt at the beginning of each iteration.We are curious about how well it performs in selecting the most suitable method among the available options.The planning module is expected to select from PoT and EoT at the beginning because these two methods can be verified with both active and passive verification.To demonstrate the necessity of the planning module, we conduct an experiment in which XoT is asked to execute each method in a predefined order.Whether to switch the method is still determined by the verification module.We break down the performance of each dataset with respect to different combinations of methods in Table 4.</p>
<p>Our findings align with two design ethos of the planning module.First, it demonstrates robustness across different datasets.While specific combinations excel at different datasets, XoT equipped with the planning module outperforms all other predetermined combinations on average.For instance, on GSM-hard, the combination of PoT and EoT achieves the best performance, which highlights the importance of leveraging external tools to handle calculation involving large numbers.Additionally, on SingleEQ and MultiArith where XoT fails to offer improvement, the combination of two methods proves to be efficient, surpassing the single method baselines.With the inclusion of the planning module, XoT can dynamically adjust the  execution order based on different questions, which ensures a more consistent and robust performance.Second, the planning module enhances efficiency, facilitating XoT to reach the final answer in fewer iterations by always starting from the most possible method.To illustrate, on GSM8K, XoT needs 1.46 iterations on average in comparison with 1.58 iterations with the fixed EPC order (EoT-&gt;PoT-&gt;CoT, the best performing fixed order).Specifically, 68.8% of the questions are resolved in the first iteration with XoT, as opposed to 57.2% when employing the fixed EPC order.</p>
<p>Reasoning How important is it to try different methods instead of exclusively relying on a single method?To investigate this, we restrict the available method options to utilizing PoT only, denoted as PoT 3 .In other words, if the generated solution fails to pass the verification, it reconsiders its reasoning using the same prompting method instead of changing to another.The results are demonstrated in Figure 6.PoT 3 uses the same few-shot examples in three iterations while PoT Table 4: Results across different datasets without the planning module.We manually define the execution sequence, denoted as the combination of the first letter in each method.For example, 'PEC' indicates PoT-EoT-CoT.This suggests the necessity employing various reasoning methods in our framework.</p>
<p>Verification The verification module facilitates seamless switching between iterations.We here explore how helpful the active and passive verifications are. Figure 7  such a simplistic verification approach yields an alarmingly high false positive rate of 89.5% and 41.0%, as shown in Table 5.This drawback is particularly critical as our XoT's essence lies in the ability to adaptively switch methods, and a high false positive rate restricts the model's ability to explore alternative method options.By additionally incorporating active verification, despite a slight compromise in accuracy, the false positive rate is substantially reduced by 56.8% and 24.3%.We also note that this approach inevitably leads to an increase in the false negative rate.However, this is a minor drawback as the subsequent method options still have chances to get it correct.Consequently, employing active verification offers 2.3% gains to the overall XoT performance.Additionally, we explore the necessity of the iterative nature of XoT by removing the entire verification module.In this scenario, we only reason once with the most suitable method suggested by the planning module.The results are presented in Table 6.As our planning module mainly chooses the method from PoT or EoT, we here restrict the available methods to PoT and EoT only in XoT framework, which is denoted as 'XoT (only PE)'.By removing the verification module, the framework, denoted by 'XoT (w/o verification)' is no more capable of rechecking the answer thus cannot  perform iterative attempts to switch methods.This leads to a performance degradation of 4.9% and 2.9% on GSM8K and SVAMP respectively.</p>
<p>Comparison with Majority Voting</p>
<p>We additionally conduct experiments involving the majority vote of three distinct methods.The vote is based on three answers generated by three methods (one answer per method).As shown in Table 7, taking the majority vote of the three methods achieves 82.59 on average, while XoT achieves better performance at 84.63.Additionally, we observe that the majority vote fails on datasets containing questions that align exceptionally well with a specific method.Specifically, the majority vote achieves 79.73 on Algebra, while XoT achieves 89.94.The majority vote needs to execute all three methods to reach an answer, while XoT will stop when the answer passes the verification.We calculate the total token count as #total_token = #input_token + #output_token * 2, according to OpenAI's pricing policy 3 .As shown from the table, XoT is able to achieve higher performance with a lower budget, exhibiting a reduction of 16.7% in expenses.The token count includes all the incontext examples used and is averaged across the number of the total questions in 9 datasets.</p>
<p>Self-refinement</p>
<p>The design principle underlying XoT is its adaptable capability to switch methods, allowing for smooth integration with research aimed at improving individual methods.The line of iterative refinement methods enhances the model performance 3 https://openai.com/pricingby asking the model to rethink on its previous response, serving as a good alternative for the reasoning module in XoT.Specifically, before moving on to another method at each iteration, we allow the model to first make self refinement on its current approach, making the best use of current method.</p>
<p>Inspired by previous work (Madaan et al., 2023), after reasoning with one method for the first time, we require the model to analyze its response lineby-line and summarize several advice to mitigate the potential errors.Then, the model answers the question for a second time in the same method, with the summarized advice as a hint.After that, we verify the results produced by the second round and determine whether to switch to another method.</p>
<p>To achieve the iterative refinement in CoT, we follow Zheng et al. (2023) to progressively hint the model with the answers generated before.For PoT and EoT, we follow the released self-refinement prompts from Madaan et al. (2023).The results are shown in Table 8.We only allow the model to think twice using each prompting method.Though adding only one round of refinement yields marginal improvement within each single method, their collaboration contributes to a more significant improvement under XoT framework.</p>
<p>Conclusion</p>
<p>We propose XoT, an integrated problem solving framework that utilizes diverse reasoning thoughts to prompt LLMs.XoT integrates planning, reasoning and verification into a unified framework, enabling the model to explore multiple methods based on the active and passive verification of the solutions.We conduct extensive experiments on 10 math reasoning datasets to thoroughly evaluate the advantages of each module and showcase the efficacy of our proposed approach.Further results also show that the design ethos of XoT can generalize to logic reasoning domain.We consider its generalisation to more diverse tasks as a compelling avenue for future exploration.</p>
<p>A Further Analysis</p>
<p>A.1 Generalisation to logical domain</p>
<p>We analyze the generalisability of XoT framework to logical reasoning domain.One recent work (Pan et al., 2023) proposed LogicLM to solve logical reasoning questions using First Order Logic expressions and executed them in external symbolic reasoners.Following LogicLM, we design similar formal language expressions to represent First Order Logic and conduct experiments on FOLIO (Han et al., 2022), an expert-written, logically complex and diverse dataset for natural language reasoning.Our findings in Table 9 suggest that different methods in logical domain also show strong complementarity, achieving 77.45% under oracle setting.After involving the verification module, XoT performs at 62.75% on the validation set of FOLIO.These results underscore the applicability of XoT as a general problem solving framework.</p>
<p>A.2 Experiments on other models</p>
<p>We further assess the performance of XoT across various base models, such as Llama-2 series (Touvron et al., 2023b).The results are shown in Table 10, and we illustrate the performance scaling  10. curve in Figure 8.With less capable models, different prompting methods still demonstrate strong complementarity under oracle setting.Our observations suggest that smaller models tend to yield suboptimal results, likely due to the unbalanced performance across different reasoning approaches and the models' limited capability for active verification.This limitation inhibits the model's ability to timely switch between methods.However, as the model's size increases, XoT consistently shows its strength across the datasets.</p>
<p>A.3 Proportion of XoT</p>
<p>Figure 9 illustrates the proportion of different methods that XoT selects as the final answers.On GSM8K, 56.7% questions end up being solved with PoT, while 28.3% are tackled by EoT.The remaining 15% is left for CoT to solve.</p>
<p>B XoT with self refinement</p>
<p>We here offer the details of how we combine iterative self-refinement with XoT framework.As shown in Figure 10, the self refinement process can be integrated in the reasoning module, where the dashed line indicates rethinking using the same method.When the desired number of self refinement iterations is reached, the generated solutions will proceed to the verification module.Then the verification will determine whether to use the current solution or change to another method.</p>
<p>C Examples</p>
<p>In this section, we show the input and output examples of each module in XoT.Full prompts are available in public Github repository: https: //github.com/tengxiaoliu/XoT.For EoT, we use sympy4 library to solve the linear equations.</p>
<p>Figure 2 :
2
Figure 2: Complementarity of X-of-Thought methods on different datasets.The stacked bars indicate the best performance achieved by using one, two and three methods separately.Employing multiple methods under oracle setting can offer significant performance gains.</p>
<p>QuestionFigure 3 :
3
Figure 3: In particular cases where CoT and PoT fall short, EoT successfully solves the problem, which serves as a good complement.</p>
<p>Question##Figure 4 :
4
Figure4: Overview of XoT.Following the suggestion of the planning module, XoT first reasons with PoT.However, the generated answer fails in the verification module.In the second iteration, the selected method is EoT.The reasoning module successfully generates the solution that passes the verification.</p>
<p>Figure 5 :
5
Figure5: The correlation between oracle performance and final improvement.A higher oracle gain allows more room for XoT to improve.</p>
<p>PoT 3 Figure 6 :
36
Figure 6: Repeatedly exploiting the same method (PoT 3 ) results in limited complementarity compared to XoT with three methods.PoT 3 -d denotes we use different few-shot examples in three iterations.</p>
<p>Figure 7 :
7
Figure 7: Comparison of passive and active verifications.The blue and green matrices represent verifications for PoT and EoT respectively.</p>
<p>Figure 8 :
8
Figure 8: Performance scaling curve on different base models.The performance is averaged across the four datasets shown in Table10.</p>
<p>Figure 9 :
9
Figure 9: The proportion of different methods that XoT finally chooses as the answer on GSM8K.</p>
<p>Figure 10 :
10
Figure 10: Self refinement can be integrated in the XoT framework.The dashed block indicates the reasoning module with the inclusion self refinement.Within each self refinement process, the model repeatedly exploits the same method.</p>
<p>Table 1 :
1
Statistics of the datasets we used.# Steps denotes the average number of reasoning steps in the gold answers.⋆ indicates a rough estimate due to the inconsistent rationale formats.</p>
<p>Table 2 :
2
GSM8K SVAMP AQuA ⋆ Algebra GSM-hard AddSub SingleOP SingleEQ MultiArith Average Main experiment results across various math reasoning datasets.Under oracle setting, XoT switches the method if the generated answer does not match the gold answers.⋆ denotes we only use passive verification.∆ represents the improvement over the best performing baseline.
CoT80.2 0.279.5 0.655.1 1.0 81.5 0.842.4 0.188.4 0.393.4 0.394.3 0.197.5 0.379.14PoT77.2 0.379.5 0.349.2 1.0 62.5 0.761.8 0.488.4 0.293.4 0.498.1 0.197.2 0.078.59EoT63.8 0.469.6 0.746.7 0.5 82.3 0.553.8 0.271.6 1.075.4 0.485.8 0.878.6 0.669.73XoT83.3 0.583.6 0.661.7 0.6 89.9 0.363.4 0.590.5 0.494.3 0.397.7 0.197.3 0.384.63oracle 92.5 0.292.7 0.377.0 1.4 95.5 0.574.3 0.493.9 0.397.5 0.099.1 0.199.3 0.091.31∆+3.1+4.1+6.6+7.6+1.6+2.1+0.9-0.4-0.2+5.49InterAlgebra Precalculus Geometry NumTheory Probability PreAlgebra Algebra OverallCoT17.8 0.420.3 0.424.4 0.432.2 1.030.4 0.656.6 0.449.7 0.4 35.77 0.4PoT14.4 0.115.5 0.18.8 0.331.2 0.719.6 0.236.5 0.238.2 0.1 25.79 0.1XoT25.1 0.126.0 0.325.3 0.748.1 0.636.1 0.462.0 0.257.3 0.4 42.81 0.0oracle28.1 0.231.2 0.127.6 0.450.5 0.139.0 0.768.0 0.464.1 0.4 47.35 0.2∆+7.3+5.7+0.9+15.9+5.7+5.4+7.6+7.04</p>
<p>Table 3 :
3
Experiment results on MATH dataset.We only employ two methods and passive verification on MATH.</p>
<p>3 -d uses differente examples randomly sampled from the training set.It is observed that under orcale setting, repetitive exploitation of a single method has limited complementarity of 84.08%, which is 8.64% less than XoT.As a result, the final performance reflects such a gap with PoT 3 of 78.39% and XoT of 82.71%.
Methods GSM8K SVAMP AQuA Algebra GSM-hard AddSub SingleOP SingleEQ MultiArith AveragePE77.7 0.380.7 0.256.7 1.0 81.7 0.563.4 0.389.6 0.393.8 0.398.0 0.295.0 0.281.85PC81.8 0.282.7 0.661.7 1.5 83.6 0.559.6 0.490.4 0.094.4 0.298.3 0.197.8 0.283.36EP80.9 0.480.8 0.458.0 0.6 83.8 0.964.6 0.388.4 0.494.1 0.596.7 0.097.8 0.282.80EC82.4 0.581.4 0.660.0 0.6 92.0 0.356.2 0.487.3 0.493.7 0.295.1 0.197.3 0.282.82EPC82.6 0.582.6 0.663.1 1.0 89.9 0.363.1 0.488.7 0.694.5 0.396.7 0.097.5 0.084.29PEC82.6 0.483.1 0.561.8 1.0 85.3 0.563.3 0.390.1 0.394.4 0.398.2 0.197.4 0.384.02XoT83.3 0.583.6 0.661.7 0.6 89.9 0.363.4 0.590.5 0.494.3 0.397.7 0.197.3 0.384.63</p>
<p>Table 5 :
5
Ablation results of different verification methods on GSM8K.Employing active verification significantly reduces false positive rate and results in a notable improvement in the overall XoT performance.</p>
<p>Table 6 :
6
Ablation results of excluding the entire verification module on GSM8K and SVAMP.XoT (only PE) is equipped with the verification module.The lack of this module compromises its ability for iterative methodswitching, resulting in diminished performance.
illustrates the performancecomparison when considering different verificationaspects. If we solely depend on passive verifica-tion, only 2.43% of the PoT results and 24.18% ofthe EoT results are deemed "incorrect" and subse-quently advanced to the next iteration. However,</p>
<p>GSM8K SVAMP AQuA Algebra GSM-hard AddSub SingleOP SingleEQ MultiArith Average #Tokens
XoT 83.3 0.583.6 0.6 61.7 0.6 89.9 0.363.4 0.590.5 0.494.3 0.397.7 0.197.3 0.384.634.5kVote 82.4 0.284.7 0.8 55.6 1.9 79.7 0.561.3 1.189.4 0.494.4 0.197.2 0.198.5 0.282.595.4k</p>
<p>Table 7 :
7
Comparison between XoT and Majority Voting.XoT outperforms the majority vote approach in a more efficient manner, yielding an average gain of 2.04 with a reduction of 16.7% in token count.#Tokens denotes the average number of tokens consumed for one case (including prompts, question and response).
ACC ACC + refineCoT 80.481.7PoT 76.976.9EoT 64.166.5XoT 82.784.5Table 8: Results of adding self-refinement within rea-soning module on GSM8K test set.</p>
<p>Table 9 :
9
XoT performance on logical reasoning task FOLIO validation set.Normal text reasoning and formal language FOL are complement to each other under oracle setting and XoT framework.
Method FOLIO ACCCoT58.82FOL42.65Oracle77.45XoT62.75
Code is publicly available at: https://github.com/ tengxiaoliu/XoT.
https://www.sympy.org/
AcknowledgementsWe would like to thank the anonymous reviewers for their valuable suggestions and feedback.This work was supported by the National Natural Science Foundation of China (No. 62236004 and No.  62022027).LimitationsWe acknowledge that our approach falls short on easier and more straightforward datasets where different methods exhibit limited complementary relations.Our current approach relies on the availability of diverse prompting methods for reasoning tasks.Further research is required to explore new problem solving methods for general reasoning tasks.Moreover, we observe that our method works better on larger base models.Although different reasoning methods do exhibit notable complementarity on smaller models, the inherent potential is not yet fully unleashed in current XoT design.Ethics StatementThe data used in our work all comes from public dataset, and our proposed method can be further integrated with other methods.Our work is conformant to ACL Ethics Policy.Input:You need to choose the best method for the given question.Each method has its own strength.The methods are described as follows:-Python Program: This method generates a Python program that can solve the given question.It takes in the question and possible context and produces a program.Normally, we consider using this method when the questions and contexts involve forward reasoning, such as arithmetic operations over multiple numbers, or when the questions involve complex logical operations, such as "if-else" statements.-System of linear equations: This method builds a math model and generates a system of linear equations that contains the answer as an unknown variable.Normally, we consider using this method when the questions and contexts involve an unknown variable that must be used to build an equation, especially when the question can be better modeled with abstract mathematical declarations, or when the unknown variable appears at the beginning of the questions and needs backward reasoning to solve.Below are some examples that choose the most appropriate method for the math word problems.{Examples} Question: Alyssa, Keely, and Kendall ordered 100 chicken nuggets from a fast-food restaurant.Keely and Kendall each ate twice as many as Alyssa.How many did Alyssa eat? Method:Output: System of linear equations  Program: nuggets_total = 100 nuggets_keely = 2 * nuggets_alyssa nuggets_kendall = 2 * nuggets_alyssa nuggets_alyssa = (nuggets_total -nuggets_keely -nuggets_kendall) / 5 ans = nuggets_alyssa Python Interpreter: NameError: name 'nuggets_alyssa' is not defined
Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 10.48550/arXiv.2211.12588CoRR, abs/2211.125882022</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, 10.48550/arXiv.2304.05128CoRR, abs/2304.051282023</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, 10.48550/arXiv.2204.02311M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, CoRR, abs/2110.141682021</p>
<p>Edward A Feigenbaum, Julian Feldman, Computers and thought. 1963</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, 10.48550/arXiv.2210.00720CoRR, abs/2210.007202022</p>
<p>PAL: program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, 10.48550/arXiv.2211.10435CoRR, abs/2211.104352022</p>
<p>FOLIO: natural language reasoning with first-order logic. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq R Joty, Alexander R Fabbri, Wojciech Kryscinski, 10.48550/arXiv.2209.00840CoRR, abs/2209.008402022Xi Victoria Lin, Caiming Xiong, and Dragomir Radev</p>
<p>Solving math word problems by combining language models with symbolic solvers. Joy He-Yueya, Gabriel Poesia, Rose E Wang, Noah D Goodman, 10.48550/arXiv.2304.09102CoRR, abs/2304.091022023</p>
<p>Measuring mathematical problem solving with the MATH dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021. the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 20212021. December 2021</p>
<p>PLANNER: A language for proving theorems in robots. Carl Hewitt, Proceedings of the 1st International Joint Conference on Artificial Intelligence. the 1st International Joint Conference on Artificial IntelligenceWashington, DC, USAWilliam Kaufmann1969. May 7-9, 1969</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, 10.3115/v1/d14-1058Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, 10.48550/arXiv.2303.05398CoRR, abs/2303.053982023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, NeurIPS. 2022</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , 10.1162/tacl_a_00160Trans. Assoc. Comput. Linguistics. 32015</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, 10.18653/v1/n16-1136The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. San Diego California, USAThe Association for Computational Linguistics2016. June 12-17, 2016NAACL HLT 2016</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven Chu, -Hong Hoi, NeurIPS. 2022</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, V Vinay, Ambrose Ramasesh, Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022In NeurIPS</p>
<p>On the advance of making language models better reasoners. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 10.48550/arXiv.2206.02336CoRR, abs/2206.023362022</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017. 2017. July 30 -August 41</p>
<p>Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, 10.48550/arXiv.2304.09842CoRR, abs/2304.09842Chameleon: Plug-and-play compositional reasoning with large language models. 2023</p>
<p>A survey of deep learning for mathematical reasoning. Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, Kai-Wei Chang, 10.48550/arXiv.2212.10535CoRR, abs/2212.105352022</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 10.48550/arXiv.2303.17651CoRR, abs/2303.176512023</p>
<p>10.48550/arXiv.2303.08774CoRR, abs/2303.08774GPT-4 technical report. 2023OpenAI</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023. December 6-10, 2023</p>
<p>Are NLP models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, 10.18653/v1/2021.naacl-main.168Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021Association for Computational Linguistics2021. June 6-11, 2021</p>
<p>Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, Boi Faltings, 10.48550/arXiv.2304.01904CoRR, abs/2304.01904REFINER: reasoning feedback on intermediate representations. 2023</p>
<p>The art of SOCRATIC QUESTIONING: zero-shot multimodal reasoning with recursive thinking and selfquestioning. Jingyuan Qi, Zhiyang Xu, Ying Shen, Minqian Liu, Di Jin, Qifan Wang, Lifu Huang, 10.48550/arXiv.2305.14999CoRR, abs/2305.149992023</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.48550/arXiv.2212.09597CoRR, abs/2212.095972022</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, 10.18653/v1/d15-1202Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalThe Association for Computational Linguistics2015. 2015. September 17-21, 2015</p>
<p>Reasoning about quantities in natural language. Subhro Roy, Tim Vieira, Dan Roth, 10.1162/tacl_a_00118Trans. Assoc. Comput. Linguistics. 32015</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, 10.48550/arXiv.2303.11366CoRR, abs/2303.113662023</p>
<p>R Bas, Pei Steunebrink, Ben Wang, Goertzel, 10.1007/978-3-319-41649-6Artificial General Intelligence -9th International Conference, AGI 2016. Lecture Notes in Computer Science. New York, NY, USASpringer2016. July 16-19, 20169782</p>
<p>Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurélien Azhar, Armand Rodriguez, Joulin, 10.48550/arXiv.2302.13971CoRR, abs/2302.13971</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/arXiv.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288</p>
<p>Learn from mistakes through cooperative interaction with study assistant. Danqing Wang, Lei Li, 10.48550/arXiv.2305.13829CoRR, abs/2305.138292023</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Denny Zhou, 10.48550/arXiv.2203.11171CoRR, abs/2203.111712022</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, 10.18653/v1/d17-1088Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017. 2017. September 9-11, 2017</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, 10.48550/arXiv.2305.10601CoRR, abs/2305.106012023</p>
<p>Automatic model selection with large language models for reasoning. Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Qizhe Xie, 10.48550/arXiv.2305.14333CoRR, abs/2305.143332023</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 10.48550/arXiv.2304.09797CoRR, abs/2304.097972023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, Ed H Chi, 10.48550/arXiv.2205.10625CoRR, abs/2205.106252022</p>            </div>
        </div>

    </div>
</body>
</html>
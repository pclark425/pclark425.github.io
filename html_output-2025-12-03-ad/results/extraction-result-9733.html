<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9733 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9733</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9733</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-278996939</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.22752v1.pdf" target="_blank">Climate Finance Bench</a></p>
                <p><strong>Paper Abstract:</strong> Climate Finance Bench introduces an open benchmark that targets question-answering over corporate climate disclosures using Large Language Models. We curate 33 recent sustainability reports in English drawn from companies across all 11 GICS sectors and annotate 330 expert-validated question-answer pairs that span pure extraction, numerical reasoning, and logical reasoning. Building on this dataset, we propose a comparison of RAG (retrieval-augmented generation) approaches. We show that the retriever's ability to locate passages that actually contain the answer is the chief performance bottleneck. We further argue for transparent carbon reporting in AI-for-climate applications, highlighting advantages of techniques such as Weight Quantization.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9733.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9733.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge automated grading framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated evaluation method that asks a language model to compare a model-generated answer to a gold/reference answer and output a discrete label (2=correct, 1=incomplete, 0=incorrect), used to scale grading of 330 RAG answers over corporate climate disclosures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Question answering over climate-finance documents (retrieval-augmented generation outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude 3.5 Sonnet (primary); also Llama3.1 8B Instruct variants were tested as graders.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Given the question, the RAG-generated answer and the gold/reference answer, the LLM was prompted to output only a single numeric label (2=correct, 1=incomplete, 0=incorrect). Prompt variants were tested: (a) Llama3.1 8B without question reminder, (b) Llama3.1 8B with question reminder, (c) Claude 3.5 Sonnet with question reminder; the best-performing setting (Claude with question included) was used for main experiments. See Appendix A and prompt design description.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotators labeled a sample of 330 RAG answers using the same 3-level scale (correct / incomplete / incorrect). Annotators were ESG analysts; gold answers were produced by ESG experts with a two-step review and final QC. Exactness on numeric values and supporting textual evidence were key criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Reported metrics include soft-match (counts where {correct,incomplete} vs. human considered agreement) and hard-match (only {correct}). Table 2: Llama3.1 8B (no question) soft 82.4% / hard 50.0%; Llama3.1 8B (with question) soft 84.4% / hard 54.2%; Claude 3.5 (with question) soft 83.9% / hard 68.7%. Distribution shift (Table 3): human labels = {Correct 52.7%, Incomplete 19.1%, Incorrect 28.2%}; Claude = {Correct 41.8%, Incomplete 20.0%, Incorrect 38.2%}. Type I (false accept) = 18 (5.45%), Type II (false reject) = 54 (16.36%) (see Appendix A/A.5 and Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using an LLM as judge yields a conservative bias: fewer answers are labeled correct and more are labeled incorrect compared to humans (≈9.9 percentage points fewer 'correct', ≈10.0 pp more 'incorrect'); this produces a higher Type II (false-reject) rate (~16%), i.e., genuinely correct answers are sometimes downgraded. The automated grader can therefore (a) under-credit partially correct or stylistically different-but-correct responses, (b) be stricter on exactness of numeric values/units, and (c) introduce distributional differences relative to human judgments that make reported accuracies slightly conservative.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete figures from the paper: Table 3 shows humans labeled 174/330 (52.7%) answers as correct while Claude labeled 138/330 (41.8%) as correct; incorrect labels rose from 93 (28.2%) under human grading to 126 (38.2%) under Claude. Appendix A reports Type I errors = 18 (5.45%) and Type II errors = 54 (16.36%), demonstrating that the grader underrates correct answers roughly three times more often than it overcredits wrong ones. The authors note that some genuinely correct answers are penalized, making model accuracies slightly conservative when graded automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite the conservative bias, the LLM grader (Claude 3.5 with question) achieved high alignment: soft agreement ≈83.9% and the highest hard-match observed (68.7%), i.e., it matched human 'correct' labels on more than two-thirds of cases. The paper frames this conservatism as desirable for a benchmark (to avoid inflating scores through false positives). Prompt design matters: including the question in the grader prompt improved alignment. Also, LLM-as-a-Judge enabled scalable, reproducible grading and large reductions in manual overhead, and the authors accept the conservative trade-off for fully automated scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results: 'Manual Evaluation and LLM-as-a-Judge' (main text); Appendix A (A.1–A.5), Table 2, Table 3, Table 4, and A.5 (Type I/II error discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Climate Finance Bench', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9733.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9733.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude 3.5 Sonnet (as grader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet (used as LLM-as-a-Judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial instruction-tuned LLM that the authors used as the primary automated grader; when prompted with question + generated answer + gold answer it produced discrete labels and showed the highest hard-match alignment with human graders in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation / grading of question-answering outputs in a climate-finance RAG benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude 3.5 Sonnet (with question included in prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>For grading, the prompt included the question, the RAG answer and the gold answer; the model was instructed to output only a single numeric label (2/1/0). The Claude grader's configuration (with question included) delivered the best alignment and was used for the main automated evaluations. See Appendix A prompt variants and main Results.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>See entry for LLM-as-a-Judge: human annotation on 330 answers by ESG analysts with two-step expert review and QC; labels were correct / incomplete / incorrect with emphasis on numeric exactness and textual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Claude (with question) soft-match 83.9% and hard-match 68.7% against human labels (Table 2). Label distribution comparison (Table 3) and confusion matrix (Table 4) underpin Type I/II calculations (Type I = 18 / 330 = 5.45%; Type II = 54 / 330 = 16.36%).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Claude's grader is noticeably stricter than humans: it assigns roughly 9.9 percentage points fewer 'correct' labels and 10.0 percentage points more 'incorrect' labels; it therefore increases false rejections of correct answers and can penalize answers that are semantically correct but differ in wording, units, or presentation.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>From Table 3: human correct=174 vs Claude correct=138. From Type I/II reporting: Type II (false reject) = 54 cases where Claude judged answers incorrect or incomplete while humans judged them correct. The paper states 'The grader is conservative: it underrates correct answers three times more than is overcredits wrong ones.'</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Claude nevertheless achieved the best hard-match (68.7%) and maintained soft-match parity with Llama3.1 variants (~84%), suggesting strong alignment in the majority of cases. Authors argue the conservatism is acceptable for benchmarking to avoid inflated scores. Including the question in the prompt improved Claude's performance as a grader.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Main Results 'Manual Evaluation and LLM-as-a-Judge'; Appendix A (Tables 2, 3, 4; A.4 Distribution Shift; A.5 Type I and Type II Risk).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Climate Finance Bench', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9733.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9733.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1 8B Instruct (as grader)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama-3.1 8B Instruct (used/tested as an automated grader)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight instruction-tuned model tested as an automated grader in two prompt variants: without a question reminder and with a question reminder; it achieved good soft agreement but lower hard-match vs. Claude 3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation / grading of question-answering outputs in a climate-finance RAG benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Llama3.1 8B Instruct (two grader prompt variants: without question reminder; with question reminder).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Grader prompt variants: (1) Llama3.1 8B without question reminder, (2) Llama3.1 8B with question reminder. In both cases the model received the RAG output and gold answer and was asked to return only 0/1/2. Performance compared to human judgments was measured (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>See LLM-as-a-Judge entry: 330 answers labeled by human annotators (ESG analysts), two-step expert review and QC.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Llama3.1 8B (no question) soft-match 82.4% / hard-match 50.0%; Llama3.1 8B (with question) soft-match 84.4% / hard-match 54.2% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Compared to human grading, Llama3.1 graders showed lower hard-match rates (i.e., fewer exact 'correct' matches) and thus are less reliable to fully replace human judgments; prompt context (including question) matters to improve alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Hard-match for Llama variants was 50.0% (no question) and 54.2% (with question), substantially below Claude's 68.7%, indicating more frequent disagreements where the model fails to mark answers as strictly 'correct' even when humans do.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Soft-match agreement was still ~82–84%, showing reasonable overall parity when 'correct' and 'incomplete' are treated jointly. Including the question in the grader prompt improved agreement, indicating prompt engineering can reduce some losses.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Appendix A (A.2 Prompt Design, Table 2) and main Results 'Manual Evaluation and LLM-as-a-Judge'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Climate Finance Bench', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FINANCEBENCH: A new benchmark for financial question answering <em>(Rating: 2)</em></li>
                <li>Searching for best practices in retrieval-augmented generation <em>(Rating: 1)</em></li>
                <li>Climate-FEVER: A Dataset for Verification of Real-World Climate Claims <em>(Rating: 1)</em></li>
                <li>Survey of Hallucination in Natural Language Generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9733",
    "paper_id": "paper-278996939",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-Judge",
            "name_full": "LLM-as-a-Judge automated grading framework",
            "brief_description": "An automated evaluation method that asks a language model to compare a model-generated answer to a gold/reference answer and output a discrete label (2=correct, 1=incomplete, 0=incorrect), used to scale grading of 330 RAG answers over corporate climate disclosures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Question answering over climate-finance documents (retrieval-augmented generation outputs).",
            "llm_judge_model": "Claude 3.5 Sonnet (primary); also Llama3.1 8B Instruct variants were tested as graders.",
            "llm_judge_setup": "Given the question, the RAG-generated answer and the gold/reference answer, the LLM was prompted to output only a single numeric label (2=correct, 1=incomplete, 0=incorrect). Prompt variants were tested: (a) Llama3.1 8B without question reminder, (b) Llama3.1 8B with question reminder, (c) Claude 3.5 Sonnet with question reminder; the best-performing setting (Claude with question included) was used for main experiments. See Appendix A and prompt design description.",
            "human_evaluation_setup": "Human annotators labeled a sample of 330 RAG answers using the same 3-level scale (correct / incomplete / incorrect). Annotators were ESG analysts; gold answers were produced by ESG experts with a two-step review and final QC. Exactness on numeric values and supporting textual evidence were key criteria.",
            "agreement_metric": "Reported metrics include soft-match (counts where {correct,incomplete} vs. human considered agreement) and hard-match (only {correct}). Table 2: Llama3.1 8B (no question) soft 82.4% / hard 50.0%; Llama3.1 8B (with question) soft 84.4% / hard 54.2%; Claude 3.5 (with question) soft 83.9% / hard 68.7%. Distribution shift (Table 3): human labels = {Correct 52.7%, Incomplete 19.1%, Incorrect 28.2%}; Claude = {Correct 41.8%, Incomplete 20.0%, Incorrect 38.2%}. Type I (false accept) = 18 (5.45%), Type II (false reject) = 54 (16.36%) (see Appendix A/A.5 and Table 4).",
            "losses_identified": "Using an LLM as judge yields a conservative bias: fewer answers are labeled correct and more are labeled incorrect compared to humans (≈9.9 percentage points fewer 'correct', ≈10.0 pp more 'incorrect'); this produces a higher Type II (false-reject) rate (~16%), i.e., genuinely correct answers are sometimes downgraded. The automated grader can therefore (a) under-credit partially correct or stylistically different-but-correct responses, (b) be stricter on exactness of numeric values/units, and (c) introduce distributional differences relative to human judgments that make reported accuracies slightly conservative.",
            "examples_of_loss": "Concrete figures from the paper: Table 3 shows humans labeled 174/330 (52.7%) answers as correct while Claude labeled 138/330 (41.8%) as correct; incorrect labels rose from 93 (28.2%) under human grading to 126 (38.2%) under Claude. Appendix A reports Type I errors = 18 (5.45%) and Type II errors = 54 (16.36%), demonstrating that the grader underrates correct answers roughly three times more often than it overcredits wrong ones. The authors note that some genuinely correct answers are penalized, making model accuracies slightly conservative when graded automatically.",
            "counterexamples_or_caveats": "Despite the conservative bias, the LLM grader (Claude 3.5 with question) achieved high alignment: soft agreement ≈83.9% and the highest hard-match observed (68.7%), i.e., it matched human 'correct' labels on more than two-thirds of cases. The paper frames this conservatism as desirable for a benchmark (to avoid inflating scores through false positives). Prompt design matters: including the question in the grader prompt improved alignment. Also, LLM-as-a-Judge enabled scalable, reproducible grading and large reductions in manual overhead, and the authors accept the conservative trade-off for fully automated scaling.",
            "paper_reference": "Results: 'Manual Evaluation and LLM-as-a-Judge' (main text); Appendix A (A.1–A.5), Table 2, Table 3, Table 4, and A.5 (Type I/II error discussion).",
            "uuid": "e9733.0",
            "source_info": {
                "paper_title": "Climate Finance Bench",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Claude 3.5 Sonnet (as grader)",
            "name_full": "Anthropic Claude 3.5 Sonnet (used as LLM-as-a-Judge)",
            "brief_description": "A commercial instruction-tuned LLM that the authors used as the primary automated grader; when prompted with question + generated answer + gold answer it produced discrete labels and showed the highest hard-match alignment with human graders in this study.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Evaluation / grading of question-answering outputs in a climate-finance RAG benchmark.",
            "llm_judge_model": "Claude 3.5 Sonnet (with question included in prompt).",
            "llm_judge_setup": "For grading, the prompt included the question, the RAG answer and the gold answer; the model was instructed to output only a single numeric label (2/1/0). The Claude grader's configuration (with question included) delivered the best alignment and was used for the main automated evaluations. See Appendix A prompt variants and main Results.",
            "human_evaluation_setup": "See entry for LLM-as-a-Judge: human annotation on 330 answers by ESG analysts with two-step expert review and QC; labels were correct / incomplete / incorrect with emphasis on numeric exactness and textual evidence.",
            "agreement_metric": "Claude (with question) soft-match 83.9% and hard-match 68.7% against human labels (Table 2). Label distribution comparison (Table 3) and confusion matrix (Table 4) underpin Type I/II calculations (Type I = 18 / 330 = 5.45%; Type II = 54 / 330 = 16.36%).",
            "losses_identified": "Claude's grader is noticeably stricter than humans: it assigns roughly 9.9 percentage points fewer 'correct' labels and 10.0 percentage points more 'incorrect' labels; it therefore increases false rejections of correct answers and can penalize answers that are semantically correct but differ in wording, units, or presentation.",
            "examples_of_loss": "From Table 3: human correct=174 vs Claude correct=138. From Type I/II reporting: Type II (false reject) = 54 cases where Claude judged answers incorrect or incomplete while humans judged them correct. The paper states 'The grader is conservative: it underrates correct answers three times more than is overcredits wrong ones.'",
            "counterexamples_or_caveats": "Claude nevertheless achieved the best hard-match (68.7%) and maintained soft-match parity with Llama3.1 variants (~84%), suggesting strong alignment in the majority of cases. Authors argue the conservatism is acceptable for benchmarking to avoid inflated scores. Including the question in the prompt improved Claude's performance as a grader.",
            "paper_reference": "Main Results 'Manual Evaluation and LLM-as-a-Judge'; Appendix A (Tables 2, 3, 4; A.4 Distribution Shift; A.5 Type I and Type II Risk).",
            "uuid": "e9733.1",
            "source_info": {
                "paper_title": "Climate Finance Bench",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Llama3.1 8B Instruct (as grader)",
            "name_full": "Meta Llama-3.1 8B Instruct (used/tested as an automated grader)",
            "brief_description": "An open-weight instruction-tuned model tested as an automated grader in two prompt variants: without a question reminder and with a question reminder; it achieved good soft agreement but lower hard-match vs. Claude 3.5.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Evaluation / grading of question-answering outputs in a climate-finance RAG benchmark.",
            "llm_judge_model": "Llama3.1 8B Instruct (two grader prompt variants: without question reminder; with question reminder).",
            "llm_judge_setup": "Grader prompt variants: (1) Llama3.1 8B without question reminder, (2) Llama3.1 8B with question reminder. In both cases the model received the RAG output and gold answer and was asked to return only 0/1/2. Performance compared to human judgments was measured (Table 2).",
            "human_evaluation_setup": "See LLM-as-a-Judge entry: 330 answers labeled by human annotators (ESG analysts), two-step expert review and QC.",
            "agreement_metric": "Llama3.1 8B (no question) soft-match 82.4% / hard-match 50.0%; Llama3.1 8B (with question) soft-match 84.4% / hard-match 54.2% (Table 2).",
            "losses_identified": "Compared to human grading, Llama3.1 graders showed lower hard-match rates (i.e., fewer exact 'correct' matches) and thus are less reliable to fully replace human judgments; prompt context (including question) matters to improve alignment.",
            "examples_of_loss": "Hard-match for Llama variants was 50.0% (no question) and 54.2% (with question), substantially below Claude's 68.7%, indicating more frequent disagreements where the model fails to mark answers as strictly 'correct' even when humans do.",
            "counterexamples_or_caveats": "Soft-match agreement was still ~82–84%, showing reasonable overall parity when 'correct' and 'incomplete' are treated jointly. Including the question in the grader prompt improved agreement, indicating prompt engineering can reduce some losses.",
            "paper_reference": "Appendix A (A.2 Prompt Design, Table 2) and main Results 'Manual Evaluation and LLM-as-a-Judge'.",
            "uuid": "e9733.2",
            "source_info": {
                "paper_title": "Climate Finance Bench",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FINANCEBENCH: A new benchmark for financial question answering",
            "rating": 2,
            "sanitized_title": "financebench_a_new_benchmark_for_financial_question_answering"
        },
        {
            "paper_title": "Searching for best practices in retrieval-augmented generation",
            "rating": 1,
            "sanitized_title": "searching_for_best_practices_in_retrievalaugmented_generation"
        },
        {
            "paper_title": "Climate-FEVER: A Dataset for Verification of Real-World Climate Claims",
            "rating": 1,
            "sanitized_title": "climatefever_a_dataset_for_verification_of_realworld_climate_claims"
        },
        {
            "paper_title": "Survey of Hallucination in Natural Language Generation",
            "rating": 1,
            "sanitized_title": "survey_of_hallucination_in_natural_language_generation"
        }
    ],
    "cost": 0.012156249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Climate Finance Bench
28 May 2025</p>
<p>Rafik Mankour 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Yassine Chafai 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Hamada Saleh 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Ghassen Ben Hassine 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Thibaud Barreau 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Peter Tankov 
Institut Louis Bachelier † CREST
Institut Polytechnique de Paris and Institut Louis Bachelier
ENSAE</p>
<p>Climate Finance Bench
28 May 20258BED00D4C4959D231F74E9200D50D3AAarXiv:2505.22752v1[cs.CL]climate financeESG disclosureretrieval-augmented generationlarge language modelsquestion answeringbenchmark datasetsustainability reportingcarbon footprintquantizationinformation retrieval
Climate FinanceBench introduces an open benchmark that targets question-answering over corporate climate disclosures using Large Language Models.We curate 33 recent sustainability reports in English drawn from companies across all 11 GICS sectors and annotate 330 expert-validated question-answer pairs that span pure extraction, numerical reasoning, and logical reasoning.Building on this dataset, we propose a comparison of RAG (retrieval-augmented generation) approaches.We show that the retriever's ability to locate passages that actually contain the answer is the chief performance bottleneck.We further argue for transparent carbon reporting in AI-for-climate applications, highlighting advantages of techniques such as Weight Quantization.</p>
<p>Introduction</p>
<p>Climate finance aims to measure and control climate-related financial risks, to redirect financial flows towards the green sectors of the economy, and to provide incentives for the brown sectors to reduce their carbon footprint.All this requires reliable measurement of corporate climate risk exposures and environmental impacts.Yet regulators and investors frequently cite a climate-data gap: key indicators such as Scope 1-3 emissions, decarbonization</p>
<p>• We establish a benchmark dataset comprising 33 climate disclosures, 330 analyst-curated questions and expert-validated reference answers that span data extraction, numerical reasoning and logical inference.</p>
<p>• We compare multiple RAG configurations: minimal dense retrieval, hybrid dense + BM25 and reranking, across several LLM back-ends, quantization levels and prompt strategies.</p>
<p>• We provide an open, reproducible test-bed that reports both answer accuracy and the carbon footprint of each configuration, so that future research can optimize statistical as well as environmental efficiency.</p>
<p>We assemble a representative sample of reports across all 11 GICS sectors.By emphasizing factual correctness, including exact numbers and units, and by measuring retrieval coverage, we aim to help both researchers and practitioners develop trustworthy, resource-efficient question-answering systems for the climate-finance domain.</p>
<p>Literature Review</p>
<p>The rapid commercial rollout of large language models has brought an equally rapid migration toward retrieval-augmented generation.In less than two years, RAG has moved from a research curiosity to the backbone of production systems such as Bing Chat, Perplexity.ai and the many domain-specific applications built with LangChain or LlamaIndex.Because it grounds outputs in verifiable documents and avoids expensive model fine-tuning, RAG is now the de-facto recipe for deploying LLMs in high-stakes settings ranging from medical question answering to legal contract analysis [8,9].The climate-finance domain is no exception: analysts need fact-checked answers, regulators demand provenance, and datasets change too frequently for parametric retraining alone.Against this backdrop, we summarize below the core architectural choices in modern RAG pipelines and how they inform our benchmark design.</p>
<p>Retrieval-Augmented Generation (RAG).Early RAG systems [10] combine a dense bi-encoder retriever with a seq2seq generator, injecting the top-k retrieved passages into the generation context so that answers remain grounded in source documents.Subsequent work shows that feeding dozens of passages and letting the decoder fuse evidence (the FiD architecture) yields large gains on knowledge-intensive tasks [11].Best-practice surveys now recommend a hybrid pipeline (dense embeddings + BM25) followed by a cross-encoder reranker for precision, plus prompt instructions that discourage hallucination [15].In short:</p>
<p>• dense retrieval captures semantic paraphrases,</p>
<p>• sparse lexical search excels at exact figures and units, and</p>
<p>• reranking re-scores candidates jointly with the query to identify the truly relevant ones.</p>
<p>The generator is then tasked to quote or cite its supporting text, a practice that empirically reduces unsupported claims and eases human verification.</p>
<p>Open-source efforts such as FinGPT [12] and ESG-BERT [13] also show promise in adapting general-purpose LLMs to domain-specific tasks like ESG disclosure classification and financial forecasting.We believe that climate-finance QA tasks would benefit from similar fine-tuning initiatives.</p>
<p>The FinanceBench paper [14] introduced a finance-specific benchmark to test LLMs in a question-answering setting.Their dataset covered a selection of 10 000 questions across 40 publicly traded US companies from various sectors, drawn from real financial documents such as 10-K, 10-Q and 8-K filings, and they did a human evaluation over a subset of 150 evaluation questions.Although FinanceBench aimed to provide a broad finance-oriented benchmark, its reported performance evaluations were done primarily via human annotations, with a relatively permissive notion of "correctness" (e.g., minor deviations in units were not treated as fully incorrect) to ensure a good-faith understanding of the capabilities of the models.</p>
<p>Moreover, FinanceBench explored several LLM configurations, including "open book," "closed book," "retrieval," and "long context."Among these, only the "retrieval" setting fully aligns with RAG-based question answering.Inspired by the FinanceBench methodology, we focus here on climate-related disclosures, adopting and extending the retrieval pipeline to better handle complex disclosures such as tables and figures.We also incorporate insights from best-practice studies on retrieval-augmented generation [15] to refine our approach.</p>
<p>In addition to FinanceBench [14], several other benchmarks focus on numerical and reasoning-based question answering from financial documents.FinQA [17], ConvFinQA [18] and TAT-QA [19] emphasize numerical reasoning and hybrid table-text data.These datasets are relevant for evaluating climate-finance models, especially those requiring computation over reported KPIs.</p>
<p>Within the ESG and climate disclosure space, Climate-FEVER [20] and ClimRetrieve [16] provide valuable baselines for factual verification and information retrieval respectively.While Climate-FEVER focuses on verifying real-world claims, ClimRetrieve offers climate-specific documents and retrieval annotations.</p>
<p>More recently, the Golden Touchstone benchmark [21] provided a bilingual and comprehensive evaluation framework for finance-specific LLMs and BloombergGPT [25] demonstrated how domain pretraining boosts performance in financial QA.</p>
<p>Limitations of current methodologies and proposed improvements</p>
<p>Dataset scope.Current methodologies evaluate sophisticated reasoning but often provide pre-extracted snippets for each question, leaving the retrieval challenge unsolved.Fulldocument retrieval often covers only U.S. SEC filings or a limited set of sustainability reports.</p>
<p>Retrieval transparency.Few benchmarks disclose the exact retrieval pipeline used in baselines; hybrid search, cross-encoder reranking and document preprocessing are rarely benchmarked side-by-side, hindering reproducibility and progress measurement.</p>
<p>Evaluation methodology.Some benchmarks label an answer as correct only when the exact text string matches the reference answer.The challenge is to avoid letting hallucinations slip through without penalizing semantically correct answers written with different wording, units or abbreviations.A fairer scheme should (i) award partial credit to answers that cover only part of the required information while taking into account their incompleteness and (ii) scale to large datasets.We therefore adopt automated grading with an LLM-as-a-Judge, while keeping a human-in-the-loop design.</p>
<p>Our contribution.Climate Finance Bench fills these gaps by: 1. using complete ESG and climate reports (33 documents across all 11 GICS sectors), thereby testing end-to-end retrieval on long, heterogeneous PDFs;</p>
<ol>
<li>
<p>releasing a reference hybrid + reranking pipeline so that future work has a documented, strong baseline to iterate on;</p>
</li>
<li>
<p>enforcing a non-binary, 3-point grading scheme based on LLM-as-a-Judge;</p>
</li>
<li>
<p>establishing estimates for the carbon emissions associated to the tools we experiment on.</p>
</li>
</ol>
<p>By unifying retrieval, reasoning and environmental accountability, our benchmark aims for trustworthy, resource-efficient question answering in the climate-finance domain.</p>
<p>3 Methods</p>
<p>Data Collection and Curation</p>
<p>3.1.1Selection of Reports.</p>
<p>We gathered 33 climate reports from large publicly traded companies spanning multiple regions (e.g., CAC40 and DAX40 in continental Europe, FTSE in the UK, S&amp;P500 in the US) and covering all 11 GICS sectors.We ensured:</p>
<p>• At least one company from Communication Services, Real Estate and Health Care sectors.</p>
<p>• At least two companies from each of the 8 remaining sectors, preferably from different sub-sectors.</p>
<p>• Exactly one recent climate or sustainability report per company, capturing the latest relevant fiscal year.</p>
<p>The full list of selected companies is available in Appendix C.</p>
<p>Question Formulation and Annotation.</p>
<p>We relied on ESG experts to provide 10 questions per report.The questions reflect two modalities (metric-related and domain-related) and three categories:</p>
<ol>
<li>Pure Extraction: directly retrieve facts.</li>
</ol>
<p>Example: Has the company identified significant decarbonization levers?If yes, detail them.Seven analysts carried out the following steps:</p>
<p>Numerical</p>
<p>• Carefully read the assigned climate report(s).</p>
<p>• Provide written, reference answers (the "Gold Standard") for each of the 10 questions, along with document excerpts, page numbers and an indication of whether the relevant information was found in text, a table, or a figure.</p>
<p>• Follow a unified annotation guide to ensure consistent handling of numerical values, units and references.An adapted version of this guide is available in Appendix D for reference.</p>
<p>Two ESG domain experts resolved ambiguous cases and a final quality-control check was performed to confirm the validity of all annotations.Ultimately, we obtained 330 questionanswer pairs.</p>
<p>Resulting Dataset Structure.</p>
<p>We store our dataset in a table of 330 rows and 13 columns.Each row corresponds to a single question about a specific company's report, containing:</p>
<p>• Company name and Fiscal year.</p>
<p>• Question ID, Question text, Type of question.</p>
<p>• Answer (reference/Gold Standard).</p>
<p>• Documents, Pages, Document extracts, Extract type.</p>
<p>Hardware and Environment</p>
<p>We conducted the experiments primarily on a GPU environment available through Kaggle (Notebooks hosted on GCP).This environment provides:</p>
<p>• 60 GB of storage and 30 GB of RAM.</p>
<p>• Access to a GPU P100 (16 GB memory).</p>
<p>For LLMs with publicly available APIs (Claude Sonnet and GPT-4o), as well as Qwen2.5 and DeepSeek R1 via Nebius' API, calls were invoked directly from the Kaggle notebook.</p>
<p>Data Extraction and Chunking</p>
<p>Our RAG pipeline builds on the foundational architecture introduced by Lewis et al. [10], while incorporating best practices from the FiD architecture [11] and Atlas model [22].These methods highlight the importance of fusing retrieved passages in the decoder and optimizing passage selection for generation.</p>
<p>In designing the retrieval system, we consulted evaluations from the KILT [23] and BEIR [24] benchmarks, which stress the need for robust lexical-dense hybrid retrieval and reproducible metrics across knowledge-intensive tasks.</p>
<p>PDF Extraction.We experimented with two main approaches:</p>
<p>• LangChain loaders that use the Unstructured library, providing a quick way to parse text.</p>
<p>• Docling, which converts PDF files to HTML/Markdown to preserve more structure (e.g., tables, figure captions).</p>
<p>Although Docling can better retain table formatting, it sometimes introduces noise such as HTML tags, which can complicate retrieval.</p>
<p>Chunking Strategy.To avoid having either overly large text chunks or fragments broken mid-table, we used:</p>
<p>• A base chunk size of 2048 tokens, with an overlap of 204 tokens (i.e., 10%).</p>
<p>• Logic to avoid breaking tables and to merge small paragraphs with relevant headings.</p>
<p>Vector Indexing and Retrieval</p>
<p>We vectorized all chunks using sentence-transformers/all-mpnet-base-v2 and stored them in a FAISS index for nearest-neighbor retrieval.We explored two RAG retrieval configurations:</p>
<p>• Minimal: return the top k = 12 chunks based solely on cosine similarity scores with the question embedding.</p>
<p>• Best Practices: a "hybrid" approach combining:</p>
<ol>
<li>
<p>Semantic and lexical retrieval: weighted combination of top results from a dense (semantic) retriever (75%) and BM25 (25%).</p>
</li>
<li>
<p>Fusion and Reranking: we first fuse the top 20 chunks using Reciprocal Rank Fusion, select 8 best chunks, then apply a cross-encoder reranker to the next 12 to pick 4 additional relevant chunks, for a total of 12.</p>
</li>
</ol>
<p>LLMs for Generation</p>
<p>We tested five LLMs under both minimal and hybrid retrieval:
• Llama3.1 8B Instruct 1 and Llama3.1 8B Instruct quantized in 4-bit.
• Mistral Nemo Instruct 12B 2 quantized in 4-bit.</p>
<p>• Claude Sonnet 3.5 2024-06-20 3 .</p>
<p>• GPT-4o 4 .</p>
<p>Two more LLMs were tested under hybrid retrieval only:</p>
<p>• Qwen2.5-72B 5 .</p>
<p>• DeepSeek R1 6 .</p>
<p>We set the temperature to 0.2 (for more deterministic answers) and limited the maximum output to 512 tokens to avoid overly long responses.All queries shared the same two-part prompt shown below, with a system prompt instructing the LLM to answer strictly based on the retrieved chunks, in order to refrain from hallucinating data.</p>
<p>Curly-brace placeholders ({company}, {context}, {question}) are filled at run-time:</p>
<p>1 https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct 2 https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407 3 https://www.anthropic.com/index/claude-3-family 4 https://openai.com/index/hello-gpt-4o 5https://huggingface.co/Qwen/Qwen2.5-72BTo compare full-precision with compressed models under identical conditions and to fit Mistral Nemo Instruct 12B on the 16GB GPU available in our local environment, we performed post-training, 4-bit weight quantization with the bitsandbytes library (v0.43).Both Llama3.1 8B Instruct and Nemo Instruct 12B were loaded and no additional fine-tuning was applied.We evaluated the quantized checkpoints in inference-only mode, using exactly the same decoding hyper-parameters (temperature 0.2, top-p 0.95, max new tokens=512) as for the baselines.Quantization reduced resident GPU memory from 16GB to 6GB for Llama3.1 8B Instruct and from 24GB to 9GB for Nemo Instruct 12B, enabling local execution.</p>
<p>Results</p>
<p>Manual Evaluation and LLM-as-a-Judge</p>
<p>We first performed a human evaluation of RAG outputs.Human annotators labeled each answer as either correct, incomplete, or incorrect, with exactness in numeric values and appropriate textual evidence being key factors.However, human evaluation is time-consuming and can be subjective.</p>
<p>To reduce manual overhead, we tested an LLM-based grader (LLM-as-a-Judge) that compares each RAG-generated answer to the reference answer, factoring in the context of the original question.We found the highest alignment with human judgments (over 80% agreement) when using Claude with the question prompt included.Thus, for the main experiments, we rely on the LLM-as-a-Judge framework for scalable evaluation.</p>
<p>A full description of the LLM-as-a-Judge evaluation protocol is available in Appendix A.</p>
<p>Comparing RAG Configurations and LLMs</p>
<p>Minimal RAG.</p>
<p>Under the minimal retrieval approach, smaller or quantized LLMs (Llama3 8B Instruct, Llama3 Instruct 4-bit, Nemo Instruct 4-bit) achieved 35-40% correct responses.Larger models (Claude and GPT-4o) performed significantly better, around 50-55% correct.As shown in Figure 1, GPT-4o and Claude 3.5 outperform smaller 4-bit models by roughly 15 percentage points under the minimal setting.</p>
<p>Hybrid RAG (BM25 + Reranking).</p>
<p>Adding BM25 lexical retrieval and reranking yielded substantial gains for larger LLMs.As illustrated in Figure 2, Claude 3.5 remains in the lead at 62%, but the performance gap has narrowed: DeepSeek R1 attains 60%, while Qwen2.5-72Breaches 44%.In other words, switching from Claude 3.5 to DeepSeek R1 incurs an ≈ 2pp drop, even though Claude consumed about five times less output tokens per answer on average.This suggests that retrieval quality, rather than model capacity, seems to be the current bottleneck in our experiments.</p>
<p>Minimal vs. Hybrid.</p>
<p>Hybrid retrieval helps large instruction-tuned models but can hurt smaller or highly-quantized ones.Qwen2.5-72B and DeepSeek R1 were not run under minimal retrieval, so direct deltas are unavailable.Incremental improvements on the generation side alone are unlikely to break the 65% barrier without a better retriever.Figure 3 details these jumps: BM25 adds +4.3pp, reranking another +3.0pp, while an un-filtered HTML conversion costs 4.8pp.</p>
<p>Effect of Docling Conversion.</p>
<p>While Docling potentially preserves the layout of tables and figures, we observed a performance decrease of several percentage points, likely due to extra HTML tokens and parsing noise degrading retrieval's performance.This suggests that more advanced post-processing might be required to capitalize on Docling's structural advantages.For smaller-scale Llama models, we found that 4-bit quantization introduced only minor accuracy differences (within 1-2%), while significantly reducing memory usage and energy consumption.Figure 4 illustrates this comparison for LLaMA-3.1 8B, where the unquantized and quantized versions perform similarly across all three correctness classes.In resource-constrained settings, 4-bit quantization is therefore a compelling strategy to lower computational cost and associated carbon emissions without critically hurting correctness.</p>
<p>Performance by Question Type</p>
<p>Our question set includes extraction, numerical reasoning and logical reasoning categories, each varying in difficulty.Figure 5 reveals patterns that are not obvious from aggregate accuracy alone.</p>
<p>In our best configuration (Claude 3.5 + Hybrid retrieval), numerical reasoning questions had the highest correct-answer rate, when one might expect numerical reasoning to be harder than pure extraction (69.7% vs. 65.7%correct).A qualitative error analysis indicates why: in some of the numerical questions, the relevant arithmetic had already been carried out in the source document, so the task collapses into precise retrieval plus unit normalization.Logical questions expose a retrieval bottleneck as logical-reasoning items require chaining multiple facts scattered across a report.Broad or ambiguous queries (e.g., "Which topics have been assessed to be material?")caused more errors or incomplete answers.</p>
<p>Properly structured queries, with clear numeric or factual targets, were most reliably answered.Future iterations of Climate Finance Bench will therefore log intermediate reasoning traces to diagnose whether errors originate from retrieval omissions or reasoning failures and to build custom processes per question in order to improve the success rate of answers.</p>
<p>GHG Emissions and Environmental Footprint</p>
<p>Measuring emissions from AI usage aligns with growing interest in sustainable ML practices.Patterson et al. [26] provide methodology to estimate the carbon impact of training large neural networks, while Schwartz et al. [27] advocate for "Green AI," urging the community to prioritize energy efficiency in both training and deployment.</p>
<p>In keeping with the climate focus, we also conducted a rough measurement of the greenhouse gas (GHG) emissions attributed to these experiments:</p>
<p>• For Claude and GPT-4o, we estimated an upper bound per query using external providers like EcoLogits, then generalized across all runs.</p>
<p>• For local or Kaggle-based models, we approximated total GPU usage with CodeCarbon logs, dividing by the total number of queries to compute a per-query footprint.</p>
<p>• We could not obtain estimates for Qwen and DeepSeek as they were called through Nebius's API.</p>
<p>Aggregating CPU, memory and GPU energy logs, CodeCarbon estimates for local runs and per-query upper-bound figures from EcoLogits for the proprietary API calls, we estimated the GHG footprint of each model with a confidence interval due to the wide uncertainty bands published for OpenAI's and Anthropic's (Claude) back-end infrastructure.This breakdown illustrates why future work should favor lightweight local models to factor in environmental impacts, whenever accuracy allows, and report vendor-side carbon metrics more transparently if practitioners are to measure carbon impacts with tighter bounds.</p>
<p>A full description of the emissions estimation protocol is available in Appendix B.</p>
<p>Limitations</p>
<p>This first release of Climate Finance Bench covers only 33 sustainability reports, mostly large-capitalisation firms headquartered in Europe or North America and is therefore not representative of emerging-market issuers, small and medium enterprises, or non-English filings.</p>
<p>Despite a two-step review process, Gold Answers for multi-step reasoning items still contain a degree of subjectivity, which may propagate noise into model-versus-human comparisons.</p>
<p>Because climate disclosures are largely self-reported, any inaccuracy or greenwashing in the underlying documents can bias both retrieval and evaluation.</p>
<p>Sector-wide queries based on information spanning multiple companies have not been considered yet.</p>
<p>Finally, the benchmark depends on PDF extraction and English-language processing.Heterogeneous web formats and multilingual reports, which are common in practice, remain outside the current scope and should be addressed in future iterations.</p>
<p>Conclusion and Practical Implications</p>
<p>Our strongest configuration (Claude 3.5 combined with hybrid dense sparse retrieval and cross-encoder reranking) answered 62% of the 330 question benchmark correctly and a further 10% answers were partially incomplete.Put differently, roughly three-quarters of the outputs were at least directionally useful, while one quarter remained factually wrong or unsupported.</p>
<p>What these numbers mean in practice.</p>
<p>• Augmented analyst workflows.At 62 %+10 % accuracy, a RAG pipeline can already serve as first-pass summarization or evidence-surfacing tools.In a typical ESG due-diligence loop, analysts spend the majority of their time locating passages, tables and footnotes.Automating that step can cut reading time even when every answer is manually verified afterwards.</p>
<p>• Human-in-the-loop is still mandatory.A 25-30 % error rate remains too high for regulatory disclosure, portfolio weighting or automated sustainability scoring.Every generated answer therefore needs stronger safeguards against hallucination and a review layer.In practice, showing the retrieved snippets alongside the model's answer enables an experienced analyst to validate or override the response in a few seconds.</p>
<p>• Retrieval dominates further gains.Numerical and logical questions fail largely because the correct passage never reaches the context window.Hence marginal gains from scale (moving to larger proprietary models) are smaller than gains from smarter retrieval or domain-aware chunking.</p>
<p>• Environmental trade-offs.Deploying Claude 3.5 or GPT-4o in production multiplies carbon emissions per query relative to a quantized local models.Organizations that prioritize sustainability can already choose lighter models with human review to keep both error rates and emissions within acceptable bounds.</p>
<p>The benchmark therefore positions current RAG technology not as a replacement for ESG analysts but as a force multiplier that can save time, reduce tediousness and broaden coverage, while leaving final decision-making to human expertise.</p>
<p>Moving Forward</p>
<p>We plan to incorporate additional data sources to broaden coverage of companies, sectors and sustainability metrics.</p>
<p>We also anticipate exploring new retrieval strategies (e.g., domain-specific expansions, robust table extraction) and extended evaluation metrics beyond simple correctness, such as faithfulness to source and the ability to handle multi-document summaries.</p>
<p>A first key direction is the development of adaptive answering methods.By tailoring retrieval prompts and selection strategies to the type of question (e.g., numerical, logical, extractive), we can significantly reduce the semantic gap between query and document content.Typed-RAG, for instance, introduces a type-aware decomposition method that improves answer precision for complex question formats [28].Similarly, task-aware retrieval using instructions improves retrieval relevance and accuracy while reducing the need for model size scaling [29].These approaches allow RAG systems to better align context construction with task intent, ultimately improving factual accuracy and minimizing hallucination.</p>
<p>Another line of enhancement involves incorporating structured knowledge representations into the pipeline.We plan to prototype a GraphRAG variant, which extracts ESG-specific entities, numeric values and relations to construct a knowledge graph used during retrieval.This hybrid approach supports symbolic reasoning, enables multi-hop inference, and improves retrieval faithfulness [30].Recent results show that combining document graphs with entityaware retrieval yields better coverage and more structured answers in focused summarization and QA tasks [31].This is especially relevant for climate disclosures, where facts are often interdependent and scattered across tables, figures and text.</p>
<p>To improve cost-efficiency and reduce emissions, we also aim to implement dynamic model routing.Inspired by cascade models and energy-aware inference pipelines [32,33], this method routes queries to lightweight models for simple lookups and reserves larger models for complex or high-risk questions.In a benchmark context, this means using quantized models or symbolic solvers when appropriate, reducing unnecessary compute load while preserving accuracy.Dynamic routing aligns with our sustainability goals by lowering energy use and latency without sacrificing interpretability or correctness.</p>
<p>Finally, we plan to explore the Agentic RAG paradigm, which allows an orchestration agent to select and sequence retrieval tools at runtime.By combining document search, knowledge graph access, and symbolic modules under agentic control, the system can flexibly plan how to answer a question, rather than relying on a fixed RAG pipeline.The ReAct framework [34] and Toolformer [35] demonstrate that language models equipped with actionselection capabilities can improve factual consistency, reduce hallucinations, and generate more interpretable reasoning traces.In our case, such orchestration can enhance trust in climate-finance QA outputs and ensure that high-stakes queries follow robust, auditable decision paths.</p>
<p>Data accessibility</p>
<p>We have made our dataset and code files available in a dedicated repository: github.com/Pladifes/climatefinance bench All dataset assets (PDF excerpts, questions and gold answers) are distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC-BY-NC-SA 4.0) licence.</p>
<p>The repository includes notebooks so that users can:</p>
<p>• generate vector stores from reports and • run RAG pipelines by choosing from multiple retrieval and LLM options.</p>
<p>Users can experiment with various RAG configurations, test different LLMs and compare results.</p>
<p>Acknowledgements</p>
<p>We would like to thank the analysts of the Institut Louis Bachelier Labs for their availability in building and curating this benchmark: Pauline Aumard, Ana Vallejo, Adel Medjbari, Iker Tardio, Gabriel Levy, and Lina Jouker.Their question design and document-level annotations were essential to the study.</p>
<p>We also thank Yuri Vorontosov (QuePasa) for his valuable recommendations and for providing compute resources that made these experiments possible.</p>
<p>A.4 Distribution Shift</p>
<p>Table 3 contrasts the label distributions produced by human graders and by Claude 3.5.The LLM is noticeably stricter: it assigns 9.9 pp fewer correct labels and 10.0 pp more incorrect labels, while the incomplete share stays nearly unchanged.</p>
<p>A.5 Type I and Type II Risk</p>
<p>We gauge the reliability of the automatic grader using Table 4.We consider type I error (false accept) when the LLM judges an answer correct (label 2) while the human grader says it is incorrect or merely incomplete, and type II error (false reject) when the LLM judges an answer incorrect (label 0) or incomplete (label 1) while the human grader deems it correct.</p>
<p>With a set of 330 questions:</p>
<p>Type I errors = 4 + 14 = 18 (5.45%)Type II errors = 18 + 36 = 54 (16.36%)</p>
<p>The grader is conservative: it underrates correct answers three times more than is overcredits wrong ones.This is desirable in a benchmark where false positives would inflate model scores.</p>
<p>The 16 % Type II rate means some genuinely correct answers are penalized.Reported accuracies for all models are therefore slightly conservative, a trade-off we accept for fully automated scaling.Take-away.Despite a small distribution shift, Claude 3.5 aligns with human judgements on more than two-thirds of hard cases, providing a cost-effective and reproducible grading mechanism for the remainder of our experiments.</p>
<p>B Appendix B GHG Emissions Computation Methodology</p>
<p>What is at stake?Large-scale language models already contribute an electricity demand on par with that of some small nations, and their footprint is growing faster than most mitigation pathways allow.If research communities omit credible carbon accounting, three risks arise: (i) Scientific integrity: results that overlook energy cost may promote architectures that are infeasible under tightening carbon budgets; (ii) Legal exposure: institutions that publish "AI-for-climate" tools without disclosing their own emissions may soon contravene emerging disclosure laws; (iii) Capital-allocation bias: investors and policymakers, lacking transparent numbers, could funnel resources toward high-footprint solutions that erode limited global carbon budgets instead of toward lower-impact alternatives.Robust footprint measurement is therefore not an optional add-on but a prerequisite for credible, actionable climate-finance research.</p>
<p>B.1 Why measure the carbon footprint?</p>
<p>Regulatory and fiduciary context.The European Corporate Sustainability Reporting Directive (CSRD) and disclosure frameworks such as TCFD 7 and IFRS S28 increasingly require firms and not only heavy industry to quantify Scope 2 (electricity) and Scope 3 (up/down-stream) greenhouse-gas (GHG) emissions.Research groups that propose AI tools for climate finance therefore face a dual responsibility: (1) to demonstrate that the use-phase of their models does not undermine the very decarbonisation goals they seek to advance, and (2) to provide transparent numbers that downstream users can incorporate into their own value-chain accounting.</p>
<p>Where do the emissions come from in a RAG pipeline?</p>
<p>• Model inference.Each forward pass through a large-parameter LLM drives a GPU at dozens to hundreds of watts.Although training is more energy-intensive per hour, repeated inference requests dominate in a production search-and-answer workload.API calls to proprietary back-ends move this energy use off-premises but do not eliminate it.</p>
<p>• Retrieval and indexing.Dense vectorisation of long PDF reports and nearestneighbour search in FAISS are CPU and memory-heavy and can also leverage GPU if OCR is involved.While it is a one-off cost in our benchmark, a live system that ingests frequent ESG filings must re-index regularly, making this a non-negligible share of total energy.</p>
<p>• Evaluation.Using an LLM-as-a-Judge multiplies the number of model invocations, adding its own emissions line item.</p>
<p>• Data movement and cooling.Every gigabyte shuttled between object storage, RAM and GPU DRAM and every kilowatt-hour dissipated as heat requires additional electricity that is rarely met with 100 % renewables.</p>
<p>Why are the resulting emissions global?Computation is executed in hyperscale datacentres scattered across the United States, Europe and Asia, each connected to a distinct electricity grid with its own carbon intensity.A single Kaggle session may run in Iowa (coal-heavy mix) today and in Belgium (higher share of wind) tomorrow; similarly, an OpenAI or Anthropic request may land on hardware in Oregon, Virginia or Dublin.Consequently, the same Python script yields different real-world emissions depending on where the scheduler places the job and on hourly fluctuations in energy mix.Embodied carbon in GPUs, network switches and cooling infrastructure further spreads the climate impact across manufacturing hubs in Taiwan, South Korea and the wider semiconductor supply chain.Quantifying the footprint therefore requires both local energy logs and global carbon-intensity factors, as reflected in the methodology that follows.</p>
<p>B.2 Study overview</p>
<p>We distinguish two broad sources of energy consumption in our study:</p>
<ol>
<li>
<p>Local inference runs (vector-store creation, RAG pipelines and LLM generation on Kaggle GPUs/CPUs).</p>
</li>
<li>
<p>Remote API calls to proprietary LLM providers (OpenAI's GPT-4o and Anthropic's Claude 3.5), including the LLM-as-a-Judge evaluation phase.</p>
</li>
</ol>
<p>B.3 Remote API calls</p>
<p>For each prompt sent to GPT-4o or Claude we queried the EcoLogits9 service, which returns a minimum and maximum estimate of GHG emissions (in kg CO 2 eq) given the token counts and the limited public information on vendor infrastructure.Because the underlying hardware and regional electricity mixes are opaque, we adopt the maximum value as a conservative upper bound, compute the sample mean ēAPI and standard deviation σ API across a calibration batch and then scale by the total number N API of queries:</p>
<p>E API = N API ēAPI with uncertainty ± N API σ API .</p>
<p>B.4 Local runs (Kaggle)</p>
<p>Instrumentation.We wrapped every Kaggle notebook in a CodeCarbon10 (v 2.8) context manager to log:</p>
<p>• CPU and RAM energy draw on the host machine;</p>
<p>• GPU run-time in seconds, captured via nvidia-smi.</p>
<p>Converting energy to emissions.Let E CPU+RAM and t GPU be the cumulative energy (kWh) and GPU run-time (h) for a given run.For each physical host we resolve its ISO country code via CodeCarbon, look up the country-level carbon-intensity CI (kg CO 2 eq/kWh) from the Climate Change Performance Index database 11 and compute
E local = CI (E CPU+RAM + t GPU × 0.250),
because the NVIDIA P100 on Kaggle is rated at 250.00 W TDP.</p>
<p>Example.For a typical 330-question batch on an unquantized Llama3.1 8B model with a server located in the USA: 11 https://ccpi.org2. Company's name: exactly the folder name that contains that company's reports.
E CPU+RAM ≈ 0.</p>
<p>Fiscal year</p>
<p>• Use the fiscal year covered, not the publication year.</p>
<p>• If the question spans several years, list them: 2020, 2021.</p>
<p>• Check that the year matches the report contents.</p>
<ol>
<li>Question ID: one ID for each of the ten questions (e.g.Q1).</li>
</ol>
<p>Question</p>
<p>• Copy exactly from the master question list.</p>
<p>• Replace FYXXXX with the concrete year, e.g.FY2020.</p>
<ol>
<li>Type of question: PE (Pure Extraction), NR (Numerical Reasoning) or LR (Logical Reasoning).</li>
</ol>
<p>Answer</p>
<p>• Write a concise English answer once found.</p>
<p>• If the information is not in the report, write exactly:</p>
<p>Not available in the retrieved information.</p>
<p>• You may ask internal or external chatbots, but validate the response against the PDF.</p>
<p>Documents</p>
<p>• Use the exact PDF filename(s).</p>
<p>• Multiple documents: list in chronological order, separated by commas (e.g.</p>
<p>doc1.pdf, doc2.pdf).</p>
<p>Pages</p>
<p>• Give the PDF page numbers, not the printed page labels.</p>
<p>• One document: doc1{page17, page26}.Multiple documents: doc1{page9}, doc2{page1, page27}.</p>
<p>• doc1 = first file named in the Documents column, doc2 = second file named, etc.</p>
<p>Annotation Examples</p>
<p>Examples are provided in the workbook:</p>
<p>6 https://arxiv.org/abs/2501.12948-----System prompt (fixed) -----You are a documentary assistant.Answer the question about the mentioned company based on the provided context that was extracted from climate or sustainability reports.Do not add any additional notes.If the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.Here are three examples of the format to follow in your reply: ### Human: Does the company have a climate change mitigation objective for FY2023?AI: Yes, the company aims to become net zero by 2030 on its Scope 1, 2 and 3 emissions.Human: Does the company have a climate change mitigation objective for FY2023?AI: No, the company clarifies its intention not to pursue a net-zero target.Human: Does the company disclose a Transition Plan for FY2023?If yes, highlight its main characteristics.AI: Not available in the retrieved information.### -----User prompt (filled per query) -----Here are excerpts from documents about the company {company}: ### {context} ### Here's the question asked by the user: Question: &lt;&lt;&lt; {question} &gt;&gt;&gt;</p>
<p>Figure 1 :
1
Figure 1: Accuracy breakdown (correct, incomplete, incorrect) for the Minimal RAG configuration across five LLMs.</p>
<p>Figure 2 :
2
Figure 2: Accuracy breakdown (correct, incomplete, incorrect) for the Hybrid RAG configuration across the seven LLMs tested.</p>
<p>Figure 3 :
3
Figure 3: Stepwise impact of successive retrieval upgrades on answer quality (Minimal RAG → + BM25 lexical → + reranking → + HTML conversion).Adding BM25 improves the correct-answer rate from 54.8% to 59.1%, and the hybrid dense-sparse &amp; reranking scheme lifts it further to 62.1%.Introducing Docling's HTML conversion without extra post-processing brings the score down to 57.3%, indicating that raw structural noise can offset earlier gains.Bars show absolute counts (annotated) and the associated share of the 330-question test set.</p>
<p>Figure 4 :
4
Figure 4: Comparison of LLaMA 3.1-8B Unquantized and 4-bit Quantized under the minimal RAG setting.Quantization leads to negligible accuracy loss while significantly reducing resource usage.</p>
<p>Figure 5 :
5
Figure 5: Break-down of answer quality for each question category under the best-performing setup (Claude 3.5 + hybrid retrieval).Numerical reasoning edges out pure extraction, while logical reasoning lags behind because it demands multi-hop synthesis across passages.</p>
<p>Reasoning: extract figures and/or perform calculations.
3. Logical Reasoning: combine multiple data points to infer an answer.Example: Does the company have a decarbonization trajectory compatible with a 1.5°Cor 2 °C scenario?Example: What is the company's carbon intensity (in tCO 2 / million USD) for FY 2023?If not reported, compute it by dividing total carbon emissions by that year's revenue.</p>
<p>Table 1
1
provides emissions per question answered for each model.
Model / configurationEmissions (g CO 2 eq / query) Uncertainty (g CO 2 eq / query)LLaMA 3.1-8B (full precision)2.79-LLaMA 3.1-8B (4-bit)0.70-Mistral NeMo-12B (4-bit)1.15-GPT-4o (API)7.18± 4.00Claude 3.5 Sonnet (API)8.15± 4.52Vector-store build *0.30n/a</p>
<p>Table 1 :
1
Average GHG emissions per query.The asterisk marks a one-off indexing cost if diluted over 330 queries, shown for scale.Two points emerge: (i) API calls dominate: GPT-4o or Claude generate higher emissions than smaller models, disproportionately to the improvement in performance; (ii) Quantitation pays off : the 4-bit quantization of Llama3.1 8B reduced emissions by a factor of ≈ 4, yielding an emissions saving of roughly 75% with negligible accuracy loss.Retrieval and indexing remain almost negligible by comparison.</p>
<p>Table 2 :
2
Human-LLM agreement on the 330-answer test set
Grader setupSoft matchHard matchLlama3.1 8B Instruct (no question)272/330 = 82.4% 165/330 = 50.0%Llama3.1 8B Instruct (with question) 280/330 = 84.4% 179/330 = 54.2%Claude 3.5 Sonnet (with question)277/330 = 83.9% 227/330 = 68.7%Claude 3.5 achieved the highest hard-match rate (68.7%) while maintaining soft-matchparity with Llama3.1 8B Instruct, and is therefore used for all subsequent automatic evalua-tions.</p>
<p>Table 3 :
3
Human vs. Claude 3.5 label distribution on the 330-answer test set
CorrectIncomplete IncorrectHuman evaluation 174 (52.7%) 63 (19.1%)93 (28.2%)LLM-as-a-Judge138 (41.8%) 66 (20.0%) 126 (38.2%)</p>
<p>Table 4 :
4
Confusion matrix (Claude 3.5 vs. human labels).
LLM labelHuman label Incorrect (0) Incomplete (1) Correct (2)Incorrect (0)8364Incomplete (1)252414Correct (2)1836120</p>
<p>27 kWh, t GPU ≈ 4.70 h, CI USA = 0.35 kg CO 2 eq / kWh, ⇒ E local ≈ (0.27 + 4.7 × 0.250) (kWh) × 0.349 (kg CO 2 eq / kW h) ≈ 0.50 kg CO 2 eq.</p>
<p>10 .
10
Document extracts • Copy-paste the full paragraph / table / figure caption.• One extract: doc1{<extract>}.Several extracts: doc1{...}, doc1{...}, doc2{...}.• For a table or figure, copy textual content rather than screenshots.• If the PDF is image-only, screenshot, run OCR (e.g.ChatGPT), and verify the text.11.Extract type: choose from text, table, figure, text+table, text+figure 12. Comments: pick one of these options</p>
<p>• Nothing to report (default) • Uncertain • Additional comments 13.Additional comments: fill only if the previous field is Additional comments.Write a short, clear remark.14.Validity status of the annotation: ESG expert only.Options: Validated by the expert / Modified by the expert.The default entry is To be validated.</p>
<p>•</p>
<p>Text answer → sheet Examples / Example 1 • Table answer → sheet Examples / Example 2 • Figure answer → sheet Examples / Example 3
5. Quality-Control Procedure
A dedicated ESG expert reviews all annotations, with special attention to rows whose Comments field is Uncertain or Additional comments.The expert then sets the Validity status of the annotation to</p>
<p>Task Force on Climate-related Financial Disclosures.
International Financial Reporting Standards S2.
https://ecologits.ai/latest/
https://codecarbon.io/
FundingThis work was financed by Agence Nationale de Recherche via the Pladifes project (ANR-21-ESRE-0036).A Appendix A Automated Grading with a LLM-as-a-Judge A.1 Human vs. Automatic Evaluation Human grading.For a sample of 330 RAG answers, each one was labeled by an annotator on a three-level scale:• Correct: matches the gold answer within a narrow tolerance for wording;• Incomplete: on the right track but missing key details;• Incorrect: factually wrong or off-topic.LLM-as-a-Judge.To scale evaluation we asked a LLM to grade the same answers.Given the question, the RAG answer and the gold answer, the LLM must output only a number corresponding to the labels: 2 (correct), 1 (incomplete), 0 (incorrect), with no commentary.A.2 Prompt DesignWe tested three variants:1. Llama3.1 8B Instruct, without a reminder of the question; 2. Llama3.1 8B Instruct, with a reminder of the question; 3. Claude 3.5 Sonnet, with a reminder of the question.A.3 Agreement with Human JudgementsA soft match counts {correct, incomplete} as agreement, whereas a hard match counts only {correct}.B.5 Emissions summary.Vector-store generation 0.10 n/a † GPU energy for NeMo was approximated using LLaMA 3.1 power draw; the true value may differ.Table5summarises the per-run carbon footprint, including evaluation.Three patterns stand out:1. API calls dominate.Remote inference on GPT-4o and Claude accounts for ∼5 kg CO 2 eq-roughly 77.00 % of the subtotal for a single RAG configuration.It is expected because these models are substantialy bigger in size compared to the models we ran locally and each prompt triggers an opaque multi-GPU back-end and must include large uncertainty margins.2. Quantization pays off.Compressing LLaMA 3.1 from full precision to 4-bit slashes emissions from 0.92 to 0.23 kg CO 2 eq (≈ 75% savings) while retaining answer quality within two percentage points.Similar gains are expected for NeMo, although its figure is an approximation.3. Vector stores are not the main emission point.End-to-end indexing of all sustainability reports emitted just 0.10 kg CO 2 eq, two orders of magnitude below the API footprint, showing that retrieval costs are negligible compared with repeated LLM inference.Doubling the experiment to test two RAG pipelines raises the subtotal from 6.60 to 13.19 kg CO 2 eq, and the overall benchmark footprint (including index generation) to 13.29 ± 5.62 kg CO 2 eq.The error band is driven almost entirely by the vendor-side uncertainty on GPT-4o and Claude.Further transparency from providers about region-level energy accounting would narrow these bounds and help researchers optimize low-carbon deployments.B.6 Limitations• Lack of fine-grained telemetry for GPT-4o/Claude forces us to rely on coarse upper bounds.• Kaggle does not guarantee the same data-centre region across sessions, so we approximate with the reported country code for each run.• Carbon-intensity figures rely on national averages; data-centre-specific power-purchase agreements (PPAs) could yield substantially lower actual emissions.• Provider-side uncertainty accounts only for hardware variation, not for idle overhead, networking, or cooling system coefficient of performance (COP).We build a database that links 33 corporate climate reports to• ten analyst-style questions per report,• the reference answers, and• the evidence passages used to derive those answers, so that retrieval-augmented generation (RAG) systems can be evaluated on the same tasks.Rigorous annotation is critical for trustworthy results.This guide sets out the tools, rules, and examples you must follow.All fields in the database must be written in English.If in doubt, contact your referents before proceeding.Reports to AnnotateAll reports are supplied as PDF files in the shared drive:Company Reports/Verify that every document you use is a climate or sustainability report.Filling in the "Annotation Table" SheetEach annotator completes all columns for their assigned rows, except Validity status of the annotation, which is for the ESG expert only.Column-by-column instructionsPractical Tips• You may upload the PDF to our internal or external chatbots, ask the question, and then verify the answer in the report.• If the report is an image (no selectable text), take a screenshot, run OCR, and check the transcription carefully.
Bridging Data Gaps: Data Availability and Needs for Addressing Climate-Related Financial Risks. NGFS Technical Document. 2022Network for Greening the Financial System</p>
<p>2023 Status Report: Task Force on Climate-related Financial Disclosures. Financial Stability, Board , 2023FSB</p>
<p>Project Gaia -Enabling climate risk analysis using generative AI. March 2024Bank for International Settlements</p>
<p>Informing climate risk analysis using textual information -A research agenda. Deutsche Bundesbank, 2024-012024Technical Report</p>
<p>Language Models are Few-Shot Learners. T B Brown, NeurIPS. 332020</p>
<p>Openai, arXiv:2303.08774GPT-4 Technical Report. 2023arXiv preprint</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, ACM Computing Surveys. 55122023</p>
<p>Augmented Language Models: a Survey. G Mialon, arXiv:2302.078422023arXiv preprint</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Y Guo, arXiv:2305.096752023arXiv preprint</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. P Lewis, NeurIPS. 332020</p>
<p>Leveraging Passage Retrieval with Generative Models for Open-Domain Question Answering. G Izacard, E Grave, ICLR2021</p>
<p>FinGPT: Open-Source Financial Large Language Model. H Yang, arXiv:2306.060312023arXiv preprint</p>
<p>S Mehta, arXiv:2204.11110ESG-BERT: A Pre-trained Model for ESG Corporate Disclosures Classification. 2022arXiv preprint</p>
<p>FINANCEBENCH: A new benchmark for financial question answering. P Islam, arXiv:2311.119442023arXiv preprint</p>
<p>Searching for best practices in retrieval-augmented generation. X Wang, arXiv:2407.012192024arXiv preprint</p>
<p>ClimRetrieve: A benchmarking dataset for information retrieval from corporate climate disclosures. T Schimanski, arXiv:2406.098182024arXiv preprint</p>
<p>FinQA: A Dataset of Numerical Reasoning over Financial Data. Z Chen, EMNLP. 2021</p>
<p>ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. Z Chen, EMNLP. 2022</p>
<p>TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Text Data in Finance. Y Zhu, EMNLP. 2021</p>
<p>Climate-FEVER: A Dataset for Verification of Real-World Climate Claims. T Diggelmann, arXiv:2012.006142020arXiv preprint</p>
<p>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models. X Wu, arXiv:2411.062722024arXiv preprint</p>
<p>Atlas: Few-shot Learning with Retrieval-Augmented Language Models. G Izacard, ICLR. 2022</p>
<p>KILT: A Benchmark for Knowledge Intensive Language Tasks. F Petroni, NAACL. 2021</p>
<p>BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models. N Thakur, CIKM. 2021</p>
<p>BloombergGPT: A Large Language Model for Finance. S Shen, arXiv:2303.175642023arXiv preprint</p>
<p>Carbon Emissions and Large Neural Network Training. D Patterson, Communications of the ACM. 202265</p>
<p>Green AI. R Schwartz, Communications of the ACM. 63122020</p>
<p>Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering. D Lee, arXiv:2503.158792025arXiv preprint</p>
<p>Task-aware Retrieval with Instructions. A Asai, 2023Findings of the Association for Computational Linguistics</p>
<p>Graph Retrieval-Augmented Generation: A Survey. B Peng, arXiv:2408.089212024arXiv preprint</p>
<p>From Local to Global: A GraphRAG Approach to Query-Focused Summarization. D Edge, arXiv:2404.161302024arXiv preprint</p>
<p>Language Model Cascades: Token-Level Uncertainty and Beyond. N Gupta, International Conference on Learning Representations (ICLR). 2024</p>
<p>Investigating Energy Efficiency and Performance Trade-offs in LLM Inference Across Tasks and DVFS Settings. P J Maliakel, arXiv:2501.082192024arXiv preprint</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, arXiv:2302.047612023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-249953673</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2206.11795v1.pdf" target="_blank">Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos</a></p>
                <p><strong>Paper Abstract:</strong> Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1702.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1702.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VPT (text-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Video PreTraining foundation model with closed-caption text conditioning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VPT behavioral-cloning policy (transformer + ResNet backbone) that was pretrained on large-scale web video (pseudo-labelled by an IDM) and then fine-tuned / conditioned with natural-language closed captions to produce a weakly steerable embodied agent in Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VPT foundation model (text-conditioned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>ResNet front-end + residual transformer policy (causal during BC), ~220M parameter variant used for text-conditioning experiments; originally trained by behavioral cloning on IDM-generated pseudo-labels from large web video. For text-conditioning an extra MLP processes a 4096-d text embedding per frame and the MLP output is added to the model activations before the transformer layers.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>video paired with spoken-language closed captions (visual + natural language commentary)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining: VPT foundation model trained on web_clean (≈70k hours of filtered Minecraft video) using pseudo-labels from an IDM trained on contractor data (IDM contractor dataset ≈1962 hours). Text-conditioning used the subset of videos with closed captions (~17k hours of content with captions). The text-conditioned model used a 220M parameter VPT model fine-tuned with caption embeddings for 4 epochs on the captioned subset.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Minecraft (Survival mode) — early-game behaviors and downstream RL tasks (e.g., diamond pickaxe task)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D open-ended Minecraft survival environment (native human mouse+keyboard interface). Evaluation includes zero-shot BC rollouts (60 minute episodes) and RL fine-tuning for long horizon, hard-exploration goals (e.g., sequence of items culminating in diamond pickaxe). Tasks require navigation, precise GUI inventory interactions (drag-and-drop), crafting, mining, smelting, and combat.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>The text input is natural-language closed captions / commentary (no action tokens); it is a conditioning signal (treated as instructions/descriptors), not a textual action space.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>High-dimensional discrete hierarchical human action space: ~20 binary keypresses plus discretized relative mouse X/Y (11 bins each) implemented as hierarchical heads (policy dimension ≈8461 + 121-d camera head). Includes mutually-exclusive grouping for certain keys and hierarchical camera-on flag.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Closed captions are converted to a 4096-d embedding (OpenAI embedding API), passed through a two-layer MLP (2×2048 hidden), and the MLP output is added to the model activations before the transformer (per-frame additive conditioning). Thus text conditions the visual-action policy via activation injection rather than by learning an explicit symbolic-to-motor mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB first-person frames (rendered 640×360, downsampled to 128×128); in-game GUI overlays (health/hunger/hotbar), rendered mouse cursor when GUI open; no explicit depth or auxiliary sensors provided to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Text-conditioning produced measurable steerability: conditioned prompts such as "I'm going to explore" / "I'm going to find water" caused the agent to travel significantly farther from spawn; conditioning on "collect seeds / chop wood / collect dirt" increased collection of the targeted early-game items (Figure 20). Quantitative exact increases are not reported in absolute numbers in the text (plots show statistically significant increases over baseline but paper does not provide scalar values in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baseline is the same VPT foundation model without text conditioning; the unconditioned model exhibits lower travel distance and collects fewer of the specifically targeted early-game items under the same evaluations. Exact numeric baselines are not tabulated in the text for the captioned-subset experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Text fine-tuning: applied to the 220M VPT model on the captioned-subset (~17k hours) for 4 epochs (no per-episode sample counts reported). The steerability effect emerged after this short fine-tuning (4 epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified numerically in the paper; qualitative claim: a small amount of caption-conditioned fine-tuning (4 epochs on available captioned data) yielded measurable steering ability, but no explicit sample-efficiency multiplier is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Availability of spoken commentary that often describes intent in online videos (semantic alignment between captions and subsequent actions); dense visual grounding in the pretrained VPT visual-action prior; per-frame additive conditioning that injects language features directly into the policy activations; large captioned subset (~17k hours) to fine-tune on.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Weak steerability: (1) captions are noisy, often lack punctuation/capitalization (authors applied rpunct punctuation), (2) coarse temporal conditioning (30s chunks — same text across chunk), (3) the pretrained visual-action prior (human behavior prior) can dominate the weak task-conditioning signal so agent often follows typical human behavior rather than the requested instruction, and (4) limited coverage of rarer skills in captioned data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Injecting natural-language embeddings (from closed captions) into a large video-pretrained behavioral policy can produce weak but statistically measurable steerability in a 3D embodied agent; however the effect is modest: the agent is biased by the strong human-behavior prior learned from video and conditioning is limited by noisy, temporally coarse captions. The method shows promise but requires more data / better alignment to achieve reliable language-conditioned control.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1702.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1702.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP RN50x64 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP RN50x64 (ResNet CLIP model used for frame feature extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CLIP ResNet model pretrained with natural language supervision (image–text pairs) used to extract per-frame embeddings which feed an SVM classifier to filter 'clean' Minecraft frames from web videos for VPT training data curation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP RN50x64 (feature extractor used for data curation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>ResNet-based CLIP model (RN50x64) that maps images to joint image-text embedding space learned via contrastive image–text pretraining; used here as a frozen feature extractor for frame classification.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text pairs / natural-language supervision (contrastive image-text pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper cites CLIP and uses the RN50x64 variant to get embeddings (CLIP input 448×448, embedding length 1024). The original CLIP pretraining corpus details are from the referenced CLIP paper (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Indirect: data curation step for the Minecraft VPT pipeline (enables downstream embodied agent training)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not a direct embodied-task model; CLIP embeddings were used to train an RBF-kernel SVM classifier to detect 'Minecraft Survival Mode - No Artifacts' frames. This filtering produced the web_clean dataset (~70k hours) used to train VPT agents acting in Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>CLIP embeddings extracted per frame were used as input features to train an RBF-kernel SVM classifier (trained on 8800 labeled frames from Mechanical Turk) to detect 'clean' survival frames; classifier applied at 3 fps and segments with ≥80% clean frames were kept.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB frames resized to CLIP input resolution (448×448); classification used CLIP embeddings; frames come from typical video resolutions (original 640×360 before downsampling).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Enabled construction of the web_clean dataset (~70k hours) used to pretrain VPT; allowed removal of artifactual / out-of-distribution frames. No direct numeric uplift of final agent performance attributed solely to CLIP is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Not reported; without CLIP-based embedding + SVM filtering the authors would have had a larger unfiltered dataset (~270k hours) requiring different filtering, but the effect on agent performance is not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP's image-text learned features generalize to frames from diverse web videos, making supervised frame-classification feasible with a relatively small annotation set (8800 labeled images). This enabled large-scale, cleaner pretraining data for the embodied policy.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Not discussed in detail; potential issues include domain differences between web video frames and in-environment renderer frames, and false positives/negatives in filtering, but authors do not quantify these.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-supervised vision models (CLIP) can be repurposed as robust feature extractors to curate large, clean video datasets for training embodied agents — an indirect but practical transfer of language-supervised pretraining to embodied-agent pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1702.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1702.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI text embeddings (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI embedding API (text embedding service) used to embed closed captions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained text embedding API used to convert closed-caption text into fixed-length vectors (4096-d) which were processed by an MLP and injected into the VPT policy to condition behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>OpenAI embedding (4096-d) → MLP conditioning pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Precomputed 4096-dimensional text embeddings (obtained from the OpenAI embedding API) were passed through a randomly initialized MLP (two hidden layers of size 2048) whose outputs were added to the VPT model activations before transformer layers to provide per-frame language conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained text embedding model trained on broad text corpora (external OpenAI embedding service); used to embed closed-caption transcripts from videos.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Paper used the OpenAI embedding API to produce 4096-d vectors for caption chunks; exact pretraining corpus of the embedding model is not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Caption-conditioned VPT agent in Minecraft (same as VPT text-conditioning experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See VPT text-conditioned entry: Minecraft survival tasks requiring visual perception and precise GUI interactions; captions used as conditioning to steer agent behavior in episodic evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Closed captions (natural language sentences) used as conditioning input; not an action-space per se.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Same high-dimensional hierarchical keyboard+mouse action space used by VPT agents.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Text embeddings (OpenAI API) → MLP → additive injection into per-frame activations prior to transformer layers; no symbolic grounding or structured mapping was learned explicitly — conditioning is via continuous activation modulation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Same as VPT: RGB visual frames (128×128 to model), GUI overlays and rendered cursor included.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Conditioned agent showed statistically significant increases in travel distance and in collection of some early-game items (seeds, wood, dirt) when prompted with matching text; authors report qualitative/plot-based evidence rather than tabulated scalar improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Unconditioned VPT model (no caption embeddings) exhibits lower responsiveness to textual prompts; exact numeric baselines not provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Caption fine-tuning used 30s-chunked captions over the available ~17k hours and was run for 4 epochs on the 220M model; exact sample-to-effect counts are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Per-frame additive conditioning is simple and fast to apply; availability of captioned video data with commentary that correlates with actions; large pretrained visual-action prior to receive the conditioning signal.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Noisy captions (most lacked punctuation/capitalization and needed punctuation restoration), coarse temporal alignment (30s chunks), and dominant visual-behavior prior reduced the strength of conditioning — steering remained weak for many intended tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Off-the-shelf text embeddings can be injected into a large video-pretrained policy to produce limited language-conditioned control in a complex 3D embodied domain; this provides a promising prototype but requires better temporal alignment, cleaner instruction data, and stronger conditioning mechanisms for practical language-conditioned embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>A data-driven approach for learning to control computers <em>(Rating: 1)</em></li>
                <li>World of bits: An open-domain platform for web-based agents <em>(Rating: 1)</em></li>
                <li>A survey of reinforcement learning informed by natural language <em>(Rating: 1)</em></li>
                <li>Playing hard exploration games by watching YouTube <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1702",
    "paper_id": "paper-249953673",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "VPT (text-conditioned)",
            "name_full": "Video PreTraining foundation model with closed-caption text conditioning",
            "brief_description": "A VPT behavioral-cloning policy (transformer + ResNet backbone) that was pretrained on large-scale web video (pseudo-labelled by an IDM) and then fine-tuned / conditioned with natural-language closed captions to produce a weakly steerable embodied agent in Minecraft.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "VPT foundation model (text-conditioned variant)",
            "model_agent_description": "ResNet front-end + residual transformer policy (causal during BC), ~220M parameter variant used for text-conditioning experiments; originally trained by behavioral cloning on IDM-generated pseudo-labels from large web video. For text-conditioning an extra MLP processes a 4096-d text embedding per frame and the MLP output is added to the model activations before the transformer layers.",
            "pretraining_data_type": "video paired with spoken-language closed captions (visual + natural language commentary)",
            "pretraining_data_details": "Pretraining: VPT foundation model trained on web_clean (≈70k hours of filtered Minecraft video) using pseudo-labels from an IDM trained on contractor data (IDM contractor dataset ≈1962 hours). Text-conditioning used the subset of videos with closed captions (~17k hours of content with captions). The text-conditioned model used a 220M parameter VPT model fine-tuned with caption embeddings for 4 epochs on the captioned subset.",
            "embodied_task_name": "Minecraft (Survival mode) — early-game behaviors and downstream RL tasks (e.g., diamond pickaxe task)",
            "embodied_task_description": "3D open-ended Minecraft survival environment (native human mouse+keyboard interface). Evaluation includes zero-shot BC rollouts (60 minute episodes) and RL fine-tuning for long horizon, hard-exploration goals (e.g., sequence of items culminating in diamond pickaxe). Tasks require navigation, precise GUI inventory interactions (drag-and-drop), crafting, mining, smelting, and combat.",
            "action_space_text": "The text input is natural-language closed captions / commentary (no action tokens); it is a conditioning signal (treated as instructions/descriptors), not a textual action space.",
            "action_space_embodied": "High-dimensional discrete hierarchical human action space: ~20 binary keypresses plus discretized relative mouse X/Y (11 bins each) implemented as hierarchical heads (policy dimension ≈8461 + 121-d camera head). Includes mutually-exclusive grouping for certain keys and hierarchical camera-on flag.",
            "action_mapping_method": "Closed captions are converted to a 4096-d embedding (OpenAI embedding API), passed through a two-layer MLP (2×2048 hidden), and the MLP output is added to the model activations before the transformer (per-frame additive conditioning). Thus text conditions the visual-action policy via activation injection rather than by learning an explicit symbolic-to-motor mapping.",
            "perception_requirements": "RGB first-person frames (rendered 640×360, downsampled to 128×128); in-game GUI overlays (health/hunger/hotbar), rendered mouse cursor when GUI open; no explicit depth or auxiliary sensors provided to the model.",
            "transfer_successful": true,
            "performance_with_pretraining": "Text-conditioning produced measurable steerability: conditioned prompts such as \"I'm going to explore\" / \"I'm going to find water\" caused the agent to travel significantly farther from spawn; conditioning on \"collect seeds / chop wood / collect dirt\" increased collection of the targeted early-game items (Figure 20). Quantitative exact increases are not reported in absolute numbers in the text (plots show statistically significant increases over baseline but paper does not provide scalar values in text).",
            "performance_without_pretraining": "Baseline is the same VPT foundation model without text conditioning; the unconditioned model exhibits lower travel distance and collects fewer of the specifically targeted early-game items under the same evaluations. Exact numeric baselines are not tabulated in the text for the captioned-subset experiments.",
            "sample_complexity_with_pretraining": "Text fine-tuning: applied to the 220M VPT model on the captioned-subset (~17k hours) for 4 epochs (no per-episode sample counts reported). The steerability effect emerged after this short fine-tuning (4 epochs).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": "Not quantified numerically in the paper; qualitative claim: a small amount of caption-conditioned fine-tuning (4 epochs on available captioned data) yielded measurable steering ability, but no explicit sample-efficiency multiplier is reported.",
            "transfer_success_factors": "Availability of spoken commentary that often describes intent in online videos (semantic alignment between captions and subsequent actions); dense visual grounding in the pretrained VPT visual-action prior; per-frame additive conditioning that injects language features directly into the policy activations; large captioned subset (~17k hours) to fine-tune on.",
            "transfer_failure_factors": "Weak steerability: (1) captions are noisy, often lack punctuation/capitalization (authors applied rpunct punctuation), (2) coarse temporal conditioning (30s chunks — same text across chunk), (3) the pretrained visual-action prior (human behavior prior) can dominate the weak task-conditioning signal so agent often follows typical human behavior rather than the requested instruction, and (4) limited coverage of rarer skills in captioned data.",
            "key_findings": "Injecting natural-language embeddings (from closed captions) into a large video-pretrained behavioral policy can produce weak but statistically measurable steerability in a 3D embodied agent; however the effect is modest: the agent is biased by the strong human-behavior prior learned from video and conditioning is limited by noisy, temporally coarse captions. The method shows promise but requires more data / better alignment to achieve reliable language-conditioned control.",
            "uuid": "e1702.0",
            "source_info": {
                "paper_title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "CLIP RN50x64 (used)",
            "name_full": "CLIP RN50x64 (ResNet CLIP model used for frame feature extraction)",
            "brief_description": "A CLIP ResNet model pretrained with natural language supervision (image–text pairs) used to extract per-frame embeddings which feed an SVM classifier to filter 'clean' Minecraft frames from web videos for VPT training data curation.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_agent_name": "CLIP RN50x64 (feature extractor used for data curation)",
            "model_agent_description": "ResNet-based CLIP model (RN50x64) that maps images to joint image-text embedding space learned via contrastive image–text pretraining; used here as a frozen feature extractor for frame classification.",
            "pretraining_data_type": "Image–text pairs / natural-language supervision (contrastive image-text pretraining)",
            "pretraining_data_details": "The paper cites CLIP and uses the RN50x64 variant to get embeddings (CLIP input 448×448, embedding length 1024). The original CLIP pretraining corpus details are from the referenced CLIP paper (not detailed in this paper).",
            "embodied_task_name": "Indirect: data curation step for the Minecraft VPT pipeline (enables downstream embodied agent training)",
            "embodied_task_description": "Not a direct embodied-task model; CLIP embeddings were used to train an RBF-kernel SVM classifier to detect 'Minecraft Survival Mode - No Artifacts' frames. This filtering produced the web_clean dataset (~70k hours) used to train VPT agents acting in Minecraft.",
            "action_space_text": null,
            "action_space_embodied": null,
            "action_mapping_method": "CLIP embeddings extracted per frame were used as input features to train an RBF-kernel SVM classifier (trained on 8800 labeled frames from Mechanical Turk) to detect 'clean' survival frames; classifier applied at 3 fps and segments with ≥80% clean frames were kept.",
            "perception_requirements": "RGB frames resized to CLIP input resolution (448×448); classification used CLIP embeddings; frames come from typical video resolutions (original 640×360 before downsampling).",
            "transfer_successful": true,
            "performance_with_pretraining": "Enabled construction of the web_clean dataset (~70k hours) used to pretrain VPT; allowed removal of artifactual / out-of-distribution frames. No direct numeric uplift of final agent performance attributed solely to CLIP is reported.",
            "performance_without_pretraining": "Not reported; without CLIP-based embedding + SVM filtering the authors would have had a larger unfiltered dataset (~270k hours) requiring different filtering, but the effect on agent performance is not quantified.",
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "CLIP's image-text learned features generalize to frames from diverse web videos, making supervised frame-classification feasible with a relatively small annotation set (8800 labeled images). This enabled large-scale, cleaner pretraining data for the embodied policy.",
            "transfer_failure_factors": "Not discussed in detail; potential issues include domain differences between web video frames and in-environment renderer frames, and false positives/negatives in filtering, but authors do not quantify these.",
            "key_findings": "Language-supervised vision models (CLIP) can be repurposed as robust feature extractors to curate large, clean video datasets for training embodied agents — an indirect but practical transfer of language-supervised pretraining to embodied-agent pipelines.",
            "uuid": "e1702.1",
            "source_info": {
                "paper_title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "OpenAI text embeddings (used)",
            "name_full": "OpenAI embedding API (text embedding service) used to embed closed captions",
            "brief_description": "A pretrained text embedding API used to convert closed-caption text into fixed-length vectors (4096-d) which were processed by an MLP and injected into the VPT policy to condition behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "OpenAI embedding (4096-d) → MLP conditioning pipeline",
            "model_agent_description": "Precomputed 4096-dimensional text embeddings (obtained from the OpenAI embedding API) were passed through a randomly initialized MLP (two hidden layers of size 2048) whose outputs were added to the VPT model activations before transformer layers to provide per-frame language conditioning.",
            "pretraining_data_type": "Pretrained text embedding model trained on broad text corpora (external OpenAI embedding service); used to embed closed-caption transcripts from videos.",
            "pretraining_data_details": "Paper used the OpenAI embedding API to produce 4096-d vectors for caption chunks; exact pretraining corpus of the embedding model is not specified in the paper.",
            "embodied_task_name": "Caption-conditioned VPT agent in Minecraft (same as VPT text-conditioning experiment)",
            "embodied_task_description": "See VPT text-conditioned entry: Minecraft survival tasks requiring visual perception and precise GUI interactions; captions used as conditioning to steer agent behavior in episodic evaluations.",
            "action_space_text": "Closed captions (natural language sentences) used as conditioning input; not an action-space per se.",
            "action_space_embodied": "Same high-dimensional hierarchical keyboard+mouse action space used by VPT agents.",
            "action_mapping_method": "Text embeddings (OpenAI API) → MLP → additive injection into per-frame activations prior to transformer layers; no symbolic grounding or structured mapping was learned explicitly — conditioning is via continuous activation modulation.",
            "perception_requirements": "Same as VPT: RGB visual frames (128×128 to model), GUI overlays and rendered cursor included.",
            "transfer_successful": true,
            "performance_with_pretraining": "Conditioned agent showed statistically significant increases in travel distance and in collection of some early-game items (seeds, wood, dirt) when prompted with matching text; authors report qualitative/plot-based evidence rather than tabulated scalar improvements.",
            "performance_without_pretraining": "Unconditioned VPT model (no caption embeddings) exhibits lower responsiveness to textual prompts; exact numeric baselines not provided in text.",
            "sample_complexity_with_pretraining": "Caption fine-tuning used 30s-chunked captions over the available ~17k hours and was run for 4 epochs on the 220M model; exact sample-to-effect counts are not reported.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Per-frame additive conditioning is simple and fast to apply; availability of captioned video data with commentary that correlates with actions; large pretrained visual-action prior to receive the conditioning signal.",
            "transfer_failure_factors": "Noisy captions (most lacked punctuation/capitalization and needed punctuation restoration), coarse temporal alignment (30s chunks), and dominant visual-behavior prior reduced the strength of conditioning — steering remained weak for many intended tasks.",
            "key_findings": "Off-the-shelf text embeddings can be injected into a large video-pretrained policy to produce limited language-conditioned control in a complex 3D embodied domain; this provides a promising prototype but requires better temporal alignment, cleaner instruction data, and stronger conditioning mechanisms for practical language-conditioned embodied agents.",
            "uuid": "e1702.2",
            "source_info": {
                "paper_title": "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2,
            "sanitized_title": "learning_transferable_visual_models_from_natural_language_supervision"
        },
        {
            "paper_title": "A data-driven approach for learning to control computers",
            "rating": 1,
            "sanitized_title": "a_datadriven_approach_for_learning_to_control_computers"
        },
        {
            "paper_title": "World of bits: An open-domain platform for web-based agents",
            "rating": 1,
            "sanitized_title": "world_of_bits_an_opendomain_platform_for_webbased_agents"
        },
        {
            "paper_title": "A survey of reinforcement learning informed by natural language",
            "rating": 1,
            "sanitized_title": "a_survey_of_reinforcement_learning_informed_by_natural_language"
        },
        {
            "paper_title": "Playing hard exploration games by watching YouTube",
            "rating": 1,
            "sanitized_title": "playing_hard_exploration_games_by_watching_youtube"
        }
    ],
    "cost": 0.01932875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos
23 Jun 2022</p>
<p>Bowen Baker 
OpenAI ‡ University of British Columbia</p>
<p>Peter Zhokhov peterz@openai.com 
OpenAI ‡ University of British Columbia</p>
<p>Adrien Ecoffet adrien@openai.com 
OpenAI ‡ University of British Columbia</p>
<p>Brandon Houghton brandon@openai.com 
OpenAI ‡ University of British Columbia</p>
<p>Raul Sampedro 
OpenAI ‡ University of British Columbia</p>
<p>Jeff Clune jclune@gmail.com 
OpenAI ‡ University of British Columbia</p>
<p>Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos
23 Jun 20220C286BCC5ED7DC00FE1604C1F1A07932arXiv:2206.11795v1[cs.LG]
2][3][4][5][6]However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way.We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos.Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -here, online videos of people playing Minecraft -from which we can then train a general behavioral prior.Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zeroshot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning.For many tasks our models exhibit humanlevel performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.* This was a large effort by a dedicated team.Each author made huge contributions on many fronts over long time periods.All members were full time on the project for over six months.BB, IA, PZ, and JC were on the original VPT project team and were thus involved for even longer (over a year).Aside from those original team members, author order is random.It was also randomized between IA and PZ.</p>
<p>Introduction</p>
<p>Work in recent years has demonstrated the efficacy of pretraining large and general foundation models 7 on noisy internet-scale datasets for use in downstream tasks in natural language [1][2][3][4] and computer vision. 5,6,8For sequential decision domains (e.g.robotics, game playing, and computer usage) where agents must repeatedly act within an environment, a wealth of data also exists on the web; however, most of this data is in the form of unlabeled video (i.e.without the actions taken at each frame), making it much less straightforward to train a behavioral prior in these domains than it is in e.g.natural language.In a few rare settings, such as Chess, Go, and StarCraft, there already exist large datasets with action labels from various online platforms that researchers have used for imitation learning. 9,103][14][15][16][17][18] Many virtual tasks, e.g.navigating websites, using Photoshop, booking flights, etc., can be very hard to learn with RL and do not have large, commonly available sources of labeled data. 19,20In this paper, we seek to extend the paradigm of training large, general-purpose foundation models to sequential decision domains by utilizing freely available internet-scale unlabeled video datasets with a simple semi-supervised imitation learning method.We call this method Video PreTraining (VPT) and demonstrate its efficacy in the domain of Minecraft.</p>
<p>2][23][24][25] Furthermore, most prior semi-supervised imitation learning work was tested in the relatively low data regime; because we experiment with far more data (∼70k hours of unlabeled video), we hypothesize that we can achieve good performance with a much simpler method, a trend that has proven true for pretraining in other modalities such as text. 1 In particular, given a large but unlabeled dataset, we propose generating pseudo-labels by gathering a small amount of labeled data to train an inverse dynamics model (IDM) that predicts the action taken at each timestep in a video.Behavioral cloning (BC) can require a large amount of data because the model must learn to infer intent and the distribution over future behaviors from only past observations.In contrast, the inverse dynamics modeling task is simpler because it is non-causal, meaning it can look at both past and future frames to infer actions.In most settings, environment mechanics are far simpler than the breadth of human behavior that can take place within the environment, suggesting that non-causal IDMs could require far less data to train than causal BC models.Using pseudo-labels generated from the IDM, we then train a model to mimic the distribution of behavior in the previously unlabeled dataset with standard behavioral cloning at scale, which does not require any model rollouts and thus does not suffer from any potential exploration bottlenecks in the environment.Finally, we show we can fine-tune this model to downstream tasks with either behavioral cloning or reinforcement learning.8][29][30][31] In this work we use the native human interface for Minecraft so that we can (1) most accurately model the human behavior distribution and reduce domain shift between video data and the environment, (2) make data collection easier by allowing our human contractors to play the game without modification, and (3) eliminate the need to hand-engineer a custom interface for models to interact with the environment.This choice means that our models play at 20 frames per second and must use a mouse and keyboard interface to interact with human GUIs for crafting, smelting, trading, etc., including dragging items to specific slots or navigating the recipe book with the mouse cursor (Fig. 1).Compared to prior work in Minecraft that uses a lower frame rate and constructs crafting and attacking macros, 30,[32][33][34] using the native human interface drastically increases the environment's exploration difficulty, making most simple tasks near impossible with RL from scratch.Even the simple task of gathering a single wooden log while already facing a tree takes 60 consecutive attack actions with the human interface, meaning the chance for a naive random policy to succeed is 1 2 60 .While this paper shows results in Minecraft only, the VPT method is general and could be applied to any domain.</p>
<p>In Section 4 we show that the VPT foundation model has nontrivial zero-shot performance, accomplishing tasks impossible to learn with RL alone, such as crafting planks and crafting tables (tasks requiring a human proficient in Minecraft a median of 50 seconds or ∼970 consecutive actions).Through fine-tuning with behavioral cloning to smaller datasets that target more specific behavior distributions, our agent is able to push even further into the technology tree, crafting stone tools (taking a human a median of 2.3 minutes or ∼2790 actions).Finally, fine-tuning via RL produces the most dramatic improvements: our agent is able to craft diamond tools, an unprecedented result in Minecraft made even more challenging by using the native human interface.This task requires a proficient human a median upwards of 20 minutes or ∼24000 actions.The main contributions of this work are (1) we are the first to show promising results applying semi-supervised imitation learning to extremely large, noisy, and freely available video datasets for sequential decision domains, (2) we show that such pretraining plus fine-tuning enables agents to solve tasks that were otherwise impossible to learn, (3) we show that labeled contractor data is far more efficiently used within the VPT method than it would be by directly training a foundation model from it and ( 4) we open source our contractor data, trained model weights, and Minecraft environment for future research into learning to act via semi-supervised imitation learning at scale.</p>
<p>Preliminaries and Related Work</p>
<p>Imitation learning methods [35][36][37][38] seek to construct a policy that accurately models the distribution of behavior in some dataset D = {(o i , a i )}, i ∈ {1...N } of action-observation pairs.In order to roll out these policies in an environment, they must be causal, meaning they condition on observations from the current timestep t and past timesteps only, i.e. π ∼ p(a t |o 1 ...o t ).Imitation learning is simplest when demonstrations are labeled with corresponding actions.Imitating labeled trajectories has seen success in aerial vehicles, 39,40 self-driving cars, 41,42 board games, 9,43 and video games. 10,44en labeled demonstrations are not available, standard behavioral cloning will not work; however, there is a large body of work in imitating behavior from unlabeled demonstrations. 22For instance, GAIL 23 constructs an adversarial objective incentivizing the trained policy to exhibit behaviors indistinguishable from those in the target dataset.Edwards et al. 45 propose to first learn a latent policy using unlabeled demonstrations and then map the learned latent actions to real actions with a small amount of environment interaction.Peng et al. 46 first use motion-capture methods to track agent positions in videos and then train RL agents to match these waypoints.Similarly, Behbahani et al. 47 and Aytar et al. 48task a RL agent to match waypoints; however, they construct waypoints that are embeddings from unsupervised feature learning models.Pathak et al. 49 and Nair et al. 50train goal conditioned policies to take actions that advance the current state towards expert-provided goal states expressed as high dimensional visual waypoints.Most similar to our own work, Torabi et al. 24 simultaneously train (1) an inverse dynamics model (IDM), 51 which aims to uncover the underlying action between timesteps given observations of past and future timesteps, e.g.p IDM (a t |o t , o t+1 ), and (2) a behavioral cloning (BC) model on trajectories of observations labeled with the IDM.Data to train the IDM is collected by rolling out the BC model in the target environment such that both models improve in tandem.However, at any point in training if there are sequences in the dataset that the IDM performs poorly on, it requires that the BC model perform those or similar sequences in order for the IDM to improve and correctly label them.Therefore, if the BC model does not explore efficiently, it could severely slow down learning.In order to avoid this potential issue we opted for a simpler two-stage approach: we first train an IDM on a small number of labeled trajectories collected from human contractors (they play the game as would normally as we record their keypresses and mouse movements).Because human contractors reach most relevant parts of the state space, we can hold the IDM fixed throughout BC training.</p>
<p>Compared to most previous work in semi-supervised imitation learning, we experiment in the much more complex and open-ended environment of Minecraft.[33][34][52][53][54][55][56][57][58][59][60] A large body of work focuses on small, custom-made Minecraft worlds with tasks such as navigation, 53,60 block placing, 54,55 instruction following, 58,59 combat, 56 and others. 28,31,57Work operating in the massive, randomly generated environments of Minecraft itself has included hill climbing, 52 automated curriculum learning 30 and, most closely related to the RL experiments presented in Sec.3][34] However, to the best of our knowledge, there is no published work that operates in the full, unmodified human action space, which includes drag-and-drop inventory management and item crafting.</p>
<p>Methods</p>
<p>Inverse Dynamics Models (IDM) VPT, illustrated in Figure 2, requires we first collect a small amount of labeled contractor data with which to train an inverse dynamics model p IDM (a t |o 1...T ), which seeks to minimize the negative log-likelihood of an action at timestep t given a trajectory of T observations o t : t ∈ [1...T ].In contrast to an imitation learning policy, the IDM can be non-causal, meaning its prediction for a t can be a function of both past and future events, i.e. o t &gt;t .Compared to the behavioral cloning objective of modeling the distribution of human intent given past frames only, we hypothesize that inverting environment dynamics is easier and more data efficient to learn.Indeed, Sec. 4.1 will show that the IDM objective is much easier to learn, and furthermore Sec.4.6 will show that with very little labeled data (as few as 100 hours) we can train a fairly accurate IDM.This IDM can be used to label online videos, providing the large amount of data required for the harder task of behavioral cloning.See appendices D and B for IDM training and data collection details.</p>
<p>Data Filtering</p>
<p>We gather a large dataset of Minecraft videos by searching the web for related keywords (Appendix A).Online videos often (1) include overlaid artifacts, such as a video feed of the player's face, channel logos, watermarks, etc., (2) are collected from platforms other than a computer with different gameplay, or (3) are from different game modes, e.g. in Minecraft we only want "survival mode" where players start from scratch and must gather or craft all their items.We call data "clean" if it does not contain visual artifacts and is from survival mode, and call all other data "unclean."With enough data, a large enough model, and enough training compute, a BC model trained on both unclean and clean videos would likely still perform well in a clean Minecraft environment.However, for simplicity and training compute efficiency, we choose to filter out unclean segments of video (note that a video may contain both clean and unclean segments).We do this by training a model to filter out unclean segments using a small dataset (8800) of images sampled from online videos labeled by contractors as clean or unclean (Appendix A.2).</p>
<p>VPT Foundation Model</p>
<p>We train a foundation model with standard behavioral cloning, i.e. minimizing the negative log-likelihood of actions predicted by the IDM on clean data.For a particular trajectory of length T we minimize
min θ t∈[1...T ] − log π θ (a t |o 1 , . . . , o t ), where a t ∼ p IDM (a t |o 1 , . . . , o t , . . . , o T )(1)
As we will see in the following sections, this model exhibits nontrivial zero-shot behavior and can be fine-tuned with both imitation learning and RL to perform even more complex skills.</p>
<p>Results</p>
<p>Performance of the Inverse Dynamics Model</p>
<p>The IDM architecture is comprised primarily of a temporal convolution layer, a ResNet 62 image processing stack, and residual unmasked attention layers, from which the IDM simultaneously predicts keypresses and mouse movements (see Appendix D for IDM architecture and training details).A key hypothesis behind our work is that IDMs can be trained with a relatively small amount of labeled data.While more data improves both mouse movement and keypress predictions, our best  Basic mining refers to collection of dirt, gravel, or sand (all materials that can be gathered without tools).Logs are obtained by repeatedly hitting trees for three seconds, a difficult feat for an RL agent to achieve as we show in Sec.4.4.Planks can be crafted from logs, and crafting tables crafted from planks.Crafting requires using in-game crafting GUIs, and proficient humans take a median of 50 seconds (970 consecutive actions) to make a crafting table.</p>
<p>We now explore the emergent behavior learned by a behavioral cloning policy trained on an extremely large, but noisy, internet dataset labeled with our IDM.To collect the unlabeled internet dataset, we searched for publicly available videos of Minecraft play with search terms such as "minecraft survival for beginners."These searches resulted in ∼270k hours of video, which we filtered down to "clean" video segments yielding an unlabeled dataset of ∼70k hours, which we refer to as web_clean (Appendix A has further details on data scraping and filtering).We then generated pseudo-labels for web_clean with our best IDM (Section 3) and then trained the VPT foundation model with behavioral cloning.Preliminary experiments suggested that our model could benefit from 30 epochs of training and that a 0.5 billion parameter model was required to stay in the efficient learning regime 63 for that training duration (Appendix H), which took ∼9 days on 720 V100 GPUs.</p>
<p>We evaluate our models by measuring validation loss (Fig. 4, left) and rolling them out in the Minecraft environment.Unless otherwise noted, in all environment evaluations we spawn agents in a standard survival mode game where they play for 60 minutes, i.e. 72000 consecutive actions, and we plot the mean and shade the standard error of the mean for various game statistics such as crafting and collection rates (Fig. 4, right).The VPT foundation model quickly learns to chop down trees to collect logs, a task we found near impossible for an RL agent to achieve with the native human interface (Sec.4.4).It also learns to craft those logs into wooden planks and then use those planks to craft a crafting table, which are required to unlock most other technology in the game and take a human proficient in Minecraft approximately 50 seconds (970 consecutive actions) to collect.While these behaviors are fairly complex in the native human action space, the VPT foundation model crafts these items at a rate far below that of our proficient contractors, e.g. on average our contractors craft 5.44 crafting tables in 60 minutes of play versus 0.19 for the foundation model.The model also crafts a non-negligible amount of wooden sticks, which are required to make wooden tools; collects various flowers and crafts dyes from them; kills zombies that appear during the night; hunts wild animals; collects various berries and mushrooms and eats them; and finds game-generated villages from which to collect various rare items from chests.iv) While training and validation loss decrease healthily over training (Fig. 4, left), loss on our contractor dataset (which the VPT model does not train on) begins increasing after 7 epochs.Contractor data could be out-of-distribution because our contractors may have a different distribution of play or because there is some impactful visual domain shift compared to videos from the web.While one could have expected this would be predictive of declining evaluation performance, we do not see notable game statistics from the VPT foundation model rollouts (Figure 4, right) decrease over training, and in the next section we show that BC fine-tuning performance continually improves as the VPT foundation model trains.We provide more insight into this curious phenomenon in Appendix H.</p>
<p>Fine-Tuning with Behavioral Cloning</p>
<p>Foundation models are designed to have a broad behavior profile and be generally capable across a wide variety of tasks.To incorporate new knowledge or allow them to specialize on a narrower task distribution, it is common practice to fine-tune these models to smaller, more specific datasets. 1The VPT foundation model trained on the broad web_clean dataset had nontrivial zero-shot performance; it was able to craft a crafting table yet unable to go past this in the technology tree.As a case study into BC fine-tuning, we attempt to improve the VPT foundation model's ability to collect and craft these "early game" items by fine-tuning to two narrower datasets targeted at Minecraft behavior within the first few minutes of players starting in a fresh world.In the first dataset, contractor_house, contractors have 10 minutes to build a basic house from scratch using primarily wood, sand, and dirt.Collecting contractor data can be difficult and expensive, so we also construct a dataset earlygame_keyword by searching for videos online with descriptions that match keywords such as "new world", "let's play episode 1", etc.; this is a subset of web_clean and is labeled with the IDM.See Appendix B.4 and A.3 for full descriptions of both datasets.Fine-tuning to earlygame_keyword results in a large boost compared to the zero-shot foundation model: 2.5x more crafting tables, 6.1x more planks, 4.3x more logs, and 5.5x more crafting overall (Fig. 5).However, when fine-tuning to this dataset we did not see any new behaviors emerge, only a refinement of existing skills.We saw an even bigger improvement when fine-tuning to the contractor_house dataset: 213x more crafting tables, 59x more wooden planks, 7x more logs, and 59x more crafting over all.In addition, we saw the emergence of crafting wooden tools, which requires placing a crafting table on the ground, opening it to reveal a new crafting interface, and then using it to craft wooden tools.This entire sequence takes a proficient human player a median of 1.2 minutes (1390 consecutive actions) to accomplish.The model goes further and collects cobblestone, which requires a wooden pickaxe to mine, and crafts stone tools, requiring it to again use a crafting table; this takes a proficient human player a median of 2.3 minutes (2790 consecutive actions).v) Despite the foundation model's zero-shot rollout performance plateauing 1/3 into training (Fig. 4, right), fine-tuning performance does continue to increase throughout foundation model training (Fig. 5, right).Additionally, there is a stark difference in performance when training from scratch vs. fine-tuning from the VPT foundation model (Fig. 5 right, comparing the left and rightmost points).</p>
<p>Effect of Foundation Model Quality on BC Fine-Tuning</p>
<p>Fine-Tuning with Reinforcement Learning</p>
<p>Figure 6: Typical sequence of items for obtaining a diamond pickaxe.Below each item is the median time and number of actions contractors required to obtain that item and the percentage of contractors that got the item within 10 minutes.The median time to obtain a diamond pickaxe is unknown (except that it is &gt; 20m) because contractors obtained this item in less than 50% of 20-minute episodes.</p>
<p>To demonstrate the efficacy of RL fine-tuning, we chose the challenging goal of obtaining a diamond pickaxe within 10 minutes starting from a fresh Minecraft survival world.Doing so involves acquiring a sequence of difficult-to-obtain items that require complex skills like mining, inventory management, crafting with and without a crafting table, tool use, operating a furnace, and mining at the lowest depths, where many hazards like enemies and lava exist (Fig. 6).Adding to the difficulty, progress can be easily lost by dropping items, destroying items, or dying.Obtaining a diamond pickaxe more often than not takes a proficient human over 20 minutes (24,000 actions).</p>
<p>Agents are rewarded for each item obtained in the sequence, with lower rewards for items that have to be collected in bulk and higher rewards for items near the end of the sequence.Agents are optimized with the phasic policy gradient 64  A major problem when fine-tuning with RL is catastrophic forgetting 65,66 because previously learned skills can be lost before their value is realized.For instance, while our VPT foundation model never exhibits the entire sequence of behaviors required to smelt iron zero-shot, it did train on examples of players smelting with furnaces.It therefore may have some latent ability to smelt iron once the many prerequisites to do so have been performed.To combat the catastrophic forgetting of latent skills such that they can continually improve exploration throughout RL fine-tuning, we add an auxiliary Kullback-Leibler (KL) divergence loss between the RL model and the frozen pretrained policy. 10aining from a randomly initialized policy fails to achieve almost any reward, underscoring how hard an exploration challenge the diamond pickaxe task is for RL in the native human action space (Fig. 7a).The model never learns to reliably collect logs, typically the first of many steps to obtaining a diamond pickaxe (Fig. 7b).RL fine-tuning from the VPT foundation model does substantially better (Fig. 7a), learning everything up to mining iron ore and crafting furnaces.(Fig. 7c).However, this agent fails at smelting an iron ingot, the next item required to get further into the tech tree, likely (v) Sample Videos: https://www.youtube.com/playlist?list=PLNAOIb_agjf2yDSs4AqcoyPv4z_eWUiKm because the zero-shot probability that the VPT foundation model smelts an iron ingot is too low, even when given the prerequisite materials.</p>
<p>Results further improve by first BC fine-tuning the VPT Foundation Model to the earlygame_keyword dataset (the early-game model, Sec.4.3) and then fine-tuning with RL (Fig. 7a), which in preliminary experiments we found to perform better than first fine-tuning to contractor_house followed by fine-tuning with RL (Appendix G.2).The three-phase training (pretraining, BC fine-tuning, and then RL fine-tuning) succeeds in learning extremely difficult tasks: it achieves over 80% reliability on iron pickaxes, almost 20% reliability on collecting diamonds, and 2.5% reliability on obtaining a diamond pickaxe (Fig. 7d).For comparison, human players given the objective of obtaining a diamond pickaxe collect these items in 57%, 15%, and 12% of episodes, respectively, meaning our model is human-level for crafting iron pickaxes and mining diamonds.</p>
<p>Others have managed to obtain diamonds with ∼ 0.1% reliability in 15 minutes 32,33 but always with a simplified action space designed to ease exploration.To the best of our knowledge, we are the first to report non-zero success rates on crafting a diamond pickaxe.vi) Finally, we validated the importance of the KL loss to the pretrained model during RL fine-tuning.The treatment without a KL loss obtains only items early in the sequence (logs, planks, sticks, and crafting tables) limiting its reward (Fig. 7a).This failure to progress further into the sequence is likely because, while the initial skills of chopping logs and crafting planks are being learned with RL, subsequent skills like crafting a wooden pickaxe are lost due to catastrophic forgetting.</p>
<p>Data Scaling Properties of the Foundation Model</p>
<p>In this section we validate a core hypothesis behind this work: that it is far more effective to use labeled contractor data to train an IDM within the VPT method than it is to directly train a BC foundation model from that same small contractor dataset.If we could cheaply collect a labeled contractor dataset of a similar order of magnitude as web_clean, then this would not be important; however, collecting that scale of data would have cost millions of dollars.Figure 8 compares foundation models trained on increasing orders of magnitude of data from 1 hour up to the full ∼70k web_clean dataset.Foundation models trained up to and including 1k hours are trained on the IDM (vi) Videos found at https://www.youtube.com/playlist?list=PLNAOIb_agjf3e_UKweM5pQUSfTw8r-WfcThis section investigates how downstream BC performance is affected by IDM quality.We train IDMs on increasingly larger datasets and use each to independently label the earlygame_keyword dataset (this smaller dataset was chosen due to a limited compute budget).We then train a BC model from scratch on each dataset and report game statistics for each model as a function of IDM contractor dataset size (Fig. 9).</p>
<p>IDMs trained on at least 10 hours of data are required for any crafting, and the crafting rate increases quickly up until 100 hours of data, after which there are few to no gains and differences are likely due to noise.Similarly, crafting tables are only crafted after 50 or more hours of IDM data, and again gains plateau after 100 hours.While in all previous experiments we use our best IDM trained on 1962 hours of data, these results suggest we could reduce that number to as low as 100 hours.</p>
<p>Discussion and Conclusion</p>
<p>The results presented in this paper help pave the path to utilizing the wealth of unlabeled data on the web for sequential decision domains.Compared to generative video modeling or contrastive methods that would only yield representational priors, VPT offers the exciting possibility of directly learning to act during pretraining and using these learned behavioral priors as extremely effective exploration priors for RL.VPT could even be a better general representation learning method even when the downstream task is not learning to act in that domain-for example, fine-tuning to explain what is happening in a video-because arguably the most important information in any given scene would be present in features trained to correctly predict the distribution over future human actions.We leave this intriguing direction to future work.</p>
<p>Future work could improve results with more data (we estimate we could collect &gt;1M hours) and larger, better-tuned models.Furthermore, all the models in this work condition on past observations only; we cannot ask the model to perform specific tasks.Appendix I presents preliminary experiments on conditioning our models on closed captions (text transcripts of speech in videos), showing they become weakly steerable; we believe this a rich direction for future research.Also, loss was not consistently correlated with downstream evaluation metrics (Sec.4.2), which often made progress slow and hard-won.Another fruitful future direction would be to investigate the correlation between various training metrics and downstream evaluations.Finally, while we do not anticipate any direct negative societal impacts from the models trained in this work, as VPT improves and expands to other domains it will be important to assess and mitigate harms that emerge with other forms of pretraining on internet datasets, such as emulating inappropriate behavior. 67 conclusion, VPT extends the paradigm of training large and general purpose behavioral priors from freely available internet-scale data to sequential decision domains.Our models exhibited impressive zero-shot behavior and, when fine-tuned with RL, achieved an unprecedented result of crafting a diamond pickaxe in Minecraft (all the more difficult given the human interface).We further showed that contractor data is far better used within the VPT pipeline than to train a foundation model directly and that only a small amount of contractor data (about $2000 USD) was required to unlock massive amounts of unlabeled online data for use in BC.Finally, learning with the human keyboard and mouse interface is highly general and allows losslessly modeling the entire distribution of human behavior.While we only experiment in Minecraft, we believe that VPT provides a general recipe for training behavioral priors in hard, yet generic, action spaces in any domain that has a large amount of freely available unlabeled data, such as computer usage.</p>
<p>To do so, we asked contractors to label a set of random video frames (images) from Minecraft videos (N=8800).These images were from a random subset of the videos we collected toward the beginning of the project (Section A.1).</p>
<p>A.2.1 Label Collection</p>
<p>We asked 5 workers on Amazon Mechanical Turk (mTurk) that we selected with a sample qualification task to label random screen capture images to be used in training the classifier.A sample worker interface that the workers saw on mTurk is given in Figure 10.</p>
<p>We asked workers to label videos as being in one of the following three categories (see Figure 11 for visual examples of each class):</p>
<p>Creative Mode</p>
<p>Creative mode only has an item hotbar and should be classified as None of the Above.</p>
<p>Label Descriptions</p>
<p>• Minecraft Survival Mode -No Artifacts: These images will be clean screenshots from the Minecraft survival mode gameplay without any noticeable artifacts.• Minecraft Survival Mode -with Artifacts: These images will be valid survival mode screenshots, but with some added artifacts.Typical artifacts may include image overlays (a logo/brand), text annotations, a picture-in-picture of the player, etc. • None of the Above: Use this category when the image is not a valid Minecraft survival screenshot.It may be a non-Minecraft frame or from a different game mode.In non-survival game modes such as the creative mode, the health/hunger bars will be missing from the image, the item hotbar may or may not be still present.</p>
<p>In total, we spent $319.96 on human labeling experiments on mTurk, of which $159.98 was directly paid to workers.The remaining amount was spent towards Amazon platform fees.The workers received $0.01 per labeled image, at an hourly compensation of $7.20 (based on an estimated labeling time of 5 seconds/image -in our internal sample run of the same task, we found the average labeling time to be &lt; 3 seconds).</p>
<p>Since we perform rigorous keyword and metadata based filtering of videos (as described in A.1) from which we served sample images to be labeled, serving offensive content to workers was extremely low risk and no such images were detected during our manual checks.We only collected labels during our experiment, and the workers were fully anonymized via the mTurk platform, therefore no personally identifiable information (PII) was collected.</p>
<p>SVM Training</p>
<p>With the image labels collected as described in the previous section, we trained a classifier to extract video segments that consist of frames from the Minecraft Survival Mode -No Artifacts category.Given a set of labeled images, we obtain embeddings for each image using the RN50x64 ResNet CLIP Model. 6This is a ResNet-based CLIP model that is scaled up to have approximately 64x the compute of a ResNet-50.We then train a Support Vector Machine (SVM) using the RBF kernel to obtain a frame classifier.We use the Scikit-learn 68 SVM implementation with the parameter configuration given in Table 2.</p>
<p>Finally, we apply the classifier to frames of raw video sequences at a rate of 3 frames/second.We filter for videos that consist of at least 80% "clean" frames at this stage (Classes Minecraft Survival Mode -with Artifacts and None of the Above are both considered not clean).From this set, we apply a median filter (with a kernel size of 7) to the labels and segment videos by splitting the "clean" segments that are at least 5s in duration.The result of this is our final web_clean dataset.</p>
<p>A.3 early_game Dataset</p>
<p>The early_game dataset is a ∼3000 hour subset of web_clean targeted at "early game" Minecraft behavior, i.e. instances where players start in a fresh world with no items.We obtain the metadata text that accompanies the videos in web_clean and determine whether any of the following regular expressions match: • start</p>
<p>• beginning</p>
<p>• (new|fresh|clean).*(world|game|play)</p>
<p>• from scratch</p>
<p>From this set of videos, we take only the first 5 minutes of each video.</p>
<p>B Contractor Data B.1 Recording Contractor Play</p>
<p>Our contractors use a custom Minecraft recorder that we built that records their actions and game video feeds as they play.The recorder is implemented using the MCP-Reborn (github.com/Hexeption/MCP-Reborn)modding package.To ensure that the recorder environment is as close as possible to the Minecraft environment used for RL rollouts and evaluations (Appendix C), we use the same underlying game engine for both.The recorder is a Java app that runs in a window mode, with constant resolution of 1280x760.Brightness is set to 0 (the "gloomy" setting in Minecraft), which is the default setting.Other graphics settings (field of view, GUI scale) are fixed to the values used in the Minecraft environment (C.1); we explicitly prevented users from changing graphics settings.Unlike the environment, the recorder allows all keyboard key presses and continuous (as opposed to binned) mouse actions.On every game step (or "tick") the frame buffer used to display the game window is downsized to 640x360 and written into a video file.In-game actions are recorded in a separate JSONL file (a text file where each line is a JSON-formatted string).All recordings are chunked into 5 minute clips: after each 5 minute segment of contractor game play the recorder automatically uploads the video file, the JSONL file with actions, as well as a Minecraft state file.</p>
<p>To ensure that contractors cannot corrupt each other's data, we provided every contractor with an individual cloud bucket, as well as with credentials giving write access only to that bucket.Credentials also included adjective-adjective-noun names (e.g.grumpy-amethyst-chipmunk), generated with the namegenerator python package to ensure contractor anonymity when we publish the data.</p>
<p>B.2 Contractor Contract</p>
<p>We recruited contractors by posting the following offer on the UpWork freelancing platform.</p>
<p>"We are collecting data for training AI models in Minecraft.You'll need to install java, download the modified version of Minecraft (that collects and uploads your play data), and play Minecraft survival mode!Paid per hour of gameplay.Prior experience in Minecraft not necessary.We do not collect any data that is unrelated to Minecraft from your computer."</p>
<p>We had the applications open for a day, and then randomly selected 10 applicants for the first round of contractors.Later in the project, as we needed more data and as some contractors asked to terminate their contracts, we added more applicants from the original pool as well as referrals from the currently working contractors.The contractors were paid $20 per hour (minus Upwork platform fees and applicable taxes).All of the results presented in this paper are based on about 4,500 hours of data (including data recorded to gather statistics of human play that was not used for training), which cost us around $90,000.Over the course of the project, we collected some data we did not use due to bugs in the recorder and for some ideas we ultimately did not pursue.In total, we spent about $160k for contractor compensation over the course of the project.However, as we discuss in Sec.4.6, we could likely obtain most of our results with an IDM trained using only $2000 worth of data, i.e. the foundation VPT model, BC fine-tuning to the earlygame_keyword dataset, and the RL fine-tuning results.Collecting the contractor_house dataset cost about $8000.Because we used the IDM trained on about 2000 hours of contractor data, the actual cost of contractor data for those results was around $40,000.</p>
<p>In early stages of the project, we were planning to use contractor data solely for the purpose of training the IDM.As such, no specific tasks were given, other than "play the survival mode of Minecraft like you normally would."Later in the project, we requested that contractors perform specific tasks in Minecraft, such as:</p>
<p>• Collect as many units of wood as possible, using only wooden or stone tools (treechop)</p>
<p>• Start a new world every 30 minutes of game play • Build a basic house in 10 minutes using only dirt, wood, sand, and either wooden or stone tools (contractor_house, more details below in Appendix B.4). • Starting from a new world and an empty inventory, find resources and craft a diamond pickaxe in 20 minutes (obtain_diamond_pickaxe).This dataset was used to obtain statistics for how long it takes humans on average to complete this task (and the subtasks required to complete it) when obtaining a diamond pickaxe is their goal.</p>
<p>Since we only recorded in-game events and videos, the data does not include personally identifiable information.That being said, the contractors could theoretically use Minecraft's open-world property to generate personally identifiable information and/or offensive content (e.g. by using Minecraft blocks to write their name or offensive messages, then finding a spot from which the message would be visible).In practice, we have not seen any attempts to do so in the contractor videos that we watched.Of course, we train our BC models on videos from the internet of people playing Minecraft, and if such behavior is in those videos our model could also potentially learn it, although we expect such behavior is rare enough that our model would not be likely to reproduce it.</p>
<p>B.3 Data for the Inverse Dynamics Model.</p>
<p>Since the IDM's task is to infer actions given the video, any labelled data is appropriate for IDM training.In practice, we included general gameplay as well as the treechop task data described in the previous section, which amounted to a total of 1962 hours.Due to collecting datasets like contractor_house only at late stages of the project, they were not included in IDM training.</p>
<p>B.4 contractor_house.</p>
<p>The contractor_house contains about 420 hours of data.We asked contractors to build a basic house in 10 minutes, using only basic dirt, wood, and sand, blocks.Each trajectory starts in a newly generated world and a timer forcibly ends a trajectory after a 20 minute time limit.For this task, many contractors chose to begin their trajectories by crafting basic tools and building blocks, specifically it was common for the first 2 minutes to be spent crafting a wooden pickaxe and then mining stone for an assortment of stone tools before gathering more building blocks and beginning to create their structure.</p>
<p>C Minecraft environment details</p>
<p>Our Minecraft training environment is a hybrid between MineRL 27 and the MCP-Reborn (github.com/Hexeption/MCP-Reborn)Minecraft modding package.Unlike the regular Minecraft game, in which the server (or the "world") always runs at 20Hz and the client runs as fast as rendering can complete (typically at 60-100Hz), in our version the client and server run in the same thread at the same frequency.This allows us to run the environment slower or faster than real time, while avoiding artifacts like missing chunks of the world.The action and observation spaces are similar to those of MineRL environments and are described in more detail in the following subsections.</p>
<p>The environment also returns diagnostic information, such as in-game stats, contents of the agent's inventory, whether any in-game GUI is open, etc., which we use for tracking and recording but not as inputs to the models.The episode length is 10 minutes for RL experiments and 60 minutes for BC model evaluations.The agent can "die" in a number of ways, such as staying under water for too long and drowning, being killed by hostile mobs, or falling from a tall structure.We do not terminate the episode on agent "death".Instead, just as for humans in the regular Minecraft game, the agent drops all its items when it dies and respawns at a random spot close to the initial spawning spot in the same Minecraft world.The policy state is not masked on death, so the model can remember the fact that it has died and act accordingly.</p>
<p>C.1 Observation space</p>
<p>The environment observations are simply the raw pixels from the Minecraft game that a human would see.Unlike MineRL, we do not remove overlays like the hotbar, health indicators, and the animation of a moving hand shown in response to the attack or "use" actions.The field of view is 70 degrees, which corresponds to the Minecraft default.GUI scale (a parameter controlling the size of the in-game GUI) is set to 2, and brightness is set to 2 (which is not a Minecraft default, but is very frequently used in online videos).The rendering resolution is 640x360, which is downsampled to 128x128 before being input to the models.We empirically found 128x128 to be the smallest resolution for which in-game GUI elements are still discernible, and then chose that to minimize compute costs.Whenever an in-game GUI is open, we additionally render an image of a mouse cursor at the appropriate mouse position to match what a human player's operating system does (Fig. 12).</p>
<p>C.2 Action space</p>
<p>Our action space includes almost all actions directly available to human players, such as keypresses, mouse movements, and clicks.The specific binary actions we include are shown in Table 3.</p>
<p>One difference between the human action space and our agent's is that we disallow typing arbitrary letters, which is only useful for entering text into the search bar of the crafting recipe book.Humans can either do that or browse the recipe book with the mouse, the latter of which our agent can still do.However, because we do allow the agent to press letters that are also shortcuts for actions (e.g.outside of the GUI, the "W" key triggers the forward action) agents are able to press a few keys within the GUI (W, A, S, D, E, Q) that produce letters if the recipe book search bar is selected.We have not seen agents attempt to search the recipe book with these letters.Instead, our agents navigate the recipe book with the mouse or craft by dragging items around the crafting window.</p>
<p>drop</p>
<p>Q key Drop a single item from the stack of items the player is currently holding.If the player presses ctrl-Q then it drops the entire stack.In the GUI, the same thing happens except to the item the mouse is hovering over.</p>
<p>hotbar.[1-9] keys 1 -9 Switch active item to the one in a given hotbar cell.</p>
<p>Table 3: Binary actions included in the action space.https://minecraft.fandom.com/wiki/Controls has more detailed descriptions of each action.</p>
<p>In addition to the binary (on/off) keypress actions, our action space also includes mouse movements.As with human gameplay, when in-game GUIs are not open, mouse X and Y actions change the agent's yaw and pitch, respectively.When a GUI is open, camera actions move the mouse cursor.Mouse movements are relative (i.e. they move the mouse or camera relative to the current position, and thus their effect depends on the current position).</p>
<p>Inventory interaction in Minecraft requires fine-grained mouse movements to achieve tasks such as crafting and smelting, while mining and navigating the world can be achieved with coarser mouse action.To be able to achieve both with the same action space, we implemented mouse movements as a set of discrete actions with foveated binning along each axis (Fig. 13), which in preliminary experiments we found to improve crafting performance.</p>
<p>D Inverse Dynamics Model Training Details D.1 IDM Architecture</p>
<p>The IDM model has approximately 0.</p>
<p>Camera bin</p>
<p>Figure 13: Relative camera angle or mouse movement in pixels vs. action bin.The same binning is used for both X and Y coordinates.The binning is foveated, meaning that binning is more fine-grained for smaller movements and more coarse-grained for larger movements.There are 11 bins for each axis (X and Y).The center of each bin (indicated with green circles) is used when un-discretizing movements (that is, when converting from an action expressed as a bin to a camera angle or mouse movement).</p>
<p>as it incorporates neighboring temporal information immediately, and we show results comparing IDM performance with and without this layer in Figure 14.This comparison was made on the default (1962-hour) IDM dataset.Training Progress (Epoch) This initial temporal convolutional layer is followed by a ResNet 62 image processing network.In this part of the model, no extra temporal information is shared between neighboring frames; however, since each frame was first processed with the temporal convolution, some temporal information is present at this stage.The ResNet image processing network is comprised of three subsequent stacks with widths W = {64, 128, 128}.Each stack is comprised of, in order, (1) an initial 3x3 convolutional layer with 1-pixel zero padding at the embedding boundary (such that the outgoing embedding dimensions are the same as the incoming embedding dimension) with W output channels, (2) a 3x3 max pooling with stride 2 and padding 1 such that the embedding width and height are halved, and (3) two classic ResNet blocks as defined in He et al. 62 with each layer also having W output channels.
6 5 4 3 2 1 0 1 Mouse R 2 With 3D Conv No 3D Conv
The output of the ResNet stack is flattened into a 1-dimensional vector of size 2 17 = 131072 (one vector for each frame in the video) such that at this stage there are 128 vectors of size 131072.Each vector is independently processed with two frame-wise dense layers with 256 output activations and then 4096 output activations, respectively.The result is then fed through 4 subsequent non-causal (umasked) residual transformer 69 blocks.Each block first has an unmasked attention layer, i.e. frames may attend to future frames, with 32 attention heads of dimension 128 each and a surrounding residual connection that skips this layer.The embedding is then passed through a frame-wise dense layer with output dimension 16384 and another with output dimension returning to 4096; a single residual connection skips past this pair of frame-wise dense layers (not skipping past each layer separately, but skipping the pair).All dense layers have their weights tied through time, so each frame in the video is processed with the same weights.</p>
<p>Finally, independent dense layer heads for each action are pulled from the final embedding -a 2 class on/off categorical parameterized with a softmax for each available key as well as a 11-way categorical for both the discretized horizontal and vertical mouse movements (See Appendix C.2 for details on the action space).</p>
<p>Each dense layer or convolutional layer in the network is preceded by a layernorm 70 and followed by a ReLU non-linearity.Weights are initialized with Fan-In initialization 71 and biases are initialized to zero.</p>
<p>D.2 IDM Training</p>
<p>The total loss for the network is the sum of each independent action prediction loss (one for each key and one for both mouse directions).Each independent loss is the negative log-likelihood of the correct action.We use the ADAM 72 optimizer with a linear learning rate decay.We use an initial learning rate of 0.003, a batch size of 128 (where each item in the batch is a video sequence of 128 frames), and a weight decay of 0.01.Hyperparameters were tuned in preliminary experiments.The IDM is trained on our contractor collected dataset for 20 epochs.This took 4 days on 32 A100 GPUs.</p>
<p>We add data augmentation to each video segment; augmentations are randomly sampled once per segment such they are temporally consistent.Using the Pytorch 73 transforms library, we adjust the hue by a random factor between -0.2 and 0.2, saturation between 0.8 and 1.2, brightness between 0.8 and 1.2, and contrast between 0.8 and 1.2.We also randomly rotate the image between -2 and 2 degrees, scale it by a random factor between 0.98 and 1.02, shear it between -2 and 2 degrees, and translate it between -2 and 2 pixels in both the x and y dimensions.</p>
<p>Due the large computational cost of running all of the experiments in this paper, training results are from one run of training (for IDM, BC, and RL training): this non-ideal situation is mitigated because deep learning training tends to be low variance 74,75 and because we often have data points from sweeps (e.g. on dataset size) that suggest overall trends.</p>
<p>D.3 Generating Pseudo Labels with the IDM</p>
<p>Section 4.1 shows that inverse dynamics modeling is a much easier task than behavioral cloning because IDMs can be non-causal.The IDM is trained to simultaneously predict all 128 actions for each video sequence, so the IDM will effectively be causal for frames at the end of the video clip because future frames are not included in the sequence.For this reason, we apply the IDM over a video using a sliding window with stride 64 frames and only use the pseudo-label prediction for frames 32 to 96 (the center 64 frames).By doing this, the IDM prediction at the boundary of the video clip is never used except for the first and last frames of a full video.</p>
<p>E Foundation Model Behavioral Cloning E.1 Foundation Model Architecture</p>
<p>The behavioral cloning model architecture is the same as the IDM architecture described in Appendix D.1 except that we modify the architecture so that it is causal (i.e.cannot see the future when making predictions).This means the BC architecture does not have the initial non-causal convolution the IDM has (this layer is omitted completely).Furthermore, the residual transformer layers are now causally masked (as is standard in language modeling) and we do Transformer-XL-style 76 training where frames can attend to keys and values from past batches within the same video.We also use a Transformer-XL-style relative attention position embedding.</p>
<p>E.2 Null Action Filtering</p>
<p>The most common action humans take is the null action (no keypresses or mouse movements), which accounts for 35% of all actions they take.Among other reasons, a player may take the null action to wait for something in the game to finish, to pause between actions, or to take a break to grab a glass of water.Early on in the project we found that the BC model would take a much larger fraction than 35% of null actions, often upwards of 95%.In order to prevent this behavior we removed frames with null actions from the dataset.We compare a few different treatments: we filter nulls if there have been 1, 3, or 21 frames of consecutive null actions, and include a treatment that does not perform any null filtering.Null action filtering generally helps, increasing all crafting rates (Figure 15 left).Filtering only groups of 3 performed slightly better than filtering all null action or groups of 21.Initial experiments indicated that filtering all null actions was better; however, after further model tuning and after we had already trained our largest models, we found that filtering only groups of 3 or more null actions performed best.Due to compute constraints we were not able to redo all experiments with this setting, but doing so would be a reasonable choice for any future work.</p>
<p>E.3 Joint Hierarchical Action Space</p>
<p>We originally worked with a factored action space, where each keypress could be independently on or off, and this choice was independent of whether the mouse was being moved.This could cause issues for modeling the human behavior distribution exactly.Say for a given state, humans either with 50% probability (a) move forward and attack or with 50% probability (b) move left and drop their item.The best a factored distribution can do is to assign 50% probability to each of the 4 constituent actions because it chooses to press each button simultaneously and independently.See Appendix C.2 for details on the entire action space.</p>
<p>For this reason, we implemented a joint distribution over actions; however, the full joint distribution over 20 binary buttons and two mouse movement dimensions discretized into 11 bins each would result in in 2 20 × 11 2 ≈ 1.2 × 10 8 possible combinations.This is far too large for many reasons, e.g. the final layer from the transformer stack with a dimension of 4096 would need to be mapped to each combination resulting in 4096 × 1.2 × 10 8 ≈ 5.2 × 10 11 parameters for this final layer alone.In order to reduce this we noted that many buttons in Minecraft have no effect when simultaneously pressed; for example, if a player tries to move forward and backward at the same time, they remain in place.Below we list the the sets of mutually exclusive actions.Furthermore, the inventory button is exclusive with all other buttons and mouse movement.</p>
<p>Mutually Exclusive Actions forward, back left, right sprint, sneak hotbar.[1-9]   Even reducing the joint action space to reflect these mutually exclusive combinations still results in a huge action space when combined with the discretized mouse movements, i.e. 3 3 ×10×2 4 ×11 2 +1 ≈ 5.2 × 10 5 .This calculation results from 3 3 for the 3 sets of 2 mutually exclusive keys above where taking neither in the set is an option, ×10 for the 9 hotbar keys or no hotbar keypress, ×2 4 for the remaining binary 4 keys: use, drop, attack, and jump, ×11 2 for mouse movements, and finally +1 for the inventory button which is mutually exclusive with all other actions.∼ 5.2 × 10 5 is still quite large so we chose to implement a hierarchical binary action for camera being moved or not.If this action is on, then there is a secondary discrete action head with 121 classes (the joint distribution of mouse movements because each discretized mouse direction has 11 bins) that determines where to move the mouse.If the hierarchical action is off, then there is no mouse movement, loss for the secondary mouse movement action is masked during training, and the secondary action head need not be sampled during evaluations.While this no longer models the full joint distribution, it is quite a bit better than the factored action space since dependencies between keypresses as well as whether or not to move the mouse (although not which mouse movement) are modeled jointly.The resulting action space has dimension 3 3 × 10 × 2 4 × 2 + 1 = 8461 (the 11 2 dimensional multiplier for camera movement has been replaced by a multiplier of 2 here, corresponding to a binary action for whether or not to move the mouse) with an additional 121-dimension head for the joint camera movements.In the future it would be interesting to implement sequential conditional action spaces to more completely model the joint distribution.</p>
<p>In Figure 15 (right) we compare environment rollout performance between BC models with the hierarchical joint action space and with the factored action space.Environment statistics are fairly comparable; however, we see that the factored action space model samples far more null actions.This is an important example of the factored action space failing to correctly model the distribution in the dataset because, due to null action filtering, there are 0 null actions in the dataset these models train on.Despite this, the factored model samples many null actions because the prediction for each key is not conditioned on other keypresses.</p>
<p>E.4 Foundation Model Training</p>
<p>The foundation model training is similar to the IDM training, with the exception of labels being IDM-generated pseudo labels.The hyperparameters used for foundation model training are listed in Table 4.</p>
<p>Hyperparameter</p>
<p>F Behavioral Cloning Fine-Tuning</p>
<p>Behavior cloning fine-tuning is similar to the foundation model training, except we either use a focused subset of all the videos (early_game dataset, described in A.3) with pseudo labels, or contractor data (contractor_house dataset, described in B.4) with ground-truth labels.The hyperparameters used for behavior cloning fine-tuning are listed in Table 5.We used 16 A100 GPUs for about 6 hours when fine-tuning on contractor_house dataset, and 16 A100 GPUs for about 2 days when fine-tuning on early_game dataset.RL experiments were performed with the phasic policy gradient (PPG) algorithm, 64 an RL algorithm based on the proximal policy optimization (PPO) algorithm 77 that increases sample efficiency by performing additional passes over the collected data to optimize the value function as well as an auxiliary value function.These algorithms have been described extensively in previous work, 64,77 so here we describe them only briefly.A major inefficiency when training on-policy algorithms is that, to remain on-policy, one can only take a single gradient step before new rollout data needs to be gathered to continue optimization.To alleviate the potentially destructive effects of taking multiple optimization steps in a single iteration, PPO prevents the policy from changing too much in a single step by clipping the loss when the difference between the current policy and the policy before the update becomes too large. 77We also use generalized advantage estimation (GAE), which can speed-up credit assignment by looking more than 1 step into the future when determining the advantage of an action, with the look-ahead being determined by hyperparameter λ. 78 PPG improves the sample efficiency of PPO when the policy and value function share the same network by following different optimization processes for the policy, the value function, and their shared representation.PPG splits optimization in two phases: a wake phase and a sleep phase.In the wake phase, the policy and value function are optimized as in normal PPO training, with the only exception being that every sample is used at most once, which prevents the policy from overfitting on these samples.In the sleep phase PPG optimizes the value function and an auxiliary value function (which is optimized with the exact same loss as the regular value function, but its output is never used during training), while keeping a Kullback-Leibler (KL) divergence loss to the policy before the start of the sleep phase to ensure that the policy does not change.Because the policy is not optimized in this step, PPG does allow samples to be reused multiple times in this phase.The assumption behind optimizing the value function during the sleep phase is that value function optimization is less sensitive to being trained multiple times on the same sample.Optimizing the auxiliary value function does not directly affect either the value function or the policy, but it can improve the shared representation of both functions (the assumption being that predicting the value-function requires encoding all features that are important for distinguishing states).The coefficients for the three losses (value function loss, auxiliary value function loss, and KL loss) are listed in Table 6.In our experiments a single iteration consists of two sleep cycles and one wake cycle.</p>
<p>Hyperparameter</p>
<p>Because the value and auxiliary value functions are not optimized during BC pre-training, they are initialized at the start of RL fine-tuning.Each value function is implemented as a single, fully connected layer on top of the last residual transformer block of the pretrained model (Appendix D.1).The weights of the auxiliary value function are randomly initialized while the weights of the regular value function are initialized with zero weights, which appeared to prevent destructive updates early in training that could happen with a randomly initialized value function.To prevent the value-function loss from having gradients that depend greatly on the magnitude of the reward, we normalize the value-function target by subtracting the mean and dividing by the standard deviation, which are estimated through an exponentially weighted moving average.</p>
<p>To prevent catastrophically forgetting the skills of the pretrained network when RL fine-tuning, we apply an auxiliary KL divergence loss between the RL model and the frozen pretrained policy. 10This loss is defined as:
L klpt = ρKL(π pt , π θ )(2)
Where π θ is the the policy being trained, π pt is the frozen pretrained policy, KL(π pt , π θ ) is the Kullback-Leibler divergence between the policy being trained and the pretrained policy, and ρ is a coefficient to weight this loss relative to other losses.</p>
<p>In the fine-tuning experiments, this KL divergence loss replaces the common entropy maximization loss, which is often added to RL experiments to encourage exploration. 79,80The idea behind entropy maximization is that, when all actions appear to have equal value, such as when the agent has not learned about the next reward, it should maximize its entropy to increase the chance that it discovers the next reward.Blindly exploring by maximizing entropy is effective when the state and action spaces are sufficiently small or the reward is sufficiently dense, but becomes infeasible when the state and action spaces are large and rewards are sparse, which is the case in the diamond-pickaxe task.Instead of blindly exploring through uniform-random actions, we assume that the pretrained policy has an action distribution that is much more likely to take sequences of actions that lead to interestingly new states, and thus, in states where the agent assigns equal value to each of its actions, it should mimic the action-distribution of the pretrained policy instead of a uniform-random action distribution.In experiments with a randomly initialized policy we do include the entropy maximization loss with a coefficient of 0.01, which has been an effective setting in other Minecraft work. 30Empirically, we found that a high coefficient ρ for this KL divergence loss would prevent the agent from properly optimizing the reward function while a low coefficient ρ was ineffective at  6: Hyperparameters for RL experiments.These are the hyperparameters for all treatments with two exceptions.First, when fine-tuning from the early-game model without a KL divergence loss, in addition to the KL divergence loss being set to 0, the learning rate was set to 3 × 10 −6 (the best setting out of a sweep over 5 different learning rates), as we that performance was substantially lower with the standard learning rate of 2 × 10 −5 and the agent did not even learn to collect logs.We suspect that the reason that the learning rate needed to be lowered when fine-tuning without a KL loss is that the KL loss prevents making optimization steps that change the policy too much in a single step, especially in early iterations when the value function has not been optimized yet, and the KL loss thus makes it possible to optimize with a higher learning rate.Second, when running RL from a randomly initialized policy there is no KL divergence loss or KL divergence decay, but instead we use an entropy bonus of 0.01, which reportedly worked well in previous work. 30otecting the learned skills of the pretrained policy and preventing catastrophic forgetting.As such, we start with a relatively high coefficient ρ and decay it by a fixed factor after each iteration (Table 6).This method protects policy skills in early iterations while guaranteeing that the policy can eventually maximize the reward function, regardless of how different its behavior has to be to do so relative to the pretrained policy.</p>
<p>For the reward function we estimated the rough quantities of each item that a human player might gather when trying to craft a diamond pickaxe, and we reward the model for gathering up to that quantity for each item.We started these estimates by iterating over the technology tree backward from a diamond pickaxe and adding the requirements for each item to the reward function (e.g.first we added a diamond pickaxe to the reward function, then we added the 3 diamonds and 2 sticks required for crafting a diamond pickaxe, then we added the 1 iron pickaxe required for mining diamonds, and so on).Then we added coal and torches to the reward function, with coal being useful as fuel when smelting iron and for crafting torches while the torches themselves improve visibility and prevent enemies from spawning.Finally, we reward the model for bringing additional logs (5 logs are required to craft all items in the reward function, but we reward up to 8 logs), which can be used as fuel or crafted into a crafting table or sticks if the agent runs out.In practice the agent rarely collects the additional logs, places the torches, or uses coal as fuel when smelting, but the reward function was based on human expectations on what would be useful to execute this task, rather than designed around how an RL model behaves after training.Finally, to encourage the agent to keep mining diamonds and crafting diamond pickaxes after it has crafted its first diamond pickaxe, we did not put a limit on the number of diamonds or diamond pickaxes that would be rewarded.</p>
<p>The rewards for the different items are separated into 4 tiers, roughly depending on how late a player would usually get the relevant item.The first tier consists of all wooden and stone items and has a base reward of 1, the second tier consists of all items requiring coal with a base reward of 2, the third tier consists of all items requiring iron with a base reward of 4, and the final tier is diamond with a base reward of 8. Thus items later in the sequence of items towards a diamond pickaxe generally give a higher reward.To make sure that the agent does not over-value items that are supposed to be gathered in bulk (e.g. the agent is rewarded for up to 20 planks but only up to 1 crafting table, which can cause the agent to focus on planks at the expense of creating a crafting table), we divide the base reward of each item by the total quantity that the agent gets rewarded for (for the purpose of determining the reward, the total quantity for diamonds is 3 and the total quantity for diamond pickaxes is 1, even though we did not put a limit on the number of these items being rewarded).For example, the agent is rewarded for 3 iron ore, which has a base reward of 4 for being in the iron tier and up to 3 blocks of iron ore are rewarded, thus the reward per block of iron ore is 4/3.The quantity and reward for each item are listed in Table 7.</p>
<p>While every item in the sequence towards a diamond pickaxe is rewarded, the reward function is still sparse and, in some cases, even deceptive.The sparsity comes from the fact that it can take thousands of actions to find the next reward, even after the agent has acquired all the necessary prerequisites (e.g.human players often take more than 10,000 actions to find a diamond after crafting an iron pickaxe).The reward function can be deceptive when the most efficient method for getting one item can make it far more difficult to get the next item.For example, a good strategy for the agent to craft a stone pickaxe quickly is to mine (i.e.spend a few seconds to pick up) its crafting table after crafting a wooden pickaxe, such that the agent has immediate access to a crafting table as soon as it has collected enough cobblestone.However, the fastest way to get a reward for gathering cobblestone is to mine down immediately after crafting a wooden pickaxe, while leaving the crafting table behind.Thus following the optimal strategy for gathering cobblestone makes it more difficult to learn to craft a stone pickaxe.</p>
<p>Experiments ran for approximately 6 days (144 hours) on 80 GPUs (for policy optimization) and 56,719 CPUs (mostly for collecting rollouts from Minecraft).In this time the algorithm performed roughly 4,000 optimization iterations and collected roughly 1.4 million Minecraft episodes consisting of 12,000 frames each, for a total of 16.8 billion frames.</p>
<p>G.2 Reinforcement Learning Fine-Tuning Additional Data</p>
<p>Additional figures that are helpful for understanding the main results of the RL fine-tuning experiments are presented in this section.First, we show the items-over-training figure when RL fine-tuning from the early-game model without a KL loss (Fig. 16).When training without a KL loss, the model only learns to obtain the four items that the early-game model is capable of getting zero-shot, which are logs, planks, sticks, and crafting tables.Second, we present preliminary experiments in which we directly compare RL fine-tuning from the house-building model and RL fine-tuning from the early-game model (Fig. 17).These experiments differ from the main experiments in that, for both treatments shown here, the KL loss coefficient was set to 0.4, the learning rate was set to 6 × 10 −5 , and the reward for each item was 1/quantity for all items (i.e.items closer to the diamond pickaxe did not have an increased reward).While RL fine-tuning from the house-building model initially worked better than RL fine-tuning from the early-game model, fine-tuning from the early-game model worked better after 800,000 episodes and showed signs of smelting iron ingots, which is why the early-game model was chosen for the main experiments.In contrast to the treatment with a KL-penalty, it does not learn any items beyond these initial four, likely because skills that are not performed zero-shot, and for which the model thus does not initially see any reward, are catastrophically forgotten while the first four items are learned.</p>
<p>H Foundation Model Scaling</p>
<p>In early experiments we found that increasing model size led to models staying in the efficient learning regime longer into training. 63Here we compare the 0.5B model described in Section 4.2 to both a 248M and 71M parameter model.Both of these models are trained for 15 epochs as compared to the 30 epochs the 0.5B model trained for.These models have the same architecture as the 0.5B model but each layer in the 248M parameter model has 1/2 the width and each layer in the 71M parameter model 1/3 the width.The 71M model was trained with an initial learning rate of 0.001586, batch size of 480, and weight decay of 0.044506.The 248M model had an initial learning rate of 0.001831, batch size of 640, and weight decay of 0.051376.</p>
<p>In Figure 18 we show validation loss on web_clean with IDM pseudo-labels, loss on the contractor dataset used to train the IDM with ground truth labels collected during contractor play, and zero-shot environment performance for the 71M, 248M, and 0.5B models.While larger models have better validation loss on web_clean, these results do not tell the clear story that the 0.5B model is better than its smaller counterparts.The 71M model has the lowest contractor dataset loss while having the highest web_clean loss, and it also has the best zero-shot environment performance.In fact, we see that the 71M model even had non-zero wooden tool crafting (Fig. 18 bottom left).The 248M model also appears to be better at crafting than the 0.5B, and also has lower contractor dataset loss.</p>
<p>While the zero-shot results suggest smaller models are better, fine-tuning tells another story.When fine-tuning to contractor_house, model size rank ordering reverses and now the 0.5B model performs best both in validation loss (Fig. 19 left) and in environment performance (Fig. 19 right)  followed by the 248M model and then the 71M model.Environment model rollouts are performed using the same game engine that we use to collect contractor data, which could be visually distinct from videos taken from the web.It is plausible that the larger models overfocus on the visual peculiarities in web data during pretraining since they have worse contractor data loss (Fig. 18 top middle), and this causes them to perform more poorly in the environment zero-shot.However, we hypothesize that because the contractor_house dataset we fine-tune to is collected from our game engine, the larger models that are a better overall Minecraft prior (as indicated by lower web_clean validation loss in Fig. 18 top left) can quickly shift their low level features to perform better on data coming from our game engine, resulting in better environment rollout performance.This hypothesis is further supported by Fig. 19 (middle) showing loss on the contractor dataset collected for IDM training, which has no overlap with contractor_house.After just a few steps of fine-tuning to contractor_house, all models quickly improve in loss on the full IDM contractor dataset, with larger models now performing best.While not conclusive, we believe this investigation provides some intuition for future studies of model scaling for sequential decision making problems.</p>
<p>I Text Conditioning</p>
<p>Goal-conditioned policies 81,82 make it possible for a single agent to perform a wide variety of goals in a single environment, which is particularly relevant in open-ended environments such as Minecraft.In recent work, goal specification has increasingly taken the form of domain specific languages 83 , or even natural language 84,85 .The benefits of language-conditioned agents can be tremendous, especially natural-language-conditioned agents, as their goal space contains a wide variety of potentially very complex tasks.Text conditional models have shown an amazing ability to perform tasks zero-shot (or learn them few-shot) including generalizing in impressive ways via the compositional and combinatorial possibilities allowed by natural language (e.g.GPT 1 and DALL•E 2 86 ).We hypothesize that we should expect similar capabilities to emerge with natural-languageconditioned virtual agents, if they are similarly trained on enormous amounts of data (that goes from a natural language description to a sequence of actions that completes the specified goal).In this section we take preliminary steps toward that future.Our preliminary experiments provide evidence that it is possible to pretrain a natural-language-conditioned model for Minecraft using the general approach presented in this paper (VPT) plus conditioning on the speech that often accompanies videos.</p>
<p>In online videos, the human actor sometimes indicates their intent in their verbal commentary (e.g."Let's go chop some trees to make a wooden axe" or "now let's learn how to crop photos in Photoshop").Conditioning on this closed caption data could produce a steerable pre-trained model: i.e., it may later be possible to condition the model with text such as "I am going to craft a wooden pickaxe" or "I am going to build a house," and have the agent perform those tasks specifically rather than simply follow typical human behavior (as was investigated in the rest of this paper).An alternate way to produce a steerable agent is via RL fine-tuning, which we could have done in Section 4.4 by adding a bit indicating the task to be completed, as has been done in prior work 30 .However, conditioning on natural language offers many benefits over that approach.First, it is flexible and powerful, being able to express any task.Second, one does not need to preconceive of the task to be completed ahead of time.This would allow for general, capable, zero-shot agents like GPT, but extending those capabilities to embodied tasks such as completing tasks on computers or in simulated 3D worlds.Third, text conditioning can be used even when tasks are difficult to specify via reward functions (e.g."Let's build a house" or-if the agent is capable of doing it-more complex things like "I will now build a castle surrounded by a moat").In the limit, VPT+text could conceivably produce powerful, capable, natural-language-conditional agents with the powers of GPT to meta-learn, follow instructions, and complete tasks zero or few shot, but in the form of agents that can act in virtual worlds, complete tasks on computers, and in other similar embodied sequential decision domains.We do not reach those lofty goals in this work, but we began a first step towards exploring in that direction.</p>
<p>Many Minecraft videos feature audio commentary from the player.This commentary is sometimes present in the form of closed captions for the videos, or could be extracted post-hoc using automated speech recognition (ASR). 87Our dataset features about 17k hours of content with associated closed captions.</p>
<p>We fine-tuned the 220 million parameter VPT foundation model used in the RL-fine-tuning experiments (chosen vs. 0.5B for the same reason: to reduce compute costs) with an additional text-conditioning input on the subset of our data for which closed captions are available.To obtain the conditioning input, we first split videos into 30 second chunks.The same text is associated with every frame in a given chunk, and is made up of all the closed captions occurring within that chunk, as well as the line of text preceding and following the chunk (if any).Because the vast majority (around 95%) of our closed caption data lacks capitalization and punctuation, it is punctuated using the rpunct library 88 .We then obtain a text embedding vector of length 4,096 from the OpenAI embedding API 89 , which is processed by a randomly initialized multi-layer perceptron (MLP) with two hidden layers of size 2,048.The resulting activations are added for each frame to the pretrained model activations before the transformer layers (pretransformerActivations += mlp(textEmbedding)).The model is fine-tuned for four epochs.</p>
<p>Our model shows evidence of steerability.When conditioned on sentences that incite the agent to explore (such as "I'm going to explore" and "I'm going to find water") the agent travels significantly farther from its spawn point (Figure 20a).Additionally, we can steer the agent to preferentially collect early game items such as seeds, wood, and dirt by conditioning with text such as "I'm going to collect seeds/chop wood/collect dirt" (Figure 20b,c,d).</p>
<p>While our results show some level of steerability, more work is required to increase it.For example, we were not able to successfully steer agents to gather flowers or to hunt, both of which are possible in the early game, but less common (and, in the case of hunting animals, much more difficult) than gathering dirt, wood, or seeds.Likewise, an experiment in which the agent is presented with a crafting window and various resources, and conditioned to craft a given item (e.g."I'm going to craft a wooden axe") failed to show that the conditioning had a significant effect on which items got crafted.Instead, it seemed the agent was more influenced by the prior, unconditional probability of what human players would craft next given the resources available, which is not too surprising since in Minecraft, especially in the early game, there is a relatively consistent path to gathering resources in a specific order go produce more powerful tools (Fig. 6).For example, if the agent had the resources to make a stone pickaxe and we asked it instead to make a (weaker) wooden pickaxe, it often would make the stone pickaxe anyway.Finally, looking at videos of agent behaviors failed to convince us that the "house" conditioning causes the agents to take more steps towards building a house than other variants.</p>
<p>Thus, our results show that it is possible to train a somewhat steerable natural-language-conditioned agent.However, its steerability is still too weak to be practically useful, and it is far from what we believe could be accomplished with more research, data, and training compute.Another exciting research direction is to have the model predict future text as well as just the next action.</p>
<p>Figure 1 :
1
Figure 1: Example Minecraft crafting GUI.Agents use the mouse and keyboard to navigate menus and drag and drop items.</p>
<p>Figure 2 :
2
Figure 2: Video Pretraining (VPT) Method Overview.</p>
<p>Figure 3 :
3
Figure 3: (Left) IDM keypress accuracy and mouse movement R 2 (explained variance 61 ) as a function of dataset size.(Right) IDM vs. behavioral cloning data efficiency.</p>
<p>Figure 3 (Figure 4 :
34
Figure3(right) validates our hypothesis that IDMs are far more data efficient than BC models, likely because inverting environment mechanics is far easier than modeling the entire distribution of human behavior.The IDM is two orders of magnitude more data efficient than a BC model trained on the same data and improves more quickly with more data.This evidence supports the hypothesis that it is more effective to use contractor data within the VPT pipeline by training an IDM than it is to train a foundation model from contractor data directly (Sections 4.5 and 4.6 provide additional evidence).</p>
<p>Figure 5 :
5
Figure 5: (Left) Collection and crafting rates for three policies: the zero-shot VPT foundation model, and the VPT foundation model BC fine-tuned to the earlygame_keyword or contractor_house datasets.BC fine-tuning to either dataset improves performance, including (for the contractor_house dataset) yielding wooden and stone tools.Proficient Minecraft players take a median of 1.2 minutes (1390 actions) to construct wooden tools and 2.3 minutes (2790 actions) to construct stone tools.(Right) Collection and crafting rates for VPT foundation model snapshots throughout training after they are BC fine-tuned to the contractor_house dataset.In general, crafting-related behaviors increase throughout foundation model training.Fig. 4 defines the other task terms (logs, planks, crafting tables, and total crafting).</p>
<p>Figure 7 :
7
Figure 7: RL Fine-tuning results.(a) RL from a randomly initialized model fails to get almost any reward, RL fine-tuning from the VPT foundation model performs substantially better with a reward near 13, and RL fine-tuning from the early-game model performs best with a reward of 25.When training the early-game model without a KL loss to the original policy (No KL-loss) progress stalls after 100,000 episodes, suggesting that the skills necessary to make further progress have been catastrophically forgotten.(b) RL from a randomly initialized model occasionally collects sticks by breaking leaves (an easy but inefficient method of getting sticks that does not require logs or planks) and never learns to reliably collect logs.(c) RL fine-tuning from the VPT Foundation model learns everything in the curriculum up to iron ore and making furnaces, but fails to learn to use the furnace to smelt iron ingots.(d) RL fine-tuning from the early-game model learns to obtain (at human-level) all items in the sequence towards a diamond pickaxe and crafts a diamond pickaxe in 2.5% of episodes.</p>
<p>Figure 8 :
8
Figure 8: (Left) Zero-shot rollout performance of foundation models trained on varying amounts of data.Models to the left of the dashed black line (points ≤1k hours) were trained on contractor data (ground-truth labels), and models to the right were trained on IDM pseudo-labeled subsets of web_clean.Due to compute limitations, this analysis was performed with smaller (71 million parameter) models except for the final point, which is the 0.5 billion parameter VPT foundation model.(Right) The corresponding performance of each model after BC fine-tuning each model to the contractor_house dataset.</p>
<p>Figure 9 :
9
Figure 9: Zero-shot performance of BC models trained from scratch on the earlygame_keyword dataset labeled with IDMs that were trained on increasing amounts of contractor data.</p>
<p>Figure 10 :
10
Figure 10: Amazon Mechanical Turk worker interface showing an example labeling task</p>
<p>Figure 12 :
12
Figure 12: (Left) Sample of a Minecraft frame in the original resolution (640x360) with an in-game GUI open.The mouse cursor can be seen in the center of the image.This particular GUI shows the player's inventory and can be used to craft very basic items.(Middle) We downsample images to 128x128 for computational reasons.Shown is a downsampled observation with an in-game GUI for crafting.This is the resolution used by our models.(Right) A 128x128 observation as seen by our models without in-game GUI.The health, hunger, hotbar overlays, and agent hand can be seen in the lower part of the image.</p>
<p>Figure 14 :
14
Figure 14: Effect of 3-D Convolution in the IDM Architecture.</p>
<p>b a s ic m in in g lo g s p la n k s c r a f t in g t a b le s t o t a l c r a f t in g n u ll a c t io n s 10 Figure 15 :
1015
Figure 15: (Left) Effect of Null Action Filtering during training.We compare environment metrics and number of sampled null action during rollouts (rightmost group of columns) for the following treatments: no null action filtering (blue), filtering all null actions (green), filtering only groups of 3 or more null actions (red), and filtering only groups of 21 or more null actions (purple).(Right) Hierarchical versus Factored Action Spaces.</p>
<p>Figure 16 :
16
Figure 16: Items obtained when RL fine-tuning from the early-game model without a KL loss.The model learns to obtain all items that the early-game model can craft zero-shot, which are logs, planks, sticks, and a crafting table.In contrast to the treatment with a KL-penalty, it does not learn any items beyond these initial four, likely because skills that are not performed zero-shot, and for which the model thus does not initially see any reward, are catastrophically forgotten while the first four items are learned.</p>
<p>Figure 17 :
17
Figure17: Preliminary experiments when RL fine-tuning from the early-game model compared to RL fine-tuning from the house-building model.(Left) While reward initially increases faster when fine-tuning from the house-building model, fine-tuning form the early-game model eventually obtains a slightly higher reward.(Right) RL fine-tuning from the early-game model has a higher likelihood of smelting an iron-ingot, which is why the early-game model was chosen for future RL fine-tuning experiments.</p>
<p>Figure 19 :
19
Figure 19: contractor_house fine-tuning performance versus model size.(Left) Loss on the contractor_house holdout validation set.(Middle) Loss on the full contractor dataset collected to train the IDM; this dataset is disjoint from contractor_house.(Right) Environment rollout performance at the end of fine-tuning.</p>
<p>1 .
1
Minecraft Survival Mode -No Artifacts: Video frames (images) that correspond to the Minecraft Survival game mode that do not contain any non-game visual artifacts (e.g.subscribe buttons, channel logos, advertisements, picture-in-picture of the narrator, etc.).2.Minecraft Survival Mode -with Artifacts: Video frames (images) of theMinecraft Survival game mode that include such visual artifacts.Survival Mode Valid survival mode videos have health/hunger bars and an item hotbar at the bottom of the screen.</p>
<ol>
<li>None of the Above: Video frames (images) that are not from the Minecraft survival game mode, including those from other Minecraft game modes such as creative mode or even other games/topics entirely.The full set of instructions workers received are as follows (note that we also included multiple image examples from each category in the worker instructions, similar to the sample subset provided in Figure11):Please help us identify screenshots that belong only to the survival mode in Minecraft.Everything else (Minecraft creative mode, other games, music videos, etc.) should be marked as None of the above.Survival mode is identified by the info at the bottom of the screen:• a health bar (row of hearts)• a hunger bar (row of chicken drumsticks) • a bar showing items held</li>
</ol>
<p>Table 2 :
2
68ature Extraction Details and SVM Configuration.The parameters are for the SVM implementation in Scikit-learn68.
CLIP Model SpecificationRN50x64 (see text)CLIP Input Image Resolution448x448x3CLIP Embedding Feature Length 1024KernelrbfSVM ParametersC20Gamma scaleClass 1 2200Sample SizeClass 2 2200Class 3 4400• (ep|episode|eps|day|session|sesh|chapter|chap.|series|part|parte|pt|round|day|tâ . p|bölüm|episodio|epizod|pizod)( )*(.1|#1|1|.01|#01|01|one[ˆ0-9]|$)</p>
<p>5 billion trainable weights.The input to the IDM is 128 consecutive image frames (128 frames of video), each of which has dimensions 128 × 128 × 3.</p>
<p>The IDM is tasked with predicting the action at each frame.All image pixel values are first divided by 255.0 such that they lie within the range [0, 1].The first layer of the IDM is a 3-D convolution with 128 learnable filters with a temporal kernel width of 5 and spatial kernel widths of 1.This convolution is non-causal, meaning that embeddings at time index t are functions of pixel values at times t − 2, t − 1, t, t + 1, and t + 2. We found this layer to be extremely important in IDM training</p>
<p>Table 4 :
4
Hyperparameters for foundation model training
ValueLearning rate0.002147Weight decay0.0625Epochs30Batch size880</p>
<p>Table 5 :
5
Hyperparameters for behavior cloning fine-tuning
ValueLearning rate0.000181Weight decay0.039428Epochs2Batch size16G Reinforcement Learning Fine-Tuning
G.1 Reinforcement Learning Fine-Tuning Training Details</p>
<p>Table 7 :
7
Reward per item and total quantity rewarded.
ItemQuantity rewarded Reward per itemLog81/8Planks201/20Stick161/16Crafting table11Wooden pickaxe11Cobblestone111/11Stone pickaxe11Furnace11Coal52/5Torch161/8Iron ore34/3Iron ingot34/3Iron pickaxe14Diamondinf8/3Diamond pickaxeinf8</p>
<p>Figure 18: Training and Zero-Shot Performance versus Model Scale.In the first two plots the x-axis is compute normalized to that used by the 71M parameter model, such that after 15 epochs of training the 71M model has used 1 "compute".The 248M parameter model and the 71M model are trained on the same amount of data (15 epochs), and the 0.5B parameter model is trained on 30 epochs of data.(Top Left) Loss on the web_clean validation dataset.(Top Middle) Loss on the IDM contractor dataset; note that these models were trained only on web_clean and not on any contractor data.(Top Right) Zero-shot environment rollout performance at the end of training.(Bottom) Zero-shot environment rollout performance over training for the 71M model (bottom left), 248M model (bottom middle), and 0.5B model (bottom right).
Web Clean Loss2.5 4 6 10 10 110 3 Loss Web Clean Validation Dataset 10 2 10 1 10 0 ~Compute Zero-Shot Performance over Training (71M Parameter Model)10 1 71M 248M 0.5BIDM Contractor Dataset Loss4 6 10 2.5 10 110 3 Zero-Shot Performance over Training 10 2 10 1 10 0 Loss on IDM Contractor Dataset ~Compute (248M Parameter Model)71M 248M 0.5B 10 1Collected or Crafted10 1 10 0 10 1 10 2 10 1Zero-Shot Performance vs Model Size basic mining logs planks crafting tables total crafting Zero-Shot Performance over Training (0.5B Parameter Model)71M 248M 0.5BCrafting or Collection0 10 3 10 2 10 1 10 0basic mining logs plankscrafting tables total crafting wooden tools0 10 3 10 2 10 1 10 00 10 3 10 2 10 1 10 0024 Training Progress (Epoch) 6 8 10 121416024 Training Progress (Epoch) 6 8 10 1214160510 Training Progress (Epoch) 15 2025contractor_house Validation Loss2.2 2.4 2.6 2.8 3.00.00.51.0 Fine-Tuning Epoch 1.5 2.0 Loss on contractor_house 2.53.0 71M 248M 0.5BLoss on IDM Contractor Dataset2.4 2.5 2.6 2.7 2.8 2.9 3.00.00.5 Loss on full IDM Contractor Dataset 1.0 1.5 2.0 2.5 Fine-Tuning Epoch3.0 71M 248M 0.5BCollected or Crafted10 3 10 2 10 1 10 0 10 1 10 2 10 3 10 4basic mining Fine-Tuning to Contractor House Dataset logs planks crafting tables total crafting wooden tools stone tools 71M 248M 0.5B
(iv)  Sample videos: https://www.youtube.com/playlist?list=PLNAOIb_agjf3U3rSvG_BCWqJ869NdBhcP
AcknowledgementsWe thank the following people for helpful discussions and support: Bob McGrew, Ken Stanley, Joel Lehman, Ilya Sutskever, Wojciech Zaremba, Ingmar Kanitscheider, David Farhi, Glenn Powell, Jonathan Gordon, and the OpenAI supercomputing team, especially Christian Gibson, Ben Chess, and Christopher Berner.Supplementary Information A Collecting Internet Data A.1 Initial Unclean Dataset CurationOur goal was to curate a video dataset of Minecraft gameplay from the survival game mode.Additionally, we prefer the data come from game modes as close as possible to our evaluation environment, meaning preferably coming from Minecraft version 1.16, being on a computer (which uses a mouse and keyboard vs. video game controllers with keypads and other buttons), being single-(vs.multi-) player, and having the default look of the game (vs.modifications that alter that style, such as to make it look realistic).To try to accomplish these goals, we collect a dataset by performing keyword searches of publicly available videos on the internet.A list of search queries we used are given in For videos that have metadata available, we perform an additional step of metadata-based filtering to eliminate videos that do not fit our target distribution.In this step, we look for a list of blacklist keywords in the video title and description and reject videos that contain these terms.The blacklist keywords we use are: {ps3, ps4, ps5, xbox 360, playstation, timelapse, multiplayer, minecraft pe, pocket edition, skyblock, realistic minecraft, how to install, how to download, realmcraft, animation}.This process yielded us ∼270k hours of unlabeled data, which we filter down to only a "clean" subset as described in the next section.A.2 Training a Model to Filter out Unclean Video SegmentsWe restrict the scope of this work to the Minecraft Survival game mode and therefore limit our training dataset to clips that are obtained from this mode that are relatively free from visual artifacts.8. Statistics are measured over 5 minute episodes.(a) Distance traveled by the agent .Both "explore" and "water" text strings should encourage a steerable agent to move more than when doing other tasks, which is what occurs.Grass (which is needed to get seeds) is not present in all biomes, which is likely why the "seed" condition produces more travel (as the agent sometimes needs to move to a biome with grass).The travel distance is the Euclidean distance from the spawn point to the farthest point the agent reached during the episode on the horizontal (x-z) plane.(b) Collection of wheat seeds.The "seed" variant collects substantially more than other variants, as expected of a steerable agent.(c) Collection of oak (the most common type of wood) logs.The "wood" variant collects significantly more oak logs, as is to be expected of a steerable agent (we speculate that the "water" variant collects less because there are no trees in water).(d) Collection of dirt.The "dirt" and "dig" variants collect a large amount, and are the variants that are (indirectly in the case of "dig") conditioned to collect dirt.It is easy to mistakenly aim at the ground rather than at grass or trees when collecting seeds or wood, which likely explains the slightly higher amount of dirt collected by these variants.In all cases, the error bars are 95% confidence intervals of the mean, over 1,000 episodes per conditioning variant.Treatments for which the bars in each bar plot do not overlap are statistically significantly different at a p &lt; 0.05 level.Variant name String
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.106832019arXiv preprint</p>
<p>Exploring the limits of weakly supervised pretraining. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens Van Der Maaten, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)2018</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>On the opportunities and risks of foundation models. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney Von Arx, Jeannette Michael S Bernstein, Antoine Bohg, Emma Bosselut, Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Scaling vision transformers. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, Lucas Beyer, CoRR, abs/2106.045602021</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, Nature. 52975872016</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, Nature. 57577822019</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.066802019arXiv preprint</p>
<p>Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob Mcgrew, Igor Mordatch, arXiv:1909.07528Emergent tool use from multi-agent autocurricula. 2019arXiv preprint</p>
<p>Human-level performance in 3d multiplayer games with population-based reinforcement learning. Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos, Avraham Ruderman, Science. 36464432019</p>
<p>Agent57: Outperforming the atari human benchmark. Adrià Puigdomènech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan Daniel Guo, Charles Blundell, International Conference on Machine Learning. PMLR2020</p>
<p>Unifying count-based exploration and intrinsic motivation. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, Remi Munos, Advances in neural information processing systems. 292016</p>
<p>Yuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, arXiv:1810.12894Exploration by random network distillation. 2018arXiv preprint</p>
<p>First return, then explore. Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, Jeff Clune, Nature. 59078472021</p>
<p>A data-driven approach for learning to control computers. David Peter C Humphreys, Toby Raposo, Gregory Pohlen, Rachita Thornton, Alistair Chhaparia, Josh Muldal, Petko Abramson, Alex Georgiev, Adam Goldin, Santoro, arXiv:2202.081372022arXiv preprint</p>
<p>World of bits: An open-domain platform for web-based agents. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, Percy Liang, Proceedings of the 34th International Conference on Machine Learning. Doina Precup, Yee Whye Teh, the 34th International Conference on Machine LearningPMLRAug 201770</p>
<p>Algorithms for inverse reinforcement learning. Stuart J Andrew Y Ng, Russell, Icml. 200012</p>
<p>Recent advances in imitation learning from observation. Faraz Torabi, Garrett Warnell, Peter Stone, arXiv:1905.135662019arXiv preprint</p>
<p>Generative adversarial imitation learning. Jonathan Ho, Stefano Ermon, Advances in neural information processing systems. 292016</p>
<p>Faraz Torabi, Garrett Warnell, Peter Stone, arXiv:1805.01954Behavioral cloning from observation. 2018arXiv preprint</p>
<p>Imitation from observation: Learning to imitate behaviors from raw video via context translation. Yuxuan Liu, Abhishek Gupta, Pieter Abbeel, Sergey Levine, 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE2018</p>
<p>Most played games in 2021, ranked by peak concurrent players. Twinfinite Staff, </p>
<p>Brandon William H Guss, Nicholay Houghton, Phillip Topin, Cayden Wang, Manuela Codel, Ruslan Veloso, Salakhutdinov, arXiv:1907.13440Minerl: A large-scale dataset of minecraft demonstrations. 2019arXiv preprint</p>
<p>A deep hierarchical approach to lifelong learning in minecraft. Chen Tessler, Shahar Givony, Tom Zahavy, Daniel Mankowitz, Shie Mannor, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Sample efficient reinforcement learning through learning from demonstrations in minecraft. Christian Scheller, Yanick Schraner, Manfred Vogel, NeurIPS 2019 Competition and Demonstration Track. PMLR2020</p>
<p>Multi-task curriculum learning in a complex, visual, hard-exploration domain. Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, arXiv:2106.148762021MinecraftarXiv preprint</p>
<p>Control of memory, active perception, and action in minecraft. Junhyuk Oh, Valliappa Chockalingam, Honglak Lee, International Conference on Machine Learning. PMLR2016</p>
<p>Align-rudder: Learning from few demonstrations by reward redistribution. Markus Vihang P Patil, Marius-Constantin Hofmarcher, Matthias Dinu, Patrick M Dorfer, Johannes Blies, Jose A Brandstetter, Sepp Arjona-Medina, Hochreiter, arXiv:2009.141082020arXiv preprint</p>
<p>Alexey Skrynnik, Aleksey Staroverov, Ermek Aitygulov, Kirill Aksenov, Vasilii Davydov, Aleksandr I Panov, arXiv:2006.09939Forgetful experience replay in hierarchical reinforcement learning from demonstrations. 2020arXiv preprint</p>
<p>Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning. Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, arXiv:2112.049072021arXiv preprint</p>
<p>Alvinn: An autonomous land vehicle in a neural network. A Dean, Pomerleau, Advances in neural information processing systems. 11988</p>
<p>Is imitation learning the route to humanoid robots?. Stefan Schaal, Trends in cognitive sciences. 361999</p>
<p>A survey of robot learning from demonstration. Sonia Brenna D Argall, Manuela Chernova, Brett Veloso, Browning, Robotics and autonomous systems. 5752009</p>
<p>Imitation learning: A survey of learning methods. Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, Chrisina Jayne, ACM Computing Surveys (CSUR). 5022017</p>
<p>Learning to fly. Claude Sammut, Scott Hurst, Dana Kedzier, Donald Michie, Machine Learning Proceedings. Elsevier1992. 1992</p>
<p>A machine learning approach to visual perception of forest trails for mobile robots. Alessandro Giusti, Jérôme Guzzi, Dan C Cireşan, Fang-Lin He, Juan P Rodríguez, Flavio Fontana, Matthias Faessler, Christian Forster, Jürgen Schmidhuber, Gianni Di Caro, IEEE Robotics and Automation Letters. 122015</p>
<p>End to end learning for self-driving cars. Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, arXiv:1604.073162016arXiv preprint</p>
<p>End-to-end driving via conditional imitation learning. Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, Alexey Dosovitskiy, IEEE international conference on robotics and automation (ICRA). 2018. 2018IEEE</p>
<p>Computing "elo ratings" of move patterns in the game of go. Rémi Coulom, ICGA journal. 3042007</p>
<p>Deep q-learning from demonstrations. Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Imitating latent policies from observation. Ashley Edwards, Himanshu Sahni, Yannick Schroecker, Charles Isbell, International conference on machine learning. PMLR2019</p>
<p>Sfv: Reinforcement learning of physical skills from videos. Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey Levine, ACM Transactions On Graphics (TOG). 3762018</p>
<p>Learning from demonstration in the wild. Feryal Behbahani, Kyriacos Shiarlis, Xi Chen, Vitaly Kurin, Sudhanshu Kasewa, Ciprian Stirbu, Joao Gomes, Supratik Paul, Frans A Oliehoek, Joao Messias, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Playing hard exploration games by watching youtube. Yusuf Aytar, Tobias Pfaff, David Budden, Thomas Paine, Ziyu Wang, Nando De Freitas, Advances in neural information processing systems. 201831</p>
<p>Zero-shot visual imitation. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A Efros, Trevor Darrell, ICLR2018</p>
<p>Combining self-supervised learning and imitation for vision-based rope manipulation. Ashvin Nair, Dian Chen, Pulkit Agrawal, Phillip Isola, Pieter Abbeel, Jitendra Malik, Sergey Levine, 10.1109/ICRA.2017.79892472017</p>
<p>Learning inverse dynamics: a comparison. Duy Nguyen-Tuong, Jan Peters, Matthias Seeger, Bernhard Schölkopf, European symposium on artificial neural networks, number CONF. 2008</p>
<p>Exploratory gradient boosting for reinforcement learning in complex domains. David Abel, Alekh Agarwal, Fernando Diaz, Akshay Krishnamurthy, Robert E Schapire, arXiv:1603.041192016arXiv preprint</p>
<p>Dilip Arumugam, Ki Jun, Sophie Lee, Saskin, Michael L Littman, arXiv:1902.04257Deep reinforcement learning from policy-dependent human feedback. 2019arXiv preprint</p>
<p>Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. Alexander Trott, Stephan Zheng, Caiming Xiong, Richard Socher, Advances in Neural Information Processing Systems. 201932</p>
<p>Deep reinforcement learning with model learning and monte carlo tree search in minecraft. Stephan Alaniz, arXiv:1803.084562018arXiv preprint</p>
<p>Fighting zombies in minecraft with deep reinforcement learning. Hiroto Udagawa, Tarun Narasimhan, Shim-Young Lee, 2016Stanford UniversityTechnical report</p>
<p>Hierarchical and interpretable skill acquisition in multi-task reinforcement learning. Tianmin Shu, Caiming Xiong, Richard Socher, arXiv:1712.072942017arXiv preprint</p>
<p>Zero-shot task generalization with multi-task deep reinforcement learning. Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli, International Conference on Machine Learning. PMLR2017</p>
<p>Learning to execute or ask clarification questions. Zhengxiang Shi, Yue Feng, Aldo Lipani, arXiv:2204.083732022arXiv preprint</p>
<p>Teacher-student curriculum learning. Tambet Matiisen, Avital Oliver, Taco Cohen, John Schulman, IEEE transactions on neural networks and learning systems. 201931</p>
<p>Principles and procedures of statistics. Principles and procedures of statistics. Robert George, Douglas Steel, James Hiram Torrie, 1960</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Phasic policy gradient. Karl W Cobbe, Jacob Hilton, Oleg Klimov, John Schulman, Proceedings of the 38th International Conference on Machine Learning. Marina Meila, Tong Zhang, the 38th International Conference on Machine LearningPMLR18-24 Jul 2021139Proceedings of Machine Learning Research</p>
<p>Biological underpinnings for lifelong learning machines. Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, Josh Bongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, Nature Machine Intelligence. 432022</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences2017114</p>
<p>On the dangers of stochastic parrots: Can language models be too big??. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Scikitlearn: Machine learning in python. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Journal of machine Learning research. 122011</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>Efficient backprop. Léon Yann A Lecun, Genevieve B Bottou, Klaus-Robert Orr, Müller, Neural networks: Tricks of the trade. Springer2012</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Pytorch: An imperative style, highperformance deep learning library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. Razvan Yann N Dauphin, Caglar Pascanu, Kyunghyun Gulcehre, Surya Cho, Yoshua Ganguli, Bengio, Advances in neural information processing systems. 201427</p>
<p>How transferable are features in deep neural networks?. Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson, Advances in neural information processing systems. 201427</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Ruslan Quoc V Le, Salakhutdinov, arXiv:1901.028602019arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Highdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.024382015arXiv preprint</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 831992</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>Hindsight experience replay. Advances in neural information processing systems. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. 201730</p>
<p>Universal value function approximators. Tom Schaul, Daniel Horgan, Karol Gregor, David Silver, International conference on machine learning. PMLR2015</p>
<p>Openended learning leads to generally capable agents. Adam Team, Anuj Stooke, Catarina Mahajan, Charlie Barros, Jakob Deck, Jakub Bauer, Maja Sygnowski, Max Trebacz, Michael Jaderberg, Mathieu, arXiv:2107.12808Open Ended Learning. 2021arXiv preprint</p>
<p>A survey of reinforcement learning informed by natural language. Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette, Shimon Whiteson, Tim Rocktäschel, 10.24963/ijcai.2019/880Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-1972019</p>
<p>Creating multimodal interactive agents with imitation and self-supervised learning. Deepmind Interactive, Agents Team, Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, Petko Georgiev, Alex Goldin, Tim Harley, arXiv:2112.037632021arXiv preprint</p>
<p>Hierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXiv:2204.061252022arXiv preprint</p>
<p>Automatic speech recognition. Dong Yu, Li Deng, 2016Springer1</p>
<p>. Daulet Nurmanbetov. rpunct. May 25 2021. 2022-04-22</p>
<p>Text and code embeddings by contrastive pre-training. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, arXiv:2201.100052022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
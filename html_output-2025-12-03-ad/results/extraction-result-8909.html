<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8909 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8909</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8909</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-01a81822fe7f9ddd57fb899831c9cfe7de1829ca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/01a81822fe7f9ddd57fb899831c9cfe7de1829ca" target="_blank">Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention</a></p>
                <p><strong>Paper Venue:</strong> Journal of Chemical Information and Modeling</p>
                <p><strong>Paper TL;DR:</strong> A neural network that creates molecules that meet some desired multiple target conditions based on a deep understanding of chemical language is proposed (generative chemical Transformer, GCT), which allows a deeper understanding of molecular structures beyond the limitations ofchemical language itself.</p>
                <p><strong>Paper Abstract:</strong> Discovering new materials better suited to specific purposes is an important issue in improving the quality of human life. Here, a neural network that creates molecules that meet some desired multiple target conditions based on a deep understanding of chemical language is proposed (generative chemical Transformer, GCT). The attention mechanism in GCT allows a deeper understanding of molecular structures beyond the limitations of chemical language itself which cause semantic discontinuity by paying attention to characters sparsely. The significance of language models for inverse molecular design problems is investigated by quantitatively evaluating the quality of the generated molecules. GCT generates highly realistic chemical strings that satisfy both chemical and linguistic grammar rules. Molecules parsed from the generated strings simultaneously satisfy the multiple target properties and vary for a single condition set. These advances will contribute to improving the quality of human life by accelerating the process of desired material discovery.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8909.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8909.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Chemical Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conditional variational generative model that embeds a Pre-Layer-Normalization Transformer encoder/decoder into a conditional VAE to generate SMILES strings that satisfy user-specified properties (logP, tPSA, QED).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Generative Chemical Transformer (GCT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer-based conditional Variational Autoencoder (cVAE) with multi-head self-attention (Pre-LN Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>MOSES benchmarking dataset (samples from ZINC): training set ~1.7M molecules; constraints: 250-350 Da, rotatable bonds ≤7, allowed atoms C,N,S,O,F,Cl,Br,H, no charged atoms, cycles ≤8. RDKit used to compute property labels (logP, tPSA, QED).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo generation of drug-like small molecules / material discovery (property-conditioned molecular design).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional latent-space sampling: encoder maps input SMILES + condition (logP,tPSA,QED) to a 128-d Gaussian latent space; at inference sample latent codes (sequence-length-aware), concatenate with target condition, and decode with Transformer decoder using 4-beam search to autoregressively generate SMILES tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Measured by MOSES novelty metric on 30,000 generated molecules: GCT-WarmUp novelty = 0.8144; GCT-SGDR novelty = 0.6756. Validity high (GCT-WarmUp 0.9853, GCT-SGDR 0.9916). Additional similarity/diversity metrics (SNN, FCD, Frag, Scaf) reported in benchmarking (see Evaluation Metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditions (logP, tPSA, QED) are embedded and input to encoder/decoder; model is trained to reconstruct SMILES given these hints so that sampling from latent space with specified conditions yields molecules tailored to those property targets. Reported mean absolute errors between requested and generated properties: |ΔlogP|=0.177, |ΔtPSA|=2.923, |ΔQED|=0.035.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES benchmarking metrics on 30k samples: validity, uniqueness@1k and @10k, internal diversity (IntDiv), filters (toxic/chemical filters), novelty, Similarity to Nearest Neighbor (SNN), Fréchet ChemNet Distance (FCD), fragment similarity (Frag), scaffold similarity (Scaf). Property-matching evaluated via MAE for logP, tPSA, QED (computed with RDKit).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GCT produces highly plausible SMILES: validity 98.5% (WarmUp)–99.2% (SGDR). Uniqueness and internal diversity comparable to other generative baselines. GCT-WarmUp achieved novelty 0.8144 and SNN(Test)=0.6179; FCD(Test)=0.4017 (worse than the best baseline VAE FCD=0.099). GCT matches conditional targets well on average (MAE: logP 0.177, tPSA 2.923, QED 0.035). The model can produce multiple distinct candidates per condition (variational behavior) and allows some control of molecular size via latent-code sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to MOSES baselines: VAE (validity 0.9767, novelty 0.6949, SNN 0.6257, FCD 0.0990), CharRNN (validity 0.9748, novelty 0.8419, FCD 0.0732), LatentGAN (validity 0.8966, novelty 0.9498), AAE and JTN-VAE also reported. GCT generally outperforms many baselines on validity and passes filters, has competitive diversity and scaffold/fragment similarity, but on some distributional metrics (e.g., FCD) VAE/CharRNN baselines can be better.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Reported issues and design choices: risk of posterior collapse (addressed by KL-annealing and using Pre-LN Transformer); QED is harder to match precisely because it is an aggregate artificial score; sequence-length mismatch between sampled latent-code length and generated SMILES length — GCT does not perfectly learn sequence-length distribution; results vary with learning-rate scheduling (WarmUp vs SGDR) affecting FCD/novelty; stereochemistry and charged atom tokens were not modeled (excluded from MOSES), so generated chemistry excludes those features; no experimental synthesis/biological validation reported (metrics are in silico).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8909.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (Pre-LN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pre-Layer-Normalization Transformer (used inside GCT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of the Transformer encoder-decoder architecture using pre-layer normalization to stabilize gradient flow; used as both encoder and decoder blocks inside GCT to provide strong attention-based language understanding of SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pre-LN Transformer (multi-head attention, 6 encoder and 6 decoder blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (attention-based sequence-to-sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same as GCT (MOSES training set of ~1.7M SMILES); tokens embedded to 512-dim; 28 token vocabulary (SmilesPE tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Representation and autoregressive generation of SMILES in conditional molecular design (part of GCT).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Self-attention based encoding of SMILES and properties; decoder attends to latent-code+condition and previously decoded tokens to generate SMILES autoregressively (beam search).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>N/A as standalone; contributes to GCT novelty/validity numbers above.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enables GCT to pay attention across SMILES characters to infer molecular graph context beyond linear token order, facilitating conditional generation matching structural-dependent properties.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>N/A separately; performance measured as part of GCT via MOSES metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pre-LN Transformer blocks in GCT appear to help prevent posterior collapse when used with KL-annealing and to enable attention heads to capture non-local structural relationships in SMILES (visualized attention indicating pairing of spatially adjacent atoms that are distant in SMILES string).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Traditional RNN-based decoders (e.g., CharRNN) lack the sparse non-local attention capability; the Transformer enabled better handling of SMILES semantic discontinuities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Transformer+VAE combinations can be vulnerable to posterior collapse; required KL-annealing and gradient-stabilizing design choices (Pre-LN) to mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8909.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditional Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network conditioned on target properties to generate SMILES strings directly given those conditions (one-to-one mapping between input condition and output).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conditional RNN (cRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (RNN) conditioned on properties</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Typically trained on SMILES strings with associated properties (paper references/use in literature; MOSES contains compatible training data).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Conditional molecular generation (property-conditioned SMILES generation).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive SMILES generation conditioned on target properties (condition vectors fed to RNN).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not quantified in this paper; mentioned that cRNNs struggle to generate diverse molecules for a single condition because RNN mapping tends toward one-to-one mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditions are directly input; effective at producing molecules matching conditions but limited in producing multiple varied candidates per condition.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>General generative metrics (validity, uniqueness, property-matching), but no numerical results given here for cRNN specifically beyond literature references.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Paper mentions cRNN can be used for conditional generation but has a limitation: because RNNs are inherently more one-to-one mapping, they provide limited variability per condition compared to variational generative models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with generative/variational models (e.g., cVAE) which can produce diverse candidates per condition by sampling latent codes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Limited diversity for a given condition set (difficulty generating multiple candidates for a single precondition).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8909.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Language model + VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-model coupled Variational Autoencoder approaches</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that compress SMILES (or other chemical string representations) into a latent space with a VAE and decode to generate novel SMILES by sampling latent codes; sometimes combined with language-model components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE-based molecular generators (language-model hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational Autoencoder with sequence encoder/decoder (often RNN or Transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora (e.g., ZINC/MOSES-like datasets); trained to reconstruct SMILES and learn continuous latent representations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation and exploration of chemical space; conditional variations used for property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode SMILES to latent vector, then sample in latent space and decode to SMILES; can be combined with Bayesian optimization in latent space for property optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported baseline VAE (MOSES) novelty = 0.6949; validity = 0.9767; SNN = 0.6257; FCD = 0.0990 (per MOSES benchmarks cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Often extended with conditional vectors or post-hoc optimization in latent space (Bayesian optimization, genetic algorithms) to tune properties; not as tightly integrated with attention-based language understanding unless using Transformer components.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (validity, novelty, SNN, FCD, Frag, Scaf) are standard benchmarks used to evaluate these models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAE baselines achieve competitive distributional metrics (notably strong FCD in MOSES reference) but have lower validity than GCT in this study; these models enable smooth latent-space navigation for optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in MOSES benchmark table: VAE shows stronger FCD (closer distribution to reference) than GCT in some experimental settings, indicating better alignment with training distribution for some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>VAEs can produce blurry/invalid outputs if not carefully regularized and require latent-space optimization methods for targeted property discovery (two-step pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8909.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LatentGAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Generative Adversarial Network (LatentGAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN-based model adapted to molecular generation by learning to generate latent vectors (or directly strings) to produce novel SMILES via an adversarial scheme; included here as a MOSES benchmark baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LatentGAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Generative Adversarial Network (GAN) adapted to molecular latent spaces / strings</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES datasets (MOSES/ZINC-like data used in MOSES benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation (drug-like chemical space exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>GAN-based generation (generate latent codes or sequences from noise via adversarial training), followed by decoding to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>High novelty reported in MOSES baseline: novelty = 0.9498, but lower validity = 0.8966 (per MOSES table in paper). SNN(Test)=0.5371, FCD(Test)=0.2968.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Not inherently conditional in baseline; can be extended with conditioning strategies but baseline MOSES use is unconditional.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics (validity, novelty, SNN, FCD, Frag, Scaf).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LatentGAN baseline produces high novelty but lower validity than GCT and other baselines; diversity and fragment/scaffold similarities are competitive but some distributional metrics are weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared in MOSES table where LatentGAN shows highest novelty but lower validity and SNN relative to GCT and some other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>GAN training instability, mode coverage problems, and lower guaranteed validity for chemically-correct SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8909.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CharRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character-level RNN trained to autoregressively generate SMILES strings one character/token at a time; used as MOSES benchmark baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CharRNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (character-level language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES corpora (MOSES/ZINC-like datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Unconditional or conditional SMILES generation for molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive generation of SMILES characters/tokens conditioned on prior tokens; can be combined with reinforcement learning or conditioning mechanisms for property steering.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MOSES baseline CharRNN novelty = 0.8419; validity = 0.9748; FCD(Test)=0.0732 (strong FCD), SNN(Test)=0.6015.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditional variants exist (e.g., cRNN) but baseline CharRNN often unconditional.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES metrics as reported in the paper's benchmark table.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>CharRNN baseline shows strong distributional match on FCD and high novelty, though GCT improves validity and some other metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>CharRNN performs very well on FCD but GCT achieves higher validity; CharRNN lacks explicit attention mechanism to capture non-local SMILES structural discontinuities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>RNNs can struggle with long-range dependencies introduced by SMILES branching and ring syntax; less explicit handling of semantic discontinuity compared to attention-based models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8909.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoencoder regularized by adversarial training to shape the latent-space distribution; cited as a MOSES baseline for molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Adversarial Autoencoder (AAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoencoder with adversarial regularization (GAN-style discriminator on latent space)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES datasets (MOSES/ZINC-like).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation and latent-space sampling for property exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode SMILES to latent code, adversarially enforce a prior on latent space, then decode sampled latents to SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MOSES baseline AAE novelty = 0.7931; validity = 0.9368; SNN(Test)=0.6081; FCD(Test)=0.5555.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Can be extended with conditioning or latent optimization for property-focused generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES benchmarking metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>AAE baseline shows moderate novelty and lower validity than GCT; fragment/scaffold metrics competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>AAE generally underperforms GCT and some baselines on validity and FCD in the reported MOSES comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Adversarial training can be unstable; balancing reconstruction and adversarial objectives is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8909.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8909.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>JTN-VAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Junction Tree Variational Autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VAE that models molecular graphs by decomposing molecules into tree-structured clusters (junction trees) to better capture scaffold and substructure generation; included as a MOSES baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Junction Tree VAE (JTN-VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Graph-based VAE modeling molecule substructures and assembly (junction-tree representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>SMILES/graph datasets (MOSES/ZINC-like used in MOSES benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular graph generation with improved scaffold validity and synthetic feasibility emphasis.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Encode molecule into junction-tree of molecular fragments and a graph; decode by assembling fragments according to learned distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MOSES baseline JTN-VAE novelty = 0.9143; validity = 1.0 (perfect) in the presented table; SNN(Test)=0.5477; FCD(Test)=0.3954.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Designed to better capture chemically-valid scaffold assembly, often improving validity/scaffold-matching.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MOSES benchmarking metrics (reported in paper's comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>JTN-VAE shows perfect validity in MOSES baseline reported here and high novelty; scaffold similarity metrics variable depending on test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>JTN-VAE emphasizes graph-aware generation and tends to have very high validity compared to sequence-only models; GCT trades graph-awareness for attention-based language understanding and conditional latent sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Graph-fragment assembly is more complex and may limit generality to arbitrary scaffolds; not sequence-based so different trade-offs exist compared to SMILES Transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention', 'publication_date_yy_mm': '2021-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Attention Is All You Need <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8909",
    "paper_id": "paper-01a81822fe7f9ddd57fb899831c9cfe7de1829ca",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "GCT",
            "name_full": "Generative Chemical Transformer",
            "brief_description": "A conditional variational generative model that embeds a Pre-Layer-Normalization Transformer encoder/decoder into a conditional VAE to generate SMILES strings that satisfy user-specified properties (logP, tPSA, QED).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Generative Chemical Transformer (GCT)",
            "model_type": "Transformer-based conditional Variational Autoencoder (cVAE) with multi-head self-attention (Pre-LN Transformer)",
            "model_size": null,
            "training_data": "MOSES benchmarking dataset (samples from ZINC): training set ~1.7M molecules; constraints: 250-350 Da, rotatable bonds ≤7, allowed atoms C,N,S,O,F,Cl,Br,H, no charged atoms, cycles ≤8. RDKit used to compute property labels (logP, tPSA, QED).",
            "application_domain": "De novo generation of drug-like small molecules / material discovery (property-conditioned molecular design).",
            "generation_method": "Conditional latent-space sampling: encoder maps input SMILES + condition (logP,tPSA,QED) to a 128-d Gaussian latent space; at inference sample latent codes (sequence-length-aware), concatenate with target condition, and decode with Transformer decoder using 4-beam search to autoregressively generate SMILES tokens.",
            "novelty_of_chemicals": "Measured by MOSES novelty metric on 30,000 generated molecules: GCT-WarmUp novelty = 0.8144; GCT-SGDR novelty = 0.6756. Validity high (GCT-WarmUp 0.9853, GCT-SGDR 0.9916). Additional similarity/diversity metrics (SNN, FCD, Frag, Scaf) reported in benchmarking (see Evaluation Metrics).",
            "application_specificity": "Conditions (logP, tPSA, QED) are embedded and input to encoder/decoder; model is trained to reconstruct SMILES given these hints so that sampling from latent space with specified conditions yields molecules tailored to those property targets. Reported mean absolute errors between requested and generated properties: |ΔlogP|=0.177, |ΔtPSA|=2.923, |ΔQED|=0.035.",
            "evaluation_metrics": "MOSES benchmarking metrics on 30k samples: validity, uniqueness@1k and @10k, internal diversity (IntDiv), filters (toxic/chemical filters), novelty, Similarity to Nearest Neighbor (SNN), Fréchet ChemNet Distance (FCD), fragment similarity (Frag), scaffold similarity (Scaf). Property-matching evaluated via MAE for logP, tPSA, QED (computed with RDKit).",
            "results_summary": "GCT produces highly plausible SMILES: validity 98.5% (WarmUp)–99.2% (SGDR). Uniqueness and internal diversity comparable to other generative baselines. GCT-WarmUp achieved novelty 0.8144 and SNN(Test)=0.6179; FCD(Test)=0.4017 (worse than the best baseline VAE FCD=0.099). GCT matches conditional targets well on average (MAE: logP 0.177, tPSA 2.923, QED 0.035). The model can produce multiple distinct candidates per condition (variational behavior) and allows some control of molecular size via latent-code sequence length.",
            "comparison_to_other_methods": "Compared to MOSES baselines: VAE (validity 0.9767, novelty 0.6949, SNN 0.6257, FCD 0.0990), CharRNN (validity 0.9748, novelty 0.8419, FCD 0.0732), LatentGAN (validity 0.8966, novelty 0.9498), AAE and JTN-VAE also reported. GCT generally outperforms many baselines on validity and passes filters, has competitive diversity and scaffold/fragment similarity, but on some distributional metrics (e.g., FCD) VAE/CharRNN baselines can be better.",
            "limitations_and_challenges": "Reported issues and design choices: risk of posterior collapse (addressed by KL-annealing and using Pre-LN Transformer); QED is harder to match precisely because it is an aggregate artificial score; sequence-length mismatch between sampled latent-code length and generated SMILES length — GCT does not perfectly learn sequence-length distribution; results vary with learning-rate scheduling (WarmUp vs SGDR) affecting FCD/novelty; stereochemistry and charged atom tokens were not modeled (excluded from MOSES), so generated chemistry excludes those features; no experimental synthesis/biological validation reported (metrics are in silico).",
            "uuid": "e8909.0",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Transformer (Pre-LN)",
            "name_full": "Pre-Layer-Normalization Transformer (used inside GCT)",
            "brief_description": "A variant of the Transformer encoder-decoder architecture using pre-layer normalization to stabilize gradient flow; used as both encoder and decoder blocks inside GCT to provide strong attention-based language understanding of SMILES.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Pre-LN Transformer (multi-head attention, 6 encoder and 6 decoder blocks)",
            "model_type": "Transformer (attention-based sequence-to-sequence model)",
            "model_size": null,
            "training_data": "Same as GCT (MOSES training set of ~1.7M SMILES); tokens embedded to 512-dim; 28 token vocabulary (SmilesPE tokenization).",
            "application_domain": "Representation and autoregressive generation of SMILES in conditional molecular design (part of GCT).",
            "generation_method": "Self-attention based encoding of SMILES and properties; decoder attends to latent-code+condition and previously decoded tokens to generate SMILES autoregressively (beam search).",
            "novelty_of_chemicals": "N/A as standalone; contributes to GCT novelty/validity numbers above.",
            "application_specificity": "Enables GCT to pay attention across SMILES characters to infer molecular graph context beyond linear token order, facilitating conditional generation matching structural-dependent properties.",
            "evaluation_metrics": "N/A separately; performance measured as part of GCT via MOSES metrics.",
            "results_summary": "Pre-LN Transformer blocks in GCT appear to help prevent posterior collapse when used with KL-annealing and to enable attention heads to capture non-local structural relationships in SMILES (visualized attention indicating pairing of spatially adjacent atoms that are distant in SMILES string).",
            "comparison_to_other_methods": "Traditional RNN-based decoders (e.g., CharRNN) lack the sparse non-local attention capability; the Transformer enabled better handling of SMILES semantic discontinuities.",
            "limitations_and_challenges": "Transformer+VAE combinations can be vulnerable to posterior collapse; required KL-annealing and gradient-stabilizing design choices (Pre-LN) to mitigate.",
            "uuid": "e8909.1",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "cRNN",
            "name_full": "Conditional Recurrent Neural Network",
            "brief_description": "A recurrent neural network conditioned on target properties to generate SMILES strings directly given those conditions (one-to-one mapping between input condition and output).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Conditional RNN (cRNN)",
            "model_type": "Recurrent Neural Network (RNN) conditioned on properties",
            "model_size": null,
            "training_data": "Typically trained on SMILES strings with associated properties (paper references/use in literature; MOSES contains compatible training data).",
            "application_domain": "Conditional molecular generation (property-conditioned SMILES generation).",
            "generation_method": "Direct autoregressive SMILES generation conditioned on target properties (condition vectors fed to RNN).",
            "novelty_of_chemicals": "Not quantified in this paper; mentioned that cRNNs struggle to generate diverse molecules for a single condition because RNN mapping tends toward one-to-one mapping.",
            "application_specificity": "Conditions are directly input; effective at producing molecules matching conditions but limited in producing multiple varied candidates per condition.",
            "evaluation_metrics": "General generative metrics (validity, uniqueness, property-matching), but no numerical results given here for cRNN specifically beyond literature references.",
            "results_summary": "Paper mentions cRNN can be used for conditional generation but has a limitation: because RNNs are inherently more one-to-one mapping, they provide limited variability per condition compared to variational generative models.",
            "comparison_to_other_methods": "Contrasted with generative/variational models (e.g., cVAE) which can produce diverse candidates per condition by sampling latent codes.",
            "limitations_and_challenges": "Limited diversity for a given condition set (difficulty generating multiple candidates for a single precondition).",
            "uuid": "e8909.2",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "Language model + VAE",
            "name_full": "Language-model coupled Variational Autoencoder approaches",
            "brief_description": "Approaches that compress SMILES (or other chemical string representations) into a latent space with a VAE and decode to generate novel SMILES by sampling latent codes; sometimes combined with language-model components.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "VAE-based molecular generators (language-model hybrid)",
            "model_type": "Variational Autoencoder with sequence encoder/decoder (often RNN or Transformer-based)",
            "model_size": null,
            "training_data": "SMILES corpora (e.g., ZINC/MOSES-like datasets); trained to reconstruct SMILES and learn continuous latent representations.",
            "application_domain": "De novo molecular generation and exploration of chemical space; conditional variations used for property optimization.",
            "generation_method": "Encode SMILES to latent vector, then sample in latent space and decode to SMILES; can be combined with Bayesian optimization in latent space for property optimization.",
            "novelty_of_chemicals": "Reported baseline VAE (MOSES) novelty = 0.6949; validity = 0.9767; SNN = 0.6257; FCD = 0.0990 (per MOSES benchmarks cited in this paper).",
            "application_specificity": "Often extended with conditional vectors or post-hoc optimization in latent space (Bayesian optimization, genetic algorithms) to tune properties; not as tightly integrated with attention-based language understanding unless using Transformer components.",
            "evaluation_metrics": "MOSES metrics (validity, novelty, SNN, FCD, Frag, Scaf) are standard benchmarks used to evaluate these models in the paper.",
            "results_summary": "VAE baselines achieve competitive distributional metrics (notably strong FCD in MOSES reference) but have lower validity than GCT in this study; these models enable smooth latent-space navigation for optimization.",
            "comparison_to_other_methods": "Compared in MOSES benchmark table: VAE shows stronger FCD (closer distribution to reference) than GCT in some experimental settings, indicating better alignment with training distribution for some metrics.",
            "limitations_and_challenges": "VAEs can produce blurry/invalid outputs if not carefully regularized and require latent-space optimization methods for targeted property discovery (two-step pipelines).",
            "uuid": "e8909.3",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "LatentGAN",
            "name_full": "Latent Generative Adversarial Network (LatentGAN)",
            "brief_description": "A GAN-based model adapted to molecular generation by learning to generate latent vectors (or directly strings) to produce novel SMILES via an adversarial scheme; included here as a MOSES benchmark baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LatentGAN",
            "model_type": "Generative Adversarial Network (GAN) adapted to molecular latent spaces / strings",
            "model_size": null,
            "training_data": "SMILES datasets (MOSES/ZINC-like data used in MOSES benchmarks).",
            "application_domain": "De novo molecular generation (drug-like chemical space exploration).",
            "generation_method": "GAN-based generation (generate latent codes or sequences from noise via adversarial training), followed by decoding to SMILES.",
            "novelty_of_chemicals": "High novelty reported in MOSES baseline: novelty = 0.9498, but lower validity = 0.8966 (per MOSES table in paper). SNN(Test)=0.5371, FCD(Test)=0.2968.",
            "application_specificity": "Not inherently conditional in baseline; can be extended with conditioning strategies but baseline MOSES use is unconditional.",
            "evaluation_metrics": "MOSES metrics (validity, novelty, SNN, FCD, Frag, Scaf).",
            "results_summary": "LatentGAN baseline produces high novelty but lower validity than GCT and other baselines; diversity and fragment/scaffold similarities are competitive but some distributional metrics are weaker.",
            "comparison_to_other_methods": "Compared in MOSES table where LatentGAN shows highest novelty but lower validity and SNN relative to GCT and some other baselines.",
            "limitations_and_challenges": "GAN training instability, mode coverage problems, and lower guaranteed validity for chemically-correct SMILES.",
            "uuid": "e8909.4",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "CharRNN",
            "name_full": "Character-level Recurrent Neural Network",
            "brief_description": "A character-level RNN trained to autoregressively generate SMILES strings one character/token at a time; used as MOSES benchmark baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "CharRNN",
            "model_type": "Recurrent Neural Network (character-level language model)",
            "model_size": null,
            "training_data": "SMILES corpora (MOSES/ZINC-like datasets).",
            "application_domain": "Unconditional or conditional SMILES generation for molecule design.",
            "generation_method": "Autoregressive generation of SMILES characters/tokens conditioned on prior tokens; can be combined with reinforcement learning or conditioning mechanisms for property steering.",
            "novelty_of_chemicals": "MOSES baseline CharRNN novelty = 0.8419; validity = 0.9748; FCD(Test)=0.0732 (strong FCD), SNN(Test)=0.6015.",
            "application_specificity": "Conditional variants exist (e.g., cRNN) but baseline CharRNN often unconditional.",
            "evaluation_metrics": "MOSES metrics as reported in the paper's benchmark table.",
            "results_summary": "CharRNN baseline shows strong distributional match on FCD and high novelty, though GCT improves validity and some other metrics.",
            "comparison_to_other_methods": "CharRNN performs very well on FCD but GCT achieves higher validity; CharRNN lacks explicit attention mechanism to capture non-local SMILES structural discontinuities.",
            "limitations_and_challenges": "RNNs can struggle with long-range dependencies introduced by SMILES branching and ring syntax; less explicit handling of semantic discontinuity compared to attention-based models.",
            "uuid": "e8909.5",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "AAE",
            "name_full": "Adversarial Autoencoder",
            "brief_description": "An autoencoder regularized by adversarial training to shape the latent-space distribution; cited as a MOSES baseline for molecule generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Adversarial Autoencoder (AAE)",
            "model_type": "Autoencoder with adversarial regularization (GAN-style discriminator on latent space)",
            "model_size": null,
            "training_data": "SMILES datasets (MOSES/ZINC-like).",
            "application_domain": "Molecular generation and latent-space sampling for property exploration.",
            "generation_method": "Encode SMILES to latent code, adversarially enforce a prior on latent space, then decode sampled latents to SMILES.",
            "novelty_of_chemicals": "MOSES baseline AAE novelty = 0.7931; validity = 0.9368; SNN(Test)=0.6081; FCD(Test)=0.5555.",
            "application_specificity": "Can be extended with conditioning or latent optimization for property-focused generation.",
            "evaluation_metrics": "MOSES benchmarking metrics.",
            "results_summary": "AAE baseline shows moderate novelty and lower validity than GCT; fragment/scaffold metrics competitive.",
            "comparison_to_other_methods": "AAE generally underperforms GCT and some baselines on validity and FCD in the reported MOSES comparison.",
            "limitations_and_challenges": "Adversarial training can be unstable; balancing reconstruction and adversarial objectives is challenging.",
            "uuid": "e8909.6",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        },
        {
            "name_short": "JTN-VAE",
            "name_full": "Junction Tree Variational Autoencoder",
            "brief_description": "A VAE that models molecular graphs by decomposing molecules into tree-structured clusters (junction trees) to better capture scaffold and substructure generation; included as a MOSES baseline.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Junction Tree VAE (JTN-VAE)",
            "model_type": "Graph-based VAE modeling molecule substructures and assembly (junction-tree representation)",
            "model_size": null,
            "training_data": "SMILES/graph datasets (MOSES/ZINC-like used in MOSES benchmarks).",
            "application_domain": "De novo molecular graph generation with improved scaffold validity and synthetic feasibility emphasis.",
            "generation_method": "Encode molecule into junction-tree of molecular fragments and a graph; decode by assembling fragments according to learned distributions.",
            "novelty_of_chemicals": "MOSES baseline JTN-VAE novelty = 0.9143; validity = 1.0 (perfect) in the presented table; SNN(Test)=0.5477; FCD(Test)=0.3954.",
            "application_specificity": "Designed to better capture chemically-valid scaffold assembly, often improving validity/scaffold-matching.",
            "evaluation_metrics": "MOSES benchmarking metrics (reported in paper's comparison table).",
            "results_summary": "JTN-VAE shows perfect validity in MOSES baseline reported here and high novelty; scaffold similarity metrics variable depending on test splits.",
            "comparison_to_other_methods": "JTN-VAE emphasizes graph-aware generation and tends to have very high validity compared to sequence-only models; GCT trades graph-awareness for attention-based language understanding and conditional latent sampling.",
            "limitations_and_challenges": "Graph-fragment assembly is more complex and may limit generality to arbitrary scaffolds; not sequence-based so different trade-offs exist compared to SMILES Transformers.",
            "uuid": "e8909.7",
            "source_info": {
                "paper_title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
                "publication_date_yy_mm": "2021-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Attention Is All You Need",
            "rating": 2
        }
    ],
    "cost": 0.01603975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention</h1>
<p>Hyunseung Kim ${ }^{\dagger}$, Jonggeol $\mathrm{Na}^{\ddagger, <em>}$, Won Bo Lee ${ }^{\dagger, </em>}$<br>${ }^{\dagger}$ School of Chemical and Biological Engineering, Seoul National University, Gwanak-ro 1, Gwanakgu, Seoul 08826, Republic of Korea<br>${ }^{\ddagger}$ Department of Chemical Engineering and Materials Science, Graduate Program in System Health Science and Engineering, Ewha Womans University, Seoul 03760, Republic of Korea</p>
<p>Correspondence and requests for materials should be addressed to
J.N. (email: jgna@ewha.ac.kr) or W.B.L. (email: wblee@snu.ac.kr).</p>
<h1>ABSTRACT:</h1>
<p>Discovering new materials better suited to specific purposes is an important issue in improving the quality of human life. Here, a neural network that creates molecules that meet some desired conditions based on a deep understanding of chemical language is proposed (Generative Chemical Transformer, GCT). The attention mechanism in GCT allows a deeper understanding of molecular structures beyond the limitations of chemical language itself which cause semantic discontinuity by paying attention to characters sparsely. It is investigated that the significance of language models for inverse molecular design problems by quantitatively evaluating the quality of the generated molecules. GCT generates highly realistic chemical strings that satisfy both chemical and linguistic grammar rules. Molecules parsed from generated strings simultaneously satisfy the multiple target properties and vary for a single condition set. These advances will contribute to improving the quality of human life by accelerating the process of desired material discovery.</p>
<h1>INTRODUCTION</h1>
<p>Material discovery is a research field that searches for new materials that fit specific purposes. Searching molecular structures that simultaneously satisfy the multiple desired target conditions requires a high level of expert knowledge and considerable time and cost, since the chemical space is vast; the number of organic molecules less than 500 Da exceeds $10^{60} .^{1}$ For this reason, inferring the desired molecules using artificial intelligence with transcendental learning can help to accelerate the process of material discovery.</p>
<p>In recent years, attempts have been made to utilize artificial neural networks in chemistry fields where the relationship between molecular structure and physical properties is complex. To learn the molecular structures in neural nets, the molecular structure must be expressed as structured data. Molecules are made up of atoms and connecting bonds, so they are similar to graphs. However, techniques for expressing molecular structures in the form of strings also have been studied to facilitate the construction and utilization of molecular information as a database, since the form of string data is more convenient to handle than the form of graph data. ${ }^{2-6}$ The Simplified Molecular-Input Line-Entry System (SMILES) ${ }^{4-6}$ developed in the 1980s is the most popular artificial language used to express molecular structures in detail. Similar to natural language, SMILES also has an arrangement of characters according to grammar and context.</p>
<p>Several approaches have demonstrated that Natural Language Processing (NLP) models are applicable to inverse molecular design problems by generating molecules expressed in chemical language via a language model to select a character that follows the currently generated string, ${ }^{7-11}$ a language model combined with a Variational Autoencoder (VAE) ${ }^{12}$ to compress molecular information into latent space and re-sample the latent code to create various strings, ${ }^{13,14}$ a language model combined with a Generative Adversarial Network (GAN) ${ }^{15}$ to create strings from noise, ${ }^{16}$ or a language model combined with reinforcement learning to reward a natural character that follows the currently generated string. ${ }^{17}$ However, why language model-based molecular generators work well has not been</p>
<p>comprehensively analyzed.</p>
<p>An important point in material discovery is to search the molecular structures that meet multiple desired target conditions. The desired molecules can be discovered in 2 steps by applying additional optimization or navigation process to the generative model: Bayesian optimization, ${ }^{13,18}$ particle swarm optimization, ${ }^{19}$ genetic optimization, ${ }^{20-22}$ or Monte Carlo tree search. ${ }^{23-25}$ Another method is to use conditional models which create molecules with the given target conditions in 1 step. The latter can shorten the time consumed to discover the desired molecules and directly control the molecules to be generated. For this reason, attempts have been made to directly input conditions to a recurrent neural network (Conditional Recurrent Neural Network, cRNN ${ }^{26}$ ). Unlike generative models, since a Recurrent Neural Network (RNN) is based on one-to-one matching of input and output, there is a limitation in using it to infer the various molecular candidates with a single condition set; here, the generative models refer to models that can output various results by decoding latent codes sampled from the distribution of latent variables (latent space).</p>
<p>Here, Generative Chemical Transformer (GCT), which embeds Transformer ${ }^{27}$ —a state-of-the-art architecture that became a breakthrough for NLP problems by using an attention mechanism ${ }^{28}$ —into a conditional variational generative model is proposed. From the point of view of data recognition and processing, GCT is close to a conditional variational generator that embodies the language recognition ability of the attention mechanism in Transformer. To use GCT for material discovery, it is intended to take advantage of both the high-performance language model and the conditional generative model. GCT is analyzed by quantitatively evaluating the generated molecules and investigate the significance of language models for inverse molecular design problems. It is shown that a deep understanding of the molecular geometric structure learned from chemical language by paying attention to each character in chemical strings sparsely helps to generate chemical strings satisfying the chemical valance rule and syntax of the chemical language (grammar understanding). The strings are parsed into highly realistic molecules (context understanding). Additionally, it is demonstrated that the conditional</p>
<p>variational generator, which is the skeleton of GCT, helps to generate molecules that satisfy multiple preconditions simultaneously (conditional generator) and varies for a single precondition set (variational generator). In addition, the autoencoder, a substructure of GCT, makes the molecular size controllable.</p>
<h1>■ MATERIALS AND METHODS</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Structural limitation of language forms. (a) An example of a non-Hamiltonian graph. A Hamiltonian graph has a path that passes through all the points in the graph only once, and a non-Hamiltonian graph does not have such a path. (b) An example of a non-Hamiltonian molecular graph and its SMILES string: 4-(2-aminopropyl)-2-methoxyphenol. Each atom is labelled with a circled number. Different colors refer to different branches. (c) In natural language, words that are semantically close within a sentence are not always structurally</p>
<h1>Overcoming Structural Limits of Language via Attention.</h1>
<p>It is expected that introducing an attention mechanism to language-based inverse molecular design problems can help networks understand the geometric structures of molecules beyond the limitations of chemical language itself. SMILES, a chemical language, represents molecules as one-dimensional text. It is a powerful language since a one-SMILES string is converted into one exact molecule. Unfortunately, however, since most molecules are non-Hamiltonian graphs, it is self-evident that semantic discontinuity occurs whenever a branch in a molecule is translated into a one-dimensional string (Figure 1a). SMILES distinguishes each branch with open and close parentheses and creates a gap between the distance of two characters within the string and the distance of the corresponding two atoms in the molecular graph. An example is shown in Figure 1b. Even though atom2 and atom13 in Figure 1b are geometrically adjacent, they are far from each other in the string. In other words, the longer the branch is, the harder it is to imagine the molecular structure by reading the chemical string in order (similar to memory cells). A similar phenomenon occurs in natural language (Figure 1c). The longer the sentence is, the larger the gap between the words within the sentence and the semantic similarity. Unfortunately, in chemical language, there are more areas where semantic discontinuity occurs. In the field of NLP, by introducing an attention mechanism to this problem, language models are able to pay sparse attention to semantically related parts; it is a departure from the traditional way of perceiving context in the order of sentences (memory cells). Transformer is an architecture involving an attention mechanism in the form of a neural network, and it became a breakthrough in cognitive ability. This study was started with the expectation that the attention mechanism could help structural understanding beyond the semantic discontinuity of chemical language. The results and discussion for this problem will be covered in a later section.</p>
<h2>SMILES Language.</h2>
<p>SMILES expresses the atoms, bonds, and rings that make up molecules as a string. An atom is represented by the alphabet of the element symbol, and a bond is represented by a single bond (-), double bonds $(=)$, and triple bonds $(#)$, depending on the type. In general, a bond that can be easily inferred through the atoms or ring structure of the surrounding atom is omitted. The notation of hydrogen is also omitted in SMILES string if single bond hydrogen can be explicitly inferred by the chemical valence rule, however, single bond hydrogen can be clearly indicated by using $[\mathrm{H}]$ if the bond is implicit. For charged atoms, where the number of hydrogen bonds cannot be determined explicitly, atoms and formal charges are written together in brackets [ ]. The beginning and end of each ring are expressed with the same digit, and the pair must be correct; if a ring is open, it must be closed. The atoms present in the aromatic ring are written in lowercase, while the atoms outside the ring are capitalized. The branches in molecules are indicated by opening and closing parentheses (see Figure 1b). A more detailed description of SMILES is in the ref. ${ }^{4-6}$.</p>
<h1>Tokenization and Token Embedding.</h1>
<p>To input SMILES string to language models, the process of tokenizing by semantic units is necessary. The SmilesPE ${ }^{29}$ tokenizer was used to tokenize the SMILES strings included in the training data of Molecular Sets benchmarking platforms (MOSES). ${ }^{30}$ In total, 28 types of tokens are used: 4 special tokens (<unknown $>,&lt;\mathrm{pad}&gt;,&lt;\mathrm{sos}&gt;$, and $&lt;$ eos $&gt;), 13$ atom tokens $(&lt;\mathrm{C}&gt;,&lt;\mathrm{c}&gt;,&lt;\mathrm{O}&gt;,&lt;\mathrm{o}&gt;,&lt;\mathrm{N}&gt;,&lt;\mathrm{n}&gt;,&lt;\mathrm{F}&gt;$, $&lt;\mathrm{S}&gt;,&lt;\mathrm{s}&gt;,&lt;\mathrm{Cl}&gt;,&lt;\mathrm{Br}&gt;,&lt;[\mathrm{nH}]&gt;$, and $&lt;[\mathrm{H}]&gt;)$, 3 bond tokens $(&lt;-&gt;,&lt;=&gt;$, and $&lt;#&gt;), 2$ branch tokens $(&lt;(&gt;$ and $&lt;$ ) $&gt;$ ), and 6 ring tokens $(&lt;1&gt;,&lt;2&gt;,&lt;3&gt;,&lt;4&gt;,&lt;5&gt;$, and $&lt;6&gt;)$. Note that tokens related to charged atom (e.g. $&lt;[\mathrm{O}-]&gt;,&lt;[\mathrm{n}+]&gt;)$ and tokens related to stereochemistry (e.g. $&lt;/&gt;,&lt;\backslash&gt;)$ were not considered as they are not covered by MOSES database. Each token that constitutes the SMILES string is one-hot encoded in 28 dimensions and embedded in 512 dimensions. The condition of GCT is also embedded in 512 dimensions.</p>
<h1>Attention Mechanism.</h1>
<p>The attention mechanism is the core of Transformer's language cognition abilities. The attention mechanism allows Transformer to self-learn which token of the input string is better to focus on to perform a given task better. The attention mechanism uses three vectors: the query $Q$, the key $K$, and the value $V$. The attention mechanism calculates the similarity between Q and all keys in K , and the calculated similarity is multiplied by the value corresponding to the key to calculate the attention scores. The scale-dot attention used in the Transformer is the same as eq $1{ }^{27}$ :</p>
<p>$$
\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</p>
<p>where $d_{k}$ is the dimension of $K$ and $d_{k}$ must correspond to the dimension $d_{q}$ of $Q$.</p>
<p>The Transformer uses multi-head attention instead of single-head attention (eq 2) ${ }^{27}$ :</p>
<p>$$
\begin{gathered}
\operatorname{MultiHead}(Q, K, V)=\operatorname{Concat}\left(\text { head }<em h="h">{1}, \ldots, \text { head }</em> \
\text { where head }}\right) W^{O<em i="i">{i}=\operatorname{Attention}\left(Q W</em>\right)
\end{gathered}
$$}^{Q}, K W_{i}^{K}, V W_{i}^{V</p>
<p>Where $\quad W_{i}^{Q} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{K} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d_{\text {model }} \times d_{k}}, W_{i}^{O} \in \mathbb{R}^{h d_{v} \times d_{\text {model }}} \quad$ and $d_{\text {model }}=d_{k} \times h=d_{v} \times h$. Here, $h$ is the number of multi-head and $d_{v}$ is the dimension of $V$. Each head (head $<em i="i">{i}$ ) calculates an attention score between $Q$ and $K$ from different viewpoints using the different weights belonging to each head $\left(W</em>\right)$.}^{Q}, W_{i}^{K}\right.$, and $\left.W_{i}^{V</p>
<h2>Generative Chemical Transformer.</h2>
<p>We designed GCT, an architecture that embeds Transformer-one of the most advanced language models-into a Conditional Variational Autoencoder (cVAE), ${ }^{31}$ which creates molecules with target properties based on a deep understanding of chemical language. Transformer, the core of GCT's language recognition ability, is mainly used as a Neural Machine Translator (NMT). It consists of an</p>
<p>encoder and a decoder (Figure 2a). The encoder receives a sentence to be translated and understands the received sentence through self-attention. Then, the processed information from sentence comprehension is passed to the decoder. The decoder iteratively selects the next token that will follow the translated sentence up to this point, referring to the information received from the encoder and the sentences translated up to the previous step; if there is no translated sentence at the beginning of translation, the special token 'start of sentence <sos>' is used. The decoder uses the input information to iteratively select the next token that will follow the translated sentence up to the previous step. Finally, the translation ends when the decoder selects a special token 'end of sentence <eos>'.</p>
<p>GCT is a structure that inserts a low-dimensional conditional Gaussian latent space between the encoder and the decoder of the Pre-Layer Normalization (Pre-LN) Transformer. ${ }^{32}$ (Figure 2b). PreLN Transformer is a modified version of the original (Post-Layer Normalization, Post-LN) Transformer. The combination of language models and variational autoencoders is vulnerable to posterior collapse. ${ }^{33}$ A complete solution to posterior collapse has yet to be identified; however, it is known that Kullback-Leibler divergence (KL) annealing can alleviate this problem. ${ }^{34}$ Since KL annealing (KLA) controls the gradient size, adopting Pre-LN Transformer-designed to stabilize the gradient flow of the (Post-LN) Transformer-can facilitate KLA manipulation. The loss function of GCT is as follows:</p>
<p>$$
L\left(\emptyset, \theta ; x_{e n c}, x_{&lt;t}, x_{t}, c\right)=k_{w} D_{K L}\left(q_{\emptyset}\left(z \mid x_{e n c}, c\right) | p(z \mid c)\right)-E_{q_{\emptyset}\left(z \mid x_{e n c}, c\right)}\left[\log g_{\theta}\left(x_{t} \mid z, x_{&lt;t}, c\right)\right]
$$</p>
<p>where $D_{K L}(\cdot)$ is the KL divergence, and $E[\cdot]$ is the expectation of $\cdot . q_{\emptyset}$ is a parameterized encoder function, $g_{\emptyset}$ is a parameterized decoder function (generator), $p(\cdot \mid c)$ is a conditional Gaussian prior. Here, $\emptyset, \theta, x_{e n c}, x_{&lt;t}, z, x_{t}, c, k_{w}$ are the parameter set of the encoder, the parameter set of the decoder, the input of the encoder, the input of the decoder, the latent variables, the reconstruction target, the conditions, and the weight for KLA, respectively. The encoder and decoder each consist of six Pre-LN Transformer blocks. Each block has dimensions of 512 and 8 -head attention, and the dimension of the feed-forward block is 2,048 . The Gaussian latent space is designed in 128</p>
<p>dimensions.</p>
<p>The self-attention block of the encoder obtains the concatenated array of the SMILES string and three different properties: the octanol-water partition coefficient (logP), the topological Polar Surface Area (tPSA), and the Quantitative Estimate of Drug-likeness (QED). ${ }^{35}$ The encoder-decoder attention block in the decoder obtains the concatenated array of latent code and condition (three properties), and the self-attention block in the decoder obtains only the SMILES string. In the training phase, GCT performs the task of reconstructing the SMILES string-input through the encoder-by referring to the given hints on the molecular properties. In this process, the low-dimensional latent space acts as the model's bottleneck to find as much meaningful information that can be restored to the decoder as possible by exploiting the limited information passed through the bottleneck. Then, meaningful latent variables for molecular structures and properties are represented in the low-dimensional continuous latent space. In the inference phase, a sampled latent code and target properties are input into the learned decoder, and the decoder selects the next tokens iteratively through a 4-beam search; which is a kind of tree search method.</p>
<p>A dropout rate of GCT is 0.3 applied. Learning is conducted by the Adam optimizer. ${ }^{36}$ The initial learning rate is $10^{-4}$. The expansion rate of the momentum is 0.9 and the expansion rate of the adaptive term is 0.98 . Two methods are applied to schedule the learning rate (GCT-WarmUP and GCT-SGDR in Table 1). One is to use the warm-up scheduler (eq 4) ${ }^{27}$ :</p>
<p>$$
\eta=3 d_{\text {model }}^{-0.5} \cdot \min \left(s_{\text {current }}{ }^{-0.5}, s_{\text {warmup }}{ }^{-1.5}\right)
$$</p>
<p>where, $\eta$ means learning rate, $s_{\text {current }}$ means current training step, and $s_{\text {warmup }}$ means warmup steps. $s_{\text {warmup }}$ is set to 100,000 . The other is to use Stochastic Gradient Descent with warm Restart (SGDR) of one epoch cycle (eq 5) ${ }^{37}$ :</p>
<p>$$
\eta=\eta_{\min }+0.5\left(\eta_{\max }-\eta_{\min }\right)\left(1+\cos \left(\frac{s_{\text {current }}}{s_{\text {cycle }}} \pi\right)\right)
$$</p>
<p>where, $\eta_{\min }$ means minimum learning rate, $\eta_{\max }$ means maximum learning rate, $s_{\text {cycle }}$ is learning rate scheduling step cycle. Here, $\eta_{\min }, \eta_{\max }$, and $s_{\text {cycle }}$ are set to $0,0.0001$, and 1-epoch steps, respectively. KL annealing was applied to increase $k_{w}$ from 0.02 to 0.50 at 0.02 intervals per epoch for a total of 25 epochs. Other details of GCT are provided in SI.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. De novo molecular generation via GCT. (a) Transformer model for NMT: an example of translating Latin into English. It iteratively selects the next English word by referring to the Latin sentence and the English sentence translated up to the previous step. (b) In the process of learning to reproduce the input chemical formula, GCT learns the molecular structure and three different properties: $\log \mathrm{P}, \mathrm{tPSA}$, and QED. It represents the information of molecular structure and properties in the latent space during the learning process. (c) The trained GCT generates a de novo molecule that satisfies the target properties by decoding the molecular information sampled from the latent space and the given preconditions.</p>
<h1>Datasets \&amp; Benchmark.</h1>
<p>The GCT model was trained and benchmarked using a database of MOSES benchmarking platforms. The MOSES database is a benchmarking dataset for drug discovery created by sampling molecules from the ZINC is Not Commercial (ZINC) database ${ }^{38}$ —composed of commercially available compounds-that satisfy specific conditions: a weight in the range from 250 to 350 daltons, number of rotatable bonds is not greater than 7 , not containing charged atoms or atoms other than $\mathrm{C}, \mathrm{N}, \mathrm{S}, \mathrm{O}$, $\mathrm{F}, \mathrm{Cl}, \mathrm{Br}$, and H or cycles longer than 8 atoms. The MOSES database consists of training samples (1.7 M), test samples ( 176 k ), and scaffold test samples ( 176 k ), which have scaffolds that never appear in the training samples. It is also designed to closely match the distribution between the datasets. The three additional properties ( $\log \mathrm{P}, \mathrm{tPSA}$, and QED) computed from RDKit ${ }^{39}$ are used for GCT learning.</p>
<p>In general, the quality of network training can be evaluated by measuring how different the model's predicted and the actual labels are. However, for molecular generative models, the small mean loss does not guarantee that the generative model performs well because the artifacts in the generated molecules, which are not observed in the mean loss measurement, may not fit the chemical valence rule or may make the molecules unrealistic. For this reason, the quality of the generated molecules needs to be checked against the following criteria:</p>
<ul>
<li>How plausible are the generated molecules?</li>
<li>Do the generated molecules satisfy the target properties?</li>
<li>Can multiple candidates be generated for a single precondition set?</li>
<li>Can de novo molecules be created in a short time?</li>
</ul>
<p>In total, 30,000 SMILES strings are generated by the trained GCT model and evaluated by MOSES benchmarking score metrics (Table 1). In addition to relatively simple scores such as the validity, uniqueness, internal diversity, filters, and novelty, the MOSES benchmarking platform also provides metrics that can measure similarity with reference molecules such as the Similarity to a Nearest</p>
<p>Neighbor (SNN), ${ }^{30}$ Fréchet ChemNet Distance (FCD), ${ }^{40}$ Fragment similarity (Frag), ${ }^{30}$ and Scaffold similarity (Scaf). ${ }^{30}$</p>
<p>The SNN score is calculated via eq 6 :</p>
<p>$$
\operatorname{SNN}(G, R)=\frac{1}{|G|} \sum_{m_{G} \in G} \max <em R="R">{m</em>\right)
$$} \in R} T\left(m_{G}, m_{R</p>
<p>where $G$ and $R$ refer to the set of molecules generated and reference molecules, respectively, $m$ stands for Morgan fingerprints, ${ }^{41}$ and $T(A, B)$ stands for the Tanimoto similarity ${ }^{42}$ between set $A$ and set $B$.</p>
<p>The FCD uses activation of the penultimate layer in ChemNet and is designed to predict bioactivity. It calculates the difference in the distributions between $G$ and $R$ via eq 7 :</p>
<p>$$
\operatorname{FCD}(G, R)=\left|\mu_{G}-\mu_{R}\right|^{2}+\operatorname{Tr}\left(\sum_{G}+\sum_{R}-2\left(\sum_{G} \sum_{R}\right)^{1 / 2}\right)
$$</p>
<p>where $\mu$ is the mean, $\Sigma \quad$ is the covariance, and $\operatorname{Tr}(\cdot)$ is the trace operator.</p>
<p>The Frag score is calculated via eq 8 :</p>
<p>$$
\operatorname{Frag}(G, R)=\frac{\sum_{f \in F}\left(c_{f}(G) \cdot c_{f}(R)\right)}{\sqrt{\sum_{f \in F} c_{f}^{2}(G)} \sqrt{\sum_{f \in F} c_{f}^{2}(R)}}
$$</p>
<p>where $F$ is the set of 58,315 unique BRICS fragments, ${ }^{43}$ and $C_{f}(A)$ is the frequency with which fragment $f \in F$ appears in the molecules in set $A$.</p>
<p>The Scaf score is calculated via eq 9 :</p>
<p>$$
\operatorname{Scaf}(G, R)=\frac{\sum_{s \in S}\left(c_{s}(G) \cdot c_{s}(R)\right)}{\sqrt{\sum_{s \in S} c_{s}^{2}(G)} \sqrt{\sum_{s \in S} c_{s}^{2}(R)}}
$$</p>
<p>where $S$ is the set of 448,854 unique Bemis-Murcko scaffolds ${ }^{44}$ and $C_{S}(A)$ is the frequency at which scaffold $s \in S$ appears in the molecules in set A .</p>
<h1>Condition Sampling.</h1>
<p>The properties considered in this problem are $\log \mathrm{P}, \mathrm{tPSA}$, and QED. A three-dimensional histogram was derived after dividing each property into 1,000 equal sections between the maximum and minimum values in the training data. Then, the cells were sampled according to the probability that data samples exist in each cell; here, the probability is the number of samples in that cell out of the total samples. Next, uniform noise was added at the center value of the cell to create condition sets for the 30,000 molecules to be generated; the sizes of the uniform noise for $\log \mathrm{P}$-axis, tPSA-axis, and QED-axis are applied to not exceed the size of the cell sides in each axis direction.</p>
<h2>Latent Variable Sampling.</h2>
<p>As mentioned earlier, the dimension of latent variables are set to 128; 128-number of latent variables. However, since the number of tokens constituting a SMILES string is various for each molecule, the sequence length of the latent variables is applied differently each time; $\mathbb{R}^{128 \times \text { sequence_length }}$. Here, the sequence length means the number of tokens constituting the SMILES string. The sequence length used for each molecular generation was sampled from a normal distribution. The mean and variance of the normal distribution were derived from the number of tokens constituting the SMILES strings in the MOSES training dataset. After the sequence length is determined, the values of the latent variables are sampled from the standard normal distribution.</p>
<h1>RESULTS AND DISCUSSION</h1>
<p>Table 1 Comparison of the MOSES benchmarking results</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">GCT (ours)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MOSES Reference Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GCT- <br> WarmUp</td>
<td style="text-align: center;">GCT- <br> SGDR</td>
<td style="text-align: center;">$\mathrm{VAE}^{9,30}$</td>
<td style="text-align: center;">$\mathrm{AAE}^{30,45}$</td>
<td style="text-align: center;">Char <br> RNN ${ }^{9,30}$</td>
<td style="text-align: center;">Latent <br> GAN ${ }^{16,30}$</td>
<td style="text-align: center;">JTN <br> -VAE ${ }^{30,46}$</td>
</tr>
<tr>
<td style="text-align: center;">Validity ${ }^{a}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.9853</td>
<td style="text-align: center;">0.9916</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9767 \ &amp; \pm 0.0012 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9368 \pm \ &amp; 0.0341 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9748 \ &amp; \pm 0.0264 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8966 \ &amp; \pm 0.0029 \end{aligned}$</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">Unique@1k ${ }^{b}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
<td style="text-align: center;">$1.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;">Unique@10k ${ }^{\text {c }}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.9981</td>
<td style="text-align: center;">0.9797</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9984 \ &amp; \pm 0.0005 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9973 \ &amp; \pm 0.002 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9994 \ &amp; \pm 0.0003 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9968 \ &amp; \pm 0.0002 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9996 \ &amp; \pm 0.0003 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">IntDiv ${ }^{d}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8531</td>
<td style="text-align: center;">0.8458</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8558 \ &amp; \pm 0.0004 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8557 \ &amp; \pm 0.0031 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8562 \ &amp; \pm 0.0005 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8565 \ &amp; \pm 0.0007 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8551 \ &amp; \pm 0.0034 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Filters ${ }^{e}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.9956</td>
<td style="text-align: center;">0.9982</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9970 \ &amp; \pm 0.0002 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9960 \ &amp; \pm 0.0006 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9943 \ &amp; \pm 0.0034 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9735 \ &amp; \pm 0.0006 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9760 \ &amp; \pm 0.0016 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Novelty ${ }^{\text {f }}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.8144</td>
<td style="text-align: center;">0.6756</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.6949 \ &amp; \pm 0.0069 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.7931 \ &amp; \pm 0.0285 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8419 \ &amp; \pm 0.0509 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9498 \ &amp; \pm 0.0006 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9143 \ &amp; \pm 0.0058 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">SNN $^{9}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.6179</td>
<td style="text-align: center;">0.6513</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.6257 \ &amp; \pm 0.0005 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.6081 \ &amp; \pm 0.0043 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.6015 \ &amp; \pm 0.0206 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5371 \ &amp; \pm 0.0004 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5477 \ &amp; \pm 0.0076 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TestSF</td>
<td style="text-align: center;">0.5771</td>
<td style="text-align: center;">0.5990</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5783 \ &amp; \pm 0.0008 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5677 \ &amp; \pm 0.0045 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5649 \ &amp; \pm 0.0142 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5132 \ &amp; \pm 0.0002 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5194 \ &amp; \pm 0.007 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{FCD}^{\mathrm{h}}$</td>
<td style="text-align: center;">$\downarrow$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.4017</td>
<td style="text-align: center;">0.7980</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.0990 \ &amp; \pm 0.0125 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5555 \ &amp; \pm 0.2033 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.0732 \ &amp; \pm 0.0247 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.2968 \ &amp; \pm 0.0087 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.3954 \ &amp; \pm 0.0234 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TestSF</td>
<td style="text-align: center;">0.8031</td>
<td style="text-align: center;">0.9949</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5670 \ &amp; \pm 0.0338 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 1.0572 \ &amp; \pm 0.2375 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.5204 \ &amp; \pm 0.0379 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8281 \ &amp; \pm 0.0117 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9382 \ &amp; \pm 0.0531 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Frag ${ }^{i}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.9973</td>
<td style="text-align: center;">0.9922</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9994 \ &amp; \pm 0.0001 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9910 \ &amp; \pm 0.0051 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9998 \ &amp; \pm 0.0002 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9986 \ &amp; \pm 0.0004 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9965 \ &amp; \pm 0.0003 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TestSF</td>
<td style="text-align: center;">0.9952</td>
<td style="text-align: center;">0.8562</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9984 \ &amp; \pm 0.0003 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9905 \ &amp; \pm 0.0039 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9983 \ &amp; \pm 0.0003 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9972 \ &amp; \pm 0.0007 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9947 \ &amp; \pm 0.0002 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Sca ${ }^{8}$</td>
<td style="text-align: center;">$\uparrow$</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.8905</td>
<td style="text-align: center;">0.8562</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9386 \ &amp; \pm 0.0021 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9022 \ &amp; \pm 0.0375 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.9242 \ &amp; \pm 0.0058 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8867 \ &amp; \pm 0.0009 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.8964 \ &amp; \pm 0.0039 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TestSF</td>
<td style="text-align: center;">0.0921</td>
<td style="text-align: center;">0.0551</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.0588 \ &amp; \pm 0.0095 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.0789 \ &amp; \pm 0.009 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.1101 \ &amp; \pm 0.0081 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.1072 \ &amp; \pm 0.0098 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0.1009 \ &amp; \pm 0.0105 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>MOSES benchmarking was performed on 30,000 SMILES strings generated by GCT. Then, the scores obtained from GCT were compared with the scores of other generative models provided by the MOSES benchmarking platform: VAE, AAE, CharRNN, LatentGAN, and JTN-VAE. Ten indicators were used to evaluate the quality of the resulting molecules (a-j). aThe ratio of valid SMILES strings. ${ }^{\text {b }}$ The ratio of unique samples out of 1,000 generated molecules. cThe ratio of unique samples out of 10,000 generated molecules. dThe internal diversity of the generated molecules. eThe ratio of passing through toxic substance filters. fThe ratio of generated molecules that do not exist in the training data. gThe average Tanimoto similarity for all generated molecules; the similarity between a generated molecule and the most similar molecule among the reference data is calculated. hMeasurement of the distance between the generated molecules and the reference molecules using the activation of the penultimate layer of the ChemNet bioactivity prediction model. iThe cosine similarity between the frequency in which the BRICS appear in the reference data and the frequency appearing in the generated molecules. ${ }^{j}$ The cosine similarity between the frequency in which specific scaffolds appear in the</p>
<p>reference data and the frequency appearing in the generated molecules. The up arrow means that a higher value is better, and the down arrow means that a lower value is better.</p>
<h1>How Plausible Are the Generated Molecules?</h1>
<p>To evaluate whether the generated SMILES strings represent plausible molecular structures, analysis from two perspectives is required. The first analysis is whether the generated SMILES strings can generate valid molecular graphs, in other words, whether the generated SMILES strings satisfy both the chemical valence rule and the syntax of the SMILES language. From the benchmarking results, it was found that more than $98.5 \%$ of the generated SMILES strings are valid; GCT-WarmUp shows the validity of $98.5 \%$ and GCT-SGDR shows the validity of $99.2 \%$. It is the highest value among languagebased models (Table 1a). This ability depends on how well the generative machine can understand the geometry of molecules through SMILES strings. To determine the character (corresponding to atom, bond, or branch) followed by the given chemical string that satisfies the chemical valence rule and the grammar of the SMILES language, the geometry of the molecules (connectivity of each atom and the branches) present in the string must be understood. It seems that the attention mechanism applied to GCT helps the neural network to understand the grammar of chemical language beyond the semantic discontinuity of the SMILES language. It tends to be consistent with the results (visualized example of attention score for diproxadol) shown in ref ${ }^{47}$.</p>
<p>Figure 3 shows the results for two extreme examples of how to pay attention to the characters within the SMILES string. Atom1, atom2, and atom13 in Figure 3a are located close to each other in the molecular graphs but far away from each other within the SMILES string. Although only a SMILES string was provided to GCT, it is recognized that atom1, atom2, and atom13 are related to each other $(\bullet)$; Figure 3b shows that GCT-SGDR recognizes the relationship between atom2 and atom22, 23 ( $\boldsymbol{\sim}$ ). It also recognizes atoms corresponding to a particular branch $(\boldsymbol{\Delta})$ and recognizes the ring type of branch $(\boldsymbol{\nabla})$. Each attention-head recognizes the molecular structure according to different viewpoints. In</p>
<p>addition to the attention-heads visualized in Figure 3a and Figure 3b, the other attention-heads are shown in Figure S2. In summary, the attention mechanism applied to GCT seems to help GCT to recognize the molecular structure hidden in the one-dimensional text. This claim is consistent with the claim of ref. ${ }^{47}$ regarding the role of attention mechanism.</p>
<p>The second analysis is how similar the molecular structures parsed from the valid SMILES strings are similar to real molecules. Four metrics can measure this: the SNN, FCD, Frag, and Scaf. Four metrics were evaluated on two test sets present in MOSES: Test set and TestSF set. Molecules constituting the test set have the same scaffolds of molecules constituting the train set. On the other hand, TestSF consists of molecules including scaffolds that are not included in the train set. Except for SNN (Test) and SNN (TestSF) of GCT-SGDR, and Scaf (TestSF) of GCT-WarmUp, VAE shows better scores than GCT. However, compared to the scores of other reference models, it doesn't seem that the scores of GCT are poor. SNN (Test) and SNN (TestSF) of GCT-WarmUp are higher than the score of all the other reference models except VAE. FCD (Test) of GCT-WarmUp is better than AAE and FCD (TestSF) of GCT-WarmUp is better than FCD (TestSF) of AAE, LatentGAN, and JTN-VAE. Frag (Test) and Frag (TestSF) of GCT-WarmUp is higher than Frag (Test) and Frag (TestSF) of AAE and JTNVAE. Scaf (Test) of GCT-WarmUp is higher than LatentGAN and Scaf (TestSF) of GCT-WarmUp is higher than VAE and AAE.</p>
<p>a
<img alt="img-2.jpeg" src="img-2.jpeg" />
b
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3. Visualized examples for self-attention scores of an attention head in encoder blocks. (a) Visualization results of the fourth head in the second encoder block for 4-(2-aminopropyl)-2-methoxyphenol. (b) Visualization results of the third head in the second encoder block for 5,6-bis(p-methoxyphenyl)-3-methyl-1,2,4-triazine. These are two extreme cases where the atom adjacent to atom2 in each molecular graph (atom13 in a and atom23 in $\mathbf{b}$ is far away in each SMILES string. The yellow lines in the black box at the bottom of the figure show which tokens were given a high attention score from the query vector by attention. The higher the attention score, the opaquer the yellow line. Each color in the SMILES string and the molecular graph represents each branch of the SMILES string separated with parentheses. The visualization scheme is borrowed from ref. ${ }^{48}$.</p>
<h1>Do the Generated Molecules Satisfy the Target Properties?</h1>
<p>GCT generates molecular structures that satisfy multiple target properties. Figure 4a-c are the results of comparing the properties of 30,000 molecules generated from GCT (calculated from RDKit) and target properties (preconditions given in GCT). Since $\log \mathrm{P}$ and tPSA are physical properties directly related to the molecular structure, it is possible to generate a molecular structure corresponding to the target property based on an understanding of the molecular structure. However, the QED is an artificial index designed to determine the likeness to drugs quantitatively through geometric averages</p>
<p>of eight different properties, so it is relatively difficult for the QED; this phenomenon is also found with cRNNs. The absolute mean errors between the target conditions for each property and the properties of the generated molecule are $0.177(\log P), 2.923$ (tPSA), and 0.035 (QED).</p>
<p>The length of the generated SMILES string depends on the length of the latent code since GCT has an autoencoder (AE) structure; it is trained to reconstruct information input into the encoder. In the training phase, the length of the latent code appears equal to the length of the string input into the encoder and the length of the string output from the decoder; in fact, these are slightly different depending on whether the $&lt;$ sos $&gt;$ and $&lt;$ eos $&gt;$ tokens are used in the input and output design; however, the nature of the string is not different (see SI). In the inference phase, the length of the input latent code and the length of the generated SMILES string did not match perfectly and GCT does not learn the distribution of sequence lengths (Figure 4d). However, it seems that the length of the generated SMILES string which is related to the size of the molecule can be manipulated to some extent by adjusting the length of the latent code.</p>
<p>To check whether the multiple given target properties are satisfied simultaneously, the properties of generated molecules were compared to the 10 precondition sets that were sampled from the distribution of training data (Figure 4e-g). The conditional model, which is a skeleton of GCT, generates molecules that simultaneously satisfy multiple target properties well. Furthermore, the variational generator in GCT makes it possible to generate various molecules under the same precondition set (Figure S3-12).</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4 Comparison of target properties (preconditions given to GCT) and properties of 30,000 generated molecules. (a)-(c), The x -axis refers to the target property, and the y -axis refers to the property of the generated molecule. (d), The x -axis refers to the length of the latent code, and the y -axis refers to the number of tokens constituting the generated SMILES string. (e)-(g), 10 precondition sets randomly sampled from the MOSES data distribution (red line) and properties of the generated molecules (blue dots). (h), The length of the latent code randomly sampled from the MOSES data distribution (red marker) and the number of tokens constituting the generated molecule (blue dots).</p>
<h1>Can De Novo Molecules Be Created?</h1>
<p>Whether a generative model can create de novo molecules is an important criterion that determines its applicability for material discovery. The novelty score refers to the probability of generating a new molecule that does not exist in the training data (Table 1f). Note that only a high novelty score does not guarantee that it is a good generator since odd and unrealistic molecules can increase the novelty score. Hence, the novelty score should be used in conjunction with indicators to</p>            </div>
        </div>

    </div>
</body>
</html>
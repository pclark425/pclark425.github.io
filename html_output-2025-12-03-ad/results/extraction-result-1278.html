<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1278 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1278</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1278</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-789783016fb708abbc061790612ebe91273c05d3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/789783016fb708abbc061790612ebe91273c05d3" target="_blank">(More) Efficient Reinforcement Learning via Posterior Sampling</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> An O(τS/√AT) bound on expected regret is established, one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm.</p>
                <p><strong>Paper Abstract:</strong> Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an O(τS/√AT) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1278.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1278.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PSRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Posterior Sampling for Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An episodic reinforcement learning algorithm that at the start of each episode samples a full MDP from the posterior over environments and then follows the policy that is optimal for that sampled MDP for the episode; exploration arises from posterior uncertainty (Thompson sampling style).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>(More) Efficient Reinforcement Learning via Posterior Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Posterior Sampling for Reinforcement Learning (PSRL)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>PSRL maintains a Bayesian posterior over MDPs (transition probabilities and reward distributions). At the start of each episode it samples one MDP from the posterior, computes an optimal policy for that sampled MDP (via standard planning / dynamic programming), and executes that policy for the duration of the episode; the posterior is updated from observed transitions and rewards. Key components: prior over MDP (e.g., Dirichlet for transitions, normal-gamma for rewards), posterior update from history, sample MDP, planner (to compute optimal policy for sampled MDP), episodic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Thompson sampling / posterior sampling (Bayesian adaptive experimental design)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>PSRL adapts by maintaining a posterior distribution over unknown MDP parameters (transitions and rewards) and sampling a complete MDP from this posterior at the start of each episode; the sampled MDP determines a policy that is executed for that episode. The choice of what to try next is governed by posterior uncertainty — policies are selected with probability equal to their posterior probability of being optimal, so exploration is proportional to uncertainty and concentrates as the posterior sharpens with observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Finite-horizon Markov Decision Processes (MDPs); experiments on RiverSwim and randomly generated discrete MDPs</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic discrete MDPs with finite state and action spaces, rewards bounded in [0,1], episodic finite horizon (or converted to finite-horizon via effective horizon for discounted problems). Environments used in experiments: RiverSwim (6-state chain with stochastic transitions and sparse large reward to the right) and randomly sampled 10-state, 5-action MDPs. The paper assumes full observability of states (standard MDP), not POMDPs.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Discrete state/action spaces: e.g., RiverSwim (S=6, A=2, episode length τ=20 in experiments); random MDP experiments used S=10, A=5, episodes or infinite-horizon variants up to T=10,000 or 100,000 timesteps in simulations. The theoretical results assume finite S, A and horizon τ; rewards in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Theoretical Bayesian regret bound: E[Regret(T)] = O(τ S √(A T log(S A T))) (stated as ~tilde{O}(τ S √(A T))). Empirical cumulative regret (over 10,000 time steps, 20 Monte Carlo runs): Random MDP (τ-episodes): PSRL = 1.04e4 cumulative regret; Random MDP (∞-horizon): PSRL = 7.30e3; RiverSwim (τ-episodes, τ=20): PSRL = 6.88e1; RiverSwim (∞-horizon): PSRL = 1.06e2. Figures show PSRL dramatically outperforms UCRL2 on RiverSwim, including on longer runs (up to 100,000 steps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline optimistic algorithm UCRL2 (same experimental settings): Random MDP (τ-episodes): UCRL2 = 5.92e4; Random MDP (∞-horizon): UCRL2 = 1.13e5; RiverSwim (τ-episodes): UCRL2 = 1.26e3; RiverSwim (∞-horizon): UCRL2 = 3.64e3 (all cumulative regret over 10,000 steps). PSRL outperforms this optimistic baseline by large margins in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Analytic scaling indicates PSRL is sample-efficient with Bayesian regret growing as O(τ S √(A T log(S A T))), i.e., sublinear in T and roughly matching state-of-the-art dependence on S, A and T (near √T). Empirically, PSRL attains much lower cumulative regret within 10k steps in tested MDPs (see numeric regrets above) and continues to outperform on 100k-step runs in RiverSwim.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration occurs implicitly via posterior sampling: policies are chosen according to posterior probability of optimality (probability matching). As more data is observed the posterior concentrates around the true MDP, reducing randomness and shifting behavior toward exploitation of high-value policies. No explicit optimism/bonus terms are used; exploration rate adapts naturally to uncertainty in posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Empirical comparison to UCRL2 (optimistic algorithm). Discussion and analytical comparison to optimistic approaches including UCRL2 and REGAL; related mention of BOSS and BEB as other Bayesian/optimistic algorithms in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>1) The paper proves a Bayesian regret bound for PSRL: E[Regret(T)] = O(τ S √(A T log(S A T))). 2) This is one of the first non-optimism-based algorithms with such a finite-time regret guarantee, and the bound is close to state-of-the-art. 3) Empirically, PSRL substantially outperforms UCRL2 on RiverSwim and randomly generated MDPs (orders-of-magnitude lower cumulative regret in some cases). 4) PSRL is computationally simpler: requires solving one sampled MDP per episode rather than optimizing over confidence sets. 5) PSRL is robust to prior misspecification in experiments (the paper shows PSRL learns quickly despite a misspecified prior).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>1) The theoretical guarantee is Bayesian (expected under the prior) — frequentist guarantees are only derived asymptotically or for MDPs with non-zero prior probability; worst-case frequentist bounds are not the main focus. 2) Analysis assumes finite discrete state/action spaces and bounded rewards; extensions to continuous or partially observable environments are not provided. 3) Computational cost may still be non-trivial if planning in the sampled MDP is expensive, though simpler than optimistic set planning. 4) Confidence bounds used in the analysis are conservative; constants could be tightened. 5) No experiments on partially observable (POMDP) or non-stationary environments are reported, so applicability there is untested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>The algorithm variant for the infinite-horizon (non-episodic) setting can use doubling episodes (start new episode when visit-counts double), and the paper notes PSRL can be applied similarly; also the analysis introduces confidence sets primarily for bounding regret though the algorithm itself uses posterior sampling rather than explicit optimism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '(More) Efficient Reinforcement Learning via Posterior Sampling', 'publication_date_yy_mm': '2013-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Bayesian framework for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Near-optimal regret bounds for reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to optimize via posterior sampling <em>(Rating: 2)</em></li>
                <li>Near-Bayesian exploration in polynomial time <em>(Rating: 1)</em></li>
                <li>Bayesian sparse sampling for on-line reward optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1278",
    "paper_id": "paper-789783016fb708abbc061790612ebe91273c05d3",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "PSRL",
            "name_full": "Posterior Sampling for Reinforcement Learning",
            "brief_description": "An episodic reinforcement learning algorithm that at the start of each episode samples a full MDP from the posterior over environments and then follows the policy that is optimal for that sampled MDP for the episode; exploration arises from posterior uncertainty (Thompson sampling style).",
            "citation_title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
            "mention_or_use": "use",
            "agent_name": "Posterior Sampling for Reinforcement Learning (PSRL)",
            "agent_description": "PSRL maintains a Bayesian posterior over MDPs (transition probabilities and reward distributions). At the start of each episode it samples one MDP from the posterior, computes an optimal policy for that sampled MDP (via standard planning / dynamic programming), and executes that policy for the duration of the episode; the posterior is updated from observed transitions and rewards. Key components: prior over MDP (e.g., Dirichlet for transitions, normal-gamma for rewards), posterior update from history, sample MDP, planner (to compute optimal policy for sampled MDP), episodic execution.",
            "adaptive_design_method": "Thompson sampling / posterior sampling (Bayesian adaptive experimental design)",
            "adaptation_strategy_description": "PSRL adapts by maintaining a posterior distribution over unknown MDP parameters (transitions and rewards) and sampling a complete MDP from this posterior at the start of each episode; the sampled MDP determines a policy that is executed for that episode. The choice of what to try next is governed by posterior uncertainty — policies are selected with probability equal to their posterior probability of being optimal, so exploration is proportional to uncertainty and concentrates as the posterior sharpens with observed data.",
            "environment_name": "Finite-horizon Markov Decision Processes (MDPs); experiments on RiverSwim and randomly generated discrete MDPs",
            "environment_characteristics": "Unknown stochastic discrete MDPs with finite state and action spaces, rewards bounded in [0,1], episodic finite horizon (or converted to finite-horizon via effective horizon for discounted problems). Environments used in experiments: RiverSwim (6-state chain with stochastic transitions and sparse large reward to the right) and randomly sampled 10-state, 5-action MDPs. The paper assumes full observability of states (standard MDP), not POMDPs.",
            "environment_complexity": "Discrete state/action spaces: e.g., RiverSwim (S=6, A=2, episode length τ=20 in experiments); random MDP experiments used S=10, A=5, episodes or infinite-horizon variants up to T=10,000 or 100,000 timesteps in simulations. The theoretical results assume finite S, A and horizon τ; rewards in [0,1].",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Theoretical Bayesian regret bound: E[Regret(T)] = O(τ S √(A T log(S A T))) (stated as ~tilde{O}(τ S √(A T))). Empirical cumulative regret (over 10,000 time steps, 20 Monte Carlo runs): Random MDP (τ-episodes): PSRL = 1.04e4 cumulative regret; Random MDP (∞-horizon): PSRL = 7.30e3; RiverSwim (τ-episodes, τ=20): PSRL = 6.88e1; RiverSwim (∞-horizon): PSRL = 1.06e2. Figures show PSRL dramatically outperforms UCRL2 on RiverSwim, including on longer runs (up to 100,000 steps).",
            "performance_without_adaptation": "Baseline optimistic algorithm UCRL2 (same experimental settings): Random MDP (τ-episodes): UCRL2 = 5.92e4; Random MDP (∞-horizon): UCRL2 = 1.13e5; RiverSwim (τ-episodes): UCRL2 = 1.26e3; RiverSwim (∞-horizon): UCRL2 = 3.64e3 (all cumulative regret over 10,000 steps). PSRL outperforms this optimistic baseline by large margins in the reported experiments.",
            "sample_efficiency": "Analytic scaling indicates PSRL is sample-efficient with Bayesian regret growing as O(τ S √(A T log(S A T))), i.e., sublinear in T and roughly matching state-of-the-art dependence on S, A and T (near √T). Empirically, PSRL attains much lower cumulative regret within 10k steps in tested MDPs (see numeric regrets above) and continues to outperform on 100k-step runs in RiverSwim.",
            "exploration_exploitation_tradeoff": "Exploration occurs implicitly via posterior sampling: policies are chosen according to posterior probability of optimality (probability matching). As more data is observed the posterior concentrates around the true MDP, reducing randomness and shifting behavior toward exploitation of high-value policies. No explicit optimism/bonus terms are used; exploration rate adapts naturally to uncertainty in posterior.",
            "comparison_methods": "Empirical comparison to UCRL2 (optimistic algorithm). Discussion and analytical comparison to optimistic approaches including UCRL2 and REGAL; related mention of BOSS and BEB as other Bayesian/optimistic algorithms in literature.",
            "key_results": "1) The paper proves a Bayesian regret bound for PSRL: E[Regret(T)] = O(τ S √(A T log(S A T))). 2) This is one of the first non-optimism-based algorithms with such a finite-time regret guarantee, and the bound is close to state-of-the-art. 3) Empirically, PSRL substantially outperforms UCRL2 on RiverSwim and randomly generated MDPs (orders-of-magnitude lower cumulative regret in some cases). 4) PSRL is computationally simpler: requires solving one sampled MDP per episode rather than optimizing over confidence sets. 5) PSRL is robust to prior misspecification in experiments (the paper shows PSRL learns quickly despite a misspecified prior).",
            "limitations_or_failures": "1) The theoretical guarantee is Bayesian (expected under the prior) — frequentist guarantees are only derived asymptotically or for MDPs with non-zero prior probability; worst-case frequentist bounds are not the main focus. 2) Analysis assumes finite discrete state/action spaces and bounded rewards; extensions to continuous or partially observable environments are not provided. 3) Computational cost may still be non-trivial if planning in the sampled MDP is expensive, though simpler than optimistic set planning. 4) Confidence bounds used in the analysis are conservative; constants could be tightened. 5) No experiments on partially observable (POMDP) or non-stationary environments are reported, so applicability there is untested in this paper.",
            "additional_notes": "The algorithm variant for the infinite-horizon (non-episodic) setting can use doubling episodes (start new episode when visit-counts double), and the paper notes PSRL can be applied similarly; also the analysis introduces confidence sets primarily for bounding regret though the algorithm itself uses posterior sampling rather than explicit optimism.",
            "uuid": "e1278.0",
            "source_info": {
                "paper_title": "(More) Efficient Reinforcement Learning via Posterior Sampling",
                "publication_date_yy_mm": "2013-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Bayesian framework for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Near-optimal regret bounds for reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Learning to optimize via posterior sampling",
            "rating": 2
        },
        {
            "paper_title": "Near-Bayesian exploration in polynomial time",
            "rating": 1
        },
        {
            "paper_title": "Bayesian sparse sampling for on-line reward optimization",
            "rating": 1
        }
    ],
    "cost": 0.009736249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>(More) Efficient Reinforcement Learning via Posterior Sampling</h1>
<p>Ian Osband<br>Stanford University<br>Stanford, CA 94305<br>iosband@stanford.edu</p>
<p>Benjamin Van Roy<br>Stanford University<br>Stanford, CA 94305<br>bvr@stanford.edu</p>
<p>Daniel Russo<br>Stanford University<br>Stanford, CA 94305<br>djrusso@stanford.edu</p>
<h4>Abstract</h4>
<p>Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an $\tilde{O}(\tau S \sqrt{A T})$ bound on expected regret, where $T$ is time, $\tau$ is the episode length and $S$ and $A$ are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.</p>
<h2>1 Introduction</h2>
<p>We consider the classical reinforcement learning problem of an agent interacting with its environment while trying to maximize total reward accumulated over time [1, 2]. The agent's environment is modeled as a Markov decision process (MDP), but the agent is uncertain about the true dynamics of the MDP. As the agent interacts with its environment, it observes the outcomes that result from previous states and actions, and learns about the system dynamics. This leads to a fundamental tradeoff: by exploring poorly-understood states and actions the agent can learn to improve future performance, but it may attain better short-run performance by exploiting its existing knowledge.</p>
<p>Naïve optimization using point estimates for unknown variables overstates an agent's knowledge, and can lead to premature and suboptimal exploitation. To offset this, the majority of provably efficient learning algorithms use a principle known as optimism in the face of uncertainty [3] to encourage exploration. In such an algorithm, each state and action is afforded some optimism bonus such that their value to the agent is modeled to be as high as is statistically plausible. The agent will then choose a policy that is optimal under this "optimistic" model of the environment. This incentivizes exploration since poorly-understood states and actions will receive a higher optimism bonus. As the agent resolves its uncertainty, the effect of optimism is reduced and the agent's behavior approaches optimality. Many authors have provided strong theoretical guarantees for optimistic algorithms [4, 5, 6, 7, 8]. In fact, almost all reinforcement learning algorithms with polynomial bounds on sample complexity employ optimism to guide exploration.</p>
<p>We study an alternative approach to efficient exploration, posterior sampling, and provide finite time bounds on regret. We model the agent's initial uncertainty over the environment through a prior distribution. ${ }^{1}$ At the start of each episode, the agent chooses a new policy, which it follows for the duration of the episode. Posterior sampling for reinforcement learning (PSRL) selects this policy through two simple steps. First, a single instance of the environment is sampled from the posterior distribution at the start of an episode. Then, PSRL solves for and executes the policy that is optimal under the sampled environment over the episode. PSRL randomly selects policies according to the probability they are optimal; exploration is guided by the variance of sampled policies as opposed to optimism.</p>
<p>The idea of posterior sampling goes back to 1933 [9] and has been applied successfully to multi-armed bandits. In that literature, the algorithm is often referred to as Thompson sampling or as probability matching. Despite its long history, posterior sampling was largely neglected by the multi-armed bandit literature until empirical studies [10, 11] demonstrated that the algorithm could produce state of the art performance. This prompted a surge of interest, and a variety of strong theoretical guarantees are now available [12, 13, 14, 15]. Our results suggest this method has great potential in reinforcement learning as well.</p>
<p>PSRL was originally introduced in the context of reinforcement learning by Strens [16] under the name "Bayesian Dynamic Programming", ${ }^{2}$ where it appeared primarily as a heuristic method. In reference to PSRL and other "Bayesian RL" algorithms, Kolter and Ng [17] write "little is known about these algorithms from a theoretical perspective, and it is unclear, what (if any) formal guarantees can be made for such approaches." Those Bayesian algorithms for which performance guarantees exist are guided by optimism. BOSS [18] introduces a more complicated version of PSRL that samples many MDPs, instead of just one, and then combines them into an optimistic environment to guide exploration. BEB [17] adds an exploration bonus to states and actions according to how infrequently they have been visited. We show it is not always necessary to introduce optimism via a complicated construction, and that the simple algorithm originally proposed by Strens [16] satisfies strong bounds itself.</p>
<p>Our work is motivated by several advantages of posterior sampling relative to optimistic algorithms. First, since PSRL only requires solving for an optimal policy for a single sampled MDP, it is computationally efficient both relative to many optimistic methods, which require simultaneous optimization across a family of plausible environments [4, 5, 18], and to computationally intensive approaches that attempt to approximate the Bayes-optimal solutions directly [18, 19, 20]. Second, the presence of an explicit prior allows an agent to incorporate known environment structure in a natural way. This is crucial for most practical applications, as learning without prior knowledge requires exhaustive experimentation in each possible state. Finally, posterior sampling allows us to separate the algorithm from the analysis. In any optimistic algorithm, performance is greatly influenced by the manner in which optimism is implemented. Past works have designed algorithms, at least in part, to facilitate theoretical analysis for toy problems. Although our analysis of posterior sampling is closely related to the analysis in [4], this worst-case bound has no impact on the algorithm's actual performance. In addition, PSRL is naturally suited to more complex settings where design of an efficiently optimistic algorithm might not be possible. We demonstrate through a computational study in Section 6 that PSRL outperforms the optimistic algorithm UCRL2 [4]: a competitor with similar regret bounds over some example MDPs.</p>
<h1>2 Problem formulation</h1>
<p>We consider the problem of learning to optimize a random finite horizon MDP $M=$ $\left(\mathcal{S}, \mathcal{A}, R^{M}, P^{M}, \tau, \rho\right)$ in repeated finite episodes of interaction. $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $R_{a}^{M}(s)$ is a probability distribution over reward realized when selecting action $a$ while in state $s$ whose support is $[0,1], P_{a}^{M}\left(s^{\prime} \mid s\right)$ is the probability of transitioning to state $s^{\prime}$ if action $a$ is selected while at state $s, \tau$ is the time horizon, and $\rho$ the initial state distribution. We define the MDP and all other random variables we will consider with</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>respect to a probability space $(\Omega, \mathcal{F}, \mathbb{P})$. We assume $\mathcal{S}, \mathcal{A}$, and $\tau$ are deterministic so the agent need not learn the state and action spaces or the time horizon.</p>
<p>A deterministic policy $\mu$ is a function mapping each state $s \in \mathcal{S}$ and $i=1, \ldots, \tau$ to an action $a \in \mathcal{A}$. For each MDP $M=\left(\mathcal{S}, \mathcal{A}, R^{M}, P^{M}, \tau, \rho\right)$ and policy $\mu$, we define a value function</p>
<p>$$
V_{\mu, i}^{M}(s):=\mathbf{E}<em j="i">{M, \mu}\left[\sum</em>}^{\tau} \bar{R<em j="j">{a</em>=s\right]
$$}}^{M}\left(s_{j}\right) \mid s_{i</p>
<p>where $\bar{R}<em j="j">{a}^{M}(s)$ denotes the expected reward realized when action $a$ is selected while in state $s$, and the subscripts of the expectation operator indicate that $a</em>(s)=$ $\max }=\mu\left(s_{j}, j\right)$, and $s_{j+1} \sim$ $P_{a_{j}}^{M}\left(\cdot \mid s_{j}\right)$ for $j=i, \ldots, \tau$. A policy $\mu$ is said to be optimal for MDP $M$ if $V_{\mu, i}^{M<em _mu_prime="\mu^{\prime">{\mu^{\prime}} V</em>$ that is optimal for $M$.
The reinforcement learning agent interacts with the MDP over episodes that begin at times $t_{k}=(k-1) \tau+1, k=1,2, \ldots$ At each time $t$, the agent selects an action $a_{t}$, observes a scalar reward $r_{t}$, and then transitions to $s_{t+1}$. If an agent follows a policy $\mu$ then when in state $s$ at time $t$ during episode $k$, it selects an action $a_{t}=\mu\left(s, t-t_{k}\right)$. Let $H_{t}=$ $\left(s_{1}, a_{1}, r_{1}, \ldots, s_{t-1}, a_{t-1}, r_{t-1}\right)$ denote the history of observations made prior to time $t$. A reinforcement learning algorithm is a deterministic sequence $\left{\pi_{k} \mid k=1,2, \ldots\right}$ of functions, each mapping $H_{t_{k}}$ to a probability distribution $\pi_{k}\left(H_{t_{k}}\right)$ over policies. At the start of the $k$ th episode, the algorithm samples a policy $\mu_{k}$ from the distribution $\pi_{k}\left(H_{t_{k}}\right)$. The algorithm then selects actions $a_{t}=\mu_{k}\left(s_{t}, t-t_{k}\right)$ at times $t$ during the $k$ th episode.
We define the regret incurred by a reinforcement learning algorithm $\pi$ up to time $T$ to be}, i}^{M}(s)$ for all $s \in \mathcal{S}$ and $i=1, \ldots, \tau$. We will associate with each MDP $M$ a policy $\mu^{M</p>
<p>$$
\operatorname{Regret}(T, \pi):=\sum_{k=1}^{\lceil T / \tau\rceil} \Delta_{k}
$$</p>
<p>where $\Delta_{k}$ denotes regret over the $k$ th episode, defined with respect to the MDP $M^{*}$ by</p>
<p>$$
\Delta_{k}=\sum_{s \in \mathcal{S}} \rho(s)\left(V_{\mu^{<em>}, 1}^{M^{</em>}}(s)-V_{\mu_{k}, 1}^{M^{*}}(s)\right)
$$</p>
<p>with $\mu^{<em>}=\mu^{M^{</em>}}$ and $\mu_{k} \sim \pi_{k}\left(H_{t_{k}}\right)$. Note that regret is not deterministic since it can depend on the random MDP $M^{*}$, the algorithm's internal random sampling and, through the history $H_{t_{k}}$, on previous random transitions and random rewards. We will assess and compare algorithm performance in terms of regret and its expectation.</p>
<h1>3 Posterior sampling for reinforcement learning</h1>
<p>The use of posterior sampling for reinforcement learning (PSRL) was first proposed by Strens [16]. PSRL begins with a prior distribution over MDPs with states $\mathcal{S}$, actions $\mathcal{A}$ and horizon $\tau$. At the start of each $k$ th episode, PSRL samples an MDP $M_{k}$ from the posterior distribution conditioned on the history $H_{t_{k}}$ available at that time. PSRL then computes and follows the policy $\mu_{k}=\mu^{M_{k}}$ over episode $k$.</p>
<h2>Algorithm: Posterior Sampling for Reinforcement Learning (PSRL)</h2>
<p>Data: Prior distribution $f, \mathrm{t}=1$
for episodes $k=1,2, \ldots$ do
sample $M_{k} \sim f\left(\cdot \mid H_{t_{k}}\right)$
compute $\mu_{k}=\mu^{M_{k}}$
for timesteps $j=1, \ldots, \tau$ do
sample and apply $a_{t}=\mu_{k}\left(s_{t}, j\right)$
observe $r_{t}$ and $s_{t+1}$
$t=t+1$
end
end</p>
<p>We show PSRL obeys performance guarantees intimately related to those for learning algorithms based upon OFU, as has been demonstrated for multi-armed bandit problems [15]. We believe that a posterior sampling approach offers some inherent advantages. Optimistic algorithms require explicit construction of the confidence bounds on $V_{\mu, 1}^{M^{<em>}}(s)$ based on observed data, which is a complicated statistical problem even for simple models. In addition, even if strong confidence bounds for $V_{\mu, 1}^{M^{</em>}}(s)$ were known, solving for the best optimistic policy may be computationally intractable. Algorithms such as UCRL2 [4] are computationally tractable, but must resort to separately bounding $\bar{R}<em a="a">{a}^{M}(s)$ and $P</em>(s)$ with high probability for each $s, a$. These bounds allow a "worst-case" mis-estimation simultaneously in every state-action pair and consequently give rise to a confidence set which may be far too conservative.}^{M</p>
<p>By contrast, PSRL always selects policies according to the probability they are optimal. Uncertainty about each policy is quantified in a statistically efficient way through the posterior distribution. The algorithm only requires a single sample from the posterior, which may be approximated through algorithms such as Metropolis-Hastings if no closed form exists. As such, we believe PSRL will be simpler to implement, computationally cheaper and statistically more efficient than existing optimistic methods.</p>
<h1>3.1 Main results</h1>
<p>The following result establishes regret bounds for PSRL. The bounds have $\tilde{O}(\tau S \sqrt{A T})$ expected regret, and, to our knowledge, provide the first guarantees for an algorithm not based upon optimism:
Theorem 1. If $f$ is the distribution of $M^{*}$ then,</p>
<p>$$
\mathbb{E}\left[\operatorname{Regret}\left(T, \pi_{\tau}^{\mathrm{PS}}\right)\right]=O(\tau S \sqrt{A T \log (S A T)})
$$</p>
<p>This result holds for any prior distribution on MDPs, and so applies to an immense class of models. To accommodate this generality, the result bounds expected regret under the prior distribution (sometimes called Bayes risk or Bayesian regret). We feel this is a natural measure of performance, but should emphasize that it is more common in the literature to bound regret under a worst-case MDP instance. The next result provides a link between these notions of regret. Applying Markov's inequality to (1) gives convergence in probability.
Corollary 1. If $f$ is the distribution of $M^{*}$ then for any $\alpha&gt;\frac{1}{2}$,</p>
<p>$$
\frac{\operatorname{Regret}\left(T, \pi_{\tau}^{\mathrm{PS}}\right)}{T^{\alpha}} \underset{p}{\rightarrow} 0
$$</p>
<p>As shown in the appendix, this also bounds the frequentist regret for any MDP with non-zero probability. State-of-the-art guarantees similar to Theorem 1 are satisfied by the algorithms UCRL2 [4] and REGAL [5] for the case of non-episodic RL. Here UCRL2 gives regret bounds $\tilde{O}(D S \sqrt{A T})$ where $D=\max <em _pi="\pi">{s^{\prime} \neq s} \min </em>)$ where $\Psi \leq D$ is the span of the of the optimal value function. However, there is so far no computationally tractable implementation of this algorithm.} \mathbb{E}\left[T\left(s^{\prime} \mid M, \pi, s\right)\right]$ and $T\left(s^{\prime} \mid M, \pi, s\right)$ is the first time step where $s^{\prime}$ is reached from $s$ under the policy $\pi$. REGAL improves this result to $\tilde{O}(\Psi S \sqrt{A T</p>
<p>In many practical applications we may be interested in episodic learning tasks where the constants $D$ and $\Psi$ could be improved to take advantage of the episode length $\tau$. Simple modifications to both UCRL2 and REGAL will produce regret bounds of $\tilde{O}(\tau S \sqrt{A T})$, just as PSRL. This is close to the theoretical lower bounds of $\sqrt{S A T}$-dependence.</p>
<h2>4 True versus sampled MDP</h2>
<p>A simple observation, which is central to our analysis, is that, at the start of each $k$ th episode, $M^{<em>}$ and $M_{k}$ are identically distributed. This fact allows us to relate quantities that depend on the true, but unknown, MDP $M^{</em>}$, to those of the sampled MDP $M_{k}$, which is</p>
<p>fully observed by the agent. We introduce $\sigma\left(H_{t_{k}}\right)$ as the $\sigma$-algebra generated by the history up to $t_{k}$. Readers unfamiliar with measure theory can think of this as "all information known just before the start of period $t_{k}$." When we say that a random variable X is $\sigma\left(H_{t_{k}}\right)$ measurable, this intuitively means that although X is random, it is deterministically known given the information contained in $H_{t_{k}}$. The following lemma is an immediate consequence of this observation [15].
Lemma 1 (Posterior Sampling). If $f$ is the distribution of $M^{*}$ then, for any $\sigma\left(H_{t_{k}}\right)$ measurable function $g$,</p>
<p>$$
\mathbb{E}\left[g\left(M^{*}\right) \mid H_{t_{k}}\right]=\mathbb{E}\left[g\left(M_{k}\right) \mid H_{t_{k}}\right]
$$</p>
<p>Note that taking the expectation of (2) shows $\mathbb{E}\left[g\left(M^{<em>}\right)\right]=\mathbb{E}\left[g\left(M_{k}\right)\right]$ through the tower property.
Recall, we have defined $\Delta_{k}=\sum_{s \in \mathcal{S}} \rho(s)\left(V_{\mu^{</em>}, 1}^{M^{<em>}}(s)-V_{\mu_{k}, 1}^{M^{</em>}}(s)\right)$ to be the regret over period $k$. A significant hurdle in analyzing this equation is its dependence on the optimal policy $\mu^{*}$, which we do not observe. For many reinforcement learning algorithms, there is no clean way to relate the unknown optimal policy to the states and actions the agent actually observes. The following result shows how we can avoid this issue using Lemma 1. First, define</p>
<p>$$
\tilde{\Delta}<em _in="\in" _mathcal_S="\mathcal{S" s="s">{k}=\sum</em>(s)\right)
$$}} \rho(s)\left(V_{\mu_{k}, 1}^{M_{k}}(s)-V_{\mu_{k}, 1}^{M^{*}</p>
<p>as the difference in expected value of the policy $\mu_{k}$ under the sampled MDP $M_{k}$, which is known, and its performance under the true MDP $M^{*}$, which is observed by the agent.
Theorem 2 (Regret equivalence).</p>
<p>$$
\mathbb{E}\left[\sum_{k=1}^{m} \Delta_{k}\right]=\mathbb{E}\left[\sum_{k=1}^{m} \tilde{\Delta}_{k}\right]
$$</p>
<p>and for any $\delta&gt;0$ with probability at least $1-\delta$,
Proof. Note, $\Delta_{k}-\tilde{\Delta}<em _in="\in" _mathcal_S="\mathcal{S" s="s">{k}=\sum</em> \rho(s)\left(V_{\mu^{}<em>}, 1}^{M^{</em>}}(s)-V_{\mu_{k}, 1}^{M_{k}}(s)\right) \in[-\tau, \tau]$. By Lemma $1, \mathbb{E}\left[\Delta_{k}-\right.$ $\left.\tilde{\Delta}<em t__k="t_{k">{k} \mid H</em>\right]=0$. Taking expectations of these sums therefore establishes the claim.}</p>
<p>This result bounds the agent's regret in epsiode $k$ by the difference between the agent's estimate $V_{\mu_{k}, 1}^{M_{k}}\left(s_{t_{k}}\right)$ of the expected reward in $M_{k}$ from the policy it chooses, and the expected reward $V_{\mu_{k}, 1}^{M^{<em>}}\left(s_{t_{k}}\right)$ in $M^{</em>}$. If the agent has a poor estimate of the MDP $M^{<em>}$, we expect it to learn as the performance of following $\mu_{k}$ under $M^{</em>}$ differs from its expectation under $M_{k}$. As more information is gathered, its performance should improve. In the next section, we formalize these ideas and give a precise bound on the regret of posterior sampling.</p>
<h1>5 Analysis</h1>
<p>An essential tool in our analysis will be the dynamic programming, or Bellman operator $\mathcal{T}_{\mu}^{M}$, which for any MDP $M=\left(\mathcal{S}, \mathcal{A}, R^{M}, P^{M}, \tau, \rho\right)$, stationary policy $\mu: \mathcal{S} \rightarrow \mathcal{A}$ and value function $V: \mathcal{S} \rightarrow \mathbb{R}$, is defined by</p>
<p>$$
\mathcal{T}<em _mu="\mu">{\mu}^{M} V(s):=\bar{R}</em>\right)
$$}^{M}(s, \mu)+\sum_{s^{\prime} \in \mathcal{S}} P_{\mu(s)}^{M}\left(s^{\prime} \mid s\right) V\left(s^{\prime</p>
<p>This operation returns the expected value of state $s$ where we follow the policy $\mu$ under the laws of $M$, for one time step. The following lemma gives a concise form for the dynamic programming paradigm in terms of the Bellman operator.
Lemma 2 (Dynamic programming equation). For any MDP $M=\left(\mathcal{S}, \mathcal{A}, R^{M}, P^{M}, \tau, \rho\right)$ and policy $\mu: \mathcal{S} \times{1, \ldots, \tau} \rightarrow \mathcal{A}$, the value functions $V_{\mu}^{M}$ satisfy</p>
<p>$$
V_{\mu, i}^{M}=\mathcal{T}<em _mu_="\mu," i_1="i+1">{\mu(\cdot, i)}^{M} V</em>
$$}^{M</p>
<p>for $i=1 \ldots \tau$, with $V_{\mu, \tau+1}^{M}:=0$.</p>
<p>In order to streamline our notation we will let $V_{\mu, i}^{<em>}:=V_{\mu, i}^{M^{</em>}}, V_{\mu, i}^{k}(s):=V_{\mu, i}^{M_{k}}(s), \mathcal{T}<em _mu="\mu">{\mu}^{k}:=\mathcal{T}</em>}^{M_{k}}$, $\mathcal{T<em _mu="\mu">{\mu}^{<em>}:=\mathcal{T}_{\mu}^{M^{</em>}}$ and $P</em>^{<em>}(\cdot \mid s):=P_{\mu(s)}^{M^{</em>}}(\cdot \mid s)$.</p>
<h1>5.1 Rewriting regret in terms of Bellman error</h1>
<p>$$
\mathbb{E}\left[\tilde{\Delta}<em _mu__k="\mu_{k">{k} \mid M^{<em>}, M_{k}\right]=\mathbb{E}\left[\sum_{i=1}^{\tau}\left[\left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, i)}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, i)</em>}\right) V</em>\right]
$$}, i+1}^{k}\left(s_{t_{k}+i}\right)\right] \mid M^{*}, M_{k</p>
<p>To see why (6) holds, simply apply the Dynamic programming equation inductively:</p>
<p>$$
\begin{aligned}
\left(V_{\mu_{k}, 1}^{k}-V_{\mu_{k}, 1}^{<em>}\right)\left(s_{t_{k}+1}\right)= &amp; \left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, 1)}^{k} V_{\mu_{k}, 2}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, 1)</em>} V_{\mu_{k}, 2}^{<em>}\right)\left(s_{t_{k}+1}\right) \
= &amp; \left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, 1)}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, 1)</em>}\right) V_{\mu_{k}, 2}^{k}\left(s_{t_{k}+1}\right) \
&amp; +\sum_{s^{\prime} \in \mathcal{S}}\left{P_{\mu_{k}(\cdot, 1)}^{<em>}\left(s^{\prime} \mid s_{t_{k}+1}\right)\left(V_{\mu_{k}, 2}^{</em>}-V_{\mu_{k}, 2}^{k}\right)\left(s^{\prime}\right)\right} \
= &amp; \left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, 1)}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, 1)<em>}\right) V_{\mu_{k}, 2}^{k}\left(s_{t_{k}+1}\right)+\left(V_{\mu_{k}, 2}^{</em>}-V_{\mu_{k}, 2}^{k}\right)\left(s_{t_{k}+1}\right)+d_{t_{k}+1} \
= &amp; \cdots \
= &amp; \sum_{i=1}^{\tau}\left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, i)}^{k}-\mathcal{T<em k="k">{\mu</em>
\end{aligned}
$$}(\cdot, i)}^{*}\right) V_{\mu_{k}, i+1}^{k}\left(s_{t_{k}+i}\right)+\sum_{i=1}^{\tau} d_{t_{k}+i</p>
<p>where $d_{t_{k}+i}:=\sum_{s^{\prime} \in \mathcal{S}}\left{P_{\mu_{k}(\cdot, i)}^{<em>}\left(s^{\prime} \mid s_{t_{k}+i}\right)\left(V_{\mu_{k}, i+1}^{</em>}-V_{\mu_{k}, i+1}^{k}\right)\left(s^{\prime}\right)\right}-\left(V_{\mu_{k}, i+1}^{<em>}-V_{\mu_{k}, i+1}^{k}\right)\left(s_{t_{k}+i}\right)$.
This expresses the regret in terms two factors. The first factor is the one step Bellman error $\left[\left(\mathcal{T}<em k="k">{\mu</em>}(\cdot, i)}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, i)</em>}\right) V_{\mu_{k}, i+1}^{k}\left(s_{t_{k}+i}\right)\right]$ under the sampled MDP $M_{k}$. Crucially, (6) depends only the Bellman error under the observed policy $\mu_{k}$ and the states $s_{1}, . ., s_{T}$ that are actually visited over the first $T$ periods. We go on to show the posterior distribution of $M_{k}$ concentrates around $M^{<em>}$ as these actions are sampled, and so this term tends to zero.
The second term captures the randomness in the transitions of the true MDP $M^{</em>}$. In state $s_{t}$ under policy $\mu_{k}$, the expected value of $\left(V_{\mu_{k}, i+1}^{<em>}-V_{\mu_{k}, i+1}^{k}\right)\left(s_{t_{k}+i}\right)$ is exactly $\sum_{s^{\prime} \in \mathcal{S}}\left{P_{\mu_{k}(\cdot, i)}^{</em>}\left(s^{\prime} \mid s_{t_{k}+i}\right)\left(V_{\mu_{k}, i+1}^{<em>}-V_{\mu_{k}, i+1}^{k}\right)\left(s^{\prime}\right)\right}$. Hence, conditioned on the true MDP $M^{</em>}$ and the sampled MDP $M_{k}$, the term $\sum_{i=1}^{\tau} d_{t_{k}+i}$ has expectation zero.</p>
<h3>5.2 Introducing confidence sets</h3>
<p>The last section reduced the algorithm's regret to its expected Bellman error. We will proceed by arguing that the sampled Bellman operator $\mathcal{T}<em k="k">{\mu</em>}(\cdot, i)}^{k}$ concentrates around the true Bellman operatior $\mathcal{T<em k="k">{\mu</em>}(\cdot, i)}^{*}$. To do this, we introduce high probability confidence sets similar to those used in [4] and [5]. Let $\hat{P<em a="a">{a}^{t}(\cdot \mid s)$ denote the emprical distribution up period $t$ of transitions observed after sampling $(s, a)$, and let $\hat{R}</em>}^{t}(s)$ denote the empirical average reward. Finally, define $N_{t_{k}}(s, a)=\sum_{t=1}^{t_{k}-1} \mathbb{1<em t="t">{\left{\left(s</em>$. Define the confidence set for episode $k$ :}, a_{t}\right)=(s, a)\right}}$ to be the number of times $(s, a)$ was sampled prior to time $t_{k</p>
<p>$$
\mathcal{M}<em a="a">{k}:=\left{M:\left|\hat{P}</em>(\cdot \mid s)\right|}^{t}(\cdot \mid s)-P_{a}^{M<em k="k">{1} \leq \beta</em>}(s, a) \&amp;\left|\hat{R<em a="a">{a}^{t}(s)-R</em>(s, a) \forall(s, a)\right}
$$}^{M}(s)\right| \leq \beta_{k</p>
<p>Where $\beta_{k}(s, a):=\sqrt{\frac{14 S \log \left(2 S A m t_{k}\right)}{\max \left{1, N_{t_{k}}(s, a)\right}}}$ is chosen conservatively so that $\mathcal{M}<em k="k">{k}$ contains both $M^{*}$ and $M</em> \leq \tau$ we can decompose regret as follows:}$ with high probability. It's worth pointing out that we have not tried to optimize this confidence bound, and it can be improved, at least by a numerical factor, with more careful analysis. Now, using that $\tilde{\Delta}_{k</p>
<p>$$
\sum_{k=1}^{m} \tilde{\Delta}<em k="1">{k} \leq \sum</em>}^{m} \tilde{\Delta<em _left_123_M__k="\left{M_{k">{k} \mathbb{1}</em>, M^{<em>} \in \mathcal{M}<em k="1">{k}\right}}+\tau \sum</em>}^{m}\left[\mathbb{1<em k="k">{\left{M</em>} \notin \mathcal{M<em>{k}\right}}+\mathbb{1}</em>{\left{M^{</em>} \notin \mathcal{M}_{k}\right}}\right]
$$</p>
<p>Now, since $\mathcal{M}<em t__k="t_{k">{k}$ is $\sigma\left(H</em>}}\right)$-measureable, by Lemma 1.1, $\mathbb{E}\left[\mathbb{1<em k="k">{\left{M</em>} \notin \mathcal{M<em t__k="t_{k">{k}\right}}\right] H</em>}}=$ $\mathbb{E}\left[\mathbb{1<em k="k">{\left{M^{<em>} \notin \mathcal{M}<em t__k="t_{k">{k}\right}}\right] H</em>\left(M^{}}$. Lemma 17 of [4] shows ${ }^{3} \mathbb{P</em>} \notin \mathcal{M}</em>(s, a)$, which implies}\right) \leq 1 / m$ for this choice of $\beta_{k</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\sum_{k=1}^{m} \tilde{\Delta}<em k="1">{k}\right] &amp; \leq \mathbb{E}\left[\sum</em>}^{m} \tilde{\Delta<em _left_123_M__k="\left{M_{k">{k} \mathbb{1}</em>, M^{<em>} \in \mathcal{M}<em k="1">{k}\right}}\right]+2 \tau \sum</em>\left{M^{}^{m} \mathbb{P</em>} \notin \mathcal{M}<em k="1">{k}\right} \
&amp; \leq \mathbb{E}\left[\sum</em>}^{m} \mathbb{E}\left[\tilde{\Delta<em k="k">{k} \mid M^{<em>}, M_{k}\right] \mathbb{1}<em k="k">{\left{M</em>, M^{</em>} \in \mathcal{M}</em>\right]+2 \tau \
&amp; \leq \mathbb{E} \sum_{k=1}^{m} \sum_{i=1}^{\tau}\left|\left(\mathcal{T}}\right}<em k="k">{\mu</em>}(\cdot, i)}^{k}-\mathcal{T<em k="k">{\mu</em>^{}(\cdot, i)<em>}\right) V_{\mu_{k}, i+1}^{k}\left(s_{t_{k}+i}\right)\right| \mathbb{1}<em k="k">{\left{M</em>, M^{</em>} \in \mathcal{M}<em k="1">{k}\right}}+2 \tau \
&amp; \leq \tau \mathbb{E} \sum</em>\right), 1\right}+2 \tau
\end{aligned}
$$}^{m} \sum_{i=1}^{\tau} \min \left{\beta_{k}\left(s_{t_{k}+i}, a_{t_{k}+i</p>
<p>We also have the worst-case bound $\sum_{k=1}^{m} \tilde{\Delta}<em k="1">{k} \leq T$. In the technical appendix we go on to provide a worst case bound on $\min \left{\tau \sum</em>$, which completes our analysis.}^{m} \sum_{i=1}^{\tau} \min \left{\beta_{k}\left(s_{t_{k}+i}, a_{t_{k}+i}\right), 1\right}, T\right}$ of order $\tau S \sqrt{A T \log (S A T)</p>
<h1>6 Simulation results</h1>
<p>We compare performance of PSRL to UCRL2 4]: an optimistic algorithm with similar regret bounds. We use the standard example of RiverSwim 21, as well as several randomly generated MDPs. We provide results in both the episodic case, where the state is reset every $\tau=20$ steps, as well as the setting without episodic reset.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: RiverSwim - continuous and dotted arrows represent the MDP under the actions "right" and "left".</p>
<p>RiverSwim consists of six states arranged in a chain as shown in Figure 1. The agent begins at the far left state and at every time step has the choice to swim left or right. Swimming left (with the current) is always successful, but swimming right (against the current) often fails. The agent receives a small reward for reaching the leftmost state, but the optimal policy is to attempt to swim right and receive a much larger reward. This MDP is constructed so that efficient exploration is required in order to obtain the optimal policy. To generate the random MDPs, we sampled 10 -state, 5 -action environments according to the prior.
We express our prior in terms of Dirichlet and normal-gamma distributions over the transitions and rewards respectively. ${ }^{4}$ In both environments we perform 20 Monte Carlo simulations and compute the total regret over 10,000 time steps. We implement UCRL2 with $\delta=0.05$ and optimize the algorithm to take account of finite episodes where appropriate. PSRL outperformed UCRL2 across every environment, as shown in Table 1. In Figure 2, we show regret through time across 50 Monte Carlo simulations to 100,000 time-steps in the RiverSwim environment: PSRL's outperformance is quite extreme.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Total regret in simulation. PSRL outperforms UCRL2 over different environments.</p>
<table>
<thead>
<tr>
<th></th>
<th>Random MDP</th>
<th>Random MDP</th>
<th>RiverSwim</th>
<th>RiverSwim</th>
</tr>
</thead>
<tbody>
<tr>
<td>Algorithm</td>
<td>$\tau$-episodes</td>
<td>$\infty$-horizon</td>
<td>$\tau$-episodes</td>
<td>$\infty$-horizon</td>
</tr>
<tr>
<td>PSRL</td>
<td>$1.04 \times 10^{4}$</td>
<td>$7.30 \times 10^{3}$</td>
<td>$6.88 \times 10^{1}$</td>
<td>$1.06 \times 10^{2}$</td>
</tr>
<tr>
<td>UCRL2</td>
<td>$5.92 \times 10^{4}$</td>
<td>$1.13 \times 10^{5}$</td>
<td>$1.26 \times 10^{3}$</td>
<td>$3.64 \times 10^{3}$</td>
</tr>
</tbody>
</table>
<h1>6.1 Learning in MDPs without episodic resets</h1>
<p>The majority of practical problems in reinforcement learning can be mapped to repeated episodic interactions for some length $\tau$. Even in cases where there is no actual reset of episodes, one can show that PSRL's regret is bounded against all policies which work over horizon $\tau$ or less [6]. Any setting with discount factor $\alpha$ can be learned for $\tau \propto(1-\alpha)^{-1}$.</p>
<p>One appealing feature of UCRL2 [4] and REGAL [5] is that they learn this optimal timeframe $\tau$. Instead of computing a new policy after a fixed number of periods, they begin a new episode when the total visits to any state-action pair is doubled. We can apply this same rule for episodes to PSRL in the $\infty$-horizon case, as shown in Figure 2. Using optimism with KL-divergence instead of $L^{1}$ balls has also shown improved performance over UCRL2 [22], but its regret remains orders of magnitude more than PSRL on RiverSwim.
<img alt="img-1.jpeg" src="img-1.jpeg" />
(a) PSRL outperforms UCRL2 by large margins (b) PSRL learns quickly despite misspecified prior</p>
<p>Figure 2: Simulated regret on the $\infty$-horizon RiverSwim environment.</p>
<h2>7 Conclusion</h2>
<p>We establish posterior sampling for reinforcement learning not just as a heuristic, but as a provably efficient learning algorithm. We present $\tilde{O}(\tau S \sqrt{A T})$ Bayesian regret bounds, which are some of the first for an algorithm not motivated by optimism and are close to state of the art for any reinforcement learning algorithm. These bounds hold in expectation irrespective of prior or model structure. PSRL is conceptually simple, computationally efficient and can easily incorporate prior knowledge. Compared to feasible optimistic algorithms we believe that PSRL is often more efficient statistically, simpler to implement and computationally cheaper. We demonstrate that PSRL performs well in simulation over several domains. We believe there is a strong case for the wider adoption of algorithms based upon posterior sampling in both theory and practice.</p>
<h2>Acknowledgments</h2>
<p>Osband and Russo are supported by Stanford Graduate Fellowships courtesy of PACCAR inc., and Burt and Deedee McMurty, respectively. This work was supported in part by Award CMMI-0968707 from the National Science Foundation.</p>
<h1>References</h1>
<p>[1] A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for markov decision processes. Mathematics of Operations Research, 22(1):222-255, 1997.
[2] P. R. Kumar and P. Varaiya. Stochastic systems: estimation, identification and adaptive control. Prentice-Hall, Inc., 1986.
[3] T.L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics, 6(1):4-22, 1985.
[4] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. The Journal of Machine Learning Research, 99:1563-1600, 2010.
[5] P. L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 35-42. AUAI Press, 2009.
[6] R. I. Brafman and M. Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003.
[7] S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of London, 2003.
[8] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209-232, 2002.
[9] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
[10] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Information Processing Systems (NIPS), 2011.
[11] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):639-658, 2010.
[12] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. arXiv preprint arXiv:1209.3353, 2012.
[13] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. arXiv preprint arXiv:1209.3352, 2012.
[14] E. Kauffmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal finite time analysis. In International Conference on Algorithmic Learning Theory, 2012.
[15] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. CoRR, abs/1301.2609, 2013.
[16] M. Strens. A Bayesian framework for reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning, pages 943-950, 2000.
[17] J. Z. Kolter and A. Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 513-520. ACM, 2009.
[18] T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sampling for on-line reward optimization. In Proceedings of the 22nd international conference on Machine learning, pages 956-963. ACM, 2005.
[19] A. Guez, D. Silver, and P. Dayan. Efficient bayes-adaptive reinforcement learning using samplebased search. arXiv preprint arXiv:1205.3109, 2012.
[20] J. Asmuth and M. L. Littman. Approaching bayes-optimalilty using monte-carlo tree search. In Proc. 21st Int. Conf. Automat. Plan. Sched., Freiburg, Germany, 2011.
[21] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
[22] S. Filippi, O. Cappé, and A. Garivier. Optimism in reinforcement learning based on kullbackleibler divergence. CoRR, abs/1004.5229, 2010.</p>
<h1>A Relating Bayesian to frequentist regret</h1>
<p>Let $\mathcal{M}$ be any family of MDPs with non-zero probability under the prior. Then, for any $\epsilon&gt;0$, $\alpha&gt;\frac{1}{2}$ :</p>
<p>$$
\mathbb{P}\left(\frac{\operatorname{Regret}\left(T, \pi_{\tau}^{P S}\right)}{T^{\alpha}}&gt;\epsilon \mid M^{*} \in \mathcal{M}\right) \rightarrow 0
$$</p>
<p>This provides regret bounds even if $M^{*}$ is not distributed according to $f$. As long as the true MDP is not impossible under the prior, we will have an asymptotic frequentist regret close to the theoretical lower bounds of in $T$-dependence of $O(\sqrt{T})$.</p>
<p>Proof. We have for any $\epsilon&gt;0$ :</p>
<p>$$
\begin{aligned}
\frac{\mathbb{E}\left[\operatorname{Regret}\left(T, \pi_{\tau}^{P S}\right)\right]}{T^{\alpha}} &amp; \geq \mathbb{E}\left[\frac{\operatorname{Regret}\left(T, \pi_{\tau}^{P S}\right)}{T^{\alpha}} \mid M^{<em>} \in \mathcal{M}\right] \mathbb{P}\left(M^{</em>} \in \mathcal{M}\right) \
&amp; \geq \epsilon \mathbb{P}\left(\frac{\operatorname{Regret}\left(T, \pi_{\tau}^{P S}\right)}{T^{\alpha}} \mid M^{<em>} \in \mathcal{M}\right) \mathbb{P}\left(M^{</em>} \in \mathcal{M}\right)
\end{aligned}
$$</p>
<p>Therefore via theorem (1), for any $\alpha&gt;\frac{1}{2}$ :</p>
<p>$$
\mathbb{P}\left(\frac{\operatorname{Regret}\left(T, \pi_{\tau}^{P S}\right)}{T^{\alpha}} \mid M^{<em>} \in \mathcal{M}\right) \leq\left(\frac{1}{\epsilon \mathbb{P}\left(M^{</em>} \in \mathcal{M}\right)}\right) \frac{\mathbb{E}\left[\operatorname{Regret}\left(T, \pi_{\tau}^{P} S_{\tau}\right)\right]}{T^{\alpha}} \rightarrow 0
$$</p>
<h2>B Bounding the sum of confidence set widths</h2>
<p>We are interested in bounding $\min \left{\tau \sum_{k=1}^{m} \sum_{i=1}^{\tau} \min \left{\beta_{k} s_{t_{k}+i}, a_{t_{k}+i}\right), 1\right}, T}$ which we claim is $O\left(\tau S \sqrt{A T \log (S A T)}\right.$ for $\left.\beta_{k}(s, a):=\sqrt{\frac{14 S \log \left(2 S A m t_{k}\right)}{\max \left{1, N_{t_{k}}(s, a)\right}}}\right)$.</p>
<p>Proof. In a manner similar to [4] we can say:</p>
<p>$$
\sum_{k=1}^{m} \sum_{i=1}^{\tau} \sqrt{\frac{14 S \log \left(2 S A m t_{k}\right)}{\max \left{1, N_{t_{k}}(s, a)\right}}} \leq \sum_{k=1}^{m} \sum_{i=1}^{\tau} \mathbb{1}<em t__k="t_{k">{\left{N</em>}} \leq \tau\right}}+\sum_{k=1}^{m} \sum_{i=1}^{\tau} \mathbb{1<em t__k="t_{k">{\left{N</em>
$$}}&gt;\tau\right}} \sqrt{\frac{14 S \log \left(2 S A m t_{k}\right)}{\max \left{1, N_{t_{k}}(s, a)\right}}</p>
<p>Now, the consider the event $\left(s_{t}, a_{t}\right)=(s, a)$ and $\left(N_{t_{k}}(s, a) \leq \tau\right)$. This can happen fewer than $2 \tau$ times per state action pair. Therefore, $\sum_{k=1}^{m} \sum_{i=1}^{\tau} \mathbb{1}\left(N_{t_{k}}(s, a) \leq \tau\right) \leq 2 \tau S A$.Now, suppose $N_{t_{k}}(s, a)&gt;\tau$. Then for any $t \in\left{t_{k}, . ., t_{k+1}-1\right}, N_{t}(s, a)+1 \leq N_{t_{k}}(s, a)+\tau \leq 2 N_{t_{k}}(s, a)$. Therefore:</p>
<p>$$
\begin{aligned}
\sum_{k=1}^{m} \sum_{t=t_{k}}^{t_{k+1}-1} \sqrt{\frac{1\left(N_{t_{k}}\left(s_{t}, a_{t}\right)&gt;\tau\right)}{N_{t_{k}}\left(s_{t}, a_{t}\right)}} &amp; \leq \sum_{k=1}^{m} \sum_{t=t_{k}}^{t_{k+1}-1} \sqrt{\frac{2}{N_{t}\left(s_{t}, a_{t}\right)+1}}=\sqrt{2} \sum_{t=1}^{T}\left(N_{t}\left(s_{t}, a_{t}\right)+1\right)^{-1 / 2} \
&amp; \leq \sqrt{2} \sum_{s, a} \sum_{j=1}^{N_{T+1}(s, a)} j^{-1 / 2} \leq \sqrt{2} \sum_{s, a} \int_{x=0}^{N_{T+1}(s, a)} x^{-1 / 2} d x \
&amp; \leq \sqrt{2 S A \sum_{s, a} N_{T+1}(s, a)}=\sqrt{2 S A T}
\end{aligned}
$$</p>
<p>Note that since all rewards and transitions are absolutely constrained $\in[0,1]$ our regret</p>
<p>$$
\begin{aligned}
\min \left{\tau \sum_{k=1}^{m} \sum_{i=1}^{\tau} \min \left{\beta_{k}\left(s_{t_{k}+i}, a_{t_{k}+i}\right), 1\right}, T\right} &amp; \leq \min \left{2 \tau^{2} S A+\tau \sqrt{28 S^{2} A T \log (S A T)}, T\right} \
&amp; \leq \sqrt{2 \tau^{2} S A T}+\tau \sqrt{28 S^{2} A T \log (S A T)} \leq \tau S \sqrt{30 A T \log (S A T)}
\end{aligned}
$$</p>
<p>Which is our required result.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Our confidence sets are equivalent to those of [4] when the parameter $\delta=1 / m$.
${ }^{4}$ These priors are conjugate to the multinomial and normal distribution. We used the values $\alpha=1 / S, \mu=\sigma^{2}=1$ and pseudocount $n=1$ for a diffuse uniform prior.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
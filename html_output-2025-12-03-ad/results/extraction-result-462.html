<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-462 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-462</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-462</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-271534146</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.19526v1.pdf" target="_blank">Impact of Decoding Methods on Human Alignment of Conversational LLMs</a></p>
                <p><strong>Paper Abstract:</strong> To be included into chatbot systems, Large language models (LLMs) must be aligned with human conversational conventions. However, being trained mainly on web-scraped data gives existing LLMs a voice closer to informational text than actual human speech. In this paper, we examine the effect of decoding methods on the alignment between LLM-generated and human conversations, including Beam Search, Top K Sampling, and Nucleus Sampling. We present new measures of alignment in substance, style, and psychometric orientation, and experiment with two conversation datasets. Our results provide subtle insights: better alignment is attributed to fewer beams in Beam Search and lower values of P in Nucleus Sampling. We also find that task-oriented and open-ended datasets perform differently in terms of alignment, indicating the significance of taking into account the context of the interaction.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e462.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e462.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DecodingParamsVariability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Variability induced by decoding parameter choices (beam count, top-K, top-P, temperature) in conversational LLM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper experimentally quantifies how decoding hyperparameters (Beam Search beam count, Top-K sampling, Nucleus/Top-P sampling, and temperature perturbations) and contextual factors (dataset type, instruction finetuning, model variant) produce variability in alignment between LLM-generated and human conversations, using cross-entropy-based alignment metrics and multilevel regression for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 and Llama 3 Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Natural language processing / conversational AI (human alignment of chatbots)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Measure the effect of decoding methods on alignment between LLM-generated and human conversational turns across stylistic, psychometric, and semantic metrics by generating next-turn utterances for two human-human dialogue corpora (BOLT and CraigslistBargains).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Decoding hyperparameters (number of beams in Beam Search; K in Top-K sampling; P in Nucleus/Top-P sampling); temperature perturbations (mentioned, results in appendix); deterministic vs stochastic decoding (greedy/beam search vs sampling); dataset/context type (BOLT chit-chat vs CraigslistBargains task-oriented); model variant and instruction finetuning (Llama 3 vs Llama 3 Instruct).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Cross-entropy between feature vectors of generated vs human utterances (used for stylistic and psychometric measures), absolute difference in length for verbosity, BERTScore for semantic similarity, percent change in cross-entropy relative to base (greedy) decoding, and coefficients from multilevel linear models with p-values (statistical significance reported, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Quantitative summaries reported include percent changes in alignment (change in cross-entropy relative to greedy base) for decoding perturbations; example reported results: using low P (e.g., P=0.5) yields an overall improvement over base — Llama 3: 3.73% improvement vs base with P=0.5, Llama 3 Instruct: 1.40% improvement; 2-beam Beam Search outperforms greedy decoding (qualitative statement); lower P values (0.6-0.7) show best average performance while P=1.0 significantly decreases alignment. Top-K values showed no consistent trend across metrics. Multilevel model coefficients (and associated p<0.001) indicate statistically significant associations for some parameters (e.g., P value positive for Politeness, Negotiation, Verbosity).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Comparison of alignment scores across decoding parameter settings, across two datasets and two model variants; percent change in cross-entropy relative to base; multilevel linear model coefficients and statistical significance (p-values) to test robustness of parameter effects across dataset/model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Trends were reported as consistent across the two Llama variants (both instruction-finetuned and non-finetuned) with 'fewer beams' and 'lower P values' improving alignment for both models; however, the non-instruction-finetuned model showed larger relative improvements from decoding changes (example: 3.73% improvement for Llama 3 at P=0.5 vs 1.40% for Llama 3 Instruct). No explicit replication counts or run-to-run reproducibility (random-seed level) statistics were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Inclusion of very low-probability tokens (e.g., P=1.0 including many low-prob tokens) degrades alignment; open-ended datasets (BOLT) lead to greater divergence and more variability across long conversations compared to goal-oriented datasets (CraigslistBargains); limited coverage of style facets (only Politeness and Negotiation studied) restricts generalizability; the paper does not report seed-level or run-level nondeterminism controls so run-to-run stochasticity is not explicitly addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use lower P values for nucleus sampling (examples: P in 0.5–0.7 range), limit number of beams in Beam Search (e.g., 2 beams rather than many beams), combine low-P Nucleus Sampling with a small-beam Beam Search as a recommended strategy, and (implicitly) prefer decoding methods that avoid including very low-probability tokens; appendix includes temperature perturbations (noted but not emphasized in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported effectiveness examples: P=0.5 produced an overall improvement over base greedy decoding (Llama 3: +3.73% relative decrease in cross-entropy; Llama 3 Instruct: +1.40%); 2-beam Beam Search qualitatively outperformed greedy decoding, while increasing beams beyond 2 diminished improvements. No run-level variance reduction numbers (e.g., standard deviations) for mitigation methods were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decoding hyperparameters materially drive variability in human-alignment metrics: fewer beams (e.g., 2) and lower nucleus P values (≈0.5–0.7) generally improve alignment (decrease cross-entropy) relative to greedy decoding, while including the lowest-probability tokens (P=1.0) harms alignment; Top-K sampling showed no consistent effect. These trends held across two datasets and both Llama 3 variants, though instruction-finetuned models showed smaller relative gains from decoding changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Impact of Decoding Methods on Human Alignment of Conversational LLMs', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A thorough examination of decoding methods in the era of llms <em>(Rating: 2)</em></li>
                <li>The curious case of neural text degeneration <em>(Rating: 2)</em></li>
                <li>Hierarchical neural story generation <em>(Rating: 2)</em></li>
                <li>Sequence transduction with recurrent neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-462",
    "paper_id": "paper-271534146",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "DecodingParamsVariability",
            "name_full": "Variability induced by decoding parameter choices (beam count, top-K, top-P, temperature) in conversational LLM experiments",
            "brief_description": "This paper experimentally quantifies how decoding hyperparameters (Beam Search beam count, Top-K sampling, Nucleus/Top-P sampling, and temperature perturbations) and contextual factors (dataset type, instruction finetuning, model variant) produce variability in alignment between LLM-generated and human conversations, using cross-entropy-based alignment metrics and multilevel regression for analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 and Llama 3 Instruct",
            "model_size": "8B",
            "scientific_domain": "Natural language processing / conversational AI (human alignment of chatbots)",
            "experimental_task": "Measure the effect of decoding methods on alignment between LLM-generated and human conversational turns across stylistic, psychometric, and semantic metrics by generating next-turn utterances for two human-human dialogue corpora (BOLT and CraigslistBargains).",
            "variability_sources": "Decoding hyperparameters (number of beams in Beam Search; K in Top-K sampling; P in Nucleus/Top-P sampling); temperature perturbations (mentioned, results in appendix); deterministic vs stochastic decoding (greedy/beam search vs sampling); dataset/context type (BOLT chit-chat vs CraigslistBargains task-oriented); model variant and instruction finetuning (Llama 3 vs Llama 3 Instruct).",
            "variability_measured": true,
            "variability_metrics": "Cross-entropy between feature vectors of generated vs human utterances (used for stylistic and psychometric measures), absolute difference in length for verbosity, BERTScore for semantic similarity, percent change in cross-entropy relative to base (greedy) decoding, and coefficients from multilevel linear models with p-values (statistical significance reported, p &lt; 0.001).",
            "variability_results": "Quantitative summaries reported include percent changes in alignment (change in cross-entropy relative to greedy base) for decoding perturbations; example reported results: using low P (e.g., P=0.5) yields an overall improvement over base — Llama 3: 3.73% improvement vs base with P=0.5, Llama 3 Instruct: 1.40% improvement; 2-beam Beam Search outperforms greedy decoding (qualitative statement); lower P values (0.6-0.7) show best average performance while P=1.0 significantly decreases alignment. Top-K values showed no consistent trend across metrics. Multilevel model coefficients (and associated p&lt;0.001) indicate statistically significant associations for some parameters (e.g., P value positive for Politeness, Negotiation, Verbosity).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Comparison of alignment scores across decoding parameter settings, across two datasets and two model variants; percent change in cross-entropy relative to base; multilevel linear model coefficients and statistical significance (p-values) to test robustness of parameter effects across dataset/model differences.",
            "reproducibility_results": "Trends were reported as consistent across the two Llama variants (both instruction-finetuned and non-finetuned) with 'fewer beams' and 'lower P values' improving alignment for both models; however, the non-instruction-finetuned model showed larger relative improvements from decoding changes (example: 3.73% improvement for Llama 3 at P=0.5 vs 1.40% for Llama 3 Instruct). No explicit replication counts or run-to-run reproducibility (random-seed level) statistics were reported.",
            "reproducibility_challenges": "Inclusion of very low-probability tokens (e.g., P=1.0 including many low-prob tokens) degrades alignment; open-ended datasets (BOLT) lead to greater divergence and more variability across long conversations compared to goal-oriented datasets (CraigslistBargains); limited coverage of style facets (only Politeness and Negotiation studied) restricts generalizability; the paper does not report seed-level or run-level nondeterminism controls so run-to-run stochasticity is not explicitly addressed.",
            "mitigation_methods": "Use lower P values for nucleus sampling (examples: P in 0.5–0.7 range), limit number of beams in Beam Search (e.g., 2 beams rather than many beams), combine low-P Nucleus Sampling with a small-beam Beam Search as a recommended strategy, and (implicitly) prefer decoding methods that avoid including very low-probability tokens; appendix includes temperature perturbations (noted but not emphasized in main text).",
            "mitigation_effectiveness": "Reported effectiveness examples: P=0.5 produced an overall improvement over base greedy decoding (Llama 3: +3.73% relative decrease in cross-entropy; Llama 3 Instruct: +1.40%); 2-beam Beam Search qualitatively outperformed greedy decoding, while increasing beams beyond 2 diminished improvements. No run-level variance reduction numbers (e.g., standard deviations) for mitigation methods were reported.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Decoding hyperparameters materially drive variability in human-alignment metrics: fewer beams (e.g., 2) and lower nucleus P values (≈0.5–0.7) generally improve alignment (decrease cross-entropy) relative to greedy decoding, while including the lowest-probability tokens (P=1.0) harms alignment; Top-K sampling showed no consistent effect. These trends held across two datasets and both Llama 3 variants, though instruction-finetuned models showed smaller relative gains from decoding changes.",
            "uuid": "e462.0",
            "source_info": {
                "paper_title": "Impact of Decoding Methods on Human Alignment of Conversational LLMs",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A thorough examination of decoding methods in the era of llms",
            "rating": 2,
            "sanitized_title": "a_thorough_examination_of_decoding_methods_in_the_era_of_llms"
        },
        {
            "paper_title": "The curious case of neural text degeneration",
            "rating": 2,
            "sanitized_title": "the_curious_case_of_neural_text_degeneration"
        },
        {
            "paper_title": "Hierarchical neural story generation",
            "rating": 2,
            "sanitized_title": "hierarchical_neural_story_generation"
        },
        {
            "paper_title": "Sequence transduction with recurrent neural networks",
            "rating": 1,
            "sanitized_title": "sequence_transduction_with_recurrent_neural_networks"
        }
    ],
    "cost": 0.00955,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Impact of Decoding Methods on Human Alignment of Conversational LLMs</p>
<p>Shaz Furniturewala 
Birla Institute of Technology and Science
Pilani</p>
<p>Department of Communications and New Media
National University of Singapore</p>
<p>Kokil Jaidka 
NUS Center for Trusted Internet and Community
National University of Singapore</p>
<p>Department of Communications and New Media
National University of Singapore</p>
<p>Yashvardhan Sharma 
Birla Institute of Technology and Science
Pilani</p>
<p>NUS Center for Trusted Internet and Community</p>
<p>Impact of Decoding Methods on Human Alignment of Conversational LLMs
2E06BBA15F9B53C6DFEF94F10577A2B3
To be included into chatbot systems, Large language models (LLMs) must be aligned with human conversational conventions.However, being trained mainly on web-scraped data gives existing LLMs a voice closer to informational text than actual human speech.In this paper, we examine the effect of decoding methods on the alignment between LLM-generated and human conversations, including Beam Search, Top K Sampling, and Nucleus Sampling.We present new measures of alignment in substance, style, and psychometric orientation, and experiment with two conversation datasets.Our results provide subtle insights: better alignment is attributed to fewer beams in Beam Search and lower values of P in Nucleus Sampling.We also find that task-oriented and open-ended datasets perform differently in terms of alignment, indicating the significance of taking into account the context of the interaction.</p>
<p>Introduction</p>
<p>As large language models (LLMs) continue to evolve, their integration into chatbot systems has increasingly focused on not just understanding but also on aligning with human conversational norms.Models are trained and finetuned to be 'perfect assistants' which has inadvertently given them a voice that is eager, overly enthusiastic, and marked by use of words and phrases that feature prominently in informational and instructional texts but not so much in true human conversations (Zhou et al., 2024).Therefore, LLM-human alignment is a crucial problem and has been studied across various contexts, such as coding, problem-solving, summarization, translation, and reasoning (for a review, see Shi et al., 2024).Among various techniques explored to improve this alignment, the perturbation of decoding parameters-such as Beam Search, Top K Sampling, and Nucleus Sampling-has shown promise.These decoding methods, encompassing both deterministic strategies like beam search and stochastic approaches such as temperature scaling, fundamentally influence how a model generates text.Preliminary studies suggest that while deterministic methods may better adhere to specific instructions, stochastic methods like P and K sampling could excel in scenarios involving unaligned models by introducing variability in responses (Shi et al., 2024).Despite their potential, the impact of these methods on the quality of chatbot outputs, particularly in mimicking human conversational patterns, has not been comprehensively analyzed.Achieving a high degree of alignment between the outputs of these models and actual human interactions is crucial not only for maintaining the natural flow of dialogue but also for ensuring the relevance and contextuality of the responses provided by chatbots.Yet, current evaluation methods are limited in their ability to assess whether these systems successfully emulate the human-like attributes essential for nuanced interactions.For instance, most work focuses on automatic evaluation methods such as BLEU, ROUGE, and METEOR with some others using classifiers trained on human judgement (Yeh et al., 2021).While there has been some work in the creation of psychological metrics (Giorgi et al., 2023), it merely focuses on broad aspects of dialog like emotion and personality.A study of dialogue dynamics requires an understanding of the deeper subtleties of interpersonal engagement beyond content, such as style and psychological orientation.Unlike emotion, style and psychological orientation are nuanced and multifaceted aspects of communication that have not been studied as much and are harder to accurately measure and control.This paper aims to bridge this gap by systematically investigating the effects of different decoding arXiv: 2407.19526v1 [cs.CL] 28 Jul 2024 methods on the alignment between chatbot outputs and human-like responses.We hypothesize that adjusting these decoding parameters can significantly enhance the naturalistic appeal and user engagement of chatbot conversations.To test this hypothesis, we employ a novel methodological approach, analyzing the performance of conversational LLMs through a series of experiments involving real human conversations.Our work offers the following contributions:</p>
<p>• Two new parallel corpora of synthetic LLMgenerated conversations, curated through turnby-turn prompts with real-world dialogues sourced from two human-human datasets, collected across a variety of decoding methods.• New metrics for measuring LLM alignment to human conversations in substance, style, and psychometric orientation.</p>
<p>Our findings aim to provide deeper insights into the practical applications of decoding methods and their potential to improve the human-likeness of chatbot interactions, thereby guiding future developments in chatbot design and deployment.</p>
<p>Empirical Evaluation</p>
<p>In this section we describe the datasets used for our experiments, the metrics employed to measure humane conversational traits, and the decoding methods used in the LLM's generation process.We created a turn-by-turn synthetic dataset of LLM generated conversations, adhering to a structured process across each conversation turn.Each conversation began with the opening turns of a conversation from one of the two datasets we considered-BOLT and CraiglistBargains-and we invited each LLM we considered to generate the next utterance by the speaker indicated.We then evaluated human-LLM conversation alignment along dimensions of Style, Psychometrics, and Semantic content.In this work we use Llama 3 (8B) and Llama 3 Instruct (8B) for our experiments.Further, we vary the decoding methods during generation utilising Beam Search, Top K Sampling, and Nucleus Sampling, to gain insights into their impact on the quality of generated conversations.</p>
<p>Datasets</p>
<p>BOLT SMS/Chat Dataset (Chen, Song et al., 2018), developed by the Linguistic Data Consortium consists of naturally occurring English conversations involving native speakers.The corpus contains 18,429 two-person conversations totaling 3,674,802 words across 375,967 messages.For the purposes of this work, 2640 conversations ranging from 5 to 125 turns were used.</p>
<p>CraigslistBargains (He et al., 2018) is a collection of 6682 human-human negotiation conversations between AMT agents.The agents are assigned the role of buyer and seller and are asked to negotiate the price of a real Craigslist listing.For this work, 5357 conversations ranging from 5 to 28 turns were used.</p>
<p>Measures</p>
<p>The following 6 metrics measure the stylistic, psychometric, and semantic similarity between the human and LLM generated texts.They are relative measures, using the original text as a reference in comparison with the LLM generated text.Each measure is computed at the utterance level and averaged across the entire conversation to arrive at a score.</p>
<p>Stylistic</p>
<p>Style is a broad concept with various aspects.For the purposes of this paper we picked two aspects that are relevant to the datasets being used and are significantly impacted by the decoding parameters as seen in Figure 2.</p>
<p>Politeness We used the ConvoKit Library (Danescu-Niculescu-Mizil et al., 2013) to compute 21 characteristics representing facets of politeness, including deference, hedging, gratitude, factuality, among others.We then calculated the cross entropy score between each human and LLM generated utterance.</p>
<p>Negotiation Based on the work done by Niculae et al. ( 2015), we extracted 8 linguistic cues from each utterance including Claim, Premise, Contingency, Expansion, Temporal (Past and Future), Subject, and Comparison.We use these linguistic cues as a negotiation vector and compute the cross entropy score between the human and LLM generated utterance.</p>
<p>Pyschometric</p>
<p>Self Concept We annotated 10,956 text messages from the BOLT dataset for the presence of three characteristics of self concept: Autonomy, Competence, and Relatedness.These annotations were done by Amazon Mechanical Turk workers on an interface we designed that provided positive and negative examples of each characteristic.We fine-tuned a classifier on this data and computed the cross entropy score between the predictions for the human and LLM generated utterances.</p>
<p>Empathy We finetune an empathy classifier on the dataset created by Buechel et al. (2018).It contains 1860 short texts annotated for empathic concern.This classifier predicts the presence of empathic concern in the human and LLM generated utterances and we compute the cross entropy score between them.</p>
<p>Semantic</p>
<p>Verbosity For each utterance, we measure verbosity as the absolute difference between the length of the human and LLM generated utterances.</p>
<p>Semantic Similarity We compute the semantic similarity between the human and LLM generated utterances using BERTScore (Zhang* et al., 2020).</p>
<p>Decoding Parameters</p>
<p>The standard generation setup uses the default temperature value of 1.0 and deterministic greedy decoding with no sampling.In the appendix we also display results on temperature perturbations.</p>
<p>Beam Search (Luong et al., 2015;Graves, 2012) Using this decoding strategy, we can allow the model to evaluate multiple hypotheses at a time and ultimately pick the sequence that has the highest overall probability.While it is computationally more expensive, it can generate sequences that begin with low probability tokens but have the overall highest probability.In this work we evaluate beam search with 2, 4, 6, and 8 beams.</p>
<p>Top K Sampling Introduced by Fan et al. (2018), this generation strategy filters out the K most probable next tokens and redistributes the probability mass among them.Then, based on their new probabilities, the next token is randomly chosen among them.In this work we evaluate Top-K Sampling with K = 30, 40, 50, 60, and 70.</p>
<p>Nucleus Sampling (Top P) (Holtzman et al., 2020) This sampling method filters the smallest number of tokens whose probability cumulatively exceeds P. In this manner, it dynamically changes the number of tokens being filtered based on the probability distribution.We evaluate Nucleus Sampling with P = 0.6, 0.7, 0.8, 0.9, 1.0.</p>
<p>Results</p>
<p>In this section we will analyse the results of the experiments described in Section 2. Initially, our analysis focuses on identifying how variants of different decoding parameters perform, then we examine turn-level results.</p>
<p>Table 1 displays the average change in alignment scores over the base decoding method for each decoding parameter perturbation.The change in comparison to base is measured as the decrease in the cross entropy score of the generated text with the ground truth.Thus, a higher percentage change represents a greater decrease in the cross entropy score indicated better alignment with the human responses.These scores are averaged across both datasets (BOLT, CraigslistBargains) and across both models (Llama 3, Llama 3 Instruct).We notice that using 2 Beams outperforms the base greedy decoding strategy, however, further increasing the number of beams diminishes this increase in performance, indicating a potential local minimum (or a local maximum in alignment).Lower values of P (0.6-0.7) have the best performance while P=1.0 demonstrates a significant decrease in alignment compared to base (greedy) decoding.This indicates that some of the least likely tokens in the vocabulary contribute to the drop in alignment when included in the sequence.Finally, there no observable trend in the perturbations of Top K Sampling with all values of K performing, on average, similarly to the base method, i.e., greedy decoding.</p>
<p>In Figure 1, we plot the scores (scaled down and smoothened) divided by dataset and along with the number of turns in the conversation.This allows us to examine task-specific performance as a function of the length of the conversation.We see that as conversations get longer, the LLM is able to more accurately emulate these traits.Notably, this trend applies for negotiation on BOLT but not on CraigslistBargains where the performance quickly plateaus.For both datasets, Politeness, Self-Concept, Empathy, and Verbosity follow a similar trend of improving performance as the conversations get longer with beam search and nucleus sampling consistently outperforming Top K sampling.This is consistent with our previous conclusions about these two decoding methods.In addition, it indicates that for these metrics, alignment is correlated with the amount of context provided.</p>
<p>Figure 2: The parameters effecting significant positive and negative changes in style, psychometrics and semantics of LLM conversations.Calculated using multi-level models controlling for model and dataset differences.</p>
<p>To validate our insights and conclusions from these experiments, we compute the correlation coefficients of the decoding parameters with the measures.In Figure 2 we plot the coefficients of multilevel linear models for each of the six metrics calculated for the three decoding methods-Beam Search, Top K Sampling, Top P Sampling-and the number of turns or the length of a conversation.A positive coefficient indicates that a high value for that parameter predicts better alignment and a negative coefficient predicts worse alignment.Asterisks represent statistically significant associations (p&lt;0.001).The first notable observation from the figure is that Top K sampling does not have any significant non-zero coefficients for any of the given metrics.Beam search only has nonzero coefficients with Negotation and Verbosity, having a positive coefficient for the former and negative one for the latter.Politeness, Negotiation, and Verbosity all have positive coefficients for P value and proportionally negative correlations for the number of turns.</p>
<p>Discussion and Conclusion</p>
<p>The broader context of the datasets appears to affect the quality of generated conversations, as BOLT, being a chit-chat dataset, does not require the same amount of negotiation as the task oriented CraigslistBargains and allows the LLM to adapt to these traits quickly.Similarly, the models show decreasing Semantic Similarity on BOLT compared to CraigslistBargains where performance stays consistently high across conversations.The goal-oriented task of CraigslistBargains tends to have highly probable responses in a specific direc-tion for each input.On the contrary, BOLT is very open-ended with each dialog allowing the conversation to go in many different directions.A similar effect is seen in the quality metrics, where over the course of a long conversation, the lack of structure in the task is seen to lead to more deviations by the LLM in BOLT compared to CraigslistBargains, manifesting as the decreased alignment performance seen in the graph.Our experiments suggest that lower P values improve instruction adherence, while top-K sampling, unlike nucleus sampling, has a smaller impact, as the fixed number of tokens being filtered each time results in much less control over the redistributed probability mass as compared to variable tokens with a fixed cumulative probability threshold.Thus, the best decoding method for human aligned conversational LLM output is likely a combination of Low P Nucleus Sampling and Beam Search with a small number of beams.</p>
<p>A larger number of beams incorporates more obscure, lower probability words into the sequence that leads to worse alignment, possibly through introducing linguistic artifacts such as obscure words and longer texts to the sequence that undo the potential advantages of having more beams.</p>
<p>Limitations</p>
<p>One particular limitation of our work is the usage of two specific aspects of style: Politeness and Negotiation.To ensure concise insights we limited the experiments to these two aspects since they pertain the most to the task specific dataset we used (CraigslistBargains).We believe the results observed for these two aspects should translate to other facets of style on other datasets and we hope to address this by expanding on these experiments in our future work.</p>
<p>A.2 Impact of Instruction Finetuning</p>
<p>Table 2 shows the complete results for both Llama 3 variants, with and without instruction finetuning.</p>
<p>From the table we can see that the trends are identical among them.Fewer beams and lower P values show better human alignment for both models, with K values showing no consistent trend.However, notably, the model not instruction finetuned appears to show larger improvements in alignment when using decoding methods compared to the instruction finetuned variant.Llama 3 shows a 3.73% overall improvement over base when using P=0.5 compared to only 1.40% for Llama 3 Instruct.</p>
<p>A.3 Justification for using cross entropy score</p>
<p>We compute the four stylistic and psychometric measures as the cross entropy scores between feature vectors of the generated text and the ground truth.These feature vectors are largely all ndimensional one-hot encoded vectors.Much like multi-class classification tasks where minimizing cross entropy is equivalent to maximizing likelihood, lower cross-entropy for the feature vectors of these four measures indicates higher alignment with the ground truth human dialog.</p>
<p>Figure 1 :
1
Figure 1: Turn-based scores for each decoding parameter, averaged across all perturbations and both models (Llama 2 and Llama 3).</p>
<p>Table 1 :
1
Average change in alignment across the six metrics for various values of the decoding methods.
Beams ChangePChange K Change23.82%0 0.5 -1.80% 20 -3.21% 2.35% 1 1.64%41.37%0.6 0.7 2.50% 40 -1.19% 1.85% 30 -5.02%6-2.33%0.8 -5.66% 50 -1.33% 0.9 -3.09% 60 -0.55%80.21%1-2.67% 70 2.99%</p>
<p>Table 2 :
2
Scores for Llama 3 and Llama 3 Instruct on all six psychological metrics for various values of the decoding parameters.
Politeness Negotiation Self Concept Empathy Verbosity Semantic SimilarityLlama 3 Instruct0.3120.0990.6660.5410.2320.622Number of Beams20.3110.0930.6660.5460.2210.63940.3140.0870.6670.5440.2160.62860.3120.0730.6670.5480.2270.61980.3100.1040.6660.5300.2310.629P Value00.3150.1040.6670.5580.2030.6400.50.3150.1010.6670.5440.2130.6460.60.3130.1110.6670.5680.2070.6060.70.3150.1080.6660.5430.2140.6200.80.3120.1040.6660.5160.2290.6120.90.3130.0950.6650.5380.2180.60410.3130.0960.6660.5700.2200.644K Value10.3150.1030.6670.5580.2030.640200.3130.1200.6680.5340.2140.615300.3070.1100.6660.5380.2210.620400.3060.1080.6670.5040.2250.622500.3090.1060.6650.5360.2400.635600.3120.0880.6660.5600.2120.627700.3120.1030.6660.5450.2240.629Llama 30.3160.1020.6610.6610.1740.553Number of Beams20.3150.0820.6420.6660.1410.46540.3140.1040.6360.6540.1410.49660.3140.1110.6680.6670.1980.47780.3100.0880.6670.6680.1970.472P Value00.3130.0730.6660.6210.2000.4810.50.3160.0870.6430.6620.1470.4990.60.3140.0760.6660.6820.2160.5070.70.3120.0950.6670.6470.2070.5340.80.3190.0970.6670.6750.2280.4980.90.3100.0780.6650.6670.2050.46210.3150.1030.6680.6630.1850.487K Value10.3130.0820.6660.6240.2020.481200.3150.1150.6640.6680.1830.440300.3170.1060.6690.6690.2140.499400.3220.0910.6120.6740.2160.446500.3140.0900.6680.6470.1970.494600.3130.1060.5980.6570.1830.467700.3120.0650.6120.6460.2010.477
AcknowledgementsThis work is supported by the Ministry of Education, Singapore under its MOE AcRF TIER3 Grant (MOE-MOET32022-0001).The travel grant for this research is supported by the Department of Communication and New Media at the National University of Singapore.
Modeling empathy and distress in reaction to news stories. Sven Buechel, Anneke Buffone, Barry Slaff, Lyle Ungar, João Sedoc, 10.18653/v1/D18-1507Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>. Song Chen, Fore, Dana, Strassel, Stephanie, Haejoong Lee, Jonathan Wright, 10.35111/HKFC-78652018Bolt english sms/chat</p>
<p>A computational approach to politeness with application to social factors. Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Daniel Jurafsky, Jure Leskovec, Christopher Potts, Annual Meeting of the Association for Computational Linguistics. 2013</p>
<p>Hierarchical neural story generation. Angela Fan, Mike Lewis, Yann Dauphin, arXiv:1805.048332018Preprint</p>
<p>Psychological metrics for dialog system evaluation. Salvatore Giorgi, Shreya Havaldar, Farhan Ahmed, Zuhaib Akhtar, Shalaka Vaidya, Gary Pan, Lyle H Ungar, H Andrew Schwartz, Joao Sedoc, arXiv:2305.147572023Preprint</p>
<p>Decoupling strategy and generation in negotiation dialogues. Alex Graves, Derek Chen, Anusha Balakrishnan, Percy Liang, 10.18653/v1/D18-1256arXiv:1211.3711Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2012. 2018arXiv preprintSequence transduction with recurrent neural networks</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.097512020Preprint</p>
<p>Addressing the rare word problem in neural machine translation. Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, Wojciech Zaremba, 10.3115/v1/P15-1002Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, China20151Association for Computational Linguistics</p>
<p>Linguistic harbingers of betrayal: A case study on an online strategy game. Srijan Vlad Niculae, Jordan Kumar, Cristian Boyd-Graber, Danescu-Niculescu-Mizil, 10.3115/v1/P15-1159Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Chufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, Wai Lam, arXiv:2402.06925A thorough examination of decoding methods in the era of llms. 2024arXiv preprint</p>
<p>Yi-Ting Yeh, Maxine Eskenazi, Shikib Mehri, arXiv:2106.03706A comprehensive assessment of dialog evaluation metrics. 2021Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
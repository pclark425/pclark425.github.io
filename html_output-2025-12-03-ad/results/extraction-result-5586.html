<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5586 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5586</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5586</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-5fd951377b3a41ef80f9b15f617ce33d17118e47</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5fd951377b3a41ef80f9b15f617ce33d17118e47" target="_blank">Few-Shot Table-to-Text Generation with Prompt-based Adapter</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The core insight design of the PA is to inject prompt templates for augmenting domain-speciﬁc knowledge and table-related representations into the model for bridging the structural gap between tabular data and descriptions through adapters.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5586",
    "paper_id": "paper-5fd951377b3a41ef80f9b15f617ce33d17118e47",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006034749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Adapting Knowledge for Few-shot Table-to-Text Generation</h1>
<p>Zhixin Guo, <em>Student Member, IEEE</em>, Mingxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Guanjie Zheng, Xinbing Wang, <em>Senior Member, IEEE</em>, and Chenghu Zhou</p>
<p><strong>Abstract</strong>—Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt unlabeled domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to employ the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-domain, few-shot natural language generation (NLG) data sets: Humans, Songs, and Books. Compared to previous state-of-the-art approaches, our model achieves superior performance in terms of both fluency and accuracy as judged by human and automatic evaluations.</p>
<p><strong>Index Terms</strong>—Few-shot generation, table-to-text generation, knowledge adaption.</p>
<h2>I. INTRODUCTION</h2>
<p>Generating descriptive text from structured data [1], i.e., table-to-text generation, is an important research problem for various downstream natural language generation (NLG) applications. Some representative examples are question answering [2], [3], [4], dialog [5], report generation [6], [7], [8], [9], and biographical description [10], demonstrating the great potential of table-to-text generation for use in extensive real-world scenarios.</p>
<p>The main challenge in table-to-text generation is the structural difference between the table and the natural language text. With the blossoming of deep neural networks, Pretrained Language Model (PLM)-based NLG systems have shown a remarkable ability to produce fluent text</p>
<p>Zhixin Guo, Mingxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, and Xinbing Wang are with the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University.</p>
<p>E-mail: {stjgzx, galaxy_ymx, qi_jiexing, jianpingzhou, ziwei.he, xwang8}@sjtu.edu.cn</p>
<p>Guanjie Zheng is with John Hopcroft Center for Computer Science, Shanghai Jiao Tong University.</p>
<p>E-mail: gjzheng@sjtu.edu.cn</p>
<p>Chenghu Zhou is with the Institute of Geographic Sciences and Natural Resources Research, China.</p>
<p>E-mail: zhouchsjtu@gmail.com</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Fig. 1.</strong> An example of a table-text pair for few-shot table-to-text generation from the Humans data set. On the right-hand-side are a template of the key-value pair for table linearization and a description of the table. The red text indicates the content supported by the tabular data.</p>
<p>with informative content and have achieved state-of-the-art performance in many table-to-text tasks, such as WIKIBIO [10], RotoWire [11], and ToTTo [12]. However, these methods depend on a large training data set, and this data-hungry nature prevents neural models from being widely adopted for real-world applications.</p>
<p>To mitigate this challenge, researchers are investigating various techniques to leverage the prior knowledge of PLMs fully, such as switch policy [13] and table structure reconstruction [14]. Recently, with the development of prompt learning, the "prompt-tuning" paradigm with PLMs has also been explored in table-to-text generation. Prefix-tuning [15] and prefix-controlled generation [16] modify the encoder-decoder architecture with task-specific vectors as prompts, improving the efficiency of leveraging prior knowledge. These methods rely on fine-tuning the semantic knowledge and linguistic patterns learned from a large corpus during pretraining.</p>
<p>Although fluency improves significantly, these methods always fabricate phrases unsupported by the table. Due to the limitation of insufficient training data, these methods are inadequate to capture the representation differences between the tabular data and descriptions. [17], who first attempted the knowledge-augmentation method through a retrieval-based framework for providing related background information, significantly improved few-shot table-to-text generation. However, due to the input length limitation of their considered PLM, they only considered the top <em>n</em>-retrieved sentences, leaving out most of the information. In general, the main challenge, bridging the gap between the tabular data and text, is still underexplored under the few-shot condition due</p>
<p>to the inherent shortcoming of PLMs lacking domain-specific knowledge.</p>
<p>To address this problem, we propose the AKG framework, which breaks through the bottleneck of lacking domainspecific knowledge. Compared to the previous study [17], we have improved the efficiency of domain knowledge usage without adding additional resources. We leverage an unlabeled corpus pertinent to table contexts to craft and refine tablerelated prompt templates. These templates are then seamlessly integrated into the AKG framework employing a modularized pretraining approach, enabling the dynamic adaptation of domain-specific insights. To evaluate our approach comprehensively, we evaluate the proposed method on a multidomain table-to-text data set. Compared with previous state-of-the-art approaches, our method achieves remarkable improvement in fluency and faithfulness of the generated contents as judged by human and automatic evaluations. Moreover, we also perform extensive ablation studies of AKG. In particular, the results also illustrate that our model outputs are highly faithful and fluent. In short, our contributions can be summarized as follows:</p>
<ul>
<li>We propose a novel framework AKG for few-shot table-to-text generation that attempts to alleviate the insufficient-data limitation to bridge the topological gap between tabular data and text. AKG enables the model to make full use of a large domain-specific knowledge corpus.</li>
<li>We design an effective modularized pretraining strategy for adapting the prompt templates to inject table-related representation and domain-specific knowledge. The modularized pretraining strategy effectively integrates various tasks to employ the domain-knowledge.</li>
<li>We conduct extensive experiments on a multi-domain table-to-text data set encompassing three distinct areas: Humans, Books, and Songs. Both automatic and human evaluations report state-of-the-art performance.</li>
</ul>
<h2>II. RELATED WORK</h2>
<p>As it is an essential objective in many real-world scenarios, researchers have investigated NLG from tabular data for many years. Early conventional generation systems followed the pipeline paradigm, such as macro/micro planning [18] and template-based content selection [19], [20], [21]. Such pipeline approaches significantly rely on feature engineering and template design. Later works, with the blooming of deep neural networks, employed neural-based methods and achieved remarkable performance in table-to-text generative challenges like WIKIBIO [22], RotoWire [11], WebNLG [23], E2E [24], and ToTTo [12]. Researchers explored optimizing deep neural networks to bridge the gap between structured data and text, such as copy mechanism [25] and content-aware generation [14]. However, such methods rely on a tremendous amount of labeled data. Towards targeting real-world applications, studies of low-resource generation from structured data have gained increased attention. Zeroshot learning for question generation from knowledge graphs [26] and open-domain few-shot table-to-text generation [13] illustrate that PLMs suffer from limited labeled data in a few-shot setting due to the lack of domain-specific knowledge.</p>
<p>To alleviate labeled data dependency, researchers attempted to modify the architecture of the PLM to improve the efficiency of using prior information. The Semantically Conditioned Variational AutoEncoder (SCVAE) [27] was proposed for semantic-controlled generation. Learning domain-invariant representations achieved impressive performance, allowing for improvements of the lowresource Semantically Conditioned Long Short-Term Memory (SCLSTM) settings. Similarly, [28] employed the same idea of designing a refinement adjustment LSTM-based component to select and control the semantic information better. Apart from these, algorithms represented by Model-Agnostic MetaLearning (MAML) [29] for improving parameter efficiency have also been investigated in NLG. For example, [30] proposed Meta-NLG for task-oriented dialogue systems based on the MAML algorithm by defining meta-tasks for adapting to low-resource NLG tasks. [31] suggested structured meta-learning for knowledge-aware algorithms for text classification. However, these explorations can barely be generalized directly to table-to-text generation due to different application scenarios.</p>
<p>Recently, prompt tuning has achieved impressive success in table-to-text NLG. Prefix-tuning [15] prepends learned task-specific vectors and performs well. Prefix-controlled [16] generators use prefix vectors as a planned signal to guide and control the output of PLMs. Plan-then-generate [32] uses a content planner to generate key facts as a prompt signal to instruct generation. Although these approaches reduce the number of parameters in the model compared to previous methods following the prompt-tuning paradigm, the lack of domain-specific knowledge still needs to be addressed. [17] demonstrated a retrieval-based generation framework to provide background knowledge and commonsense reasoning information. However, due to the input length limitation of the considered PLMs, their method only takes the top $n$-retrieved sentences, leaving out most of the information. Unlike previous works, we propose AKG for table-to-text generation and focus on improving fluency and accuracy with a modularized pretraining strategy for injecting table-related representation and domain-specific knowledge.</p>
<h2>III. PRELIMINARIES</h2>
<p>In this section, we briefly introduce the relevant research lines of our work: table-to-text generation, the Prototype-To-Generate (P2G) framework, and the parameter-efficient adapter.</p>
<p>Table-to-text generation The objective of the table-to-text task is to synthesize coherent and accurate descriptions in natural language that accurately reflect the contents of a given table. In the context of this research, the training data set is represented as $D=\left{(T,R)<em i="1">{i}\right}</em>$}^{|D|}$, where each tuple $(T,R)_{i</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. An overview of the AKG framework. We propose a modularized pretraining strategy that targets AKG. The pretraining strategy consists of three modules: (a) Generation Module, (b) Knowledge-Augmentation Module, and (c) Fine-tuning Module. Throughout the generation module pretraining process, we divide the generation module into two tasks: a prototype-selection task and a language-modeling task. The prototype-selection task selects related prototypes to guide the generation of PLMs, and the language-modeling task employs a PLM as the backbone generator to generate fluent sentences. We pretrain these two tasks separately. We insert the Knowledge-based Adapter (KA) after the final layer of each encoder and decoder and adapt the generated prompt templates through the KA during the pretraining of the knowledge-augmentation module. All the parameters except those of the KA are frozen when pretraining the knowledge-augmentation module. The knowledge-augmentation module brings at least two benefits: (1) it enables us to use the large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLM's inherent shortcomings of lacking domain knowledge; and (2) it allows us to design various tasks to employ the domain-specific knowledge. Finally, we fine-tune the pretrained modularized model on three data sets: Humans, Books, and Songs. Throughout the fine-tuning process, the parameters of the KA are frozen to retain the learned knowledge pretrained from the knowledge-augmentation module. The red text indicates the tabular data supporting the fact, and the blue text indicates that the (incorrect) fact conflicts with the information in the original table.</p>
<p>comprises a linearized table <em>T</em> and its associated reference text <em>R</em>. The table <em>T</em> is composed of one or more tabular data elements, denoted as <em>T</em> = {<em>t</em><sub>1</sub>, . . . , <em>t</em><sub>|n|</sub>}. Each element <em>t</em><sub>i</sub> = {<em>a</em><sub>i</sub>; <em>v</em><sub>i</sub>} within the table represents an attribute-value pair, with <em>a</em><sub>i</sub> and <em>v</em><sub>i</sub> potentially being a string/number, a phrase, or a complete sentence. Subsequently, <em>R</em> = {<em>r</em><sub>1</sub>, . . . , <em>r</em><sub>|m|</sub>} signifies the set of reference texts, with each <em>r</em><sub>i</sub> providing a narrative description that corresponds to the table <em>T</em>. This framework facilitates the generation of natural language descriptions that are both fluent and faithful to the tabular data presented.</p>
<p>Unlike the rich corpus data used for PLMs, tabular data contain complex topology structures with few narrative descriptions, which are far from natural language representations. Fig. 1 illustrates an example of tabular data from the Humans data set. According to the experimental results of [14], the tabular data format significantly enhances generation performance. In this paper, to make the tabular data more compatible with the internal representations of PLMs, we leverage a template for table linearization. As shown in Fig. 1, we use the key-value pair template "name: Alan Oppenheimer" as "Name is Alan Oppenheimer", then stack all key-value pairs to form a sentence, that is, "Name is Alan Oppenheimer; Birth Date is 23, April, 1930; Birth Place is New York, U.S.; ...".</p>
<p><strong>P2G framework</strong> Previous work by P2G [17] achieved impressive results. [17] designed a retrieval-based framework by retrieving domain-related information from an external information retrieval (IR) system as prototype memory to guide the PLM in generating fluent and faithful sentences. Given input tabular data <em>T</em> with its corresponding reference text <em>R</em>, P2G first retrieves the <em>n</em> most related corresponding sentences from the IR system and generates prototype memory according to the semantic features of the candidates. Then, the method concatenates the prototype memory with the tabular data as input to the PLM to produce corresponding descriptions. As the method of [17] yielded reliable achievements in improving fluency and faithfulness, we employ a prototype-selection model to generate a plan to guide generation.</p>
<p><strong>Parameter-efficient adapter</strong> The investigation into the use of adapters, as delineated in the studies by [33], [34], [35], and [36], has attracted considerable interest in the field of transfer</p>
<p>learning. This interest is primarily due to the demonstrated effectiveness of adapters in optimizing parameter utilization within diverse applications. Notably, their utility extends to enhancing the performance of task-oriented dialogue systems, as evidenced by the contributions of [37] and [38]. Distinct from the conventional application of adapters across multiple tasks, our approach necessitates an efficient, architecture-neutral plug-in model capable of seamlessly integrating prompt and domain-specific knowledge to facilitate the generation process. To fulfill these criteria, we have adopted the method proposed by [33] as the foundation for our knowledge-based adapter (KA), incorporating it within our generative framework as a knowledge-augmentation module.</p>
<h2>IV. Methodology</h2>
<p>This study introduces a novel approach to few-shot table-to-text generation by presenting a knowledge-augmentation methodology that leverages the AKG framework to refine prompts. Fig. 2 illustrates the comprehensive architecture of our proposed method. Within this framework, we integrate a prototype-selector, as described in [17], which guides the PLM during the generation phase. Moreover, we develop a knowledge-augmentation task designed to enrich the model with prompt templates and domain-specific knowledge via the Knowledge Adapter (KA). This KA is strategically positioned after the terminal layer of each encoder and decoder in the backbone generator, as depicted in Fig. 2. Our approach is characterized by a modularized pretraining strategy that segments the table-to-text generation process into three distinct modules: generation, knowledge augmentation, and fine-tuning. This modular structure facilitates the model’s comprehensive exploitation of a vast domain-specific corpus and allows each submodule to engage in specialized pretraining tasks. These tasks are tailored to bolster the generative capabilities of the model, thereby enhancing its performance and adaptability in generating text from tabular data.</p>
<p>As illustrated in Fig. 2(a), our methodology employs a modular approach to the generation module, integrating both prototype-selection and language-modeling tasks. The objective of the prototype-selection task is to discern and extract relevant prototypes from an unlabeled corpus that align with the contextual requirements of the table. Simultaneously, the language-modeling task utilizes a PLM to generate cohesive sentences from linearized table formats. This dual-task strategy addresses and mitigates the structural discontinuity between tabular data and narrative descriptions, facilitating a seamless transition from tabular data representation to natural language interpretation.</p>
<p>To enhance the accuracy of the generated content, our methodology is refined by pretraining a knowledge-augmentation module, which integrates table-specific prompt templates via the KA. This module undertakes a pivotal knowledge-augmentation task aimed at redefining prompt templates, which is achieved by reconstructing the prompt templates, effectively overcoming the inherent deficiencies of PLMs through the strategic employment of a vast corpus of unlabeled, domain-specific knowledge. As shown in Fig. 2(b), the knowledge-augmentation module first freezes the backbone generator and further pretrains the KA independently. Specifically, pretraining of the knowledge-augmentation module relies on reconstructing the prompt templates extracted from the unlabeled domainspecific corpus, distinct from the generation task. Such a modularization strategy results in at least three advantages: (1) each module can be easily integrated; (2) it allows the integration of different types of generation-support tasks; and (3) it enables the model to make full use of any unlabeled domain-specific corpus.</p>
<p>As shown in Fig. 2(c), we introduce a fine-tuning module for fusing the linguistic and semantic patterns and the augmented knowledge by pretraining the generation and knowledge-augmentation modules separately. Throughout the fine-tuning process, we freeze the parameters of the KA to retain the augmented knowledge.</p>
<h3>IV-A Generation Module Pretraining</h3>
<p>1) Prototype-Selection Task: We employ a prototype retriever to select prototypes that relate to the input tabular data from the IR system. The task of prototype selection is to predict the similarity between the tabular data and the retrieved prototypes. Given the input tabular data, $T$, with the reference text, $R$, the retriever retrieves the $n$ most related corresponding prototype, $P$, from the IR system, $B$. Each candidate sentence of $B$ is defined as $b$ and each retrieved candidate set is defined as $B^{\prime}$. We utilize a Bidirectional Encoder Representations from Transformers (BERT)-based model [39] to get the representation of the prototype and evaluate its similarity to the target table, $T$. The similarity score is denoted as $f(T,b)$, and $P$ is then defined as:</p>
<p>$P=\underset{B^{\prime}\in B,\left|B^{\prime}\right|=n}{\arg\max}\sum_{b \in B^{\prime}} f(T, b).$ (1)</p>
<p>It is computed by the linear projection of the average embedding of concatenated text, [T:b], by BERT. In order to select the most related candidate sentences, we utilize the hinge loss-based objective during the training process. Given target table $T$ with reference text $R$ and the retrieved candidate set $B^{\prime}$, the learning objective is defined as:</p>
<p>$L_{P S}=\sum_{j=1}^{k}\max\left(0,1-f(T,R)+f\left(T,b_{j}\right)\right),$ (2)</p>
<p>where $b_{j} \in B^{\prime}$ and $k$ is the number of text candidates sampled from $B^{\prime}$.
2) Language-Modeling Task: The language-modeling task aims to train the PLM to generate sentences that describe the tabular data. The AKG framework is model agnostic. Thus the backbone generator can be any generation model. Our experiments utilized BART-large [40], an encoder-decoder architecture transformer, as our backbone generator for its</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. Illustration of prompt generation. (a) The original tabular data, (b) illustration of the entity-detection process, and (c) the entity value mask process.</p>
<p>remarkable performance in generative challenges. Given the structured data, T, the prototype, P, and the reference text, R, the learning objective of the sequence generator is the cross-entropy loss, defined as Equation 3:</p>
<p>$$L_{LM} = -\sum_{i=1}^{|R|} \log P_{\mathscr{D} + \mathscr{A}}(R_i |R_{&lt;i}; \mathscr{E} + \mathscr{A}([P : T])), \qquad(3)$$</p>
<p>where $\mathscr{E} + \mathscr{A}$ and $\mathscr{D} + \mathscr{A}$ denote the configurations where the KA is integrated subsequent to the encoder and decoder, respectively. While the proposed method is agnostic to the choice of the particular PLM, we leave such validation for future work.</p>
<h3><em>B. Knowledge-Augmentation Module Pretraining</em></h3>
<p><em>1) Prompt Generation:</em> The main idea of generating prompts is to replace the entities associated with the tabular data in order to reconstruct the knowledge representation. Unlike data augmentation in other areas, we require the augmented knowledge to follow two crucial rules: (1) it must contain table-related factors to bridge the structural gap between the tabular data and the texts the data represent; and (2) the domain-specific knowledge is injected to solve the insufficient training data problem. As shown in Fig. 3, the prompt-generation process consists of two steps, <em>entity detection</em> and <em>entity value mask</em>:</p>
<ul>
<li><strong>Entity detection</strong>. We first detect all entities and their attributes related to the tabular data in the unlabeled corpus provided by [17]. This process is accomplished by systematically matching each word's string in the table against individual words within the corpus. For example, "New York, U.S." and "Alan Oppenheim" are detected, which are shown in Fig. 3(b).</li>
<li><strong>Entity value mask</strong>. We apply an entity value mask to generate prompts. As shown in Fig. 3(c), we replace "New York" with "<Mask>".</li>
</ul>
<p><em>2) Knowledge-Augmentation Task:</em> We strategically incorporate the KA subsequent to the final layer within the backbone generator's encoder and decoder. This precise integration facilitates the injection of prompts, enhancing the model's ability to assimilate and process the relevant knowledge. The intuition of such a design is that the pluggable knowledge adapter satisfies the lightweight, model-agnostic requirement and is adequate for fine-tuning entirely new tasks. The knowledge adapter consists of a down-projection matrix, $W_{down}$, and an up-projection matrix, $W_{up}$, as shown in Equation 4:</p>
<p>$$h \leftarrow W_{up} \cdot (W_{down} \cdot h) + r,\tag{4}$$</p>
<p>$W_{down}$ projects the hidden states, $h$, into the lower-dimension $d_{bottleneck}$ and $W_{up}$ projects back into the original dimension of hidden states with a residual connection, $r$.</p>
<p>The initialization of the backbone generator's parameters is conducted using the parameters derived from the antecedent generation module pretraining phase. All parameters except the KA are frozen during the pretraining of the knowledge-augmentation module, which allows the KA to retain the knowledge learned from the knowledge-augmentation task. Given the masked prompts $B = {b_1, b_2, \cdots, b_i}$ and the target sentence $\bar{B} = { \bar{b}_1, \bar{b}_2, \cdots, \bar{b}_i}$, the distribution is $P_K(\bar{B}|B)$ and the learning objective is defined as Equation 5:</p>
<p>$$L_K = -\sum_{i=1}^{|\bar{B}|} \log P_K(\bar{B}<em _i="&lt;i">i | \bar{B}</em>$$}; B). \tag{5</p>
<h3><em>C. Knowledge Fusion through the Fine-Tuning Module</em></h3>
<p>Finally, following the distinct pretraining of the language-generation and knowledge-augmentation modules, we delineate fine-tuning the pretrained system via the knowledge-fusion task. This task is designed to refine the integrated model by leveraging the insights from both pretraining modules. The initialization of the parameters for both the backbone generator and the KA is executed by employing the parameters obtained from the preceding phase of knowledge-augmentation module pretraining. Throughout the knowledge-fusion phase, the parameters of the KA are meticulously frozen to safeguard the integrity of the enhanced knowledge. Given the tabular data, T, the prototype, P, and the reference text, R, the learning objective of the sequence generator is the cross-entropy loss, defined as Equation 6:</p>
<p>$$L_{LM} = -\sum_{i=1}^{|R|} \log P_{\mathscr{D} + \mathscr{A}}(R_i | R_{&lt;i}; \mathscr{E} + \mathscr{A}([P : T])),\tag{6}$$</p>
<p>where $\mathscr{E} + \mathscr{A}$ and $\mathscr{D} + \mathscr{A}$ denote the configurations where the KA is integrated subsequent to the encoder and decoder, respectively. While the proposed P2G framework is agnostic to the choice of the particular PLM, we leave such validation for future work.</p>
<h3>V. EXPERIMENTS</h3>
<h4><em>A. Data Sets and Experimental Setup</em></h4>
<p>Adhering to the experimental framework established by [13], we conduct an evaluation of our method across three distinct benchmarks within the WIKIBIO data set [22], specifically: Humans, Books, and Songs. Table I furnishes a comprehensive overview of the data sets spanning these domains. Compared to the Books and Songs data sets, the Humans data set is characterized by statistical attributes.</p>
<p>TABLE I: OVERVIEW OF THE DATA IN HUMANS, BOOKS, AND SONGS. CELL REFERS TO THE MEAN TOTAL CELL COUNT ACROSS THE DATA SETS. ESSENTIAL CELL SIGNIFIES THE CELLS WHOSE CONTENT IS INTEGRAL TO THE DESCRIPTIONS, INDICATING THE RELEVANCE OF SPECIFIC DATA POINTS TO THE NARRATIVE CONTEXT. TABLE LENGTH INDICATES THE AVERAGE WORD COUNT FOR EACH TABLE, PROVIDING INSIGHTS INTO THE DATA SET'S COMPLEXITY. CORPUS LENGTH REFLECTS THE AVERAGE WORD COUNT OF EACH PROTOTYPE RETRIEVED THROUGH THE INFORMATION RETRIEVAL (IR) SYSTEM, HIGHLIGHTING THE EXTENT OF BACKGROUND INFORMATION CONSIDERED. DESCRIPTION LENGTH SPECIFIES THE AVERAGE WORD COUNT OF EACH DESCRIPTION ASSOCIATED WITH THE TABLES, ILLUSTRATING THE DETAIL AND BREADTH OF THE NARRATIVE CONTENT.</p>
<table>
<thead>
<tr>
<th>Statics</th>
<th>Humans</th>
<th>Books</th>
<th>Songs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Cell</td>
<td>$\mathbf{1 2 . 8 5}$</td>
<td>11.28</td>
<td>10.32</td>
</tr>
<tr>
<td>Essential Cell</td>
<td>$\mathbf{5 . 5 5}$</td>
<td>4.78</td>
<td>4.64</td>
</tr>
<tr>
<td>Table Length</td>
<td>$\mathbf{3 9 2 . 2 6}$</td>
<td>288.51</td>
<td>291.18</td>
</tr>
<tr>
<td>Corpus Length</td>
<td>$\mathbf{2 1 . 2 9}$</td>
<td>18.07</td>
<td>20.05</td>
</tr>
<tr>
<td>Description Length</td>
<td>$\mathbf{2 5 . 6 0}$</td>
<td>19.12</td>
<td>19.22</td>
</tr>
</tbody>
</table>
<p>TABLE II: GPT-3.5 TURBO RESULTS (ZERO-SHOT).</p>
<table>
<thead>
<tr>
<th></th>
<th>ROUGE-4</th>
<th>BLEU</th>
<th>PARENT-F</th>
</tr>
</thead>
<tbody>
<tr>
<td>Book</td>
<td>4.17</td>
<td>1.26</td>
<td>8.19</td>
</tr>
<tr>
<td>Human</td>
<td>2.92</td>
<td>1.86</td>
<td>9.69</td>
</tr>
<tr>
<td>Song</td>
<td>3.87</td>
<td>0.57</td>
<td>6.22</td>
</tr>
</tbody>
</table>
<p>Notably, the statistical attributes Cell, Essential Cell, and Table Length reveal that the tabular data within the Humans domain encapsulates a more significant amount of critical information pertinent to their respective descriptions. We perform experiments in few-shot settings by varying the training set size from ${50,100,200,500}$. The size of the validation set is 1000, and the size of the test set is 13587, 5252, and 11879 for Humans, Books, and Songs, respectively.</p>
<p>To elucidate the intricacies of our experimental task, as depicted in Table II, we undertook a supplementary analysis focusing on the performance metrics of the extensively recognized, large-scale GPT-3.5 turbo model in a zero-shot context [41]. This examination aimed to contextualize our results amidst the latest developments in the field of generative pre-trained models. Our analysis underscores that despite the GPT-3.5 model's prevalent success across many applications, it encounters challenges in synthesizing coherent narratives from linearized tabular data.</p>
<p>We leverage the design and settings of [17] for the development of the Information Retrieval (IR) system. Lucene [42], [43] was utilized to pre-index all sentences contained within the English Wikipedia corpus as of the December 2018 dump. This pre-indexing process enables the IR system to retrieve a set of 100 candidate sentences efficiently, referred to as candidates $B$, for each table under consideration. Following this retrieval, the role of the prototype selector emerges, with its primary function being to discern and in alignment with the approach established by [17], we select the top three sentences from the candidate set R, hereby designated as the prototypes. In calibrating the prototype selector during its training phase, we assign the value of 5 to the parameter k, as specified in Equation 2. This procedural framework assures a methodical selection strategy to identify the sentences most pertinent to act as prototypes, optimizing the subsequent processing phases. In the course of developing the knowledge adapter, we employed an unlabeled corpus sourced from Wikipedia, as detailed by [17]. Our experimental framework across three distinct domains was characterized by a deliberate modification of the training data set size, which was established at 500 instances. Conversely, the composition of the test set was defined by a total of 1000 instances.</p>
<p>In our study, the BART-large model [40] serves as the foundational generator, implemented via the Hugging Face Library [44]. To ensure optimal performance across all pretraining tasks, we employ a learning rate of $3 \times 10^{-5}$, utilizing the Adam optimization algorithm [45]. This computational framework is executed on an NVIDIA GeForce RTX 3090 GPU, providing the necessary hardware acceleration to efficiently manage the extensive computational demands associated with processing large-scale data sets and model training.</p>
<h2>B. Baseline Models</h2>
<p>In our study, we juxtapose our approach against preceding state-of-the-art methodologies in the domain of few-shot table-to-text generation, which are utilized as benchmark baselines. These baseline methodologies are categorized into three distinct groups for comparative analysis: naive sequence-to-sequence (seq2seq)-based, PLM-based, and retrieval-based methods. This classification facilitates a comprehensive evaluation, allowing us to delineate our approach's advancements over a diverse array of existing strategies within the scope of few-shot table-to-text generation.</p>
<h2>Naive seq2seq-based models:</h2>
<ul>
<li>Structure-Aware [22]: a structure-aware seq2seq architecture consisting of a field-gating encoder and a dual-attention-based generator, can generate coherent and fluent descriptions.</li>
<li>Pivot [46]: a two-stage generation model, consisting of key fact prediction from tables and surface realization for generation, achieves remarkable performance for lowresource table-to-next generation.</li>
</ul>
<h2>PLM-based models:</h2>
<ul>
<li>Switch Policy with PLM [13]: the first approach suggested for the few-shot NLG task based on PLM. The authors suggest a switch policy to balance generating or copying from the table contents. Switch+GPT2 and Switch+BART-large were implemented by [13], and [16], respectively.</li>
<li>TableGPT [14]: a further study based on Swtich+GPT2 that utilizes two auxiliary tasks for structure construction and content matching to generate faithful text.</li>
</ul>
<p>TABLE III
BLEU-4 RESULTS FOR THE HUMANS, BOOKS, AND SONGS DOMAINS. ALL (R) ARE COPIED FROM THE ORIGINAL PAPER. BOLD DENOTES THE BEST PERFORMANCE OF THIS EVALUATION. THE SECOND-BEST ONES ARE LABELED WITH $\dagger$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training set size</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">Structure-Aware (R)</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">13.1</td>
</tr>
<tr>
<td style="text-align: center;">Pivot (R)</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">31.7</td>
</tr>
<tr>
<td style="text-align: center;">SwitchPolicy (R)</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;">TableGPT (R)</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-Tuning+T5 (R)</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">41.1</td>
</tr>
<tr>
<td style="text-align: center;">BART-large</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">45.9</td>
</tr>
<tr>
<td style="text-align: center;">T5-prefix (R)</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">43.5</td>
</tr>
<tr>
<td style="text-align: center;">PCG (R)</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">42.5</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">AMG (R)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">45.1</td>
</tr>
<tr>
<td style="text-align: center;">Retri-Gen(R)</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">16.5</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">15.9</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.7</td>
</tr>
<tr>
<td style="text-align: center;">P2G (T5)(R)</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">$46.2^{\dagger}$</td>
<td style="text-align: center;">$50.1^{\dagger}$</td>
<td style="text-align: center;">$41.2^{\dagger}$</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">50.7</td>
</tr>
<tr>
<td style="text-align: center;">P2G (BART-large)</td>
<td style="text-align: center;">$40.4^{\dagger}$</td>
<td style="text-align: center;">$44.5^{\dagger}$</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">$44.4^{\dagger}$</td>
<td style="text-align: center;">$47.6^{\dagger}$</td>
<td style="text-align: center;">$49.8^{\dagger}$</td>
<td style="text-align: center;">$50.1^{\dagger}$</td>
<td style="text-align: center;">$50.4^{\dagger}$</td>
<td style="text-align: center;">$52.2^{\dagger}$</td>
<td style="text-align: center;">$53.1^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">AKG (BART-large) [Ours]</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">52.1</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">53.9</td>
</tr>
</tbody>
</table>
<p>TABLE IV
ROUGE-4 RESULTS FOR THE HUMANS, BOOKS, AND SONGS DOMAINS. ALL (R) ARE COPIED FROM THE ORIGINAL PAPER. THE BASELINE, WHICH DOES NOT EVALUATE THE ROUGE-4 METRIC, IS NOT SHOWN IN THIS TABLE. BOLD DENOTES THE BEST PERFORMANCE OF THIS EVALUATION. THE SECOND-BEST ONES ARE LABELED WITH $\dagger$. THE RESULT OF HUMANS WITH 200 TRAINING INSTANCES, AKG (BART-LARGE) IS SLIGHTLY POORER THAN P2G (T5) BUT BETTER THAN P2G (BART-LARGE). OUR METHODS STILL ACHIEVES THE BEST PERFORMANCE IN THE CASE OF SIMILAR PARAMETRIC MODELS</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training set size</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">Structure-Aware (R)</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">5.8</td>
</tr>
<tr>
<td style="text-align: center;">Pivot(R)</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">17.3</td>
<td style="text-align: center;">10.7</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">SwitchPolicy (R)</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">TableGPT (R)</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;">BART-large</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">33.1</td>
<td style="text-align: center;">35.2</td>
</tr>
<tr>
<td style="text-align: center;">T5-prefix (R)</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">21.2</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">33.9</td>
</tr>
<tr>
<td style="text-align: center;">Retri-Gen (R)</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">4.9</td>
</tr>
<tr>
<td style="text-align: center;">P2G (T5)(R)</td>
<td style="text-align: center;">$27.9^{\dagger}$</td>
<td style="text-align: center;">$30.8^{\dagger}$</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">$37.3^{\dagger}$</td>
<td style="text-align: center;">$28.3^{\dagger}$</td>
<td style="text-align: center;">$30.5^{\dagger}$</td>
<td style="text-align: center;">$33.8^{\dagger}$</td>
<td style="text-align: center;">$36.1^{\dagger}$</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">40.1</td>
</tr>
<tr>
<td style="text-align: center;">P2G (BART-large)</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">$42.4^{\dagger}$</td>
<td style="text-align: center;">$41.6^{\dagger}$</td>
<td style="text-align: center;">$43.7^{\dagger}$</td>
<td style="text-align: center;">$44.3^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">AKG (BART-large) [Ours]</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">$33.3^{\dagger}$</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">29.6</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">46.1</td>
</tr>
</tbody>
</table>
<p>TABLE V
PARENT-F RESULTS FOR THE HUMANS, BOOKS, AND SONGS DOMAINS. ALL (R) ARE COPIED FROM THE ORIGINAL PAPER. THE BASELINE, WHICH DOES NOT EVALUATE THE PARENT-F METRIC, IS NOT SHOWN IN THIS TABLE. BOLD DENOTES THE BEST PERFORMANCE OF THIS EVALUATION. THE SECOND-BEST ONES ARE LABELED WITH $\dagger$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Training set size</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">SwitchPolicy (R)</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">40.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">42.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">44.8</td>
</tr>
<tr>
<td style="text-align: center;">BART-large</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">46.3</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">47.1</td>
</tr>
<tr>
<td style="text-align: center;">AMG (R)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">$51.9^{\dagger}$</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">46.0</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">48.6</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">46.9</td>
</tr>
<tr>
<td style="text-align: center;">Prefix-Tuning+T5 (R)</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">36.8</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">34.6</td>
</tr>
<tr>
<td style="text-align: center;">PCG (R)</td>
<td style="text-align: center;">$46.7^{\dagger}$</td>
<td style="text-align: center;">$48.3^{\dagger}$</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">$46.3^{\dagger}$</td>
<td style="text-align: center;">46.2</td>
<td style="text-align: center;">47.5</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">46.0</td>
</tr>
<tr>
<td style="text-align: center;">P2G (BART-large)</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">$47.4^{\dagger}$</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">$49.7^{\dagger}$</td>
<td style="text-align: center;">$45.5^{\dagger}$</td>
<td style="text-align: center;">$47.0^{\dagger}$</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">$47.9^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">AKG (BART-large) [Ours]</td>
<td style="text-align: center;">47.1</td>
<td style="text-align: center;">49.8</td>
<td style="text-align: center;">$50.3^{\dagger}$</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">$48.7^{\dagger}$</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">$48.3^{\dagger}$</td>
<td style="text-align: center;">49.3</td>
</tr>
</tbody>
</table>
<ul>
<li>BART-Large [40]: a powerful PLM for generative challenges.</li>
<li>T5-Prefix [47]: a T5 PLM, which is utilized for conditional generation with special prefix tokens.</li>
<li>AMG [48]: a PLM-based approach with multigrain attention on table slots and tokens with a dynamic memory mechanism to backtrack the allocation of table slots.</li>
<li>Prefix-tuning [15]: a prompt-tuning method that prepends a continuous token to preserve prior knowledge of the PLM. The performance of few-shot table-to-text generation was explored by [16].</li>
<li>PCG [16]: a prompt-tuning method with both prefixtuning and hard prompt to control generation content.</li>
</ul>
<h2>Retrieval-based models:</h2>
<ul>
<li>Retri-Gen [49]: a retrieval-based approach that retrieves and edits a prototype response from a predefined index for sentence generation.</li>
<li>P2G [17]: the authors proposed a retrieval-based framework that utilizes an IR system to provide a prototype for improving generation quality.</li>
</ul>
<h2>C. Automatic Evaluation</h2>
<p>Following the previous settings [13], [16], [17], we perform automatic evaluation with BLEU-4 [50] and ROUGE-4 [51] to measure the similarity between the generation of systems and the reference descriptions. BLEU-4 calculates the geometric mean of the precision over 4 grams of the output text. ROUGE-4 counts the number of overlapping 4 gram tokens between the generated description and the ideal summaries. In addition, we investigate the automatic evaluation of PARENT [52], a token overlap-based metric that estimates the fidelity of the generated sentence with respect to both the original table and the reference sentence. In our experiments, we report the F1 score of PARENT, denoted PARENT-F.</p>
<p>TABLE VI
Human EVALUATION RESULTS. $\uparrow$ DENOTES THE HIGHER THE BETTER AND
$\downarrow$ DENOTES THE LOWER THE BETTER. BOLD DENOTES THE BEST
PERFORMANCE OF THIS EVALUATION</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Sup $\uparrow$</th>
<th style="text-align: center;"># Cont $\downarrow$</th>
<th style="text-align: center;">Fluency $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">P2G (BART-large)</td>
<td style="text-align: center;">3.99</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">2.35</td>
</tr>
<tr>
<td style="text-align: left;">AKG (BART-large) [Ours]</td>
<td style="text-align: center;">$\mathbf{4 . 2 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 6}$</td>
<td style="text-align: center;">$\mathbf{2 . 7 4}$</td>
</tr>
</tbody>
</table>
<p>Table III and Table IV show the BLEU4 and ROUGE-4 results of our experiments. Our approach achieves state-of-the-art performance in the three domains, demonstrating the robustness and universality of our approach. Under nearparametric conditions, our approach provides a significant boost compared to previous methods. As shown in Fig. 4, compared with P2G (BART-large), which produces the second-best result with a similar number of parameters, our approach improves by, on average, $4 \%, 3 \%$, and $2 \%$ for BLEU and $9 \%, 5 \%$, and $4 \%$ for ROUGE on the Humans, Books, and Songs data sets, respectively. The results show
that our method can produce fluent descriptions.
Concerning the results of BLEU4 and ROUGE-4, the PLM-based methods significantly improve the fluency and coherence of the yielded sentences compared to the naive Seq2seq methods. By extending GPT2 [53], TableGPT [14], and SwitchPolicy, [13] achieved remarkable performance over the previous naive method. By increasing the number of model parameters and optimizing the encoder-decoder, T5 [47], BART-large [40], Prefix-Tuning+T5 [15], and PCG, [16] further improved the generation quality. However, the lack of domain-specific knowledge of PLMs becomes a bottleneck to bridging the gap between tabular data and descriptions. The P2G research of [17] introduced a retrieval-based method via the unlabeled domain-specific knowledge corpus and provided a new way to overcome the shortcomings of PLM-based methods. However, this method leaves out most of the information. Our approach provides an effective solution that targets this shortcoming according to the results.</p>
<p>Our approach also achieves better performance for PARENT-F than the other baseline methods. According to the results, compared to P2G (BART-large), our performance is better by, on average, $1.7 \%$ for nine terms and, on average, only $0.5 \%$ worse for three terms in PARENT-F. As we use the result of ROUGE-4 and BLEU as the primary evaluation standards throughout our training process, partial PARENTF accuracy is sacrificed. At the same time, during the human evaluation, we found that the knowledge-augmentation method also affects the PARENT-F score while enriching the generated content.</p>
<h2>D. Human Evaluation</h2>
<p>We also conduct a human evaluation to compare the AKG with the closest baseline, P2G (BART-large). All volunteers are postgraduate computer science students. Following the settings in [13], we evaluate each generated sentence with two tasks: faithfulness and fluency evaluation. The experiments are performed on the Humans data set with 100 training instances. We randomly select 100 generated sentences with the corresponding tabular data. In order to reduce variance caused by the participants, each example is scored by three different people.</p>
<p>Faithfulness aims to evaluate the correct information in the generated sentences. Only all information supported by the table makes the generated sentence faithful. Throughout the evaluation, each evaluator was asked to count the number of contained facts supported by the table data, noted as $# S u p$, and the number of contradictory facts, noted as $#$ Cont. We report the average number of $# S u p$ and $#$ Cont in Table VI. Fluency tries to evaluate the fluency of the generated sentences. A sentence is fluent if it is grammatically correct and natural. The raters were asked to rate the output in terms of fluency on a three-point Likert scale ( 0,1 , or 2 ). We report the average results in Table VI. The results show that our method provides a significant improvement over P2G (BART-large) for all metrics (sign test with a p-value $&lt;0.05$ ).</p>
<p>TABLE VII
Ablation study of the BLEU4 results for the Humans, Books, and Songs domains. Bold denotes the best performance of this EVALUATION</p>
<p>| Domain | Humans | | | | Books | | | | Songs | | | |
| Training set size | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 |
| AKG | 43.3 | 45.9 | 47.4 | 51.3 | 43.6 | 45.2 | 49.3 | 50.8 | 51.3 | 52.1 | 52.6 | 53.9 |
| -KA | 40.4 | 44.5 | 46.0 | 49.9 | 41.0 | 44.4 | 47.6 | 49.8 | 50.1 | 50.4 | 52.2 | 53.1 |
| -PT | 40.7 | 44.8 | 48.0 | 50.9 | 36.4 | 37.1 | 42.6 | 45.9 | 41.4 | 43.4 | 44.6 | 46.1 |
| -KA\&amp;PT | 39.2 | 44.0 | 46.5 | 49.6 | 36.0 | 37.2 | 42.4 | 45.6 | 41.4 | 43.7 | 44.0 | 45.9 |</p>
<p>TABLE VIII
Ablation study of ROUGE-4 results for the Humans, Books, and Songs domains. Bold denotes the best performance of this EVALUATION</p>
<p>| Domain | Humans | | | | Books | | | | Songs | | | |
| Training set size | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 |
| AKG | 28.2 | 31.5 | 33.3 | 39.2 | 29.6 | 31.1 | 34.6 | 36.9 | 43.7 | 43.9 | 44.8 | 46.1 |
| -KA | 24.8 | 28.9 | 31.4 | 36.6 | 27.1 | 30.4 | 33.6 | 34.8 | 42.4 | 41.6 | 43.7 | 44.3 |
| -PT | 26.2 | 30.9 | 34.6 | 37.4 | 24.1 | 26.4 | 28.9 | 31.7 | 30.8 | 33.2 | 34.3 | 37.4 |
| -KA\&amp;PT | 23.7 | 28.6 | 32.6 | 36.7 | 24.2 | 26.3 | 27.6 | 30.6 | 30.8 | 32.5 | 33.1 | 35.2 |</p>
<p>TABLE IX
Ablation study of the PARENT-F results for the Humans, Books, and Songs domains. Bold denotes the best performance of this EVALUATION</p>
<p>| Domain | Humans | | | | Books | | | | Songs | | | |
| Training set size | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 | 50 | 100 | 200 | 500 |
| AKG | 47.1 | 49.8 | 50.3 | 52.6 | 46.9 | 49.1 | 48.7 | 49.9 | 47.9 | 48.0 | 48.3 | 49.3 |
| -KA | 44.3 | 48.2 | 49.3 | 51.4 | 44.4 | 47.4 | 49.2 | 49.7 | 45.5 | 47.0 | 48.5 | 47.9 |
| -PT | 46.4 | 49.5 | 51.0 | 53.0 | 45.1 | 45.8 | 47.4 | 45.9 | 45.5 | 46.8 | 46.7 | 47.3 |
| -KA\&amp;PT | 44.4 | 48.2 | 49.5 | 51.3 | 42.9 | 45.5 | 46.3 | 47.3 | 44.7 | 46.8 | 44.9 | 47.1 |</p>
<h2>E. Ablation Study</h2>
<p>We conduct an ablation study on the BART-large model to systematically assess the impact of each proposed technique. In this context, the suffix "-KA" signifies the exclusion of knowledge adapter, thereby relying solely on PT, which aligns with the process of training P2G utilizing BART-large. Conversely, "-PT" denotes the removal of the prototype selector, entailing the application of KA for knowledge injection and leveraging the BART-large model as the primary generator to craft fluent descriptions from linearized tabular data. Additionally, "-KA\&amp;PT" illustrates the scenario where only the backbone generator (BART-large) is employed, devoid of any augmentations. Table VII, Table VIII, and Table IX demonstrate the results of BLEU4, ROUGE-4, and PARENT-F, respectively, for the Humans, Books, and Songs domains, and for training set sizes of 50, 100, 200, and 500.</p>
<p>We further investigate the impact of the number of prototypes, $n$ in Equation 2, on model performance with the setting of the suffix "-KA". Specifically, we train both BARTlarge and BART-small models using 50 and 500 instances from the Humans data set while varying the size of $n$. As illustrated in Table X, the outcomes encompass metrics such as BLEU,</p>
<p>ROUGE-4, and PARENT-F. For the Bart model, our findings indicate that when $n$ is relatively small, the performance metrics remain consistent. However, as $n$ approaches 10, a noticeable decline in performance is observed. Contrarily, the Bart-large model exhibits negligible sensitivity to variations in the number of prototypes, underscoring a distinct behavior in comparison to its smaller counterpart. This observed divergence is presumably due to the incremental inclusion of $n$ introducing ancillary or non-pertinent information within the tabular context, thus infusing noise and detrimentally impacting model performance. However, this adverse effect is conspicuously attenuated in models characterized by a broader parameter set and an increased volume of training instances, thereby indicating a diminished vulnerability to noise-induced degradation. Additionally, an evaluative comparison between models integrated with the PT module and those devoid of it-irrespective of employing BART-small or BART-large-unveils a tangible enhancement attributable to the PT module. Aligning with the experimental framework proposed in [32], we adopted $n=3$ in our experimental setup to maintain consistency and relevance in our investigation.</p>
<p>We also explored the impact of utilizing an unlabeled</p>
<p>TABLE X
An EMPIRICAL INVESTIGATION WAS UNDERTAKEN TO ASCERTAIN THE INFLUENCE OF PROTOTYPE QUANTITY ( $n$ ), EMPLOYING THE HUMAN DATA SET UNDER BOTH 50-SHOT AND 500-SHOT TRAINING SCENARIOS.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">n</th>
<th style="text-align: center;">BART-small</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BART-large</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">500</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">500</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">ROUGE-4</td>
<td style="text-align: center;">PARENT-F</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">ROUGE-4</td>
<td style="text-align: center;">PARENT-F</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">ROUGE-4</td>
<td style="text-align: center;">PARENT-F</td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">ROUGE-4</td>
<td style="text-align: center;">PARENT-F</td>
</tr>
<tr>
<td style="text-align: center;">Nah</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">51.3</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">39.8</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">51.4</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">45.2</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">52.3</td>
</tr>
<tr>
<td style="text-align: center;">6</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">49.1</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">52.4</td>
</tr>
<tr>
<td style="text-align: center;">7</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">50.5</td>
<td style="text-align: center;">36.3</td>
<td style="text-align: center;">51.7</td>
</tr>
<tr>
<td style="text-align: center;">8</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">49.3</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">51.9</td>
</tr>
<tr>
<td style="text-align: center;">9</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">52.7</td>
</tr>
</tbody>
</table>
<p>corpus on the performance of KA, particularly in the context of the "-PT" suffix. Following the methodology outlined in previous work [17], we divided the unlabeled corpus sourced from Wikipedia into two segments: 500 sentences for training and 1,000 sentences for testing purposes. In this experimental setup, we trained the BART-large model using 50 instances from the Humans data set while systematically varying the size of the unlabeled corpus in increments of 100, from 100 to 500 sentences. The results, detailed in Table XI, were evaluated using metrics such as BLEU, ROUGE-4, and PARENT-F. Our findings suggest that there is a notable improvement in performance with the increase in the size of the unlabeled corpus used. However, after the data volume exceeds 400 sentences, the experimental results begin to decline slightly due to the introduction of noise adversely affecting the model. Nonetheless, when compared to models without KA integration, a significant enhancement in overall results is observed, underscoring the substantial improvement brought about by incorporating KA.</p>
<p>The outcomes of this ablation study elucidate that integrating both the PT and the KA substantially enhances performance beyond that achievable by the backbone generator alone, with the exception observed in the Humans data set. An examination of the data sets, as detailed in Table I, reveals that the tabular data about Humans encompasses more information than the other two data sets. Paradoxically, this increased information content reduces complexity yet simultaneously diminishes the performance efficacy of PT. The comprehensive application of all proposed techniques results in further amelioration of experimental outcomes, underscoring the synergistic potential of these methodologies in enhancing model performance across diverse data contexts.</p>
<h2>F. Case Studies</h2>
<p>In Fig. 4, we present two generated examples of our model against the strongest baseline P2G (BART-large), along with the references from the Humans domain under 100 instances. The blue text indicates facts that are supported by the tabular data, and the red text indicates facts that are incorrect or not</p>
<p>TABLE XI
AN EMPIRICAL INVESTIGATION WAS UNDERTAKEN TO ASCERTAIN THE INFLUENCE OF UTILIZING THE EXTERNAL UNLABELED CORPUS SENTENCES, EMPLOYING THE HUMAN DATA SET UNDER BOTH 50-SHOT TRAINING SCENARIOS.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Utilized Corpus</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">ROUGE-4</th>
<th style="text-align: center;">PARENT-F</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Nah</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">44.4</td>
</tr>
<tr>
<td style="text-align: left;">100</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: left;">200</td>
<td style="text-align: center;">$\mathbf{4 1 . 7}$</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: left;">300</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">46.6</td>
</tr>
<tr>
<td style="text-align: left;">400</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">$\mathbf{2 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">500</td>
<td style="text-align: center;">40.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">46.4</td>
</tr>
</tbody>
</table>
<p>shown in the tabular data.</p>
<p>As seen in the first example, all attribute-value pairs are mentioned in the references and the two generated sentences. The reference sentences refer to "date of death unknown", which contradicts the original input. The two generated sentences introduce the correct "date of death" and the corresponding place via the domain-specific corpus. However, the P2G (BART-large) framework yields the fragment "fencing champion", which is far from the reference sentence and the tabular data. In contrast, the AKG framework performs better in balancing the use of the facts of the original table and the domain-specific knowledge. A similar issue can be seen in the second example. P2G (BART-large) generates the fragment "director, producer, and journalist", which is not mentioned in the tabular data or the references.</p>
<p>These results further illustrate that the AKG framework brings benefits in terms of balancing the use of facts from the original spreadsheet and domain-specific knowledge. The fluency and accuracy also improve.</p>
<h2>VI. CONCLUSIONS</h2>
<p>In this paper, we propose the AKG framework for fewshot table-to-text generation. Taking advantage of the adapting prompt design and the modularized pretraining strategy, we inject representations of the linguistic and semantic patterns of table-related descriptions and exploit a large domain-specific</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. Two example tables from the Human test data set, where the yielded texts from the different methods are trained with 100 training data points. The blue text denotes that the (incorrect) fact conflicts with the information in the original table. The red text indicates the fact is supported by the tabular data.</p>
<p>knowledge corpus fully. With the modularization strategy, our framework can devise various pretraining tasks to enhance the generative task, achieving high fluency and accuracy. Experimental results on three benchmark data sets show that our framework achieves superior performance in both fluency and faithfulness metrics. Our code and other related resources can be found in https://github.com/sjtugzx/AKG.</p>
<h2>VII. ACKNOWLEDGMENTS</h2>
<p>We thank the anonymous reviewers for their thoughtful comments. We thank Zhouhan Lin, Zhiyu Chen, and Junxian He for their valuable advice. Guanjie Zheng is the corresponding author of this paper. We acknowledge the financial support of the Ministry of Science and technology of the People's Republic China grant #2022YFB3904200 and National Science Foundation of China grant #42050105, #61960206002, #62020106005, #62032020, #62061146002, #62272301. This work is supported by the Deep-time Digital Earth (DDE) Big Science Program.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] A. Gatt and E. Krahmer, "Survey of the state of the art in natural language generation: Core tasks, applications and evaluation," <em>Journal of Artificial Intelligence Research</em>, vol. 61, pp. 65–170, 2018.</li>
<li>[2] Z. Chen, W. Chen, C. Smiley, S. Shah, I. Borova, D. Langdon, R. Moussa, M. Beane, T.-H. Huang, B. R. Routledge et al., "Finqa: A dataset of numerical reasoning over financial data," in <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, 2021, pp. 3697–3711.</li>
<li>[3] M. Ghazvininejad, C. Brockett, M.-W. Chang, B. Dolan, J. Gao, W.-t. Yih, and M. Galley, "A knowledge-grounded neural conversation model," in <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 32, no. 1, 2018.</li>
<li>[4] W. Chen, M.-W. Chang, E. Schlinger, W. Y. Wang, and W. W. Cohen, "Open question answering over tables and text," in <em>International Conference on Learning Representations</em>, 2020.</li>
<li>[5] H. He, A. Balakrishnan, M. Eric, and P. Liang, "Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings," in <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2017, pp. 1766–1776.</li>
<li>[6] S. Wiseman, S. M. Shieber, and A. M. Rush, "Challenges in data-to-document generation," in <em>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, 2017, pp. 2253–2263.</li>
<li>[7] S. Murakami, A. Watanabe, A. Miyazawa, K. Goshima, T. Yanase, H. Takamura, and Y. Miyao, "Learning to generate market comments from stock prices," in <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2017, pp. 1374–1384.</li>
<li>[8] S. A. Hasan and O. Farri, "Clinical natural language processing with deep learning," in <em>Data science for healthcare</em>. Springer, 2019, pp. 147–171.</li>
<li>[9] Z. Guo, J. Zhou, J. Qi, M. Yan, Z. He, G. Zheng, Z. Lin, X. Wang, and C. Zhou, "Towards controlled table-to-text generation with scientific reasoning," <em>arXiv preprint arXiv:2312.05402</em>, 2023.</li>
<li>[10] R. Lebret, D. Grangier, and M. Auli, "Neural text generation from structured data with application to the biography domain," in <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>, 2016, pp. 1203–1213.</li>
<li>[11] H. Iso, Y. Uehara, T. Ishigaki, H. Noji, E. Aramaki, I. Kobayashi, Y. Miyao, N. Okazaki, and H. Takamura, "Learning to select, track, and generate for data-to-text," in <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 2019, pp. 2102–2113.</li>
<li>[12] A. Parikh, X. Wang, S. Gehrmann, M. Faruqui, B. Dhingra, D. Yang, and D. Das, "Totto: A controlled table-to-text generation dataset," in <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020, pp. 1173–1186.</li>
<li>[13] Z. Chen, H. Eavani, W. Chen, Y. Liu, and W. Y. Wang, "Few-shot nlg with pre-trained language model," in <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 2020, pp. 183–190.</li>
<li>[14] H. Gong, Y. Sun, X. Feng, B. Qin, W. Bi, X. Liu, and T. Liu, "Tablegpt: Few-shot table-to-text generation with table structure reconstruction and content matching," in <em>Proceedings of the 28th International Conference on Computational Linguistics</em>, 2020, pp. 1978–1988.</li>
<li>[15] X. L. Li and P. Liang, "Prefix-tuning: Optimizing continuous prompts for generation," in <em>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, 2021, pp. 4582–4597.</li>
<li>[16] Y. Luo, M. Lu, G. Liu, and S. Wang, "Few-shot table-to-text generation with prefix-controlled generator," in <em>Proceedings of the 29th International Conference on Computational Linguistics</em>, 2022, pp. 6493–6504.</li>
<li>[17] Y. Su, Z. Meng, S. Baker, and N. Collier, "Few-shot table-to-text generation with prototype memory," in <em>Findings of the Association for Computational Linguistics: EMNLP 2021</em>, 2021, pp. 910–917.</li>
<li>[18] E. Reiter and R. Dale, "Building applied natural language generation systems," <em>Natural Language Engineering</em>, vol. 3, no. 1, pp. 57–87, 1997.</li>
<li>[19] P. Liang, M. I. Jordan, and D. Klein, "Learning semantic correspondences with less supervision," in <em>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</em>, 2009, pp. 91–99.</li>
<li>[20] M. Walker, O. Rambow, and M. Rogati, "Spot: A trainable sentence planner," in <em>Second Meeting of the North American Chapter of the Association for Computational Linguistics</em>, 2001.</li>
<li>[21] W. Lu, H. T. Ng, and W. S. Lee, "Natural language generation with tree conditional random fields," in <em>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</em>. Singapore: Association for Computational Linguistics, Aug. 2009, pp. 400–409.</li>
<li>[22] T. Liu, K. Wang, L. Sha, B. Chang, and Z. Sui, "Table-to-text generation by structure-aware seq2seq learning," in <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>, 2018.</li>
<li>[23] C. Gardent, A. Shimorina, S. Narayan, and L. Perez-Beltrachini, "The webnlg challenge: Generating text from rdf data," in <em>Proceedings of the 10th International Conference on Natural Language Generation</em>, 2017, pp. 124–133.</li>
</ul>
<p>[24] J. Novikova, O. Dusek, and V. Rieser, "The e2e dataset: New challenges for end-to-end generation," in 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics, 2017, pp. 201-206.
[25] A. See, P. J. Liu, and C. D. Manning, "Get to the point: Summarization with pointer-generator networks," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp. 1073-1083.
[26] H. Elsahar, C. Gravier, and F. Laforest, "Zero-shot question generation from knowledge graphs for unseen predicates and entity types," in Proceedings of NAACL-HLT, 2018, pp. 218-228.
[27] B.-H. Tseng, F. Kreyssig, P. Budzianowski, I. Casanueva, Y.-C. Wu, S. Ultes, and M. Gasic, "Variational cross-domain natural language generation for spoken dialogue systems," in Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue, 2018, pp. 338343 .
[28] V.-K. Tran and M. Le Nguyen, "Dual latent variable model for lowresource natural language generation in dialogue systems," in Proceedings of the 22nd Conference on Computational Natural Language Learning, 2018, pp. 21-30.
[29] C. Finn, P. Abbeel, and S. Levine, "Model-agnostic meta-learning for fast adaptation of deep networks," in International conference on machine learning. PMLR, 2017, pp. 1126-1135.
[30] F. Mi, M. Huang, J. Zhang, and B. Faltings, "Meta-learning for lowresource natural language generation in task-oriented dialogue systems," in Proceedings of the 28th International Joint Conference on Artificial Intelligence, 2019, pp. 3151-3157.
[31] H. Yao, Y.-x. Wu, M. Al-Shedivat, and E. Xing, "Knowledge-aware meta-learning for low-resource text classification," in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 1814-1821.
[32] Y. Su, D. Vandyke, S. Wang, Y. Fang, and N. Collier, "Plan-thengenerate: Controlled data-to-text generation via planning," in Findings of the Association for Computational Linguistics: EMNLP 2021, 2021, pp. 895-909.
[33] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, "Parameter-efficient transfer learning for nlp," in International Conference on Machine Learning. PMLR, 2019, pp. 2790-2799.
[34] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen et al., "Lora: Low-rank adaptation of large language models," in International Conference on Learning Representations.
[35] H. Liu, D. Tam, M. Mohammed, J. Mohta, T. Huang, M. Bansal, and C. Raffel, "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning," in Advances in Neural Information Processing Systems.
[36] E. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," 2021.
[37] L. Qin, X. Xu, L. Wang, Y. Zhang, and W. Che, "Modularized pretraining for end-to-end task-oriented dialogue," IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.
[38] D. Emelin, D. Bonadiman, S. Alqahtani, Y. Zhang, and S. Mansour, "Injecting domain knowledge in language models for task-oriented dialogue systems," arXiv preprint arXiv:2212.08120, 2022.
[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[40] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension," in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 7871-7880.
[41] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou, C. Gong, Y. Shen et al., "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models," arXiv preprint arXiv:2303.10420, 2023.
[42] P. Yang, H. Fang, and J. Lin, "Amerini: Enabling the use of lucene for information retrieval research," in Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, 2017, pp. 1253-1256.
[43] A. Białecki, R. Muir, G. Ingersoll, and L. Imagination, "Apache lucene 4," in SIGIR 2012 workshop on open source information retrieval, 2012, p. 17.
[44] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cotac, T. Rault, R. Louf, M. Funtowicz et al., "Transformers: State-of-the-art natural language processing," in Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, 2020, pp. 38-45.
[45] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds., 2015. [Online]. Available: http://arxiv.org/abs/1412.6980
[46] S. Ma, P. Yang, T. Liu, P. Li, J. Zhou, and X. Sun, "Key fact as pivot: A two-stage model for low resource table-to-text generation," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2047-2057.
[47] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu et al., "Exploring the limits of transfer learning with a unified text-to-text transformer." J. Mach. Learn. Res., vol. 21, no. 140, pp. 1-67, 2020.
[48] W. Zhao, Y. Liu, Y. Wan, and P. Yu, "Attend, memorize and generate: Towards faithful table-to-text generation in few shots," in Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 4106-4117. [Online]. Available: https://aclanthology.org/2021.findings-emnlp. 347
[49] Y. Wu, F. Wei, S. Huang, Y. Wang, Z. Li, and M. Zhou, "Response generation by context-aware prototype editing," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. $7281-7288$.
[50] K. Pupineni, S. Roukos, T. Ward, and W.-J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311-318.
[51] C.-Y. Lin, "Rouge: A package for automatic evaluation of summaries," in Text summarization branches out, 2004, pp. 74-81.
[52] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and W. Cohen, "Handling divergent reference texts when evaluating table-to-text generation," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 4884-4895.
[53] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners."
[54] J. Pfeiffer, A. Rücklé, C. Poth, A. Kamath, I. Vulić, S. Ruder, K. Cho, and I. Gurevych, "Adapterhub: A framework for adapting transformers," in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020): Systems Demonstrations. Online: Association for Computational Linguistics, 2020, pp. 46-54. [Online]. Available: https://www.aclweb.org/anthology/2020.emnlp-demos. 7</p>
<h2>APPENDIX A</h2>
<h2>EXPERIMENT DETAILS</h2>
<p>We apply Adam [45] as our optimizer with the learning rate set to 0.00003 . The mini-batch size is set to 16 . We use BART-large [40] as our backbone generator with the Hugging Face Library [44] with default settings. We set $&lt;\operatorname{sep}&gt;,&lt;\cos &gt;$, and $&lt;$ context_start $&gt;$ as special tokens to the BART vocabulary throughout the templating process. Moreover, we insert the knowledge adapter, based on the AdapterHub Library [54], into each encoder and decoder layer. For more details, refer to our released code and data at https://github.com/sjtugzx/PromptMize.git.</p>
<h2>APPENDIX B</h2>
<h2>DETAILS OF THE HUMAN EVALUATION SETUP</h2>
<p>To perform the human evaluation, we randomly select 100 generated sentences of the closest baseline, P2G (BART-large), and AKG with the corresponding table-reference pairs from the Humans data set with 100 paring instances. To reduce human bias, we randomly shuffle these 200 samples before presenting them to the three annotators. All volunteers were postgraduate students with NLP-related knowledge. Following previous research [13], we evaluate each generated sentence with two tasks: evaluating faithfulness and language fluency. Throughout the evaluation, all annotators are asked to follow</p>
<table>
<thead>
<tr>
<th style="text-align: center;">INPUT TABLE</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">name : diego barrado[fullname : diego armando barrado]birth date : 27 february 1981]birth place : bragado, argentina[height : 1,770 ]position : midfielder[currentclub : boca unidos]clubnumber : tba[years : 2004 - 2005 2006 - 2007 2007 - 2008 2011 - 2014 2014 -]clubs : $\rightarrow$ racing club ( loan ) $\rightarrow$ colón ( loan ) $\rightarrow$ olimpo ( loan ) atlético tucumán boca unidos[caps : 72293328122 19]goals : 550360 ]pcupdate : 21 march 2015 article title : diego barrado</td>
</tr>
<tr>
<td style="text-align: center;">Reference: diego armando barrado ( born 27 february 1981 in argentina ) is an argentine footballer.</td>
</tr>
<tr>
<td style="text-align: center;">AKG: diego armando barrado ( born 27 february 1981 in bragado ) is an argentinian footballer who plays for boca unidos as a midfielder.</td>
</tr>
<tr>
<td style="text-align: center;">-KA: diego armando barrado ( born 27 february 1981 ) is an argentine professional footballer who plays for boca unidos as a midfielder.</td>
</tr>
<tr>
<td style="text-align: center;">-PT: diego armando barrado ( born february 27 , 1981 in bragado ) is an argentine footballer who plays for boca unidos as a midfielder.</td>
</tr>
<tr>
<td style="text-align: center;">-KA\&amp;PT: diego armando barrado ( born 27 february 1981 ) is an argentinian professional footballer who plays for boca unidos as a midfielder.</td>
</tr>
<tr>
<td style="text-align: center;">INPUT TABLE</td>
</tr>
<tr>
<td style="text-align: center;">name : joris andringa[office : governor of new netherland]term start : december 15, 1673 ' ' ( commissioned )' ' ]term end : ' ' never entered upon his duties . ' 'predecessor : anthony colve[birth date : 1635]birth place : netherlands[death date : 28 may 1676]death place : mediterranean sea] profession : naval officer[article title : joris andringa</td>
</tr>
<tr>
<td style="text-align: center;">Reference: joris andringa ( 1635 -- may 28 , 1676 ) was a dutch naval officer .</td>
</tr>
<tr>
<td style="text-align: center;">AKG: joris andringa ( 1635 -- 28 may 1676 ) was the first governor of new netherland .</td>
</tr>
<tr>
<td style="text-align: center;">-KA: joris andringa ( 1635 -- 28 may 1676 ) was the first governor of new netherland .</td>
</tr>
<tr>
<td style="text-align: center;">-PT: joris andringa ( december 15, 1635 -- may 28 , 1676 ) was the first governor of new netherland .</td>
</tr>
<tr>
<td style="text-align: center;">-KA\&amp;PT: joris andringa ( ; 1635 -- 28 may 1676 ) was the governor of new netherland from december 15, 1673 to his death in 1676 .</td>
</tr>
<tr>
<td style="text-align: center;">INPUT TABLE</td>
</tr>
<tr>
<td style="text-align: center;">name : alexandra nessmar[birth date : 23 june 1994]birth place : sweden[role : rider]article title : alexandra nessmar</td>
</tr>
<tr>
<td style="text-align: center;">Reference: alexandra nessmar ( born 23 june 1994 ) is a swedish racing cyclist .</td>
</tr>
<tr>
<td style="text-align: center;">AKG: alexandra nessmar ( born 23 june 1994 ) is a swedish rider .</td>
</tr>
<tr>
<td style="text-align: center;">-KA: alexandra nessmar ( born 23 june 1994 ) is a sweden actress .</td>
</tr>
<tr>
<td style="text-align: center;">-PT: alexandra nessmar ( born 23 june 1994 ) is a former swedish motorcycle rider .</td>
</tr>
<tr>
<td style="text-align: center;">-KA\&amp;PT: alexandra nessmar ( born 23 june 1994 ) is a swedish professional mountain bike racer .</td>
</tr>
</tbody>
</table>
<p>Fig. 5. Additional examples of generated results from the ablation study. Red denotes information supported by the tabular data, blue denotes information that contradicts the tabular data, and green denotes grammatical mistakes that influence fluency.
the annotation guidelines.</p>
<h2>Human Evaluation Guidelines</h2>
<p>Give original tabular data and the generated descriptions. Annotators are asked to annotate and statistic the following three tasks:</p>
<ul>
<li>#Sup: Count the content supported by the tabular data.</li>
<li>#Cont: Count the content contradicting the tabular data.</li>
<li>Fluency: Estimate the fluency of the generated sentences. (Ignore the faithfulness, the sentence is fluent if it is grammatical and natural.) Likert scale is $1,2,3.1$ denotes the sentences containing obvious grammatical errors or are poorly formed. 2 denotes the sentences flow smoothly, with problems that do not affect the reading. 3 denotes that the sentences are fluent without any mistakes.</li>
</ul>
<h2>Table Example:</h2>
<p>Tabular Data:
name: michael phillip wojewoda
image: michael phillip wojewoda . jpg
caption: wojewoda performing with the rheostatics in 2007 at massey hall
background: nonvocalinstrumentalist
origin: toronto, ontario, canada
genre: indie rock
occupation: musician, record producer
associated acts: rheostatics space invaders
article title: michael phillip wojewoda
Generated Sentences: michael phillip wojewoda is an indie rock musician, record producer, and guitarist .</p>
<h2>Ranking Example:</h2>
<ul>
<li>#Sup: 4. Reason: michael phillip wojewoda, indie rock, musician, record producer are supported by the tabular data.</li>
<li>#Cont: 1. Reason: guitarist is not supported by the tabular data.</li>
<li>Fluency: 2. Reason: The overall expression is fluent. There are no grammatical errors, and any mistakes affect the human reading.</li>
</ul>
<h2>APPENDIX C</h2>
<h2>MORE EXAMPLES OF GENERATED RESULTS</h2>
<p>In this part, we provide more generated examples from the ablation study. The generated results are shown in Fig. 5. From the results, we can see that our model can generate fluent and diverse sentences. The prompt planner and the knowledge adapter improve the generation fluency and faithfulness to different extents compared with the baseline model BARTlarge. After applying all techniques, we can see that the generated quality is further improved. These results further demonstrate the applicability and generalization ability of our model.</p>            </div>
        </div>

    </div>
</body>
</html>
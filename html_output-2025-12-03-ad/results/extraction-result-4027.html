<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-94.html">extraction-schema-94</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-4dc5cbc458298ade91b8f87f2d2cc50d4cd764ec</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4dc5cbc458298ade91b8f87f2d2cc50d4cd764ec" target="_blank">Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel deep-learning-based framework to discriminate individuals with AD utilizing a multimodal and multiscale deep neural network and delivers 82.4% accuracy and 94.23% sensitivity in classifying individuals with clinical diagnosis of probable AD.</p>
                <p><strong>Paper Abstract:</strong> Alzheimer’s Disease (AD) is a progressive neurodegenerative disease where biomarkers for disease based on pathophysiology may be able to provide objective measures for disease diagnosis and staging. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo measurements of structure and function (glucose metabolism) in a living brain. It is hypothesized that combining multiple different image modalities providing complementary information could help improve early diagnosis of AD. In this paper, we propose a novel deep-learning-based framework to discriminate individuals with AD utilizing a multimodal and multiscale deep neural network. Our method delivers 82.4% accuracy in identifying the individuals with mild cognitive impairment (MCI) who will convert to AD at 3 years prior to conversion (86.4% combined accuracy for conversion within 1–3 years), a 94.23% sensitivity in classifying individuals with clinical diagnosis of probable AD, and a 86.3% specificity in classifying non-demented controls improving upon results in published literature.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neurodegeneration (atrophy & hypometabolism)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive neurodegeneration characterized by brain atrophy and reduced glucose metabolism (hypometabolism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper treats Alzheimer's disease as a progressive neurodegenerative disorder whose detectable imaging manifestations are regional brain atrophy (structural loss) and reduced resting-state glucose metabolism (hypometabolism) measurable with MRI and FDG-PET, respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Progressive neurodegeneration reflected by regional brain atrophy and decreased neuronal glucose metabolism (hypometabolism) underlying cognitive decline and the MCI → AD trajectory.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Evidence is observational from a human imaging cohort (ADNI) comprising N=1242 subjects and 2402 MRI/FDG-PET scans. The paper shows that FDG-PET hypometabolism and MRI-derived regional atrophy features discriminate subjects on an AD trajectory (pNC, pMCI, sAD) from stable controls (sNC) with statistically useful classification accuracies (see detection_performance). The authors cite prior literature linking FDG-PET metabolic decline and MRI atrophy to AD (refs 5-7, 33, 34). No molecular-level causal experiments (e.g., amyloid/tau mechanistic assays or animal experiments) are provided; evidence is associative (human observational imaging + machine-learning classification).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Structural MRI (regional volumes/patch volumes) and FDG-PET (regional mean glucose uptake); used separately and combined (multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Patch-wise MRI volume (representing regional atrophy) and patch-wise mean FDG-PET intensity (representing regional glucose metabolism/hypometabolism); brainstem mean intensity used as PET normalization reference.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Using ADNI cross-validated experiments (subjects n=626 for the pMCI vs sMCI comparison): MRI-alone accuracy 75.44% (sensitivity 77.27%, specificity 76.19%); FDG-PET-alone accuracy 81.53% (sensitivity 78.20%, specificity 82.47%); multimodal (MRI+PET) accuracy 82.93% (sensitivity 79.69%, specificity 83.84%). For sAD vs sNC (multiscale fusion): FDG-PET acc 84.46% (sen 79.89%, spe 91.90%), MRI acc 81.89% (sen 75.49%, spe 92.30%), multimodal acc 84.59% (sen 80.17%, spe 91.77%). When training the classifier with combined AD-trajectory samples (pNC,pMCI,sAD) and testing earlier timepoints the reported sensitivity for identifying individuals at risk was ~90.08% at ~1 year prior, 85.61% at ~2 years prior, and ~81.20% at ~3 years prior to clinical conversion; elsewhere the paper reports ~85.68% accuracy for 3-year conversion prediction (minor inconsistency noted in the manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Prodromal / early detection (mild cognitive impairment and up to ~1–3 years prior to clinical conversion); discriminates sNC vs subjects on AD trajectory (pNC, pMCI, sAD).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human observational imaging study using ADNI cohort (retrospective cross-validation machine-learning analysis on structural MRI and FDG-PET data).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Limitations stated by the authors: (1) Limited sample size relative to typical deep-learning needs, especially very small pNC subgroup (n=18) which reduces reliability for predicting conversion in baseline-normal subjects; (2) Clinical diagnoses (used as ground truth) are not 100% accurate, so imaging signatures overlap with non-AD conditions and may confound classifiers; (3) FDG-PET and MRI features are not perfectly specific to AD — similar patterns may appear in other conditions; (4) Need for better characterization of AD-specific imaging features or improvements in imaging modalities to improve classifier performance; (5) Potential for overfitting mitigated by ensemble/early stopping but generalizability beyond ADNI remains untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FDG-PET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fluorodeoxyglucose positron emission tomography (FDG-PET) measuring resting-state cerebral glucose metabolism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>FDG-PET measures regional brain glucose uptake; hypometabolism patterns are used here as functional biomarkers to detect AD-related neuronal dysfunction and predict conversion from MCI/NC to AD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Not a cause per se, but FDG-PET hypometabolism is interpreted as a marker of neuronal dysfunction resulting from AD-related neurodegeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>In this human ADNI imaging study FDG-PET-derived patch mean intensities (normalized to brainstem mean) provided stronger discrimination between progressive and stable cases than structural MRI alone; authors also cite prior work linking FDG-PET decline to AD progression (refs 6,7,33,34). Evidence is associative and based on classification performance in cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Acquisition of resting-state FDG-PET scans; co-registration to skull-stripped T1 MRI; intensity normalized by brainstem mean; extraction of patch-wise mean intensity features; multiscale patching (patch sizes 500, 1000, 2000 voxels) then fed into deep neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Patch-wise mean FDG uptake (normalized to brainstem) indicating regional hypometabolism; specific spatial patterns across patches learned by the DNN; greater discriminative power than MRI in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>FDG-PET alone: pMCI vs sMCI accuracy 81.53% (sensitivity 78.20%, specificity 82.47%) on n=626 subject comparison. For sAD vs sNC multiscale FDG-PET accuracy 84.46% (sensitivity 79.89%, specificity 91.90%). Early-detection sensitivities reported ~90.08% (1 year), 85.61% (2 years), ~81.20% (3 years) before conversion when classifier trained on combined AD-trajectory samples.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Prodromal / MCI and up to ~3 years before clinical conversion (early detection).</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human imaging / machine-learning classification using ADNI FDG-PET scans (cross-validated).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Authors note FDG-PET is more sensitive than MRI for prodromal detection but not perfectly specific; normalization choice (brainstem) is justified but could be a source of variability; small numbers in pNC subgroup limit conclusions about early-normal converters; FDG-PET patterns can overlap with other neurodegenerative or metabolic conditions; no external test cohort reported, so generalizability remains uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural MRI (patch volumes)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T1-weighted structural MRI-derived patch-wise volumes representing regional brain atrophy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>High-resolution structural MRI scans were segmented into anatomical ROIs then subdivided into multiscale spatial patches; the (transformed) patch volumes were used as biomarkers of regional atrophy to detect AD-related structural degeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Again, not a molecular cause: MRI-detected regional atrophy is treated as a macroscopic manifestation of AD-related neurodegeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Patch volumes extracted using FreeSurfer segmentation (87 ROIs) and non-rigid registration (LDDMM) were predictive of AD trajectory but performed less well than FDG-PET alone in this dataset. Evidence is based on machine-learning classification performance using ADNI scans (see detection_performance).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>T1-weighted MRI; FreeSurfer segmentation into gray matter ROIs then k-means based patching at multiscale (500, 1000, 2000 voxels); template-to-subject non-rigid registration; use of transformed patch volume as feature.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Patch volume (regional atrophy measure) and related multiscale structural features (texture, thickness, density, shape as noted in background) derived from MRI.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>MRI-alone performance for pMCI vs sMCI: accuracy 75.44% (sensitivity 77.27%, specificity 76.19%). For sAD vs sNC multiscale MRI accuracy 81.89% (sensitivity 75.49%, specificity 92.30%). Multimodal fusion usually improved overall accuracy relative to MRI alone.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Detects structural changes in MCI and dementia stages; less sensitive than FDG-PET for early/prodromal detection in this study but contributes complementary information in multimodal fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human imaging / machine-learning classification using ADNI T1 MRI scans (cross-validated).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Structural MRI findings were less discriminative than FDG-PET for prodromal detection in this dataset; MRI-based features also lack perfect specificity (overlap with non-AD causes of atrophy); patch-definition and registration introduce methodological variability; classification limited by sample size and by imperfect clinical ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of proposed causes or mechanisms of Alzheimer's disease, and any methods or biomarkers used for its detection, including evidence, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMDNN (multimodal multiscale DNN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal Multiscale Deep Neural Network (MMDNN) integrating multiscale patch-wise MRI and FDG-PET features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage deep neural network: six modality/scale-specific DNNs (three patch scales × two modalities) whose learned latent representations are concatenated and fed to a fusion DNN; trained with unsupervised pretraining (stacked autoencoders) and supervised fine-tuning with ensemble validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_cause</strong></td>
                            <td>Not applicable as a biological cause; the model is a detection algorithm that operationalizes the hypothesis that multiscale multimodal imaging patterns reflect AD neurodegeneration.</td>
                        </tr>
                        <tr>
                            <td><strong>cause_evidence</strong></td>
                            <td>Evidence of usefulness comes from cross-validated classification results on ADNI data (N=1242 subjects, 2402 scans): fusion MMDNN outperformed unimodal networks (e.g., multimodal pMCI vs sMCI accuracy 82.93% vs PET 81.53% and MRI 75.44%). Early-detection sensitivities up to ~90% (1 year) were reported when training on AD-trajectory samples. The paper reports experiments demonstrating improved accuracy with multiscale fusion versus single-scale inputs (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Machine-learning classifier built from multiscale patch-wise MRI volumes and FDG-PET mean intensities; unsupervised pretraining via stacked autoencoders, supervised fine-tuning with Adam optimization, dropout, early stopping, and ensemble of 10 networks for voting; 10-fold cross-validation at subject level.</td>
                        </tr>
                        <tr>
                            <td><strong>biomarker_or_finding</strong></td>
                            <td>Latent representations learned from multiscale patch-wise MRI and FDG-PET features produce discriminative feature vectors for classification of AD trajectory vs controls; the model highlights complementary information across scales and modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_performance</strong></td>
                            <td>Overall cross-validated performance for pMCI vs sMCI (n=626): multimodal MMDNN accuracy 82.93% (sensitivity 79.69%, specificity 83.84%). For combined AD-trajectory vs sNC the multimodal multiscale classifier achieved accuracy up to 86.44% (sensitivity 86.52%, specificity 86.32%). Early-prediction sensitivities reported ~90.08% (1 year), 85.61% (2 years), ~81.20% (3 years).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_stage</strong></td>
                            <td>Effective at prodromal stage (MCI) and for predicting conversion up to ~3 years prior in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>study_type</strong></td>
                            <td>Human imaging machine-learning study (cross-validated classifier trained and tested on ADNI imaging cohort).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_counter_evidence</strong></td>
                            <td>Model limitations discussed: requirement of large datasets for deep-learning (ADNI size smaller than ideal), potential bias from small validation sets (mitigated by ensemble voting), limited number of pNC subjects (n=18) reducing reliability for predicting conversion from normal baseline, dependence on accuracy of clinical diagnostic labels (not 100% specific), possible overfitting/generalizability concerns because external validation cohort not presented, and the biological non-specificity of imaging features (patterns may appear in other conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images', 'publication_date_yy_mm': '2017-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pre-clinical detection of alzheimer's disease using fdg-pet, with or without amyloid imaging. <em>(Rating: 2)</em></li>
                <li>Associations between cognitive, functional, and fdg-pet measures of decline in ad and mci. <em>(Rating: 2)</em></li>
                <li>Direct voxel-based comparison between grey matter hypometabolism and atrophy in Alzheimer's disease. <em>(Rating: 2)</em></li>
                <li>Accurate multimodal probabilistic prediction of conversion to alzheimer's disease in patients with mild cognitive impairment. <em>(Rating: 2)</em></li>
                <li>Prediction of mci to ad conversion, via mri, csf biomarkers, and pattern classification. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4027",
    "paper_id": "paper-4dc5cbc458298ade91b8f87f2d2cc50d4cd764ec",
    "extraction_schema_id": "extraction-schema-94",
    "extracted_data": [
        {
            "name_short": "Neurodegeneration (atrophy & hypometabolism)",
            "name_full": "Progressive neurodegeneration characterized by brain atrophy and reduced glucose metabolism (hypometabolism)",
            "brief_description": "The paper treats Alzheimer's disease as a progressive neurodegenerative disorder whose detectable imaging manifestations are regional brain atrophy (structural loss) and reduced resting-state glucose metabolism (hypometabolism) measurable with MRI and FDG-PET, respectively.",
            "citation_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images",
            "mention_or_use": "use",
            "proposed_cause": "Progressive neurodegeneration reflected by regional brain atrophy and decreased neuronal glucose metabolism (hypometabolism) underlying cognitive decline and the MCI → AD trajectory.",
            "cause_evidence": "Evidence is observational from a human imaging cohort (ADNI) comprising N=1242 subjects and 2402 MRI/FDG-PET scans. The paper shows that FDG-PET hypometabolism and MRI-derived regional atrophy features discriminate subjects on an AD trajectory (pNC, pMCI, sAD) from stable controls (sNC) with statistically useful classification accuracies (see detection_performance). The authors cite prior literature linking FDG-PET metabolic decline and MRI atrophy to AD (refs 5-7, 33, 34). No molecular-level causal experiments (e.g., amyloid/tau mechanistic assays or animal experiments) are provided; evidence is associative (human observational imaging + machine-learning classification).",
            "detection_method": "Structural MRI (regional volumes/patch volumes) and FDG-PET (regional mean glucose uptake); used separately and combined (multimodal).",
            "biomarker_or_finding": "Patch-wise MRI volume (representing regional atrophy) and patch-wise mean FDG-PET intensity (representing regional glucose metabolism/hypometabolism); brainstem mean intensity used as PET normalization reference.",
            "detection_performance": "Using ADNI cross-validated experiments (subjects n=626 for the pMCI vs sMCI comparison): MRI-alone accuracy 75.44% (sensitivity 77.27%, specificity 76.19%); FDG-PET-alone accuracy 81.53% (sensitivity 78.20%, specificity 82.47%); multimodal (MRI+PET) accuracy 82.93% (sensitivity 79.69%, specificity 83.84%). For sAD vs sNC (multiscale fusion): FDG-PET acc 84.46% (sen 79.89%, spe 91.90%), MRI acc 81.89% (sen 75.49%, spe 92.30%), multimodal acc 84.59% (sen 80.17%, spe 91.77%). When training the classifier with combined AD-trajectory samples (pNC,pMCI,sAD) and testing earlier timepoints the reported sensitivity for identifying individuals at risk was ~90.08% at ~1 year prior, 85.61% at ~2 years prior, and ~81.20% at ~3 years prior to clinical conversion; elsewhere the paper reports ~85.68% accuracy for 3-year conversion prediction (minor inconsistency noted in the manuscript).",
            "detection_stage": "Prodromal / early detection (mild cognitive impairment and up to ~1–3 years prior to clinical conversion); discriminates sNC vs subjects on AD trajectory (pNC, pMCI, sAD).",
            "study_type": "Human observational imaging study using ADNI cohort (retrospective cross-validation machine-learning analysis on structural MRI and FDG-PET data).",
            "limitations_or_counter_evidence": "Limitations stated by the authors: (1) Limited sample size relative to typical deep-learning needs, especially very small pNC subgroup (n=18) which reduces reliability for predicting conversion in baseline-normal subjects; (2) Clinical diagnoses (used as ground truth) are not 100% accurate, so imaging signatures overlap with non-AD conditions and may confound classifiers; (3) FDG-PET and MRI features are not perfectly specific to AD — similar patterns may appear in other conditions; (4) Need for better characterization of AD-specific imaging features or improvements in imaging modalities to improve classifier performance; (5) Potential for overfitting mitigated by ensemble/early stopping but generalizability beyond ADNI remains untested.",
            "uuid": "e4027.0",
            "source_info": {
                "paper_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "FDG-PET",
            "name_full": "Fluorodeoxyglucose positron emission tomography (FDG-PET) measuring resting-state cerebral glucose metabolism",
            "brief_description": "FDG-PET measures regional brain glucose uptake; hypometabolism patterns are used here as functional biomarkers to detect AD-related neuronal dysfunction and predict conversion from MCI/NC to AD.",
            "citation_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images",
            "mention_or_use": "use",
            "proposed_cause": "Not a cause per se, but FDG-PET hypometabolism is interpreted as a marker of neuronal dysfunction resulting from AD-related neurodegeneration.",
            "cause_evidence": "In this human ADNI imaging study FDG-PET-derived patch mean intensities (normalized to brainstem mean) provided stronger discrimination between progressive and stable cases than structural MRI alone; authors also cite prior work linking FDG-PET decline to AD progression (refs 6,7,33,34). Evidence is associative and based on classification performance in cross-validation.",
            "detection_method": "Acquisition of resting-state FDG-PET scans; co-registration to skull-stripped T1 MRI; intensity normalized by brainstem mean; extraction of patch-wise mean intensity features; multiscale patching (patch sizes 500, 1000, 2000 voxels) then fed into deep neural networks.",
            "biomarker_or_finding": "Patch-wise mean FDG uptake (normalized to brainstem) indicating regional hypometabolism; specific spatial patterns across patches learned by the DNN; greater discriminative power than MRI in this dataset.",
            "detection_performance": "FDG-PET alone: pMCI vs sMCI accuracy 81.53% (sensitivity 78.20%, specificity 82.47%) on n=626 subject comparison. For sAD vs sNC multiscale FDG-PET accuracy 84.46% (sensitivity 79.89%, specificity 91.90%). Early-detection sensitivities reported ~90.08% (1 year), 85.61% (2 years), ~81.20% (3 years) before conversion when classifier trained on combined AD-trajectory samples.",
            "detection_stage": "Prodromal / MCI and up to ~3 years before clinical conversion (early detection).",
            "study_type": "Human imaging / machine-learning classification using ADNI FDG-PET scans (cross-validated).",
            "limitations_or_counter_evidence": "Authors note FDG-PET is more sensitive than MRI for prodromal detection but not perfectly specific; normalization choice (brainstem) is justified but could be a source of variability; small numbers in pNC subgroup limit conclusions about early-normal converters; FDG-PET patterns can overlap with other neurodegenerative or metabolic conditions; no external test cohort reported, so generalizability remains uncertain.",
            "uuid": "e4027.1",
            "source_info": {
                "paper_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "Structural MRI (patch volumes)",
            "name_full": "T1-weighted structural MRI-derived patch-wise volumes representing regional brain atrophy",
            "brief_description": "High-resolution structural MRI scans were segmented into anatomical ROIs then subdivided into multiscale spatial patches; the (transformed) patch volumes were used as biomarkers of regional atrophy to detect AD-related structural degeneration.",
            "citation_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images",
            "mention_or_use": "use",
            "proposed_cause": "Again, not a molecular cause: MRI-detected regional atrophy is treated as a macroscopic manifestation of AD-related neurodegeneration.",
            "cause_evidence": "Patch volumes extracted using FreeSurfer segmentation (87 ROIs) and non-rigid registration (LDDMM) were predictive of AD trajectory but performed less well than FDG-PET alone in this dataset. Evidence is based on machine-learning classification performance using ADNI scans (see detection_performance).",
            "detection_method": "T1-weighted MRI; FreeSurfer segmentation into gray matter ROIs then k-means based patching at multiscale (500, 1000, 2000 voxels); template-to-subject non-rigid registration; use of transformed patch volume as feature.",
            "biomarker_or_finding": "Patch volume (regional atrophy measure) and related multiscale structural features (texture, thickness, density, shape as noted in background) derived from MRI.",
            "detection_performance": "MRI-alone performance for pMCI vs sMCI: accuracy 75.44% (sensitivity 77.27%, specificity 76.19%). For sAD vs sNC multiscale MRI accuracy 81.89% (sensitivity 75.49%, specificity 92.30%). Multimodal fusion usually improved overall accuracy relative to MRI alone.",
            "detection_stage": "Detects structural changes in MCI and dementia stages; less sensitive than FDG-PET for early/prodromal detection in this study but contributes complementary information in multimodal fusion.",
            "study_type": "Human imaging / machine-learning classification using ADNI T1 MRI scans (cross-validated).",
            "limitations_or_counter_evidence": "Structural MRI findings were less discriminative than FDG-PET for prodromal detection in this dataset; MRI-based features also lack perfect specificity (overlap with non-AD causes of atrophy); patch-definition and registration introduce methodological variability; classification limited by sample size and by imperfect clinical ground truth.",
            "uuid": "e4027.2",
            "source_info": {
                "paper_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images",
                "publication_date_yy_mm": "2017-10"
            }
        },
        {
            "name_short": "MMDNN (multimodal multiscale DNN)",
            "name_full": "Multimodal Multiscale Deep Neural Network (MMDNN) integrating multiscale patch-wise MRI and FDG-PET features",
            "brief_description": "A two-stage deep neural network: six modality/scale-specific DNNs (three patch scales × two modalities) whose learned latent representations are concatenated and fed to a fusion DNN; trained with unsupervised pretraining (stacked autoencoders) and supervised fine-tuning with ensemble validation.",
            "citation_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images",
            "mention_or_use": "use",
            "proposed_cause": "Not applicable as a biological cause; the model is a detection algorithm that operationalizes the hypothesis that multiscale multimodal imaging patterns reflect AD neurodegeneration.",
            "cause_evidence": "Evidence of usefulness comes from cross-validated classification results on ADNI data (N=1242 subjects, 2402 scans): fusion MMDNN outperformed unimodal networks (e.g., multimodal pMCI vs sMCI accuracy 82.93% vs PET 81.53% and MRI 75.44%). Early-detection sensitivities up to ~90% (1 year) were reported when training on AD-trajectory samples. The paper reports experiments demonstrating improved accuracy with multiscale fusion versus single-scale inputs (Table 3).",
            "detection_method": "Machine-learning classifier built from multiscale patch-wise MRI volumes and FDG-PET mean intensities; unsupervised pretraining via stacked autoencoders, supervised fine-tuning with Adam optimization, dropout, early stopping, and ensemble of 10 networks for voting; 10-fold cross-validation at subject level.",
            "biomarker_or_finding": "Latent representations learned from multiscale patch-wise MRI and FDG-PET features produce discriminative feature vectors for classification of AD trajectory vs controls; the model highlights complementary information across scales and modalities.",
            "detection_performance": "Overall cross-validated performance for pMCI vs sMCI (n=626): multimodal MMDNN accuracy 82.93% (sensitivity 79.69%, specificity 83.84%). For combined AD-trajectory vs sNC the multimodal multiscale classifier achieved accuracy up to 86.44% (sensitivity 86.52%, specificity 86.32%). Early-prediction sensitivities reported ~90.08% (1 year), 85.61% (2 years), ~81.20% (3 years).",
            "detection_stage": "Effective at prodromal stage (MCI) and for predicting conversion up to ~3 years prior in this dataset.",
            "study_type": "Human imaging machine-learning study (cross-validated classifier trained and tested on ADNI imaging cohort).",
            "limitations_or_counter_evidence": "Model limitations discussed: requirement of large datasets for deep-learning (ADNI size smaller than ideal), potential bias from small validation sets (mitigated by ensemble voting), limited number of pNC subjects (n=18) reducing reliability for predicting conversion from normal baseline, dependence on accuracy of clinical diagnostic labels (not 100% specific), possible overfitting/generalizability concerns because external validation cohort not presented, and the biological non-specificity of imaging features (patterns may appear in other conditions).",
            "uuid": "e4027.3",
            "source_info": {
                "paper_title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer’s Disease using structural MR and FDG-PET images",
                "publication_date_yy_mm": "2017-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pre-clinical detection of alzheimer's disease using fdg-pet, with or without amyloid imaging.",
            "rating": 2
        },
        {
            "paper_title": "Associations between cognitive, functional, and fdg-pet measures of decline in ad and mci.",
            "rating": 2
        },
        {
            "paper_title": "Direct voxel-based comparison between grey matter hypometabolism and atrophy in Alzheimer's disease.",
            "rating": 2
        },
        {
            "paper_title": "Accurate multimodal probabilistic prediction of conversion to alzheimer's disease in patients with mild cognitive impairment.",
            "rating": 2
        },
        {
            "paper_title": "Prediction of mci to ad conversion, via mri, csf biomarkers, and pattern classification.",
            "rating": 2
        }
    ],
    "cost": 0.01337175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis of Alzheimer's Disease using structural MR and FDG-PET images</h1>
<p>Donghuan Lu ${ }^{1, <em>}$, Karteek Popuri ${ }^{1}$, Weiguang Ding ${ }^{1}$, Rakesh Balachandar ${ }^{1}$, Mirza Faisal Beg ${ }^{1}$, and for the Alzheimer's Disease Neuroimaging Initiative ${ }^{+}$<br>${ }^{1}$ School of Engineering Science, Simon Fraser University, Burnaby, V5A 1S6, Canada<br></em>ludonghuan9@gmail.com<br>${ }^{+}$Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http :<br>//adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf</p>
<h4>Abstract</h4>
<p>Alzheimer's Disease (AD) is a progressive neurodegenerative disease. Amnestic mild cognitive impairment (MCI) is a common first symptom before the conversion to clinical impairment where the individual becomes unable to perform activities of daily living independently. Although there is currently no treatment available, the earlier a conclusive diagnosis is made, the earlier the potential for interventions to delay or perhaps even prevent progression to full-blown AD. Neuroimaging scans acquired from MRI and metabolism images obtained by FDG-PET provide in-vivo view into the structure and function (glucose metabolism) of the living brain. It is hypothesized that combining different image modalities could better characterize the change of human brain and result in a more accuracy early diagnosis of AD. In this paper, we proposed a novel framework to discriminate normal control(NC) subjects from subjects with AD pathology (AD and NC, MCI subjects convert to AD in future). Our novel approach utilizing a multimodal and multiscale deep neural network was found to deliver a $85.68 \%$ accuracy in the prediction of subjects within 3 years to conversion. Cross validation experiments proved that it has better discrimination ability compared with results in existing published literature.</p>
<h2>Introduction</h2>
<p>Alzheimer's disease (AD), the most common dementia, affecting 1 out of 9 people over the age of 65 years ${ }^{1}$. Alzheimer's diseases involves progressive cognitive impairment, commonly associated with early memory loss, requiring assistance for activities of self care during advanced stages. Alzheimer's is posited to evolve through a prodromal stage which is commonly referred to as the mild cognitive impairment(MCI) stage and $10-15 \%$ of individuals with MCI, progress to $\mathrm{AD}^{2}$ each year. With improved life expectancy, it is estimated that about $1.2 \%$ of global population will develop Alzheimer's disease by $2046^{3}$ thereby affecting millions of individuals directly, as well as many more indirectly through the effects on their families and caregivers. Current Alzheimer's research targets reliable prodromal identification of patients harbouring Alzheimer's pathology, for reasons that early intervention could potentially change the course of illness. Clinical diagnosis involves rigorous evaluation to rule out non-Alzheimer's causes for cognitive decline but this is however limited by specificity of identifying prodromal AD. Hence, we see the necessity of a tool to reliably detect and identify prodromal Alzheimer's disease.</p>
<p>Efforts to understand AD pathology in the past resulted in identifying neuroimaging as one of the promising tool for prodromal diagnosis ${ }^{4}$. Neuroimaging involving magnetic resonance imaging (MRI) ${ }^{5}$ and fluorodeoxyglucose positron emission tomography (FDG-PET) ${ }^{6}$ were the unique imaging modalities recognized as useful tools to identify individuals with prodromal AD. MRI offers structural details such as texture, thickness, density and shape of various brain regions, while FDG-PET measures the resting state glucose metabolism ${ }^{7}$, reflecting the functional activity of the underlying tissue ${ }^{6}$. FDG-PET and MRI are frequently employed neuroimaging techniques in computer-aided diagnosis of neurodegenerative diseases. There has been considerable efforts to use structural MRI ${ }^{8-10}$ or FDG-PET ${ }^{11,12}$ or a combination with other biomarkers ${ }^{13,14}$ to develop automated computer aided tools for prodromal diagnosis of Alzheimer's disease.</p>
<p>Deep neural networks have been studied extensively and proven to have the best performance for many recognition tasks ${ }^{15}$. The application of deep neural networks in recognition of AD-related patterns has also attracted interests in its application for</p>
<p>prodromal $\mathrm{AD}^{16-18}$. By applying deep neural network to extract features, such as stacked autoencoder(SAE) or Deep Boltzmann Machine(DBM), these approaches outperform other popular machine learning methods, e.g., support vector machine (SVM) and random forest techniques. However, one of the major hurdles preventing applications of deep learning in neuroimaging is that it requires a large data set to train the model, while the available number of images are limited to several hundred or thousand which are much less than feature dimension of data sample. To overcome this challenge, one popular approach is to segment images into patches and extract features from each patch ${ }^{16-18}$. However, down sampling the input data can result in the loss of discriminative information which could be a potential reason why previous methods haven't achieved satisfying accuracy for this diagnostic task. A common extension to pattern mining and feature extraction for image recognition is multiscale processing ${ }^{19,20}$. By extracting features at different resolutions or scales, multiscale features can better characterize images for classification task and a recent study indicates it can also improve the classification performance of deep neural networks ${ }^{21}$.</p>
<p>Therefore, we are proposing a novel approach to combine multiscale and multimodal processing with deep neural network for the early diagnosis of AD. Through cross validation experiments with more than 1000 subjects, we demonstrated that 1) our network can learn hidden patterns from multiscale and multimodal features for the detection of AD pathology; 2) our approach outperform previous methods regarding the discriminative task of potential AD subjects; and, 3) our network can identify the subjects who are going to convert to AD in 3 years with an accuracy of $85.68 \%$ which is a promising result.</p>
<h1>Methods</h1>
<p>There are two major steps in the proposed framework: 1)image preprocessing: segment both MRI scans and FDG-PET images into patches, and extract features from each patch; and, 2)classification: train a deep neural network to learn the patterns that discriminate AD pathology, and then use it to classify individuals with AD pathology.</p>
<h2>Data</h2>
<p>Data used in the preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD).</p>
<p>For a comprehensive validation of the proposed method, it is emphasized that all the available ADNI subjects( $\mathrm{N}=1242$ ) with both a T1-weighted MRI scan and FDG-PET image at the time of preparation of this manuscript were used in this study. These subjects were categorized into 5 groups: 1) stable Normal controls (sNC): 360 subjects diagnosed to be NC at baseline and remained the same at the time of preparation of this manuscript; 2) stable MCI (sMCI): 409 subjects diagnosed to be MCI at all time points(at least for 2 years); 3) progressive NC (pNC): 18 subjects evaluated to be NC at baseline but have progressed to probable AD at the time of preparation of this manuscript; 4) progressive MCI (pMCI): 217 subjects evaluated to be MCI at baseline and progressed to probable AD; 5) stable Alzheimer's disease (sAD): 238 subjects diagnosed to be AD for all available time points. Demographic and clinical information of the subjects are shown in Table 1. Numbers in brackets are the number of male and female subjects in the second row,, while in the rest 3 rows the two number represent the minimum and maximum value of age, education year and MMSE(Mini-Mental State Examination) score. It was worth mentioning that each subject could have multiple scans at different time points. In total there were 2402 FDG-PET scans and 2402 MRI images used in this study. Detailed descriptions of the ADNI subject cohorts, image acquisition protocols procedures and post-acquisition preprocessing procedures can be found at http://www. adni-info.org.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Mean(min-max)</th>
<th style="text-align: center;">sNC</th>
<th style="text-align: center;">sMCI</th>
<th style="text-align: center;">pNC</th>
<th style="text-align: center;">pMCI</th>
<th style="text-align: center;">sAD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Count (M/F)</td>
<td style="text-align: center;">$360(167 / 193)$</td>
<td style="text-align: center;">$409(239 / 170)$</td>
<td style="text-align: center;">$18(11 / 7)$</td>
<td style="text-align: center;">$217(126 / 91)$</td>
<td style="text-align: center;">$238(141 / 97)$</td>
</tr>
<tr>
<td style="text-align: center;">Age</td>
<td style="text-align: center;">$73.4(60-94)$</td>
<td style="text-align: center;">$74(56-91)$</td>
<td style="text-align: center;">$77(68-84)$</td>
<td style="text-align: center;">$74(55-89)$</td>
<td style="text-align: center;">$75(55-90)$</td>
</tr>
<tr>
<td style="text-align: center;">Education</td>
<td style="text-align: center;">$16.5(6-20)$</td>
<td style="text-align: center;">$15.8(7-20)$</td>
<td style="text-align: center;">$15.7(12-20)$</td>
<td style="text-align: center;">$16.0(8-20)$</td>
<td style="text-align: center;">$15.3(4-20)$</td>
</tr>
<tr>
<td style="text-align: center;">MMSE</td>
<td style="text-align: center;">$29.1(24-30)$</td>
<td style="text-align: center;">$28.0(22-30)$</td>
<td style="text-align: center;">$29.4(27-30)$</td>
<td style="text-align: center;">$26.5(9-30)$</td>
<td style="text-align: center;">$23.2(18-27)$</td>
</tr>
</tbody>
</table>
<p>Table 1. Subject Demographics. In the second row, the first number in each cell is the total number of subjects and the numbers in brackets represents the number of male and female. In the last 3 rows, the first number in each cell is the mean and the two numbers in brackets are the minimum and maximum value of age, education year and MMSE score, respectively.</p>
<h2>Image Processing</h2>
<p>Unlike typical image recognition problems where deep learning has shown to be effective, our data set is relatively small. Hence directly using this smaller database of images to train the deep neural network is not likely to deliver high classification accuracy.</p>
<p>However, contrary to typical image recognition tasks, where the images contain large heterogeneity, the images in this database are all human brain images acquired with similar pose and scale which show relatively much less heterogeneity in comparison. Therefore we applied following processing steps to extract patch-wise features as shown in Figure 1: FreeSurfer 5.122 was used to segment each T1 structural MRI image into gray matter and white matter followed by subdivision of the gray matter into 87 anatomical regions of interest (ROI). The Freesurfer segmentation were quality controlled by an expert neuroanatomist and any errors noted were manually corrected. Then, for a standard T1 MRI image, a voxel-wise $k$-means clustering based on spatial coordinates was performed to segment each ROI into patches based on its spatial information ${ }^{23}$. The size of patch was predefined as 500,1000 and 2000 voxels in this study and resulted into 1488, 705 and 343 patches, respectively. It was designed to keep enough detailed information as well as avoiding too large feature dimension considering the limited number of data samples were available in this study. Subsequently, each ROI of the standard template MRI was registered to the same ROI of every target image via a high-dimensional non-rigid registration method (LDDMM ${ }^{24}$ ). The registration maps were then applied to the patch-wise segmentation of the standard template. This transformed the template segmentation into each target MRI space so the target images were subdivided into the same number of patches. It worth mentioning that after the transformation, the size of a template patch in different images is not the same due to non-rigid registration encoding local expansion/contraction and hence is one of the features used to represent the regional information of a given structural brain scan. Then, for each target subject, the FDG-PET image of the subject was co-registered to its skull-stripped T1 MRI scan with a rigid transformation using FSL-FLIRT program ${ }^{25}$ based on normalized mutual information. The degrees of freedom (DOF) was set as 12 and Normalized correlation was used as cost function. The mean intensity in the brainstem region of the FDG-PET image was the chosen reference to normalize the voxel intensities in that individual brain metabolism image, because brainstem region was most unlikely to be affected by AD. The mean intensity of each patch was used as an element to form the feature vector to represent the metabolism activity, and the volume of each patch was used to represent the brain atrophy.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Flowchart of extracting patch-wise features from MRI scans and FDG-PET images. Each subject was segmented into patches through registration to a segmented template. Patch volume and mean intensity of FDG-PET were extracted as the feature to represent each patch.</p>
<h1>Multiscale Deep Neural Network</h1>
<p>With the features extracted from MRI and FDG-PET images, we trained a Multimodal Multiscale Deep Neural Network (MMDNN) to perform the classification. As shown in Fig 2, the network consists of two parts. The first parts was 6 independent deep neural network (DNN) corresponding to each scale of a single modality. The second part was another DNN used to fuse the features extracted from these 6 DNNs. The input data of this DNN was the concatenated latent representation learned from each single DNN. The DNNs in two parts shared the same structure. For each DNN, the number of nodes for each hidden layer were set as $3 N, \frac{3}{4} N$ and 100 respectively, where $N$ denotes the dimension of input feature vector. The number of nodes was chosen to explore all possible hidden correlation across features from different patches in the first layer and gradually reduce the number of features in the following layers to avoid over-fitting. We trained each DNN with two steps, unsupervised</p>
<p>pre-training and supervised fine-tuning, respectively. Then all the parameters of MMDNN was tuned together.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Multimodal Multiscale Deep Neural Network. 1488, 705 and 343 are input feature dimension(number of patches) extracted from different scales. For each layer, its number of nodes was denoted by the number on its top left. For each scale of each image modality, its patch-wise biomarkers were feed to a single DNN. The features from these 6 DNNs were fused by another DNN to generate the final classification result.</p>
<ul>
<li>Unsupervised Pre-training For unsupervised pre-training step, each DNN was trained as a stacked-autoencoder(SAE). Autoencoder is an artificial neural network used for unsupervised learning of non-linear hidden patterns from input data. It consists of three layers, input layer, hidden layer and output layer, for which two nearby layers are fully-connected. Three functions are used to define an autoencoder, encoding function, decoding function and loss function. In this study, encoding function is defined as: $y=s\left(W_{1} x+b_{1}\right)$, where $x$ is the input data, $y$ is the latent representation, $W_{1}$ is the weight matrix, $b_{1}$ is the bias term and $s$ is the activation function for which we used rectified linear function $\max (0, x)$. Similarly, decoding function can be represented as: $z=s\left(W_{2} y+b_{2}\right)$, where we constrained it with tied weight $W_{1}=W^{T}$ and $z$ is the reconstructed data which is supposed to be close to input $x$. Squared error $\frac{1}{2}|x-z|^{2}$ is applied as loss function to optimize the network. The hypothesis is that the latent representation can capture the main factors of variation in the data. Comparing with another popular unsupervised feature learning method principle component analysis (PCA), the activation function enables the network to capture non-linear factors of data variation, especially when multiple encoders and decoders are stacked to form a SAE. To fully train the network, we applied greedy layer wise training ${ }^{26}$ approach where every hidden layer was trained separately.</li>
<li>Supervised Fine-tuning After pre-training, the first three layers of a DNN were initialized with the parameters of encoders from pre-trained SAE followed by a softmax output layer. At first we trained the output layer independently while fixing the parameters of first 3 layers. Then we fine-tuned the whole network as Multilayer Perceptron (MLP) with subject labels for criterion. The network outputs the probabilities of a subject belongs to each class and the class with highest probability determines the output label of the subject. If we use $x^{i}, y^{i}$ to represent the input feature vector and label of the $i_{t h}$ sample, respectively, the loss function based on cross entropy can be displayed as:</li>
</ul>
<p>$$
H(i)=-\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{2}\left[\mathbb{1}\left{y^{i}=j\right} \log \left(h\left(x^{i}\right)_{j}\right]\right.
$$</p>
<p>where $N$ is the number of input samples, $j$ represents the class of samples, and $h$ represents the network function.</p>
<ul>
<li>Optimization of Network Every training step of the networks were performed via back propagation with Adam algorithm ${ }^{27}$. It is a first-order gradient-based optimization algorithm which has been proven to be computational efficient and appropriate for training deep neural network. During training stage, the training set was randomly split into mini</li>
</ul>
<p>batches ${ }^{28}$ where each of them contains 50 samples in this study. At every iteration, only a single mini batch was used for optimization. After every batch has been used once, the training set was reordered and randomly divided again so that each batch would have different samples in different epoch.</p>
<ul>
<li>Dropout In order to prevent deep neural network from overfitting, regularization is necessary to reduce its generalization error. In this study, we used dropout ${ }^{29}$ to learn more robust features and prevent overfitting. In the dropout layer, some units were randomly dropped, providing a way to combine exponentially many different neural networks. In this study, we inserted dropout layers after every hidden layer. In each iteration of training stage, only half of hidden units were randomly selected to feed the results to the next layer, while in the testing stage all hidden units were kept to perform the classification. By avoiding training all hidden units on every training sample, this regularization technique not only prevented complex co-adaptations on training data and decrease overfitting, but also reduced the amount of computation and improved training speed.</li>
<li>Early Stopping Another approach we used to prevent overfitting is early stopping. Because deep architecture were trained with iterative back propagation, the network were prone to be more adaptive to training data after every epoch. At a certain point, improving the network's fit to the training set will start to decrease generalization accuracy. In order to terminate the optimization algorithm before over-fitting, early stopping was used to provide guidance for how many iterations are needed. In the cross validation experiment, after dividing the data set into training and testing, we further split the training samples into a training set and a validation set. The networks were trained only with data in the former set, while samples in the latter set was used to determine when to stop the algorithm: while the network has the highest generalization accuracy for validation set. In actual training, we stopped the optimization if the validation accuracy had ceased to increase for 50 epochs.</li>
<li>Ensemble Classifiers Although early stopping has proven to be useful in most deep learning problems, relatively small data set limited the number of samples we could use for validation. And a small validation set may not able to represent the whole data set resulting in a biased network. Therefore, we resorted to ensemble multiple classifiers to perform more stable and robust classification. Instead of selecting a single validation set, we randomly divided the training set into 10 sets and used them to train 10 different networks to 'vote' for the classification. At the training stage, for network $i$, set $i$ would be used for validation while the rest 9 sets were used for training. At the testing stage, the test samples were feed into all these networks resulting in 10 sets of probabilities. For each sample, the probabilities from 10 networks was added and the class with highest probability was the classification result of this sample. Although the performance of ensemble classifiers may not be better than single network in every occasion, this strategy can statistically improve the classification accuracy as well as the robustness and stability of the classifier.</li>
</ul>
<h1>Results and Discussion</h1>
<h2>Experiments Setup</h2>
<p>The proposed deep neural network was built with Tensorflow ${ }^{30}$, an open source deep learning toolbox provided by Google. First, to compare the discriminant ability with state-of-the-art methods, 10 -fold cross validation experiments were applied to classify sMCI and pMCI subjects. Then we performed three experiments with different training sets to test whether the images of pNC and pMCI contain AD pathology or not. For these experiments, 4 data sets: sNC, sAD, pNC and pMCI, were divided into two groups in 3 different ways: 1) subjects of sNC and sAD were considered as group 1, and subjects of pNC and pMCI belonged to group 2; 2) subjects of pMCI, sNC and sAD belonged to group1, and pNC were considered as group2; 3) all subjects were considered as group1. For each experiment, we applied a 10 -fold cross validation on group1. The subjects of group 1 were randomly divided into 10 subsets, with 9 sets used for training and the rest set combined with subjects of group 2 used for testing. As detailed in the Methods Section, 10\% of training subjects were randomly selected as validation set for early stopping to prevent overfitting. 10 networks with different validation set were trained to 'vote' for the final classification result. Noticing it was not images but subjects we were splitting, so images from different time point of the same subject won't be used for both training and testing.</p>
<h2>Compare with State-of-the-Art Methods</h2>
<p>Researchers in the past have worked on classification of subjects with progressive cognitive decline and those with stable cognitive impairment. pMCI were recognized as individuals with high risk of AD, while the sMCI were considered as those with no risk or low risk of AD in these studies. To evaluate the performance of our approach, we compared the classification accuracy of pMCI vs. sMCI with previous methods using the same data modality, i.e. T1-weighted MRI and FDG-PET ${ }^{13,18,31,32}$. The proposed network outperformed the state-of-the-art method in classifying pMCI and sMCI individuals, irrespective of using single or multimodal imaging, as shown in Table 2. It is worth to mention that in the study of Chen et.al ${ }^{32}$, they performed</p>
<p>domain transfer learning to exploit the auxiliary domain data(AD/NC subjects) to improve the classification. Even though, the acuccracy of our methods without auxiliary knowledge was $3.5 \%$ accuracy than theirs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Modality</th>
<th style="text-align: center;"># Subjects</th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Sensitivity</th>
<th style="text-align: center;">Specificity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Young et al.</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">69.8</td>
</tr>
<tr>
<td style="text-align: center;">Liu et al.</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">64.29</td>
<td style="text-align: center;">74.07</td>
</tr>
<tr>
<td style="text-align: center;">Suk et al.</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">72.42</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">90.98</td>
</tr>
<tr>
<td style="text-align: center;">Cheng et al.</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">72.1</td>
</tr>
<tr>
<td style="text-align: center;">Proposed</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">$\mathbf{6 2 6}$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4 4}$</td>
<td style="text-align: center;">77.27</td>
<td style="text-align: center;">76.19</td>
</tr>
<tr>
<td style="text-align: center;">Young et al.</td>
<td style="text-align: center;">PET</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">64.6</td>
</tr>
<tr>
<td style="text-align: center;">Liu et al.</td>
<td style="text-align: center;">PET</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">57.14</td>
<td style="text-align: center;">82.41</td>
</tr>
<tr>
<td style="text-align: center;">Suk et al.</td>
<td style="text-align: center;">PET</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">70.75</td>
<td style="text-align: center;">25.45</td>
<td style="text-align: center;">96.55</td>
</tr>
<tr>
<td style="text-align: center;">Cheng et al.</td>
<td style="text-align: center;">PET</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">67.9</td>
</tr>
<tr>
<td style="text-align: center;">Proposed</td>
<td style="text-align: center;">PET</td>
<td style="text-align: center;">$\mathbf{6 2 6}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 5 3}$</td>
<td style="text-align: center;">78.20</td>
<td style="text-align: center;">82.47</td>
</tr>
<tr>
<td style="text-align: center;">Young et al.</td>
<td style="text-align: center;">MRI+PET+APOE</td>
<td style="text-align: center;">143</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: center;">Liu et al.</td>
<td style="text-align: center;">PET+MRI</td>
<td style="text-align: center;">234</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">76.19</td>
<td style="text-align: center;">70.37</td>
</tr>
<tr>
<td style="text-align: center;">Suk et al.</td>
<td style="text-align: center;">PET+MRI</td>
<td style="text-align: center;">204</td>
<td style="text-align: center;">75.92</td>
<td style="text-align: center;">48.04</td>
<td style="text-align: center;">95.23</td>
</tr>
<tr>
<td style="text-align: center;">Cheng et al.</td>
<td style="text-align: center;">PET+MRI+CSF</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">79.4</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;">Proposed</td>
<td style="text-align: center;">PET+MRI</td>
<td style="text-align: center;">$\mathbf{6 2 6}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 9 3}$</td>
<td style="text-align: center;">79.69</td>
<td style="text-align: center;">83.84</td>
</tr>
</tbody>
</table>
<p>Table 2. Accuracy(\%), Sensitivity(\%), and Specificity(\%) of the proposed network comparing with state-of-the-art methods. The third row is the number of subjects used in the experiments</p>
<h1>AD Pathology Classification</h1>
<p>One problem of sMCI subjects was that we only know they remained stable at the time of preparation this manuscript, but they could still progress to AD or other mental disease in the future. Although the the sMCI vs. pMCI experiment were commonly used to test the discriminate ability of classifiers in recent studies, the classification result of sMCI subjects may not be very accurate. Therefore, we performed the second experiment involved classifying individuals with only known Alzheimer's progression ( pNC , pMCI and sAD ) and normal controls ( sNC ). We investigated the performance of the classifier by using various combinations of samples during training phase. At a very basic level, we trained the classifier by discriminating sAD and sNC, at the next level pMCI and sAD were combined to represent the Alzheimer's group and trained to discriminate them from the sNC group. In the last level we combined pNC, pMCI and sAD to represent the Alzheimer's group to discriminate from the sNC group. The features extracted by the deep neural network are displayed in Fig 3. We observed the accuracy and sensitivity of the classifier progressively improved by additionally training with pMCI and pNC, while the specificity decreased, as displayed in Table 4. Further, the classifier performance was marginally better with the combination of FDG-PET and structural measurements compared to the performances with individual modalities. Interestingly, the classifier performance of structural imaging measurements were inferior to that of FDG-PET measurements. Supporting, the fact that FDG-PET, a measure of neuronal activity is a better tool to identify prodromal Alzheimer's as compared to structural images ${ }^{33,34}$.</p>
<h2>Multiscale Classification</h2>
<p>The classification accuracy of features extracted with different scales are listed in Table 3. We could not recognize any trend of increasing or decreasing classification performance with the changes in patch size. Therefore, features with higher resolution do not necessarily cover the discriminative information of lower resolution features. However, fusing multiscale features yielded superior accuracy as compared to uniscale, suggesting the network has the ability to capture discriminative information across the coarse to fine resolutions.</p>
<h2>Early Diagnosis</h2>
<p>We also investigated the classifier's ability to identify individuals with high risk of acquiring Alzheimer's, prior to disease onset. The classifier trained with the combined sample of Alzheimer's trajectory ( pNC , pMCI and sAD ) was superior, as compared to the classifier trained with sAD alone. As the network classifier was trained with patterns of AD trajectory using pNC and pMCI, the network was able to achieve exceptional classification performances in identifying individuals with AD risk, i.e the classifier recognized individuals with AD risk with a of $90.08 \%, 85.61 \%$ and $81.20 \%$, approximately at 1,2 , and 3 years prior to disease onset respectively. Studies in the past have predicted AD onset, using unimodal or multimodal investigation. Few studies have used PET as a single modality or in combination with structural MRI, CSF or cognitive measures to predict</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Features fed to the output layer. From left to right the training set is sNC vs sAD, sNC vs sAD and pMCI, and sNC vs sAD, pMCI and pNC, respectively. The y axis represents the units of the second from last layer, while x axis denotes the different data samples. This figure shows the different patterns as distilled by the deep learning network from the sNC, pNC, pMCI and AD images.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Set</th>
<th style="text-align: center;">Modality</th>
<th style="text-align: center;">$\mathbf{5 0 0}$</th>
<th style="text-align: center;">$\mathbf{1 0 0 0}$</th>
<th style="text-align: center;">$\mathbf{2 0 0 0}$</th>
<th style="text-align: center;">Multiscale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">sAD vs. sNC</td>
<td style="text-align: center;">FDG-PET</td>
<td style="text-align: center;">84.29</td>
<td style="text-align: center;">83.76</td>
<td style="text-align: center;">83.89</td>
<td style="text-align: center;">84.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">81.27</td>
<td style="text-align: center;">81.58</td>
<td style="text-align: center;">81.01</td>
<td style="text-align: center;">81.89</td>
</tr>
<tr>
<td style="text-align: center;">sAD and pMCI</td>
<td style="text-align: center;">FDG-PET</td>
<td style="text-align: center;">85.34</td>
<td style="text-align: center;">84.80</td>
<td style="text-align: center;">84.87</td>
<td style="text-align: center;">85.46</td>
</tr>
<tr>
<td style="text-align: center;">vs. sNC</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">82.18</td>
<td style="text-align: center;">82.69</td>
<td style="text-align: center;">82.10</td>
<td style="text-align: center;">82.77</td>
</tr>
<tr>
<td style="text-align: center;">sAD, pMCI</td>
<td style="text-align: center;">FDG-PET</td>
<td style="text-align: center;">85.43</td>
<td style="text-align: center;">85.28</td>
<td style="text-align: center;">84.93</td>
<td style="text-align: center;">85.89</td>
</tr>
<tr>
<td style="text-align: center;">and pNC vs. sNC</td>
<td style="text-align: center;">MRI</td>
<td style="text-align: center;">81.69</td>
<td style="text-align: center;">82.04</td>
<td style="text-align: center;">81.64</td>
<td style="text-align: center;">82.45</td>
</tr>
</tbody>
</table>
<p>Table 3. Accuracy(\%) using features at different scales of different modality.</p>
<p>AD onset ${ }^{13,32,35-38}$. The accuracy of 3 year prediction in the present network analysis was superior than those reported in the quoted studies. Studies predicting the illness onset, using structural MRI as a standalone tool or in addition to other clinical variables, have reported accuracy values inferior than to those obtained using PET ${ }^{5,39-49}$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Classification accuracy of different training sets. From left to right the training set is sNC vs sAD, sAD and pMCI, sAD, pMCI and pNC, respectively. The y axis represents accuracy, while x axis denotes time(year) to conversion in which ' 0 ' means the subjects are already diagnosed as AD at the imaging visit. The number in legend is the classification accuracy of all time points. Noticing the accuracy increases with more data used for training.</p>
<p>Deep neural network is a strong tool for accurate recognition of objects, by a-priori training of images with well characterized objects. Hence the basic requirement for accurate classification using DNN tool are providing large number of images (usually in millions) and well characterized objects during supervised training phase ${ }^{15}$. Therefore a compromise in the a-priori knowledge of the objects (features of Alzheimer's) provided during training would limit the accuracy of the subsequent classification. As our current understanding of AD pathogenesis and its precise characteristics in FDG-PET and structural MRI images are limited, DNN suffered jeopardy in achieving $100 \%$ accurate classification. The clinical criteria for the diagnosis of AD involves a series of evaluations to provide near precise diagnosis. Despite rigorous evaluations, clinically diagnosed individuals with AD are not $100 \%$ accurate and hence the FDG-PET and structural MRI characters can overlap with conditions other than AD,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Set</th>
<th style="text-align: center;">FDG-PET</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Volume</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multimodal</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">Spe</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">Spe</td>
<td style="text-align: center;">Acc</td>
<td style="text-align: center;">Sen</td>
<td style="text-align: center;">Spe</td>
</tr>
<tr>
<td style="text-align: center;">sAD vs. sNC</td>
<td style="text-align: center;">84.46</td>
<td style="text-align: center;">79.89</td>
<td style="text-align: center;">91.90</td>
<td style="text-align: center;">81.89</td>
<td style="text-align: center;">75.49</td>
<td style="text-align: center;">92.30</td>
<td style="text-align: center;">84.59</td>
<td style="text-align: center;">80.17</td>
<td style="text-align: center;">91.77</td>
</tr>
<tr>
<td style="text-align: center;">sAD and pMCI vs. sNC</td>
<td style="text-align: center;">85.46</td>
<td style="text-align: center;">85.01</td>
<td style="text-align: center;">86.19</td>
<td style="text-align: center;">82.77</td>
<td style="text-align: center;">79.76</td>
<td style="text-align: center;">87.65</td>
<td style="text-align: center;">85.96</td>
<td style="text-align: center;">85.65</td>
<td style="text-align: center;">86.45</td>
</tr>
<tr>
<td style="text-align: center;">sAD, pMCI and pNC vs. sNC</td>
<td style="text-align: center;">85.89</td>
<td style="text-align: center;">85.62</td>
<td style="text-align: center;">86.32</td>
<td style="text-align: center;">82.45</td>
<td style="text-align: center;">80.23</td>
<td style="text-align: center;">86.06</td>
<td style="text-align: center;">86.44</td>
<td style="text-align: center;">86.52</td>
<td style="text-align: center;">86.32</td>
</tr>
</tbody>
</table>
<p>Table 4. Accuracy(\%), sensitivity(\%) and specificity(\%) of different modality using different training sets.
including NC. Therefore the DNN trained with less accurately characterized images (FDG-PET and structural MRI) was unable to achieve $100 \%$ accurate classification. We propose an improvement in characterization of AD features by either upgrading FDG-PET and structural imaging methods or an increment in the understanding of AD specific pathogenesis, would positively impact the classification accuracy of the DNN classifier tool in future studies.</p>
<h1>Conclusion</h1>
<p>In summary, we proposed a deep neural network to identify individuals at risk of developing Alzheimer's disease. We trained the classifier with patterns hidden in different resolutions and different modalities to distinguish subjects with Alzheimer's trajectory ( pNC , pMCI and sAD ) and those without cognitive deficits ( sNC ). Our results show the classifier's ability to successfully distinguish individuals with AD pathology from sNC with a remarkable accuracy of $82.93 \%$ using cross validation experiments. We observed the performance of network classifier built by the combination of FDG-PET and structural MRI images was better than those built with either structural MRI or FDG-PET alone. Further the classifier trained with the combined sample of pNC, pMCI and sAD (Alzheimer's trajectory) was found to yield the highest classification accuracy. Lastly, our experiment to recognize individuals with AD pathology, prior to illness onset demonstrated a sensitivity of $85.68 \%$ in 3 years earlier to illness onset. Hence, the proposed deep neural network classifier can be a potential tool of choice in the future for early prediction of AD pathology. The number of pNC subjects was limited in this study resulting in a relatively low accuracy for pNC , as more data is accumulated in the future, we expect better accuracy in the prediction of NC subjects with AD pathology.</p>
<h2>References</h2>
<ol>
<li>Association, A. et al. 2011 alzheimer's disease facts and figures. Alzheimer's \&amp; dementia: journal Alzheimer's Assoc. 7, 208 (2011).</li>
<li>Petersen, R. C. et al. Mild cognitive impairment: ten years later. Arch. neurology 66, 1447-1455 (2009).</li>
<li>Brookmeyer, R., Johnson, E., Ziegler-Graham, K. \&amp; Arrighi, H. M. Forecasting the global burden of alzheimer's disease. Alzheimer's \&amp; dementia 3, 186-191 (2007).</li>
<li>Weiner, M. W. et al. Recent publications from the Alzheimer's Disease Neuroimaging Initiative: Reviewing progress toward improved AD clinical trials. Alzheimers Dement 13, e1-e85 (2017).</li>
<li>Davatzikos, C., Bhatt, P., Shaw, L. M., Batmanghelich, K. N. \&amp; Trojanowski, J. Q. Prediction of mci to ad conversion, via mri, csf biomarkers, and pattern classification. Neurobiol. aging 32, 2322-e19 (2011).</li>
<li>Landau, S. M. et al. Associations between cognitive, functional, and fdg-pet measures of decline in ad and mci. Neurobiol. aging 32, 1207-1218 (2011).</li>
<li>Mosconi, L. et al. Pre-clinical detection of alzheimer's disease using fdg-pet, with or without amyloid imaging. J. Alzheimer's Dis. 20, 843-854 (2010).</li>
<li>Farhan, S., Fahiem, M. A. \&amp; Tauseef, H. An ensemble-of-classifiers based approach for early diagnosis of alzheimer's disease: Classification using structural features of brain images. Comput. mathematical methods medicine 2014 (2014).</li>
<li>Korolev, S., Safiullin, A., Belyaev, M. \&amp; Dodonova, Y. Residual and plain convolutional neural networks for 3d brain mri classification. arXiv preprint arXiv:1701.06643 (2017).</li>
<li>Payan, A. \&amp; Montana, G. Predicting alzheimer's disease: a neuroimaging study with 3d convolutional neural networks. arXiv preprint arXiv:1502.02506 (2015).</li>
<li>
<p>Gray, K. R. et al. Multi-region analysis of longitudinal fdg-pet for the classification of alzheimer's disease. NeuroImage 60, 221-229 (2012).</p>
</li>
<li>
<p>Toussaint, P.-J. et al. Resting state fdg-pet functional connectivity as an early biomarker of alzheimer's disease using conjoint univariate and independent component analyses. Neuroimage 63, 936-946 (2012).</p>
</li>
<li>Young, J. et al. Accurate multimodal probabilistic prediction of conversion to alzheimer's disease in patients with mild cognitive impairment. NeuroImage: Clin. 2, 735-745 (2013).</li>
<li>Zhang, D. et al. Multimodal classification of alzheimer's disease and mild cognitive impairment. Neuroimage 55, 856-867 (2011).</li>
<li>Krizhevsky, A., Sutskever, I. \&amp; Hinton, G. E. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097-1105 (2012).</li>
<li>Liu, S. et al. Multimodal neuroimaging feature learning for multiclass diagnosis of alzheimer's disease. IEEE Transactions on Biomed. Eng. 62, 1132-1140 (2015).</li>
<li>Liu, S. et al. Early diagnosis of alzheimer's disease with deep learning. In Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium on, 1015-1018 (IEEE, 2014).</li>
<li>Suk, H.-I., Lee, S.-W., Shen, D., Initiative, A. D. N. et al. Hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis. NeuroImage 101, 569-582 (2014).</li>
<li>Zhang, W., Zelinsky, G. \&amp; Samaras, D. Real-time accurate object detection using multiple resolutions. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, 1-8 (IEEE, 2007).</li>
<li>Lowe, D. G. Distinctive image features from scale-invariant keypoints. Int. journal computer vision 60, 91-110 (2004).</li>
<li>Tang, Y. \&amp; Mohamed, A.-r. Multiresolution deep belief networks. In AISTATS, 1203-1211 (2012).</li>
<li>Dale AM, S. M., Fischl B. Cortical surface-based analysis. ii: Inflation, flattening, and a surface-based coordinate system. Neuroimage 9(2), 195-207 (1999).</li>
<li>Raamana, P. R. et al. Thickness network features for prognostic applications in dementia. Neurobiol. aging 36, S91-S102 (2015).</li>
<li>Younes, M. B. M. M. A. T. L. Computing large deformation metric mappings via geodesic flows of diffeomorphisms. Int. journal computer vision 61.2, 139-157 (2005).</li>
<li>Jenkinson, M., Bannister, P., Brady, M. \&amp; Smith, S. Improved optimization for the robust and accurate linear registration and motion correction of brain images. Neuroimage 17, 825-841 (2002).</li>
<li>Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H. et al. Greedy layer-wise training of deep networks. Adv. neural information processing systems 19, 153 (2007).</li>
<li>Kingma, D. \&amp; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).</li>
<li>Bengio, Y. Practical recommendations for gradient-based training of deep architectures. In Neural networks: Tricks of the trade, 437-478 (Springer, 2012).</li>
<li>Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I. \&amp; Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929-1958 (2014).</li>
<li>Abadi, M., Agarwal, A., Barham, P., Brevdo, E. \&amp; et al. TensorFlow: Large-scale machine learning on heterogeneous systems (2015). URL http://tensorflow.org/. Software available from tensorflow.org.</li>
<li>Liu, K., Chen, K., Yao, L. \&amp; Guo, X. Prediction of mild cognitive impairment conversion using a combination of independent component analysis and the cox model. Front. human neuroscience 11 (2017).</li>
<li>Cheng, B., Liu, M., Zhang, D., Munsell, B. C. \&amp; Shen, D. Domain transfer learning for mci conversion prediction. IEEE Transactions on Biomed. Eng. 62, 1805-1817 (2015).</li>
<li>Kawachi, T. et al. Comparison of the diagnostic performance of FDG-PET and VBM-MRI in very mild Alzheimer's disease. Eur. J. Nucl. Medicine Mol. Imaging 33, 801-809 (2006).</li>
<li>Chételat, G. et al. Direct voxel-based comparison between grey matter hypometabolism and atrophy in Alzheimer's disease. Brain 131, 60-71 (2008).</li>
<li>Xu, L., Wu, X., Chen, K. \&amp; Yao, L. Multi-modality sparse representation-based classification for alzheimer's disease and mild cognitive impairment. Comput. methods programs biomedicine 122, 182-190 (2015).</li>
<li>
<p>Cabral, C. et al. Predicting conversion from mci to ad with fdg-pet brain images at different prodromal stages. Comput. biology medicine 58, 101-109 (2015).</p>
</li>
<li>
<p>Zhang, D. \&amp; Shen, D. Predicting future clinical changes of MCI patients using longitudinal and multimodal biomarkers. PLoS ONE 7, e33182 (2012).</p>
</li>
<li>Shaffer, J. L. et al. Predicting cognitive decline in subjects at risk for Alzheimer disease by using combined cerebrospinal fluid, MR imaging, and PET biomarkers. Radiol. 266, 583-591 (2013).</li>
<li>Eskildsen, S. F., Coupe, P., Garcia-Lorenzo, D., Fonov, V. \&amp; et al. Prediction of Alzheimer's disease in subjects with mild cognitive impairment from the ADNI cohort using patterns of cortical thinning. Neuroimage 65, 511-521 (2013).</li>
<li>Moradi, E. et al. Machine learning framework for early mri-based alzheimer's conversion prediction in mci subjects. Neuroimage 104, 398-412 (2015).</li>
<li>Korolev, I. O., Symonds, L. L., Bozoki, A. C., Initiative, A. D. N. et al. Predicting progression from mild cognitive impairment to alzheimer's dementia using clinical, mri, and plasma biomarkers via probabilistic pattern classification. PloS one 11, e0138866 (2016).</li>
<li>Misra, C., Fan, Y. \&amp; Davatzikos, C. Baseline and longitudinal patterns of brain atrophy in MCI patients, and their use in prediction of short-term conversion to AD: results from ADNI. Neuroimage 44, 1415-1422 (2009).</li>
<li>Ye, J. et al. Sparse learning and stability selection for predicting MCI to AD conversion using baseline ADNI data. BMC Neurol 12, 46 (2012).</li>
<li>Gaser, C., Franke, K., Kloppel, S., Koutsouleris, N. \&amp; Sauer, H. BrainAGE in Mild Cognitive Impaired Patients: Predicting the Conversion to Alzheimer's Disease. PLoS ONE 8, e67346 (2013).</li>
<li>Cuingnet, R. et al. Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database. Neuroimage 56, 766-781 (2011).</li>
<li>Wolz, R., Julkunen, V., Koikkalainen, J., Niskanen, E. \&amp; et al. Multi-method analysis of MRI images in early diagnostics of Alzheimer's disease. PLoS ONE 6, e25446 (2011).</li>
<li>Chupin, M., Gerardin, E., Cuingnet, R., Boutet, C. \&amp; et al. Fully automatic hippocampus segmentation and classification in Alzheimer's disease and mild cognitive impairment applied on data from ADNI. Hippocampus 19, 579-587 (2009).</li>
<li>Cho, Y., Seong, J. K., Jeong, Y., Shin, S. Y. \&amp; et al. Individual subject classification for Alzheimer's disease based on incremental learning using a spatial frequency representation of cortical thickness data. Neuroimage 59, 2217-2230 (2012).</li>
<li>Coupe, P., Eskildsen, S. F., Manjon, J. V., Fonov, V. S. \&amp; Collins, D. L. Simultaneous segmentation and grading of anatomical structures for patient's classification: application to Alzheimer's disease. Neuroimage 59, 3736-3747 (2012).</li>
</ol>
<h1>Consortia</h1>
<h2>for the Alzheimer's Disease Neuroimaging Initiative</h2>
<p>Michael Weiner ${ }^{2}$, Paul Aisen ${ }^{3}$, Ronald Petersen ${ }^{4}$, Cliford Jack ${ }^{5}$, William Jagust ${ }^{6}$, John Trojanowki ${ }^{7}$, Arthur Toga ${ }^{8}$, Laurel Beckett ${ }^{9}$, Robert Green ${ }^{10}$, Andrew Saykin ${ }^{11}$, John Morris ${ }^{12}$, Leslie Shaw ${ }^{12}$, Jefrey Kaye ${ }^{13}$, Joseph Quinn ${ }^{13}$, Lisa Silbert ${ }^{13}$, Betty Lind ${ }^{13}$, Raina Carter ${ }^{13}$, Sara Dolen ${ }^{13}$, Lon Schneider ${ }^{8}$, Sonia Pawluczyk ${ }^{8}$, Mauricio Beccera ${ }^{8}$, Liberty Teodoro ${ }^{8}$, Bryan Spann ${ }^{8}$, James Brewer ${ }^{14}$, Helen Vanderswag ${ }^{14}$, Adam Fleisher ${ }^{14}$, Judith Heidebrink ${ }^{15}$, Joanne Lord ${ }^{15}$, Sara Mason ${ }^{5}$, Colleen Albers ${ }^{5}$, David Knopman ${ }^{5}$, Kris Johnson ${ }^{5}$, Rachelle Doody ${ }^{16}$, Javier Villanueva-Meyer ${ }^{16}$, Munir Chowdhury ${ }^{16}$, Susan Rountree ${ }^{16}$, Mimi Dang ${ }^{16}$, Yaakov Stern ${ }^{17}$, Lawrence Honig ${ }^{17}$, Karen Bell ${ }^{17}$, Beau Ances $^{12}$, John Morris ${ }^{12}$, Maria Carroll ${ }^{12}$, Mary Creech ${ }^{2}$, Erin Franklin ${ }^{12}$, Mark Mintun ${ }^{12}$, Stacy Schneider ${ }^{12}$, Angela Oliver ${ }^{12}$, Daniel Marson ${ }^{18}$, Randall Grifth ${ }^{18}$, David Clark ${ }^{18}$, David Geldmacher ${ }^{18}$, John Brockington ${ }^{18}$, Erik Roberson ${ }^{18}$, Marissa Natelson Love ${ }^{18}$, Hillel Grossman ${ }^{19}$, Efe Mitsis ${ }^{19}$, Raj Shah ${ }^{20}$, Leyla deToledo-Morrell ${ }^{20}$, Ranjan Duara ${ }^{21}$, Daniel Varon ${ }^{21}$, Maria Greig ${ }^{21}$, Peggy Roberts ${ }^{21}$, Marilyn Albert ${ }^{22}$, Chiadi Onyike ${ }^{22}$, Daniel D'Agostino ${ }^{22}$, Stephanie Kielb ${ }^{22}$, James Galvin ${ }^{23}$, Brittany Cerbone ${ }^{23}$, Christina Michel ${ }^{23}$, Dana Pogorelec ${ }^{23}$, Henry Rusinek ${ }^{23}$, Mony de Leon ${ }^{23}$, Lidia Glodzik ${ }^{23}$, Susan De Santi ${ }^{23}$, P. Doraiswamy ${ }^{24}$, Jefrey Petrella ${ }^{24}$, Salvador Borges-Neto ${ }^{24}$, Terence Wong ${ }^{24}$, Edward Coleman ${ }^{24}$, Charles Smith ${ }^{25}$, Greg Jicha ${ }^{25}$, Peter Hardy ${ }^{25}$, Partha Sinha ${ }^{25}$, Elizabeth Oates ${ }^{25}$, Gary Conrad ${ }^{25}$, Anton Porsteinsson ${ }^{26}$, Bonnie Goldstein ${ }^{26}$, Kim Martin ${ }^{26}$, Kelly Makino ${ }^{26}$, M. Ismail ${ }^{26}$, Connie Brand ${ }^{26}$, Ruth Mulnard ${ }^{27}$, Gaby Thai ${ }^{27}$, Catherine Mc-Adams-Ortiz ${ }^{27}$, Kyle Womack ${ }^{28}$, Dana Mathews ${ }^{28}$, Mary Quiceno ${ }^{28}$, Allan Levey ${ }^{29}$, James Lah ${ }^{29}$, Janet Cellar ${ }^{29}$, Jefrey Burns ${ }^{30}$, Russell Swerdlow ${ }^{30}$, William Brooks ${ }^{30}$, Liana Apostolova ${ }^{31}$, Kathleen Tingus ${ }^{31}$, Ellen Woo ${ }^{31}$, Daniel Silverman ${ }^{31}$, Po Lu ${ }^{31}$, George Bartzokis ${ }^{31}$, Neill Graf-Radford ${ }^{32}$, Francine Parftt ${ }^{32}$, Tracy Kendall ${ }^{32}$, Heather Johnson ${ }^{32}$, Martin Farlow ${ }^{11}$, Ann Marie Hake ${ }^{11}$, Brandy Matthews ${ }^{11}$, Jared Brosch ${ }^{11}$, Scott Herring ${ }^{11}$, Cynthia Hunt ${ }^{11}$, Christopher Dyck ${ }^{33}$, Richard Carson ${ }^{33}$, Martha MacAvoy ${ }^{33}$, Pradeep Varma ${ }^{33}$, Howard Chertkow ${ }^{34}$, Howard Bergman ${ }^{34}$, Chris Hosein ${ }^{34}$, Sandra Black ${ }^{35}$, Bojana Stefanovic ${ }^{35}$, Curtis Caldwell ${ }^{35}$, Ging-Yuek Robin Hsiung ${ }^{36}$, Howard Feldman ${ }^{36}$, Benita Mudge ${ }^{36}$, Michele Assaly ${ }^{36}$, Elizabeth Finger ${ }^{37}$,</p>
<p>Stephen Pasternack ${ }^{37}$, Irina Rachisky ${ }^{37}$, Dick Trost ${ }^{37}$, Andrew Kertesz ${ }^{37}$, Charles Bernick ${ }^{38}$, Donna Munic ${ }^{38}$, MarekMarsel Mesulam ${ }^{39}$, Kristine Lipowski ${ }^{39}$, Sandra Weintraub ${ }^{39}$, Borna Bonakdarpour ${ }^{39}$, Diana Kerwin ${ }^{39}$, Chuang-Kuo Wu $^{39}$, Nancy Johnson ${ }^{39}$, Carl Sadowsky ${ }^{40}$, Teresa Villena ${ }^{40}$, Raymond Scott Turner ${ }^{41}$, Kathleen Johnson ${ }^{41}$, Brigid Reynolds ${ }^{41}$, Reisa Sperling ${ }^{42}$, Keith Johnson ${ }^{42}$, Gad Marshall ${ }^{42}$, Jerome Yesavage ${ }^{43}$, Joy Taylor ${ }^{43}$, Barton Lane ${ }^{43}$, Allyson Rosen ${ }^{43}$, Jared Tinklenberg ${ }^{43}$, Marwan Sabbagh ${ }^{44}$, Christine Belden ${ }^{44}$, Sandra Jacobson ${ }^{44}$, Sherye Sirrel ${ }^{44}$, Neil Kowall ${ }^{45}$, Ronald Killiany ${ }^{45}$, Andrew Budson ${ }^{45}$, Alexander Norbash ${ }^{45}$, Patricia Lynn Johnson ${ }^{45}$, Thomas Obisesan ${ }^{46}$, Saba Wolday ${ }^{46}$, Joanne Allard ${ }^{46}$, Alan Lerner ${ }^{47}$, Paula Ogrocki ${ }^{47}$, Curtis Tatsuoka ${ }^{47}$, Marianne Fatica ${ }^{47}$, Evan Fletcher ${ }^{48}$, Pauline Maillard ${ }^{48}$, John Olichney ${ }^{48}$, Charles DeCarli ${ }^{48}$, Owen Carmichael ${ }^{48}$, Smita Kittur ${ }^{49}$, Michael Borrie ${ }^{50}$, T-Y Lee ${ }^{50}$, RobBartha ${ }^{50}$, Sterling Johnson ${ }^{51}$, Sanjay Asthana ${ }^{51}$, Cynthia Carlsson ${ }^{51}$, Steven Potkin ${ }^{52}$, Adrian Preda ${ }^{52}$, Dana Nguyen ${ }^{52}$, Pierre Tariot ${ }^{53}$, Anna Burke ${ }^{53}$, Nadira Trncic ${ }^{53}$, Adam Fleisher ${ }^{53}$, Stephanie Reeder ${ }^{53}$, Vernice Bates ${ }^{54}$, Horacio Capote ${ }^{54}$, Michelle Rainka ${ }^{54}$, Douglas Scharre ${ }^{55}$, Maria Kataki ${ }^{55}$, Anahita Adeli ${ }^{55}$, Earl Zimmerman ${ }^{56}$, Dzintra Celmins ${ }^{56}$, Alice Brown ${ }^{56}$, Godfrey Pearlson ${ }^{57}$, Karen Blank ${ }^{57}$, Karen Anderson ${ }^{57}$, Laura Flashman ${ }^{58}$, Marc Seltzer ${ }^{58}$, Mary Hynes ${ }^{58}$, Robert Santulli ${ }^{58}$, Kaycee Sink ${ }^{59}$, Leslie Gordineer ${ }^{59}$, Jef Williamson ${ }^{59}$, Pradeep Garg ${ }^{59}$, Franklin Watkins ${ }^{59}$, Brian Ott ${ }^{60}$, Henry Querfurth ${ }^{60}$, Geofrey Tremont ${ }^{60}$, Stephen Salloway ${ }^{61}$, Paul Malloy ${ }^{61}$, Stephen Correia ${ }^{61}$, Howard Rosen ${ }^{62}$, Bruce Miller ${ }^{62}$, David Perry ${ }^{62}$, Jacobo Mintzer ${ }^{63}$, Kenneth Spicer ${ }^{63}$, David Bachman ${ }^{63}$, Nunzio Pomara ${ }^{64}$, Raymundo Hernando ${ }^{65}$, Antero Sarrael ${ }^{64}$, Norman Relkin ${ }^{65}$, Gloria Chaing ${ }^{65}$, Michael Lin ${ }^{65}$, Lisa Ravdin ${ }^{65}$, Amanda Smith ${ }^{66}$, Balebail Ashok Raj ${ }^{66}$ \&amp; Kristin Fargher ${ }^{66}$.
${ }^{2}$ Magnetic Resonance Unit at the VA Medical Center and Radiology, Medicine, Psychiatry and Neurology, University of California, San Francisco, USA. ${ }^{3}$ San Diego School of Medicine, University of California, California, USA. ${ }^{4}$ Mayo Clinic, Minnesota, USA. ${ }^{5}$ Mayo Clinic, Rochester, USA. ${ }^{6}$ University of California, Berkeley, USA. ${ }^{7}$ University of Pennsylvania, Pennsylvania, USA. ${ }^{8}$ University of Southern California, California, USA. ${ }^{9}$ University of California, Davis, California, USA. ${ }^{10}$ MPH Brigham and Women's Hospital/Harvard Medical School, Massachusetts, USA. ${ }^{11}$ Indiana University, Indiana, USA. ${ }^{12}$ Washington University St. Louis, Missouri, USA. ${ }^{13}$ Oregon Health and Science University, Oregon, USA. ${ }^{14}$ University of California-San Diego, California, USA. ${ }^{15}$ University of Michigan, Michigan, USA. ${ }^{16}$ Baylor College of Medicine, Houston, State of Texas, USA. ${ }^{17}$ Columbia University Medical Center, South Carolina, USA. ${ }^{18}$ University of Alabama - Birmingham, Alabama, USA. ${ }^{19}$ Mount Sinai School of Medicine, New York, USA. ${ }^{20}$ Rush University Medical Center, Rush University, Illinois, USA. ${ }^{21}$ Wien Center, Florida, USA. ${ }^{22}$ Johns Hopkins University, Maryland, USA. ${ }^{23}$ New York University, NY, USA. ${ }^{24}$ Duke University Medical Center, North Carolina, USA. ${ }^{25}$ University of Kentucky, Kentucky, USA. ${ }^{26}$ University of Rochester Medical Center, NY, USA. ${ }^{27}$ University of California, Irvine, California, USA. ${ }^{28}$ University of Texas Southwestern Medical School, Texas, USA. ${ }^{29}$ Emory University, Georgia, USA. ${ }^{30}$ University of Kansas, Medical Center, Kansas, USA. ${ }^{31}$ University of California, Los Angeles, California, USA. ${ }^{32}$ Mayo Clinic, Jacksonville, Jacksonville, USA. ${ }^{33}$ Yale University School of Medicine, Connecticut, USA. ${ }^{34}$ McGill University, Montreal-Jewish General Hospital, Montreal, Canada. ${ }^{35}$ Sunnybrook Health Sciences, Ontario, USA. ${ }^{36}$ U.B.C. Clinic for AD \&amp; Related Disorders, Vancouver, BC, Canada. ${ }^{37}$ Cognitive Neurology St. Joseph's, Ontario, USA. ${ }^{38}$ Cleveland Clinic Lou Ruvo Center for Brain Health, Ohio, USA. ${ }^{39}$ Northwestern University, San Francisco, USA. ${ }^{40}$ Premiere Research Inst (Palm Beach Neurology), west Palm Beach, USA. ${ }^{41}$ Georgetown University Medical Center, Washington DC, USA. ${ }^{42}$ Brigham and Women's Hospital, Massachusetts, USA. ${ }^{43}$ Stanford University, California, USA. ${ }^{44}$ Banner Sun Health Research Institute, Sun City, AZ 85351, USA. ${ }^{45}$ Boston University, Massachusetts, USA. ${ }^{46}$ Howard University, Washington DC, USA. ${ }^{47}$ Case Western Reserve University, Ohio, USA. ${ }^{48}$ University of California, Davis - Sacramento, California, USA. ${ }^{49}$ Neurological Care of CNY, Liverpool, NY 13088, USA. ${ }^{50}$ Parkwood Hospital, Pennsylvania, USA. ${ }^{51}$ University of Wisconsin, Wisconsin, USA. ${ }^{52}$ University of California, Irvine - BIC, USA. ${ }^{53}$ Banner Alzheimer's Institute, Phoenix, AZ 85006, USA. ${ }^{54}$ Dent Neurologic Institute, NY, USA. ${ }^{55}$ Ohio State University, Ohio, USA. ${ }^{56}$ Albany Medical College, NY, USA. ${ }^{57}$ Hartford Hospital, Olin Neuropsychiatry Research Center, Connecticut, USA. ${ }^{58}$ Dartmouth-Hitchcock Medical Center, New Hampshire, USA. ${ }^{59}$ Wake Forest University Health Sciences, North Carolina, USA. ${ }^{60}$ Rhode Island Hospital, state of Rhode Island, Providence, RI 02903, USA. ${ }^{61}$ Butler Hospital, Providence, Rhode Island, USA. ${ }^{62}$ University of California, San Francisco, USA. ${ }^{63}$ Medical University South Carolina, Charleston, SC 29425, USA. ${ }^{64}$ Nathan Kline Institute, Orangeburg, New York, USA. ${ }^{65}$ Cornell University, Ithaca, New York, USA. ${ }^{66}$ USF Health Byrd Alzheimer's Institute, University of South Florida, Tampa, FL 33613, USA.</p>
<h1>Acknowledgements</h1>
<p>This work was supported by National Science Engineering Research Council (NSERC), Canadian Institutes of Health Research (CIHR), Michael Smith Foundation for Health Research (MSFHR), Brain Canada, Genome BC and the Pacific Alzheimer Research Foundation (PARF). Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer's Association; Alzheimer's</p>
<p>Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research \&amp; Development, LLC.; Johnson \&amp; Johnson Pharmaceutical Research \&amp; Development LLC.; Lumosity; Lundbeck; Merck \&amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www. fnih. org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer's Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.</p>
<h1>Author contributions statement</h1>
<p>Donghuan Lu and Weiguang Ding built the deep neural network. Donghuan Lu and Karteek Popuri processed the neuroimage data. Rakesh Balachander and Mirza Faisal Beg designed the experiments and interpreted the results. All authors reviewed the manuscript.</p>
<h2>Additional information</h2>
<p>Competing Interest: The authors declare that they have no competing interest.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8592 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8592</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8592</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-275211830</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.00759v3.pdf" target="_blank">Enhancing Transformers for Generalizable First-Order Logical Entailment</a></p>
                <p><strong>Paper Abstract:</strong> Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning. This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it. Transformers'capability of first-order reasoning is further captured by whether they can conduct first-order logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries. We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability. Results on our comprehensive dataset showed that transformers \textit{outperform} previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on their reasoning capability. Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices. Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8592.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8592.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer Encoder with Guided Attention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A logic-aware transformer encoder that adds two inductive biases — LogiRPE (logic-aware relative positional biases) and Free-Variable Pooling — to improve generalizable first-order logical entailment on KG query answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TEGA (Transformer Encoder with Guided Attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer built and evaluated in this paper. Key components: LogiRPE (trainable logical bias tensors indexed by token types and relative distance) integrated into attention, and Free-Variable Pooling (max-pooling over final hidden states of free-variable tokens). Models use a 3-layer transformer configuration with embedding size 400 (trained from scratch on linearized EFO queries), and entities are predicted by ranking via embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge Graph Query Answering (Existential First-Order logical entailment benchmark introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Answering EFO queries (DNF of conjunctions/disjunctions over one-hop atomics and negation) over knowledge graphs where KG edges are materialized into model parameters; evaluation includes in-distribution (seen) and two OOD dimensions: concept shift (knowledge OOD) and covariate shift (unseen query types). Metric: Mean Reciprocal Rank (MRR) reported separately for ID (Q)/OOD (Q) and ID (K)/OOD (K).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Introduce logic-aware attention (LogiRPE) that adds key/value bias vectors indexed by token types and relative distance; replace standard [CLS]-style pooling with Free-Variable Pooling to aggregate answer-variable representations. Trained end-to-end on KG query answering data; compared to vanilla transformers and prior KGQA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Substantial improvements vs transformer baselines across three KGs (MRR percentages): FB15k — ID(Q)=55.0, OOD(Q)=34.1, ID(K)=38.8, OOD(K)=22.4; FB15k-237 — ID(Q)=64.3, OOD(Q)=20.1, ID(K)=42.6, OOD(K)=16.0; NELL995 — ID(Q)=75.0, OOD(Q)=19.2, ID(K)=56.6, OOD(K)=13.7 (values taken from Table 7 of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to the best transformer baseline (Transformer + Relative PE: FB15k ID(Q)=48.1, OOD(Q)=32.2, ID(K)=35.4, OOD(K)=21.5), TEGA improves ID(Q) by ~6.9–11.5 MRR points and gives consistent gains in both knowledge and query-type generalization; ablations show both LogiRPE and Free-Variable Pooling are individually effective (removing LogiRPE or Free-Variable Pooling reduces performance). Also outperforms prior task-specific KGQA methods evaluated in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset/setting limitation: evaluation restricted to EFO queries with a single free variable. The paper reports that previously proposed inductive biases (e.g., adjacency masking, directed distance encoding) are ineffective or harmful under RPE and that some KGE styles (e.g., TransE) underperform compared to ComplEx/DistMult or learned embeddings. No claims about performance on multi-free-variable queries or open-ended natural language problems.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Logic-aware attention and pooling tailored to logical token types substantially improve transformer reasoning on EFO entailment; relative positional encodings (RPE) are preferred for robust generalization to unseen query permutations and unseen query types; prior inductive biases designed for absolute positional encodings may mismatch RPE and fail. Designing inductive biases that align with positional encoding semantics is crucial for improving transformer logical-entailment performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Transformers for Generalizable First-Order Logical Entailment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8592.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8592.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer + Relative PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer Encoder with Relative Positional Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoder-only transformer using relative positional encoding (RPE) evaluated from-scratch on KG query answering; shown to achieve strong performance and generalizability for first-order logical entailment tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder (encoder-only) with Relative Positional Encoding</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard encoder-only transformer (three layers in experiments, embedding size 400) trained on tokenized linearized EFO queries where KG knowledge is parameterized in weights and outputs are produced by ranking entity embeddings; uses relative positional encodings (RPE) such as Shaw-style relative position representations or rotary embeddings variants assessed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge Graph Query Answering (EFO logical entailment benchmark developed in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Answering existential FO queries over knowledge graphs with evaluation under concept shift (unobserved KG facts) and covariate shift (unseen, compositionally crafted query types); measured with MRR across ID/OOD splits.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Evaluate positional encoding variants (Absolute PE, Disentangled PE, Rotary PE, Relative PE), test different query syntaxes (Lisp-like vs EFO), token embedding sources (pretrained KGEs vs learned embeddings), and architecture inductive biases (adjacency masking, directed distance encoding).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline transformer with Relative PE achieved strong results (examples from paper): FB15k (overall/broad metrics shown) — baseline figures used in comparisons: ID(Q)=48.1, OOD(Q)=32.2, ID(K)=35.4, OOD(K)=21.5 (these baseline values are reported as transformer + Relative PE in Table 7). In broader comparisons (Table 2), transformers with RPE generally outperform prior KGQA methods.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Transformer + Relative PE outperforms traditional KGQA methods in the benchmark (examples from Table 2: BetaE~26.9 MRR, ConE~35.5, CQD~33.1, LMPNN~32.5, SQE-LSTM~39.9 vs Transformer variants often 48–54 MRR depending on dataset and PE). Under RPE, many previously helpful architectural inductive biases (developed for absolute PE) do not yield gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Although RPE improves permutation robustness and generalizability, vanilla transformers still have limited expressiveness for logical relationships unless augmented (e.g., TEGA). Some architecture-level inductive biases (adjacency masking) harm performance under RPE by restricting attention and message passing; absolute PE models suffer substantial drops under permutation reversals and show worse OOD generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Relative PE is superior for capturing structural relationships in linearized logical queries and yields stronger OOD generalizability (query-type and knowledge). However, to capture token-type specific logical relations (e.g., between relation tokens, negation, parentheses), augmenting RPE with logic-aware biases (LogiRPE) yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Transformers for Generalizable First-Order Logical Entailment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8592.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8592.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer variants (APE / DPE / RoPE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer encoder variants with Absolute, Disentangled, and Rotary Positional Encodings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Several positional encoding strategies for transformers (Absolute PE, Disentangled PE, Rotary PE) were evaluated; they differ in permutation/structural robustness and interact differently with architecture-level inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer encoder variants (Absolute PE, Disentangled PE, Rotary PE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformers with different positional encoding schemes: Absolute (sinusoidal APE), Disentangled PE (DeBERTa-style), and Rotary PE (RoPE). Experiments used 3-layer transformers with embedding size 400 trained on EFO queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Knowledge Graph Query Answering (EFO logical entailment benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same EFO KGQA benchmark evaluating FO entailment generalization under knowledge and query-type distribution shifts using MRR.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Direct comparison of positional encodings while keeping other model hyperparameters fixed; also evaluated how previously proposed inductive biases (adjacency masking, directed distance encoding) interact with each PE scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported example MRRs on FB15k (from Table 2): Absolute PE ~46.9 MRR, Disentangled PE ~51.7 MRR, Rotary PE ~50.1 MRR (numbers are reported in the paper's comparative table). Disentangled and Rotary outperform Absolute in some settings but RPE (relative PE) overall gives best OOD generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>All transformer PE variants outperform many previous KGQA-specific baselines (e.g., BetaE, ConE) in the benchmark. However, relative PE yields the best generalizability; inductive biases that helped APE (e.g., adjacency masking) do not translate to RPE.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Absolute PE transformers are sensitive to permutations and reversed query orders causing performance drops; inductive biases tuned for APE may degrade performance under DPE/RoPE/RPE. The paper cautions that prior architectural choices may mismatch newer PE schemes.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Choice of positional encoding critically affects transformers' ability to capture logical structure: RPE provides superior structural/generalization properties, while some proposed inductive biases must be redesigned to be compatible with RPE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Transformers for Generalizable First-Order Logical Entailment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8592.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8592.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Large Language Models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large pre-trained language models (various cited studies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work as having demonstrated enhanced logical reasoning abilities on certain tasks in prior studies, but not evaluated within this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large Language Models (various — cited studies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced as general-purpose pre-trained transformer LMs (citations in paper include works reporting enhanced logical reasoning in LLMs and use of LLMs for theorem proving and logical tasks); no concrete model configuration or sizes are evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Various logical reasoning benchmarks (referenced — e.g., theorem proving, natural language inference, Logicbench cited)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Prior work has evaluated LLMs on arithmetic reasoning, symbolic first-order logic, theorem proving, natural-language logical inference, and dedicated benchmarks (Logicbench, LogicNLI etc.); the present paper only cites these findings in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Related methods in the literature include chain-of-thought, symbolic/neuro-symbolic augmentation, and combining LLMs with symbolic solvers; this paper does not apply those methods experimentally to LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No experimental results reported here; the paper notes elsewhere that measuring reasoning beyond memorization requires OOD tests (citing Saparov et al., 2023) and that LLM reasoning can be fragile under noisy observations (cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Authors acknowledge growing evidence that LLMs can perform logical reasoning, but emphasize that their study focuses on parameterized-knowledge transformers for formal FO entailment (KGQA) with controlled OOD evaluation; they suggest principled OOD setups are needed to distinguish genuine reasoning from distributional fitting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Transformers for Generalizable First-Order Logical Entailment', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models <em>(Rating: 2)</em></li>
                <li>Testing the general deductive reasoning capacity of large language models using ood examples <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 1)</em></li>
                <li>CLR-fact: Evaluating the complex logical reasoning capability of large language models over factual knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8592",
    "paper_id": "paper-275211830",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "TEGA",
            "name_full": "Transformer Encoder with Guided Attention",
            "brief_description": "A logic-aware transformer encoder that adds two inductive biases — LogiRPE (logic-aware relative positional biases) and Free-Variable Pooling — to improve generalizable first-order logical entailment on KG query answering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "TEGA (Transformer Encoder with Guided Attention)",
            "model_description": "Encoder-only transformer built and evaluated in this paper. Key components: LogiRPE (trainable logical bias tensors indexed by token types and relative distance) integrated into attention, and Free-Variable Pooling (max-pooling over final hidden states of free-variable tokens). Models use a 3-layer transformer configuration with embedding size 400 (trained from scratch on linearized EFO queries), and entities are predicted by ranking via embedding similarity.",
            "model_size": null,
            "reasoning_task_name": "Knowledge Graph Query Answering (Existential First-Order logical entailment benchmark introduced in this paper)",
            "reasoning_task_description": "Answering EFO queries (DNF of conjunctions/disjunctions over one-hop atomics and negation) over knowledge graphs where KG edges are materialized into model parameters; evaluation includes in-distribution (seen) and two OOD dimensions: concept shift (knowledge OOD) and covariate shift (unseen query types). Metric: Mean Reciprocal Rank (MRR) reported separately for ID (Q)/OOD (Q) and ID (K)/OOD (K).",
            "method_or_approach": "Introduce logic-aware attention (LogiRPE) that adds key/value bias vectors indexed by token types and relative distance; replace standard [CLS]-style pooling with Free-Variable Pooling to aggregate answer-variable representations. Trained end-to-end on KG query answering data; compared to vanilla transformers and prior KGQA methods.",
            "performance": "Substantial improvements vs transformer baselines across three KGs (MRR percentages): FB15k — ID(Q)=55.0, OOD(Q)=34.1, ID(K)=38.8, OOD(K)=22.4; FB15k-237 — ID(Q)=64.3, OOD(Q)=20.1, ID(K)=42.6, OOD(K)=16.0; NELL995 — ID(Q)=75.0, OOD(Q)=19.2, ID(K)=56.6, OOD(K)=13.7 (values taken from Table 7 of the paper).",
            "baseline_comparison": "Compared to the best transformer baseline (Transformer + Relative PE: FB15k ID(Q)=48.1, OOD(Q)=32.2, ID(K)=35.4, OOD(K)=21.5), TEGA improves ID(Q) by ~6.9–11.5 MRR points and gives consistent gains in both knowledge and query-type generalization; ablations show both LogiRPE and Free-Variable Pooling are individually effective (removing LogiRPE or Free-Variable Pooling reduces performance). Also outperforms prior task-specific KGQA methods evaluated in the benchmark.",
            "limitations_or_failures": "Dataset/setting limitation: evaluation restricted to EFO queries with a single free variable. The paper reports that previously proposed inductive biases (e.g., adjacency masking, directed distance encoding) are ineffective or harmful under RPE and that some KGE styles (e.g., TransE) underperform compared to ComplEx/DistMult or learned embeddings. No claims about performance on multi-free-variable queries or open-ended natural language problems.",
            "insights_or_conclusions": "Logic-aware attention and pooling tailored to logical token types substantially improve transformer reasoning on EFO entailment; relative positional encodings (RPE) are preferred for robust generalization to unseen query permutations and unseen query types; prior inductive biases designed for absolute positional encodings may mismatch RPE and fail. Designing inductive biases that align with positional encoding semantics is crucial for improving transformer logical-entailment performance.",
            "uuid": "e8592.0",
            "source_info": {
                "paper_title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Transformer + Relative PE",
            "name_full": "Transformer Encoder with Relative Positional Encoding",
            "brief_description": "Encoder-only transformer using relative positional encoding (RPE) evaluated from-scratch on KG query answering; shown to achieve strong performance and generalizability for first-order logical entailment tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer encoder (encoder-only) with Relative Positional Encoding",
            "model_description": "Standard encoder-only transformer (three layers in experiments, embedding size 400) trained on tokenized linearized EFO queries where KG knowledge is parameterized in weights and outputs are produced by ranking entity embeddings; uses relative positional encodings (RPE) such as Shaw-style relative position representations or rotary embeddings variants assessed.",
            "model_size": null,
            "reasoning_task_name": "Knowledge Graph Query Answering (EFO logical entailment benchmark developed in paper)",
            "reasoning_task_description": "Answering existential FO queries over knowledge graphs with evaluation under concept shift (unobserved KG facts) and covariate shift (unseen, compositionally crafted query types); measured with MRR across ID/OOD splits.",
            "method_or_approach": "Evaluate positional encoding variants (Absolute PE, Disentangled PE, Rotary PE, Relative PE), test different query syntaxes (Lisp-like vs EFO), token embedding sources (pretrained KGEs vs learned embeddings), and architecture inductive biases (adjacency masking, directed distance encoding).",
            "performance": "Baseline transformer with Relative PE achieved strong results (examples from paper): FB15k (overall/broad metrics shown) — baseline figures used in comparisons: ID(Q)=48.1, OOD(Q)=32.2, ID(K)=35.4, OOD(K)=21.5 (these baseline values are reported as transformer + Relative PE in Table 7). In broader comparisons (Table 2), transformers with RPE generally outperform prior KGQA methods.",
            "baseline_comparison": "Transformer + Relative PE outperforms traditional KGQA methods in the benchmark (examples from Table 2: BetaE~26.9 MRR, ConE~35.5, CQD~33.1, LMPNN~32.5, SQE-LSTM~39.9 vs Transformer variants often 48–54 MRR depending on dataset and PE). Under RPE, many previously helpful architectural inductive biases (developed for absolute PE) do not yield gains.",
            "limitations_or_failures": "Although RPE improves permutation robustness and generalizability, vanilla transformers still have limited expressiveness for logical relationships unless augmented (e.g., TEGA). Some architecture-level inductive biases (adjacency masking) harm performance under RPE by restricting attention and message passing; absolute PE models suffer substantial drops under permutation reversals and show worse OOD generalizability.",
            "insights_or_conclusions": "Relative PE is superior for capturing structural relationships in linearized logical queries and yields stronger OOD generalizability (query-type and knowledge). However, to capture token-type specific logical relations (e.g., between relation tokens, negation, parentheses), augmenting RPE with logic-aware biases (LogiRPE) yields further gains.",
            "uuid": "e8592.1",
            "source_info": {
                "paper_title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Transformer variants (APE / DPE / RoPE)",
            "name_full": "Transformer encoder variants with Absolute, Disentangled, and Rotary Positional Encodings",
            "brief_description": "Several positional encoding strategies for transformers (Absolute PE, Disentangled PE, Rotary PE) were evaluated; they differ in permutation/structural robustness and interact differently with architecture-level inductive biases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Transformer encoder variants (Absolute PE, Disentangled PE, Rotary PE)",
            "model_description": "Encoder-only transformers with different positional encoding schemes: Absolute (sinusoidal APE), Disentangled PE (DeBERTa-style), and Rotary PE (RoPE). Experiments used 3-layer transformers with embedding size 400 trained on EFO queries.",
            "model_size": null,
            "reasoning_task_name": "Knowledge Graph Query Answering (EFO logical entailment benchmark)",
            "reasoning_task_description": "Same EFO KGQA benchmark evaluating FO entailment generalization under knowledge and query-type distribution shifts using MRR.",
            "method_or_approach": "Direct comparison of positional encodings while keeping other model hyperparameters fixed; also evaluated how previously proposed inductive biases (adjacency masking, directed distance encoding) interact with each PE scheme.",
            "performance": "Reported example MRRs on FB15k (from Table 2): Absolute PE ~46.9 MRR, Disentangled PE ~51.7 MRR, Rotary PE ~50.1 MRR (numbers are reported in the paper's comparative table). Disentangled and Rotary outperform Absolute in some settings but RPE (relative PE) overall gives best OOD generalizability.",
            "baseline_comparison": "All transformer PE variants outperform many previous KGQA-specific baselines (e.g., BetaE, ConE) in the benchmark. However, relative PE yields the best generalizability; inductive biases that helped APE (e.g., adjacency masking) do not translate to RPE.",
            "limitations_or_failures": "Absolute PE transformers are sensitive to permutations and reversed query orders causing performance drops; inductive biases tuned for APE may degrade performance under DPE/RoPE/RPE. The paper cautions that prior architectural choices may mismatch newer PE schemes.",
            "insights_or_conclusions": "Choice of positional encoding critically affects transformers' ability to capture logical structure: RPE provides superior structural/generalization properties, while some proposed inductive biases must be redesigned to be compatible with RPE.",
            "uuid": "e8592.2",
            "source_info": {
                "paper_title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Large Language Models (LLMs)",
            "name_full": "Large pre-trained language models (various cited studies)",
            "brief_description": "Mentioned in related work as having demonstrated enhanced logical reasoning abilities on certain tasks in prior studies, but not evaluated within this paper's experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Large Language Models (various — cited studies)",
            "model_description": "Referenced as general-purpose pre-trained transformer LMs (citations in paper include works reporting enhanced logical reasoning in LLMs and use of LLMs for theorem proving and logical tasks); no concrete model configuration or sizes are evaluated in this paper.",
            "model_size": null,
            "reasoning_task_name": "Various logical reasoning benchmarks (referenced — e.g., theorem proving, natural language inference, Logicbench cited)",
            "reasoning_task_description": "Prior work has evaluated LLMs on arithmetic reasoning, symbolic first-order logic, theorem proving, natural-language logical inference, and dedicated benchmarks (Logicbench, LogicNLI etc.); the present paper only cites these findings in related work.",
            "method_or_approach": "Related methods in the literature include chain-of-thought, symbolic/neuro-symbolic augmentation, and combining LLMs with symbolic solvers; this paper does not apply those methods experimentally to LLMs.",
            "performance": null,
            "baseline_comparison": null,
            "limitations_or_failures": "No experimental results reported here; the paper notes elsewhere that measuring reasoning beyond memorization requires OOD tests (citing Saparov et al., 2023) and that LLM reasoning can be fragile under noisy observations (cited works).",
            "insights_or_conclusions": "Authors acknowledge growing evidence that LLMs can perform logical reasoning, but emphasize that their study focuses on parameterized-knowledge transformers for formal FO entailment (KGQA) with controlled OOD evaluation; they suggest principled OOD setups are needed to distinguish genuine reasoning from distributional fitting.",
            "uuid": "e8592.3",
            "source_info": {
                "paper_title": "Enhancing Transformers for Generalizable First-Order Logical Entailment",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logicbench: Towards systematic evaluation of logical reasoning ability of large language models",
            "rating": 2,
            "sanitized_title": "logicbench_towards_systematic_evaluation_of_logical_reasoning_ability_of_large_language_models"
        },
        {
            "paper_title": "Testing the general deductive reasoning capacity of large language models using ood examples",
            "rating": 2,
            "sanitized_title": "testing_the_general_deductive_reasoning_capacity_of_large_language_models_using_ood_examples"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 1,
            "sanitized_title": "generative_language_modeling_for_automated_theorem_proving"
        },
        {
            "paper_title": "CLR-fact: Evaluating the complex logical reasoning capability of large language models over factual knowledge",
            "rating": 1,
            "sanitized_title": "clrfact_evaluating_the_complex_logical_reasoning_capability_of_large_language_models_over_factual_knowledge"
        }
    ],
    "cost": 0.016387,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Transformers for Generalizable First-Order Logical Entailment
10 Jul 2025</p>
<p>Tianshi Zheng tzhengad@connect.ust.hk 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Jiazheng Wang 
Department of Computer Science and Engineering
Beihang University
BeijingChina</p>
<p>Zihao Wang 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Jiaxin Bai 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Hang Yin 
Department of Mathematical Sciences
Tsinghua University
BeijingChina</p>
<p>Zheye Deng 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Yangqiu Song 
Department of Computer Science and Engineering
HKUST
Hong Kong SARChina</p>
<p>Jianxin Li 
Department of Computer Science and Engineering
Beihang University
BeijingChina</p>
<p>Enhancing Transformers for Generalizable First-Order Logical Entailment
10 Jul 2025D9D5FD1136B737BFD89B12334039EA7EarXiv:2501.00759v3[cs.CL]
Transformers, as the fundamental deep learning architecture, have demonstrated great capability in reasoning.This paper studies the generalizable first-order logical reasoning ability of transformers with their parameterized knowledge and how to improve it.Transformers' capability of first-order reasoning is further captured by whether they can conduct firstorder logical entailment, which is quantitatively measured by their performance in answering knowledge graph queries.We establish the connections between (1) two types of distribution shifts studied in out-of-distribution generalization and (2) unseen knowledge and query settings discussed in the task of knowledge graph query answering, which makes it possible to characterize the fine-grained generalizability.Results on our comprehensive dataset showed that transformers outperform previous methods designed particularly for this task and provided detailed empirical evidence about the impact of the input query syntax, token embedding, and transformer architectures on their reasoning capability.Interestingly, our results revealed the mismatch of positional encoding and other design choices of transformer architectures in previous practices.Motivated by this, we propose TEGA, a logic-aware architecture that significantly improves the performance in generalizable first-order logical entailment.</p>
<p>Introduction</p>
<p>As a fundamental architecture in deep learning, transformers possess strong reasoning capabilities on various tasks, including arithmetic reasoning (Saxton et al., 2019;Hendrycks et al., 2021), symbolic reasoning for first-order logic rules (Dehghani et al., 2019;Lample and Charton, 2019), settheoretic operations (Barrett et al., 2018), and theorem proving (Polu and Sutskever, 2020).Besides, transformers have also demonstrated proficiency in * Equal Contribution logical reasoning over natural language (Han et al., 2022;Tian et al., 2021).To distinguish whether transformers conduct the reasoning rather than fitting the data distribution, recent studies in natural language reasoning further measure the capabilities of transformers for out-of-demonstration samples (Saparov et al., 2023).However, their discussions are limited in two aspects: (1) they only concern the reasoning ability of transformers with in-context knowledge, and (2) they fail to elicit the connection of out-of-demonstration samples with two distribution shifts (Moreno-Torres et al., 2012) for the study of out-of-distribution generalization.</p>
<p>In this paper, we take a step further to understand the generalizable reasoning capability of transformers.What sets us apart from (Saparov et al., 2023) is that (1) our investigation covers the transformer reasoning with parameterized knowledge, which suits many scenarios when the related knowledge is not explicit for users and only questions are given, and (2) we further realize two distribution shifts (Moreno-Torres et al., 2012) in our first order reasoning tasks, which boils down to the process of first-order logical entailment, where we verify whether a first-order sentence (conclusion) is the logical consequence of a set of known first-order sentences (premises) or not (Marker, 2002).</p>
<p>Specifically, we study the first-order logical entailment with Knowledge Graphs (KG), leading to the widely discussed task of knowledge graph query answering.In our setting, knowledge in KGs is parameterized in models and forms implicit premises.( §2) The process of logical entailment occurs when identifying the answer to a logical query.Therefore, the two distribution shifts -concept shift and covariant shift -are naturally credited to the unobserved knowledge and the unseen query types in our experimental settings.( §3) For better evaluation, we build our own benchmark with fifty-five types of logical queries over three knowledge graphs (FB15k (Bordes et al., 2013), FB15k-237 (Toutanova and Chen, 2015), and NELL995 (Carlson et al., 2010)), including all existing features discussed in recent literature (Yin et al., 2023b).Our benchmark results suggest that transformers can handle first-order logical entailment even compared to the methods particularly designed for this task.( §4)</p>
<p>We conducted extensive experiments to characterize the impact of three main aspects of solving logical entailment on reasoning capability: the query syntax, the learning of token embedding, and the transformer architecture.Our results provide strong empirical clues on improving transformers' reasoning capability by choosing the proper formal language syntax, positional encodings, semantics in pre-trained KG embeddings, and inductive biases in transformer architectures.( §5) Interestingly, our results demonstrated the superior performance and generalizability of relative positional encoding (RPE) over traditional absolute positional encoding (APE).However, previous studies have only proposed inductive biases within the APE setting, which we prove ineffective in the RPE setting.To fill this gap, we propose TEGA (Transformer Encoder with Guided Attention), a novel modeling methodology that facilitates effective reasoning with logic-aware guidance in selfattention.Our study shows that TEGA substantially improves the performance and generalizability upon transformers under the RPE setting.( §6)</p>
<p>Overall, this work makes three key technical contributions:</p>
<ol>
<li>Benchmark.We conducted comprehensive benchmarking of transformer architectures against baseline models for knowledge graph query answering, using a new dataset that incorporates two distribution shifts.</li>
</ol>
<p>2.</p>
<p>Exploration.We systematically investigated critical design choices throughout the modeling process, including query syntax and permutation, token embedding, and transformer architecture designs.</p>
<p>3.</p>
<p>Method.We introduced TEGA, a novel architecture that improves logical entailment performance and generalizability through two inductive biases.We validated its effectiveness through extensive experiments and ablations.</p>
<p>Our code and data can be found at https://github.com/HKUST-KnowComp/TEGA.</p>
<p>Preliminaries</p>
<p>This section briefly introduces First-Order (FO) logical entailment and its restricted application in knowledge graph query answering.By revealing the connection, we show that the key ability to address complex queries is the first-order logical entailment.For simplicity, our introduction only restricts to a finite entity set E and a finite binary relation set R. No functions are mentioned.Detailed presentation of first-order logic can be found in model theory literature (Marker, 2002).</p>
<p>First-order Logical Entailment</p>
<p>Definition 1 (First-order sentence).The set of firstorder formulae is the minimal set F such that 1. p(s, o) ∈ F , where p is the relation, s, o can be either an entity in E or a variable.
2. If s ∈ F , then ¬s ∈ F ; If s, t ∈ F , then s ∧ t ∈ F and s ∨ t ∈ F . 3. If s(x) ∈ F and x is a variable that is not qua- ntified, then ∀x.s(x) ∈ F and ∃x.s(x) ∈ F .
We say a variable x in a formula is quantified if there is a quantifier (∃x or ∀x).Otherwise, we say a variable is free.A sentence is a formula without free variables.</p>
<p>Next, we introduce first-order logical entailment, the central concept of first-order reasoning.In general, the entailment is a process of verifying the conlusion (one sentence to verify) given the knowledge (a set of given premises).Notably, knowledge, conclusion and verification are the three key components of the definition.For first-order logical entailment, the knowledge and conclusion are restricted as first-order sentences, and the verification process is subject to the first-order logical calculus.A set of FO sentences {p i , i = 1, ..., n} entails a FO conclusion s is denoted as:
{p i , i = 1, ..., n} knowledge or premises |= s conclusion (1)
After verification, its truth value suggests whether s is the logical consequence of the premises.</p>
<p>Knowledge Graph Query Answering</p>
<p>Knowledge graph query answering is an important application of FO logical entailment.Its importance comes from both the importance of knowledge graphs and the queries in database systems.</p>
<p>Here, we connect the task of KG query answering and FO entailment from the aspects of knowledge, conclusion, and verification.</p>
<p>Knowledge.Given an entity set E and relation set R, a knowledge graph is defined as a set of triplets G = {(h i , r i , t i )}, where h i , t i ∈ E are entities and r i ∈ R are relations.Each triple (h, r, t) in KG can be regarded as the simplest form of firstorder sentence r(h, t).Thus, the knowledge graph G provided the premises.</p>
<p>Answers to the query as multiple conclusions to verify.Existing research for KG query answering usually focuses on the restricted families of FO queries, specifically, the Existential First-Order (EFO) queries.An EFO query is represented in Disjunctive Normal Form (DNF) as a disjunction of one or multiple conjunctions:
q[V ? ] = V ? .∃V 1 , V 2 , ..., V n : c 1 ∨ c 2 ∨ ... ∨ c m (2)
In equation 2, V ? is the free variable, V 1 , V 2 , ..., V n are existential variables, while c 1 , c 2 , ..., c m are conjunctions of one-hop atomic formulas or their negation, defined as
c i = α i1 ∧ α i2 ∧ ... ∧ α ik i and α ij = r(h, t) or ¬r(h, t)
, where r ∈ R, h and t are either constant entities e ∈ E or variables.We can see that an EFO query is not a FO sentence because it contains a free variable to query.The goal of answering a query is to find all possible entities a such that G |= q[a/V ?], where q[a/V ?] is a FO sentence where the free variable V ? is substituted by entity a.To summarize, the answer set
A(G, q) = {a ∈ E : G |= q[a/V ? ] = True}. (3)
Next, we show that the key condition to be verified, i.e., G |= q[a/V ?] = True, relies on the knowledge graph G.By FO logical calculus, evaluating G |= q[a/V ?] is equivalent to:
V 1 ,...,Vn∈E m i=1 (G |= αi1) V ? =a ∧ ... ∧ (G |= α ik i ) V ? =a . (4)
We could see that whether G |= q[a/V ?] is True essentially depends on many times of FO logical entailment on atomic formulae G |= α ij , where each of these depends on whether the atomic r(h, t) associated to α ij is contained in G (or not in G for the negation case).</p>
<p>Distribution Shifts in KGQA</p>
<p>This section introduces the formulation of distribution shifts in KG query answering tasks.Such tasks are connected to first-order reasoning due to their close connection with logical entailment.</p>
<p>By clearly presenting their connection with distribution shifts studied in statistical ML community (Moreno-Torres et al., 2012), they are then able to measure the generalizable first-order reasoning capability of deep learning models, in particular transformers, with parameterized knowledge.Problem formulation.A dataset for KG query answering is denoted as a set of samples {(x i , y i )} drawn from the joint distribution P (X, Y ).Here, X represents the random variable corresponding to an EFO query, with x i as individual samples, and Y represents the random variable for the answer set, with y i as individual samples.</p>
<p>This paper studies transformers, where the input X is tokenized and fed into the transformer architecture, and the output Y is predicted with a classifier, where all entities in the KG are ranked by embedding similarities.In both the training and testing phases, the knowledge graph G is not explicitly accessed by the model.In this way, the necessary condition for transformers to correctly answer the queries is that the knowledge in KG is materialized into their parameters.To examine whether transformers possess the generalizable first-order reasoning capability, we consider two classic types of out-of-distribution shifts: concept shift and covariant shift.They are connected to unobserved knowledge and unseen query types.</p>
<p>Two Types of Distribution Shifts</p>
<p>Out-Of-Distribution (OOD) generalization is one of the key topics in machine learning research (Vapnik, 1991;Quionero-Candela et al., 2009).It considers the shifts in the joint distribution P (X, Y ) from the training to the testing phases.Equation 5 decomposed the distribution shift by applying conditional probability formula, introducing two basic types of OOD shifts: concept shift refers to the changes in P (Y |X), while covariate shift refers to the changes in P (X).Dataset Operations Features Unseen Types pro.int.uni.neg.exi.mul.cyc.
GQE ✓ ✓ 0 Q2B ✓ ✓ ✓ 4 BetaE ✓ ✓ ✓ ✓ 4 FIT ✓ ✓ ✓ ✓ ✓ ✓ ✓ 10 SQE ✓ ✓ ✓ ✓ 29 Ours ✓ ✓ ✓ ✓ ✓ ✓ ✓ 32
Table 1: Comparison between different benchmarks on their supported logical operations, query features, and the number of unseen query types.</p>
<p>testing stage, they are evaluated on answers from the full graph, denoted as A(G, q).This setup introduces a concept shift in the conditional distribution P (Y |X) between training and testing because the change of knowledge causes different results of entailment, see Equation (3).Therefore, the set difference A ood = A(G, q) − A(G o , q) contains the entities that can only be derived when the model generalizes from knowledge G o to G. By measuring the performances of models on A ood , we are able to access how a model generalizes under the concept shift caused by unobserved knowledge.This is also termed knowledge generalization or knowledge inference in the literature (Sun et al., 2021).</p>
<p>Covariate Shift by Unseen Query Types We also examine the capability of query answering models to generalize and maintain performance on query types that are unseen in the training stage.</p>
<p>In our testing data, we include new query types that are combinatorially crafted from the training query types, introducing a covariate shift in the distribution of P (X).The models' performances on these unseen types are measured to evaluate their generalizability to covariate shifts in the query type dimension.This is also termed combinatorial generalization or compositional generalization in the literature (Wang et al., 2021;Yin et al., 2023a).</p>
<p>Evaluation Benchmark</p>
<p>To better investigate the performance in generalizable FO reasoning, we build a new benchmark of KG query answering for our experiment.We first introduce the dataset construction process and evaluation settings, and then present our results of transformers in comparison to previous methods.</p>
<p>Dataset Construction</p>
<p>Table 1 compares the feature and statistics of query types selected in our benchmark with previous benchmarks in KG query answering, namely GQE dataset (Hamilton et al., 2019), Q2B dataset (Ren et al., 2020), BetaE dataset (Ren and Leskovec, 2020), FIT dataset (Yin et al., 2023b), and SQE dataset (Bai et al., 2023b).We included the entire set of set operations and query features in our selection of 23 seen query types.Moreover, we compositionally crafted 32 unseen query types that are only used in the testing stage to examine models' OOD generalizability in the query type dimension.For instance1 , suppose a query answering model is trained on query type 2in
[(r1(s1,f))&amp;(!(r2(s2,f)))] and 2p [(r1(s1,e1))&amp;(r2(e1,f))],
we expect it can also handle the unseen query type in2p
[(r1(s1,e1))&amp;(!(r2(s2,e1)))&amp;(r3(e1,e2))&amp; (r4(e2,f))],
which is compositionally designed based on the previous two.All queries are sampled from the following three knowledge graphs: FB15k (Bollacker et al., 2008;Bordes et al., 2013)
, FB15k- Dataset Model Type ID (Q) OOD (Q) All Queries ID (K) OOD (K) ID (K) OOD (K) ID (K) OOD (K) FB15k
BetaE (Ren and Leskovec, 2020 237 (Toutanova and Chen, 2015), and NELL995 (Carlson et al., 2010).The details of all query types and their statistics in our benchmark are provided in Appendix A. We also provide query graphs for graph-augmented methods (Liu et al., 2022;Xu et al., 2023) in our experiments, with detailed definitions in Appendix F.</p>
<p>Evaluation Settings</p>
<p>In accordance with the two types of distribution shifts discussed above, we evaluate the performance of query answering models across the two corresponding dimensions using the mean reciprocal rank (MRR) metric.In the knowledge dimension, we employ the notation ID (K) and OOD (K) to represent the performance on the answer set
A id = A(G o , q) and A ood = A(G, q) − A(G o , q),
respectively.Concurrently, in the query type dimension, we utilize the notation ID (Q) and OOD (Q) to represent the performance on seen query types and unseen query types, respectively.Higher scores in ID (K) and ID (Q) reflect the models' effectiveness in performing logical entailment on in-distribution data, while higher scores in OOD (K) and OOD (Q) demonstrated better generalizability of the models.It is also noteworthy to mention that settings in ID (K) and OOD (K) denote the concept shifts, while those in ID (Q) and OOD (Q) capture the covariate shift.For details regarding the metric calculation, please refer to Appendix G.</p>
<p>General Benchmarking Results</p>
<p>We evaluate transformer encoders2 against five existing methods for KG query answering, with details provided in Appendix B. Meanwhile, we compare four positional encoding settings in transformers: Absolute PE is the default sinusoidal positional encoding introduced in the original transformer (Vaswani et al., 2023) Lisp-Like Syntax (p,(r3),(i,(p,(r1),(s1)),(p,(r2),(s2))))</p>
<p>Entity Relation</p>
<p>Pre-trained Semantics Learned Semantics OOD generalizability in the query type dimension.
"! "" "# #! ! # " ! ## ! #! " # " " ## " # ! $ #" $ ## $ … … … … "! "" "# #! ! # " ! ## ! #! " # " " ## " # ! $ #" $ ## $ … … … … … … … … Answer set ! (Output ") Query # (Input $)
In contrast, RPE and RoPE transformers demonstrates much stronger generalizability, as it can better capture structural information from the distances between tokens and flexibly handle query sequences of unseen lengths.Overall, RPE transformers achieve the best performance and OOD generalizability on first-order logical entailment.</p>
<p>Transformer Architectures and First-order Reasoning Capability</p>
<p>In this section, we further investigate the dependencies between transformers' performance and multiple design choices throughout the modeling process.Figure 2 depicts the three-stage pipeline for deriving the answer set A (output Y ) of query q (input X) using transformers.We present the impact of query syntax in Section 5.1, token embeddings in Section 5.2, and transformer architecture in Section 5.3.We summarize the findings in Section 5.4.</p>
<p>Study on Query Syntax</p>
<p>Logical queries with the same semantics can be represented in different syntaxes with different formal languages.To investigate the impact of difference in formal languages, we conducted experiments on two formal languages: Lisp-like Syntax (Wang et al., 2021) represents query with fully parenthesized nested formula; EFO Syntax (Yin et al., 2023b) represents query with all one-hop atomics connected with conjunctions and disjunctions in parallel.As the Lisp-like syntax has a limited representation scope, we used a subset of our benchmark that includes 13 seen query types and 12 unseen query types that are supported by both languages.The experimental results on the impact of formal language are presented in Table 3.In APE Transformers, there is no difference between the results of the two formal languages.However, with RPE, the EFO syntax achieves much better generalizability in the query type domain.This phenomenon can be explained by the differences in the structural features of the two languages: with a par- allel structure, the distances (i.e., relative positions) between tokens with the same logical relationship are more consistent than those in a nested structure.
Model Syntax ID (Q) OOD (Q) ID (K) OOD (K) ID (K) OOD (K)
As a result, RPE can better learn the logical relationship between tokens and better generalize to unseen query types.Furthermore, even for the same query in EFO Syntax, there can be various permutations of onehop atomics, with the total number of potential permutations growing factorially with the length of the query.Therefore, it is crucial for transformers to robustly handle queries with different permutations.To evaluate this property, we selected five query types (2p, 3p, ip, pi, 2in) and reversed their permutations while maintaining the same semantics in the testing data (see Appendix C).The experimental results on query permutation are presented in Table 4.We observed a significant performance drop after reversing the query in APE transformers.However, for RPE transformers, the performance before and after reversing the permutation remains consistent, demonstrating their robustness in handling queries with modified permutations.</p>
<p>Study on Token Embeddings</p>
<p>Some methods in KG query answering (Arakelyan et al., 2021;Wang et al., 2023b;Xu et al., 2023 et al., 2016) extends this framework into a complex vector space, allowing for more nuanced interentity interactions.In our experiment, all three KG embeddings are frozen during the training stage, in comparison to a baseline with randomly initialized and jointly learned embeddings.
) Dataset KGE ID (Q) OOD (Q) ID (K) OOD (K) ID (K) OOD (K) FB15k Random
The experimental results are presented in Table 5.Two stronger KGEs, ComplEx and DistMult, can effectively improve the performance of transformer.In contrast, the performance of TransE consistently lags behind the baseline across all three datasets.This discrepancy can be attributed to the baseline model's embedding matrix learning process, which can be viewed as implicitly training a link predictor using a transformer architecture (e.g., KG-BERT (Yao et al., 2019)).This modern approach may naturally outperform earlier models like TransE.</p>
<p>Study on Transformer Architecture</p>
<p>Some existing literature (Kotnis et al., 2021;Liu et al., 2022;Xu et al., 2023) has explored the use and design of transformers in KG query answering, in which several inductive biases in transformer architecture were proposed.However, as all these methods focused on different aspects and were evaluated on different benchmarks, a clearer investigation is needed.We implemented two designs from previous works on top of transformers: Adjacency Matrices Masking: an inductive bias proposed in kgTransformer (Liu et al., 2022), which aims to enhance reasoning performance by limiting the range of self-attention to one-hop neighboring nodes in the query graph.Directed Distance Encoding: another inductive bias, introduced in Query2Triple (Xu et al., 2023), that facilitates information aggregation by injecting a directed distance message into the self-attention mechanism.The experimental results on existing transformer designs are presented in Table 6.Both inductive biases effectively improve generalizability in the query type dimension when using APE.In contrast, under RPE, these improvements are not observed, and the use of adjacency matrices masking even results in a significant decrease in performance.This decrease is attributed to the masking in self-attention, which restricts message passing to only nodes within one hop, thereby limiting the model's ability to perform complex logical reasoning.Regarding directed distance encoding, the information about directed distances is mostly contained in the relative positions of the linearized sequence, thus the improvements seen with original APE are not sustained.</p>
<p>Summary of Empirical Findings</p>
<p>In terms of query syntax, we discovered that employing formal language with parallel structure can significantly enhance OOD generalizability in the query type dimension for RPE transformers.Moreover, we observed that transformers with relative PE can robustly handle queries with different permutations.Regarding transformer architecture, we found that the inductive biases proposed in previous approaches are only effective under APE, but fail to improve upon RPE transformers, which we have shown to be the preferred setting.This mismatch between transformer design and positional encoding settings motivated us to design a methodology that effectively boosts the performance and generalizability of RPE transformers in first-order logical entailment.</p>
<p>Transformer Encoder with Guided Attention</p>
<p>One of the main challenges in KG query answering is to perform multi-hop logical reasoning following the one-hop atomics and the logical operations between them.In traditional reasoning tasks, such as language-based reasoning, transformer models perform multi-hop reasoning by procedurally forming "reasoning trees" within their attention mechanisms (Hou et al., 2023;Murty et al., 2023;Wang et al., 2023a).While in our task, it is challenging for transformers to capture the pattern of reasoning trees behind the queries, as the semantics of logical operators are neither explicitly modeled nor pre-trained, but implicitly learned from limited training queries.To navigate this conundrum, we present TEGA (Transformer Encoder with Guided Attention), a modeling methodology that boosts logical reasoning performance by providing logicaware guidance within the self-attention mechanism of transformers.The following sections will introduce the two inductive biases in TEGA and their effectiveness with experimental results.</p>
<p>Logic-aware Relative Positional Encoding</p>
<p>Our analysis so far shows that RPE is the best positional encoding for transformers in our task for its superior performance and generalizability in both knowledge and query type dimensions.However, it still has limited expressiveness in the logical relationship between tokens, as the tokens with the same relative distance can have different logical relationships among queries.To fill this gap, we propose LogiRPE, a logic-aware mechanism to enhance self-attention.Specifically, for a sequence input sequence embeddings
x i ∈ R d , i = 1, ..., n, the enhanced self-attention is computed by zi = n j=1 αij(xjW V + β V ij ), αij = exp eij n k=1 exp e ik (6) eij = xiW Q (xjW K + β K ij ) T √ d . (7)
where W Q , W K , W V ∈ R d×d are the weight matrices for query, key, and value while β V ij , β K ij ∈ R d are two logical bias terms introduced by LogiRPE to capture the missing logical relations.</p>
<p>The key feature of logical bias terms is that they should be able to discriminate different types of tokens.Let the type of the i-th token be t i from a finite type set of size t.In our study, we consider six types of tokens: parenthesis[(,)], entity[s,e,f], relation [r], conjunction[∧], disjunction[∨], and negation [!].Then the bias vector β l ij (l is either K or V ) is indexed from an trainable embedding bank ω l ∈ R t×t×n×d (consider the first three indices).
β l ij = ω l [t i , t j , |i − j|], l ∈ {K, V }. (8)
LogiRPE may also be applicable to broader tasks where tokens with logical semantics can be labeled.</p>
<p>Free-variable Pooling</p>
<p>Most existing transformer-based approaches (Bai et al., 2023b;Xu et al., 2023) utilize the last layer hidden states of the first token(s) as the final query encoding, akin to the use of the [CLS] token in the original BERT paper (Devlin et al., 2019).However, we contend that the reasoning capabilities inherent in self-attention mechanisms should effectively aggregate the query information into free variables that semantically represent the answer sets of queries.Consequently, we adopt Free-Variable Pooling, wherein the final query encoding is derived through max pooling across the last-layer hidden states of these free variables.</p>
<p>Result and Ablation</p>
<p>The experimental results of TEGA and Transformer baselines are presented in Table 7.In all three datasets, TEGA substantially improved the performance and generalizability of knowledge and query type dimensions over baselines.According to our ablation study results in Table 8, both inductive biases proposed in TEGA are effective on their own.At the same time, a stronger improvement in entailment performance can be achieved when applied together.Moreover, we provide an example of attention visualization in Appendix D, demonstrating the self-evident effect of logic-aware attention guidance within our TEGA model.</p>
<p>Related Work</p>
<p>Transformers in Logical Reasoning Transformers have exhibited remarkable performance in various forms of logical reasoning.In natural language inference (NLI) tasks (Bowman et al., 2015;Williams et al., 2018), transformers analyze the logical relationship between a premise and a hypothesis, employing deductive reasoning over statements.Furthermore, transformers have been employed for inductive reasoning within rule-based systems (Clark et al., 2020).Additionally, transformers have been applied to perform abductive reasoning in both natural language (Bhagavatula et al., 2020) and formal language (Bai et al., 2024;Gao et al., 2025) settings.Meanwhile, various techniques have been proposed to enhance logical reasoning in language understanding (Chen, 2023;Pan et al., 2023).Recent studies show that large language models (LLMs) exhibit enhanced logical reasoning abilities (Parmar et al., 2024;Li et al., 2025;Fan et al., 2025), even without intermediate reasoning steps (Zheng et al., 2025a).Logical entailment also plays a key role in hypothesis verification of LLMs, in areas such as commonsense reasoning (Zhao et al., 2024), analogical reasoning (Zheng et al., 2025b), and even scientific discovery (Takagi et al., 2023;Zheng et al., 2025c).</p>
<p>KGQA with Parameterized Knowledge Prior to this work, extensive research has been conducted on KGQA with parameterized knowledge.Regarding modeling approaches, iterative neural query encoders (Ren and Leskovec, 2020;Chen et al., 2022;Tsang et al., 2025) design representations for entity sets and execute logical operators iteratively, following the computational graph.</p>
<p>In addition, neural-symbolic methods (Zhu et al., 2022;Bai et al., 2023c;Yin et al., 2023b) incorporate link predictors and search over the symbolic space.In terms of the knowledge domain, existing benchmarks (Ren et al., 2020;Ren and Leskovec, 2020;Yin et al., 2023a) primarily utilize knowledge graphs containing general factual knowledge, while some studies have focused on commonsense knowledge (Fang et al., 2024) and eventuality knowledge (Bai et al., 2023a).Such queries can be extended to natural language settings with template or LLM-based approaches (Zheng et al., 2024(Zheng et al., , 2025d;;Zong et al., 2025;Bai et al., 2025a).The setting of KGQA with parameterized knowledge has also been extended to the concept of Neural Graph Databases (Ren et al., 2023;Bai et al., 2025b).</p>
<p>Conclusion</p>
<p>This paper investigates the generalizable first-order logical reasoning capabilities of transformers when knowledge is parameterized within model weights.By establishing connections between distribution shifts and logical entailment, we develop a principled framework for evaluating transformer performance on knowledge graph query answering tasks.Our comprehensive benchmark reveals that transformers can effectively perform first-order logical entailment, outperforming methods specifically designed for this task.Through systematic analysis of the entire modeling pipeline, we identify critical design principles that enhance reasoning capabilities.Notably, our findings reveal a mismatch between existing inductive biases and optimal positional encoding strategies, motivating the development of TEGA.This architecture introduces logic-aware self-attention mechanisms that substantially improve both performance and generalizability.Our work demonstrates that careful architectural choices, informed by the logical structure of reasoning tasks, can significantly enhance transformers' logical entailment ability.</p>
<p>Limitations</p>
<p>The scope of complex logical queries was limited to EFO queries with a single free variable, with queries containing multiple free variables reserved for future investigation.</p>
<p>A Dataset Statistics</p>
<p>In this section, we introduce the knowledge graph details and the statistics and query types of our benchmark dataset.</p>
<p>Our benchmark incorporates three widely used knowledge graphs-FB15k, FB15k-237, and NELL995-derived from large-scale KGs to evaluate reasoning capabilities.FB15k and FB15k-237 originate from Freebase, a comprehensive database of structured general knowledge covering domains such as entertainment, sports, and geography, with entities linked by diverse relations (e.g., "born-in," "part-of").NELL995 is derived from the Never-Ending Language Learning (NELL) system, which extracts structured facts from web text, focusing on real-world entities and relations.</p>
<p>Details of the query statistics for our dataset are presented in Table 9.For each knowledge graph, we sampled all one-hop projection (1p) queries from the training graph and sampled twice as many for the other 22 in-distribution query types.For validation and testing queries, we set the quantities to 8000, 5000, and 4000, respectively, following the convention established in KG literature (Ren et al., 2020;Ren and Leskovec, 2020;Bai et al., 2023b  Table 11 and 12 presented the details of query types in our benchmark.All 32 unseen query types are crafted compositionally based on the 23 seen query types.Table 13 presented the subset of 25 lisp-like compatible query types used in our experiment on query syntax.</p>
<p>B Baseline Models</p>
<p>In our benchmarking experiment, we compare transformers with five existing methods that are designed particularly for KG query answering:</p>
<p>• BetaE (Ren and Leskovec, 2020) encodes queries iteratively following the logical operations with Beta distributions.</p>
<p>• ConE (Zhang et al., 2021) represents answer sets as two-dimensional cones to better handle negations.</p>
<p>• CQD (Arakelyan et al., 2021) applies a search-based method that utilizes pre-trained link predictors for inference-time optimization.</p>
<p>• LMPNN (Wang et al., 2023b) conducts one-hop inferences with pre-trained KGE, and uses message passing to aggregate these local results into global output.</p>
<p>• SQE-LSTM (Bai et al., 2023b) encode the linearized complex query as a sequence with LSTM (Hochreiter and Schmidhuber, 1997).</p>
<p>C Query Reversion</p>
<p>The logical formula of five query types before and after reversion is provided in Table 10.
Type Name Before Reversion After Reversion 2p (r1(s1,e1))&amp;(r2(e1,f)) (r2(e1,f))&amp;(r1(s1,e1)) 3p ((r1(s1,e1))&amp;(r2(e1,e2)))&amp;(r3(e2,f)) ((r3(e2,f))&amp;(r2(e1,e2)))&amp;(r1(s1,e1)) ip ((r1(s1,e1))&amp;(r2(s2,e1)))&amp;(r3(e1,f)) ((r3(e1,f))&amp;(r2(s2,e1)))&amp;(r1(s1,e1)) pi ((r1(s1,e1))&amp;(r2(e1,f)))&amp;(r3(s2,f)) ((r3(s2,f))&amp;(r2(e1,f)))&amp;(r1(s1,e1)) 2in (r1(s1,f))&amp;(!(r2(s2,f))) (!(r2(s2,f)))&amp;(r1(s1,f))
Table 10: Details of query reversion procedure in the query permutation experiment.</p>
<p>D Attention Visualization
1p 1 r1(s1,f) 1 2p 2 (r1(s1,e1))&amp;(r2(e1,f)) 2 3p 3 (r1(s1,e1))&amp;(r2(e1,e2))&amp;(r3(e2,f)) 3 2i 1 (r1(s1,f))&amp;(r2(s2,f)) 4 3i 1 (r1(s1,f))&amp;(r2(s2,f))&amp;(r3(s3,f)) 5 ip 2 (r1(s1,e1))&amp;(r2(s2,e1))&amp;(r3(e1,f)) 6 pi 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(s2,f)) 7 2in ✓ 1 (r1(s1,f))&amp;(!(r2(s2,f))) 8 3in ✓ 1 (r1(s1,f))&amp;(r2(s2,f))&amp;(!(r3(s3,f))) 9 inp ✓ 2 (r1(s1,e1))&amp;(!(r2(s2,e1)))&amp;(r3(e1,f)) 10 pin ✓ 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(!(r3(s2,f))) 11 pni ✓ 2 (r1(s1,e1))&amp;(!(r2(e1,f)))&amp;(r3(s2,f)) 12 2u 1 (r1(s1,f))|(r2(s2,f)) 13 up 2 ((r1(s1,e1))|(r2(s2,e1)))&amp;(r3(e1,f)) 14 2m ✓ 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(e1,f)) 15 2nm ✓ ✓ 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(!(r3(e1,f))) 16 3mp ✓ 3 (r1(s1,e1))&amp;(r2(e1,e2))&amp;(r3(e2,f))&amp;(r4(e1,e2)) 17 3pm ✓ 3 (r1(s1,e1))&amp;(r2(e1,e2))&amp;(r3(e2,f))&amp;(r4(e2,f)) 18 im ✓ 2 (r1(s1,e1))&amp;(r2(s2,e1))&amp;(r3(e1,f))&amp;(r4(e1,f)) 19 2il ✓ 1 (r1(s1,f))&amp;(r2(e1,f)) 20 3il ✓ 1 (r1(s1,f))&amp;(r2(s2,f))&amp;(r3(e1,f)) 21 3c ✓ 3 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(s2,e2))&amp;(r4(e2,f))&amp;(r5(e1,e2)) 22 3cm ✓ ✓ 3 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(s2,e2))&amp;(r4(e2,f))&amp;(r5(e1,e2))&amp;(r6(e1,f)))|(r2(s2,e1)))&amp;(r3(e1,e2))&amp;(r4(e2,f)) 31 2pin ✓ 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(s2,e2))&amp;(r4(e2,f))&amp;(!(r5(s3,f))) 32 2pni ✓ 2 (r1(s1,e1))&amp;(r2(e1,f))&amp;(r3(s2,e2))&amp;(!(r4(e2,f))) 33 pn3i ✓ 2 (r1(s1,e1))&amp;(!(r2(e1,f)))&amp;(r3(s2,f))&amp;(r4(s3,f)) 34 in2p ✓ 3 (r1(s1,e1))&amp;(!(r2(s2,e1)))&amp;(r3(e1,e2))&amp;(r4(e2,f)) 35 inu ✓ 2 ((r1(s1,e1))&amp;(!(r2(s2,e1)))&amp;(r3(e1,f)))|(r4(s3,f)) 36 inpu ✓ 2 ((r1(s1,e1))&amp;(!(r2(s2,e1)))&amp;(r3(e1,f)))|((r4(s3,e2))&amp;(r5(e2,f))) 37 upni ✓ 2 ((r1(s1,e1))|(r2(s2,e1)))&amp;(r3(e1,f))&amp;(r4(s3,e2))&amp;(!(r5(e2,f))) 38 unpi ✓ 2 ((r1(s1,e1))|(r2(s2,e1)))&amp;(!(r3(e1,f)))&amp;(r4(s3,e2))&amp;(r5(e2,f)) 39 imp ✓ 3 (r1(s1,e1))&amp;(r2(s2,e1</p>
<p>G Evaluation Details</p>
<p>For each knowledge graphs, we separate their edges into training, validation, and testing edges with a ratio of approximately 8:1:1, as shown in Table 14.We construct three graphs, training graph G train , validation graph G valid , and testing graph G test with training edges, training+validation edges, and training+validation+testing edges, respectively.We adopt the mean reciprocal rank (MRR) as our evaluation metric, following the calculation presented in Equation 9 and 10.It is important to note that for all testing queries, the answers in A(G valid , q) − A(G train , q) are excluded from A ood to ensure fairness.Consequently, we define A id = A(G train , q) and A ood = A(G test , q) − A(G valid , q) for our evaluation of ID (K) and OOD (K), respectively.In each table regarding experimental results, the scores below ID (Q) (i.e., either MRR of ID (K) or OOD (K)) represent the average scores among all seen query types.While the scores below OOD (Q) represent the average scores among all unseen query types.To connect with literature in KG query answering (Bai et al., 2023b;Sun et al., 2021), the performances in ID (K) and OOD (K) are also referred to as faithfulness and knowledge inference capability, respectively.
M RR id(k) = v∈A id |A id |rank(v) (9) M RR ood(k) = v∈A ood |A ood |rank(v)(10)</p>
<p>H Transformer Architecture Comparison</p>
<p>We investigate the impact of different transformer architectures on complex query answering performance.</p>
<p>Figure 1 :
1
Figure 1: An illustration of our two-fold Out-of-distribution Generalization in knowledge and query type dimensions.Figure (a) explains the relation between OOD shifts in the two dimensions.Figure (b) and Figure (c) provide examples demonstrating the causes of OOD shifts in the two dimensions.</p>
<p>Figure 2 :
2
Figure 2: An illustration of three core stages in the pipeline of KG query answering with transformers.</p>
<p>Fig. 3
3
Fig. 3 provides an example of how attention guidance in TEGA can affect the reasoning process in selfattention.In 2in queries [(r1(s1,f))&amp;(!(r2(s2,f)))], the negation operator [!] is applied to calculate the complement of the entity set represented by the second atomic formula [(r2(s2,f))].Benefited from the attention guidance from inductive biases, TEGA can effectively aggregate the information from the second atomic formula [r2/s2/f] to the negation operator with self-attention.While in baseline (Transformer + Relative PE), the information flow between tokens are scattered and less effective (highlighted in red).As a result, TEGA outperforms Transformer + Relative PE by 21-26% in 2in queries over three knowledge graphs.</p>
<p>s1,e1))&amp;(r2(s2,e1))&amp;(r3(e1,f))&amp;(r4(s3,e2))&amp;(r5(e2,f))&amp;(r6(e1,e2))&amp;(r7(e1,fs1,e1))&amp;(r2(e1,f))&amp;(r3(s2,e2))&amp;(r4(e2,f))&amp;(r5(e1,e2))&amp;(!(r6(e1,f)))</p>
<p>Table 2 :
2)Probabilistic26.918.522.413.524.315.6ConE (Zhang et al., 2021)Geometric35.522.027.215.630.718.3CQD (Arakelyan et al., 2021)Fuzzy Logic33.120.721.511.226.415.2LMPNN (Wang et al., 2023b)GNN32.521.024.313.927.716.9SQE-LSTM (Bai et al., 2023b)RNN39.926.331.518.535.021.8Trans.+Absolute PETransformer46.931.921.813.232.321.0Trans.+Disentangled PETransformer51.733.323.713.635.421.8Trans.+Rotary PETransformer50.132.734.620.841.125.8Trans.+Relative PETransformer48.132.335.421.540.726.0BetaE (Ren and Leskovec, 2020)Probabilistic27.416.723.313.125.014.6ConE (Zhang et al., 2021)Geometric34.918.527.314.230.516.0CQD (Arakelyan et al., 2021)Fuzzy Logic32.814.120.28.625.510.9LMPNN (Wang et al., 2023b)GNN26.616.422.312.424.114.1FB15k-237SQE-LSTM (Bai et al., 2023b)RNN45.817.932.314.637.916.0Trans.+Absolute PETransformer54.419.920.89.034.913.6Trans.+Disentangled PETransformer59.020.021.38.737.013.4Trans.+Rotary PETransformer54.319.836.715.444.117.2Trans.+Relative PETransformer54.820.037.715.844.817.6BetaE (Ren and Leskovec, 2020)Probabilistic40.916.732.311.135.913.4ConE (Zhang et al., 2021)Geometric46.117.938.312.841.514.9CQD (Arakelyan et al., 2021)Fuzzy Logic42.914.728.98.334.811.0LMPNN (Wang et al., 2023b)GNN42.817.132.111.636.613.9NELL995SQE-LSTM (Bai et al., 2023b)RNN65.117.848.312.555.314.7Trans.+Absolute PETransformer68.818.925.88.343.812.7Trans.+Disentangled PETransformer73.619.329.78.748.113.1Trans.+Rotary PETransformer69.419.350.613.258.415.7Trans.+Relative PETransformer68.018.749.813.457.415.6
Experimental result (in MRR%) of Transformers in comparison with previous state-of-the-art methods with different backbones in our full benchmark.Definitions of evaluation metrics are provided in Sec.4.2.</p>
<p>Table 2 .
2
Transformers marginally outperform all baselines in both ID (K) and OOD (K) in seen query types.Both APE and DPE transformers have poor
. Relative PE (Shaw</p>
<p>Table 3 :
3
Experimental results of Transformers in FB15k-237 with different formal languages.Details of the dataset fragment are provided in Appendix A.
Trans.+Absolute PELisp-Like EFO57.1 58.120.7 20.710.0 10.44.9 5.1Trans.+Relative PELisp-Like EFO58.3 58.120.9 20.922.1 35.49.8 14.3ModelBefore ReversionAfter ReversionID(K)OOD(K)ID(K)OOD(K)Trans.+Absolute PE54.118.327.89.8Trans.+Relative PE54.317.054.516.8</p>
<p>Table 4 :
4
Experiment results of Transformers on five query types before and after reversing permutation.</p>
<p>Table 8 :
8
Ablation study on two inductive biases introduced in TEGA on FB15k.
DatasetModelID (Q)OOD (Q)ID (K) OOD (K) ID (K) OOD (K)Trans.+Absolute PE 46.931.921.813.2FB15kTrans.+Relative PE48.132.335.421.5TEGA (ours)55.034.138.822.4Trans.+Absolute PE 54.419.920.89.0FB15k-237Trans.+Relative PE54.820.037.715.8TEGA (ours)64.320.142.616.0Trans.+Absolute PE 68.818.925.88.3NELL995Trans.+Relative PE68.018.749.813.4TEGA (ours)75.019.256.613.7Table 7: Experimental results of TEGA comparing tobaselines in FB15k, FB15k-237 and NELL995.ModelID (Q)OOD (Q)ID (K) OOD (K) ID (K) OOD (K)TEGA (ours)55.034.138.822.4w/o LogiRPE51.933.237.021.6w/o Free-Variable Pooling50.933.136.522.2Trans.+Relative PE (baseline)48.132.335.421.5</p>
<p>).
Knowledge GraphTrainingValidation Testing1pOther Types All Types All TypesFB15k273,710547,4208,0008,000FB15k-237149,689299,3785,0005,000NELL995107,982215,9644,0004,000</p>
<p>Table 9 :
9
Number of queries used for each query type in our benchmark.</p>
<p>Table 11 :
11
Details of 23 seen query types with their type name, query-level features: mul:multiple relation projection edges, cyc:cycles, exi:existentially quantified variables, neg:negation, depth:longest relation projection chain (reasoning depth), and their logical formula.</p>
<p>Table 12 :
12
Details of 32 unseen query types with their type name, query-level features: mul:multiple relation projection edges, cyc:cycles, exi:existentially quantified variables, neg:negation, depth:longest relation projection chain (reasoning depth), and their logical formula.</p>
<p>Table 15 :
15
Table 15 compares three architectural variants: encoder-only (our main approach), decoder-only, and encoder-decoder configurations.The results demonstrate that the encoder-only architecture with relative positional encoding achieves the best overall performance across datasets.Comparison of different transformer architectures (in MRR%).
DatasetArchitecturePE TypeID (Q)OOD (Q)All QueriesID (K) OOD (K) ID (K) OOD (K) ID (K) OOD (K)Encoder-onlyAbsolute PE46.931.921.813.232.321.0FB15kEncoder-only Decoder-onlyRelative PE Absolute PE48.1 47.932.3 31.935.4 24.321.5 14.740.7 34.226.0 21.9Encoder-Decoder Relative PE40.128.532.019.835.423.4Encoder-onlyAbsolute PE54.420.020.89.034.913.6FB15k-237Encoder-only Decoder-onlyRelative PE Absolute PE54.8 55.220.0 19.937.7 21.915.8 9.844.8 35.817.6 14.0Encoder-Decoder Relative PE44.519.933.515.838.117.5Encoder-onlyAbsolute PE68.818.925.88.343.812.7NELL995Encoder-only Decoder-onlyRelative PE Absolute PE68.0 65.618.7 18.649.8 28.013.4 8.957.4 43.715.6 13.0Encoder-Decoder Relative PE65.519.748.713.755.716.2
Queries are represented in EFO syntax(Yin et al.,<br />
2023b).
Decoder-only and Encoder-Decoder yield comparable performance as illustrated in Appendix H.
AcknowledgementsWe thank all the anonymous reviewers and meta reviewers for their valuable comments.The authors of this paper were supported by the ITSP Platform Research Project (ITS/189/23FP) from ITC of Hong Kong, SAR, China, and the AoE (AoE/E-601/24-N), the RIF (R6021-20) and the GRF (16205322) from RGC of Hong Kong, SAR, China.Ethics StatementThe experiments were conducted on publicly available knowledge graphs, eliminating any data privacy concerns.However, it should be noticed that most approaches for generalizable logical entailment are susceptible to adversarial attacks(Dai et al., 2018;Zügner et al., 2019)and data poisoning(You et al., 2023)on knowledge graphs, which may result in unintended outcomes in applications.,(i,(p,(r4),(p,(r3),(s2))),(p,(r2),(p,(r1),(s1)))),(n,(p,(r5)(p,(r5),(p,(r4),(s3))),(p,(r3),(i,(p,(r1),(s1)),(n,(p,(r2),(s2))))))Table13: Details of 25 query types (compatible in both Lisp-like and EFO syntax) that we used as a subset for exploration on query syntax.E Knowledge Graph StatisticsDetailed statistics of the knowledge graphs selected are presented in Table14.Table 14: Details of three knowledge graphs used for the experiments, and their separation standard for training, validation, and testing edges according to(Ren and Leskovec, 2020).F Query Graph DefinitionWe provide query graph for each query types in our dataset.Query graphs can be utilized for graphaugmented methods, and have been applied in the two inductive biases discussed in Sec.5.3.Here we provide the definition: For each atomic formula or its negation α = r(h, t) or ¬r(h, t) in a conjunctive formula c, we have {(h, r), (r, t)} ∈ G c or {(h, r), (r, n), (n, t)} ∈ G c , where n denotes the negation node in the conjunctive query graph G c .By our definition of query, all conjunctive query graphs have exactly one node as a free variable.For queries that represent the disjunction of multiple conjunctive formulas, we replace all free variable nodes in every conjunctive query graph with a single union node u, and connect it to a final free variable node f , with {(u, f )} ∈ G d , where G d is the disjunctive query graph.I Technical DetailsIn this section, we describe our experiment setting in more details.Computational Resource: All transformers are trained on four NVIDIA A100 GPUs for two days, with a batch size of 1024.As shown in Table16, TEGA achieves computational efficiency comparable to that of the baseline models.Model Configuration: Table18illustrated the impact of transformer size to query answering performances.We selected a three-layer configuration for all Transformer models, for its effectiveness and comparable parameter size to other baseline methods (as shown in Table17).To train the Transformer models, we employed the label smoothing loss(Szegedy et al., 2015)with a smoothing factor of ϵ = 0.1, which helps to regularize the model and improve generalization.The learning rate was set to 0.0001, and a warm-up schedule of 1000 steps was applied to gradually increase the learning rate during the initial phase of training, allowing the model to adapt to the task and stabilize the optimization process.All Transformer models and baselines were configured with an embedding size of 400 dimensions.Regarding transformer encoder with absolute and relative PE, we employ the BertModel from the transformers library.While for disentangled PE, we use DebertaModel with the same size and configurations.Note that apart from disentangled PE, Deberta also proposed a mask decoder during the pre-training stage.However, as we train all models solely on logical queries without utilizing any pre-trained model weights, the impact of the mask decoder is negligible.Pooling Strategy: For free-variable pooling, we experimented with two pooling settings: sum pooling and mean pooling, with results presented in Table19.The results show that sum pooling over free variables is the better setting, which is also consistent with findings in GNN literature(Xu et al., 2019).
Complex query answering with neural link predictors. Erik Arakelyan, Daniel Daza, Pasquale Minervini, Michael Cochez, arXiv:2011.034592021Preprint</p>
<p>Autoschemakg: Autonomous knowledge graph construction through dynamic schema induction from web-scale corpora. Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song, ; Weiqi Wang, Chen Luo, Yangqiu Song, arXiv:2505.23628arXiv:2305.19068Jiaxin Bai, Xin Liu. 2025a. 2023aPreprintComplex query answering on eventuality knowledge graph with implicit logical constraints</p>
<p>Advancing abductive reasoning in knowledge graphs through complex logical hypothesis generation. Jiaxin Bai, Yicheng Wang, Tianshi Zheng, Yue Guo, Xin Liu, Yangqiu Song, arXiv:2312.156432024Preprint</p>
<p>Jiaxin Bai, Zihao Wang, Yukun Zhou, Hang Yin, Weizhi Fei, Qi Hu, Zheye Deng, Jiayang Cheng, Tianshi Zheng, Hong Ting Tsang, Yisen Gao, Zhongwei Xie, Yufei Li, Lixin Fan, Binhang Yuan, Wei Wang, Lei Chen, Xiaofang Zhou, Yangqiu Song, arXiv:2501.14224Top ten challenges towards agentic neural graph databases. 2025bPreprint</p>
<p>Sequential query encoding for complex query answering on knowledge graphs. Jiaxin Bai, Tianshi Zheng, Yangqiu Song, Transactions on Machine Learning Research. 2023b</p>
<p>Answering complex logical queries on knowledge graphs via query computation tree optimization. Yushi Bai, Xin Lv, Juanzi Li, Lei Hou, arXiv:2212.095672023cPreprint</p>
<p>Measuring abstract reasoning in neural networks. G T David, Felix Barrett, Adam Hill, Ari S Santoro, Timothy Morcos, Lillicrap, arXiv:1807.042252018Preprint</p>
<p>Chandra Bhagavatula, Le Ronan, Chaitanya Bras, Keisuke Malaviya, Ari Sakaguchi, Hannah Holtzman, Doug Rashkin, Scott Downey, Yejin Wen Tau Yih, Choi, arXiv:1908.05739Abductive commonsense reasoning. 2020Preprint</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt D Bollacker, Colin Evans, Praveen K Paritosh, Tim Sturge, Jamie Taylor, SIGMOD Conference. 2008</p>
<p>Translating embeddings for modeling multirelational data. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, Oksana Yakhnenko, Advances in Neural Information Processing Systems. Curran Associates, Inc201326</p>
<p>A large annotated corpus for learning natural language inference. R Samuel, Gabor Bowman, Christopher Angeli, Christopher D Potts, Manning, arXiv:1508.053262015Preprint</p>
<p>Toward an architecture for never-ending language learning. Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, Tom M Mitchell, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI'10. the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI'10AAAI Press2010</p>
<p>Learning language representations with logical inductive bias. Jianshu Chen, arXiv:2302.094582023Preprint</p>
<p>Fuzzy logic based logical query answering on knowledge graphs. Xuelu Chen, Ziniu Hu, Yizhou Sun, arXiv:2108.023902022Preprint</p>
<p>Peter Clark, Oyvind Tafjord, Kyle Richardson, arXiv:2002.05867Transformers as soft reasoners over language. 2020Preprint</p>
<p>Adversarial attack on graph structured data. Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, Le Song, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR2018Proceedings of Machine Learning Research</p>
<p>. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Łukasz Kaiser, arXiv:1807.038192019Universal transformers. Preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052019Preprint</p>
<p>Legal rule induction: Towards generalizable principle discovery from analogous judicial precedents. Wei Fan, Tianshi Zheng, Yiran Hu, Zheye Deng, Weiqi Wang, Baixuan Xu, Chunyang Li, Haoran Li, Weixing Shen, Yangqiu Song, arXiv:2505.141042025Preprint</p>
<p>Complex reasoning over logical queries on commonsense knowledge graphs. Tianqing Fang, Zeming Chen, Yangqiu Song, Antoine Bosselut, arXiv:2403.073982024Preprint</p>
<p>Controllable logical hypothesis generation for abductive reasoning in knowledge graphs. Yisen Gao, Jiaxin Bai, Tianshi Zheng, Qingyun Sun, Ziwei Zhang, Jianxin Li, Yangqiu Song, Xingcheng Fu, arXiv:2505.209482025Preprint</p>
<p>Embedding logical queries on knowledge graphs. William L Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec, arXiv:1806.014452019Preprint</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, arXiv:2209.00840Xi Victoria Lin, Caiming Xiong, and Dragomir Radev. 2022. Folio: Natural language reasoning with first-order logic. Preprint</p>
<p>Deberta: Decodingenhanced bert with disentangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, arXiv:2006.036542021Preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021Preprint</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural computation. 981997</p>
<p>Towards a mechanistic interpretation of multi-step reasoning capabilities of language models. Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut, Mrinmaya Sachan, arXiv:2310.144912023Preprint</p>
<p>Answering complex queries in knowledge graphs with bidirectional sequence encoders. Bhushan Kotnis, Carolin Lawrence, Mathias Niepert, 10.1609/aaai.v35i6.16630Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Deep learning for symbolic mathematics. Guillaume Lample, François Charton, arXiv:1912.014122019Preprint</p>
<p>Patterns over principles: The fragility of inductive reasoning in llms under noisy observations. Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song, arXiv:2502.161692025Preprint</p>
<p>Mask and reason: Pre-training knowledge graph transformers for complex logical queries. Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, Jie Tang, 10.1145/3534678.3539472Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningACM202222</p>
<p>Model Theory: An Introduction. David Marker, 10.1007/b988602002Springer-VerlagNew York</p>
<p>A unifying view on dataset shift in classification. Troy Jose G Moreno-Torres, Rocío Raeder, Nitesh V Alaiz-Rodríguez, Francisco Chawla, Herrera, Pattern recognition. 4512012</p>
<p>Grokking of hierarchical structure in vanilla transformers. Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher Manning, 10.18653/v1/2023.acl-short.38Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20232Short Papers)</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , arXiv:2305.122952023Preprint</p>
<p>Logicbench: Towards systematic evaluation of logical reasoning ability of large language models. Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral, arXiv:2404.155222024Preprint</p>
<p>Generative language modeling for automated theorem proving. Stanislas Polu, Ilya Sutskever, arXiv:2009.033932020Preprint</p>
<p>Dataset shift in machine learning. Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, Neil Lawrence, 2009</p>
<p>Neural graph reasoning: Complex logical query answering meets graph databases. Mikhail Hongyu Ren, Michael Galkin, Zhaocheng Cochez, Jure Zhu, Leskovec, arXiv:2303.146172023Preprint</p>
<p>Query2box: Reasoning over knowledge graphs in vector space using box embeddings. Weihua Hongyu Ren, Jure Hu, Leskovec, arXiv:2002.059692020Preprint</p>
<p>Beta embeddings for multi-hop logical reasoning in knowledge graphs. Hongyu Ren, Jure Leskovec, arXiv:2010.114652020Preprint</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Mehran Kazemi, Najoung Kim, He He, Advances in Neural Information Processing Systems. 202336</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, arXiv:1904.015572019Preprint</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, arXiv:1803.021552018Preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, arXiv:2104.098642023Preprint</p>
<p>Haitian Sun, Andrew O Arnold, Tania Bedrax-Weiss, Fernando Pereira, William W Cohen, arXiv:2004.03658Faithful embeddings for knowledge base queries. 2021Preprint</p>
<p>Rethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna, arXiv:1512.005672015Preprint</p>
<p>Towards autonomous hypothesis verification via language models with minimal guidance. Shiro Takagi, Ryutaro Yamauchi, Wataru Kumagai, arXiv:2311.097062023Preprint</p>
<p>Diagnosing the firstorder logical reasoning ability through LogicNLI. Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, Yaohui Jin, 10.18653/v1/2021.emnlp-main.303Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Observed versus latent features for knowledge base and text inference. Kristina Toutanova, Danqi Chen, 10.18653/v1/W15-4007Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. the 3rd Workshop on Continuous Vector Space Models and their CompositionalityBeijing, ChinaAssociation for Computational Linguistics2015</p>
<p>Complex embeddings for simple link prediction. Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard, arXiv:1606.063572016Preprint</p>
<p>Transformers for complex query answering over knowledge hypergraphs. Hong Ting Tsang, Zihao Wang, Yangqiu Song, arXiv:2504.165372025Preprint</p>
<p>Principles of risk minimization for learning theory. V Vapnik, Advances in Neural Information Processing Systems. Morgan-Kaufmann19914</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, arXiv:1706.037622023Preprint</p>
<p>Label words are anchors: An information flow perspective for understanding in-context learning. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun, arXiv:2305.141602023aPreprint</p>
<p>Logical message passing networks with one-hop inference on atomic formulas. Zihao Wang, Yangqiu Song, Ginny Y Wong, Simon See, arXiv:2301.088592023bPreprint</p>
<p>Benchmarking the combinatorial generalizability of complex query answering on knowledge graphs. Zihao Wang, Hang Yin, Yangqiu Song, arXiv:2109.089252021Preprint</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, R Samuel, arXiv:1704.054262018PreprintBowman</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, arXiv:1810.008262019Preprint</p>
<p>Query2Triple: Unified query encoding for answering diverse complex queries over knowledge graphs. Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, 10.18653/v1/2023.findings-emnlp.761Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational LinguisticsJun Zhao. 2023</p>
<p>Embedding entities and relations for learning and inference in knowledge bases. Bishan Yang, Wen Tau Yih, Xiaodong He, Jianfeng Gao, Li Deng, arXiv:1412.65752015Preprint</p>
<p>Kgbert: Bert for knowledge graph completion. Liang Yao, Chengsheng Mao, Yuan Luo, arXiv:1909.031932019Preprint</p>
<p>EFO k -cqa: Towards knowledge graph complex query answering beyond set operation. Hang Yin, Zihao Wang, Weizhi Fei, Yangqiu Song, arXiv:2307.137012023aPreprint</p>
<p>Rethinking complex queries on knowledge graphs with neural link predictors. Hang Yin, Zihao Wang, Yangqiu Song, arXiv:2304.070632023bPreprint</p>
<p>Mass: Model-agnostic, semantic and stealthy data poisoning attack on knowledge graph embedding. Xiaoyu You, Beina Sheng, Daizong Ding, Mi Zhang, Xudong Pan, Min Yang, Fuli Feng, 10.1145/3543507.3583203Proceedings of the ACM Web Conference 2023, WWW '23. the ACM Web Conference 2023, WWW '23New York, NY, USAAssociation for Computing Machinery2023. 2000-2010</p>
<p>Zhanqiu Zhang, Jie Wang, Jiajun Chen, Shuiwang Ji, Feng Wu, arXiv:2110.13715Cone embeddings for multi-hop reasoning over knowledge graphs. 2021Preprint</p>
<p>Wenting Zhao, Justin T Chiu, Jena D Hwang, Faeze Brahman, Jack Hessel, Sanjiban Choudhury, Yejin Choi, Xiang , Lorraine Li, Alane Suhr, arXiv:2311.08469Uncommonsense reasoning: Abductive reasoning about uncommon situations. 2024Preprint</p>
<p>Clr-fact: Evaluating the complex logical reasoning capability of large language models over factual knowledge. Tianshi Zheng, Jiaxin Bai, Yicheng Wang, Tianqing Fang, Yue Guo, Yauwai Yim, Yangqiu Song, arXiv:2407.205642024Preprint</p>
<p>The curse of cot: On the limitations of chain-of-thought in in-context learning. Tianshi Zheng, Yixiang Chen, Chengxi Li, Chunyang Li, Qing Zong, Haochen Shi, Baixuan Xu, Yangqiu Song, Ginny Y Wong, Simon See, arXiv:2504.050812025aPreprint</p>
<p>Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y Wong, Simon See, arXiv:2502.11176Logidynamics: Unraveling the dynamics of logical inference in large language model reasoning. 2025bPreprint</p>
<p>From automation to autonomy: A survey on large language models in scientific discovery. Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, Yangqiu Song, arXiv:2505.132592025cPreprint</p>
<p>Knowshiftqa: How robust are rag systems when textbook knowledge shifts in k-12 education?. Tianshi Zheng, Weihan Li, Jiaxin Bai, Weiqi Wang, Yangqiu Song, arXiv:2412.089852025dPreprint</p>
<p>Neural-symbolic models for logical queries on knowledge graphs. Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, Jian Tang, arXiv:2205.101282022Preprint</p>
<p>Comparisonqa: Evaluating factuality robustness of llms through knowledge frequency control and uncertainty. Qing Zong, Zhaowei Wang, Tianshi Zheng, Xiyu Ren, Yangqiu Song, arXiv:2412.202512025Preprint</p>
<p>Adversarial attacks on neural networks for graph data. Daniel Zügner, Amir Akbarnejad, Stephan Günnemann, 10.24963/ijcai.2019/872Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-192019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5460 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5460</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5460</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-2d7a14fabe2631a205e65bcac94d8e84d883492f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2d7a14fabe2631a205e65bcac94d8e84d883492f" target="_blank">Enable Language Models to Implicitly Learn Self-Improvement From Data</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5460.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5460.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PIT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ImPlicit Self-Improvement (PIT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that is trained to take an input prompt and a reference response and produce an improved response by implicitly learning improvement goals from preference data; trained via a reformulated RLHF objective that maximizes reward gap conditioned on a reference response and uses a curriculum of two RL rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 (Bison) (backbone used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM 2 (Bison) used as the backbone LLM and reward-model backbone; exact parameter count not specified in the paper. Chosen as a mid-size practical model: smaller models performed worse and larger ones were too expensive to train.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>PIT (Implicit Self-Improvement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>PIT is a separate model M_PIT trained to generate an improved response y given (x, y_ref). Training: supervised fine-tune on (x, y_l, y_w) to predict y_w given y_l; train a gap-based reward model R_PIT that directly predicts reward gaps between two responses; perform curriculum reinforcement learning in two rounds (first improve ground-truth references y_l/y_w, then improve policy-generated references sampled from M_P^RL). At inference: sample a reference y_ref from the RLHF policy M_P^RL, generate an improved response y1 = M_PIT(x, y_ref), and optionally iterate (y2 = M_PIT(x, y1), ...). PIT requires no explicit human-written rubric at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Anthropic/HH-RLHF (helpful subset), OpenAI/Summary, Synthetic instruction-following dataset</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Anthropic/HH-RLHF: dialog assistant helpfulness dataset (human preference labels); OpenAI/Summary: Reddit posts with two human summaries and preference labels; Synthetic: PaLM2-generated instruction-following tasks where y_w follows all requirements and y_l misses some requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Anthropic/HH-RLHF (GPT-4 evaluator): PIT vs Original = Win 55.47% / Lose 27.34% / Δ = +28.13%. (DeBERTa evaluator): 46.30% / 32.30% / Δ = +14.00%. OpenAI/Summary (GPT-4): PIT vs Original = 44.53% / 24.22% / Δ = +20.31%. (DeBERTa): 41.9% / 34.7% / Δ = +7.2%. Synthetic dataset (GPT-4): PIT vs Original = 48.44% / 14.84% / Δ = +33.59%. PIT vs Self-Refine: Anthropic (GPT-4) 38.28% / 42.19% / Δ = -3.91% (GPT-4 slightly prefers Self-Refine), DeBERTa 41.3% / 37.60% / Δ = +3.70% (DeBERTa slightly prefers PIT), Human eval: PIT vs Self-Refine = 47.06% / 23.53% / Δ = +23.53% in favor of PIT. Synthetic PIT vs Self-Refine (GPT-4): 45.31% / 35.16% / Δ = +10.16%. ELO ranking over 5 iterations (DeBERTa) ranks PIT iterations above Self-Refine and Original (PIT Iter4 best, PIT Iter1 second).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Original (no self-improvement) baseline: Anthropic/HH-RLHF (GPT-4): Original vs y_w = 71.85% / 17.19% / Δ = +54.69% (shows original RLHF policy is already strong). OpenAI/Summary (GPT-4): Original vs y_w = 74.22% / 8.59% / Δ = +65.63%. Synthetic: Original vs y_w (GPT-4) = 28.91% / 51.56% / Δ = -22.66% (y_w can be stronger than original on synthetic because y_w generated by a larger model in data generation).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: PIT improves original RLHF policy responses across three datasets with positive Δ (win rate minus lose rate) ranging from +7.2% (DeBERTa, OpenAI/Summary) up to +33.59% (GPT-4, Synthetic). PIT outperforms Self-Refine by human evaluation on Anthropic/HH-RLHF by Δ = +23.53% and achieves higher ELO scores than Self-Refine across iterative experiments. Ablation: curriculum RL (two-stage RL) is shown necessary—PIT trained with the full curriculum breaks out of the 'Similar' reward region while variants trained with only one RL fail to improve effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires curriculum RL: skipping the first (ground-truth-reference) RL or training on only easy data causes failure to improve strong policy outputs. Iterative inference does not always monotonically improve quality; the best number of inference iterations is task- and data-dependent (ELO experiments show non-monotonicity). PIT penalizes repetition: reward model gives negative reward when responses are identical, which can discourage trivial repeats but may interact with generation dynamics. Performance differences depend on evaluator choice (GPT-4 tends to prefer longer/detailed outputs, DeBERTa and humans favor PIT outputs differently). Training variants (e.g., adding unlikelihood loss) degraded performance in pilots. Stop criteria are needed in applications because indefinite iteration may stagnate or degrade outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5460.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5460.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based iterative self-improvement method where the LLM first reflects on its own candidate response to produce feedback, then conditions on that feedback to generate an improved response; can be applied to black-box LLMs via prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 2 (Bison) (applied via prompting to RLHF-finetuned PaLM2 policy in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implemented as prompting the RLHF-finetuned policy model M_P^RL (PaLM 2 Bison) to (1) produce analysis/feedback about its output, then (2) produce an improved response conditioned on that feedback; no model fine-tuning required.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (generate-then-reflect / self-feedback prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Two-step prompt: (a) Reflection step — present the prompt and the candidate response and ask the model to analyze and give feedback (self-critique); (b) Refinement step — feed the candidate and the feedback back to the model and ask it to produce an improved response based on the feedback. This is done at inference time and can be iterated multiple times. Uses explicit human-written rubrics in the prompt specifying the intended improvement goal (e.g., 'be more helpful: Helpful means {rubric}').</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Anthropic/HH-RLHF (helpful subset), Synthetic dataset (instruction-following); not applied to OpenAI/Summary in these experiments (summarization-specific prompts made it not applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Anthropic/HH-RLHF: conversational helpfulness/harmfulness; Synthetic: instruction-following with format/requirements checks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Anthropic/HH-RLHF (GPT-4): Self-Refine vs Original = 60.94% / 17.19% / Δ = +43.75% (improves original strongly by GPT-4 judgment). (DeBERTa): 40.30% / 31.40% / Δ = +8.90%. Synthetic (GPT-4): Self-Refine vs Original = 34.38% / 17.97% / Δ = +16.41%. In iteration ELO experiments (DeBERTa) Self-Refine iterations ranked below PIT iterations; Self-Refine Iter1 ELO = 1001 vs PIT Iter1 = 1033.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Original baseline (no self-reflection): see PIT entry; Self-Refine improves over Original per above numbers (e.g., Anthropic GPT-4 Δ +43.75%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative improvements vs original responses reported (e.g., Anthropic/HH-RLHF GPT-4 Δ +43.75%). Self-Refine also shows improvements in ELO-based iterative comparisons relative to original outputs. However, improvements are sensitive to hyperparameters (temperature) and prompt/rubric design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires explicit, carefully designed rubrics/prompts; when the improvement goal is vague (e.g., 'be more helpful') the model often responds by lengthening output or adding details that may be off-topic—this can be judged worse by humans even if some automatic evaluators prefer longer answers. Self-Refine tends to benefit from higher sampling temperatures (0.6–0.8) to be most effective, and at low temperatures it often only appends minor details rather than producing qualitatively better answers. Not applicable or less suitable for some task formats (authors did not apply it to OpenAI/Summary). Sensitivity to evaluator bias: GPT-4 (which favors long/detailed outputs) preferred Self-Refine over PIT in some comparisons while human raters preferred PIT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5460.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5460.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-debug (Chen et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method (referenced) where LLMs are used to analyze and fix their own code-generation errors by using failure signals (e.g., interpreter error messages) as feedback to iteratively repair outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Teaching large language models to self-debug</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-debug / iterative self-feedback (code debugging)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLMs generate code, execute it, collect environment/tool feedback (error messages), and use that feedback to iteratively revise the code; cited in related work as an example of self-improvement using environment feedback rather than pure prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code generation / code repair (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve code generation by running code and using runtime errors as signals to guide fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Mentioned in related work as prior art demonstrating LLM self-improvement (Chen et al., 2023) for code generation, but not evaluated/used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires an executable environment that can provide automatic and reliable feedback (not generally available for all tasks).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 2)</em></li>
                <li>Large language models can self-improve <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5460",
    "paper_id": "paper-2d7a14fabe2631a205e65bcac94d8e84d883492f",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "PIT",
            "name_full": "ImPlicit Self-Improvement (PIT)",
            "brief_description": "A model that is trained to take an input prompt and a reference response and produce an improved response by implicitly learning improvement goals from preference data; trained via a reformulated RLHF objective that maximizes reward gap conditioned on a reference response and uses a curriculum of two RL rounds.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM 2 (Bison) (backbone used in experiments)",
            "model_description": "PaLM 2 (Bison) used as the backbone LLM and reward-model backbone; exact parameter count not specified in the paper. Chosen as a mid-size practical model: smaller models performed worse and larger ones were too expensive to train.",
            "reflection_method_name": "PIT (Implicit Self-Improvement)",
            "reflection_method_description": "PIT is a separate model M_PIT trained to generate an improved response y given (x, y_ref). Training: supervised fine-tune on (x, y_l, y_w) to predict y_w given y_l; train a gap-based reward model R_PIT that directly predicts reward gaps between two responses; perform curriculum reinforcement learning in two rounds (first improve ground-truth references y_l/y_w, then improve policy-generated references sampled from M_P^RL). At inference: sample a reference y_ref from the RLHF policy M_P^RL, generate an improved response y1 = M_PIT(x, y_ref), and optionally iterate (y2 = M_PIT(x, y1), ...). PIT requires no explicit human-written rubric at inference.",
            "num_iterations": 5,
            "task_name": "Anthropic/HH-RLHF (helpful subset), OpenAI/Summary, Synthetic instruction-following dataset",
            "task_description": "Anthropic/HH-RLHF: dialog assistant helpfulness dataset (human preference labels); OpenAI/Summary: Reddit posts with two human summaries and preference labels; Synthetic: PaLM2-generated instruction-following tasks where y_w follows all requirements and y_l misses some requirements.",
            "performance_with_reflection": "Anthropic/HH-RLHF (GPT-4 evaluator): PIT vs Original = Win 55.47% / Lose 27.34% / Δ = +28.13%. (DeBERTa evaluator): 46.30% / 32.30% / Δ = +14.00%. OpenAI/Summary (GPT-4): PIT vs Original = 44.53% / 24.22% / Δ = +20.31%. (DeBERTa): 41.9% / 34.7% / Δ = +7.2%. Synthetic dataset (GPT-4): PIT vs Original = 48.44% / 14.84% / Δ = +33.59%. PIT vs Self-Refine: Anthropic (GPT-4) 38.28% / 42.19% / Δ = -3.91% (GPT-4 slightly prefers Self-Refine), DeBERTa 41.3% / 37.60% / Δ = +3.70% (DeBERTa slightly prefers PIT), Human eval: PIT vs Self-Refine = 47.06% / 23.53% / Δ = +23.53% in favor of PIT. Synthetic PIT vs Self-Refine (GPT-4): 45.31% / 35.16% / Δ = +10.16%. ELO ranking over 5 iterations (DeBERTa) ranks PIT iterations above Self-Refine and Original (PIT Iter4 best, PIT Iter1 second).",
            "performance_without_reflection": "Original (no self-improvement) baseline: Anthropic/HH-RLHF (GPT-4): Original vs y_w = 71.85% / 17.19% / Δ = +54.69% (shows original RLHF policy is already strong). OpenAI/Summary (GPT-4): Original vs y_w = 74.22% / 8.59% / Δ = +65.63%. Synthetic: Original vs y_w (GPT-4) = 28.91% / 51.56% / Δ = -22.66% (y_w can be stronger than original on synthetic because y_w generated by a larger model in data generation).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: PIT improves original RLHF policy responses across three datasets with positive Δ (win rate minus lose rate) ranging from +7.2% (DeBERTa, OpenAI/Summary) up to +33.59% (GPT-4, Synthetic). PIT outperforms Self-Refine by human evaluation on Anthropic/HH-RLHF by Δ = +23.53% and achieves higher ELO scores than Self-Refine across iterative experiments. Ablation: curriculum RL (two-stage RL) is shown necessary—PIT trained with the full curriculum breaks out of the 'Similar' reward region while variants trained with only one RL fail to improve effectively.",
            "limitations_or_failure_cases": "Requires curriculum RL: skipping the first (ground-truth-reference) RL or training on only easy data causes failure to improve strong policy outputs. Iterative inference does not always monotonically improve quality; the best number of inference iterations is task- and data-dependent (ELO experiments show non-monotonicity). PIT penalizes repetition: reward model gives negative reward when responses are identical, which can discourage trivial repeats but may interact with generation dynamics. Performance differences depend on evaluator choice (GPT-4 tends to prefer longer/detailed outputs, DeBERTa and humans favor PIT outputs differently). Training variants (e.g., adding unlikelihood loss) degraded performance in pilots. Stop criteria are needed in applications because indefinite iteration may stagnate or degrade outputs.",
            "uuid": "e5460.0"
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A prompting-based iterative self-improvement method where the LLM first reflects on its own candidate response to produce feedback, then conditions on that feedback to generate an improved response; can be applied to black-box LLMs via prompts.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "use",
            "model_name": "PaLM 2 (Bison) (applied via prompting to RLHF-finetuned PaLM2 policy in experiments)",
            "model_description": "Implemented as prompting the RLHF-finetuned policy model M_P^RL (PaLM 2 Bison) to (1) produce analysis/feedback about its output, then (2) produce an improved response conditioned on that feedback; no model fine-tuning required.",
            "reflection_method_name": "Self-Refine (generate-then-reflect / self-feedback prompting)",
            "reflection_method_description": "Two-step prompt: (a) Reflection step — present the prompt and the candidate response and ask the model to analyze and give feedback (self-critique); (b) Refinement step — feed the candidate and the feedback back to the model and ask it to produce an improved response based on the feedback. This is done at inference time and can be iterated multiple times. Uses explicit human-written rubrics in the prompt specifying the intended improvement goal (e.g., 'be more helpful: Helpful means {rubric}').",
            "num_iterations": 5,
            "task_name": "Anthropic/HH-RLHF (helpful subset), Synthetic dataset (instruction-following); not applied to OpenAI/Summary in these experiments (summarization-specific prompts made it not applicable).",
            "task_description": "Anthropic/HH-RLHF: conversational helpfulness/harmfulness; Synthetic: instruction-following with format/requirements checks.",
            "performance_with_reflection": "Anthropic/HH-RLHF (GPT-4): Self-Refine vs Original = 60.94% / 17.19% / Δ = +43.75% (improves original strongly by GPT-4 judgment). (DeBERTa): 40.30% / 31.40% / Δ = +8.90%. Synthetic (GPT-4): Self-Refine vs Original = 34.38% / 17.97% / Δ = +16.41%. In iteration ELO experiments (DeBERTa) Self-Refine iterations ranked below PIT iterations; Self-Refine Iter1 ELO = 1001 vs PIT Iter1 = 1033.",
            "performance_without_reflection": "Original baseline (no self-reflection): see PIT entry; Self-Refine improves over Original per above numbers (e.g., Anthropic GPT-4 Δ +43.75%).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative improvements vs original responses reported (e.g., Anthropic/HH-RLHF GPT-4 Δ +43.75%). Self-Refine also shows improvements in ELO-based iterative comparisons relative to original outputs. However, improvements are sensitive to hyperparameters (temperature) and prompt/rubric design.",
            "limitations_or_failure_cases": "Requires explicit, carefully designed rubrics/prompts; when the improvement goal is vague (e.g., 'be more helpful') the model often responds by lengthening output or adding details that may be off-topic—this can be judged worse by humans even if some automatic evaluators prefer longer answers. Self-Refine tends to benefit from higher sampling temperatures (0.6–0.8) to be most effective, and at low temperatures it often only appends minor details rather than producing qualitatively better answers. Not applicable or less suitable for some task formats (authors did not apply it to OpenAI/Summary). Sensitivity to evaluator bias: GPT-4 (which favors long/detailed outputs) preferred Self-Refine over PIT in some comparisons while human raters preferred PIT.",
            "uuid": "e5460.1"
        },
        {
            "name_short": "Self-debug (Chen et al., 2023)",
            "name_full": "Teaching large language models to self-debug",
            "brief_description": "A method (referenced) where LLMs are used to analyze and fix their own code-generation errors by using failure signals (e.g., interpreter error messages) as feedback to iteratively repair outputs.",
            "citation_title": "Teaching large language models to self-debug",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "reflection_method_name": "self-debug / iterative self-feedback (code debugging)",
            "reflection_method_description": "LLMs generate code, execute it, collect environment/tool feedback (error messages), and use that feedback to iteratively revise the code; cited in related work as an example of self-improvement using environment feedback rather than pure prompting.",
            "num_iterations": null,
            "task_name": "code generation / code repair (mentioned in related work)",
            "task_description": "Improve code generation by running code and using runtime errors as signals to guide fixes.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "evidence_of_improvement": "Mentioned in related work as prior art demonstrating LLM self-improvement (Chen et al., 2023) for code generation, but not evaluated/used in this paper's experiments.",
            "limitations_or_failure_cases": "Requires an executable environment that can provide automatic and reliable feedback (not generally available for all tasks).",
            "uuid": "e5460.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 2
        },
        {
            "paper_title": "Large language models can self-improve",
            "rating": 1
        }
    ],
    "cost": 0.017537249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Enabling Language Models to Implicitly Learn SELF-IMPROVEMENT</h1>
<p>Ziqi Wang ${ }^{1}$, Le Hou ${ }^{2}$, Tianjian Lu ${ }^{2}$, Yuexin Wu ${ }^{2}$, Yunxuan $\mathbf{L i}^{2}$, Hongkun $\mathbf{Y u}^{2}$, Heng $\mathbf{J i}^{1}$<br>${ }^{1}$ University of Illinois Urbana-Champaign ${ }^{2}$ Google<br>ziqiw9@illinois.edu lehou@google.com</p>
<h4>Abstract</h4>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpful and less harmful). To this end, we propose an ImPlicit Self-Improvement (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models without extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.</p>
<h2>1 INTRODUCTION</h2>
<p>LLMs (Devlin et al., 2018; Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Schulman et al., 2022; OpenAI, 2023; Anil et al., 2023) have achieved state-of-the-art results on complex tasks such as math reasoning (Wei et al., 2022; Xue et al., 2023; Zhou et al., 2023), summarization (Stiennon et al., 2020b), conversations (Schulman et al., 2022; Bai et al., 2022), schema induction (Li et al., 2023) and solving domain-specific problems (Singhal et al., 2022). The keys of LLMs success are their abilities of following instructions and aligning with human preferences (Ouyang et al., 2022; Peng et al., 2023a; Shen et al., 2023). A widely adopted approach toward them is instruction fine-tuning and reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). However, instruction fine-tuning and RLHF are imperfect, and there is always room for improvement. For example, LLMs may hallucinate information (OpenAI, 2023), have reasoning errors (Bubeck et al., 2023), and generate unhelpful and harmful contents (Bai et al., 2022). A straightforward approach is to collect more diverse and high-quality data and improve the alignment with a human-in-the-loop training paradigm (Ouyang et al., 2022), which requires extensive amount of human effort, especially for specific domains that require expert knowledge.</p>
<p>Therefore, the community has explored to use LLMs to self-improve their own response quality without human intervention. With the advent of generative language models, prompting methods have proved effective and efficient (no need to train models) with convenience (no need to serve models and can be used with black-box language models through APIs). Madaan et al. (2023) use</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The pipeline of PIT and prompting methods (Self-Refine). Upper: PIT utilizes inputs and LLM outputs for further improvement during inference. During training, a reward model will assess the gap of PIT-improved outputs and LLM outputs, and the reward is used for reinforcement learning to train PIT. Lower: Self-Refine uses LLMs to give self-feedback and then asks LLMs to improve outputs based on the self-feedback.</p>
<p>LLMs to give self-feedback and iteratively improve LLMs response quality, Chen et al. (2023) enable LLMs to self-debug to enhance code generations. Nevertheless, self-improvement in language models through prompting can be less than ideal, as humans often find it challenging to define comprehensive improvement goals and create detailed assessment rubrics. For example, improving helpfulness and harmlessness of LLMs requires careful definitions of these qualities to guide LLMs in improving their responses accordingly. As a result, prompting could make LLMs self-improve well only if the improvement goal is clear, simple, and well-defined through prompting. LLMs perform poorly in our experiments if we ask them to make responses more helpful. This is because the LLM tends to add more details to the response and thus makes the response longer, whereas more details are not always preferred since it may lead to the response going off the topic. Instead, asking language models to be polite, offer necessary information, and avoid going off topics will yield better improvements. Previous methods have also discussed similar observations on the effect of detailed rubrics. Rafailov et al. (2023) showed that using a simple rubric of Which summary is better? will yield much more disagreement with humans than a more comprehensive rubric of Which summary summarizes most important points without including unimportant details?</p>
<p>Despite the effectiveness, detailed rubrics are often hard to obtain. First, it is hard for humans to infer and write all possible rubrics. Second, when the task requires domain expertise (e.g., clinical domain (Singhal et al., 2022)), it is impractical to scale up. To this end, we switch our focus from explicitly designing rubrics to implicitly learning self-improvement from data. We notice that the preference data used to train reward models implicitly tells how to improve the response quality. Thus, we utilize this preference data to train a novel model that implicitly understands self-improvement goals from data. Using this method, we eliminate the need for rubric design and avoid the need for additional data since we only reuse the data used to train reward models.</p>
<p>We denote our approach ImPlicit Self-ImprovemenT (PIT), a novel approach that enables the model to learn self-improvement implicitly from data. Specifically, we reformulate the instruction finetuning and RLHF process and switch the training objective from maximizing response quality for given inputs to maximizing response quality gap conditioned on reference responses for given inputs. Figure 1 shows the working flows of PIT and prompting methods. PIT utlizes given inputs and reference responses and generates improved responses accordingly. Similar to prompting methods, PIT can repeat this process iteratively by replacing reference responses with improved responses. Compared with prompting methods, our method PIT does not require a manual rubric design. Extensive evaluations on two real-world datasets and one synthetic dataset show the effectiveness of PIT compared with prompting methods such as Self-Refine (Madaan et al., 2023).</p>
<h1>2 Related Work</h1>
<p>Alignment Alignment is critical for a helpful and harmless language model (Ouyang et al., 2022; Bai et al., 2022). One common way for alignment is RLHF (Ouyang et al., 2022). However, RLHF is</p>
<p>sensitive to the training details and is complicated to implement. To get rid of the RL, <em>Lu et al. (2022)</em> use quantized rewards as control tokens to continually fine-tune policy models; <em>Diao et al. (2023); Dong et al. (2023); Gulcehre et al. (2023)</em> use reward models to filter out high-reward generations for further fine-tuning. Some approaches are even simpler and do not require reward models. <em>Liu et al. (2023)</em> use human feedback as control tokens directly to fine-tune models on model generations. <em>Sun et al. (2023)</em> use human-written principles to guide language models to generate helpful and harmless responses and use generated responses to fine-tune models. <em>Zhao et al. (2023)</em> use a calibration loss to mimic RLHF. <em>Rafailov et al. (2023)</em> designs a loss function that can be theoretically proved to be identical to RLHF. <em>Xiong et al. (2023)</em> shows that online alignment is much better than offline alignment. We used RLHF for alignment in our paper, as it is the most commonly used method.</p>
<p>Self-improvement Self-improvement enables language models to improve themselves without extra human effort. Moreover, the self-improved responses can then be utilized for context distillation (<em>Askell et al., (2021)</em>) to update LLMs. <em>Huang et al. (2022)</em> use PaLM (<em>Chowdhery et al., (2022)</em>) to label more task-specific data and use the most confident labeled data to continue fine-tuning PaLM itself. However, fine-tuning language models on specific tasks may lower the overall generation performance of models (<em>Zhai et al., (2023)</em>). Therefore, researchers also put much effort into self-improvement without modifying language models themselves. <em>Shinn et al. (2023)</em> put language models into an environment and let language models reflect their actions when they meet failures. <em>Wang et al. (2023a)</em> use the Python interpreter’s error message to improve language models’ code generation abilities. However, these methods require environments that can provide automatic feedback, which may not be accessible in general. Self-Refine (<em>Madaan et al., (2023)</em>) enables LLMs to reflect on their responses and provide feedback, then ask LLMs to use feedback to improve responses. <em>Zeng et al. (2023)</em> apply Self-Refine to generate better meta-reviews for papers. <em>Xue et al. (2023)</em> guide LLMs to fine-grained feedback for math problems. The feedback could also benefit from tools. <em>Zhou et al. (2023)</em> use OpenAI’s code interpreter to collect more accurate feedback. <em>Wang et al. (2023b)</em> propose a benchmark to evaluate how models can get improved from human or AI feedback. Nevertheless, these methods require explicit prompting to collect feedback and self-improve. Our method does not require modifying model weights, interactive environment, or explicitly prompting.</p>
<h2>3 Method</h2>
<p>Policy models (i.e., “LLMs” in Figure 1) trained with RLHF generate reference responses for given inputs, whereas PIT takes the given inputs and reference responses as its inputs and generates improved responses. The different input formats between policy models and PIT (See Appendix A for details) requires reformulating RLHF training objectives to train PIT. We follow and reformulate RLHF steps proposed by <em>Ouyang et al. (2022)</em>: supervised fine-tuning, reward model training, and reinforcement learning. Although there are many other alignment methods such as Direct Preference Optimization (<em>Rafailov et al., (2023)</em>) or Preference Ranking Optimization (<em>Song et al., (2023)</em>), we choose RLHF to train policy models and PIT for two reasons: (1) RLHF is the most widely used approach for alignment (2) PIT is not aimed for alignment but for triggering self-improvement. Therefore, the alignment method is not the focus of this paper.</p>
<h3>3.1 Formulation</h3>
<p>Suppose we have data $\mathcal{D}={\left(x,y_{l},y_{w}\right)}<em l="l">{3n}$, where $x$ is the input prompt, $y</em>}$ is the worse model generation, and $y_{w}$ is the better model generation, annotated by humans. We could equally divide the data into three folds $\mathcal{D<em _text_RM="\text{RM">{\text{SFT}}$, $\mathcal{D}</em>}}$ and $\mathcal{D<em _mathrm_P="\mathrm{P">{\text{RL}}$ for the supervised fine-tuning, reward model training, and reinforcement learning, respectively. The policy model $\mathrm{M}</em>}}$ is trained to generate response $y_{\mathrm{P}}$ for the given input $x$: $y_{\mathrm{P}} \sim \mathrm{M<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}(\cdot \mid x)$. PIT model $\mathrm{M}</em>}}$ is trained to generate improved response $y_{\mathrm{PIT}}$ for given input $x$ and reference response $y_{\text {ref }}: y_{\mathrm{PIT}} \sim \mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}(\cdot \mid x, y</em>$ to denote PIT, and use superscripts such as SFT or RL to denote different checkpoints of models.}})$. In the following sections, we use subscript ${ }^{-\mathrm{P}}$ to denote the policy model and ${ }^{-\mathrm{PIT}</p>
<h3>3.2 Supervised Fine-tuning</h3>
<p>Supervised fine-tuning (SFT) is the pre-requisite step of RLHF (<em>Ouyang et al., (2022)</em>). SFT only uses $x$ and $y_{w}$, and $y_{w}$ could also be human-written responses <em>Ouyang et al. (2022)</em>. To train policy models, we follow <em>Ouyang et al. (2022)</em> and simply maximize the likelihood of $y_{w}$ : $\mathcal{L}<em _left_x_y__l="\left(x,y_{l">{\mathrm{P}}^{\mathrm{SFT}}=$ $-\sum</em>},y_{w}\right) \in \mathcal{D<em _mathrm_P="\mathrm{P">{\mathrm{SFT}}} \log \mathrm{M}</em> \mid x\right)$.}}\left(y_{w</p>
<p>Since PIT aims to improve reference responses, we need to include $y_{l}$ to train PIT: $\mathcal{L}<em _left_x_="\left(x," y__l="y_{l">{\text {PIT }}^{\text {SFT }}=$ $-\sum</em>}, y_{w}\right) \in \mathcal{D<em _PIT="{PIT" _text="\text">{\text {SFT }}} \log \mathrm{M}</em>}}\left(y_{w} \mid x, y_{l}\right)$. A natural extension for this loss function is that we can apply unlikelihood (Welleck et al., 2019) loss to $\mathrm{M<em l="l">{\text {PIT }}\left(y</em>} \mid x, y_{w}\right), \mathrm{M<em l="l">{\text {PIT }}\left(y</em>} \mid x, y_{l}\right)$ and $\mathrm{M<em w="w">{\text {PIT }}\left(y</em>\right)$. However, we find the unlikelihood loss causes performance degradation in experiments, therefore we only keep likelihood loss when training PIT with SFT.} \mid x, y_{w</p>
<h1>3.3 Reward Model Training</h1>
<p>The reward model is used to judge how good a response is and is the key component for reinforcement learning. A reward model denoted as $\mathrm{R}<em _mathrm_P="\mathrm{P">{\mathrm{P}}$ maps an input $x$ and the corresponding response $y$ to a scalar $r: r(x, y)=\mathrm{R}</em>}}(x, y)$. Since the data $\mathcal{D<em l="l">{\mathrm{RM}}$ only contains preference ( $y</em>}$ and $y_{w}$ ) and does not provide $r$ directly, Ouyang et al. (2022) trains the reward model by maximizing the reward gap between $y_{l}$ and $y_{w}: \mathcal{L<em _mathcal_D="\mathcal{D">{\mathrm{P}}^{\mathrm{RM}}=-\sum</em><em w="w">{\mathrm{RM}}} \log \sigma\left(r</em>}-r_{l}\right)$, where $r_{w}$ and $r_{l}$ are rewards, i.e., $\mathrm{R<em w="w">{\mathrm{P}}\left(x, y</em>}\right)$ and $\mathrm{R<em l="l">{\mathrm{P}}\left(x, y</em>}\right)$, respectively. Unlike $\mathrm{R<em _PIT="{PIT" _text="\text">{\mathrm{P}}$, the reward model $\mathrm{R}</em>}}$ does not focus on rewards $r$ of responses but cares about the gap $r_{\text {gap }}$ between responses: $r_{\text {gap }}\left(x, y_{1}, y_{2}\right)=\mathrm{R<em 1="1">{\text {PIT }}\left(x, y</em>}, y_{2}\right)$. Therefore, $\mathrm{R<em w="w">{\text {PIT }}$ needs to learn $\left(r</em>$. To formulate the loss function, we can use the fact that}-r_{l}\right)$ from the training data $\mathcal{D}_{\mathrm{RM}</p>
<p>$$
r_{\text {gap }}\left(x, y_{w}, y_{l}\right) \geq r_{\text {gap }}\left(x, y_{w}, y_{w}\right) \approx r_{\text {gap }}\left(x, y_{l}, y_{l}\right) \geq r_{\text {gap }}\left(x, y_{l}, y_{w}\right)
$$</p>
<p>To ensure that the reward model performs as expected in Equation 1, after training, we consider all pairwise relations mentioned above, and the loss function becomes:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{L}<em _mathcal_D="\mathcal{D">{\text {PIT }}^{\mathrm{RM}}=-\sum</em><em _gap="{gap" _text="\text">{\mathrm{RM}}}\left[\log \sigma\left(r</em>\right)\right. \
&amp; \left.+\log \sigma\left(r_{\text {gap }}^{w, w}-r_{\text {gap }}^{l, w}\right)+\log \sigma\left(r_{\text {gap }}^{l, l}-r_{\text {gap }}^{l, w}\right)\right],
\end{aligned}
$$}}^{w, l}-r_{\text {gap }}^{w, w}\right)+\log \sigma\left(r_{\text {gap }}^{w, l}-r_{\text {gap }}^{l, l}\right)+\log \sigma\left(r_{\text {gap }}^{w, l}-r_{\text {gap }}^{l, w</p>
<p>where $r_{\text {gap }}^{w, l}$ is the shortcut of $r_{\text {gap }}\left(x, y_{w}, y_{l}\right)$, etc. Although there are other options to model the reward gap, such as computing the rewards subtraction through $\mathrm{R}_{\mathrm{P}}$, we find Equation 2 is the best fit. More discussions could be found in Appendix C.</p>
<h3>3.4 REINFORCEMENT LEARNING</h3>
<p>Reinforcement learning (RL) finds a policy that maximizes expected rewards over time, aligning LLMs with human preferences. The optimization goal for RL is (Ouyang et al., 2022):</p>
<p>$$
\text { Optimization }<em _mathcal_D="\mathcal{D">{\mathrm{P}}^{\mathrm{RL}}=\sum</em><em _mathrm_P="\mathrm{P">{\mathrm{RL}}}\left[r(x, y)-\beta \log \frac{\mathrm{M}</em>\right]
$$}}^{\mathrm{RL}}(y \mid x)}{\mathrm{M}_{\mathrm{P}}^{\mathrm{SFT}}(y \mid x)</p>
<p>where $r(x, y)=\mathrm{R}<em _mathrm_P="\mathrm{P">{\mathrm{P}}(x, y)$ and $y \sim \mathrm{M}</em>}}^{\mathrm{RL}}(\cdot \mid x) . \mathrm{M<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{RL}}$ is the policy model to be optimized, which is initialized to $\mathrm{M}</em>}}^{\mathrm{SFT}} . \mathrm{M<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{SFT}}$ is $\mathrm{M}</em>$ trained in supervised fine-tuning, which is fixed in the RL. The KL divergence is used to restrict dramatic weight changes and reward hacking in RL.}</p>
<h3>3.4.1 Curriculum REINFORCEMENT LEARNING</h3>
<p>Different from Equation 3, PIT aims to improve a reference response $y_{\text {ref }}$ instead of generating a response from scratch. Therefore, the RL for PIT utilizes $x$ and $y_{\text {ref }}$ simultaneously. The difficulty is how to choose $y_{\text {ref }}$. An intuitive way is to use $y_{l}$ and $y_{w}$ provided in the dataset, and the optimization goal becomes:</p>
<p>$$
\text { Optimization }<em _mathcal_D="\mathcal{D">{\mathrm{PIT}}^{\mathrm{RL}, 0}=\sum</em><em y__mathrm_ref="y_{\mathrm{ref">{\mathrm{RL}}} \sum</em>}} \in\left{y_{l}, y_{w}\right}}\left[r_{\text {gap }}\left(x, y, y_{\text {ref }}\right)-\beta \log \frac{\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}\left(y \mid x, y</em>}}\right)}{\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{SFT}}\left(y \mid x, y</em>\right]
$$}}\right)</p>
<p>However, PIT aims to improve $\mathrm{M}<em l="l">{\mathrm{P}}^{\mathrm{RL}}$ responses, and $y</em>}$ and $y_{w}$ are chosen from the annotated data, not sampled from the $\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{P}}^{\mathrm{RL}}$. Therefore, we need to do another round of reinforcement learning, where $y</em>$ is sampled from the policy model:}</p>
<p>$$
\text { Optimization }<em _mathcal_D="\mathcal{D">{\mathrm{PIT}}^{\mathrm{RL}, 1}=\sum</em><em y__mathrm_ref="y_{\mathrm{ref">{\mathrm{RL}}} \sum</em>}} \sim \mathrm{M<em _gap="{gap" _text="\text">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid x)}\left[r</em>}}\left(x, y, y_{\text {ref }}\right)-\beta \log \frac{\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}\left(y \mid x, y</em>}}\right)}{\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{SFT}}\left(y \mid x, y</em>\right]
$$}}\right)</p>
<p>The flexibility of $y_{\text {ref }}$ in fact enables us to do multiple rounds of reinforcement learning to improve PIT further. For example, a third round Optimization ${ }<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}, 2}$ can sample $y</em>}} \sim \mathrm{M<em 1="1">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>}\right)$, where $y_{1} \sim \mathrm{M<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid x)$ with other terms unchanged. The third round enables PIT to improve the improved $\mathrm{M}</em>$ responses. In principle, this process could be extended to infinite rounds. We denote this process as curriculum reinforcement learning.}}^{\mathrm{RL}</p>
<h1>3.4.2 Discussions on Curriculum Reinforcement Learning</h1>
<p>It is worth noting that the first round Optimization $\mathrm{RL}<em 0="0">{0}{ }</em>}$ is necessary since the optimization Optimization $\mathrm{RL<em 1="1">{1}{ }</em>}$ is too hard to be optimized directly. This is because responses sampled from $\mathrm{M<em l="l">{\mathrm{P}}^{\mathrm{RL}}$ are already of high quality (higher than $y</em>}$ and $y_{w}$ in the data, as shown in Section 4.6), and are harder to be improved compared to $y_{l}$ and $y_{w}$. Therefore, curriculum reinforcement learning is needed to ensure the success of optimizing Optimization $\mathrm{RL<em 1="1">{1}{ }</em>}$. We find that removing Optimization $\mathrm{RL<em 0="0">{0}{ }</em>}$ and directly optimize Optimization $\mathrm{RL<em 1="1">{1}{ }</em>}$ will make PIT fail to improve $\mathrm{M<em 0="0">{\mathrm{P}}^{\mathrm{RL}}$ responses (Section 4.6). Besides, purely training PIT on Optimization $\mathrm{RL}</em>}{ <em _mathrm_P="\mathrm{P">{0}$ will also lead to a disaster since PIT never saw data sampled from $\mathrm{M}</em>}}^{\mathrm{RL}}$ (Section 4.6). In fact, we could even insert several intermediate rounds between the first and second rounds to build a smoother curriculum. Concretely, we could choose $y_{\text {ref }}$ sampled from intermediate checkpoints of $\mathrm{M<em 0="0">{\mathrm{P}}^{\mathrm{RL}}$ (e.g., checkpoints dumped during training and have not been optimized for high rewards yet) since their responses will be less perfect and easier to be improved. In our experiments, we use Optimization $\mathrm{RL}</em>}{ <em 1="1">{0}$ and Optimization $\mathrm{RL}</em>$ to optimize PIT. This is because we find the dataset is not hard enough to require more rounds and intermediate rounds. The algorithm block of PIT can be found in the Algorithm 1.}{ }_{1</p>
<h3>3.5 SELF-IMPROVEMENT INFERENCE</h3>
<p>Given the input $x$, we first sample $y_{\text {ref }} \sim \mathrm{M}<em 1="1">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid x)$. Then we can get improved response $y</em>} \sim$ $\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>}}\right)$. The improvement process could be repeated infinite times. For examples, we could get $y_{2} \sim \mathrm{M<em 1="1">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>}\right)$ and $y_{3} \sim \mathrm{M<em 2="2">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>\right)$, etc. The self-improvement inference process is summarized in the Algorithm 2. Moreover, PIT does not bring extra computational overhead compared with prompting methods such as Self-Refine (Madaan et al., 2023). Detailed discussions can be found in Appendix B.</p>
<h2>4 EXPERIMENTS</h2>
<p>Our experiments are designed to answer two questions: (1) Can our method improve the quality of the original response? The term 'quality' here is defined by the preference data. For example, if the preference data is to get a more helpful and less harmless response, then 'higher quality' denotes more helpful and less harmless. (2) Can our method better improve the response than prompting methods such as Self-Refine?</p>
<h3>4.1 DATASETS</h3>
<p>We use three diverse datasets for our experiments:
Anthropic/HH-RLHF. The HH-RLHF dataset (Bai et al., 2022; Ganguli et al., 2022) is released by Anthropic and is allowed for research purposes, which contains 161 K conversation pairs between humans and AI assistants for training and 8.55 K for testing. The dataset aims to train a helpful and harmless AI assistant and has two subsets: the helpful subset and the harmless subset. In our experiment, we only use the helpful subset and divide the subset equally into three folds for supervised fine-tuning, reward model training, and reinforcement learning.</p>
<p>OpenAI/Summary The OpenAI/Summary (Stiennon et al., 2020a), which is also allowed for research purposes, contains Reddit posts and two summaries for each post with human preferences. It contains 92.9 K training data and 86.1 K validation data. Similarly to the Anthropic/HH-RLHF dataset, we equally divide the training dataset into three folds for supervised fine-tuning, reward model training, and reinforcement learning, respectively.</p>
<p>Synthetic Data To test the instruction following abilities of language models, we use PaLM 2 (Unicorn) (Anil et al., 2023) to generate 13K synthetic data. Every synthetic data includes a question with multiple requirements, such as using a specific format for writing responses. It also includes a satisfactory response that meets all of the requirements, as well as an unsatisfactory response that only meets some of the requirements. Lastly, there is a reflection on how to improve the unsatisfactory response to make it satisfactory (not used in our experiments). Similarly, we divide synthetic data into three folds equally and use 128 examples for evaluation. The synthetic data generation process and examples can be found in Appendix D.</p>
<h1>4.2 Settings</h1>
<p>We use pre-trained PaLM 2 (Bison) as our backbone language model and reward model since we find smaller models’ (Gecko and Otter) generation abilities are poorer, and larger models (Unicorn) are too expensive to train. We train our models on TPU v4 <em>(Jouppi et al., 2023)</em>. In our experiments, we compare PIT with other self-improvement methods using prompts, specifically Self-Refine <em>(Madaan et al., 2023)</em>. Self-Refine uses prompts (Appendix E.1) to instruct the model $\mathrm{M}<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{RL}}$ to reflect the current response and give feedback to improve the response, then asks $\mathrm{M}</em>$ to give an improved response based on the feedback. We set the sampling temperature to be 1 for all models during inference unless stated otherwise. More experiment settings can be found in Appendix F.}}^{\mathrm{RL}</p>
<h3>4.3 Evaluation Models</h3>
<p>We use third-party LLMs and reward models that are allowed to be used for research as evaluators, similar to previous works <em>(Rafailov et al., 2023; Gulcehre et al., 2023; Dong et al., 2023)</em>. LLMs are sensitive to the rubric and need carefully designed prompts to achieve good performance for evaluation <em>(Rafailov et al., 2023)</em>, which is also aligned with our motivation. Reward models may have reward hacking phenomenon <em>(Skalse et al., 2022)</em>. To get a reliable evaluation, we use both thirdparty LLMs and reward models to do evaluations and use human evaluations when the two evaluations disagree. Evaluation prompts for LLMs are adopted from <em>Zheng et al. (2023); Rafailov et al. (2023)</em> and are shown in Appendix E.2. We compute the agreement of LLMs with ground-truth validation data (Appendix G), and GPT-4 has the highest agreement compared to ChatGPT and PaLM 2 (Unicorn) and will be used as the representative of third-party language models to do evaluations afterward. We use DeBERTa-Large (304M) <em>(He et al., 2020)</em> trained by Open Assistant ${ }^{1}$ as the representative of reward models because it uses different model architectures (compared with $\mathrm{R}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}$ and $\mathrm{R}</em>}}$ ) and full training data ( $\mathrm{R<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}$ and $\mathrm{R}</em>}}$ only use 1/3 data) and is not used to optimize models ( $\mathrm{R<em _mathrm_P="\mathrm{P">{\mathrm{PIT}}$ and $\mathrm{R}</em>$ are used for optimization), reducing the risk of reward hacking during evaluations. Moreover, it performs similarly to GPT-4 on the ground-truth validation data (Appendix G).}</p>
<p>We also compare the agreement of $\mathrm{R}<em _mathrm_P="\mathrm{P">{\mathrm{PIT}}$ and $\mathrm{R}</em>}}$ with the ground-truth labels on validation sets (Appendix G), and conclude $\mathrm{R<em _mathrm_P="\mathrm{P">{\mathrm{PIT}}$ generally outperforms $\mathrm{R}</em>$ reward distribution on the synthetic dataset (since reward models perform best on this dataset among others and the distribution pattern is more noticeable) in Figure 2. The distribution exactly follows our loss design in Equation 1. Moreover, we observe that the reward is negative when two responses are identical, indicating that PIT penalties repeating responses (or generating responses that have the same qualities as reference responses).}}$, as expected. We then draw $\mathrm{R}_{\mathrm{PIT}</p>
<h3>4.4 ReSults</h3>
<p>Since GPT-4 API is expensive and slow, we evaluate 128 examples when using GPT-4 and 1,000 examples when using the DeBERTa reward model. To reduce the noise, the reward model will give a tie if the rewards for two responses are similar. In practice, the reward model gives a tie if $\sigma\left(r_{1}-r_{2}\right) \in[0.45,0.55]$, where $r_{1}$ and $r_{2}$ denote the reward of two responses. We first sample responses $y_{\text {ref }} \sim \mathrm{M}<em _ref="{ref" _text="\text">{\mathrm{P}}^{\mathrm{RL}}(x)$ (denoted as 'Original Responses') and then use self-improvement methods to improve $y</em>$. Appendix H. 1 shows several qualitative examples.}</p>
<p>PIT improves responses’ qualities. We apply PIT to original responses and get improved responses. Then, we report the win rate of PIT against original responses. We use the difference between the win rate and the lose rate ( $\Delta$ for short) to denote the performance of PIT. Table 1 shows results. First, we find that the original responses are much better than $y_{w}$ in data, showing that $\mathrm{M}<em w="w">{\mathrm{P}}^{R L}$ is well optimized (this conclusion does not hold on synthetic data, which is expected since a much larger LLM produces $y</em>$ ). We can see PIT consistently has better qualities (ranging from $7.2 \%$ to $33.59 \%$ )</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparisons among $y_{w}$ in data, original responses, improved responses by Self-Refine, and improved responses by PIT on three datasets. Win rate / Lose rate denotes the percentage of the former model’s responses that is better / worse than the latter’s. Their difference $\Delta&gt;0$ denotes that the former response is better and vice versa. Higher $|\Delta|$ denotes a higher performance gap.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Comparison</th>
<th>Win rate / Lose rate / $\Delta$ (\%)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GPT-4</td>
<td>DeBERTa</td>
<td>Human Evaluation</td>
</tr>
<tr>
<td>Anthropic/HH-RLHF</td>
<td>Original vs. $y_{w}$</td>
<td>71.85/17.19/54.69</td>
<td>68.20/18.00/50.20</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PIT vs. Original</td>
<td>55.47/27.34/28.13</td>
<td>46.30/32.30/14.00</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Self-Refine vs. Original</td>
<td>60.94/17.19/43.75</td>
<td>40.30/31.40/8.90</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PIT vs. Self-Refine</td>
<td>38.28/42.19/ -3.91</td>
<td>41.3/37.60/3.70</td>
<td>47.06/23.53/23.53</td>
</tr>
<tr>
<td>OpenAI/Summary</td>
<td>Original vs. $y_{w}$</td>
<td>74.22/8.59/65.63</td>
<td>84.90/10.70/74.20</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PIT vs. Original</td>
<td>44.53/24.22/20.31</td>
<td>41.9/34.7/7.2</td>
<td>-</td>
</tr>
<tr>
<td>Synthetic Data</td>
<td>Original vs. $y_{w}$</td>
<td>28.91/51.56/ -22.66</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PIT vs. Original</td>
<td>48.44/14.84/33.59</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Self-Refine vs. Original</td>
<td>34.38/17.97/16.41</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>PIT vs. Self-Refine</td>
<td>45.31/35.16/10.16</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>than original responses across three datasets with both GPT-4 and DeBERTa evaluators, showing the effectiveness of our method. Since DeBERTa performs poorly and GPT-4 has a high agreement with the ground-truth labels on the synthetic data (Appendix G), we only use GPT-4 as the evaluator for the synthetic data. It is worth noting that the summation of the win rate and the lose rate is less than 1 because of the existence of the tie rate.</p>
<p>PIT improves response quality better than Self-Refine. We then compare Self-Refine with PIT and original responses. Table 1 shows the comparison results on the Anthropic/HH-RLHF dataset. GPT-4 and DeBERTa both agree that Self-Refine indeed improves response qualities. However, GPT-4 prefers Self-Refine more (3.91% better than PIT), whereas DeBERTa prefers PIT more (3.70% better than Self-Refine). This is understandable since GPT-4 and Self-Refine use manual prompts to evaluate or improve responses, making them prefer each other. On the contrary, DeBERTa and PIT are trained on the same data. Therefore, we conduct human evaluations to determine which is better and find that human prefers PIT more (23.53% better than Self-Refine). Appendix I shows details of human evaluations. The disagreement between GPT-4 and humans arises from GPT-4’s inclination to long and detailed responses and Self-Refine’s preference to generate such responses. Nonetheless, these responses can include irrelevant information not related to the questions humans raised, which is not desirable to humans. Appendix H.2 contains one concrete example of this phenomenon. Table 1 also shows results on the synthetic dataset, where we can easily conclude that PIT outperforms Self-Refine, though they all improve original responses.</p>
<h3>4.5 THE INFLUENCE OF IMPROVEMENT TEMPERATURES</h3>
<p>In the above experiments, we set the temperature to be 1 for $\mathrm{M}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}^{\mathrm{RL}}$, $\mathrm{M}</em>$ and Self-Refine. However, higher temperatures often represent diversities and randomness, while lower temperatures usually represent coherence and qualities. Intuitively, self-improvement is expected to improve the original response along one most reliable improvement direction rather than randomly explore directions (which many harm the improvement performance). Therefore, a low temperature may be preferable to restrict the diversity of the improvement. Fig 3 shows the difference between win rates and lose rates among the original responses, improved responses by Self-Refine, and improved responses by PIT under different temperatures. The original responses are generated with the temperature 1, and improved responses are generated with different temperatures, as the figure shows. To achieve cost-effectiveness, we have opted for DeBERTa as the evaluator to assess 1,000 instances of Anthropic/HH-RLHF and OpenAI/Summary. We made this choice because, as indicated in Table 1 and Section 4.4, DeBERTa correlates more closely with human preferences than GPT-4 does. Nevertheless, for the evaluation of 128 examples of synthetic data, we will rely on GPT-4 as DeBERTa is not ideal for this purpose, as shown in Appendix G. We do not report the results of Self-Refine for OpenAI/Summary since the dataset only contains summarization instructions, making Self-Refine not applicable.}}^{\mathrm{RL}</p>
<p>On Anthropic/HH-RLHF dataset, we find our method improves the original responses most with low temperatures (0.4 $\sim$ 0.6), which fits our assumptions proposed at the beginning of this section.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The difference between win rate and lose rate $(\Delta)$ among original responses, improved responses by Self-Refine, and improved responses by PIT under different temperatures. $\Delta&gt;0$ denotes the former response is better, and higher $|\Delta|$ denotes larger performance gap.</p>
<p>Table 2: The effect of curriculum reinforcement learning. A great performance drop is observed when we only use RL once. $\Delta&lt;0$ represents 'XX RL Only' response quality is worse than the compared method.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Evaluator: DeBERTa; Dataset: Anthropic/HH-RLHF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Win rate / Lose rate / $\Delta(\%)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">First RL Only</td>
<td style="text-align: center;">Second RL Only</td>
</tr>
<tr>
<td style="text-align: center;">vs. Original</td>
<td style="text-align: center;">$40.50 / 36.10 / 4.40$</td>
<td style="text-align: center;">$28.90 / 28.70 / 0.20$</td>
</tr>
<tr>
<td style="text-align: center;">vs. Self-Refine</td>
<td style="text-align: center;">$32.0 / 47.7 /-15.70$</td>
<td style="text-align: center;">$28.50 / 47.40 /-18.90$</td>
</tr>
<tr>
<td style="text-align: center;">vs. PIT</td>
<td style="text-align: center;">$20.70 / 50.80 /-30.10$</td>
<td style="text-align: center;">$19.30 / 56.00 /-36.70$</td>
</tr>
</tbody>
</table>
<p>Table 3: ELO scores (higher is better) of different improvement iterations on 1,000 conversations in Anthropic/HH-RLHF and 1,000 posts in OpenAI/Summary. Evaluator: DeBERTa.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Anthropic/HH-RLHF</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Responses</td>
<td style="text-align: center;">ELO</td>
<td style="text-align: center;">Rank</td>
</tr>
<tr>
<td style="text-align: center;">PIT (Iter 4)</td>
<td style="text-align: center;">1036</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">PIT (Iter 1)</td>
<td style="text-align: center;">1033</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">PIT (Iter 5)</td>
<td style="text-align: center;">1025</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">PIT (Iter 3)</td>
<td style="text-align: center;">1018</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">PIT (Iter 2)</td>
<td style="text-align: center;">1016</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine (Iter 1)</td>
<td style="text-align: center;">1001</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine (Iter 3)</td>
<td style="text-align: center;">995</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine (Iter 4)</td>
<td style="text-align: center;">990</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine (Iter 2)</td>
<td style="text-align: center;">982</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: center;">Self-Refine (Iter 5)</td>
<td style="text-align: center;">982</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">921</td>
<td style="text-align: center;">11</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Rewards w.r.t. training steps. Rewards are divided into three regions. The Better region denotes the model can improve original responses; The Similar region suggests the model can only generate responses that are on par with original responses; The Worse denotes the model cannot improve original responses and make original responses worse.</p>
<p>However, Self-Refine improves the original responses most with high temperatures ( $0.6 \sim 0.8$ ). We find that this is because Self-Refine tends to keep all the original response and only append extra details to the original response with low temperatures, leading to sub-optimal improvement. Corresponding examples can be found in Appendix H.3. OpenAI/Summary shows similar observations, whereas PIT is not heavily affected by temperatures on the synthetic data. This is probably because we only evaluate the instruction following ability and do not evaluate the concrete content quality on the synthetic data. Nevertheless, PIT outperforms Self-Refine under all temperatures (except a tie under the temperature 0.8 on the Anthropic/HH-RLHF dataset). Moreover, we compare PIT and SelfRefine under the best temperatures on Anthropic/HH-RLHF(i.e., 0.4 for PIT and 0.8 for Self-Refine), and find PIT still outperforms Self-Refine by $9.2 \%$.</p>
<h1>4.6 THE EFFECT OF CURRICULUM REINFORCEMENT LEARNING</h1>
<p>We conduct reinforcement learning twice in PIT , with the difficulty increases, as illustrated in Section 3.4.1. The first RL (Equation 4) aims to teach models to improve ground-truth responses, whereas the second RL (Equation 5) aims to teach models to improve $\mathrm{M}_{\mathrm{P}}^{\mathrm{RL}}$ responses. To demonstrate the importance of curriculum reinforcement learning, we trained two additional PIT variants. One model was optimized using the first RL, while the other model skipped the first RL and directly used the</p>
<p>second RL. We compare the two extra models with PIT , Self-Refine conducted by $\mathrm{M}<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{RL}}$, and original responses sampled from $\mathrm{M}</em>$ ). Examples can be found in Appendix H.4.}}^{\mathrm{RL}}$ on the Anthropic/HH-RLHF datasets. Table 2 shows that both models can only improve original responses marginally and have a large performance gap when compared to Self-Refine and PIT. The results are expected since the first model does not learn how to improve the original responses but is only trained on easier ground-truth data. On the contrary, the second model is directly facing a hard optimization problem (Table 1 shows that original responses are much better than $y_{w</p>
<p>We also show rewards w.r.t. training steps when training $\mathrm{M}<em _PIT="{PIT" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}$ with Equation 5 and training a PIT variant with second RL only in Figure 4. Rewards are given by $\mathrm{R}</em>}}\left(x, \cdot, y_{\text {ref }}\right)$, where $y_{\text {ref }} \sim$ $\mathrm{M<em _ref="{ref" _text="\text">{\mathrm{P}}^{\mathrm{RL}}(x)$ is the original response. It is clear that the model with second RL only struggles to improve $y</em>$, as expected.}}$ due to the hard optimization, whereas PIT can break through the Similar area thanks to the first RL. Besides, the starting point of PIT in the Figure 4, i.e., the PIT variant with the first RL only, fails to improve $y_{\text {ref }</p>
<h1>4.7 THE EFFECT OF IMPROVEMENT ITERATIONS</h1>
<p>Knowing when to stop self-improvement is crucial since the improvement process could be iterated infinitely in principle. The stop condition depends on the actual needs. For example, the stop condition could be a fixed wall time or the significance of each improvement (i.e., stop further improvement if the current improvement is marginal). In this section, we only focus on the response qualities and investigate if the response qualities become better with more improvement iterations. If the answer is yes, then more iterations are always welcomed. Otherwise, stop conditions need to be carefully designed in real applications. We use ELO scores (Zheng et al., 2023) to show quantitative results among models.</p>
<p>We use PIT and Self-Refine (with their best temperatures respectively) to improve 1,000 original responses for 5 iterations on Anthropic/HH-RLHF, meaning each conversation now has 11 responses. We then use the DeBERTa reward model to compare 11 responses (Appendix H. 5 shows an example), resulting $C_{11}^{2}=55$ comparisons for each conversation and 55,000 comparisons in total. We then use 55,000 comparisons to compute ELO scores ${ }^{3}$ and report results in Table 3. We get ELO scores on OpenAI/Summary similarly, except for the difference that Self-Refine is not applicable to this dataset. Table 3 shows consistent results with previous experiments that PIT (Iter 1) is better than Self-Refine (Iter 1), and the original responses have the lowest qualities. Intuitively, the best iteration for PIT should be consistent with the number of RL in curriculum reinforcement learning. However, Table 3 shows that the response qualities and the number of iterations do not follow this intuition as PIT (Iter 1) is not best. Moreover, there is not a simple positive correlation between response qualities and improvement iterations. One possible explanation may be that the datasets are too easy and do not need more iterations. This phenomenon suggests we develop stop conditions in real applications carefully. We observe that sometimes the self-improvement yields the same responses as its previous iteration with low temperatures, which could potentially serve as a stop condition. An example is shown in Appendix H.5. We can also observe that PIT is consistently better than Self-Refine regardless of the number of iterations. Therefore, PIT is a better self-improvement method than Self-Refine regardless of the stop condition.</p>
<p>Since ELO scores are affected by the order of comparisons, we randomly shuffle comparisons several times and observe the change of ranks to ensure our conclusion is not caused by randomness. We find that PIT is always better than Self-Refine, and the original responses are the worst among 11 responses, but the internal rank of PIT and Self-Refine may be changed with different shuffles, which does not affect our conclusions above.</p>
<h2>5 CONCLUSION</h2>
<p>We propose PIT, a novel approach that learns self-improvement implicitly from data.PIT does not require explicit prompting and extra data. Extensive experiments show the effectiveness of PIT on selfimprovement compared with prompting methods such as Self-Refine. We highlight our limitations and future work in Appendix J.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>REPRODUCIBILITY</h1>
<p>Datasets has been discussed in 4.1. Two of three datasets (Anthropic/HH-RLHF and OpenAI/Summary) are publicly available and can be easily found on the HuggingFace website. Appendix D describes the generation process of the remaining synthetic dataset. Evaluators are discussed in Section 4.3 and Appendix G. Moreover, Appendix E. 2 offers the evaluation prompts. Experiment settings, baselines, and details are discussed in Section 4.2 and Appendix F. Appendix E. 1 also offers prompts used for the baseline method.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>We would like to acknowledge our Google colleagues for their invaluable advice and support. In particular, we thank Music Li (Yuezhang Li) for insightful discussions and manual evaluation. We thank Tianqi Liu, Honglong Cai, and Albert Webson for their constructive advice, and Léonard Hussenot and Robert Dadashi for building RLHF infra. Finally, we would like to acknowledge Melvin Johnson, Hongkun Yu, and Denny Zhou for their support throughout the project. We also thank the anonymous reviewers for their suggestions and comments.</p>
<p>This research is also based upon work supported by U.S. DARPA ECOLE Program No. HR00112390060 and U.S. DARPA ITM Program No. FA8650-23-C-7316 and KAIROS Program No. FA8750-19-2-1004. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>REFERENCES</h2>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. Lmflow: An extensible toolkit for finetuning and inference of large foundation models. arXiv preprint arXiv:2306.12420, 2023.</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023.</p>
<p>Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.</p>
<p>Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.</p>
<p>Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, pp. 1-14, 2023.</p>
<p>Sha Li, Ruining Zhao, Manling Li, Heng Ji, Chris Callison-Burch, and Jiawei Han. Open-domain hierarchical event schema induction by incremental prompting and verification. In Proc. The 61st Annual Meeting of the Association for Computational Linguistics (ACL2023), 2023.</p>
<p>Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676, 2023.</p>
<p>Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Advances in neural information processing systems, 35:27591-27609, 2022.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>OpenAI. Gpt-4 technical report. arXiv, pp. 2303-08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023a.</p>
<p>Baolin Peng, Linfeng Song, Ye Tian, Lifeng Jin, Haitao Mi, and Dong Yu. Stabilizing rlhf through advantage model and selective rehearsal. arXiv preprint arXiv:2309.10202, 2023b.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.</p>
<p>John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022.</p>
<p>Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, et al. Mixture-of-experts meets instruction tuning: A winning combination for large language models. arXiv preprint arXiv:2305.14705, 2023.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022.</p>
<p>Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460-9471, 2022.</p>
<p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In NeurIPS, 2020a.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020b.</p>
<p>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047, 2023.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.</p>
<p>Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. Leti: Learning to generate from textual interactions. arXiv preprint arXiv:2305.10314, 2023a.</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback, 2023b.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.</p>
<p>Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf. arXiv preprint arXiv:2312.11456, 2023.</p>
<p>Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. Rcot: Detecting and rectifying factual inconsistency in reasoning by reversing chain-of-thought. arXiv preprint arXiv:2305.11499, 2023.</p>
<p>Qi Zeng, Mankeerat Sidhu, Hou Pong Chan, Lu Wang, and Heng Ji. Meta-review generation with checklist-guided iterative introspection. arXiv preprint arXiv:2305.14647, 2023.</p>
<p>Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023.</p>
<h1>A THE WORKING FLOW</h1>
<p>Given a dataset $\mathcal{D}=\left{\left(x, y_{l}, y_{w}\right)\right}$, we equally divide it into three folds $\mathcal{D}<em _mathrm_RM="\mathrm{RM">{\mathrm{SFT}}, \mathcal{D}</em>}}$ and $\mathcal{D<em _mathrm_pre="\mathrm{pre">{\mathrm{RL}}$. Starting from a pre-trained language model $\mathrm{M}</em>}}(\cdot \mid \cdot)$, we first conduct normal SFT and RLHF on $\mathcal{D}$ to obtain RLHF-finetuned model $\mathrm{M<em _mathrm_P="\mathrm{P">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid \cdot) . \mathrm{M}</em>(\cdot \mid x)$ during training and inference is:}}^{\mathrm{RL}}(\cdot \mid \cdot)$ takes a prompt $x$ as the input (e.g., Please give me a three-day travel plan about Los Angeles.) and return a response $y$. The input format of $\mathrm{M}_{\mathrm{P}}^{\mathrm{RL}</p>
<p>Human: ${x}$ Assistant:
, where ${x}$ denotes the concrete input $x$.
Then we train PIT $\mathrm{M}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{PIT}}^{\mathrm{RL}}(\cdot \mid \cdot, \cdot)$ that takes a prompt and a candidate response as inputs and returns an improved response. The input format of $\mathrm{M}</em>\right)$ is}}^{\mathrm{RL}}\left(\cdot \mid x, y_{\text {ref }</p>
<p>Human: ${x}$ Assistant: $&lt;&lt;$ Candidate $&gt;&gt;\left{y_{\text {ref }}\right}&lt;&lt;$ Improved $&gt;&gt;$
, where $}$ denotes concrete contents and $&lt;&lt;&gt;&gt;$ denotes a special token.
Similarly, the input format of $\mathrm{R}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}(x, y)$ is
Human: ${x}$ Assistant: ${y}$
, and the input format of $\mathrm{R}</em>\right)$ is
Human: ${x}$ Assistant: $&lt;&lt;$ Candidate $&gt;&gt;\left{y_{2}\right}&lt;&lt;$ Improved $&gt;&gt;\left{y_{1}\right}$}}\left(x, y_{1}, y_{2</p>
<h2>Algorithm 1: PIT Training</h2>
<p>Input: Dataset $\mathcal{D}<em _mathrm_RM="\mathrm{RM">{\mathrm{SFT}}, \mathcal{D}</em>}}$ and $\mathcal{D<em _mathrm_pre="\mathrm{pre">{\mathrm{RL}}$, a pre-trained model $\mathrm{M}</em>}}(\cdot \mid \cdot)$, and an RLHF-finetuned model $\mathrm{M<em _mathrm_SFT="\mathrm{SFT">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid \cdot)$ trained on $\mathcal{D}</em>}}, \mathcal{D<em _mathrm_RL="\mathrm{RL">{\mathrm{RM}}, \mathcal{D}</em>}}$ and $\mathrm{M<em _mathrm_PIT="\mathrm{PIT">{\mathrm{pre}}(\cdot \mid \cdot)$.
Output: PIT Model $\mathrm{M}</em>(\cdot \mid \cdot, \cdot)$}}^{\mathrm{RL}</p>
<ul>
<li>$\mathrm{M}<em _mathrm_pre="\mathrm{pre">{\mathrm{PIT}}^{\mathrm{SFT}}(\cdot \mid \cdot, \cdot) \leftarrow \mathrm{M}</em>(\cdot \mid \cdot)$}</li>
<li>for $x, y_{l}, y_{w}$ in $\mathcal{D}_{\text {SFT }} \quad \triangleright$ Supervised fine-tuning</li>
<li>Supervised fine-tuning $\mathrm{M}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{pre}}(\cdot \mid \cdot)$ using loss $\mathcal{L}</em>}}^{\mathrm{SFT}}=-\log \mathrm{M<em w="w">{\mathrm{PIT}}^{\mathrm{SFT}}\left(y</em>\right)$} \mid x, y_{l</li>
<li>$\mathrm{R}<em _mathrm_pre="\mathrm{pre">{\mathrm{PIT}}(\cdot, \cdot, \cdot) \leftarrow \mathrm{M}</em>(\cdot \mid \cdot)$}</li>
<li>for $x, y_{l}, y_{w}$ in $\mathcal{D}_{\mathrm{RM}} \quad \triangleright$ Reward model training</li>
<li>Compute reward gap $\mathrm{R}<em 1="1">{\mathrm{PIT}}\left(x, y</em>\right}$}, y_{2}\right), y_{1} \in\left{y_{l}, y_{w}\right}, y_{2} \in\left{y_{l}, y_{w</li>
<li>Optimize $\mathrm{R}_{\mathrm{PIT}}$ by Equation 2</li>
<li>$\mathrm{M}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{PIT}}^{\mathrm{RL}}(\cdot \mid \cdot, \cdot) \leftarrow \mathrm{M}</em>(\cdot \mid \cdot, \cdot)$}}^{\mathrm{SFT}</li>
<li>while not converge $\quad \triangleright$ First RL</li>
<li>Sample $x, y_{\text {ref }}$ from $\mathcal{D}<em l="l">{\mathrm{RL}}$, where $y \in\left{y</em>\right}$}, y_{w</li>
<li>Generate $y \sim \mathrm{M}<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>\right)$}</li>
<li>Use reinforcement learning algorithm such as PPO to optimize Equation 4</li>
<li>done</li>
<li>while not converge $\quad \triangleright$ Second RL</li>
<li>Sample $x$ from $\mathcal{D}<em _ref="{ref" _text="\text">{\mathrm{RL}}, y</em>(\cdot \mid x)$}} \sim \mathrm{M}_{\mathrm{P}}^{\mathrm{RL}</li>
<li>Generate $y \sim \mathrm{M}<em _ref="{ref" _text="\text">{\mathrm{PIT}}^{\mathrm{RL}}\left(\cdot \mid x, y</em>\right)$}</li>
<li>Use reinforcement learning algorithm such as PPO to optimize Equation 5</li>
<li>done</li>
<li>return $\mathrm{M}_{\mathrm{PIT}}^{\mathrm{RL}}$</li>
</ul>
<p>When conducting self-improvement during inference, we need first to get $y_{\text {ref }} \sim \mathrm{M}<em _mathrm_PIT="\mathrm{PIT">{\mathrm{P}}^{\mathrm{RL}}(\cdot \mid x)$, and then apply $\mathrm{M}</em>$ or Self-Refine to get improved responses. The self-improvement process could be done iteratively during inference. The concrete inference procedure is shown in Algorithm 2.}}^{\mathrm{RL}</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">2</span><span class="p">:</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">Improvement</span><span class="w"> </span><span class="nx">During</span><span class="w"> </span><span class="nx">Inference</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Input</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">x</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">response</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">ref</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sampled</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">RLHF</span><span class="o">-</span><span class="nx">finetuned</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">P</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">RL</span><span class="p">}}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">x</span><span class="p">)</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">PIT</span><span class="w"> </span><span class="nx">model</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">PIT</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">RL</span><span class="p">}}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="p">)</span><span class="err">\</span><span class="p">),</span>
<span class="w">    </span><span class="kp">self</span><span class="o">-</span><span class="nx">improvement</span><span class="w"> </span><span class="nx">iteration</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">Improved</span><span class="w"> </span><span class="nx">responded</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="p">&lt;</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="o">-</span><span class="nx">y</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">PIT</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">RL</span><span class="p">}}(</span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="err">\</span><span class="nx">mid</span><span class="w"> </span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">)[</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">PIT</span><span class="p">}]</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="k">Self</span><span class="o">-</span><span class="nx">Refine</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">y</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">M</span><span class="p">}</span><span class="nx">_</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">P</span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">mathrm</span><span class="p">{</span><span class="nx">RL</span><span class="p">}}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="k">Self</span><span class="o">-</span><span class="nx">Refine</span><span class="p">]</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="o">-</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">k</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="nx">done</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">y</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<h1>B Computational Cost During Inference</h1>
<p>Our approach has the same (or a bit lower) computational overhead compared with Self-Refine. The inference process is shown in Algorithm 2. Here is a detailed explanation:</p>
<p>The inference prompt of Self-Refine looks like this:
Human: Given a prompt ${x}$ and a candidate response $\left{y_{\text {ref }}\right}$ provided by one assistant, you need to improve the response to be more helpful. Helpful means {rubric}. Assistant:</p>
<p>PIT uses a much simpler prompt with only special tokens as splitters:
Human: ${x}$ Assistant: $\quad$ &lt;&lt; Candidate $&gt;&gt;\left{y_{\text {ref }}\right}&lt;&lt;$ Improved $&gt;&gt;$
The $}$ denotes the concrete content, and $&lt;&lt;&gt;&gt;$ denotes the special token. Both methods will return an improved response. The difference is that PIT does not need to explicitly write prompts since self-improvement is learned during our proposed training framework. Both methods use a one-time inference to get an improved response, and PIT in principle has the same computational cost as Self-Refine during inference, or strictly speaking even fewer computational costs due to the fewer input tokens (without complex rubrics).</p>
<h2>C Why Equation 2?</h2>
<p>We made many efforts to filter out other reward modeling options and pin down our final choice to Equation 2. Since it is not applicable to try all possible solutions, we filter out other methods based on prior work findings and our own pilot experiments. Here is our detailed selection process:</p>
<p>The first question in developing the reward model is if we can directly use $R_{p}$ (i.e., the vanilla reward model that takes $x$ and $y$ as inputs and returns a scalar reward $r$ ) to compute the reward gap between two responses $y_{1}$ and $y_{2}$, or we need to develop another reward model that directly calculates the reward gap (i.e., preference model). The most straightforward implementation using $R_{p}$ is to compute the subtraction of individual rewards of $y_{1}$ and $y_{2}$ obtained by $R_{p}$. However, previous works show that $R_{p}$ fails to faithfully reflect the response quality when $r$ is high (Bai et al., 2022). That is to say, if $r_{1}$ and $r_{2}$ are higher than a threshold, then $r_{1}&lt;r_{2}$ does not necessarily denote $y_{1}$ is worse. This phenomenon is possible because of the poor calibration of the reward model (Peng et al., 2023b). Moreover, other work (Zhao et al., 2023) shows that directly modeling the reward gap brings less noise and performs better than computing the subtraction of individual rewards. Therefore, we decided to train a reward model that directly models reward gaps.</p>
<p>Next, we need to decide the training objective. In the beginning, the training objective is simply maximizing the reward gap $r_{\text {gap }}\left(x, y_{w}, y_{l}\right)$ and minimizing $r_{\text {gap }}\left(x, y_{l}, y_{w}\right)$, which is similar to Equation 2 but without $r_{\text {gap }}\left(x, y_{w}, y_{w}\right)$ and $r_{\text {gap }}\left(x, y_{l}, y_{l}\right)$. However, we observe that LLM finds a shortcut that ignores the third argument, thus degenerating to $R_{p}$ and failing to grasp the reward gap. Therefore, we add $r_{\text {gap }}\left(x, y_{w}, y_{w}\right)$ and $r_{\text {gap }}\left(x, y_{l}, y_{l}\right)$ to prevent the degeneration.</p>
<p>We also explore the possibility of using pointwise rather than pairwise training signals used above. For example, we tried to explicitly assign $r_{\text {gap }}\left(x, y_{w}, y_{l}\right)=1, r_{\text {gap }}\left(x, y_{l}, y_{w}\right)=0$ and $r_{\text {gap }}\left(x, y_{l}, y_{l}\right)=$</p>
<p>$r_{\text {gap }}\left(x, y_{w}, y_{w}\right)=0.5$ and use MSE to train the reward model. We find this implementation is less effective than our proposed approach (e.g., Equation 2) on the held-out reward model test set of 500 examples ( $\sim 5 \%$ accuracy drop). We think this phenomenon is because the inductive bias of assigning rewards to $1,0.5$, and 0 is not preferred in the data distribution.</p>
<p>Based on the above analysis, Equation 2 is chosen as our final choice.</p>
<h1>D SYNTHETIC DATA</h1>
<p>The synthetic data aims to test the instruction following abilities. Therefore, each question contains multiple requirements. We first manually design a list of tasks (e.g., email writing and advertisement writing) and requirements (e.g., using specific formats such as HTML, Markdown, or upper case.). Then, we randomly choose a task and several requirements from the list. We instruct PaLM 2 (Unicorn) to generate a question $x_{0}$ based on the chosen task and requirements and give a response $y_{l}$ accordingly. Next, we randomly select a few more requirements and instruct PaLM 2 (Unicorn) to add extra requirements to $x_{0}$ and get a new question $x$ together with its response $y_{w}$. The difference between $y_{w}$ and $y_{l}$ is that $y_{w}$ follows all instructions whereas $y_{l}$ only follows part of instructions in $x$. It is worth noting that $y_{w}$ does not necessarily have better content than $y_{l}$. In the synthetic dataset, we only care about the instruction following ability. At last, we instruct PaLM 2 (Unicorn) to give a chain-of-thoughts reflection $c$ on improving $y_{l}$ to $y_{w} \cdot\left{\left(x, y_{l}, y_{w}, c\right)\right}$ forms the synthetic dataset.</p>
<p>We use a two-step filtering method to filter out low-quality data: (1) We prompt PaLM 2 (Unicorn) to self-verify if $y_{w}$ follows all instructions and $y_{l}$ does not. (2) We manually inspect data and select a group of high-quality data and low-quality data, and then we use these data to fine-tune a classifier to further filter our low-quality data. In the end, we obtained 13 K synthetic data in total.</p>
<p>Here is an example:
$x$ :
Create a serious tagline for a company that designs and builds roller coasters and rides for theme parks. It should follow the format "We are the best at [what the company does]". Also, make part of your answer in HTML format.
$y_{l}$ :
We are the best at designing and building the most thrilling roller coasters and rides.
$c$ :
The response is a good tagline. It is serious. It is of the format "We are the best at [what the company does]". However, it does not contain any HTML format. We can improve it by adding some HTML format. For example, we can put "designing and building the most thrilling roller coasters and rides" in italics. This would make the tagline more visually appealing.
$y_{w}$ :
We are the best at <i>designing and building the most thrilling roller coasters and rides</i>.</p>
<h2>E Prompts</h2>
<h2>E. 1 Prompts for Self-Refine</h2>
<p>Self-Refine first enables LLMs to obtain feedback, and then asks LLMs to improve responses based on the feedback.</p>
<h2>E.1.1 ANTHROPIC/HH-RLHF</h2>
<h2>Reflection to get [Feedback]:</h2>
<p>Human: Consider the following context:</p>
<h1>[INPUT]</h1>
<p>What should the assistant respond next in order to be helpful to satisfy the human's needs?</p>
<p>Here is a candidate's response:
[LLM OUTPUT]
You should analyze the response and see if the response could be further improved to be more helpful to satisfy human needs and not go off-topic (You are not required to give an improved response).</p>
<p>Your reply should only contain the analysis and no more other words.</p>
<p>Assistant: My analysis:</p>
<h2>Refinement to get [LLM-Improved Output]:</h2>
<p>Human: Consider the following context:
[INPUT]
What should the assistant repond next in order to be helpful to satisfy the human's needs?</p>
<p>Here is a candidate response:
[LLM Output]
Here is the analysis of the candidate response:
[Feedback]
Please give me the improved response that is more helpful to satisfy human's needs and does not go off-topic according to the analysis. If you do not know how to improve the response further, then just repeat the candidate response. If the improved response reuses parts of the candidate response, then just copy the reused parts in the improved response.</p>
<p>Your reply should only contain the improved response and no more other words.</p>
<p>Assistant: Improved response:</p>
<h2>E.1.2 Synthetic Data</h2>
<h2>Reflection to get [Feedback]:</h2>
<p>Consider the following instruction:
[INPUT]
Here is a candidate response:
[LLM Output]
You should analyze the response and see if the response could be further improved to better follow instructions.</p>
<p>Your reply should only contain the analysis and no more other words.</p>
<p>Response:</p>
<h2>Refinement to get [LLM-Improved Output]:</h2>
<p>Consider the following instruction:</p>
<p>[INPUT]
Here is a candidate response:
[LLM Output]
Here is the analysis of the candidate response:
[Feedback]
Please give me the improved respone that better follows
instructions according to the analysis.
If you do not know how to improve the response further, then just repeat the candidate response.</p>
<p>If the improved response reuses parts of the candidate response, then just copy the reused parts in the improved response.</p>
<p>Your reply should only contain the improved response and no more other words.</p>
<p>Response:</p>
<h1>E. 2 Evaluation Prompts</h1>
<p>Evaluation prompts need to be as detailed as possible to make LLMs fully understand the evaluation task. We combine prompts used in Rafailov et al. (2023); Zheng et al. (2023).</p>
<h2>E.2.1 Anthropic/HH-RLHF</h2>
<p>Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user's instructions better and provides more helpful responses to the user's questions. A helpful response should directly address the human questions without going off-topic. A detailed response is only helpful when it always focuses on the question and does not provide irrelevant information. A helpful response should also be consistent with the conversation context. For example, if the human is going to close the conversation, then a good response should tend to close the conversation, too, rather than continuing to provide more information. If the response is cut off, evaluate the response based on the existing content, and do not choose a response purely because it is not cut off. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor specific names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: [[A]] if assistant A is better, [[B]] if assistant B is better, and [[C]] for a tie.
--User Question--
[Input]
--The Start of Assistant A's Answer--
[OUTPUT A]
--The End of Assistant A's Answer--
--The Start of Assistant B's Answer--</p>
<h1>[OUTPUT B]</h1>
<p>--The End of Assistant B's Answer--</p>
<h2>E.2.2 OPENAI/SUMMARY</h2>
<p>Please act as an impartial judge and evaluate the summaries' quality of the Reddit posts displayed below. You should choose the summary that better summarizes the post without including unimportant or irrelevant details. A good summary is both precise and concise. Begin your evaluation by comparing the two summaries and provide a short explanation. Avoid any positional biases and ensure that the order in which the summary was presented does not influence your decision. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: [[A]] if summary A is better, [[B]] if summary B is better, and [[C]] for a tie.
--Post--
[Input]
--Summary A--
[OUTPUT A]
--The End of Summary A--
--Summary B--
[OUTPUT B]
--The End of Summary B--</p>
<h2>E.2.3 SYnthetic Data</h2>
<p>Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that better follows the user's instructions. Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: [[A]] if assistant A is better, [[B]] if assistant B is better, and [[C]] for a tie.
--User Question--
[Input]
--The Start of Assistant A's Answer--
[OUTPUT A]
--The End of Assistant A's Answer--
--The Start of Assistant B's Answer--
[OUTPUT B]
--The End of Assistant B's Answer--</p>
<h1>F EXPERIMENT SETTINGS</h1>
<p>In the supervised fine-tuning stage, we fine-tune $\mathrm{M}<em _mathrm_P="\mathrm{P">{\mathrm{PIT}}^{\mathrm{SFT}}$ and $\mathrm{M}</em>}}^{\mathrm{SFT}}$ for one epoch to avoid overfitting and set the learning rate to $3 e-5$. We use the last SFT checkpoint for RL since only this checkpoint sees all SFT data. The learning rate is set to $3 e-4$ in the reward model training. The learning rate of reinforcement learning is set to $1 e-5$. We set the context window to 512 for inputs and 512 for outputs for $\mathrm{M<em _PIT="{PIT" _text="\text">{\mathrm{P}}$, and the context window to 512 for inputs, 512 for reference outputs and 512 for outputs for $\mathrm{M}</em>}}$. We drop data that exceeds such limits. We use reward model checkpoints that achieve the highest accuracy on a held-out 128 ground-truth validation data. To select the best checkpoint of $\mathrm{M<em _mathrm_P="\mathrm{P">{\mathrm{PIT}}^{\mathrm{RL}}$ and $\mathrm{M}</em>$, we first divide all checkpoints into several groups by their KL divergence to their SFT counterparts, and then select the one to two checkpoints per group that have the highest rewards on validation data. At last, we manually check the quality of model generations on 10 to 20 examples and select the best checkpoint.}}^{\mathrm{RL}</p>
<h2>G AGREEMENT OF EVALUATORS WITH GROUND-TRUTH LABELS</h2>
<p>Table 4 shows the agreement between evaluators and ground-truth labels on the validation data. GPT-4 performs best among language models, and DeBERTa performs similarly to GPT-4. The two models are chosen to be our evaluators.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy(\%)</th>
<th style="text-align: center;">Third-Party Language Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Reward Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">PaLM 2 (Unicorn)</td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">DeBERTa*</td>
<td style="text-align: center;">$\mathrm{R}_{\text {PIT }} #$</td>
<td style="text-align: center;">$\mathrm{R}_{\mathrm{P}} #$</td>
</tr>
<tr>
<td style="text-align: center;">Anthropic/HH-RLHF</td>
<td style="text-align: center;">65.63</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">71.48</td>
<td style="text-align: center;">69.14</td>
<td style="text-align: center;">65.63</td>
<td style="text-align: center;">65.63</td>
</tr>
<tr>
<td style="text-align: center;">OpenAI/Summary</td>
<td style="text-align: center;">61.33</td>
<td style="text-align: center;">63.28</td>
<td style="text-align: center;">67.97</td>
<td style="text-align: center;">69.14</td>
<td style="text-align: center;">72.66</td>
<td style="text-align: center;">68.75</td>
</tr>
<tr>
<td style="text-align: center;">Synthetic Data</td>
<td style="text-align: center;">67.19</td>
<td style="text-align: center;">83.20</td>
<td style="text-align: center;">84.38</td>
<td style="text-align: center;">49.20</td>
<td style="text-align: center;">96.09</td>
<td style="text-align: center;">87.50</td>
</tr>
</tbody>
</table>
<p>Table 4: Agreement between automatic evaluation models and ground truth labels. We evaluate 128 examples for third-party language models and 1,000 examples for reward models. * DeBERTa reward model is trained by OpenAssistant. It is not trained on the synthetic data, causing poor performance accordingly. ${ }^{#} \mathrm{R}<em _mathrm_P="\mathrm{P">{\text {PIT }}$ and $\mathrm{R}</em>}}$ is trained by us on the $1 / 3$ of the full data as described in Section 4.1. It is used for RLHF. $\mathrm{R<em 1="1">{\text {PIT }}$ states $y</em>}$ is better than $y_{2}$ if $r_{g a p}\left(x, y_{1}, y_{2}\right)&gt;r_{g a p}\left(x, y_{2}, y_{1}\right)$, and $\mathrm{R<em 1="1">{\mathrm{P}}$ use $r\left(x, y</em>\right)$ as the criteria. Due to the low inference speed, we only evaluate 128 examples with them.}\right)&gt;r\left(x, y_{2</p>
<h2>H EXAMPLES</h2>
<h2>H. 1 COMMON EXAMPLES</h2>
<p>We show several examples randomly selected from data to give readers senses about how PIT outperform Self-Refine. As suggested by Figure 3, the temperature for PIT and Self-Refine is 0.4 and 0.8 for Anthropic/HH-RLHF dataset. For OpenAI/Summary, we use the temperature of 0.4 for PIT. We use temperature 1 and 0.4 for PIT and Self-Refine on the synthetic data.</p>
<h2>H.1.1 ANTHROPIC/HH-RLHF</h2>
<p>PIT offers more comprehensive responses without going-off topic.
[Input]: Human: What lies outside of the universe?
$y_{w}$ in Data: You're probably asking about the multiverse. What's the rest of the answer?
<a href="i.e., Original Responses">LLM Output</a>: One of the most interesting and intriguing mysteries about the universe is what, if anything, is outside of it. There are two main theories of what might lie outside of the universe. In one theory, there is some kind of universal bubble that our universe exists within. This bubble would be surrounded by a vast superspace called the quantum foam.</p>
<p>The quantum foam would consist of energy at the smallest possible scales, and it could stretch on forever in all directions. The other theory is that the universe is essentially infinite, and there is nothing outside of it. If this is true, then what lies outside of our universe is literally nothing.
[LLM-Improved Output] (i.e., Self-Refine): One of the most intriguing questions about the universe is what, if anything, is outside of it. There are several possibilities, but the two main theories are the multiverse theory and the theory of a closed universe. According to the multiverse theory, there are countless universes existing in parallel with our own, and each of these universes has its own laws of physics. The theory of a closed universe posits that our universe is a finite bubble existing within a larger superspace, and the bubble is bounded on all sides. There is also some speculation that our universe could be infinite, with no boundary or external space. This is known as the 'no-boundary' theory.
[PIT-Improved Response]: No one knows for sure, but there are a number of theories. One of the most popular is the 'multiverse' theory, which posits that there are many different universes, each with its own laws of physics. These universes could be completely different from our own, or they could be similar but with slight variations. In either case, the multiverse theory suggests that there is a vast expanse of space outside of our universe, with many different universes existing in it. Another theory is that there is nothing outside of our universe. This is sometimes called the 'cosmological principle', and it suggests that the laws of physics are the same everywhere in the universe. If this is true, then the universe would be infinite, and there would be no 'outside' of it. In either case, it is very difficult to imagine what lies outside of our universe. It is possible that there is nothing, or it is possible that there are many different universes. We may never know for sure, but it is an interesting question to ponder. I hope this answer was helpful, and I wish you the best of luck in your future explorations of the universe!</p>
<h1>H.1.2 OPENAI/SUMMARY</h1>
<p>PIT improves the original summary by adding key information without mentioning irrelevant details.
[Input]: Generate a summary of the following text: <strong><em>I am NOT planning of giving neither of them advice/opinions etc. They have lawyers for that. I'm just curious to what reddit thinks of this case. *** </em>The dog is a he, but to avoid confusion I will refer to him as "it".* On going divorce between a friend and her husband of 10 years. The dog is the husband's (he had it before the marriage). </strong>However, for most of the marriage, the wife is the one who walks it, feeds it, cleans after it.** And when it got sick (I am not familiar with animal health issues, but it has to take certain meds regularly) she's the one who takes it to the vet and also started making raw food for the dog to improve its overall health. However, he feels that the dog is "his dog" and refuses to part with it/is willing to fight over it. Some background, feel free to skip: The wife is the stay-at-home mom of two boys. She does some contract work when she could. It is not her first choice, but the husband was never home because of his career and often out of the country. (Which makes him unfit to take care of the dog) After giving birth to the second child,</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We use scripts released by Chatbot Arena (Zheng et al., 2023): https://colab.research.google. com/drive/1J2Wf7sxc9SVmGnSX_1ImhT246pxNVEip?usp=sharing&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
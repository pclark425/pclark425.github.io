<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-240 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-240</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-240</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-260154895</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.13617v2.pdf" target="_blank">GPT-3 Models are Few-Shot Financial Reasoners</a></p>
                <p><strong>Paper Abstract:</strong> Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 have achieved state-of-the-art performance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents. With this understanding, our refined prompt-engineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e240.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e240.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (GPT-3, DaVinci)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 175B-parameter autoregressive (decoder-only) language model used in this paper with few-shot prompts to perform financial numerical reasoning and arithmetic-like computations over retrieved facts and generated programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (DaVinci)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (autoregressive)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition, subtraction, multiplication, division, comparison (greater), table aggregation and multi-step arithmetic (N-step composed programs / financial formulae)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot prompting (one-shot for end-to-end full-passage), eight-shot prompting on retrieved facts, prompting to generate programs (program prediction) and external-tool decomposition (retrieval external, execution external); experiments vary between end-to-end, retrieved-only, program-generation-only, and retrieved+program (with external calculator)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>End-to-end (full passage, one-shot): 5% exact execution accuracy (20% tolerance ≈ 25%). Retrieved-passages (few-shot): execution accuracy ≈ 20% (20% tolerance ≈ 60%). Retrieved + program-prediction (no execution): program-execution-accuracy proxy increased from 20% → 50%; 20% tolerance increased from 60% → 65%. Step-complexity: 1-step execution accuracy ≈ 75%; performance decreases roughly linearly with number of steps down to ≈20% at 4-step (noted anomalous increase at 5-step). With retrieval + external calculator, GPT-3 reached near-SOTA execution accuracy (paper reports external calculator drove GPT-3 to near SOTA but does not give a single overall exact %).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>GPT-3 often produces directionally-correct outputs (correct sign, unit, and scale) even when exact numeric answers are wrong, suggesting statistical pattern capture rather than reliable algorithmic calculation; it struggles with retrieval and with precise program execution (logical operations), and benefits substantially when retrieval and deterministic execution are handled externally. The paper cites prior work arguing LMs capture statistical but not logical patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Not directly evaluated across model sizes in this paper (single GPT-3 DaVinci used); observed performance degrades with increasing problem complexity (number of reasoning steps), with a roughly linear drop in execution accuracy as step count increases (exception at 5-step). External components (retriever, external calculator) improve effective performance independent of model parameter scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Poor end-to-end retrieval (results collapse when given full passage), imprecise program execution (logical/algorithmic execution errors), degraded performance as number of arithmetic steps increases, approximate-but-not-exact numeric outputs (off-by-scale or off-by-value), difficulty reliably executing multi-step programs despite sometimes correct directional output.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to FinQANet retriever+program-generator baseline (FinQANet RoBERTa execution accuracy ≈ 61.24%) and human experts (≈91%); ablations include: full-passage one-shot vs retrieved-passages few-shot vs retrieved+program-prediction vs retrieved+external-calculator. External retrieval and external execution both materially improve GPT-3 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3 can produce directionally-correct numeric answers and can generate correct programs, but it struggles with retrieval and precise multi-step program execution; performance improves markedly when specialized retrieval and a deterministic external calculator are used, indicating arithmetic/program execution is best handled outside the LM.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e240.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e240.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computational tree calculator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External computational tree calculator (string-parsing program executor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic external tool implemented by the authors that parses generated formulas/programs (computational trees) and deterministically executes multistep arithmetic operations (add, subtract, multiply, divide, greater) to produce numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>external calculator (not a neural model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>add, subtract, multiply, divide, comparison (greater); executes multistep computational tree programs</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>tool use; deterministic execution of string-parsed programs produced by model or annotator (externalizing the execution step from the LM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Incorporating the external calculator with GPT-3 (i.e., using GPT-3 for retrieval/program-generation and the calculator for execution) drove GPT-3 to near-SOTA execution accuracy on FinQA without fine-tuning (exact consolidated percentage not reported in single number by paper).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Deterministic execution of programs eliminates LM's unreliable execution behavior; the LM is better at producing program-like representations and directional numeric outputs than reliably executing algorithmic arithmetic, so delegating exact computation to a calculator yields large accuracy gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>None intrinsic to the calculator (deterministic), but dependent on correctness of program produced by LM and correctness/availability of the numerical constants extracted from retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared implicitly to end-to-end GPT-3 execution (where GPT-3 executes programs internally) and to FinQANet generator-executor pipeline; using external execution produced large gains compared to LM-internal execution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Deterministic external execution of generated programs remedies most LM execution failures: the LM can often generate correct or near-correct programs/answers, but exact numeric computation is best performed outside the model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e240.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e240.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FinQANet (retriever+generator baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FinQANet retriever-generator baseline (FinQA authors' approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage architecture used as the main baseline on FinQA: (1) a retriever (BERT/RoBERTa-based) that selects supporting facts from documents, and (2) a program generator that produces executable programs; reported strong execution accuracy on the FinQA dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FinQA: A Dataset of Numerical Reasoning over Financial Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FinQANet (retriever + program generator; retriever uses BERT/RoBERTa, generator is LSTM in baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>retriever: encoder-based classifier (BERT / RoBERTa); generator: sequence model (LSTM) producing programs</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>table aggregation, addition, subtraction, multiplication, division, comparisons, multi-step financial formulae</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>retrieval-augmented program generation with supervised training; generator produces executable program which is then executed deterministically</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Baseline execution accuracies reported: FinQANet (BERT) ≈ 50% exec acc, FinQANet (RoBERTa) ≈ 61.24% exec acc; human experts ≈ 91.16%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Separating retrieval and deterministic program execution produces substantially higher accuracy than end-to-end LMs executing arithmetic internally; precise program construction plus deterministic execution is critical for financial numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors arise from incorrect retrieval of supporting facts or incorrect program generation; however, deterministic execution of programs (once correct) yields correct numeric answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as the main baseline for comparisons with GPT-3 experiments (contrasting retriever+deterministic-execution pipelines vs end-to-end LM-only approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A retriever + deterministic program generator/executor pipeline substantially outperforms end-to-end LM execution for multi-step financial arithmetic; retrieval and exact execution are key to high performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>FinQA: A Dataset of Numerical Reasoning over Financial Data <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>On the Paradox of Learning to Reason from Data <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-240",
    "paper_id": "paper-260154895",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3 (GPT-3, DaVinci)",
            "brief_description": "A 175B-parameter autoregressive (decoder-only) language model used in this paper with few-shot prompts to perform financial numerical reasoning and arithmetic-like computations over retrieved facts and generated programs.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "use",
            "model_name": "GPT-3 (DaVinci)",
            "model_size": "175B",
            "model_architecture": "decoder-only transformer (autoregressive)",
            "arithmetic_operation_type": "addition, subtraction, multiplication, division, comparison (greater), table aggregation and multi-step arithmetic (N-step composed programs / financial formulae)",
            "number_range_or_complexity": null,
            "method_or_intervention": "few-shot prompting (one-shot for end-to-end full-passage), eight-shot prompting on retrieved facts, prompting to generate programs (program prediction) and external-tool decomposition (retrieval external, execution external); experiments vary between end-to-end, retrieved-only, program-generation-only, and retrieved+program (with external calculator)",
            "performance_result": "End-to-end (full passage, one-shot): 5% exact execution accuracy (20% tolerance ≈ 25%). Retrieved-passages (few-shot): execution accuracy ≈ 20% (20% tolerance ≈ 60%). Retrieved + program-prediction (no execution): program-execution-accuracy proxy increased from 20% → 50%; 20% tolerance increased from 60% → 65%. Step-complexity: 1-step execution accuracy ≈ 75%; performance decreases roughly linearly with number of steps down to ≈20% at 4-step (noted anomalous increase at 5-step). With retrieval + external calculator, GPT-3 reached near-SOTA execution accuracy (paper reports external calculator drove GPT-3 to near SOTA but does not give a single overall exact %).",
            "mechanistic_insight": "GPT-3 often produces directionally-correct outputs (correct sign, unit, and scale) even when exact numeric answers are wrong, suggesting statistical pattern capture rather than reliable algorithmic calculation; it struggles with retrieval and with precise program execution (logical operations), and benefits substantially when retrieval and deterministic execution are handled externally. The paper cites prior work arguing LMs capture statistical but not logical patterns.",
            "performance_scaling": "Not directly evaluated across model sizes in this paper (single GPT-3 DaVinci used); observed performance degrades with increasing problem complexity (number of reasoning steps), with a roughly linear drop in execution accuracy as step count increases (exception at 5-step). External components (retriever, external calculator) improve effective performance independent of model parameter scaling.",
            "failure_modes": "Poor end-to-end retrieval (results collapse when given full passage), imprecise program execution (logical/algorithmic execution errors), degraded performance as number of arithmetic steps increases, approximate-but-not-exact numeric outputs (off-by-scale or off-by-value), difficulty reliably executing multi-step programs despite sometimes correct directional output.",
            "comparison_baseline": "Compared to FinQANet retriever+program-generator baseline (FinQANet RoBERTa execution accuracy ≈ 61.24%) and human experts (≈91%); ablations include: full-passage one-shot vs retrieved-passages few-shot vs retrieved+program-prediction vs retrieved+external-calculator. External retrieval and external execution both materially improve GPT-3 performance.",
            "key_finding": "GPT-3 can produce directionally-correct numeric answers and can generate correct programs, but it struggles with retrieval and precise multi-step program execution; performance improves markedly when specialized retrieval and a deterministic external calculator are used, indicating arithmetic/program execution is best handled outside the LM.",
            "uuid": "e240.0"
        },
        {
            "name_short": "Computational tree calculator",
            "name_full": "External computational tree calculator (string-parsing program executor)",
            "brief_description": "A deterministic external tool implemented by the authors that parses generated formulas/programs (computational trees) and deterministically executes multistep arithmetic operations (add, subtract, multiply, divide, greater) to produce numeric answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "external calculator (not a neural model)",
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "add, subtract, multiply, divide, comparison (greater); executes multistep computational tree programs",
            "number_range_or_complexity": null,
            "method_or_intervention": "tool use; deterministic execution of string-parsed programs produced by model or annotator (externalizing the execution step from the LM)",
            "performance_result": "Incorporating the external calculator with GPT-3 (i.e., using GPT-3 for retrieval/program-generation and the calculator for execution) drove GPT-3 to near-SOTA execution accuracy on FinQA without fine-tuning (exact consolidated percentage not reported in single number by paper).",
            "mechanistic_insight": "Deterministic execution of programs eliminates LM's unreliable execution behavior; the LM is better at producing program-like representations and directional numeric outputs than reliably executing algorithmic arithmetic, so delegating exact computation to a calculator yields large accuracy gains.",
            "performance_scaling": null,
            "failure_modes": "None intrinsic to the calculator (deterministic), but dependent on correctness of program produced by LM and correctness/availability of the numerical constants extracted from retrieved passages.",
            "comparison_baseline": "Compared implicitly to end-to-end GPT-3 execution (where GPT-3 executes programs internally) and to FinQANet generator-executor pipeline; using external execution produced large gains compared to LM-internal execution.",
            "key_finding": "Deterministic external execution of generated programs remedies most LM execution failures: the LM can often generate correct or near-correct programs/answers, but exact numeric computation is best performed outside the model.",
            "uuid": "e240.1"
        },
        {
            "name_short": "FinQANet (retriever+generator baseline)",
            "name_full": "FinQANet retriever-generator baseline (FinQA authors' approach)",
            "brief_description": "A two-stage architecture used as the main baseline on FinQA: (1) a retriever (BERT/RoBERTa-based) that selects supporting facts from documents, and (2) a program generator that produces executable programs; reported strong execution accuracy on the FinQA dataset.",
            "citation_title": "FinQA: A Dataset of Numerical Reasoning over Financial Data",
            "mention_or_use": "use",
            "model_name": "FinQANet (retriever + program generator; retriever uses BERT/RoBERTa, generator is LSTM in baseline)",
            "model_size": null,
            "model_architecture": "retriever: encoder-based classifier (BERT / RoBERTa); generator: sequence model (LSTM) producing programs",
            "arithmetic_operation_type": "table aggregation, addition, subtraction, multiplication, division, comparisons, multi-step financial formulae",
            "number_range_or_complexity": null,
            "method_or_intervention": "retrieval-augmented program generation with supervised training; generator produces executable program which is then executed deterministically",
            "performance_result": "Baseline execution accuracies reported: FinQANet (BERT) ≈ 50% exec acc, FinQANet (RoBERTa) ≈ 61.24% exec acc; human experts ≈ 91.16%.",
            "mechanistic_insight": "Separating retrieval and deterministic program execution produces substantially higher accuracy than end-to-end LMs executing arithmetic internally; precise program construction plus deterministic execution is critical for financial numeric reasoning.",
            "performance_scaling": null,
            "failure_modes": "Errors arise from incorrect retrieval of supporting facts or incorrect program generation; however, deterministic execution of programs (once correct) yields correct numeric answers.",
            "comparison_baseline": "Used as the main baseline for comparisons with GPT-3 experiments (contrasting retriever+deterministic-execution pipelines vs end-to-end LM-only approaches).",
            "key_finding": "A retriever + deterministic program generator/executor pipeline substantially outperforms end-to-end LM execution for multi-step financial arithmetic; retrieval and exact execution are key to high performance.",
            "uuid": "e240.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "FinQA: A Dataset of Numerical Reasoning over Financial Data",
            "rating": 2,
            "sanitized_title": "finqa_a_dataset_of_numerical_reasoning_over_financial_data"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "On the Paradox of Learning to Reason from Data",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        }
    ],
    "cost": 0.0108705,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GPT-3 MODELS ARE FEW-SHOT FINANCIAL REASONERS
2023</p>
<p>C David 
Wyld 
GPT-3 MODELS ARE FEW-SHOT FINANCIAL REASONERS
202310.5121/csit.2023.131216Question AnsweringGPT-3Financial Question AnsweringLarge Language ModelsInfor-mation RetrievalBERTRoBERTaFinQA
Financial analysis is an important tool for evaluating company performance. Practitioners work to answer financial questions to make profitable investment decisions, and use advanced quantitative analyses to do so. As a result, Financial Question Answering (QA) is a question answering task that requires deep reasoning about numbers. Furthermore, it is unknown how well pre-trained language models can reason in the financial domain. The current state-of-the-art requires a retriever to collect relevant facts about the financial question from the text and a generator to produce a valid financial program and a final answer. However, recently large language models like GPT-3 [1] have achieved state-of-the-artperformance on wide variety of tasks with just a few shot examples. We run several experiments with GPT-3 and find that a separate retrieval model and logic engine continue to be essential components to achieving SOTA performance in this task, particularly due to the precise nature of financial questions and the complex information stored in financial documents. With this understanding, our refined prompt-engineering approach on GPT-3 achieves near SOTA accuracy without any fine-tuning.</p>
<p>INTRODUCTION</p>
<p>Quantitative analysis is a critical tool in the financial industry. Professionals use it to ex-tract information from financial reports and make investment decisions affecting billionsof dollars every day 1 . The ability to quickly interpret the data can be the difference be-tween success and failure in today's highly competitive financial environment. Advanced experience in reasoning across structured and unstructured financial data sources and exe-cuting complicated numerical reasoning, such as comparing financial ratios of profitability or growth, are required for this type of study. These complications are exacerbated byan ever-growing quantity of financial information, making it difficult for analysts to con-duct adequate fiscal analysis and make accurate decisions. Furthermore, the vast amount of financial data gives a decisive advantage to professional investors, who can spend and hire people to manage the complexity, and creates a barrier for the average investor to participate in the market. Hence, the big question here is whether such in-depth analysis can be automated. Systems that can provide answers to financial questions would tremendously improve financial decision making and information transparency across all types of investors.</p>
<p>In this paper, we focus on Financial Question-Answering, a data analysis task that requires numerical reasoning. However, most previous research focused on the general domain, where the questions need far less calculation, such as a single-step basic math-ematical operation. In the financial sector, on the other hand, calculations can require multiple-steps and be more complicated. Therefore, Financial QA can be more difficult than traditional QA. In addition, financial QA can become problematic since the system may need to identify and grab important financial data from a variety of sources with different formats, such as tables and unstructured texts, and then develop a numerical reasoning to link all that data through calculations.</p>
<p>As an example, the New York Stock Exchange, which is one of many exchanges has a daily trading volume of $219bn, as of June 2022. With these challenges in mind, [2] introduces FinQA, a dataset of 8,281 financial QA pairings and related numerical reasoning processes that has been annotated by experts. FinQA is created by a group of eleven financial experts based on S&amp;P 500 earnings reports. The FinQA questions require data from tables as well as unstructured text. Many typical financial analyses computations, such as addition, comparison, and table aggregation, are used in the reasoning processes that answer these issues. They offer a retriever-generator QA architecture for retrieving supporting information from financial reports before gen-erating executable reasoning algorithms to answer the queries. Their suggested techniqueachieves an execution accuracy of roughly 60% percent when using pretrained language models such as BERT [3] and RoBERTa [4] transformer architectures. Although their method surpasses the general public (51%), the substantial accuracy difference between the model and human specialists (91%) clearly points in the direction of necessary further investigation.</p>
<p>In the following sections we showcase our main influential publications to this paper, present "FinQA: A Dataset of Numerical Reasoning over Financial Data" [2], our model applying a few-shot promptengineering using large language models, evaluation metrics, wrapping up with achieved results and analysis respectively.</p>
<p>PRIOR LITERATURE</p>
<p>Financial question answering involves two strands of research: general question answering (open QA), and financial language modelling. In open QA studies, authors attempt to combine effective performance with computa-tional efficiency. Whereas, finance related papers aim to contribute on compelling results in both domain-specific language models to semantics and downstream tasks such as sen-timent in the stock markets.</p>
<p>In general question answering (open QA) settings, ColBERT by [5] and ColBERT-QA by [6], tackle freezing the document encoder during training and having limited interactionwith query. Using document matrix pre-computation and late stage interaction introduced with ColBERT, ColBERT-QA finds useful passages for more questions. Improved passage relevance helps the reader component answer more accurately with greater attribution.</p>
<p>Baleen by [6] introduce a pipeline for multi-hop retrieval on top of ColBERT for thetask of Multi-hop QA.</p>
<p>Multi-hop QA involves synthesizing an answer only present in twoor more documents. Baleen extends the ColBERT late interaction for this task by summa-rizing the pertinent information from retrieved passages to inform the next retrieval, and also by allowing the document matrix representations of different documents to "focus" on distinct parts of the same query. Many Multi-hop questions are multi-part complex queries. So, different documents could attend to different aspects of the query. As a re-sult of its more deliberate architecture and its stronger retrieval modeling, Baleen raises answer-recall@20 from 89% by MDR to 96% for HotPotQA benchmark finds all required passages in 92% of the examples in HoVer-up from 45% in the baseline.</p>
<p>RAG, by [7], investigate a general-purpose fine-tuning method for retrieval-augmented generation (RAG) -models that integrate pretrained parametric and non-parametric memory for language generation. Furthermore, RAG demonstrates state-of-the-art per-formance without separate re-ranking or reader component present in many other neural retrieval systems. They compare two RAG formulations: one that uses the same retrieved texts throughout the whole produced sequence, and the other that can utilize a different passage per token. On a variety of knowledge-intensive NLP tasks, they fine-tune and assess models, and establish the state-of-the-art on three open domain QA tasks, outper-forming parametric seq2seq models and task-specific retrieve-and-extract architectures.</p>
<p>Finally, there are finance domain-specific language models. One publication by [8] developed a platform for evaluating the efficacy and performance of various sentiment analysis algorithms based on a mix of text representation methods and machine learning classifiers. They run over a hundred trials using publicly available datasets that have been tagged by financial experts. They begin by evaluating particular lexicons for sentiment analysis in finance, then expand the research to encompass word and sentence encoders, all the way up to the most recent NLP transformers. Even when big datasets are not available,the results reveal that contextual embeddings outperform lexicons and fixed word and phrase encoders in sentiment analysis. Furthermore, distilled NLP transformers generate outcomes that are equivalent to their bigger teacher models, making them appropriate for usage in production situations.</p>
<p>Specifically for financial NLP, [9], developed a language model based on BERT called FinBERT. For two financial sentiment analysis datasets, namely TRC2-financial as a sub-set of Reuters' TRC2 and Financial PhraseBank, their results demonstrate improvementsin every measurable metric compared to existing stateof-the-art results. For financial sentiment analysis, they use two more pretrained language models, ULM-Fit and ELMo, and compare them to FinBERT. They run tests to look at a variety of elements of the model, including the impact of additional pretraining on the financial corpus, training tactics to avoid catastrophic forgetting, and fine-tuning only a small portion of model layers to reduce training time without sacrificing performance. They demonstrate that FinBERT beats state-of-the-art machine learning approaches even with a smaller training set and fine-tuning only a portion of the model.</p>
<p>Another work called FinBERT (BERT for Financial Text Mining), by [10], built a domain-specific language model that has been pretrained on large-scale financial corpora to address this issue. Unlike BERT, [10] build six pretraining tasks in their FinBERT that cover more knowledge and are simultaneously trained on general corpora and financial sector corpora, allowing the model to better capture linguistic knowledge and semantic information. Their FinBERT outperforms all current state-of-the-art models, according to the results.</p>
<p>In the general domain literature, OpenQA models differ between using generation or extractive approaches, neural retrieval methods, corpus pre-computation, query-document interaction, using a separate re-ranking process or reader. ColBERT [5] adapts deep lan-guage models from BERT and provides a highly effective and competitive setup. ColBERT-QA adopts ColBERT to OpenQA in order to handle the complexity of natural language questions and improve retrieval. Baleen's condensed retrieval architecture enhances multihop accuracy and robustness while learning from weak training signals. RAG proposes a model for Seq2Seq tasks with retrieval. RAG encodes documents and queries separately with a BERT encoder and uses a BART decoder to convert query and retrieved document embedding into an answer.</p>
<p>DATA</p>
<p>We use a recently released dataset, "FinQA: A Dataset of Numerical Reasoning over Financial Data" [2], which contains 8,281 financial questions. These questions ask for a numerical answer based on a unique passage associated with the question. Each question also includes the formula, called "program", which is used to generate the final answer.</p>
<p>FinQA is based on FinTabNet [11], a dataset comprised of publicly available earnings reports of S&amp;P 500 companies from 1999 to 2019. The FinTabNet earnings reports contain tables, figures, and texts that outline important financial information of the companies.To adapt this dataset to the Financial QA task, FinQA applied data filtering techniquesto exclude many tables that were overly complicated. This filtered dataset of financial documents are then annotated further with a finance-related question, relevant passages, a formula used to calculate the answer, and the numerical answer itself.</p>
<p>Writing meaningful financial questions and annotations, however, requires specialized finance knowledge. So, the FinQA researchers recruited US-based Finance professionals to pose expert questions an for each financial document in the dataset. Separately, other finance professionals were tasked to assess the data, question quality. These assessor ex-perts reached above 90% for execution accuracy and above 85% for program accuracy with very high agreement rate (93%). Moreover, FinQA researchers also hired nonexperts from Amazon Mechanical Turk to apply the same data quality procedure to verify the expert outcome. The non-experts were only able to reach about 50% for the execution and pro-gram accuracy with very low agreement rate (60%).</p>
<p>To FinQA questions rely on the information found across the passage, including textand tables. The passage is broken up into sentences and tables are converted into text representations. These snippets are then used as a part of the retrieval task to identify the salient portions of a document for a given question. Around 23% of these questionsleverage information only from the text, 63% from the tables, and 14% from both text and tables.</p>
<p>Furthermore, each question is associated with a "program" or text-based formula that the annotator used to generate the answer. These formulas are comprised of constants, operators, numbers, and references. One example is: </p>
<p>MODEL</p>
<p>In comparison to the retriever/generator approach, we apply a few-shot prompt-engineering approach using large language models such as GPT-3 [1].</p>
<p>We focus on large language models since they are the state-of-the-art, general purposemodels that can capture the large number of tokens required for few-shot financial exam-ples (which include multiple passages and tables). This versatility allows us to evaluate these models on a variety of upstream Financial QA tasks such as Fact Retrieval, and Program Generation. </p>
<p>4.1.Retriever/Generator Baseline with FinQA</p>
<p>FinQA authors [2] establish a main baseline framework called FinQANet that is built with a retriever (to get supporting facts) and a program generator (to create answers). In the former part, supporting facts are retrieved from financial reports, which surpass 2,000 tokens, to be concatenated with the question and train a classifier using pre-trained language models such as RoBERTa [4]. Top n retrieved facts will then serve as input tothe program generator. This latter part of the framework creates the program to provide answers to questions, updating the step memory token embeddings.</p>
<p>Our project baseline follows this approach. For the retriever, we apply a pretrained BERT-base as the classifier, where the model takes the top 3 ranked facts as retrieved results. On the generator side, we use a BERT-base [3] using adjusted FinQA open source code with the Adam optimizer [18]. Experimental details are shown in Table 3. See Figure 3for a reference architecture. </p>
<p>4.2.Large Language Models</p>
<p>We use GPT-3 using the OpenAI API with the parameters listed in Table 2. GPT-3 isa 175B parameter autoregressive language model trained on 45TB from CommonCrawl, Wikipedia and others, showing stateof-the-art performance on many NLP-related tasks. Often these tasks can be performed by providing a relevant prompt to GPT-3, which the next predicted tokens represent the model predictions. Furthermore, language models like GPT-3 have shown numerical capabilities like addition, subtraction, division, etc. which is useful for the Financial QA task.</p>
<p>For our research, we trained a BERT retriever model that extracted relevant factsgiven the question and passage text. We also built a computational tree calculator which uses string parsing to execute any multistep formula using common arithmetic operationsincluding add, subtract, multiply, divide, greater. We use both these components to evalu-ate GPT-3's financial reasoning capability. The code for both the retriever and calculator are are available publicly via Colab [12].</p>
<p>METHODS</p>
<p>Evaluation Metrics</p>
<p>Financial QA requires models to retrieve relevant facts within a document, synthesize those facts into a program, and execute that program into a numerical result. We compare retriever/generator FinQA models and pre-trained language models across various tasks using Execution Accuracy as our primary metric. Execution accuracy measures how well the model performs evaluated on the final results from the generated programs. This is effectively the average number of exact matches between the ground truth and predicted answers.</p>
<p>Furthermore, we look at execution accuracy within various tolerances. This metric provides insight into the directionality of our answers and whether the model is way offor is close to the ground truth answer. Effectively, accuracy tolerances allow us to proxy model understanding of the financial task at hand.</p>
<p>Concretely, for a given tolerance of X%, number of test examples n, ground truth values ! and predicted values " " , accuracy tolerance is calculated with the formula:
1 % &amp; ' − 1 ≤ + + &amp; ' − 1 ≤ + #$!%&amp;
Where A ≤ B is 1 if the inequality holds and 0 otherwise. For our analysis, we look at tolerances between 1% to 200% at 1% increments.</p>
<p>5.2.Prompt Engineering with GPT-3</p>
<p>We experiment with large language models by decomposing the financial QA task intoits component subtasks, which are: (1) Retrieve relevant passages for the question (2) Generate a computable formula with figures from the retrieved passage, and (3) Execute the computable formula into a numeric answer.</p>
<p>Across these tasks, we explored results by applying many-shot examples for the end-to-end tasks comprising of steps (1), (2), and (3), as well as sub-tasks by incorporatingan external retriever and a custom computational tree calculator. Each task was designed using custom functions which extract relevant information to construct a prompt which would serve as input to GPT-3.</p>
<p>After many trials, we found that using eight shots gave the best performance versus token tradeoff, especially given GPT3's token size limitations for retrieval and rationale tasks. For end-to-end prompting (i.e predict an answer directly from the full financial report), we used a one-shot prompt given the large token length of financial reports and the token limitations within GPT-3 models.</p>
<p>Furthermore, to test GPT-3's financial reasoning capability, we prompt the model with financial questions that require fixed number of steps to calculate accurately. By varying this step count, we measure GPT-3's ability to handle complex, multi-step financialquestions.</p>
<p>Concretely, our methods comprise of prompting GPT-3 with (a) one-shot of the full financial report, question, and corresponding ground truth numerical answer, with a test full financial report followed by a blank answer field, (b) eight-shots of the retrieved facts from financial reports with the corresponding questions and numerical answer, followed by a test retrieved facts from a financial report followed by a blank answer field, (c) eight-shots of the retrieved facts from financial reports with the corresponding questions and programs, followed by test retrieved facts from a single financial report and question with a blank program field, and (d) eight-shots of the retrieved facts from financial reports withthe corresponding program that requires N-steps, followed by test retrieved facts from a single financial report and question that requires N-steps with a blank program field. See Figures 6 and 7 4. Few-shot learning with GPT-3. By varying the task given to GPT3, we find that the best performance occurs when retrieval and calculation happen externally from the model. Furthermore the model becomes more directionally correct with these external components, as predictions that were not exactly right shift closer to true answers Fig. 5. GPT-3</p>
<p>Step Complexity: As the number of required steps to calculate the financial question increases, GPT-3 performance decreases, with a notable exception at 5 steps, where performance increases Finally, we run our experiments with GPT-3 DaVinci, which is currently the best performing language model available publicly and so provides an upper bound on the current capabilities of pre-trained language models [1]. Details of GPT-3 hyperparameters are given in Table 2.  </p>
<p>RESULTS</p>
<p>When given the full passage context, the model only responded with the correct answer only 5% of the time, whereas SOTA performance is near 60%. Surprisingly, many of the GPT-3 answers were directionally correct with the correct sign, unit, and scale. See figure 4. This is reflected in the accuracy 20% tolerance which was 25% When given the retrieved passages, GPT-3 performance increased substantially from 5% to 20%. The accuracy within at 20% tolerance also increased to 60% compared to the full-passage case.</p>
<p>The next experiment provided GPT-3 with retrieved passages and asked it to predict the financial program, but not execute it. In this case, the accuracy increased substantially again from 20% to 50% across many trials. There was also a slight increase in answer approximation, as accuracy within a 20% tolerance increased from 60% to 65%. See Figure8 for example results.</p>
<p>Finally, we compared performance across step complexity in GPT-3. As the numberof required steps to calculate the financial question increases, GPT-3 execution accuracy decreases linearly, with a notable exception at 5 steps, where accuracy increased. At 1-step, the model performed with 75% accuracy, which beats the overall SoTA accuracy and decreased linearly for 20% for the 4-step case. See 5 for more results. See Table 4 and Figure 4 in the appendix section for examples and directionality analysis.</p>
<p>ANALYSIS</p>
<p>Overall, across the various tasks mentioned in "Methods", we found that GPT-3 performs best when the retriever and calculator are handled external to the model.</p>
<p>While in the end-to-end experiment, GPT-3 demonstrated some capability, the poor accuracy meant that the model was struggling either to retrieve, reason about financial programs, and/or perform calculations. Interestingly, despite this poor exact accuracy, many of the answers had the correct sign and scale (e.g. if the question asked for a per-centage, the model would output a positive value &lt; 1). This indicates that there is some latent financial reasoning capability in the model.</p>
<p>The retrieval experiments, where execution accuracy increased 4x, strengthened the view that GPT-3 has the capability to do financial reasoning, but struggles when asked to perform retrieval. Furthermore the results were more directionally correct, which supports the idea that GPT-3's financial reasoning was strengthened upon adding a specialized retriever model. These results also fall in line with prior retrieval enhanced model literaturesuch as as ColBERT [5], which show more efficiency by adding a retriever over the raw parameter scaling seen in GPT-3 [1].</p>
<p>Finally, incorporating an external calculator drove GPT-3 to near SOTA performance without any finetuning. The substantial increases in execution accuracy indicate that program execution is difficult for the model. This view is supported by [13], who showed that language models tend to capture statistical patterns, but not logical ones. Logical patterns, however, are required for the precise operations in financial formulae. As a result, its likely that even as language models scale, FinancialQA will continue to require a external calculator.</p>
<p>Generally, for program prediction, GPT-3's accuracy decreased as the complexity ofthe question increased. This is expected, and in line with human-level performance which also decreases as as the reasoning required increases. Interestingly, the 5-step accuracy increased over both 3-and 4-step cases. This is likely due to the fewer types of problems in the dataset that require a 5-step solution, over the variety of questions that exist in lower-step settings.</p>
<p>To summarize, we found that the large amount of information required to retrieve and generate programs made it harder for the model to generalize across retrieval, reasoning, and calculation tasks together. Furthermore, this difficulty increased linearly with the number of steps required to perform the final calculation. This indicates that the precise nature and large quantity of complex information in financial QA requires a specialized retriever and external calculator to achieve SOTA results.</p>
<p>CONCLUSION</p>
<p>Financial analysis is an important method for assessing the performance of firms. Advanced quantitative analyses are used by practitioners to answer financial queries on reports and make lucrative investment decisions. As a result, Financial Question Answering (QA) is an important question-answering task requiring in-depth numerical reasoning. A retriever must extract critical details about the financial issue from the text, and a generator must construct a legitimate computational tree and a final response. Large language models like GPT-3 have recently achieved the state-of-the-art performance on similar tasks. However, due to the nature of financial inquiries and the extensive information held in financial documents, we come up with a Financial QA system that requires a retriever and program generator. Our work produces highly promising results in terms of answers to detailed numerical financial questions that may contribute as a supporting tool to the finance industry.</p>
<p>Future work possibilities include running experiments with different hyperparameters and fine-tune language models to explore the limits of numerical inference within Financial QA, specially tackling the opportunity to use a transformer decoder in the generator and compare results with GPT-3.</p>
<p>Another path that can be explored that has the potential to add real value moving for-ward is to develop a "Conversational FinQA" dataset with human like answers to questions and context provided, that can be used to more conversational systems' developments on top of it.</p>
<p>KNOWN LIMITATIONS</p>
<p>Obstacles developing this publication include computational resources, as the large lan-guage models are resource-intensive, and future NLP practitioners will have to use models like GPT-3 sparingly.</p>
<p>Furthermore, data cleaning and filtering may pose a challenge if there are many entriesthat exceed token limits.</p>
<p>For this work, we focused primarily on complexity on a limited number of financial operations. Real world systems may expand those operations and should therefore be tested thoroughly to ensure similar accuracy and robustness.   Table 4. GPT-3 DaVinci responses given one-shot passage-question-answer example compared to gold answers. Despite not being trained on the task, GPT-3 is able to extract a plausible numerical result with one example prompt. </p>
<p>Fig. 1 .
1The Financial QA Task involves understanding a financial question, gathering relevant facts, and generating a program to calculate an answer. Figure from[2] </p>
<p>Fig. 2 .
2FinQA Retriever Example. Figure from[2] </p>
<p>summarize the FinQA dataset, there are 8,281 examples (question-answer pairs) from 2,789 pages of reports. The vocabulary consists of 22.3 thousand words. The average number of sentences in the input text is about 24. The average number of tokens in the input text is about 628. The average number of rows in the input tables is about 6. The average number of tokens in the input tables is about 59. The average number of tokensin all inputs is about 689. The maximum number of tokens in all inputs is 2,679. Finally, the average question length is about 17.</p>
<p>Fig. 3 .
3Retriever-Generator Baseline Architecture for FinQA</p>
<p>Fig.
Fig. 4. Few-shot learning with GPT-3. By varying the task given to GPT3, we find that the best performance occurs when retrieval and calculation happen externally from the model. Furthermore the model becomes more directionally correct with these external components, as predictions that were not exactly right shift closer to true answers</p>
<p>, inc . management's financial discussion and analysis gross operating revenues , fuel and purchased power expenses , and other regulatory credits gross operating revenues increased primarily due to : 2022 an increase of $ 98.0 million in fuel cost recovery revenues due to higher fuel rates...the industrial sector including the loss of a large industrial customer to cogeneration.. question: what is the growth rate in net revenue in 2003 for entergy louisiana , inc.? answer: 5.5 passage: investment advisory revenues earned on the other investment portfolios that we manage decreased $ 44 million , or 8.5% ( 8.5 % ) , to $ 477.8 million in 2009 . average assets in...decreased 1868 gwh in the industrial sector including the loss of a large industrial customer to cogeneration. . : what was the average price of shares repurchased in 2010? answer: Fig.6. Example Prompt with Full Passage context passage: our cash flow metric is reconciled to the most comparable gaap measure , as follows: . ( dollars in millions ) the net cash provided by operating activities of 2013 is $ 1807...the net cash provided by operating activities of 2012 is $ 1758 ; the net cash provided by operating activities of 2011 is $ 1595 ; ( dollars in millions ) the cash flow of 2013 is $ 1170 ; the cash flow of 2012 is $ 1225 ; the cash flow of 2011 is $ 1001 ; question: what was the average cash flow from 2011 to 2013 in millions program: divide(add(add(1001, add(1170, 1225)), const 3), const 2) passage: the weighted average interest rate under the outstanding term loans and revolving credit facility bor-rowings was 1.6% ( 1.6 % ) and 1.3% ( 1.3 % ) during the years ended december 31 , 2016 and 2015 , respectively . as of december 31 , 2016 and 2015 , the outstanding balance on the loan was $ 42.0 million and $ 44.0 million , respectively . the weighted average interest rate on the loan was 2.0% ( 2.0 question: what is the interest expense based on the average outstanding loan balance in 2016? program: Fig.7. Example Prompt for Retrieved Passage context with program output</p>
<p>Table 1. FinQA baseline performance with retriever/generator models. The retriever is given in the row description, while the program generator is a trained LSTM modelBaselines 
Exe Acc 
Prog Acc 
FinQANet (BERT) 
50 
48 
FinQANet (RoBERTa) 
61.24 
58.86 
Human Expert 
91.16 
87.49 
General Crowd 
50.68 
48.17 </p>
<p>for examples. For each experiment, we average results across 20 test examples.</p>
<p>Table 2 .
2GPT-3 experiment details</p>
<p>Table 3 .
3Retriever / Generator FinQA experiment details# GPT-3 
Response 
Gold answer </p>
<p>29 
15% 
14.30% </p>
<p>30 
13 
11.3 </p>
<p>31 
9% 
87% </p>
<p>32 
-13% 
-21% </p>
<p>33 
784000 
778000 </p>
<p>34 
0.4 
2.62 </p>
<p>35 
4.8 
4.9 </p>
<p>36 
3.50% 
3.40% </p>
<p>37 
37.81% 
37.81 </p>
<p>38 
0.8 
3.80% </p>
<p>ACKNOWLEDGEMENTSThe authors would like to thank for their families support elaborating this work as wellas Stanford University's Department of Computer Science for all the learning acquired on the areas embraced by this publication.AUTHORSImran Qreshi is currently working at Google developing large language models for customer-specific use cases. He is currently pursuing a Masters in Computer Science from University of Texas Austin. His research interests include natural language processing and algorithms.Mustafa Karakaplan holds a PhD in economics from Texas AM University. He is cur-rently a Professor of Finance at the University of South Carolina and he is specializing in artificial intelligence and machine learning at Stanford University Appendix
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-Tan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>. Zhiyu Chen, Wenhu Chen, Charese Smiley, Shah, Sameena, Borova, Iana, Dylan Langdon, Reema Moussa, Matt Beane, Huang, Ting-Hao, Bryan Routledge, Others, arXiv:2109.00122.2021Network. 136arXiv preprintChen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and others. arXiv preprint arXiv:2109.00122. 2021. Network 13 (6) (1999) 24-30.</p>
<p>Jacob Devlin, Chang , Ming-Wei Lee, Kenton Toutanova, Kristina Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. 2018.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Stoyanov, Veselin, Roberta, arXiv:1907.11692.2019A robustly optimized bert pretraining approach. arXiv preprintLiu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019</p>
<ol>
<li>ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. Omar Khattab, Matei Zaharia, Association for Computing MachineryNew York, NY, USAOmar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contex- tualized Late Interaction over BERT, page 39-48. Association for Computing Machinery, New York, NY, USA.</li>
</ol>
<p>Relevance-guided Supervision for OpenQA with ColBERT. Omar Khattab, Christopher Potts, Matei Zaharia, Transactions of the Association for Computational Linguistics. 9Omar Khattab, Christopher Potts, and Matei Zaharia. 2021b. Relevance-guided Supervision for OpenQA with ColBERT. Transactions of the Association for Computational Linguistics, 9:929-944.</p>
<p>Retrievalaugmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Ku ẗtler, Mike Lewis, Wen-Tau Yih, Tim , Sebastian Riedel, Douwe Kiela, Advances in Neural Information Processing Systems. Curran Associates, Inc33Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Ku ẗtler, Mike Lewis, Wen-tau Yih, Tim Rockt äschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval- augmented generation for knowledge-intensive nlp tasks. In Advances in Neural Information Processing Systems, volume 33, pages 9459-9474. Curran Associates, Inc.</p>
<p>Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers. Kostadin Mishev, Ana Gjorgjevikj, Irena Vodenska, Chitkushev, T Lubomir, Tra-Janov, Dimitar, Access, 10.1109/ACCESS.2020.30096268Mishev, Kostadin and Gjorgjevikj, Ana and Vodenska, Irena and Chitkushev, Lubomir T. and Tra-janov, Dimitar, journal=IEEE Access, Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers, 2020, 8, pages 131662-131682, 10.1109/ACCESS.2020.3009626</p>
<p>Finbert: Financial sentiment analysis with pre-trained language models. Dogu Araci, abs/1908.10063CoRRDogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models. CoRR, abs/1908.10063.</p>
<p>Finbert: A pre-trained financial language representation model for financial text mining. Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial IntelligenceZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2021. Finbert: A pre-trained financial language representation model for financial text mining. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 4513-4519.</p>
<p>Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. Xinyi Zheng, Douglas Burdick, Lucian Popa, Zhong, Xu, Nancy Xin Wang, Ru, Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru, Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. 2021.</p>
<p>. Prompt-Engineering With Gpt3 -Google, Colab, Prompt-Engineering with GPT3 -Google Colab. https://colab.research.google.com/drive/1P_QoRp- _cSZtRPSwhV0YBzkQRwqDSSQ5#scrollTo=7dQeON3i8wFb</p>
<p>Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, arXiv:2205.11502Guy Van den Broeck. On the Paradox of Learning to Reason from Data. arXiv preprintHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van den Broeck. On the Paradox of Learning to Reason from Data. arXiv preprint arXiv:2205.11502, 2022.</p>
<p>Safeguarding cryptographic keys. G R Blakley, Proceed-ings of the National Computer Conference, American Federation of Information, Processing Societies Proceedings. eed-ings of the National Computer Conference, American Federation of Information, essing Societies eedings48G.R. Blakley, Safeguarding cryptographic keys, in: Proceed-ings of the National Computer Conference, American Federation of Information, Processing Societies Proceedings, vol. 48, 1979, pp. 313-317.</p>
<p>How to share a secret. A Shamir, Communications of the ACM. 22A. Shamir, How to share a secret, Communications of the ACM 22 (1979) 612-613.</p>
<p>Moca : Mobile certificate authority for wireless ad hoc networks. Seung Yi, Robin Kravetso, the second anunual PKI research workshop. GaithersburgPKI 03Seung Yi and Robin Kravetso. Moca : Mobile certificate authority for wireless ad hoc networks. In the second anunual PKI research workshop (PKI 03), Gaithersburg, 2003.</p>
<p>Short Signatures from the Weil Pairing. Dan Boneh, Ben Lynn, Hovav Shacham, Journal of Cryptology. 17Dan Boneh, Ben Lynn, and Hovav Shacham (2004). "Short Signatures from the Weil Pairing". Journal of Cryptology. 17: 297-319.</p>
<p>Adam: A method for stochastic optimization. Jimmy Ba Diederik, P Kingma, arXiv:1412.6980arXiv preprintJimmy Ba Diederik P. Kingma. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980</p>
<p>A survey of security issues in mobile ad hoc networks. Djamel Djenouri, L Khelladi, N Badache, IEEE communications surveys. 7Djenouri, Djamel, L. Khelladi, and N. Badache. "A survey of security issues in mobile ad hoc networks." IEEE communications surveys 7.4 (2005): 2-28.</p>
<p>Cryptography and Network Security: Principles and Practice. William Stallings, Prentice Hall9780138690175165Stallings, William (1990-05-03). Cryptography and Network Security: Principles and Practice. Prentice Hall. p. 165. ISBN 9780138690175.</p>
<p>URSA: ubiquitous and robust access control for mobile ad hoc networks. H Luo, J Kong, P Zerfos, S Lu, L Zhang, IEEE/ACM Transactions on Networking. 126H. Luo, J. Kong, P. Zerfos, S. Lu, L. Zhang, URSA: ubiquitous and robust access control for mobile ad hoc networks, IEEE/ACM Transactions on Networking 12 (6) (2004).</p>
<p>On the utility of distributed cryptography in P2P and MANETs: the case of membership control. M Narasimha, G Tsudik, J H Yi, Proceedings of ICNP203. ICNP203M. Narasimha, G. Tsudik, J.H. Yi, On the utility of distributed cryptography in P2P and MANETs: the case of membership control, in: Proceedings of ICNP203, 2003, pp. 336-345.</p>
<p>An attack on the proactive RSA signature scheme in the URSA ad hoc network access control protocol. S Jarecki, N Saxena, J H Yi, Proceedings of the SASN04. the SASN0419S. Jarecki, N. Saxena, J.H. Yi, An attack on the proactive RSA signature scheme in the URSA ad hoc network access control protocol, in: Proceedings of the SASN04, 2004, pp. 19.</p>
<p>Perfectly-secure key distribu-tion for dynamic conferences. C Blundo, A Santis, A Herzberg, S Kutten, U Vaccaro, M Yung, Proceedings of Crypto92. Crypto92Springer-Verlag740C. Blundo, A. De Santis, A. Herzberg, S. Kutten, U. Vaccaro, M. Yung, Perfectly-secure key distribu-tion for dynamic conferences, in: Proceedings of Crypto92, LNCS, vol. 740, Springer-Verlag, 1993, pp. 471-486.</p>
<p>A quick group key distribution scheme with entity revocation. J Anzai, N Matsuzaki, T Matsumoto, Proceedings of Asiacrypt99. Asiacrypt99Springer-Verlag1716J. Anzai, N. Matsuzaki, T. Matsumoto, A quick group key distribution scheme with entity revocation, in: Proceedings of Asiacrypt99, LNCS, vol. 1716, Springer-Verlag, 1999, pp. 333-347.</p>
<p>Constructing general dynamic group key distribution schemes with decentralized user join. V Daza, J Herranz, G Sez, Proceedings of ACISP03. ACISP03Springer-Verlag2727V. Daza, J. Herranz, G. Sez, Constructing general dynamic group key distribution schemes with decentralized user join, in: Proceedings of ACISP03, LNCS, vol. 2727, Springer-Verlag, 2003, pp. 464-475.</p>            </div>
        </div>

    </div>
</body>
</html>
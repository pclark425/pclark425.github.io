<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7451 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7451</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7451</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-269757479</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.06703v1.pdf" target="_blank">Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T), a novel approach that leverages structured multi-prompt techniques with Large Language Models (LLMs) to improve classification performance over zero-shot and few-shot methods. In domains where interpretability is crucial, such as medicine and law, standard models often fall short due to their"black-box"nature. ICE-T addresses these limitations by using a series of generated prompts that allow an LLM to approach the problem from multiple directions. The responses from the LLM are then converted into numerical feature vectors and processed by a traditional classifier. This method not only maintains high interpretability but also allows for smaller, less capable models to achieve or exceed the performance of larger, more advanced models under zero-shot conditions. We demonstrate the effectiveness of ICE-T across a diverse set of data sources, including medical records and legal documents, consistently surpassing the zero-shot baseline in terms of classification metrics such as F1 scores. Our results indicate that ICE-T can be used for improving both the performance and transparency of AI applications in complex decision-making environments.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7451.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7451.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ICE-T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpretable Cross-Examination Technique</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured multi-prompt method that elicits a primary yes/no answer plus multiple secondary yes/no answers from an LLM, verbalizes them to numeric features (Yes=1, No=0, Unknown=0.5), and feeds the resulting low-dimensional feature vector into a traditional classifier to improve binary classification performance and interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125, gpt-4-0125-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained autoregressive transformer LLMs from OpenAI used for answer generation; models are used off-the-shelf for prompting (no fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (OpenAI GPT-3.5 and GPT-4 family models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple binary classification tasks (17 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of 17 binary classification tasks across domains (clinical trial criteria extraction, stance detection on Catalonia tweets, climate-paragraph detection, health-advice detection, ECtHR case violation detection, Terms-of-Service unfair clause detection, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Structured multi-prompt yes/no Q&A (ICE-T) compared to single-question zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Primary question (q0) plus n secondary yes/no questions (main experiments use n=4, i.e., 5 answers total). Secondary questions were auto-generated (in experiments) by GPT-4 using a single prompt: 'Return {n} yes/no questions ...'. For each document the same set Q of questions is used at training and inference. LLM outputs constrained to {Yes, No, Unknown}; verbalized mapping Yes=1, No=0, Unknown=0.5. Resulting vector (|Q| dim) used to train traditional classifiers (KNN, Decision Trees, Random Forest, Gaussian/Multinomial NB, AdaBoost, XGBoost) with 5-fold cross-validation and grid search selecting classifier by micro F1. Sensitivity analysis varied number of secondary questions up to 9 (10-dimensional vectors including primary).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average µF1 with ICE-T: GPT-3.5 = 0.845 (µF1); GPT-4 = 0.892 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Average zero-shot µF1: GPT-3.5 = 0.683; GPT-4 = 0.700</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>GPT-3.5: +0.162 absolute increase (0.683 -> 0.845); GPT-4: +0.192 absolute increase (0.700 -> 0.892)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Primary experiments: n=4 secondary questions; secondary questions generated by GPT-4; answers collected using gpt-3.5-turbo-0125 and gpt-4-0125-preview; outputs limited to Yes/No/Unknown; classifiers selected via 5-fold CV optimizing micro F1; verbalization mapping 1/0/0.5. Sensitivity analysis: n up to 9 secondary questions, responses from gpt-3.5, Random Forest, 100 repetitions with random feature selection.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7451.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENGLISH (clinical) example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clinical 'Does the patient speak English?' classification task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One of the clinical trial sub-tasks where ICE-T dramatically improved performance relative to zero-shot prompting on the task of determining whether a patient speaks English from medical records.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-0125-preview (explicitly cited for this example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 family preview model used for generating answers; used in zero-shot and ICE-T conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (GPT-4 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ENGLISH (clinical eligibility criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of whether a patient's medical records indicate they speak English.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot single primary yes/no question vs ICE-T multi-question verbalization (primary + 4 secondary yes/no prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot prompt: primary question only presenting MEDICAL RECORDS and asking 'Does this patient speak English...'? ICE-T prompt: same primary question plus 4 secondary targeted yes/no questions (language preference, interpreter request, primary language listed, prior consultations in English). Responses verbalized to numeric features.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICE-T (GPT-4) µF1 = 0.966</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot (GPT-4) µF1 = 0.233</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.733 absolute increase (from 0.233 to 0.966)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n=4 secondary questions (those listed in Appendix A for ENGLISH); GPT-4 used to collect answers; numeric mapping Yes=1, No=0, Unknown=0.5; classifier trained on feature vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7451.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CREATININE (clinical) example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clinical 'serum creatinine above normal' classification task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A clinical sub-task where ICE-T produced a large improvement in micro F1 compared to the zero-shot single-question approach on identifying elevated serum creatinine from records.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>same LLM used in comparison (model not explicitly singled out in the example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI LLM(s) (either GPT-3.5 or GPT-4 were used in experiments generally); the text reports ‘same large language model’ for this task comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CREATININE (clinical eligibility criterion)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of whether a patient's records indicate serum creatinine levels above the upper limit of normal.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot single primary yes/no question vs ICE-T multi-question verbalization (primary + several secondary yes/no prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Secondary questions probe presence/documentation/results of creatinine tests, physician commentary, etc.; responses constrained to Yes/No/Unknown and converted to numeric features used by a classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICE-T µF1 = 0.721</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot µF1 = 0.349</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.372 absolute increase (from 0.349 to 0.721)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n=4 secondary questions in main experiments; same question set used at train and inference; classifiers selected via 5-fold CV.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7451.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UNFAIR-ToS example (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UNFAIR-ToS sentence-level unfair-term detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Detecting unfair clauses in Terms-of-Service sentences where ICE-T using GPT-3.5 produced very large gains over zero-shot GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 preview model used to generate answers for both zero-shot and ICE-T settings in the UNFAIR-ToS task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>UNFAIR-ToS (sentence-level binary unfair-term classification)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary classification of whether a sentence from an online Terms-of-Service document contains an unfair clause.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot single-question prompt vs ICE-T multi-question structured prompting (primary + 4 secondary yes/no prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Primary question asks about violation of unfair terms directive; secondary questions probe imbalance, transparency, negotiability, and subject matter relevance. Responses mapped to numeric features for classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICE-T (GPT-3.5) µF1 = 0.887</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot (GPT-3.5) µF1 = 0.335</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.552 absolute increase (from 0.335 to 0.887)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n=4 secondary questions generated by GPT-4 and answered by GPT-3.5 in experiments; numeric mapping and classifier training as per ICE-T pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7451.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ECtHR (court cases) example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>European Court of Human Rights violation detection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A task with already high zero-shot performance where ICE-T provides only marginal improvement, demonstrating that format/prompting gains are task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125, gpt-4-0125-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Both GPT-3.5 and GPT-4 models were evaluated in zero-shot and ICE-T settings for this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ECtHR (binary detection of any ECHR article violation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given case facts (paragraphs), predict whether any article of the European Convention on Human Rights was violated.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot whole-case question vs ICE-T multi-question verbalization (primary + 4 secondary prompts about specific article categories)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Secondary prompts probe specific categories (e.g., Article 2/3 protection, discrimination, privacy/freedom of expression, fair trial). Responses converted to numeric features for classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ICE-T µF1 = 0.873 (both models)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot µF1 = 0.853 (GPT-3.5), 0.861 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Small absolute gains: +0.020 (GPT-3.5) and +0.012 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n=4 secondary questions; same ICE-T pipeline; classifier selection via CV.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7451.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Feature-count sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity analysis of number of secondary questions (feature count)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing micro F1 improves as more secondary yes/no features are added, quantified via repeated Random Forest experiments with features added incrementally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125 (used for response generation in sensitivity analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used to produce yes/no/unknown answers for up to 9 secondary questions per instance; these answers form up to 10-dimensional feature vectors (primary + secondaries).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate across 17 datasets (sensitivity study)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Investigate how µF1 changes as the number of ICE-T features increases (1 through 10 features) on average across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ICE-T with varying number of secondary yes/no prompts (1..9 secondaries)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / ablation study</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Generated n=9 secondary questions, collected responses via gpt-3.5-turbo-0125, formed 10-dim feature vectors, trained Random Forest classifiers starting with 1 random feature and incrementally adding features up to 10; repeated 100 times for randomness; averaged µF1 over iterations and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average µF1 increases from 0.76 (with fewer features) to 0.80 after adding three secondary questions, and to ~0.82 with additional features (averaged across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>µF1 ≈ 0.76 with minimal features (baseline in sensitivity study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Addition of 3 secondary questions: +0.04 absolute (0.76 -> 0.80); additional features produced further gains to ≈0.82 on average.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Responses generated with gpt-3.5-turbo-0125, Random Forest classifier fixed for this ablation, 100 repetitions for random feature selection, averaged across 17 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7451.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-size tradeoff via ICE-T</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using ICE-T to close or invert performance gaps between smaller and larger LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that a smaller model (GPT-3.5) using ICE-T can match or outperform a larger model (GPT-4) operating in a zero-shot single-prompt setting across nearly all evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (with ICE-T) vs GPT-4 (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparison between GPT-3.5 with the ICE-T multi-prompt+classifier pipeline and GPT-4 used in a direct zero-shot single-prompt setting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple binary classification tasks (17 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare average micro F1 across tasks between GPT-3.5+ICE-T and GPT-4 zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>ICE-T multi-question feature extraction (on GPT-3.5) versus single-question zero-shot (on GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / model comparison</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>The paper reports per-task µF1s and includes Figure 2 comparing GPT-3.5+ICE-T to GPT-4 zero-shot; ICE-T uses n=4 secondaries in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-3.5 with ICE-T average µF1 = 0.845</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>GPT-4 zero-shot average µF1 = 0.700</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+0.145 absolute advantage of GPT-3.5+ICE-T over GPT-4 zero-shot on average (and GPT-3.5+ICE-T outperformed or equaled GPT-4 zero-shot in nearly all tasks except two).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>n=4 secondary questions; question generation and verbalization as in ICE-T; classifier selection via CV; comparison visualized in Figure 2.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7451.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot single-question prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot single-prompt classification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline format where a single prompt containing the document and one carefully designed question (the primary question) is presented to the LLM and the model's raw answer is used directly for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0125, gpt-4-0125-preview</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard zero-shot usage of pretrained OpenAI LLMs where the model is asked the primary yes/no question without additional structured secondary prompts or downstream classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple binary classification tasks (17 datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Baseline single-prompt classification across the same datasets used for ICE-T comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Single natural-language primary question prompt (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Primary question prompt includes the document and a single explicit question; no few-shot examples or secondary decompositions; model raw answer treated as classification prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro F1 (µF1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Average zero-shot µF1: GPT-3.5 = 0.683; GPT-4 = 0.700</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot experiments used the same primary question as in ICE-T (question id=0 in Appendix A); answers collected from gpt-3.5-turbo-0125 and gpt-4-0125-preview.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7451.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7451.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-thought (CoT) prompting (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks LLMs to produce step-by-step intermediate reasoning before giving a final answer; cited as improving reasoning on several benchmarks but also noted to sometimes introduce inference-time errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>chain-of-thought prompt (explicit 'Let's think step by step' style prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Requests chain-of-thought rationales; used in related work and contrasted with ICE-T (not used as the ICE-T core mechanism in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Automatic Chain of Thought prompting in large language models <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7451",
    "paper_id": "paper-269757479",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "ICE-T",
            "name_full": "Interpretable Cross-Examination Technique",
            "brief_description": "A structured multi-prompt method that elicits a primary yes/no answer plus multiple secondary yes/no answers from an LLM, verbalizes them to numeric features (Yes=1, No=0, Unknown=0.5), and feeds the resulting low-dimensional feature vector into a traditional classifier to improve binary classification performance and interpretability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125, gpt-4-0125-preview",
            "model_description": "Pretrained autoregressive transformer LLMs from OpenAI used for answer generation; models are used off-the-shelf for prompting (no fine-tuning).",
            "model_size": "not specified (OpenAI GPT-3.5 and GPT-4 family models)",
            "task_name": "Multiple binary classification tasks (17 datasets)",
            "task_description": "A set of 17 binary classification tasks across domains (clinical trial criteria extraction, stance detection on Catalonia tweets, climate-paragraph detection, health-advice detection, ECtHR case violation detection, Terms-of-Service unfair clause detection, etc.).",
            "problem_format": "Structured multi-prompt yes/no Q&A (ICE-T) compared to single-question zero-shot prompting",
            "format_category": "prompt style / question type",
            "format_details": "Primary question (q0) plus n secondary yes/no questions (main experiments use n=4, i.e., 5 answers total). Secondary questions were auto-generated (in experiments) by GPT-4 using a single prompt: 'Return {n} yes/no questions ...'. For each document the same set Q of questions is used at training and inference. LLM outputs constrained to {Yes, No, Unknown}; verbalized mapping Yes=1, No=0, Unknown=0.5. Resulting vector (|Q| dim) used to train traditional classifiers (KNN, Decision Trees, Random Forest, Gaussian/Multinomial NB, AdaBoost, XGBoost) with 5-fold cross-validation and grid search selecting classifier by micro F1. Sensitivity analysis varied number of secondary questions up to 9 (10-dimensional vectors including primary).",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "Average µF1 with ICE-T: GPT-3.5 = 0.845 (µF1); GPT-4 = 0.892 (µF1)",
            "baseline_performance": "Average zero-shot µF1: GPT-3.5 = 0.683; GPT-4 = 0.700",
            "performance_change": "GPT-3.5: +0.162 absolute increase (0.683 -&gt; 0.845); GPT-4: +0.192 absolute increase (0.700 -&gt; 0.892)",
            "experimental_setting": "Primary experiments: n=4 secondary questions; secondary questions generated by GPT-4; answers collected using gpt-3.5-turbo-0125 and gpt-4-0125-preview; outputs limited to Yes/No/Unknown; classifiers selected via 5-fold CV optimizing micro F1; verbalization mapping 1/0/0.5. Sensitivity analysis: n up to 9 secondary questions, responses from gpt-3.5, Random Forest, 100 repetitions with random feature selection.",
            "statistical_significance": null,
            "uuid": "e7451.0",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ENGLISH (clinical) example",
            "name_full": "Clinical 'Does the patient speak English?' classification task",
            "brief_description": "One of the clinical trial sub-tasks where ICE-T dramatically improved performance relative to zero-shot prompting on the task of determining whether a patient speaks English from medical records.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-0125-preview (explicitly cited for this example)",
            "model_description": "GPT-4 family preview model used for generating answers; used in zero-shot and ICE-T conditions.",
            "model_size": "not specified (GPT-4 family)",
            "task_name": "ENGLISH (clinical eligibility criterion)",
            "task_description": "Binary classification of whether a patient's medical records indicate they speak English.",
            "problem_format": "Zero-shot single primary yes/no question vs ICE-T multi-question verbalization (primary + 4 secondary yes/no prompts)",
            "format_category": "prompt style / question type",
            "format_details": "Zero-shot prompt: primary question only presenting MEDICAL RECORDS and asking 'Does this patient speak English...'? ICE-T prompt: same primary question plus 4 secondary targeted yes/no questions (language preference, interpreter request, primary language listed, prior consultations in English). Responses verbalized to numeric features.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "ICE-T (GPT-4) µF1 = 0.966",
            "baseline_performance": "Zero-shot (GPT-4) µF1 = 0.233",
            "performance_change": "+0.733 absolute increase (from 0.233 to 0.966)",
            "experimental_setting": "n=4 secondary questions (those listed in Appendix A for ENGLISH); GPT-4 used to collect answers; numeric mapping Yes=1, No=0, Unknown=0.5; classifier trained on feature vectors.",
            "statistical_significance": null,
            "uuid": "e7451.1",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "CREATININE (clinical) example",
            "name_full": "Clinical 'serum creatinine above normal' classification task",
            "brief_description": "A clinical sub-task where ICE-T produced a large improvement in micro F1 compared to the zero-shot single-question approach on identifying elevated serum creatinine from records.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "same LLM used in comparison (model not explicitly singled out in the example)",
            "model_description": "OpenAI LLM(s) (either GPT-3.5 or GPT-4 were used in experiments generally); the text reports ‘same large language model’ for this task comparison.",
            "model_size": "not specified",
            "task_name": "CREATININE (clinical eligibility criterion)",
            "task_description": "Binary classification of whether a patient's records indicate serum creatinine levels above the upper limit of normal.",
            "problem_format": "Zero-shot single primary yes/no question vs ICE-T multi-question verbalization (primary + several secondary yes/no prompts)",
            "format_category": "prompt style / question type",
            "format_details": "Secondary questions probe presence/documentation/results of creatinine tests, physician commentary, etc.; responses constrained to Yes/No/Unknown and converted to numeric features used by a classifier.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "ICE-T µF1 = 0.721",
            "baseline_performance": "Zero-shot µF1 = 0.349",
            "performance_change": "+0.372 absolute increase (from 0.349 to 0.721)",
            "experimental_setting": "n=4 secondary questions in main experiments; same question set used at train and inference; classifiers selected via 5-fold CV.",
            "statistical_significance": null,
            "uuid": "e7451.2",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "UNFAIR-ToS example (GPT-3.5)",
            "name_full": "UNFAIR-ToS sentence-level unfair-term detection",
            "brief_description": "Detecting unfair clauses in Terms-of-Service sentences where ICE-T using GPT-3.5 produced very large gains over zero-shot GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125",
            "model_description": "GPT-3.5 preview model used to generate answers for both zero-shot and ICE-T settings in the UNFAIR-ToS task.",
            "model_size": "not specified (GPT-3.5 family)",
            "task_name": "UNFAIR-ToS (sentence-level binary unfair-term classification)",
            "task_description": "Binary classification of whether a sentence from an online Terms-of-Service document contains an unfair clause.",
            "problem_format": "Zero-shot single-question prompt vs ICE-T multi-question structured prompting (primary + 4 secondary yes/no prompts)",
            "format_category": "prompt style / question type",
            "format_details": "Primary question asks about violation of unfair terms directive; secondary questions probe imbalance, transparency, negotiability, and subject matter relevance. Responses mapped to numeric features for classifier.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "ICE-T (GPT-3.5) µF1 = 0.887",
            "baseline_performance": "Zero-shot (GPT-3.5) µF1 = 0.335",
            "performance_change": "+0.552 absolute increase (from 0.335 to 0.887)",
            "experimental_setting": "n=4 secondary questions generated by GPT-4 and answered by GPT-3.5 in experiments; numeric mapping and classifier training as per ICE-T pipeline.",
            "statistical_significance": null,
            "uuid": "e7451.3",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "ECtHR (court cases) example",
            "name_full": "European Court of Human Rights violation detection",
            "brief_description": "A task with already high zero-shot performance where ICE-T provides only marginal improvement, demonstrating that format/prompting gains are task-dependent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125, gpt-4-0125-preview",
            "model_description": "Both GPT-3.5 and GPT-4 models were evaluated in zero-shot and ICE-T settings for this dataset.",
            "model_size": "not specified",
            "task_name": "ECtHR (binary detection of any ECHR article violation)",
            "task_description": "Given case facts (paragraphs), predict whether any article of the European Convention on Human Rights was violated.",
            "problem_format": "Zero-shot whole-case question vs ICE-T multi-question verbalization (primary + 4 secondary prompts about specific article categories)",
            "format_category": "prompt style / question type",
            "format_details": "Secondary prompts probe specific categories (e.g., Article 2/3 protection, discrimination, privacy/freedom of expression, fair trial). Responses converted to numeric features for classifier.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "ICE-T µF1 = 0.873 (both models)",
            "baseline_performance": "Zero-shot µF1 = 0.853 (GPT-3.5), 0.861 (GPT-4)",
            "performance_change": "Small absolute gains: +0.020 (GPT-3.5) and +0.012 (GPT-4)",
            "experimental_setting": "n=4 secondary questions; same ICE-T pipeline; classifier selection via CV.",
            "statistical_significance": null,
            "uuid": "e7451.4",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Feature-count sensitivity",
            "name_full": "Sensitivity analysis of number of secondary questions (feature count)",
            "brief_description": "Analysis showing micro F1 improves as more secondary yes/no features are added, quantified via repeated Random Forest experiments with features added incrementally.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125 (used for response generation in sensitivity analysis)",
            "model_description": "GPT-3.5 used to produce yes/no/unknown answers for up to 9 secondary questions per instance; these answers form up to 10-dimensional feature vectors (primary + secondaries).",
            "model_size": "not specified (GPT-3.5 family)",
            "task_name": "Aggregate across 17 datasets (sensitivity study)",
            "task_description": "Investigate how µF1 changes as the number of ICE-T features increases (1 through 10 features) on average across tasks.",
            "problem_format": "ICE-T with varying number of secondary yes/no prompts (1..9 secondaries)",
            "format_category": "prompt style / ablation study",
            "format_details": "Generated n=9 secondary questions, collected responses via gpt-3.5-turbo-0125, formed 10-dim feature vectors, trained Random Forest classifiers starting with 1 random feature and incrementally adding features up to 10; repeated 100 times for randomness; averaged µF1 over iterations and tasks.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "Average µF1 increases from 0.76 (with fewer features) to 0.80 after adding three secondary questions, and to ~0.82 with additional features (averaged across tasks).",
            "baseline_performance": "µF1 ≈ 0.76 with minimal features (baseline in sensitivity study)",
            "performance_change": "Addition of 3 secondary questions: +0.04 absolute (0.76 -&gt; 0.80); additional features produced further gains to ≈0.82 on average.",
            "experimental_setting": "Responses generated with gpt-3.5-turbo-0125, Random Forest classifier fixed for this ablation, 100 repetitions for random feature selection, averaged across 17 datasets.",
            "statistical_significance": null,
            "uuid": "e7451.5",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Model-size tradeoff via ICE-T",
            "name_full": "Using ICE-T to close or invert performance gaps between smaller and larger LLMs",
            "brief_description": "Empirical observation that a smaller model (GPT-3.5) using ICE-T can match or outperform a larger model (GPT-4) operating in a zero-shot single-prompt setting across nearly all evaluated tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (with ICE-T) vs GPT-4 (zero-shot)",
            "model_description": "Comparison between GPT-3.5 with the ICE-T multi-prompt+classifier pipeline and GPT-4 used in a direct zero-shot single-prompt setting.",
            "model_size": "not specified",
            "task_name": "Multiple binary classification tasks (17 datasets)",
            "task_description": "Compare average micro F1 across tasks between GPT-3.5+ICE-T and GPT-4 zero-shot.",
            "problem_format": "ICE-T multi-question feature extraction (on GPT-3.5) versus single-question zero-shot (on GPT-4)",
            "format_category": "prompt style / model comparison",
            "format_details": "The paper reports per-task µF1s and includes Figure 2 comparing GPT-3.5+ICE-T to GPT-4 zero-shot; ICE-T uses n=4 secondaries in main experiments.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "GPT-3.5 with ICE-T average µF1 = 0.845",
            "baseline_performance": "GPT-4 zero-shot average µF1 = 0.700",
            "performance_change": "+0.145 absolute advantage of GPT-3.5+ICE-T over GPT-4 zero-shot on average (and GPT-3.5+ICE-T outperformed or equaled GPT-4 zero-shot in nearly all tasks except two).",
            "experimental_setting": "n=4 secondary questions; question generation and verbalization as in ICE-T; classifier selection via CV; comparison visualized in Figure 2.",
            "statistical_significance": null,
            "uuid": "e7451.6",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Zero-shot single-question prompting",
            "name_full": "Zero-shot single-prompt classification",
            "brief_description": "Baseline format where a single prompt containing the document and one carefully designed question (the primary question) is presented to the LLM and the model's raw answer is used directly for classification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0125, gpt-4-0125-preview",
            "model_description": "Standard zero-shot usage of pretrained OpenAI LLMs where the model is asked the primary yes/no question without additional structured secondary prompts or downstream classifier.",
            "model_size": "not specified",
            "task_name": "Multiple binary classification tasks (17 datasets)",
            "task_description": "Baseline single-prompt classification across the same datasets used for ICE-T comparisons.",
            "problem_format": "Single natural-language primary question prompt (zero-shot)",
            "format_category": "prompt style / question type",
            "format_details": "Primary question prompt includes the document and a single explicit question; no few-shot examples or secondary decompositions; model raw answer treated as classification prediction.",
            "performance_metric": "micro F1 (µF1)",
            "performance_value": "Average zero-shot µF1: GPT-3.5 = 0.683; GPT-4 = 0.700",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Zero-shot experiments used the same primary question as in ICE-T (question id=0 in Appendix A); answers collected from gpt-3.5-turbo-0125 and gpt-4-0125-preview.",
            "statistical_significance": null,
            "uuid": "e7451.7",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Chain-of-thought (CoT) prompting (related work)",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting method that asks LLMs to produce step-by-step intermediate reasoning before giving a final answer; cited as improving reasoning on several benchmarks but also noted to sometimes introduce inference-time errors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": "",
            "task_name": "",
            "task_description": "",
            "problem_format": "chain-of-thought prompt (explicit 'Let's think step by step' style prompting)",
            "format_category": "prompt style",
            "format_details": "Requests chain-of-thought rationales; used in related work and contrasted with ICE-T (not used as the ICE-T core mechanism in experiments).",
            "performance_metric": "",
            "performance_value": "",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "",
            "statistical_significance": null,
            "uuid": "e7451.8",
            "source_info": {
                "paper_title": "Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Automatic Chain of Thought prompting in large language models",
            "rating": 1,
            "sanitized_title": "automatic_chain_of_thought_prompting_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners",
            "rating": 1,
            "sanitized_title": "its_not_just_size_that_matters_small_language_models_are_also_fewshot_learners"
        }
    ],
    "cost": 0.014626499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance
8 May 2024</p>
<p>Goran Muric gmuric@inferlink.com 
InferLink Corporation
Los AngelesCalifornia</p>
<p>Ben Delay bdelay@inferlink.com 
InferLink Corporation
Los AngelesCalifornia</p>
<p>Steven Minton sminton@inferlink.com 
InferLink Corporation
Los AngelesCalifornia</p>
<p>Interpretable Cross-Examination Technique (ICE-T): Using highly informative features to boost LLM performance
8 May 20245C40926684AD5F20F4EC48E51C861156arXiv:2405.06703v1[cs.CL]
In this paper, we introduce the Interpretable Cross-Examination Technique (ICE-T), a novel approach that leverages structured multiprompt techniques with Large Language Models (LLMs) to improve classification performance over zero-shot and few-shot methods.In domains where interpretability is crucial, such as medicine and law, standard models often fall short due to their "black-box" nature.ICE-T addresses these limitations by using a series of generated prompts that allow an LLM to approach the problem from multiple directions.The responses from the LLM are then converted into numerical feature vectors and processed by a traditional classifier.This method not only maintains high interpretability but also allows for smaller, less capable models to achieve or exceed the performance of larger, more advanced models under zero-shot conditions.We demonstrate the effectiveness of ICE-T across a diverse set of data sources, including medical records and legal documents, consistently surpassing the zero-shot baseline in terms of classification metrics such as F1 scores.Our results indicate that ICE-T can be used for improving both the performance and transparency of AI applications in complex decision-making environments.</p>
<p>Introduction</p>
<p>There are numerous prompting strategies to achieve good performance using generative Large Language Models (LLMs).Take, for instance, a binary classification problem, where a system should classify the given text into one of two classes.A typical zero-shot approach is to prompt the model with a given text and carefully designed question, that will yield an appropriate answer.There are also multiple variations on that approach that include "chain-of-thought" prompting (Wei et al., 2022c;Wang et al., 2022a;Kojima et al., 2022), "few-shot learning" (Schick and Schütze, 2022;Gu et al., 2021), "self-instruct" (Wang et al., 2022b;Yang et al., 2024) prompting and "iterative refinement" (Wu et al., 2022a;Trautmann, 2023).These tactics are used to get a better sense of the model's underlying reasoning or to surpass the performance achieved by the standard zero-shot method.</p>
<p>These options are usually used in cases where using highly specialized fine-tuned LLMs is not a viable option because it is often of utmost importance to understand how decisions are made.This is especially true in fields like medicine, where decisions based on opaque, "black-box" models are usually not acceptable.Although zero-shot or fewshot prompting methods can potentially offer explanations for their reasoning, these explanations are often unstructured and lack quantifiability.On the other hand, while finely tuned models may achieve superior performance, they frequently struggle to articulate the rationale behind their outputs unless explicitly trained for this purpose, a process that is labor-intensive.Additionally, outputs from such models may also suffer from the lack of structured reasoning representation.</p>
<p>In cases where using "black-box" models is not practical, and where interpretability is important, users have the option to develop a structured reasoning process by asking several questions to achieve a desired output.There are three main problems that arise with this approach: 1) Non-experts have little chance to develop a good set of questions and rules that ensure optimal model performance; 2) Designing an accurate rule set becomes challenging since individual instances may not perfectly align with all desired criteria, resulting in a mix of positive and negative responses to different rules; and 3) The potential combinations of these rules can become overwhelmingly numerous, making it impractical to hard-code every possible scenario.</p>
<p>In the paper, we propose a method that attempts to overcome the three issues outlined above.We refer to the method as the Interpretable Cross-Examination Technique, or ICE-T for brevity.Our approach exhibits strong performance, consistently surpasses the benchmark set by a zero-shot baseline, and also offers a high level of interpretability.The core concept here is that rather than using a single prompt to get a response from an LLM and making a decision based on that single output, we engage the LLM with multiple prompts, covering various questions.We then combine the responses from all these prompts and use the outputs to make a decision.Compared to other methods that are based on multi-prompting, our approach is fundamentally different in the way the decisions are made.Specifically, we take the responses from the LLM, convert them into numerical values to create a feature vector, and then input this vector into a traditional classifier to determine the final outcome.Since, in this process we create a low-dimensional feature vector with highly informative features, we can then use relatively small classifiers to make a decision.</p>
<p>We established an experimental setup where we tested our Interpretable Cross-Examination Technique on a simple binary classification task.We tested our approach on a set of multiple datasets split on 17 different tasks and we show that:</p>
<ol>
<li>ICE-T consistently outperforms the zero-shot baseline model in most classification metrics 2. Using a smaller model with ICE-T we can achieve comparable or better results than using larger and essentially more capable model with zero-shot approach Furthermore, this approach can be highly interpretable, allowing experts to clearly understand the rationale behind the decision-making process 1 .Additionally, tools commonly used for tabular machine learning can be employed to enhance the understanding of the data.While this technique is specifically evaluated for binary classification within this paper, its applicability potentially extends across a broad spectrum of scenarios.</li>
</ol>
<p>Motivation</p>
<p>The ICE-T method was initially conceived at Infer-Link in a commercial consulting project, where we 1 Degree of interpretability may vary depending on the machine learning method selected for the final classification task.The decision on which method to employ should be guided by a consideration of the trade-offs between interpretability and performance tailored to the unique demands of each task needed to address a complex challenge in biomedical text classification.The project's goals were to develop a model that could perform at a level comparable to human experts, provide interpretable results, and allow for detection of potentially mislabelled data.Initially, conventional "black-box" models such like fine-tuned BERT-based ones underperformed, as well as zero-shot or few-shot learning methods using LLMs.This led to the creation of the ICE-T, which improved the performance of classification, while gaining interpretability and allowing for the correction of labeling errors.ICE-T was used initially for the purpose of classifying biomedical data for a specific commercial purpose.While the specifics of this initial task and data remain confidential, we have conducted further testing on additional publicly available datasets and decided to make the method publicly accessible.</p>
<p>Related Work</p>
<p>Our proposed solution addresses three core aspects of using large language models for inference: prompting, in-context learning, and interpretability.It is built on top of the ever-growing body of knowledge that comes from those domains.</p>
<p>Prompting techniques</p>
<p>Numerous techniques have been developed to improve the fundamental zero-shot approach.Among these, the "chain-of-thought" (CoT) prompting is particularly notable.This method is used to prompt the model to systematically articulate its reasoning process in a step-by-step manner before reaching a conclusion.Research has shown that chainof-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks (Wei et al., 2022b,c;Wang et al., 2022a).Even simple tweaks such as adding "Let's think step by step" before each answer can significantly outperform zero-shot LLM performances on diverse benchmark reasoning tasks (Kojima et al., 2022;Nye et al., 2021).Such generated chains that prompt language models to break down their reasoning into steps often cause errors in inference time.To reduce these errors, some researchers employ a method known as automatic Chain of Thought prompting.This technique, which generates demonstrable examples, has proven to be more effective than earlier, simpler CoT approaches (Zhang et al., 2022b).Lastly, "iterative refinement" involves repeatedly prompting the model with slightly altered versions of the original text or question, honing in on a more accurate or nuanced answer through successive iterations.Each of these strategies can be tailored to the specific needs of a task, leveraging the model's capabilities in different ways to achieve optimal performance.</p>
<p>Several approaches involve using multiple prompts in a chain, where the output of one step becomes the input for the next, thus aggregating the gains per step (Wu et al., 2022a), or decomposing complex tasks into smaller, manageable components (Trautmann, 2023).Additionally, "selfinstruct" (Wang et al., 2022b;Yang et al., 2024) prompting can be used, where the model generates its own instructions or clarifications based on the initial prompt, attempting to refine or better understand the task before generating a response.Another set of approaches uses multiple models or multiple instances of the same model to improve the performance.The additionally trained models, called "verifiers" are used to judge the correctness of model completions.At the inference time, the verifiers would select the most likely answer (Cobbe et al., 2021).</p>
<p>In-context learning</p>
<p>Large Language Models possess the remarkable ability for in-context learning (ICL), in which they acquire knowledge from a few contextual examples either during inference or during training.Numerous studies have shown that through ICL, LLMs can effectively handle a diverse set of complex tasks (Wei et al., 2022a).ICL offers several advantages, notably its ease in including human knowledge into LLMs by using various demonstrations and templates (Liu et al., 2021;Wu et al., 2022b).Furthermore, unlike traditional supervised training methods, ICL operates without the need for additional training, significantly lowering the computational costs when using models to solve new tasks (Dong et al., 2022).</p>
<p>One of the most recognizable techniques for incontext learning is "few-shot learning" (Schick andSchütze, 2022, 2020;Gu et al., 2021;Perez et al., 2021) during inference2 .Using this approach, the model is provided with a few examples of text and their corresponding labels or desired outputs within the prompt itself.This method teaches the model the context of the decision-making process, improving its accuracy on similar tasks.</p>
<p>Multiple other studies contributed to refining the ICL methods, focusing on automation, ordering, and selection of prompts.Zhou et al. (2022) introduced the Automatic Prompt Engineer (APE), which automates the generation of instructional prompts, significantly reducing manual effort and improving scalability (Zhou et al., 2022).Simultaneously, Lu et al. (2021) came up with the method to optimize the ordering of prompts.They employed entropy statistics to evaluate and identify the most effective prompt sequences (Lu et al., 2021).Rubin et al. (2021) and Liu et al. (2021) both contribute to this area but from different perspectives.Rubin et al. (2021) developed a method for efficiently retrieving prompts using annotated data, streamlining the selection process (Rubin et al., 2021).On the other hand, Liu et al. (2021) explored strategic selection methods that go beyond random sampling to leverage the few-shot capabilities of LLMs, aiming to enhance the model's performance through example selection (Liu et al., 2021).Adding to the discussion on selection strategies, Zhang et al. (2022) approached example selection as a sequential decision problem.They proposed using a reinforcement learning algorithm to discover policies that improve the generalizability of language models (Zhang et al., 2022a).This perspective introduces a dynamic element to the selection process, aligning with the strategies discussed by Rubin and Liu but through an adaptive, policy-driven approach.</p>
<p>Model interpretability</p>
<p>The challenge of interpreting complex decision processes made by LLMs has hindered their application in critical areas like medicine, where there are significant concerns about regulation (Goodman and Flaxman, 2017) and safety (Amodei et al., 2016).Furthermore, this difficulty in understanding the workings of large language models (LLMs) and similar neural network models has restricted their use in domains like science and data analysis (Kasneci et al., 2023).In such fields, the primary objective is often to derive a reliable interpretation rather than merely to implement an LLM (Singh et al., 2024).</p>
<p>The expression of uncertainty in language models is crucial for reliable LLM utilization, yet it remains a challenging area due to inherent overconfidence in model responses.Xiong et al. (2023) and Zhou et al. (2024) both highlight the overconfidence issue in LLMs.Xiong et al. question whether LLMs can express their uncertainty, observing a tendency in LLMs to mimic human patterns of expressing confidence (Xiong et al., 2023).Simlarly, Zhou et al. note that while LLMs can be prompted to express confidence levels, they remain generally overconfident and unable to convey uncertainties effectively, also when providing incorrect responses (Zhou et al., 2024).Ye et al. (2022) add that even when LLMs generate explanations, these may not accurately reflect the model's predictions nor be factually grounded in the input, particularly in tasks requiring extractive explanations (Ye and Durrett, 2022).However, all the research mentioned above note that these flawed explanations can still serve a purpose, offering a means to verify LLM predictions post-hoc.</p>
<p>It is worth mentioning feature attribution methods, used beyond the LLM realm in multiple deeplearning applications.Feature attributions in machine learning provide a relevance score to each input feature, reflecting its impact on the model's output.This methodology helps in understanding how and why certain decisions or predictions are made by a model.</p>
<p>The approaches developed by Lundberg et al. (2017) and Sundararajan et al. (2017) both delve into this topic but offer distinct methodologies and theoretical foundations.Lundberg et al. (Lundberg and Lee, 2017) introduced SHAP (SHapley Additive exPlanations), which provides a unified framework for interpreting predictions.SHAP assigns an importance value to each feature for a specific prediction, leveraging the concept of Shapley values from cooperative game theory.In contrast, Sundararajan et al. (Sundararajan et al., 2017) developed Integrated Gradients, another method focusing on the attribution of predictions to input features of deep networks.Unlike SHAP, which uses Shapley values, Integrated Gradients relies on the integration of gradients along the path from a chosen baseline to the actual input.Complementing these approaches, Ribeiro et al. (2016) proposed LIME (Local Interpretable Model-agnostic Explanations), which aims to make the predictions of any classifier understandable and reliable by learning an interpretable model localized around the prediction (Ribeiro et al., 2016).</p>
<p>Another popular method for understanding neural-network representations is probing.Conneau et al. (2018) initially introduced multiple probing tasks designed to capture simple linguistic features of sentences, setting a foundation for understanding how neural networks encode linguistic properties (Conneau et al., 2018).Clark et al. (2019) focused primarily on the behavior of attention heads within transformers.They observed that these heads often broadly attend across entire sentences, and that attention patterns in the same layer tend to exhibit similar behaviors.Crucially, their research links specific attention heads to traditional linguistic concepts like syntax and coreference, suggesting a direct relationship between the model's attention mechanisms and linguistic structures (Clark et al., 2019), although there is an ongoing debate on the explanatory power of attention in neural network (Bibal et al., 2022).Unlike Clark et al., who examine what the model attends to, Morris et al. (Morris et al., 2023) explore how information is preserved and can be retrieved from embeddings, offering insights into the reversibility and fidelity of the encoding process.Their method involves a multi-step process that iteratively corrects and re-embeds text, demonstrating the ability to recover most of the original text inputs exactly.Belrose et al. (2023) introduced a technique called causal basis extraction, which aims to identify influential features within neural networks (Belrose et al., 2023).This method stands out by focusing on the causality within network decisions.</p>
<p>In summary, while chain-of-thought prompting can generate errors during inference, requiring complex corrective approaches, in-context learning techniques also face challenges in prompt optimization and efficient retrieval.Furthermore, interpreting large language models remains problematic, exacerbated by models' tendency to exhibit overconfidence and provide unreliable or unverifiable explanations.</p>
<p>Method</p>
<p>Training the ICE-T system consists of the following steps:</p>
<ol>
<li>Generating questions: the process begins by generating a series of questions designed to prompt the Large Language Model (LLM);</li>
</ol>
<p>Generating questions</p>
<p>To train and use the system, we need to create multiple questions that more closely reflect the core principles behind the initial yes/no question.Those questions should be crafted in a way to uncover some additional details about the problem.</p>
<p>Consider a use case where an expert is building a classifier to determine eligibility for medical trials based on patient data.In such a scenario, the classifier needs to assess various clinical inclusion criteria, which are typically derived from patient medical records.One of these criteria could be the patient's language proficiency, for instance, whether they speak English.A naive formulation of this question may be to present the question to the LLM in a prompt like the following: Does this patient speak English according to their medical records.MEDICAL RECORDS: <strong>RECORDS</strong> where the <strong>RECORDS</strong> represents the appended textual medical records.Determining the answer to that question, which we call the "primary" question, may not be easy given the medical records under consideration, requiring an understanding of somewhat subtle indicators that show if a patient actually speaks English.It is highly unlikely that the medical records will directly state the answer to that question.</p>
<p>However, a series of "secondary" questions such as:</p>
<p>Is there any documentation of the patient requiring an interpreter for English during medical visits?Do the medical records contain notes written in English that indicate communication with the patient?Are there any written consents or forms completed in English by the patient?Are there any notations from providers about the patient's ability to understand and speak English?</p>
<p>may allow the model to answer directly based on the information already contained in the documents presented to it, while also serving as strong indicators for the primary question.Secondary questions are also yes/no questions.</p>
<p>Creating the secondary questions can be done in multiple ways, such as writing the questions manually using the expert knowledge or using the LLM to automatically generate a fixed size set of questions that might be useful in answering the original question.Starting from the primary question q 0 we generate n additional questions, creating a set of all questions Q = {q 0 , q 1 . . .q n }, where |Q| = n + 1.This process is shown in Figure 1 with a red box, illustrating the creation of the questions and using them during the training and inference process.The same set Q of questions is used for both training and inference.</p>
<p>The number n of secondary questions is decided based on factors such as: number of training samples, availability of the expert knowledge and the level of interpretability needed for a specific task.Our prior small-scale experiments have shown that secondary questions crafted by experts generally lead to improved performance compared to those generated by LLMs.However, in the experiments reported here, we chose a straightforward and reproducible approach where we exclusively use secondary questions created by an LLM.This choice was made to minimize human bias and showcase the method's effectiveness in scenarios where expert input is unavailable.The exact prompts used for creating secondary questions in our experiments are described in Section 5.</p>
<p>Prompting LLM</p>
<p>The LLMs are prompted in two occasions.First, they are prompted to obtain the set of secondary questions Q, as described in Section 3.1.Second, for each document, we prompt the LLM with the document and corresponding secondary questions.Then, for each question q i the output a i of the LLM is collected, creating a set of outputs for each document.The textual outputs are then assigned a numerical value and transformed into a feature vector v i , through the verbalization process explained in Section 3.3.In the training phase, the process begins by generating questions to prompt an LLM, which then provides yes/no answers.These answers are verbalized and converted into numerical feature vectors.A classifier is trained using these vectors along with their respective labels.During inference, the LLM is prompted with the same questions, and the answers are similarly processed to predict outcomes using the trained classifier.</p>
<p>Verbalizing the answers</p>
<p>The output of the LLM in response to each prompt is limited to one of three possible values: Yes, No, or Unknown, depending on the answer to the question posed in the prompt.These responses are subsequently assigned numerical values for analysis, with "Yes" translating to 1, "No" to 0, and "Unknown" to 0.5.</p>
<p>Training a classifier</p>
<p>To train a classifier, we use a set V of lowdimensional numerical vectors, where |V | = n + 1 and corresponding labels X, where each vector v i has a corresponding binary label x i .Vectors V are obtained from the training textual data after prompting LLM to generate n + 1 outputs that are then assigned a numerical value.A classifier is then trained using a 5-fold cross-validation process and grid search for the best parameters.A choice of a specific classification algorithm will depend on the size of training data, values distribution and desired performance on a specific classification metric.</p>
<p>Data</p>
<p>This work utilizes data compiled from a range of sources, attempting to include a variety of domains and document lengths.The data used in the experiments described here spans the fields of medicine, law, climate science, and politics.It also includes documents of varying sizes, from brief tweets to extensive legal documents and detailed medical records.</p>
<p>Clinical trials</p>
<p>This dataset comes from Track 1 of the 2018 National NLP Clinical Challenges (n2c2) shared tasks3 .It is designed to help in identifying patients within a corpus of longitudinal medical records who either meet or do not meet predefined selection criteria.These criteria are used for determining a patient's eligibility for inclusion in clinical trials.(Stubbs et al., 2019).The data consists of annotated American English clinical narratives for 288 patients according to whether they met a set of specific criteria.</p>
<p>Catalonia Independence Corpus</p>
<p>This dataset contains a corpus in Spanish that consist of annotated Twitter messages for automatic stance detection (Zotova et al., 2020).It encompasses data collected over a 12-day span in February and March 2019, from tweets originating in Barcelona.Originally, each tweet is categorized into one of three classes: AGAINST, FAVOR, and NEUTRAL.These classes represent the user's stance towards the topic of Catalonia's independence.For the purpose of binary classification and to facilitate more effective comparisons with other datasets, we have omitted the NEUTRAL class, focusing exclusively on the AGAINST and FAVOR categories.</p>
<p>Climate Detection Corpus</p>
<p>This dataset contains climate-related paragraphs extracted from financial disclosures by companies.</p>
<p>The text has been collected from corporate annual reports and sustainability reports.The paragraphs from those reports are hand-selected and then annotated as yes (climate-related) or no (not climaterelated) (Webersinke et al., 2021).</p>
<p>Medical health advice data</p>
<p>This dataset comprises a collection of sentences related to the medical domain, each accompanied by a label indicating whether the sentence offers medical advice.The labels can be one of three values: "strong advice", "weak advice", or "no advice".(Yu et al., 2019) For the purpose of binary classification task we combined "strong advice" and "weak advice" into a single class: "advice".The dataset includes approximately 8,000 samples, which have been divided into training and test datasets following the 80/20 rule.</p>
<p>The European Court of Human Rights (ECtHR) Data</p>
<p>The European Court of Human Rights (ECtHR) hears allegations that a state has breached human rights provisions of the European Convention of Human Rights (ECHR) (Chalkidis et al., 2019).</p>
<p>The dataset for each case includes a series of facts in form of paragraphs extracted from the case description.Additionally, each case is associated with specific articles of the European Convention on Human Rights (ECHR) that may have been violated.In many cases, multiple articles are violated at the same time.To make this a binary categorization problem, we adopted a binary labeling system.</p>
<p>Cases are marked with a "1" if any ECHR articles are violated, and a "0" if no violations are detected.</p>
<p>UNFAIR-ToS Dataset</p>
<p>The UNFAIR-ToS dataset contains 50 relevant online consumer contracts, i.e.Terms of Service (ToS) from on-line platforms (e.g., YouTube, Ebay, Facebook, etc.).Each agreement has been annotated at the sentence level to identify various types of potentially unfair clauses, which could infringe upon user rights under European consumer law.This dataset categorizes unfair terms into eight distinct groups: Arbitration, Unilateral Change, Content Removal, Jurisdiction, Choice of Law, Limitation of Liability, Unilateral Termination, and Contract by Using (Lippi et al., 2019).To transform the analysis into a binary classification problem, we re-labelled each sentence as either "unfair" if it contains any type of the identified unfair terms, or "not unfair' if it does not fall into these categories.</p>
<p>Experiments</p>
<p>We performed the experiments on a set of binary classification tasks on datasets from various domains, as described in the previous section.</p>
<p>To generate the secondary questions, we employed a large language model.Prompting it only once, we obtained a set of n secondary questions, which we accepted as provided, without any selection or modification.More specifically, we used the following prompt for creating all secondary questions:</p>
<p>Return {n} yes/no questions that would be useful to ask if you were trying to determine the answer to the following question: "{primary_question}"</p>
<p>where n is the number of additional questions we want to generate and primary_question is the primary question used to obtain the main information from the document.Note that in all our experiments n = 4.That means that for each document we use one primary and four secondary questions that are treated equally when prompting the LLM.Thus, for each document we collect five answers from the LLM that are then verbalized (assigned a numerical value) in the next step.To generate the secondary questions for all our experiments we use OpenAI's gpt-4-0125-preview model.To collect the answers in our experiments we use two generations of OpenAI's models: gpt-4-0125-preview (Achiam et al., 2023) and gpt-3.5-turbo-0125(Brown et al., 2020).</p>
<p>To choose the best classifier, we train several different classification algorithms.These include K-Nearest Neighbors, Decision Trees, Random Forest, Gaussian Naive Bayes, Multinomial Naive Bayes, AdaBoost, and XGBoost.We use a 5-fold cross-validation on our training data and also perform a grid search to fine-tune the parameters for each classifier.After training, we test them on a hold-out test set and choose the classifier that gives us the highest Micro F1 score (µF 1).Note that one can also adjust the training process to optimize for a specific performance metric if needed for a particular application.To perform these experiments, we used the scikit-learn library in Python.</p>
<p>Micro F1 score is particularly useful in datasets where some classes are significantly underrepresented, and where traditional metrics might give a misleading picture of model performance.It treats every instance as equally important, thereby giving a more accurate measure of the model's performance across the board.To calculate the µF 1, we use the following formula:
µF 1 = 2µP recision × µRecall µP recision + µRecall(1)
where µP recision = T P i T P i + F P i</p>
<p>(2)
µRecall = T P i T P i + F N i (3)
and T P , F P and F P represent number of true positives, false positives and false negatives respectively.</p>
<p>Additionally, we conducted a sensitivity analysis to enhance our understanding of the relationship between the number of features and the improvement of the µF 1.This analysis helps determine the requisite number of secondary questions to attain a desired µF 1.For each dataset, we started by creating n = 9 secondary questions and using the gpt-3.5-turbo-0125model to generate responses for each sample.The outputs from the large language model were then transformed into 10-dimensional feature vectors.Subsequently, we constructed a series of simple Random Forest classifiers, starting with a single feature and incrementally adding more features up to ten.Given the random selection of features for classification, we repeated the experiment 100 times.We computed the µF 1 for each iteration and dataset.The findings are detailed in Section 6 and illustrated in Figure 3.  GPT-4 0-shot ICE-T 0-shot ICE-</p>
<p>Results</p>
<p>The results of the classification experiments are summarized in Table 1.We can see that across all datasets, the ICE-T method consistently surpasses the zero-shot approach in performance for a given language models.Specifically, using the GPT-3.5 model, the average µF 1 for the zero-shot approach is 0.683, but it increases to 0.845 with the ICE-T method.A similar trend is observed with the larger GPT-4 model, where the average F1 score improves from 0.7 using the zero-shot approach to 0.892 with the ICE-T technique.This improvement is not constant across the datasets as we can see a significant variations in performance and in improvements across different tasks.</p>
<p>The upper portion of Table 1 showcases the findings from the clinical trial dataset, as detailed in Section 4. The dataset's contents remain consistent across all sub-tasks within this clinical trial dataset, though each sub-task involves a distinct classification criterion based on 12 different criteria.In some sub-tasks, substantial improvements were observed over the zero-shot method.For instance, in the task CREATININE (involving serum creatinine levels exceeding the upper normal limit), the zero-shot method achieved µF 1 of 0.349.In contrast, the ICE-T technique utilizing the same large language model significantly improved this score to 0.721.Similarly, for the task ENGLISH (determining if a patient speaks English) using the larger GPT4 model, the greatest increase noted exceeded 0.733 points, with the zero-shot approach at a µF 1 of 0.233 and the ICE-T technique improving it to 0.966.Analysis of tasks outside the clinical trial dataset revealed varied results, dependent on the specific domain.The task assessing "Catalonia independence" presented a notable challenge in the zero-shot setup for both models, barely achieving a µF 1 above 0.5, with no significant improvements noted with the ICE-T technique.</p>
<p>The task related to the European Court of Human Rights (ECtHR) already exhibited high baseline scores in the zero-shot setting, achieving 0.853 with GPT-3.5 and 0.861 with GPT-4.The application of the ICE-T technique yielded minimal improvement, with both models achieving a µF 1 of 0.873.A similar scenario was observed with the Health advice dataset, where enhancements were negligible.</p>
<p>However, the UNFAIR-ToS task demonstrated significant improvement using the ICE-T approach, particularly with the GPT-3.5 model.Here, the µF 1 score saw a dramatic increase from 0.335 to 0.887.</p>
<p>Furthermore, our analysis reveals that the ICE-T technique, when applied to a smaller model, can surpass or match the performance of a larger model that uses the zero-shot approach.In our experiments, we assessed the µF 1 of classification tasks executed by GPT-4 in a zero-shot setting against those performed by GPT-3.5 using the ICE-T technique across various datasets.In nearly all cases, except for two, the ICE-T-enhanced GPT-3.5 either outperformed or equaled the larger GPT-4 model on identical tasks.These findings are depicted in Figure 2.</p>
<p>We observed a minor variation in performance across different task groups.By categorizing clinical trial tasks into one group and other tasks into another, we observed a comparable average performance improvement when comparing the zero-shot to the ICE-T approach, as detailed in Table 4 in Appendix B. This consistency underscores the versatility of the ICE-T method across various domains and tasks.</p>
<p>To explore how the number of features impacts the micro F1 score (µF 1), we conducted an additional sensitivity analysis.The outcomes of this analysis are depicted in Figure 3.This figure illustrates the change of the µF 1 as we incrementally introduce more features (obtained by secondary questions).A solid orange line shows the average µF 1 across all datasets, while the surrounding shaded area indicates one standard deviation from the mean, based on 100 iterations.As anticipated, there is a consistent increase in the micro F1 score with the addition of more secondary questions.On average, adding three secondary questions increases the µF 1 score from 0.76 to 0.80, with further additions raising it to 0.82.
E N G LI S H D R U G -A B U S E A LC O H O L-A B U S E M A K E S -D E C IS IO N S M I-6 M O S U N FA IR -T o S E C tH R A S P -F O R -M I H e a lt h a d v ic e H B A 1 C M A JO R -D IA B E T E S A B D O M IN A L C lim
It is important to highlight that this figure averages results from 17 different datasets, using only the Random Forest classifier.Detailed results for each individual task are available in Figure 4 in Appendix B. The use of a single classifier in this analysis was a deliberate choice to isolate the impact of increasing the number of features, thereby minimizing the influence of classifier selection on the results.However, this choice may also limit the generalizability of the findings, as it differs from previous analyses where the optimal classifier was selected for each task.</p>
<p>Discussion</p>
<p>Our study introduces the Interpretable Cross-Examination Technique (ICE-T), a novel prompting method that integrates LLM responses with traditional classification algorithms to improve the performance on binary classification tasks.This technique addresses key limitations in zero-shot and few-shot learning by employing a structured, multi-prompt approach that transforms qualitative data into quantifiable metrics, thus allowing a small, traditional classifier to effectively make decisions.Our results confirm that ICE-T consistently sur- passes zero-shot baselines across multiple datasets and metrics, particularly in scenarios where model interpretability is crucial.This prompting strategy also demonstrates the potential for fully automated, high-performing AI systems accessible even to nonexperts.</p>
<p>The ICE-T method has demonstrated its capability to not only enhance performance over the zero-shot approach but also to do so with smaller models that might not perform as well in a zeroshot configuration.For example, the improvement in the CREATININE and ENGLISH tasks within clinical trials data underscores the method's ability to handle domain-specific challenges that require nuanced understanding, which zero-shot configurations typically struggle with.</p>
<p>Implications for Model Interpretability</p>
<p>A major advantage of the ICE-T approach is its interpretability.By generating a feature vector based on direct responses to structured prompts, experts can trace back the decision-making process, understanding which factors contributed most significantly to the model's classification.This is particularly valuable in fields like medicine and law, where decision rationale is as important as accuracy.The ability to dissect and validate each step of the model's reasoning aligns with the growing demand for transparency in AI applications, ensuring that decisions made by AI systems can be audited and trusted.</p>
<p>Moreover, ICE-T is particularly valuable in situations where fine-tuning models is not viable.Finetuned models often suffer from a significant drawback: they lack transparency and become "black boxes," making their decision-making processes obscure.This lack of interpretability is particularly problematic in regulated sectors such as healthcare, law and finance, where it's imperative to comprehend the basis of each decision.ICE-T overcomes these issues by employing a methodology that remains clear and interpretable, avoiding the opaqueness associated with fine-tuned systems.</p>
<p>Limitations and Future Work</p>
<p>Despite its strengths, the ICE-T method has some limitations.The quality of the output heavily relies on the initial set of questions generated for the model to answer.Poorly formulated questions or those that fail to capture the necessary subtleties of the task can limit the effectiveness of this technique.Moreover, the reliance on numerical scoring of textual answers might oversimplify complex answers.This can lead to a loss of nuance, especially when answers are confined to binary outputs.</p>
<p>Future research could explore more sophisticated methods for question generation, perhaps incorporating active learning where the system identifies and prioritizes questions that would most improve its understanding and performance.Additionally, exploring different methods of encoding responses into feature vectors could further enhance the model's accuracy and sensitivity to nuances in text.</p>
<p>Expanding the scope of ICE-T to tackle problems beyond binary classification could also prove beneficial.Applying this method to multi-class classification tasks or even regression problems could test the adaptability and scalability of the approach, potentially making it more applicable across a wider array of domains.This expansion could lead to significant advancements in the field of machine learning where interpretability and accuracy are crucial.</p>
<p>In conclusion, the ICE-T method presents a promising avenue for enhancing the performance and interpretability of LLMs in binary classification tasks and beyond.By bridging the gap between traditional machine learning techniques and modern LLM capabilities, this approach offers a valuable tool for applications demanding high accuracy and clear reasoning in decision-making processes.</p>
<p>Further refinements and adaptations of this technique could significantly impact the deployment of AI in critical sectors, enhancing both the reliability and accountability of automated systems.</p>
<p>Reproducibility</p>
<p>The experiment is composed of two primary phases: 1) collecting outputs from OpenAI's ChatGPT models, specifically using either gpt-4-0125-preview or gpt-3.5-turbo-0125;and 2) verbalizing the answers (converting the responses into numerical form), training classifiers and evaluating their performance on a hold-out test set.</p>
<p>The code to reproduce the verbalization, classfier training and testing is available on GitHub: https://github.com/gmuric/ICE-TDue to data usage and confidentiality constraints associated with the clinical trial dataset, we are unable to share the complete working code for the first phase.However, we provide the outputs of the LLMs we obtained.They are available in GitHub repository.We additionally provide pseudo-code that illustrates the extraction of outputs from the language models that can be used to reproduce the first part of the experiment.The complete references to the data used in the experiments are explained in Section 4. Additionally, we include a comprehensive list of the questions used to prompt the language models in Appendix A. The pseudocode for obtaining the answers from the LLM is presented below:</p>
<p>Algorithm 1 Collecting outputs from LLM for each textual document t do Prompt LLM with t and corresponding questions {Refer to Section 3.2} for each question q i related to t do a i ← Output of LLM for q i end for A ← Set of all outputs {a 1 , a 2 , . . ., a n } for each output a i in A do v i ← Numerical value of a i {Refer to Section 3.3} end for V d ← Feature vector of all v i end for Note that due to the stochastic nature of large language models, the outputs may vary with each experiment.While these variations are unlikely to significantly impact the results, minor discrepancies are possible.</p>
<p>A Questions used in ICE-T method</p>
<p>This is the list of questions used for each task in ICE-T technique.For each task, there are five questions in total, where the question with id = 0 is used in a prompt in a zero-shot approach.Other questions are secondary questions obtained as explained in Section 3.1:</p>
<p>DRUG-ABUSE</p>
<p>0 Is there any indication in the patient's records of current or past drug abuse?</p>
<p>1 Has the patient been prescribed medication with a high potential for abuse?</p>
<p>2 Are there any notes in the patient's records indicating substance abuse or dependency issues?</p>
<p>3 Has the patient previously sought treatment for substance abuse or addiction?</p>
<p>4 Are there any irregularities in the patient's prescription history that suggest misuse, such as early refill requests?</p>
<p>ALCOHOL-ABUSE</p>
<p>0 Has the patient consumed alcohol beyond the weekly recommended limits recently?</p>
<p>1 Did the patient consume more than 14 units of alcohol in the past week?</p>
<p>2 Has the patient engaged in binge drinking sessions in the last month?</p>
<p>3 Does the patient frequently consume alcohol on more than 5 days in a week?</p>
<p>4 Has the patient expressed concerns about controlling their alcohol intake recently?</p>
<p>ENGLISH 0 Does the patient speak English, according to their medical records?</p>
<p>1 Is there a language preference indicated in the patient's medical records?</p>
<p>2 Does the medical record include an interpreter request for non-English languages?</p>
<p>3 Is English listed as the patient's primary language in the medical records?</p>
<p>4 Have previous medical consultations been conducted in English, as noted in the patient's records?</p>
<p>MAKES-DECISIONS</p>
<p>0 Is there evidence that the patient makes their own medical decisions?</p>
<p>1 Does the patient have a documented history of discussing treatment options with their healthcare provider?</p>
<p>2 Has the patient previously filled out an advanced directive or a living will?</p>
<p>3 Does the patient regularly attend medical appointments alone?</p>
<p>4 Have there been instances where the patient explicitly expressed their treatment preferences or declined certain medical interventions?</p>
<p>ABDOMINAL 0 Is there a record of the patient undergoing intraabdominal surgery, small or large intestine resection, or experiencing a small bowel obstruction?</p>
<p>1 Has the patient ever undergone any form of intraabdominal surgery?</p>
<p>2 Has the patient had a resection of either the small or large intestine?</p>
<p>3 Is there a record of the patient experiencing a small bowel obstruction?</p>
<p>4 Has the patient had any surgical intervention related to the digestive system that is not explicitly mentioned above?</p>
<p>MAJOR-DIABETES</p>
<p>0 Does the patient have any major diabetes-related complications such as amputation, kidney damage, skin conditions, retinopathy, nephropathy, or neuropathy?</p>
<p>1 Has the patient experienced any significant changes in their vision or been diagnosed with retinopathy?</p>
<p>2 Is there any history of skin conditions, wounds that heal poorly, or any amputations?</p>
<p>3 Does the patient have a history of kidney damage or been diagnosed with nephropathy?</p>
<p>4 Has the patient reported any persistent numbness, pain, or tingling in their extremities indicating neuropathy?</p>
<p>ADVANCED-CAD</p>
<p>0 Is the patient currently taking two or more medications to treat CAD?</p>
<p>1 Is the patient currently being treated for coronary artery disease (CAD)?</p>
<p>2 Is the patient taking any medication to manage CAD symptoms?</p>
<p>3 Is the patient prescribed more than one medication specifically for CAD?</p>
<p>4 Are the medications the patient is taking to treat CAD being taken concurrently?</p>
<p>MI-6MOS</p>
<p>0 Has the patient had a myocardial infarction within the past 6 months from the most recent record date?</p>
<p>1 Has the patient reported any chest pain or symptoms consistent with a myocardial infarction in the past 6 months?</p>
<p>2 Has the patient undergone any cardiac diagnostic tests, such as an ECG or troponin levels, in the past 6 months?</p>
<p>3 Did the patient receive any treatment specifically for myocardial infarction, such as medication, stenting, or bypass surgery, in the past 6 months?</p>
<p>4 Is there any notation in the patient's medical records of a confirmed diagnosis of myocardial infarction within the past 6 months?</p>
<p>DIETSUPP-2MOS</p>
<p>ASP-FOR-MI</p>
<p>0 Is the patient using aspirin to prevent myocardial infarction based on their medical records?</p>
<p>1 Is there a history of cardiovascular disease in the patient's medical records?</p>
<p>2 Has the patient been prescribed aspirin for long-term use?</p>
<p>3 Do the medical records indicate a doctor's recommendation for aspirin to prevent myocardial infarction?</p>
<p>4 Is there any mention of aspirin under the patient's current medications list?UNFAIR-ToS 0 Does this sentence from company's Terms of Service violate the European Council Directive on unfair terms in consumer contracts?</p>
<p>1 Does the sentence create a significant imbalance between the parties' rights and obligations to the detriment of the consumer?</p>
<p>2 Is the term clearly understandable and transparent to a typical consumer?</p>
<p>3 Does the term determine the main subject matter of the contract, or the appropriateness of the price or remuneration, in a way that is unfair to the consumer?</p>
<p>4 Has the consumer been given an opportunity to negotiate the terms of the contract?</p>
<p>ECtHR 0 Considering all the facts, did a state breach human rights provisions of the European Convention of Human Rights?</p>
<p>1 Did the state fail to provide adequate protection to an individual or group, thereby violating Article 2 or 3 concerning the right to life and prohibition of torture?</p>
<p>2 Was there any discrimination in the state's actions or laws that contravenes Article 14 or Protocol 12 regarding the prohibition of discrimination?</p>
<p>3 Did the state unlawfully interfere with privacy, family life, freedom of expression, or freedom of assembly and association, as protected under Articles 8 to 11?</p>
<p>4 Was there a failure by the state to ensure a fair trial or access to a court, in violation of Article 6 or 13, concerning the right to a fair trial and the right to an effective remedy?</p>
<p>Climate detection</p>
<p>0 Is a given paragraph climate-related?</p>
<p>1 Does the paragraph discuss weather, temperature trends, or climate patterns?</p>
<p>2 Does the paragraph mention the impact of human activity on the environment?</p>
<p>3 Does the paragraph reference scientific studies or data on climate change?</p>
<p>4 Does the paragraph address policy or actions taken to mitigate climate impacts?</p>
<p>Health advice 0 Does the given sentence contain a medical advice?</p>
<p>1 Does the sentence recommend or suggest a specific medication, treatment, or remedy?</p>
<p>2 Does the sentence advise on health behaviors, such as diet, exercise, or sleep patterns?</p>
<p>3 Does the sentence include any wording that implies or directly states a medical diagnosis or prognosis?</p>
<p>4 Does the sentence suggest consulting a healthcare professional or seeking medical attention?</p>
<p>Catalonia indep.</p>
<p>0 Does the given sentence speak in favor of Catalonia independence?</p>
<p>1 Does the sentence express a positive opinion about Catalonia's political autonomy?</p>
<p>2 Does the sentence criticize the Spanish government's policies towards Catalonia?</p>
<p>3 Does the sentence highlight benefits or advantages of Catalonia being independent?</p>
<p>4 Does the sentence encourage actions or steps towards achieving independence for Catalonia?</p>
<p>B Additional results</p>
<p>This is the appendix section where additional results are provided.</p>
<p>2.</p>
<p>Prompting the LLM: Previously generated questions are used to prompt the LLM and collect the yes/no answers; 3. Verbalizing the answers: for each instance within the training dataset, responses to prompts are collected and converted into numerical form, thus creating a low-dimensional feature vector for each instance; 4. Training a classifier: Previously obtained vectors, together with their respective labels, are then used to train a classifier The Inference stage mirrors the training process: the LLM is presented with the same collection of questions.The responses obtained are numerically encoded in the same manner before being processed by the classifier that was trained during the Training stage.Training and inference process is illustrated in Figure 1.Each step is explained below.</p>
<p>Figure 1 :
1
Figure1: Illustration of training and inference process in ICE-T.In the training phase, the process begins by generating questions to prompt an LLM, which then provides yes/no answers.These answers are verbalized and converted into numerical feature vectors.A classifier is trained using these vectors along with their respective labels.During inference, the LLM is prompted with the same questions, and the answers are similarly processed to predict outcomes using the trained classifier.</p>
<p>Figure 2 :
2
Figure 2: Comparative performance of ICE-Tenhanced GPT-3.5 versus zero-shot GPT-4.The figure illustrates the µF 1 achieved by GPT-3.5 utilizing the ICE-T technique and GPT-4 in a zero-shot setting across multiple datasets.</p>
<p>Figure 3 :
3
Figure3: Sensitivity Analysis of Feature Count on µF 1 Score.The figure illustrates the effect of increasing the number of features (secondary questions) on the µF 1 score across 17 datasets.The solid orange line represents the average µF 1 score, and the shaded area indicates the first standard deviation from the mean across 100 repetitions.The graph demonstrates a consistent improvement in µF 1 as more features are added, with key points of increase highlighted at specific feature counts.</p>
<p>HBA1C0132</p>
<p>Have any HbA1c test results been listed in the records with a value between 6.5 and 9.5 Are there any HbA1c test results listed in the records? 2 Do any HbA1c test results fall within the 6.5 to 9.5 Is the HbA1c value specifically mentioned for each test result?4Have all the HbA1c test results been recorded and updated in the records?CREATININE 0 Is there any indication in the patient's records of serum creatinine levels above the upper limit of normal?1 Has the patient undergone any recent serum creatinine tests?Are the results of the patient's serum creatinine tests documented in their medical records?3 Do the documented serum creatinine levels exceed the established normal range?4Has there been any physician commentary or notes indicating concern over the patient's serum creatinine levels?</p>
<p>Figure 4 :
4
Figure 4: Task-Specific Sensitivity Analysis of Feature Count on µF 1 Score.Detailed view of the changes in the µF 1 score for individual tasks as the number of secondary questions increases.Each plot represents one of the 17 datasets analyzed, showing how the micro F1 score varies with the addition of features.The data underscores the variability in performance improvements across different tasks when using the Random Forest classifier.</p>
<p>Table 1 :
1
Comparison of µF 1 scores between zeroshot setting and ICE-T method.The values in bold represent the µF 1 score of the winning approach for a specific task and a language model.Horizontal line in the middle splits the clinical trial datasets and other datasets.All tasks solve the binary classification problem.
T
A different approach to achieving few-shot learning can occur also during the training phase or during fine-tuning.
https://n2c2.dbmi.hms.harvard.edu/
Our methodology employs classifiers that are trained based on data distributions. As a result, we consistently achieve peak classification metrics, which is not a realistic performance, as the minority class is absent from the test dataset.
AcknowledgmentThis material is based upon work supported by the Army ASA(ALT) SBIR CCOE under Contract No. W51701-22-C-0035 and the US Air Force under Contract No. FA8750-22-C-0511.Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Army ASA(ALT) SBIR CCOE or the US Air Force.Table2: Evaluation of ICE-T and Zero-Shot Techniques on GPT-3.5 -This table presents a comparison of F1 micro, macro, and weighted scores for various classification tasks using GPT-3.5.It compares the performance metrics between the zero-shot approach and the ICE-T technique.Additionally, the table includes sample sizes for both training and testing datasets across each task.Notable improvements can be seen in several tasks when utilizing the ICE-T technique over the zero-shot method.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, Dan Mané, arXiv:1606.06565Concrete problems in ai safety. 2016arXiv preprint</p>
<p>Eliciting latent predictions from transformers with the tuned lens. Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev Mckinney, Stella Biderman, Jacob Steinhardt, arXiv:2303.081122023arXiv preprint</p>
<p>Is attention explanation? an introduction to the debate. Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas François, Patrick Watrin, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Neural legal judgment prediction in English. 10.18653/v1/P19-1424Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational Linguistics2019Ion Androutsopoulos, and Nikolaos Aletras</p>
<p>What does bert look at? an analysis of bert's attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, arXiv:1906.043412019arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>What you can cram into a single vector: Probing sentence embeddings for linguistic properties. Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni, arXiv:1805.010702018arXiv preprint</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey on in-context learning. 2022arXiv preprint</p>
<p>European union regulations on algorithmic decision-making and a "right to explanation. Bryce Goodman, Seth Flaxman ; Yuxian, Xu Gu, Zhiyuan Han, Minlie Liu, Huang, arXiv:2109.04332AI magazine. 3832017. 2021arXiv preprintPpt: Pre-trained prompt tuning for few-shot learning</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Claudette: an automated detector of potentially unfair clauses in online terms of service. Marco Lippi, Przemysław Pałka, Giuseppe Contissa, Francesca Lagioia, Hans-Wolfgang Micklitz, Giovanni Sartor, Paolo Torroni, Artificial Intelligence and Law. 272019</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862021arXiv preprint</p>
<p>A unified approach to interpreting model predictions. M Scott, Su-In Lundberg, Lee, 201730Advances in neural information processing systems</p>
<p>Volodymyr John X Morris, Vitaly Kuleshov, Alexander M Shmatikov, Rush, arXiv:2310.06816Text embeddings reveal (almost) as much as text. 2023arXiv preprint</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>True few-shot learning with language models. Advances in neural information processing systems. Ethan Perez, Douwe Kiela, Kyunghyun Cho, 202134</p>
<p>why should i trust you?" explaining the predictions of any classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on knowledge discovery and data mining2016</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, arXiv:2112.086332021arXiv preprint</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, arXiv:2009.071182020arXiv preprint</p>
<p>True fewshot learning with prompts-a real-world perspective. Timo Schick, Hinrich Schütze, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, Jianfeng Gao, arXiv:2402.01761Rethinking interpretability in the era of large language models. 2024arXiv preprint</p>
<p>Cohort selection for clinical trials: n2c2 2018 shared task track 1. Amber Stubbs, Michele Filannino, Ergin Soysal, Samuel Henry, Özlem Uzuner, 10.1093/JAMIA/OCZ163Journal of the American Medical Informatics Association : JAMIA. 2611632019</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningPMLR201770</p>
<p>Large language model prompt chaining for long legal document classification. Dietrich Trautmann, arXiv:2308.041382023arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022aarXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022barXiv preprint</p>
<p>Nicolas Webersinke, Mathias Kraus, arXiv:2110.12010Julia Anna Bingler, and Markus Leippold. 2021. Climatebert: A pretrained language model for climate-related text. arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022aarXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc2022b35</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 2022c35</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI conference on human factors in computing systems. the 2022 CHI conference on human factors in computing systemsJun Cai. 2022a</p>
<p>Self-adaptive in-context learning: An information compression perspective for in-context example selection and ordering. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, Lingpeng Kong, arXiv:2212.103752022barXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot prompting for textual reasoning. Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan, Advances in Neural Information Processing Systems. 202436Gpt4tools: Teaching large language model to use tools via self-instruction</p>
<p>Detecting causal language use in science findings. Bei Yu, Yingya Li, Jun Wang, 10.18653/v1/D19-1473Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Active example selection for in-context learning. Yiming Zhang, Shi Feng, Chenhao Tan, arXiv:2211.044862022aarXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022barXiv preprint</p>
<p>Relying on the unreliable: The impact of language models' reluctance to express uncertainty. Kaitlyn Zhou, Jena D Hwang, Xiang Ren, Maarten Sap, arXiv:2401.067302024arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Multilingual stance detection in tweets: The Catalonia independence corpus. Elena Zotova, Rodrigo Agerri, Manuel Nuñez, German Rigau, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation ConferenceMarseille, FranceEuropean Language Resources Association2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2446 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2446</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2446</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-66.html">extraction-schema-66</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <p><strong>Paper ID:</strong> paper-272423975</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.03402v1.pdf" target="_blank">Game On: Towards Language Models as RL Experimenters</a></p>
                <p><strong>Paper Abstract:</strong> We propose an agent architecture that automates parts of the common reinforcement learning experiment workflow, to enable automated mastery of control domains for embodied agents. To do so, it leverages a VLM to perform some of the capabilities normally required of a human experimenter, including the monitoring and analysis of experiment progress, the proposition of new tasks based on past successes and failures of the agent, decomposing tasks into a sequence of subtasks (skills), and retrieval of the skill to execute - enabling our system to build automated curricula for learning. We believe this is one of the first proposals for a system that leverages a VLM throughout the full experiment cycle of reinforcement learning. We provide a first prototype of this system, and examine the feasibility of current models and techniques for the desired level of automation. For this, we use a standard Gemini model, without additional fine-tuning, to provide a curriculum of skills to a language-conditioned Actor-Critic algorithm, in order to steer data collection so as to aid learning new skills. Data collected in this way is shown to be useful for learning and iteratively improving control policies in a robotics domain. Additional examination of the ability of the system to build a growing library of skills, and to judge the progress of the training of those skills, also shows promising results, suggesting that the proposed architecture provides a potential recipe for fully automated mastery of tasks and domains for embodied agents.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2446.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2446.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GameOn VLM experimenter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLM-based RL Experimenter (prototype from 'Game On: Towards Language Models as RL Experimenters')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prototype agent architecture that uses a vision-language model (Gemini 1.5 Pro) to automate high-level RL experimenter tasks (task proposition, decomposition, skill retrieval, and analysis) while delegating low-level motor control to a language-conditioned actor-critic policy (Perceiver-Actor-Critic, PAC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>VLM-based RL Experimenter (Game On prototype)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular system in which a single multimodal VLM (Gemini 1.5 Pro) implements the curriculum module (task proposal, task decomposition, and retrieval mapping of free-text subtasks to a fixed skill library) and an analysis module (judging convergence from learning-curve images) via few-shot prompting; the embodiment module executes retrieved skill sequences with a text-conditioned Perceiver-Actor-Critic (PAC) low-level policy, collects episodes, relabels them for multiple reward functions, and performs offline RL fine-tuning. The system runs in a loop: the VLM proposes tasks given current images and past successes/failures, decomposes them into free-text steps, maps steps to known skill captions (or signals missing skills), orchestrates parallel rollout collection (10 simulator instances), and the collected data is used to fine-tune the PAC policy; an analysis prompt judges curve convergence to decide when to add skills to the library (post-hoc in this prototype).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Automated Experimentation Platform (prototype)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotics / Reinforcement Learning (simulated robotic manipulation: multi-object arrangement and stacking tasks in MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automatic mastery of an embodied manipulation domain: propose curricula of tasks (e.g., stack, build tower/pyramid/line), decompose high-level goals into sequences of low-level motor skills, execute sequences with a learned text-conditioned policy, collect diverse data, and iteratively fine-tune to learn more complex behaviors (including multi-object compositions like 3-object tower/pyramid).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Moderate combinatorial manipulation complexity: three objects with permutations of target configurations (towers, pyramids, lines) yielding combinatorial task space; continuous control with 7-DoF robot arm, multi-step sequential composition of skills increases state-distribution shift and nonlinearity. Quantities reported: pretraining dataset ~1M episodes; PAC model trained 1.5M learner steps; curriculum collection produced ~25k episodes (first round) and additional ~15k episodes after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Pre-existing large dataset was available for base skills (~1M episodes collected from single-task RL experiment) which was relabeled for base rewards; the VLM-guided collection generated new on-policy data (~25k episodes then additional ~15k) used for self-improvement. Data quality was sufficient to allow offline RL fine-tuning; reward functions for new arbitrary VLM proposals were not automated (manual addition required).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Experiment used a large multimodal VLM (Gemini 1.5 Pro) queried during data collection (parallelized across 10 simulator instances) and a 140M-parameter PAC policy trained for 1.5M steps; collected 25k+ episodes in curriculum runs and 15k in a subsequent round. Exact wall-clock compute/time or cost not reported; uses parallel simulation and repeated VLM queries (temperature settings 0.0 and 0.3 examined).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Semi-structured, quasi-static embodied control tasks: continuous, stochastic simulator dynamics (MuJoCo), clear per-skill reward functions available for the evaluated subset, discrete skill library of known skills (text captions) with deterministic mapping to reward functions; open-ended at the high-level (free-text proposals) but constrained at execution by available skills and the lack of automatic reward generation for arbitrary new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task success is measured by predefined reward thresholds per skill (success defined as reward > 0.5 per executed skill and > 0.95 for final skill in a sequence), improvement in downstream policy performance after fine-tuning (training curves, average reward across tasks), and data-diversity metrics (L2/cluster distances in vision and proprioception embedding spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Quantitative exact success rates for individual tasks are provided in training curves (Figure 3) but not listed as a single percentage; empirical outcomes: VLM-guided data (25k episodes) produced more diverse data (vision L2: pretraining 0.571 vs self-improvement 0.625; proprioception L2: 0.815 vs 0.901) and when combined with pretraining data enabled learning of pyramid tasks that were previously not learnable; tower-building remained unsuccessfully learned in the policy due to low sequencing success. Exact per-task success percentages not enumerated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Key limitations observed: (1) No automatic reward-model for arbitrary free-text proposals — decompositions that include unknown skills are discarded in the prototype (manual addition of new skills required); (2) Open-loop execution of fixed-duration skills leads to sequencing errors (policy may perform skills in isolation but fail when sequenced because terminal states of one skill are not represented as initial states of the next); (3) VLM analysis (convergence judgments) can be unstable and produce false judgments (plateaus and irregular curves); (4) inability to learn tower-building via sequencing in current data regime; (5) parsing/response-format fragility in prompting pipeline can cause plan rejections.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Factors enabling success: availability of a large multimodal VLM capable of zero-shot task proposal/decomposition/retrieval and analysis; a large pretraining dataset (~1M episodes) to bootstrap base skills; a language-conditioned low-level policy (PAC) capable of offline RL and relabeling; parallelized simulation for data collection; the ability to relabel episodes with multiple reward functions to reuse data for many skills.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared datasets: pretraining vs combined self-improvement sets showed higher diversity (vision L2: 0.571 -> 0.625; proprioception L2: 0.815 -> 0.901). Fine-tuning on combined data (pretraining + VLM-collected) outperformed fine-tuning on pretraining-only across average tasks and on individual task families (Figure 3); adding further data after introducing pyramid skills improved performance further. Tower-building remained a failure case despite VLM frequently proposing it.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>No explicit human experimenter baseline or quantitative human-vs-system comparison is provided for the full automated loop; human designers did manually add new skills and configure reward functions in this prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2446.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager (LLM-driven open-ended agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven open-ended agent (GPT-4 in referenced work) that proposes tasks and writes code to achieve goals in the Minecraft domain, maintaining a skill library of LLM-generated and self-verified executable code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-based agent that autonomously proposes tasks, writes code (e.g., JavaScript) to interface with the environment (Minecraft), and maintains a growing skill library of executable code snippets that are LLM-generated and self-verified; uses an auto-curriculum proposed by the LLM to enable open-ended exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AI Scientist / Open-ended Embodied Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Embodied agents in simulated game environments (Minecraft); open-ended behavioral discovery and task proposal/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Open-ended exploration and skill acquisition in Minecraft by proposing tasks and generating code to accomplish them; skill library consists of code that can be executed via environment APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended, large discrete and continuous state-action space of Minecraft; requires code generation, environment interaction, and skill verification; high combinatorial and long-horizon complexity. Paper provides no numeric complexity measures here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on environment interactions; code-generated skills are executed and self-verified via the environment — details and quantities are in Voyager's own work (not reported in detail in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Depends on large LLM inference (GPT-4 in cited Voyager) and environment execution; exact compute not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended and largely unstructured at the top level (free-form task proposals), but execution uses discrete executable code interfaces; clear success detectors via environment state.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion and the growth/utility of the skill library (in the referenced Voyager work).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; referenced as prior successful example of LLM-proposed auto-curriculum in Minecraft.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not detailed here; mentioned differences with Game On: Voyager's skills are code-level and executable, whereas Game On stores learned parameterized motor policies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability of LLM to propose tasks and generate executable code tied to an environment API; availability of environment for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Compared conceptually: Voyager inspires curriculum proposal and skill library ideas; Game On differs by storing learnable low-level motor policies rather than code.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2446.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoRT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoRT (LLM orchestration of robot fleets)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based orchestrator that proposes tasks on-the-fly to a fleet of robots to collect diverse data; focuses on large-scale data collection rather than in-the-loop policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoRT: Embodied foundation models for large scale orchestration of robotic agents.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoRT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses LLMs to propose skill-level tasks for a fleet of robots to collect diverse datasets; orchestrates parallel execution for data collection but does not perform active policy learning or task decomposition/sequencing in the same way as Game On.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Data-Collection / Orchestration System</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotics data collection and orchestration across multiple robots/environments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Large-scale orchestration to collect diverse robot interaction datasets by proposing tasks for robots to execute; aims to increase data diversity rather than to train policies in the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Focus on breadth and scale of data collection across many robots; complexity arises from coordinating many agents and ensuring task diversity; exact quantitative metrics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Designed to generate large datasets via many robots; specifics are in the referenced AutoRT work and not enumerated here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Requires LLM inference for many task proposals and orchestration logic; significant parallel compute for robot execution at scale (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-structured for data collection (task proposals are skill-level), less emphasis on open-ended decomposition or policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Diversity of collected data (measured by authors of AutoRT); Game On notes AutoRT reports data diversity but not downstream policy training results.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; Game On notes AutoRT measures diversity but does not validate dataset utility via policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>AutoRT does not perform in-loop active learning of policies and lacks sequencing/decomposition components; downstream utility of collected data for policy improvement not demonstrated in AutoRT as discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Scale of parallel robots and LLM-driven proposal generation that focuses on skill-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>AutoRT is contrasted with Game On: AutoRT focuses only on data collection and does not validate training policies with the collected data, while Game On uses collected data for self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2446.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan (LLM-guided decomposition with affordance weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses language models to decompose human-provided high-level tasks into sub-tasks, constraining decomposed steps by available skills and scoring them by affordances in the current scene.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SayCan decomposes user-provided high-level instructions into sub-tasks using language models, and ranks sub-tasks by an affordance model that estimates the likelihood that the robot can execute a skill in the current scene; decompositions are constrained by the available skill set.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Task Decomposition / Affordance-Grounded Planning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotics instruction following and task decomposition for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Decompose human-specified high-level goals into sequences of known skills, weighing choices by skill affordances under current scene observations; designed for instruction execution rather than autonomous task proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Structured decomposition problem with affordance estimation; complexity comes from mapping linguistic goals to executable skills conditioned on scene context; quantitative metrics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Requires available skill set and affordance models trained on perception-to-affordance data; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Uses LLM inference plus affordance scoring models; specific compute not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined decomposition with discrete skill catalog and explicit affordance evaluation; depends on human-provided goal inputs (not open-ended proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task completion rates when decompositions are executed and affordance-based ordering is used (as reported in SayCan's own work, not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; the paper notes SayCan requires human-provided instructions (limiting open-ended use).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Depends on accurate affordance estimation and availability of relevant skills; constrained to human-given tasks, not autonomous task generation.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Integration of affordance scoring with LLM decomposition and a catalog of verified low-level skills.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Game On is complementary: it does not restrict the LLM's proposals to human inputs and also performs task proposition, while SayCan focuses on decomposition of human-provided tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2446.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CaP / PromptBook</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CaP (Code-as-Policies) and PromptBook</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lines of work where LLMs write policy code using perception and control APIs to directly produce robot controllers; PromptBook refines prompting for low-level manipulation code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CaP / PromptBook</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>CaP leverages LLMs to generate code that acts as policies by calling perception and control APIs; PromptBook provides systematic prompting strategies to elicit robust low-level manipulation code from LLMs. These methods emphasize code generation for control rather than learning parameterized motor policies.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Code-as-Policies / Automated Code Generation for Control</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Robotic manipulation and control (robot policy generation via code).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Generate executable control code that, when interfaced with perception and control APIs, drives a robot to perform manipulation tasks; aims to replace hand-coded controllers or learned policies with LLM-generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Requires mapping high-level intents to low-level API calls with correct sequencing and safety; long-horizon execution and real-world constraints add complexity. Quantitative metrics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Does not primarily rely on human-labeled RL datasets; instead it relies on environment APIs and LLM capabilities to generate code, with verification via execution.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference for code generation and environment execution for verification; compute specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Structured: code interfaces are explicit; success can be detected via environment state. However, robustness and safety are concerns in real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Task execution success and reliability of generated controllers (as reported in the cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Generated code may be brittle, may not handle corner cases, and often requires environment-specific adaptation and testing.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong LLM code generation capabilities and well-specified APIs and simulation environments for verification.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Game On differs by using a learned parameterized low-level policy (PAC) conditioned on language, rather than relying on LLM-generated code as the policy itself.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2446.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWE-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWE-Agent (LLM-based end-to-end software engineering agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent-computer interface designed to perform end-to-end software engineering tasks including navigating repositories, editing code, executing programs, and automating development workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SWE-agent: Agent-computer interfaces enable automated software engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SWE-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-based software engineering agent that can interact with a development environment, edit and execute code, run tests, and iteratively modify programs to accomplish software engineering tasks with minimal human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Software Engineering / AI Programmer</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Software engineering, code generation, and development automation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate end-to-end software engineering workflows: generate, modify, execute, and test code to implement features or fix bugs, leveraging LLMs and environment interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Complex multi-step programming tasks involving syntactic and semantic correctness, testing, and environment-specific behavior; evaluation often requires program execution and test suites.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on code corpora and interaction with repositories and runtime environments; details not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference, code execution environments, and potentially CI/testing infrastructure; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Well-defined in terms of program specifications and tests, but open-ended in design choices and implementation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Ability to implement correct code changes and pass tests; metrics and success rates are in SWE-Agent's own work, not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May produce incorrect or insecure code, require human verification, and depend on the quality of environment hooks and tests.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Tight integration with execution and repository interfaces, and strong LLM coding abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as an example of LLM-based agents performing end-to-end engineering tasks, analogous in spirit to research automation but in software rather than embodied RL.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2446.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureka (LLM reward design)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureka: Human-level reward design via coding large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent approach that codes LLMs to produce reliable reward functions or reward detectors, proposed as a candidate to enable automatic addition of new skills from free-text proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Eureka: Human-level reward design via coding large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureka (LLM-based reward design)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Eureka leverages LLMs to generate reward-code and reward-detection logic to judge success of high-level tasks, with the goal of producing zero- or few-shot reward functions that can be used to label episodes and enable automatic skill acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Automated Reward Design / Success Detection</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Reinforcement learning and robotic skill synthesis where defining rewards for arbitrary free-text tasks is required.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Produce robust reward functions or success detectors from language descriptions so that an automated experimenter can add new VLM-proposed tasks as trainable skills without manual reward engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Challenging due to reward hacking/exploitation, the need for grounded perception checks, and ensuring reliability across environments; quantitative measures not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on environment observations (images, states) to evaluate generated reward code; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference to generate reward logic and environment evaluation runs for validation; specifics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Open-ended (free-text to reward mapping), requires robust perception and semantics grounding to avoid false positives/negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Reliability of reward detection and resistance to reward exploitation (as discussed in Eureka's own work).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; Game On cites Eureka as a promising candidate to enable automated reward generation but does not integrate it.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Risk of false reward detections and agent exploitation of spurious reward signals; currently not robust enough for full RL integration per Game On authors.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong LLM reasoning and careful grounding to environment observations; validation techniques to avoid reward exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Game On treats LLM-based reward generation as future work and did not use it in the prototype due to robustness concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2446.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoML-GPT / MLcopilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoML-GPT / MLcopilot (LLM-assisted AutoML agents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems that use large language models to assist or perform automated machine learning tasks such as hyperparameter tuning, model selection, or automated experimentation by interacting with code and data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automl-gpt: Automatic machine learning with gpt.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoML-GPT / MLcopilot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-based agents that assist in automating ML experimentation workflows: propose hyperparameter settings, generate code to run experiments, analyze results, and iterate. Some systems include interfaces to execute code and read files to close the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>AutoML / Automated Research Agent</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Machine learning experiment automation (hyperparameter tuning, model selection, training/evaluation pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate steps of ML experimentation such as configuring runs, tuning hyperparameters, implementing training code, and analyzing outcomes to optimize model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Search over continuous and discrete hyperparameter spaces, non-convex optimization, expensive evaluations; complexity varies with model and dataset size. Quantitative details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Relies on available datasets for the ML tasks; some systems can run experiments to generate new data/metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>Potentially high due to repeated model trainings; depends on the scale of experiments the agent runs (not quantified here).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Often well-defined optimization problems (validation metric to maximize), but expensive and noisy evaluations; amenable to automated search strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Model performance on validation/test metrics, computational budget efficiency, and comparison to human-tuned baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper; only cited as examples of LLMs assisting ML experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May be limited by the inability to run sufficiently many experiments, biased priors from prompts, and lack of domain-specific constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Ability to interface with execution environments, strong code generation and reasoning by LLMs, and well-defined evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Game On differs by focusing on embodied RL and VLM supervision for curriculum and analysis, rather than AutoML for standard ML tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2446.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2446.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated research systems, AI scientists, or automated idea generation/implementation systems, including details about what types of research problems they were applied to, the characteristics of those problems, and the success or failure rates of the automated system.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaperQA / Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaperQA and Scimon (LLM agents for scientific research assistance / inspiration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Examples of LLM-based systems proposed for assisting scientific research: PaperQA is a retrieval-augmented generative agent for literature review, while Scimon focuses on generating novel scientific inspirations optimized for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paperqa: Retrieval-augmented generative agent for scientific research.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PaperQA / Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>PaperQA uses retrieval-augmented LLM generation to help collect and summarize literature relevant to scientific questions; Scimon is a concept for systems that generate novel scientific ideas optimized for novelty and inspiration, using LLMs as creative engines.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>Hypothesis/Idea Generation and Literature-assistant Systems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_domain</strong></td>
                            <td>Scientific literature review and ideation across scientific domains; automated hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_description</strong></td>
                            <td>Automate reading and summarization of literature, generate novel research directions or hypotheses, and provide inspiration for new experiments or studies.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_complexity</strong></td>
                            <td>Open-ended creative reasoning over large literature corpora; evaluating novelty and scientific validity is challenging; specifics not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>data_availability</strong></td>
                            <td>Depends on accessible corpora of scientific papers and metadata; retrieval augmentations require indexed databases.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_requirements</strong></td>
                            <td>LLM inference plus retrieval infrastructure; specifics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_structure</strong></td>
                            <td>Highly open-ended, with weakly defined success metrics (novelty, plausibility, impact).</td>
                        </tr>
                        <tr>
                            <td><strong>success_metric</strong></td>
                            <td>Novelty of suggestions, usefulness to researchers, and ability to surface relevant literature; not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Risk of generating plausible-sounding but incorrect or untestable hypotheses, citation hallucinations, and lack of experimental follow-through.</td>
                        </tr>
                        <tr>
                            <td><strong>success_factors</strong></td>
                            <td>Strong retrieval components, human-in-the-loop verification, and grounding in experimental feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Mentioned as prior work exploring LLMs for automating aspects of scientific research; Game On focuses on automating experimental cycles in embodied RL rather than literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Game On: Towards Language Models as RL Experimenters', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An open-ended embodied agent with large language models. <em>(Rating: 2)</em></li>
                <li>AutoRT: Embodied foundation models for large scale orchestration of robotic agents. <em>(Rating: 2)</em></li>
                <li>SWE-agent: Agent-computer interfaces enable automated software engineering. <em>(Rating: 2)</em></li>
                <li>Eureka: Human-level reward design via coding large language models. <em>(Rating: 2)</em></li>
                <li>Automl-gpt: Automatic machine learning with gpt. <em>(Rating: 1)</em></li>
                <li>Paperqa: Retrieval-augmented generative agent for scientific research. <em>(Rating: 1)</em></li>
                <li>Gensim: Generating robotic simulation tasks via large language models. <em>(Rating: 1)</em></li>
                <li>Do as I can, not as I say: Grounding language in robotic affordances. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2446",
    "paper_id": "paper-272423975",
    "extraction_schema_id": "extraction-schema-66",
    "extracted_data": [
        {
            "name_short": "GameOn VLM experimenter",
            "name_full": "VLM-based RL Experimenter (prototype from 'Game On: Towards Language Models as RL Experimenters')",
            "brief_description": "A prototype agent architecture that uses a vision-language model (Gemini 1.5 Pro) to automate high-level RL experimenter tasks (task proposition, decomposition, skill retrieval, and analysis) while delegating low-level motor control to a language-conditioned actor-critic policy (Perceiver-Actor-Critic, PAC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "VLM-based RL Experimenter (Game On prototype)",
            "system_description": "A modular system in which a single multimodal VLM (Gemini 1.5 Pro) implements the curriculum module (task proposal, task decomposition, and retrieval mapping of free-text subtasks to a fixed skill library) and an analysis module (judging convergence from learning-curve images) via few-shot prompting; the embodiment module executes retrieved skill sequences with a text-conditioned Perceiver-Actor-Critic (PAC) low-level policy, collects episodes, relabels them for multiple reward functions, and performs offline RL fine-tuning. The system runs in a loop: the VLM proposes tasks given current images and past successes/failures, decomposes them into free-text steps, maps steps to known skill captions (or signals missing skills), orchestrates parallel rollout collection (10 simulator instances), and the collected data is used to fine-tune the PAC policy; an analysis prompt judges curve convergence to decide when to add skills to the library (post-hoc in this prototype).",
            "system_type": "AI Scientist / Automated Experimentation Platform (prototype)",
            "problem_domain": "Robotics / Reinforcement Learning (simulated robotic manipulation: multi-object arrangement and stacking tasks in MuJoCo)",
            "problem_description": "Automatic mastery of an embodied manipulation domain: propose curricula of tasks (e.g., stack, build tower/pyramid/line), decompose high-level goals into sequences of low-level motor skills, execute sequences with a learned text-conditioned policy, collect diverse data, and iteratively fine-tune to learn more complex behaviors (including multi-object compositions like 3-object tower/pyramid).",
            "problem_complexity": "Moderate combinatorial manipulation complexity: three objects with permutations of target configurations (towers, pyramids, lines) yielding combinatorial task space; continuous control with 7-DoF robot arm, multi-step sequential composition of skills increases state-distribution shift and nonlinearity. Quantities reported: pretraining dataset ~1M episodes; PAC model trained 1.5M learner steps; curriculum collection produced ~25k episodes (first round) and additional ~15k episodes after fine-tuning.",
            "data_availability": "Pre-existing large dataset was available for base skills (~1M episodes collected from single-task RL experiment) which was relabeled for base rewards; the VLM-guided collection generated new on-policy data (~25k episodes then additional ~15k) used for self-improvement. Data quality was sufficient to allow offline RL fine-tuning; reward functions for new arbitrary VLM proposals were not automated (manual addition required).",
            "computational_requirements": "Experiment used a large multimodal VLM (Gemini 1.5 Pro) queried during data collection (parallelized across 10 simulator instances) and a 140M-parameter PAC policy trained for 1.5M steps; collected 25k+ episodes in curriculum runs and 15k in a subsequent round. Exact wall-clock compute/time or cost not reported; uses parallel simulation and repeated VLM queries (temperature settings 0.0 and 0.3 examined).",
            "problem_structure": "Semi-structured, quasi-static embodied control tasks: continuous, stochastic simulator dynamics (MuJoCo), clear per-skill reward functions available for the evaluated subset, discrete skill library of known skills (text captions) with deterministic mapping to reward functions; open-ended at the high-level (free-text proposals) but constrained at execution by available skills and the lack of automatic reward generation for arbitrary new tasks.",
            "success_metric": "Task success is measured by predefined reward thresholds per skill (success defined as reward &gt; 0.5 per executed skill and &gt; 0.95 for final skill in a sequence), improvement in downstream policy performance after fine-tuning (training curves, average reward across tasks), and data-diversity metrics (L2/cluster distances in vision and proprioception embedding spaces).",
            "success_rate": "Quantitative exact success rates for individual tasks are provided in training curves (Figure 3) but not listed as a single percentage; empirical outcomes: VLM-guided data (25k episodes) produced more diverse data (vision L2: pretraining 0.571 vs self-improvement 0.625; proprioception L2: 0.815 vs 0.901) and when combined with pretraining data enabled learning of pyramid tasks that were previously not learnable; tower-building remained unsuccessfully learned in the policy due to low sequencing success. Exact per-task success percentages not enumerated in the text.",
            "failure_modes": "Key limitations observed: (1) No automatic reward-model for arbitrary free-text proposals — decompositions that include unknown skills are discarded in the prototype (manual addition of new skills required); (2) Open-loop execution of fixed-duration skills leads to sequencing errors (policy may perform skills in isolation but fail when sequenced because terminal states of one skill are not represented as initial states of the next); (3) VLM analysis (convergence judgments) can be unstable and produce false judgments (plateaus and irregular curves); (4) inability to learn tower-building via sequencing in current data regime; (5) parsing/response-format fragility in prompting pipeline can cause plan rejections.",
            "success_factors": "Factors enabling success: availability of a large multimodal VLM capable of zero-shot task proposal/decomposition/retrieval and analysis; a large pretraining dataset (~1M episodes) to bootstrap base skills; a language-conditioned low-level policy (PAC) capable of offline RL and relabeling; parallelized simulation for data collection; the ability to relabel episodes with multiple reward functions to reuse data for many skills.",
            "comparative_results": "Compared datasets: pretraining vs combined self-improvement sets showed higher diversity (vision L2: 0.571 -&gt; 0.625; proprioception L2: 0.815 -&gt; 0.901). Fine-tuning on combined data (pretraining + VLM-collected) outperformed fine-tuning on pretraining-only across average tasks and on individual task families (Figure 3); adding further data after introducing pyramid skills improved performance further. Tower-building remained a failure case despite VLM frequently proposing it.",
            "human_baseline": "No explicit human experimenter baseline or quantitative human-vs-system comparison is provided for the full automated loop; human designers did manually add new skills and configure reward functions in this prototype.",
            "uuid": "e2446.0",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager (LLM-driven open-ended agent)",
            "brief_description": "An LLM-driven open-ended agent (GPT-4 in referenced work) that proposes tasks and writes code to achieve goals in the Minecraft domain, maintaining a skill library of LLM-generated and self-verified executable code.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models.",
            "mention_or_use": "mention",
            "system_name": "Voyager",
            "system_description": "An LLM-based agent that autonomously proposes tasks, writes code (e.g., JavaScript) to interface with the environment (Minecraft), and maintains a growing skill library of executable code snippets that are LLM-generated and self-verified; uses an auto-curriculum proposed by the LLM to enable open-ended exploration.",
            "system_type": "AI Scientist / Open-ended Embodied Agent",
            "problem_domain": "Embodied agents in simulated game environments (Minecraft); open-ended behavioral discovery and task proposal/execution.",
            "problem_description": "Open-ended exploration and skill acquisition in Minecraft by proposing tasks and generating code to accomplish them; skill library consists of code that can be executed via environment APIs.",
            "problem_complexity": "Open-ended, large discrete and continuous state-action space of Minecraft; requires code generation, environment interaction, and skill verification; high combinatorial and long-horizon complexity. Paper provides no numeric complexity measures here.",
            "data_availability": "Relies on environment interactions; code-generated skills are executed and self-verified via the environment — details and quantities are in Voyager's own work (not reported in detail in this paper).",
            "computational_requirements": "Depends on large LLM inference (GPT-4 in cited Voyager) and environment execution; exact compute not reported in this paper.",
            "problem_structure": "Open-ended and largely unstructured at the top level (free-form task proposals), but execution uses discrete executable code interfaces; clear success detectors via environment state.",
            "success_metric": "Task completion and the growth/utility of the skill library (in the referenced Voyager work).",
            "success_rate": "Not reported in this paper; referenced as prior successful example of LLM-proposed auto-curriculum in Minecraft.",
            "failure_modes": "Not detailed here; mentioned differences with Game On: Voyager's skills are code-level and executable, whereas Game On stores learned parameterized motor policies.",
            "success_factors": "Ability of LLM to propose tasks and generate executable code tied to an environment API; availability of environment for verification.",
            "comparative_results": "Compared conceptually: Voyager inspires curriculum proposal and skill library ideas; Game On differs by storing learnable low-level motor policies rather than code.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.1",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AutoRT",
            "name_full": "AutoRT (LLM orchestration of robot fleets)",
            "brief_description": "An LLM-based orchestrator that proposes tasks on-the-fly to a fleet of robots to collect diverse data; focuses on large-scale data collection rather than in-the-loop policy learning.",
            "citation_title": "AutoRT: Embodied foundation models for large scale orchestration of robotic agents.",
            "mention_or_use": "mention",
            "system_name": "AutoRT",
            "system_description": "Uses LLMs to propose skill-level tasks for a fleet of robots to collect diverse datasets; orchestrates parallel execution for data collection but does not perform active policy learning or task decomposition/sequencing in the same way as Game On.",
            "system_type": "Automated Data-Collection / Orchestration System",
            "problem_domain": "Robotics data collection and orchestration across multiple robots/environments.",
            "problem_description": "Large-scale orchestration to collect diverse robot interaction datasets by proposing tasks for robots to execute; aims to increase data diversity rather than to train policies in the loop.",
            "problem_complexity": "Focus on breadth and scale of data collection across many robots; complexity arises from coordinating many agents and ensuring task diversity; exact quantitative metrics not provided in this paper.",
            "data_availability": "Designed to generate large datasets via many robots; specifics are in the referenced AutoRT work and not enumerated here.",
            "computational_requirements": "Requires LLM inference for many task proposals and orchestration logic; significant parallel compute for robot execution at scale (not quantified here).",
            "problem_structure": "Well-structured for data collection (task proposals are skill-level), less emphasis on open-ended decomposition or policy learning.",
            "success_metric": "Diversity of collected data (measured by authors of AutoRT); Game On notes AutoRT reports data diversity but not downstream policy training results.",
            "success_rate": "Not reported in this paper; Game On notes AutoRT measures diversity but does not validate dataset utility via policy learning.",
            "failure_modes": "AutoRT does not perform in-loop active learning of policies and lacks sequencing/decomposition components; downstream utility of collected data for policy improvement not demonstrated in AutoRT as discussed here.",
            "success_factors": "Scale of parallel robots and LLM-driven proposal generation that focuses on skill-level tasks.",
            "comparative_results": "AutoRT is contrasted with Game On: AutoRT focuses only on data collection and does not validate training policies with the collected data, while Game On uses collected data for self-improvement.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.2",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SayCan",
            "name_full": "SayCan (LLM-guided decomposition with affordance weighting)",
            "brief_description": "A method that uses language models to decompose human-provided high-level tasks into sub-tasks, constraining decomposed steps by available skills and scoring them by affordances in the current scene.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "SayCan",
            "system_description": "SayCan decomposes user-provided high-level instructions into sub-tasks using language models, and ranks sub-tasks by an affordance model that estimates the likelihood that the robot can execute a skill in the current scene; decompositions are constrained by the available skill set.",
            "system_type": "Task Decomposition / Affordance-Grounded Planning",
            "problem_domain": "Robotics instruction following and task decomposition for embodied agents.",
            "problem_description": "Decompose human-specified high-level goals into sequences of known skills, weighing choices by skill affordances under current scene observations; designed for instruction execution rather than autonomous task proposal.",
            "problem_complexity": "Structured decomposition problem with affordance estimation; complexity comes from mapping linguistic goals to executable skills conditioned on scene context; quantitative metrics not provided here.",
            "data_availability": "Requires available skill set and affordance models trained on perception-to-affordance data; not detailed in this paper.",
            "computational_requirements": "Uses LLM inference plus affordance scoring models; specific compute not given here.",
            "problem_structure": "Well-defined decomposition with discrete skill catalog and explicit affordance evaluation; depends on human-provided goal inputs (not open-ended proposals).",
            "success_metric": "Task completion rates when decompositions are executed and affordance-based ordering is used (as reported in SayCan's own work, not detailed here).",
            "success_rate": "Not reported in this paper; the paper notes SayCan requires human-provided instructions (limiting open-ended use).",
            "failure_modes": "Depends on accurate affordance estimation and availability of relevant skills; constrained to human-given tasks, not autonomous task generation.",
            "success_factors": "Integration of affordance scoring with LLM decomposition and a catalog of verified low-level skills.",
            "comparative_results": "Game On is complementary: it does not restrict the LLM's proposals to human inputs and also performs task proposition, while SayCan focuses on decomposition of human-provided tasks.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.3",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CaP / PromptBook",
            "name_full": "CaP (Code-as-Policies) and PromptBook",
            "brief_description": "Lines of work where LLMs write policy code using perception and control APIs to directly produce robot controllers; PromptBook refines prompting for low-level manipulation code generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "CaP / PromptBook",
            "system_description": "CaP leverages LLMs to generate code that acts as policies by calling perception and control APIs; PromptBook provides systematic prompting strategies to elicit robust low-level manipulation code from LLMs. These methods emphasize code generation for control rather than learning parameterized motor policies.",
            "system_type": "Code-as-Policies / Automated Code Generation for Control",
            "problem_domain": "Robotic manipulation and control (robot policy generation via code).",
            "problem_description": "Generate executable control code that, when interfaced with perception and control APIs, drives a robot to perform manipulation tasks; aims to replace hand-coded controllers or learned policies with LLM-generated code.",
            "problem_complexity": "Requires mapping high-level intents to low-level API calls with correct sequencing and safety; long-horizon execution and real-world constraints add complexity. Quantitative metrics not provided here.",
            "data_availability": "Does not primarily rely on human-labeled RL datasets; instead it relies on environment APIs and LLM capabilities to generate code, with verification via execution.",
            "computational_requirements": "LLM inference for code generation and environment execution for verification; compute specifics not provided in this paper.",
            "problem_structure": "Structured: code interfaces are explicit; success can be detected via environment state. However, robustness and safety are concerns in real-world deployment.",
            "success_metric": "Task execution success and reliability of generated controllers (as reported in the cited works).",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Generated code may be brittle, may not handle corner cases, and often requires environment-specific adaptation and testing.",
            "success_factors": "Strong LLM code generation capabilities and well-specified APIs and simulation environments for verification.",
            "comparative_results": "Game On differs by using a learned parameterized low-level policy (PAC) conditioned on language, rather than relying on LLM-generated code as the policy itself.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.4",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SWE-Agent",
            "name_full": "SWE-Agent (LLM-based end-to-end software engineering agent)",
            "brief_description": "An LLM-based agent-computer interface designed to perform end-to-end software engineering tasks including navigating repositories, editing code, executing programs, and automating development workflows.",
            "citation_title": "SWE-agent: Agent-computer interfaces enable automated software engineering.",
            "mention_or_use": "mention",
            "system_name": "SWE-Agent",
            "system_description": "An LLM-based software engineering agent that can interact with a development environment, edit and execute code, run tests, and iteratively modify programs to accomplish software engineering tasks with minimal human intervention.",
            "system_type": "Automated Software Engineering / AI Programmer",
            "problem_domain": "Software engineering, code generation, and development automation.",
            "problem_description": "Automate end-to-end software engineering workflows: generate, modify, execute, and test code to implement features or fix bugs, leveraging LLMs and environment interaction.",
            "problem_complexity": "Complex multi-step programming tasks involving syntactic and semantic correctness, testing, and environment-specific behavior; evaluation often requires program execution and test suites.",
            "data_availability": "Relies on code corpora and interaction with repositories and runtime environments; details not given here.",
            "computational_requirements": "LLM inference, code execution environments, and potentially CI/testing infrastructure; specifics not provided here.",
            "problem_structure": "Well-defined in terms of program specifications and tests, but open-ended in design choices and implementation strategies.",
            "success_metric": "Ability to implement correct code changes and pass tests; metrics and success rates are in SWE-Agent's own work, not detailed here.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "May produce incorrect or insecure code, require human verification, and depend on the quality of environment hooks and tests.",
            "success_factors": "Tight integration with execution and repository interfaces, and strong LLM coding abilities.",
            "comparative_results": "Mentioned as an example of LLM-based agents performing end-to-end engineering tasks, analogous in spirit to research automation but in software rather than embodied RL.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.5",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Eureka (LLM reward design)",
            "name_full": "Eureka: Human-level reward design via coding large language models",
            "brief_description": "A recent approach that codes LLMs to produce reliable reward functions or reward detectors, proposed as a candidate to enable automatic addition of new skills from free-text proposals.",
            "citation_title": "Eureka: Human-level reward design via coding large language models.",
            "mention_or_use": "mention",
            "system_name": "Eureka (LLM-based reward design)",
            "system_description": "Eureka leverages LLMs to generate reward-code and reward-detection logic to judge success of high-level tasks, with the goal of producing zero- or few-shot reward functions that can be used to label episodes and enable automatic skill acquisition.",
            "system_type": "Automated Reward Design / Success Detection",
            "problem_domain": "Reinforcement learning and robotic skill synthesis where defining rewards for arbitrary free-text tasks is required.",
            "problem_description": "Produce robust reward functions or success detectors from language descriptions so that an automated experimenter can add new VLM-proposed tasks as trainable skills without manual reward engineering.",
            "problem_complexity": "Challenging due to reward hacking/exploitation, the need for grounded perception checks, and ensuring reliability across environments; quantitative measures not provided here.",
            "data_availability": "Depends on environment observations (images, states) to evaluate generated reward code; not detailed in this paper.",
            "computational_requirements": "LLM inference to generate reward logic and environment evaluation runs for validation; specifics not provided.",
            "problem_structure": "Open-ended (free-text to reward mapping), requires robust perception and semantics grounding to avoid false positives/negatives.",
            "success_metric": "Reliability of reward detection and resistance to reward exploitation (as discussed in Eureka's own work).",
            "success_rate": "Not reported in this paper; Game On cites Eureka as a promising candidate to enable automated reward generation but does not integrate it.",
            "failure_modes": "Risk of false reward detections and agent exploitation of spurious reward signals; currently not robust enough for full RL integration per Game On authors.",
            "success_factors": "Strong LLM reasoning and careful grounding to environment observations; validation techniques to avoid reward exploitation.",
            "comparative_results": "Game On treats LLM-based reward generation as future work and did not use it in the prototype due to robustness concerns.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.6",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "AutoML-GPT / MLcopilot",
            "name_full": "AutoML-GPT / MLcopilot (LLM-assisted AutoML agents)",
            "brief_description": "Systems that use large language models to assist or perform automated machine learning tasks such as hyperparameter tuning, model selection, or automated experimentation by interacting with code and data.",
            "citation_title": "Automl-gpt: Automatic machine learning with gpt.",
            "mention_or_use": "mention",
            "system_name": "AutoML-GPT / MLcopilot",
            "system_description": "LLM-based agents that assist in automating ML experimentation workflows: propose hyperparameter settings, generate code to run experiments, analyze results, and iterate. Some systems include interfaces to execute code and read files to close the loop.",
            "system_type": "AutoML / Automated Research Agent",
            "problem_domain": "Machine learning experiment automation (hyperparameter tuning, model selection, training/evaluation pipelines).",
            "problem_description": "Automate steps of ML experimentation such as configuring runs, tuning hyperparameters, implementing training code, and analyzing outcomes to optimize model performance.",
            "problem_complexity": "Search over continuous and discrete hyperparameter spaces, non-convex optimization, expensive evaluations; complexity varies with model and dataset size. Quantitative details not provided in this paper.",
            "data_availability": "Relies on available datasets for the ML tasks; some systems can run experiments to generate new data/metrics.",
            "computational_requirements": "Potentially high due to repeated model trainings; depends on the scale of experiments the agent runs (not quantified here).",
            "problem_structure": "Often well-defined optimization problems (validation metric to maximize), but expensive and noisy evaluations; amenable to automated search strategies.",
            "success_metric": "Model performance on validation/test metrics, computational budget efficiency, and comparison to human-tuned baselines.",
            "success_rate": "Not reported in this paper; only cited as examples of LLMs assisting ML experimentation.",
            "failure_modes": "May be limited by the inability to run sufficiently many experiments, biased priors from prompts, and lack of domain-specific constraints.",
            "success_factors": "Ability to interface with execution environments, strong code generation and reasoning by LLMs, and well-defined evaluation metrics.",
            "comparative_results": "Game On differs by focusing on embodied RL and VLM supervision for curriculum and analysis, rather than AutoML for standard ML tasks.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.7",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "PaperQA / Scimon",
            "name_full": "PaperQA and Scimon (LLM agents for scientific research assistance / inspiration)",
            "brief_description": "Examples of LLM-based systems proposed for assisting scientific research: PaperQA is a retrieval-augmented generative agent for literature review, while Scimon focuses on generating novel scientific inspirations optimized for novelty.",
            "citation_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "mention_or_use": "mention",
            "system_name": "PaperQA / Scimon",
            "system_description": "PaperQA uses retrieval-augmented LLM generation to help collect and summarize literature relevant to scientific questions; Scimon is a concept for systems that generate novel scientific ideas optimized for novelty and inspiration, using LLMs as creative engines.",
            "system_type": "Hypothesis/Idea Generation and Literature-assistant Systems",
            "problem_domain": "Scientific literature review and ideation across scientific domains; automated hypothesis generation.",
            "problem_description": "Automate reading and summarization of literature, generate novel research directions or hypotheses, and provide inspiration for new experiments or studies.",
            "problem_complexity": "Open-ended creative reasoning over large literature corpora; evaluating novelty and scientific validity is challenging; specifics not given here.",
            "data_availability": "Depends on accessible corpora of scientific papers and metadata; retrieval augmentations require indexed databases.",
            "computational_requirements": "LLM inference plus retrieval infrastructure; specifics not provided.",
            "problem_structure": "Highly open-ended, with weakly defined success metrics (novelty, plausibility, impact).",
            "success_metric": "Novelty of suggestions, usefulness to researchers, and ability to surface relevant literature; not quantified in this paper.",
            "success_rate": "Not reported in this paper.",
            "failure_modes": "Risk of generating plausible-sounding but incorrect or untestable hypotheses, citation hallucinations, and lack of experimental follow-through.",
            "success_factors": "Strong retrieval components, human-in-the-loop verification, and grounding in experimental feasibility.",
            "comparative_results": "Mentioned as prior work exploring LLMs for automating aspects of scientific research; Game On focuses on automating experimental cycles in embodied RL rather than literature synthesis.",
            "human_baseline": "Not reported in this paper.",
            "uuid": "e2446.8",
            "source_info": {
                "paper_title": "Game On: Towards Language Models as RL Experimenters",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models.",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "AutoRT: Embodied foundation models for large scale orchestration of robotic agents.",
            "rating": 2,
            "sanitized_title": "autort_embodied_foundation_models_for_large_scale_orchestration_of_robotic_agents"
        },
        {
            "paper_title": "SWE-agent: Agent-computer interfaces enable automated software engineering.",
            "rating": 2,
            "sanitized_title": "sweagent_agentcomputer_interfaces_enable_automated_software_engineering"
        },
        {
            "paper_title": "Eureka: Human-level reward design via coding large language models.",
            "rating": 2,
            "sanitized_title": "eureka_humanlevel_reward_design_via_coding_large_language_models"
        },
        {
            "paper_title": "Automl-gpt: Automatic machine learning with gpt.",
            "rating": 1,
            "sanitized_title": "automlgpt_automatic_machine_learning_with_gpt"
        },
        {
            "paper_title": "Paperqa: Retrieval-augmented generative agent for scientific research.",
            "rating": 1,
            "sanitized_title": "paperqa_retrievalaugmented_generative_agent_for_scientific_research"
        },
        {
            "paper_title": "Gensim: Generating robotic simulation tasks via large language models.",
            "rating": 1,
            "sanitized_title": "gensim_generating_robotic_simulation_tasks_via_large_language_models"
        },
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances.",
            "rating": 1,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        }
    ],
    "cost": 0.02076525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Game On: Towards Language Models as RL Experimenters
5 Sep 2024</p>
<p>Jingwei Zhang zhangjingwei@google.com 
Equal contribution</p>
<p>Thomas Lampe thomaslampe@google.com 
Equal contribution</p>
<p>Martin Riedmiller riedmiller@google.com </p>
<p>Google DeepMind United Kingdom</p>
<p>Google DeepMind United Kingdom</p>
<p>Jost Tobias Springenberg Google DeepMind United Kingdom</p>
<p>Game On: Towards Language Models as RL Experimenters
5 Sep 2024EA21699FEAEC83D3928DB0058D4C4656arXiv:2409.03402v1[cs.AI]LLMsReinforcement LearningArtificial Scientist
We propose an agent architecture that automates parts of the common reinforcement learning experiment workflow, to enable automated mastery of control domains for embodied agents.To do so, it leverages a VLM to perform some of the capabilities normally required of a human experimenter, including the monitoring and analysis of experiment progress, the proposition of new tasks based on past successes and failures of the agent, decomposing tasks into a sequence of subtasks (skills), and retrieval of the skill to execute -enabling our system to build automated curricula for learning.We believe this is one of the first proposals for a system that leverages a VLM throughout the full experiment cycle of reinforcement learning.We provide a first prototype of this system, and examine the feasibility of current models and techniques for the desired level of automation.For this, we use a standard Gemini model, without additional fine-tuning, to provide a curriculum of skills to a language-conditioned Actor-Critic algorithm, in order to steer data collection so as to aid learning new skills.Data collected in this way is shown to be useful for learning and iteratively improving control policies in a robotics domain.Additional examination of the ability of the system ability to build a growing library of skills, and to judge the progress of the training of those skills, also shows promising results, suggesting that the proposed architecture provides a potential recipe for fully automated mastery of tasks and domains for embodied agents.</p>
<p>Introduction</p>
<p>Recent progress on leveraging large (vision) language models (VLMs/LLMs) for reinforcement learning and robotics has demonstrated their usefulness for learning robot policies [1,2,3] as well as for high-level reasoning [4,5], and they have also aided research into automated generation of reward functions for policy learning [6,7].</p>
<p>In doing so, LLMs have reduced the amount of domain-specific knowledge that an RL researcher would normally need to provide.Yet there are still many steps within the experiment workflow of training policies via reinforcement learning (RL) that currently require Figure 1: Illustration of the system architecture and the interaction of its components.The curriculum module generates free-text propositions, decomposes them into free-text steps, and tries to map those onto fixed-text skills from a library.We note in the current implementation if the retrieval step (map free-text steps onto fixed-text skills) failed then the plan will be discarded, as we do not have access to a reward model for generating arbitrary reward signals from skill captions; once such a reward model becomes available the failed retrieval should signal the training of a new skill.The generated skill sequence is executed by a text-conditioned policy of the embodiment module and unrolled into an episode, which is used to improve the policy.Performance during policy training is evaluated by the analysis module, which judges whether training has converged and skills should be added to the library.human intervention; such as deciding when an experiment has concluded or building a curriculum of tasks [8,9] to facilitate the learning of a target task.While some work exists in the literature that attempts to automate some of these steps (e.g.automated training and evaluation of standard machine learning tasks [10] or automated curriculum building [11] within the community of automated machine learning), these more automated systems usually consider the individual steps in isolation, using models specifically trained to automate a single step.</p>
<p>In this work, we set out to give a first sketch how a large vision language model (VLM) could be used to automate most of the missing capabilities that would be required to automate a reinforcement learning (RL) experiment.We propose a system architecture that uses a VLM for automating most parts of the reinforcement learning experiment loop (with the current exception of not providing the reward signal), and trains a growing set of motor skills to increasing mastery of desired domains.This architecture integrates several of the capabilities traditionally required from a human experimenter:</p>
<p>• The proposition of new tasks to perform/learn, given a set of already-known tasks.</p>
<p>• The decomposition of higher-level tasks into sequences of low-level skills; paired with retrieving the actual skill the robot possesses.</p>
<p>• Judging whether training of a set of skills has concluded, and a new round of data collection should be started for subsequent reinforcement learning.</p>
<p>To implement our agent, we focus on examining the suitability of currently available VLMs and prompting techniques for the intended level of automation; rather than attempting to expand their capabilities.We therefore limit the scope of this implementation to only some of the components of the proposed system.Notably, we do not automate the stopping of the experiment and the gradual additions of skills to the system yet, and instead present a post-hoc evaluation for both to mimic what their effect would have been.In addition, due to the unavailability of a robust model that can automatically generate reward functions for arbitrary task, we do not automate the addition of arbitrary new skills and limit our evaluation to a domain with known rewards.</p>
<p>In our prototype, all of the reasoning capabilities are driven by a single, general purpose, and publicly available VLM * , and are achieved zero-shot via prompting techniques.Data generated under this high-level VLM's supervision is then used offline to improve a separate 'low-level' policy, which is trained specifically to output actions to control a robot, and is task-conditioned on language instructions from the high-level system.To showcase the usefulness of our approach, we train a policy to perform multiple manipulation tasks on a simulated robot.We show that the VLM-guided exploration produces richer data diversity, which in turn improves performance during successive iterations of policy self-improvement via fine-tuning.In addition, we show that the same VLM can provide experiment supervision by judging the point at which an experiment should be considered to have converged.Lastly, we illustrate that if we provide the VLM with growing sets of skills from different stages of the learning process -as judged by the VLM itself -it can produce reasonable decompositions for each stage and guide learning of progressively more complicated skills.</p>
<p>Related Work</p>
<p>LLM-based Virtual Agents</p>
<p>Following the significant improvements in performance and capabilities of LLMs, the field of LLM-based agent has seen a surge of recent interest.Using an LLM as a general-purpose controller, recent work has attempted to replace components or capabilities that used to require different pieces of software, models or human researchers by using outputs generated by prompting large language models.Among these, there are, for example, works that propose general strategies to obtain enhanced inference from agentic LLMs by the use of chain-of-thought reasoning [13], selfconsistency [14], self-reflexion [15], ReACT chains and tool use [16].</p>
<p>More relevant to our work are the increasing number of LLM-empowered agents proposed to automate science and engineering tasks.For example, LLM-based software engineer agents are now being designed to assist software development, leveraging the greatly improved coding capabilities of these models.This includes work that utilizes language models to enhance various aspects of software development such as assistive AI pair-programming for interactive notebooks or algorithmic reasoning within competitive programming [17,18,19].Some recent work goes even further, e.g. the SWE-Agent [20] explores performing end-toend software engineering with LLM-based agents where a custom agent-computer interface is built to facilitate the agent to navigate repositories, edit code and execute programs.</p>
<p>On the side of automating scientific research, LLM-based agents have been proposed to perform the work of researchers: this includes generating novel research directions [21], reading through relevant literature to gather information [22], automate discovery of scientific knowledge [23], come up with hypothesis and revise it based on experimental evidence [24].In the specific field of automating machine learning research, there are studies that use LLMs to help hyper-parameter tuning of machine learning models [25,26], as well as work that gives the LLM-agent capabilities to interact with computer files and execute code; thus conducting machine learning experimentation in a more integrated fashion [10].</p>
<p>In this work, using a vision language model to both monitor the progress of a machine learning experiment and to examine the resulting performance to influence later experiments is one of the aspects that we focus on.Different to existing work, instead of automating experimentation in purely virtual domains, we perform experiments with an embodied, robotic, agent in this work for automating RL research and automating domain mastery.</p>
<p>LLM/VLM-based Embodied Agents</p>
<p>Above we have discussed works that utilize LLMs to accomplish pure virtual tasks.There are, however, also LLM-empowered methods that are designed to assist embodied agents.</p>
<p>For example, in the Minecraft domain, there is work on using large-scale foundation VLMs to learn behavior policies within the video pre-training (VPT) line of work [27,28,29].More closely related to our work is the open-ended Voyager agent [11].In particular, in Voyager, GPT-4 [30] acts as the backbone, proposing tasks for it to accomplish and writing code to help it achieve goals.It maintains a skill library which keeps track of LLM-generated and self-verified executable code interfacing with the Minecraft environment through JavaScript API; while in our case the stored skills are learned parameterized low-level control policies rather than code.They employ an auto-curriculum proposed by LLMs to enable the agent to perform open-ended exploration; we adopt a similar mechanism, but use it to facilitate automatic domain mastery via RL, although we limit our prototype application to one robotic domain with a restricted set of skills that we can easily evaluate.</p>
<p>In the robotics domain, CaP [31] is closely related to Voyager in the code-writing aspect.It leverages LLMs to write policy code using perception and control APIs to control robots.While its follow-up work PromptBook [32] provides further improvements and guidance in prompting LLMs to write code for low-level manipulation control primitives, the highlevel reasoning capability of LLMs is not highlighted in these works.Utilizing LLM-based reasoning for robotics tasks was pioneered by SayCan [33] which uses language models to decompose a given high-level task into sub-tasks, in which the decoding for decomposition is constrained by the availability of the robot skills and weighed by the affordance of skills under a current scene.In their work, instructions or high-level tasks are provided by human operators rather than suggested by the LLM itself, which restricts the usability of their method for more general and open-ended purposes such as exploration or automatic mastery of domains, as done in this work.Our work is complementary in that we do not restrict the suggestion and decomposition performed by the high-level LLM; but do restrict ourselves to a fixed set of executable low-level skills for which rewards can be computed.Likewise, Di Palo et al. [34] also uses LLMs to decompose tasks into sub-goals, and uses those as instructions for a language-conditioned policy.Similar to SayCan, the tasks in their work are explicitly provided by a user.Furthermore, it requires a separate VLM to obtain text descriptions from visual observations, as well as fine-tuning of an LLM to their specific domain.This is in contrast to our work where a native multimodal Gemini [12] model is used without the need for finetuning.On the task-proposing front, there are several works that focus on simulated domains: Wang et al. [35] propose to leverage LLMs for both proposing tasks and generating simulation code for the proposed tasks, while Xian et al. [36] further sketch a system that also includes components like code generation for reward.Since we are interested in applying our proposed system directly in the real world, these methods would need further adaptation, e.g. by adding automatic reward modeling methods that do not require access to the simulator state.</p>
<p>A perhaps most closely related LLM-assisted agent in the control/robotics domain to our work is AutoRT [37].They adopt an LLM-based approach to orchestrate a fleet of robots to collect diverse data, where tasks are proposed on-the-fly by LLMs.While the open-ended task proposition is very similar to the Voyager paradigm and our work, there are several notable differences to our approach.First, AutoRT is purely aimed at data collection without any active learning of policies 'in the loop' wheras we set out to specifically automate the process of automatically steering the learning process of a low level RL learner.Secondly, since their proposed tasks are at skill-level already, there is no decomposition component in their agent architecture and particularly no sequencing of skills.The authors also only measure the diversity of the collected data, but do not validate the quality of the data empirically by conducting any type of policy training with it.In contrast, we do use the data collected by our proposed system to perform self-improvement and bootstrap a more capable policy.</p>
<p>In order to eventually execute and evaluate arbitrary tasks, plans and subgoals proposed by an automated experimenter, one key capability would be to get a reward function from the language caption of the task.As discussed above the current work does not yet include such a step.While we do not investigate this aspect in this work (we restrict our self-improvement experiments on training skills which the reward function is known), we do note that there is a growing body of works that utilize LLMs to write reward code for desired behavior [6,38,7] and VLMs as general reward models or success detectors [39,40], which are candidates for integration into our system once reliable enough for use in reinforcement learning domains.</p>
<p>System Architecture</p>
<p>We study the setting of an embodied agent with access to a workspace containing objects that it can interact with.We propose a VLM-based agent architecture that can enable automatic mastery of the environment.By mastery we here mean that we expect the agent to be capable of accomplishing any meaningful task -for which we can measure success by a given set of reward functions -with any object in the environment by the end of the learning process, and by automatic we mean that no human researcher is required to come up with a decomposition of tasks or a curriculum for learning the tasks in a specific order during the learning process and that no researcher is needed to monitor the training progress.Our proposed agent architecture to fulfill this goal consists of the following modules:</p>
<p>• The curriculum module, which performs high-level reasoning to guide the learning process with auto-curriculum.More specifically, it is in charge of task proposition, task decomposition, skill retrieval, and keeps a record of past successful and failed episodes.</p>
<p>• The embodiment module, which maintains a skill library consisting of the skills available to the embodiment.It will execute skills assigned by the curriculum in the environment, save episode data and report back success or failure.Finally it will trigger a low-level 'Actor-Critic' RL algorithm for learning (or improving) skills from the collected data.</p>
<p>• The analysis module, which monitors the training progress of skills, reports learning status and adds converged ones to the skill library of the corresponding embodiment.</p>
<p>The Curriculum Module</p>
<p>This module generates an auto-curriculum to guide automatic mastery of domains.Each of its components (task proposition, task decomposition, skill retrieval) is realized by prompting the Gemini model.</p>
<p>In the following, we describe each component's prompts conceptually.For concrete prompt designs used in our prototype implementation, see Appendix A.</p>
<p>Task proposition.We first prompt the VLM to propose a new high-level task in freeform text; for the agent to accomplish given a current image observation and past success and failures.The VLM is prompted to output tasks that are novel and diverse while not being too far from the current capability of the agent.The proposition prompt is heavily inspired by the prompt used in Voyager [11], consisting of a description of the domain, the request for a proposal and matching reasoning leading to it, as well as a set of constraints and requirements.This fixed prompt and a number of exemplars are followed by a current image of the domain, and a growing list of successfully and unsuccessfully completed proposals from the same experiment.We refer to the appendix for a description of the instruction and how exemplars images and success detection are included in the prompt.</p>
<p>proposition_prompt = ( proposition_instruction + proposition_exemplars + current_image_observation + successful_trials + failed_trials ) proposition_reasoning, proposed_task = VLM(proposition_prompt)</p>
<p>Task decomposition.Given a free-text high-level task proposition, the list of available skills and a current image observation, we then prompt the VLM to decompose the task into a list of sub-goals/sub-tasks, again described as fee-form text without any restriction.The decomposition prompt contains a general instruction and several exemplars, to which we concatenate the free-form description of the proposed task from the previous task, the encoding of a current image of the domain, and the skills currently available (note that these skills are naturally limited to those we can evaluate a reward for in our current implementation, and thus are fixed text strings rather than free-form text).We note that although the fixed-text skills are provided to the decomposition prompt, the decomposition is not instructed to output steps using those fixed-text skills only, this is intended such that it should come up with steps that are necessary, which may or may not be available (and thus we could use the steps as a proposal for learning a new skill that should be added to our library); the effect of the availability of the returned free-text skills will be discussed further in the retrieval section.decomposition_prompt = ( decomposition_instruction + decomposition_exemplars ) decomposition_reasoning, [subtask_0, subtask_1, ..., subtask_n] = VLM( decomposition_prompt + proposed_task + current_image_observation + skill_library ) Skill retrieval.Given each decomposed free-text sub-task and the list of available fixedtext skills, we can finally retrieve the most semantically similar skill -from the available skills -to accomplish the desired sub-task.Note that several previous works perform retrieval using the embeddings of the language instructions, whereas we formulate it as a direct question answering (QA) task in text, which we find to be more robust.The retrieval prompt consists of a general instruction that states the retrieval request and the constraint of not rephrasing the retrieved skill name but picking only among the available skills, followed by an exemplar, then appended with the current list of available fixed-text skills and the free-text decomposition step to map.The result of this step then is a plan of a sequence of skills from the library that can be executed by the agent.In addition, if the retrieval of a step in the decomposition sequence fails, this can serve as a signal that a new skill is required in the policy.Such cases can then be included in the next round of policy learning by the embodiment module.We note, however, that due to the lack of a system to generate rewards for arbitrary skills, in this work we manually perform the choice of skills to add; which also means that in the current implementation, the decomposition plan will be discarded if the retrieval of any of its steps failed.</p>
<p>The Embodiment Module</p>
<p>After obtaining a decomposed list of subgoals, the curriculum will communicate the task to the corresponding embodiment to execute.After executing a sequence of skills, it judges the success of the sequence, i.e. whether the goal that led to the decomposition, has been achieved, and reports the result back to the Curriculum module.While determining the success of a sequence relies on pre-defined reward functions in our prototype system (see section 4.1 for details), it could, in theory, also draw upon LLM-based reward functions.</p>
<p>The module also collects all executed episodes in a dataset.Once a stopping criterion has been reached -classically pre-defined as a certain number of episodes, but potentially also triggered by the Analysis module in section 3.3 -a new policy learning iteration is launched with this dataset to fine-tune the previous policy; any offline policy learning algorithm could be used in this step and we refer to the next section for our specific exemplary choice.For increased data efficiency, all of the episodes are re-labeled with the rewards of all of the skills currently known to the agent, including those newly added by the curriculum module described in section 3.1.</p>
<p>The Analysis Module</p>
<p>Finally, this module examines the learning progress of skills by few-shot prompting.The prompt prefix is formatted as:</p>
<p>You are an assistant for monitoring the progress of reinforcement learning experiments.Tell whether the learning has converged or not given the plot of the curve of the accumulated reward it obtains.Give a concise reasoning of your examination result and the answer in YES or NO only.</p>
<p>Each exemplar is given in the format of:</p>
<p>Reward curve: {reward_plot_image}.Reasoning: {reasoning}.A: {has_converged}.</p>
<p>where the reward curve plot for each exemplar is plugged into {reward_plot_image}, and the exemplar reasoning and answer are placed into {reasoning} and {has_converged}.</p>
<p>For all skills for a certain embodiment, the analysis module will periodically go through the learning curves of each of them.Those that are judged as converged will be added to the available skills of that embodiment (and by extension, become available to the curriculum module for decomposition) and its training will be terminated.</p>
<p>We note that this imposes some constraints on the reward functions that can be used: they need to be normalized, and the episode duration of evaluation runs needs to be known, in order to allow meaningful scaling of the curves for analysis.</p>
<p>System Realization</p>
<p>In order to explore the feasibility of the system, we implement its components, and apply them to a simulated robotic manipulation task.</p>
<p>Module Interaction</p>
<p>The curriculum module periodically retrieves images from the environment, and includes them into the goal proposal prompt.The goal is then decomposed into steps and skill captions are retrieved.If any of the steps cannot be mapped to a known skill during retrieval, the plan is discarded, and the process repeated.If all steps are retrieved, the skill sequence is sent to the embodiment module, which uses them to condition a textconitioned learned policy; we use the perceiver-actor-critic (PAC) algorithm [3] to learn and represent such policies.The program flow is controlled by the curriculum module: after each decomposition, all of the potentially multiple embodiment module instances perform a fixed number of episode rollouts, with the skill being changed at fixed (pre-defined) intervals.We acknowledge that this approach only applies for quasi-static domains like the object arrangement tasks considered here.For more dynamic domains, it is necessary to also condition the model to return a duration for each skill, or to continuously query it as to whether to switch skills at a given point in time.</p>
<p>At the end of each rollout, the embodiment module reports whether the plan was successfully executed.Success here is defined as observing a reward &gt; 0.5 for each executed skill, and &gt; 0.95 for the last skill in the sequence; all skills in the proposed sequence must be completed to qualify as success.The curriculum module includes these success reports into its list of successful and unsuccessful plans, for use in subsequent prompts.</p>
<p>We use a chat-based interface between these modules, similar to that used by Sermanet et al. [4].This allows easily connecting them in a natural interface, which also facilitates human introspection and intervention during testing.Modules simply join a Google Meet session, and interact with each other via chat messages, as well as streaming image and video data through it.Messages can be broadcast, enabling a single high-level VLM to control the skills of multiple low-level policies at the same time, thus increasing compute efficiency in the face of otherwise expensive queries to the VLM.The setup is illustrated in Figure 2.</p>
<p>The analysis module is used outside of the experiment loop in this prototype.Rather than actually stopping the experiment, we run it after the experiment has concluded, so that we can evaluate whether the termination point chosen by it was indeed the point of convergence.</p>
<p>Policy Training</p>
<p>For the low-level control policy, we employ a Perceiver-Actor-Critic (PAC) model [3].Such a model has been shown to be trainable via offline reinforcement learning, can be text conditioned, and is able to utilize non-expert (exploration) data that our agent will generate.This in turn allows us to additionally relabel all data with multiple reward functions, and thus reuse a small amount of data to train all desired skills.</p>
<p>In PAC, skills can be represented by either conditioning the policy on language, on goal images, or a mixture thereof.Here, we purely opt for language, as this allows us to directly communicate the high-level system's skill proposals to the low-level policy.</p>
<p>Prompting</p>
<p>The high-level system is represented by a standard Gemini 1.5 Pro model [12].To design the prompts for the Gemini model, we use the publicly available OneTwo Python library [41].OneTwo is a model-agnostic layer that abstracts aspects such as injecting components into a VLM prompt template, and extracting pre-defined fields from the model's response.Each component's prompt contains a small number of exemplars which were hand-designed and include image data from previous experiments.This includes 2 each for for proposal and decomposition, 1 for retrieval, and 6 for analysis.It is also worth noting that none of the proposal exemplars contain a scene with three objects, unlike in the domain we apply it to, in order to not bias the responses.All exemplars used are provided in Appendix B</p>
<p>Experimental Results</p>
<p>Benchmark</p>
<p>To evaluate the benefits of our approach, we consider a robotic block stacking task, previously described in Bousmalis et al. [42].In this task, three colored objects in a basket need to be arranged into a desired configuration by a 7-DoF Franka Panda robot fitted with a Robotiq 2F-85 parallel gripper.The domain is implemented in the MuJoCo simulator [43].This task was chosen since it provides combinatorial complexity, which lends itself to building up more complex skills, yet is also narrow enough to allow automatic evaluation and manual reward design.</p>
<p>More specifically, our expectation with this domain is for the Gemini-aided auto-curriculum to be able to lead the agent to automatically discover and learn the object configurations such as tower and pyramid, which were previously manually designed by human researchers.</p>
<p>Auto-curriculum-based Exploration</p>
<p>To examine the ability of the system to perform task proposal and decomposition, we first train a PAC model to perform a number of simple base skills for the curriculum module to utilize.Note that the framework also allows for the agent to learn from scratch, but here as a proof of concept, we start with a base set of skills to allow for faster learning iterations.We use a pre-existing dataset of approximately 1M episodes collected from a single-task RL experiment, where an MPO agent [44] was trained to perform the different permutations of stacking a single object on top of another.The data is re-labeled with reward functions corresponding to a set of basic skills, including opening and closing the gripper, reaching an object, lifting an object, holding one object over another, and stacking them.For a full list see Appendix D. We then train a PAC model with 140M parameters for 1.5M steps, after which performance has stabilized for all skills.</p>
<p>We then use this fixed policy to perform Gemini-driven data collection, following the approach described in Section 4.1.As this data is intended for further self-improvement training, we roughly follow the CHEF approach [45] of performing a hyperparameter search to aid diversity.However, we do not vary the parameters of the slow PAC training, but instead explore different settings for the curriculum module.Firstly, we vary the sampling temperature of the VLM, using both 0.0 and 0.3.Secondly, we perform collection runs with different sets of skills made available to the agent: either all of the skills including the composite <stack A on B>, or only simpler ones up to <hold A above B>.</p>
<p>In each run, the curriculum module controls 10 simulator instances in order to parallelize data collection.Each skill proposal and decomposition sequence is also executed 5 times per robot to reduce querying load on the VLM.Decomposed plans are executed open-loop, in the sense that each skill in the sequence is maintained for a fixed duration of 20 seconds before switching to the next one.In this manner, we collect a set of 25k robot episodes in total.</p>
<p>Data Diversity</p>
<p>First, we compare the dataset used for pre-training the PAC policy (which we refer to as pretraining set) with the new dataset collected by our method (self-improvement set), using a distance metric similar to Brohan et al. [33].We do this separately for camera images and proprioception data (i.e.joint angles).For camera images, we use a CoCa image embedding [46]; for proprioception, we use the non-embedded observations, and normalize them first along each dimension and then overall for unit norm.Then we measure the relative L2 distance of these representations to each other, as well as the distance of each to their respective cluster in a k-means clustering with 5 clusters (where the clusters were learned on the pretraining set).Table 1 highlights how data in the collected curriculum module set appears to be more spread out.The diversity in vision and proprioception data can be taken to be directly beneficial for self-improvement.</p>
<p>Separately, we also compare the diversity in the embeddings of the language instruction of the skills executed throughout the episodes.We pass these through the embedding available via an older, text-only Gemini model, and contrast the diversity of the pretraining set with the combined one used for fine-tuning.We observe an L2 distance of 0.287 and cluster distance of 0.097 for the pretraining set, vs. 0.555 L2 and 0.732 cluster for the combined set.This matches our expectations, given that the original diversity was low (with only 6 permutation of the form "stack A on B"), the self-improvement set not only executes more diverse skills, but also multiple per episode.</p>
<p>When inspecting the curriculum module data visually, it also becomes apparent that it generates more complex object arrangements.For instance, we find multiple attempts to build a tower, as well as pyramid-like structures -the latter of which result e.g. from failed tower building attempts, as the model does not propose pyramid-building itself.</p>
<p>It is also worth noting that the proposals generated by the model are in fact fairly focused, and mostly cover plans such as <stack A on B and stack X on Y>, as well as <put A next to B>, for only 27 unique proposals.However, the decomposition module expands these into 102 unique skill sequences.For instance, decomposing the task of building a redblue-green tower at different times results in two plans with the same outcome but different skill sequences: <reach blue>, <lift blue>, &lt;above green», <open gripper>, <reach red>, <lift red>, <above blue>, <open gripper> <reach blue>, <lift blue>, <reach green>, <above green>, <open gripper>, <reach red>, <lift red>, <reach blue>, <above blue>, &lt;open gripper» Additional examples, both successful and failed, are provided in Appendix C.</p>
<p>Self-Improvement</p>
<p>In addition to quantifying the quality of the collected data, we also use it to perform a round of self-improvement of the pretrained PAC policy.For this, we introduce three new skill into the set learned by the model that were not available in the skill library given to the curriculum module for data collection: arranging the three objects into a pyramid shape, arranging them in an inverted pyramid, and arranging them to form a tower of three objects.We manually chose these for being the same as previously used as benchmark by Bousmalis et al. [42]; while the curriculum module frequently suggests tower building, it does not suggest the pyramid tasks during exploration.The rewards for these tasks are defined in Appendix D.</p>
<p>Data is relabeled with these new rewards in addition to the existing ones.We then fine-tune the PAC model with two datasets: once using only the original pretraining dataset, and once using the combined pretraining and self-improvement sets.In the latter case, given that the self-improvement set is much smaller than the pretraining set, we up-sample it so that both datasets contribute 50% of each training batch.We also up-sample the newly added pyramid-building skill, to in turn account for 50% of the data from each dataset.Figure 3 compares the performance of these datasets on a selection of skills.As is evident from these results, not only does the added data allow the model to learn the "pyramid" skills, but it also leads to better performance on the base skills.It is worth noting that none of the policy learns to perform tower building; this is due to the low success rate of the pretrained PAC policy when sequencing multiple skills in order to attempt stacking (since this leads to visiting states that are not represented in the original pretraining data).Failed tower building does often lead to "accidental" creation of pyramids however, which explains the better performance on those tasks.We therefore point out that it seems sensible to separate the proposal of new tasks to learn from the proposal of tasks used during data collection.</p>
<p>Finally, using the skills resulting from learning on the combined datasets we perform one additional iteration: we subsequently collect 15k more episodes with the newly trained pyramid building skills added to the skill library; and thus available to the curriculum  module.We again up-sample data so as to weight all three data sources equally.When using this data to fine-tune the PAC policy once more, performance for these skills increases substantially, as also seen in Figure 3.</p>
<p>VLM-based Performance Analysis</p>
<p>During the initial PAC policy training, we trained the model for approximately 1.5M learner steps.After running that long, we observe a degeneration of performance, particularly for "simpler" skills, which can be attributed to overfitting.Normally, a human RL experimenter would employ early stopping to avoid such effects, and stop the experiment once the learning curves for all skills appear to have converged.Here, we use Gemini to judge the convergence state of the experiment post-hoc after the training has concluded and run for an extended number of steps, in order to determine the point at which the model would have proposed early stopping.</p>
<p>All learning curves are scaled to a maximum reward of 400, which is known since rewards are clipped to [0; 1] and evaluation episodes do not exceed 400 steps.The analysis model is not otherwise informed regarding the expected total reward of each skill.</p>
<p>Figure 4 illustrates a selection of these judgments.While these judgments are not fully stable, and false ones do occur, the VLM judges an increasing number of skills as converged as training progresses.Evident errors occur mostly when judging early plateaus in the learning progress (e.g.<place blue on green> at 300k steps or <stack red on green> at 400k steps) -a limitation that would similarly affect a human practitioner if not aware of the expected final reward.Other unstable classifications involve irregular curves such as those of <lift red>.Overall, the ratings reflect both the increasing performance of the skills over time, as well as their relative difficulty, as illustrated in Figure 5, where easier skills can be seen to be judged as converged from early on, while harder ones only get judged as such later on average.</p>
<p>Progressively Adding Skills</p>
<p>A second purpose of the analysis module lies in determining which skills are trained sufficiently to be used for decomposition.In this work, we first trained the PAC policy to convergence, before starting curriculum-driven data collection.But generally, these two processes can be performed concurrently.In order to illustrate the curriculum module's ability to work with a growing set of skills, we therefore examine some of the plans generated when using those skills judged as converged in section 5.3 at certain points in time.</p>
<p>We examine the proposals and decompositions at four points of the experiment: with those skills judged successful after 200k, 500k and 800k learner updates in the first PAC training experiment, as well as the entire set of skills added for self-improvement.</p>
<p>An overview of the model responses is provided in Table 2.For more detailed outputs of the model, including the reasoning provided by the model for each response, see Appendix E.</p>
<p>We see that after both 200k and 500k steps, the proposition yields the same simple goal.But while after 200k steps the system has to use the most basic skills for decomposition, it employs the more reliable <reach green> and <grasp anything> skills at 500k steps.At 800k steps, when all skills are available, it generates more complex propositions, and directly uses the higher-level stacking skills.And with the fine-tuning skills included, the model attempts to arrange the objects into a line, which resolves into building a tower -i.e. a vertical line.</p>
<p>Discussion and Future Work</p>
<p>We have outlined an agent architecture for reinforcement learning that uses a VLM to perform various capabilities normally expected of a human experimenter in Section 3.These capabilities would allow automating the training process of the agent beyond current capabilities, and let an embodied agent autonomously acquire an ever-growing set of skills with ever-increasing mastery.</p>
<p>We implemented and evaluated a first prototype of such a system in Section 4, including the functionalities of proposing new tasks for exploration, decomposing them into skill sequences, and analyzing the progress of the learning experiment.For this first proof-of-concept system, we simplified several of the components and their interaction.This was done both to limit the scope of this study, but also in order to focus on determining whether state-of-the-art Table 2: Example propositions, decompositions and retrievals provided by the curriculum module when provided growing skill libraries from different experiment stages; all propositions and decompositions in this table take in the same image observation containing three objects (red, green, blue) shown in Appendix E. Note that the Reasoning part of the proposition response is cut short to contain only the last segment, the full response are reported in Appendix E.</p>
<p>methods and models are able to perform the required capabilities -particularly when used zero-shot, without costly fine-tuning of the VLM.</p>
<p>The prototype system showed the ability to automatically collect diverse data, which was successfully used to perform self-improvement of the control policy, and to learn new skills not learnable with a more narrow starting set (section 5).The curriculum also displayed the ability to adapt the complexity of its task propositions and plans to the complexity of the available skills.</p>
<p>Going forward, we intend to reduce the simplifications made for the prototype system and strive for full automation, with several natural next steps outlined in the following.</p>
<p>In our prototype implementation, the analysis of learning progress was performed post-hoc, to illustrate the quality of analysis at different stages.While the quality of judgments of experiment convergence was not fully reliable and suffered from the same uncertainty at plateaus that a human does, it did show potential to make correct judgments when aggregated over time.In addition, the successful application of the system to self-improvement of the policy means that even if prematurely terminating the training of skills at plateaus, training of these skills can continue in subsequent self-improvement rounds.In the future, we would therefore seek to integrate it directly into the automation and allow it to stop the experiment.</p>
<p>In the future, we also plan to include LLM-based reward functions into the architecture, once techniques are mature enough.While currently it is too easy for RL agents to exploit false reward detections, recent advances such as Eureka [7] promise zero-or few-shot LLMbased rewards without the need to train specialized models.This provides a natural next experimenter's capability to integrate: to have the system automatically add its proposals as new skills.It will also allow adding the one functionality of the curriculum module we left out thus far: to automatically add proposed actions as new skills -which requires the PAC training to be able to label datasets with rewards matching those proposals.</p>
<p>Related to this, we currently simply discard decomposed sequences if any of the steps cannot be mapped to a skill known by the policy.But encountering an unknown step provides a strong signal that there is a skill missing, and a natural next step is to include it in the next PAC training cycle as a base skill.However, doing so would again require a universal reward module, which is not presently available.</p>
<p>During evaluation of the PAC policies, we observed that skills are often not sequenced correctly, e.g. after completing skill <stack red on blue>, the policy may not perform <stack green on red> successfully, even though it can perform it in isolation.This may be either attributed to incomplete separation between skills, or as insufficient data coverage; generally, the base dataset would never have observed the terminal state of one skill as the initial state of another.This can lead to otherwise sensible plans generated by the decomposition module to not achieve task success, which in turn causes the <stack X on Y and Z on X> skills to never achieve non-zero performance after the PAC finetuning.We hypothesize that this may be rectified through repeated self-improvement, as the skill sequencing would generate more diverse data, and/or by reducing the weight of the narrowly biased pretraining set.However, such extended data collection was not feasible in this work.</p>
<p>During episode unrolls, we executed the skill sequence open-loop, maintaining each skill for a fixed amount of time.This was justified by the fact that the domain is largely static, but we do note that for more dynamic domains, it would be necessary to have the decomposition provide a duration for each skill, or determine the switching point dynamically.The latter is infeasible for Gemini-sized models due to limited inference speed, but is expected to become possible in the future, as smaller yet equally capable models become available.</p>
<p>A Prompt Design</p>
<p>Below are the concrete prompts used in our prototype system.These contain the static parts of the prompts and the format of exemplars.For actual exemplars used, see Appendix B.</p>
<p>A.1 Task proposition</p>
<p>We note that this prompt is heavily inspired by the curriculum prompt in Voyager [11].</p>
<p>You are an assistant for proposing tasks for a robot to perform; the robot has a single arm with a black gripper with two white fingers, it also has a camera looking into the workspace in front of it.</p>
<p>Propose the next task for the robot to perform given: an image observation of the current workspace, a list of the completed tasks so far, a list of failed tasks that are too hard.Give a concise reasoning of your proposal, including listing all possible spatial structures achievable by the objects you see, and then give your proposed task; you should propose tasks that form a curriculum such as to help the robot to accomplish as many diverse tasks as possible, build as many different structures as possible, following these criteria:  [open gripper, grasp the blue object, stack the blue object on top of the green object, grasp the red object, lift the red object up, stack the red object on top of the blue object] Reasoning:</p>
<p>The blue object is stacked on top of the green one.To build a three-level tower, I just need to stack the red object on top of the blue object, which I happen to know how to.A: [stack the red object on top of the blue object]</p>
<p>B.3 Retrieval</p>
<p>Retrieval exemplar 1</p>
<p>Q: stack red on blue</p>
<p>Skill library:</p>
<p>[open gripper, grasp the red object, lift the red object up, put the red object on top of the blue one] Reasoning:</p>
<p>The skill in query asks for a object configuration where the red object is on top of the blue one.Except for the 4th one, all other skills in the library concerns with the red object only, while the 4th one not only concerns the red and the blue object but also matches the desired configuration.A: put the red object on top of the blue one</p>
<p>B.4 Analysis</p>
<p>Analysis exemplar 1</p>
<p>Reward curve: Reasoning:</p>
<p>The learning curve is still going up, the learning has not converged yet.A: NO</p>
<p>Analysis exemplar 2</p>
<p>Reward curve: Reasoning:</p>
<p>The learning curve has reached a plateau and the performance has stablized, the training has converged.</p>
<p>A: YES</p>
<p>Analysis exemplar 3</p>
<p>Reward curve: Reasoning:</p>
<p>The learning curve has reached a plateau and the performance has stablized, the training has converged.</p>
<p>A: YES</p>
<p>Analysis exemplar 4</p>
<p>Reward curve: Reasoning:</p>
<p>The learning curve has has peaked an is degenerating again, the training had converged already.A: YES Analysis exemplar 5</p>
<p>Reward curve: Reasoning:</p>
<p>The learning curve had degenerated after an earlier peak but is going up again, the training has not converged yet.</p>
<p>A: NO</p>
<p>Reward curve: Reasoning:</p>
<p>There is no full learning curve yet, the training has not converged yet.</p>
<p>A: NO</p>
<p>C Response Examples</p>
<p>To illustrate the type of responses generated by the model, we present a number of examples from the beginning of a self-improvement run.</p>
<p>In the absence of successful trials, the very first proposal is to perform a tower building task.</p>
<p>Proposition exemplar EX1</p>
<p>Image observation of the current workspace: Completed tasks so far:</p>
<p>[grasp anything, open gripper, close gripper, reach red, reach green, reach blue, above red, above green, above blue, lift red, lift green, lift blue, stack red on green, stack red on blue, stack green on red, stack green on blue, stack blue on red, stack blue on green] Failed tasks that are too hard:</p>
<p>[] Reasoning:</p>
<p>I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with one object on top of another, and the third one on the side; a three-level tower with one object on top of another; a three-level slanted tower with one object on top of another but not aligned at the center.Previously, the robot has successfully built two-level towers, so trying to build a three-level tower should be both interesting and feasible to try now.A: build a three-level tower with blue on top of green on top of red Decomposition of this task illustrates the potential impact of temperature of the model; if we sample multiple decompositions in the same situation (but with different images) with a high temperature of 1.0, the model produces plans of differing complexity.</p>
<p>for our system to "discover" the heavily engineered, human-provided composite function in prior work.Thus our skill library consists of the following (where X, Y and Z are placeholders for all possible permutations of red, blue and green): open_gripper: Shaped; 0 if the gripper is closed, 1 if is maximally opened.close_gripper: Shaped; Inverse of <open_gripper>.grasp_anything: Binary; 1 if the gripper's grasp sensor is triggered, 0 otherwise.reach_X: Shaped; tangentially decaying distance between the robot's TCP and the center of object X.</p>
<p>above_X: Shaped; tangentially decaying distance between the robot's TCP and a point 10cm above the center of X.</p>
<p>lift_X: Shaped; 0 if the center of X is less than 5cm above the workspace surface, 1 if more than 10cm above, linearly interpolated between those limits.place_X_Y: Shaped; tangentially decaying distance between the center of X and a point 4cm above Y.</p>
<p>stack_X_Y: Shaped; <place_X_Y>, but set to 0 if <grasp_anything> is non-zero.</p>
<p>During the self-improvement experiments, we add three more skills, composed of the above:</p>
<p>triple_stack_X_Y_Z: Product of <stack_X_Y> and <stack_Y_Z>.</p>
<p>pyramid_X_Y_Z: Product of <stack_X_Y> and <stack_X_Z>.</p>
<p>inverse_pyramid_X_Y_Z: Product of <stack_X_Y> and <stack_Z_Y>.</p>
<p>E Progressive Skill Addition</p>
<p>Below are the detailed model responses when examining proposal and decomposition at different steps of the pre-training process in section 5.4.We note that since this is a post-hoc analysis, as the system had all skills available during the data collection experiments we performed.Thus we can not collect successful_trials and failed_trials for proposition.Therefore for all the following rounds, we use all skills available at the corresponding training step in the skill library as the successful_trials and use an empty list as the failed_trials for proposition.</p>
<p>E.1 200k Learner Steps</p>
<p>Skill library:</p>
<p>open gripper close gripper above red above green above blue reach red</p>
<p>E.3 800k Learner Steps</p>
<p>Skill library (added on top of 500k steps):</p>
<p>hold red over green hold red over blue hold green over blue hold green over red hold blue over red hold blue over green stack red on green stack red on blue stack green on blue stack green on red stack blue on red stack blue on green Proposition:</p>
<p>Image observation of the current workspace: Completed tasks so far:</p>
<p>[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green] Failed tasks that are too hard: [] Proposition response:</p>
<p>Reasoning:</p>
<p>I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with one object on top of another, and the third object next to the tower; a three-level tower with one object on top of another; a three-level slanted tower with one object on top of another but not aligned at the center.The robot has successfully built all two-level towers, so trying to build a three-level tower should be both interesting and feasible to try now.A: build a three-level tower with blue on top of red on top of green</p>
<p>E.4 After Self-improvement</p>
<p>Skill library (added on top of 800k steps): stack green on blue and red on green stack blue on green and red on blue stack red on green and blue on red stack green on red and blue on green stack blue on red and green on blue stack red on blue and green on red build a pyramid with red on top and green and blue at the bottom build a pyramid with red on top and blue and green at the bottom build a pyramid with green on top and blue and red at the bottom build a pyramid with green on top and red and blue at the bottom build a pyramid with blue on top and red and green at the bottom build a pyramid with blue on top and green and red at the bottom build an inverted pyramid with green and blue at the top and red at the bottom build an inverted pyramid with blue and green at the top and red at the bottom build an inverted pyramid with blue and red at the top and green at the bottom build an inverted pyramid with red and blue at the top and green at the bottom build an inverted pyramid with red and green at the top and blue at the bottom build an inverted pyramid with green and red at the top and blue at the bottom Proposition:</p>
<p>Image observation of the current workspace: Completed tasks so far:</p>
<p>[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green, stack green on blue and red on green, stack blue on green and red on blue, stack red on green and blue on red, stack green on red and blue on green, stack blue on red and green on blue, stack red on blue and green on red, build a pyramid with red on top and green and blue at the bottom, build a pyramid with red on top and blue and green at the bottom, build a pyramid with green on top and blue and red at the bottom, build a pyramid with green on top and red and blue at the bottom, build a pyramid with blue on top and red and green at the bottom, build a pyramid with blue on top and green and red at the bottom, build an inverted pyramid with green and blue at the top and red at the bottom, build an inverted pyramid with blue and green at the top and red at the bottom, build an inverted pyramid with blue and red at the top and green at the bottom, build an inverted pyramid with red and blue at the top and green at the bottom, build an inverted pyramid with red and green at the top and blue at the bottom, build an inverted pyramid with green and red at the top and blue at the bottom] Failed tasks that are too hard : []</p>
<p>plan = [] for query_subtask in [subtask_0, subtask_1, ..., subtask_n]: retrieval_prompt = ( retrieval_instruction + retrieval_exemplars + query_subtask + skill_library ) retrieval_reasoning, retrieved_skill = VLM(retrieval_prompt) plan.append(retrieved_skill)</p>
<p>Figure 2 :
2
Figure 2: Screenshot of a Google Meet session hosting the agent when performing a multirobot simulation experiment, with boxes annotating the different modules.Note that the curriculum module mirrors the image stream of one of the embodiment modules it currently attends to.</p>
<p>Figure 3 :
3
Figure 3: Training curves for PAC self-improvement.Adding the self-improvement set (red) consistently outperforms using only the pretraining set (green), both on the average of all tasks (top left), and on all individual task families.Adding then a third set of data collected with the newly added pyramid skills, produces even better performance (blue).</p>
<p>Figure 4 :
4
Figure 4: Example evaluation curves provided to the VLM, at different numbers of learning steps during training.Color coding denotes whether a curve was judged as converged (green)or not yet converged (red).</p>
<p>Figure 5 :
5
Figure 5: Cumulative percentage of curves judged as converged by the analysis module, per task family.</p>
<p>B. 2 1 Q
21
Task decomposition Decomposition exemplar : put the red object next to the blue object Image observation of the current workspace: Available skills: [open gripper, grasp the red object, lift the red object up, stack the red object on top of the blue object] Reasoning: All three objects are on the bottom of the basket.The robot is able to stack the red object on top of the blue object but does not know how to put the red one next to the blue one.However, the task might be accomplished by first stacking the red on blue, then lift up the red object and open the gripper, in which case the red object should drop not too far from the blue object.A: [stack the red object on top of the blue object, lift the red object up, open gripper] Decomposition exemplar 2 Q: build a three-level tower Image observation of the current workspace: Available skills:</p>
<p>Table 1 :
1
Diversity metrics for original and collected datasets, per modality.
VisionProprioceptionL2ClusterL2Clusterpretraining0.5710.3800.8150.369self-improvement 0.6250.4730.9010.551</p>
<p>It should be a single phrase.Do not propose multiple tasks at the same time.Do not mention anything else. 2. The next task should not be too hard since the robot may not have learned enough skills to complete it yet.3. The next task should be novel and interesting.The robot should look for different objects to manipulate and different object configurations to achieve.You should not ask the robot to perform the same thing over and over again.4. The robot may sometimes need to repeat some tasks if it needs to collect more data to complete more difficult tasks.Only repeat if necessary.5.The proposed task should ideally be composable of the current skills available to the robot.The robot should look for different objects to manipulate and different object configurations to achieve such that at the end of the curriculum it has "mastered" the whole workspace.
1. The next task should follow a concise format, such as "put [object_1]next to [object_2]", "stack [object_1] on top of [object_2]" etc."build a [spatial_structure] with [object_1] and [object_2] on top of[object_3]", etc.
AcknowledgmentsWe thank Ksenia Konyushkova for suggestions on prompting, Pierre Sermanet for laying the foundations of the technical infrastructure, and Jonas Adler for general discussions of the research field.Each exemplar is given in the format of:Image observation of the current workspace: {image_observation}.Completed tasks so far: {successful_trials}.Failed tasks that are too hard: {failed_trials}.Reasoning: {reasoning}.A: {proposed_task}.The evaluation content at run time will then be given in the format of:Image observation of the current workspace: {image_observation}.Completed tasks so far: {successful_trials}.Failed tasks that are too hard: {failed_trials}.And the model will respond in the following format: Reasoning: {reasoning}.A: {proposed_task}.A.2 Task decompositionYou are an assistant for helping a robot completing a given task by decomposing it into a sequence of subtasks; the robot has a single arm with a black gripper with two white fingers, it also has a camera looking into the workspace in front of it.Decompose the given task into subtasks that the robot knows how to perform given: an image observation of the current workspace, a list of the available skills of the robot.Give a concise reasoning of your decomposition and then give your result as a python list of strings, each string contains a decomposed subtask.Each exemplar is given in the format of:Image observation of the current workspace: {image_observation}.Available skills: {available_skills}.Reasoning: {reasoning}.A: {decomposed_task}.The evaluation content at run time will then be given in the format of:Image observation of the current workspace: {image_observation}.Available skills: {available_skills}.And the model will respond in the following format: Reasoning: {reasoning}.A: {decomposed_task}.A.3 Skill retrievalYou are an assistant for retrieval.Find the most semantically similar entry from a skill library given a query skill description.Pay attention to the object configurations induced by the skill, give a concise reasoning about the result and return the exact entry from the library without rephrasing it.Each exemplar is given in the format of: Q: {query_skill}.Skill library: {available_skills}.Reasoning: {reasoning}.A: {retrieved_skill}.The evaluation content at run time will then be given in the format of:And the model will respond in the following format: Reasoning: {reasoning}.A: {retrieved_skill}.B Prompt ExemplarsThis section provides the concrete exemplars that are in the prompt templates described above when querying the model.B.1 PropositionProposition exemplar 1Image observation of the current workspace: Completed tasks so far:[open gripper, grasp the red object] Failed tasks that are too hard:[] Reasoning:I see one object: red.All possible spatial structures that can be built with it: move the red object to a desired 3D point.Previously, the robot has successfully grasped the red object, so to reach a different spatial structure, you can try move it to a different planar position or lift it up.A: lift the red object upProposition exemplar 2Image observation of the current workspace: Completed tasks so far:[open gripper, grasp the red object, lift the red object up] Failed tasks that are too hard:[stack the green object on top of the red object] Reasoning:I see two objects: red, green.All possible spatial structures that can be built with them: a line where the two objects are placed next to each other; two dots where the two objects are apart from each other; a two-level tower with one object on top of another.a two-level slanted tower with one object on top of another but not aligned at the center.Previously, the robot has successfully manipulated the red object but not the green one, and it also has not built any structure with both two objects, so trying to build a two-level tower by stacking the red object on top of the green object should be both interesting and feasible to try now.A: stack the red object on top of the green object Decomposition exemplar EX2 Q: build a three-level tower with blue on top of red on top of green Image observation of the current workspace: Available skills:[grasp anything, open gripper, close gripper, reach red, reach green, reach blue, above red, above green, above blue, lift red, lift green, lift blue, stack red on green, stack red on blue, stack green on red, stack green on blue, stack blue on red, stack blue on green] Reasoning:All three objects are on the bottom of the basket.To build a three-level tower with blue on top of red on top of green, I need to first stack red on green, then stack blue on red.[grasp anything, open gripper, close gripper, reach red, reach green, reach blue, above red, above green, above blue, lift red, lift green, lift blue, stack red on green, stack red on blue, stack green on red, stack green on blue, stack blue on red, stack blue on green] Reasoning:All three objects are on the bottom of the basket.To build a three-level tower with blue on top of red on top of green, I need to first stack red on green, then stack blue on red.A: [stack red on green, stack blue on red]The same task is also proposed later, after collecting several trials, with the reasoning that it failed several times and should thus be practiced.Note that if the same trial is attempted multiple times, it is combined and denoted with a success count (x3) for brevity; in the actual prompt, these are expanded into a list.Proposition exemplar EX4Image observation of the current workspace: Completed tasks so far:[grasp anything, open gripper, close gripper, reach red, reach green, reach blue, above red, above green, above blue, lift red, lift green, lift blue, stack red on green, stack red on blue, stack green on red, stack green on blue, stack blue on red, stack blue on green, build a three-level tower with blue on top of green on top of red (x2), build a three-level tower with blue on top of red on top of green (x2)] Failed tasks that are too hard:[build a three-level tower with blue on top of green on top of red (x2), build a three-level tower with blue on top of red on top of green (x5)] Reasoning:I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with one object on top of another, and the third one on the side; a three-level tower with one object on top of another; a three-level slanted tower with one object on top of another but not aligned at the center.The robot has successfully built three-level towers but failed many times, so it needs more practice.A: build a three-level tower with blue on top of red on top of green A common failure case involves the model incorrectly injecting a description of the scene into the answer field of the response.This leads to a failure in the OneTwo library to parse the response even when it contains a task proposal, which in turn will cause the plan to be rejected.Decomposition exemplar EX5Q: build a two-level slanted tower with blue on top of redImage observation of the current workspace: Available skills:[grasp anything, open gripper, close gripper, reach red, reach green, reach blue, above red, above green, above blue, lift red, lift green, lift blue, stack red on green, stack red on blue, stack green on red, stack green on blue, stack blue on red, stack blue on green] Reasoning:All three objects are on the bottom of the basket.The robot can stack blue on red to build a two-level tower, but it might not be slanted.To make it slanted, the robot can first lift the red object up, then move above blue and drop it.A: All three objects are on the bottom of the basketD PAC Skills and Reward FunctionsThe reward functions used to train the PAC policy generally follow those used in Bousmalis et al.[42].However, we use lower-level skills: instead of the strongly shaped and staged "stack and leave" reward they use, we only use its atomic components, as we would hopeReasoning:I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with one object on top of another; a three-level tower with one object on top of another; a two-level slanted tower with one object on top of another but not aligned at the center; a three-level slanted tower with one object on top of another but not aligned at the center.Previously, the robot has successfully reached the red object but has not grasped any object yet, so trying to grasp an object should be both interesting and feasible to try now.A: grasp the green object Decomposition:Q: grasp the green objectImage observation of the current workspace: Available skills:[open gripper, close gripper, above red, above green, above blue, reach red] Decomposition response:Reasoning:The robot does not know how to grasp the green object directly.However, it can move its gripper above the green object and then close the gripper, which should result in grasping the green object.A: [above green, close gripper]Retrieval:Q: above greenSkill library:[open gripper, close gripper, above red, above green, above blue, reach red] Retrieval response:Reasoning:The query concerns with a configuration where something is above the green object.All skills except above green are irrelevant.A: above greenQ: close gripper Skill library:[open gripper, close gripper, above red, above green, above blue, reach red] Retrieval response:Reasoning:The query skill concerns with the gripper only.The second entry in the library is the only one that concerns with the gripper.A: close gripperE.2 500k Learner StepsSkill library (added on top of 200k steps): reach green reach blue grasp anything lift red Proposition:Image observation of the current workspace: Completed tasks so far:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red] Failed tasks that are too hard: [] Proposition response:Reasoning:I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with one object on top of another, and the third one on the side; a three-level tower with one object on top of another; a three-level slanted tower with one object on top of another but not aligned at the center.Previously, the robot has successfully manipulated the red object but not the green and blue ones, so trying to grasp the green object should be both interesting and feasible to try now.A: grasp the green object Decomposition:Q: grasp the green objectImage observation of the current workspace: Available skills:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red] Decomposition response:Reasoning:The robot can reach above the green object, reach the green object, and grasp anything.Therefore, the robot can grasp the green object.A: [The robot can reach above the green object, reach the green object, and grasp anything] Retrieval:Q: The robot can reach above the green object Skill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red] Retrieval response:Reasoning:The query concerns the robots capability of reaching a location above the green object.All skills in the library except above green are about robot actions, while above green describes a location relative to the green object.A: above green Retrieval:Q: reach the green objectSkill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red] Retrieval response:Reasoning:The query asks for reaching the green object.All skills except reach green are either irrelevant or describe a different object.A: reach greenRetrieval:Q: and grasp anything Skill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red] Retrieval response:Reasoning:The query skill does not induce any specific object configuration but asks for a grasping action.The only skill in the library that involves grasping is grasp anything.[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green] Decomposition response:A: grasp anythingReasoning:All three objects are on the bottom of the basket.To build a three-level tower with blue on top of red on top of green, I need to stack red on green first, then stack blue on red.A: [stack red on green, stack blue on red] Retrieval:Q: stack red on greenSkill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green]Retrieval response:Reasoning:The query asks for a specific object configuration where the red object is on top of the green one.All skills except those related to stack only involve individual object manipulation.The desired configuration is specifically achieved by stack red on green.A: stack red on green Retrieval:Q: stack blue on redSkill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green]Retrieval response:Reasoning:The query asks for a specific object configuration -blue on top of red.All other skills either manipulate a single object or create different configurations.Only stack blue on red matches the desired outcome.A: stack blue on red Proposition response:Reasoning:I see three objects: red, green, blue.All possible spatial structures that can be built with them: a line where the three objects are placed next to each other; three dots where the three objects are apart from each other; a two-level tower with two objects on top of one another; a three-level tower with three objects on top of one another; a two-level slanted tower with two objects on top of one another but not aligned at the center; a three-level slanted tower with three objects on top of one another but not aligned at the center.The robot has already built all possible two-level and three-level towers, including slanted ones.The robot has not yet built a line with all three objects.A: build a line with the red object next to the green object and the blue object next to the green object Decomposition:Q: build a line with the red object next to the green object and the blue object next to the green objectImage observation of the current workspace: Available skills:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green, stack green on blue and red on green, stack blue on green and red on blue, stack red on green and blue on red, stack green on red and blue on green, stack blue on red and green on blue, stack red on blue and green on red, build a pyramid with red on top and green and blue at the bottom, build a pyramid with red on top and blue and green at the bottom, build a pyramid with green on top and blue and red at the bottom, build a pyramid with green on top and red and blue at the bottom, build a pyramid with blue on top and red and green at the bottom, build a pyramid with blue on top and green and red at the bottom, build an inverted pyramid with green and blue at the top and red at the bottom, build an inverted pyramid with blue and green at the top and red at the bottom, build an inverted pyramid with blue and red at the top and green at the bottom, build an inverted pyramid with red and blue at the top and green at the bottom, build an inverted pyramid with red and green at the top and blue at the bottom, build an inverted pyramid with green and red at the top and blue at the bottom] Decomposition response:Reasoning:The robot can stack green on blue and red on green in one go, which will result in the desired configuration.A: [stack green on blue and red on green] Q: stack green on blue and red on greenSkill library:[open gripper, close gripper, above red, above green, above blue, reach red, reach green, reach blue, grasp anything, lift red, hold red over green, hold red over blue, hold green over blue, hold green over red, hold blue over red, hold blue over green, stack red on green, stack red on blue, stack green on blue, stack green on red, stack blue on red, stack blue on green, stack green on blue and red on green, stack blue on green and red on blue, stack red on green and blue on red, stack green on red and blue on green, stack blue on red and green on blue, stack red on blue and green on red, build a pyramid with red on top and green and blue at the bottom, build a pyramid with red on top and blue and green at the bottom, build a pyramid with green on top and blue and red at the bottom, build a pyramid with green on top and red and blue at the bottom, build a pyramid with blue on top and red and green at the bottom, build a pyramid with blue on top and green and red at the bottom, build an inverted pyramid with green and blue at the top and red at the bottom, build an inverted pyramid with blue and green at the top and red at the bottom, build an inverted pyramid with blue and red at the top and green at the bottom, build an inverted pyramid with red and blue at the top and green at the bottom, build an inverted pyramid with red and green at the top and blue at the bottom, build an inverted pyramid with green and red at the top and blue at the bottom]Retrieval response:Reasoning:The query asks for a three-object configuration with green on blue and red on green.This configuration is exactly what the 22nd skill in the library describes.A: stack green on blue and red on green
Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, Q Vuong, V Vanhoucke, H Tran, R Soricut, A Singh, J Singh, P Sermanet, P R Sanketi, G Salazar, M S Ryoo, K Reymann, K Rao, K Pertsch, I Mordatch, H Michalewski, Y Lu, S Levine, L Lee, T.-W E Lee, I Leal, Y Kuang, D Kalashnikov, R Julian, N J Joshi, A Irpan, B Ichter, J Hsu, A Herzog, K Hausman, K Gopalakrishnan, C Fu, P Florence, C Finn, K A Dubey, D Driess, T Ding, K M Choromanski, X Chen, Y Chebotar, J Carbajal, N Brown, A Brohan, M G Arenas, K Han, Proceedings of The 7th Conference on Robot Learning. J Tan, M Toussaint, K Darvish, The 7th Conference on Robot LearningPMLRNov 2023229of Proceedings of Machine Learning Research</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, T Eccles, J Bruce, A Razavi, A Edwards, N Heess, Y Chen, R Hadsell, O Vinyals, M Bordbar, N De Freitas, 2022A generalist agent</p>
<p>Offline actor-critic reinforcement learning scales to large models. J T Springenberg, A Abdolmaleki, J Zhang, O Groth, M Bloesch, T Lampe, P Brakel, S Bechtle, S Kapturowski, R Hafner, N Heess, M Riedmiller, 2024</p>
<p>P Sermanet, T Ding, J Zhao, F Xia, D Dwibedi, K Gopalakrishnan, C Chan, G Dulac-Arnold, S Maddineni, N J Joshi, P Florence, W Han, R Baruch, Y Lu, S Mirchandani, P Xu, P Sanketi, K Hausman, I Shafran, B Ichter, Y Cao, Robovqa, Multimodal long-horizon reasoning for robotics. 2023</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Z Xu, S Kirmani, B Ichter, D Driess, P Florence, D Sadigh, L Guibas, F Xia, arXiv:2401.121682024arXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. W Yu, N Gileadi, C Fu, S Kirmani, K.-H Lee, M G Arenas, H.-T L Chiang, T Erez, L Hasenclever, J Humplik, Conference on Robot Learning. PMLR2023</p>
<p>Eureka: Human-level reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, 2024</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, Journal of Machine Learning Research. 211812020</p>
<p>Curriculum reinforcement learning via constrained optimal transport. P Klink, H Yang, C D'eramo, J Peters, J Pajarinen, International Conference on Machine Learning. PMLR2022</p>
<p>Benchmarking large language models as AI research agents. Q Huang, J Vora, P Liang, J Leskovec, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>Anandkumar. Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A , 2023</p>
<p>Gemini Team, Google , arXiv:2312.11805Gemini: A family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Execution-based evaluation for opendomain code generation. Z Wang, S Zhou, D Fried, G Neubig, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Natural language to code generation in interactive data science notebooks. P Yin, W.-D Li, K Xiao, A K Rao, Y Wen, K Shi, J Howland, P Bailey, M Catasta, H Michalewski, The 61st Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Can language models solve olympiad programming?. Q Shi, M Tang, K Narasimhan, S Yao, arXiv:2404.109522024arXiv preprint</p>
<p>J Yang, C E Jimenez, A Wettig, K Lieret, S Yao, K Narasimhan, O Press, SWE-agent: Agent-computer interfaces enable automated software engineering. 2024</p>
<p>Q Wang, D Downey, H Ji, T Hope, arXiv:2305.14259Scimon: Scientific inspiration machines optimized for novelty. 2023arXiv preprint</p>
<p>Paperqa: Retrieval-augmented generative agent for scientific research. J Lála, O Odonoghue, A Shtedritski, S Cox, S G Rodriques, A White, 2023</p>
<p>Automated scientific discovery: from equation discovery to autonomous discovery systems. S Kramer, M Cerrato, S Džeroski, R King, arXiv:2305.022512023arXiv preprint</p>
<p>P Ma, T.-H Wang, M Guo, Z Sun, J B Tenenbaum, D Rus, C Gan, W Matusik, arXiv:2405.09783Llm and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. 2024arXiv preprint</p>
<p>Automl-gpt: Automatic machine learning with gpt. S Zhang, C Gong, L Wu, X Liu, M Zhou, arXiv:2305.024992023arXiv preprint</p>
<p>MLcopilot: Unleashing the power of large language models in solving machine learning tasks. L Zhang, Y Zhang, K Ren, D Li, Y Yang, arXiv:2304.149792023arXiv preprint</p>
<p>Video pretraining (vpt): Learning to act by watching unlabeled online videos. B Baker, I Akkaya, P Zhokov, J Huizinga, J Tang, A Ecoffet, B Houghton, R Sampedro, J Clune, Advances in Neural Information Processing Systems. 202235</p>
<p>Steve-1: A generative model for text-to-behavior in minecraft. S Lifshitz, K Paster, H Chan, J Ba, S Mcilraith, Advances in Neural Information Processing Systems. 202436</p>
<p>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. Z Wang, S Cai, A Liu, X Ma, Y Liang, Second Agent Learning in Open-Endedness Workshop. 2023</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>How to prompt your robot: A promptbook for manipulation skills with code as policies. M G Arenas, T Xiao, S Singh, V Jain, A Z Ren, Q Vuong, J Varley, A Herzog, I Leal, S Kirmani, D Sadigh, V Sindhwani, K Rao, J Liang, A Zeng, Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL2023. 2023</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, Conference on robot learning. PMLR2023</p>
<p>Towards a unified agent with foundation models. N Di Palo, A Byravan, L Hasenclever, M Wulfmeier, N Heess, M Riedmiller, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>Gensim: Generating robotic simulation tasks via large language models. L Wang, Y Ling, Z Yuan, M Shridhar, C Bao, Y Qin, B Wang, H Xu, X Wang, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Z Xian, T Gervet, Z Xu, Y.-L Qiao, T.-H Wang, Y Wang, arXiv:2305.10455Towards generalist robots: A promising paradigm via generative simulation. 2023arXiv preprint</p>
<p>AutoRT: Embodied foundation models for large scale orchestration of robotic agents. M Ahn, D Dwibedi, C Finn, M G Arenas, K Gopalakrishnan, K Hausman, B Ichter, A Irpan, N Joshi, R Julian, S Kirmani, I Leal, E Lee, S Levine, Y Lu, I Leal, S Maddineni, K Rao, D Sadigh, P Sanketi, P Sermanet, Q Vuong, S Welker, F Xia, T Xiao, P Xu, S Xu, Z Xu, 2024</p>
<p>Learning to learn faster from human feedback with language model predictive control. J Liang, F Xia, W Yu, A Zeng, M G Arenas, M Attarian, M Bauza, M Bennice, A Bewley, A Dostmohamed, arXiv:2402.114502024arXiv preprint</p>
<p>Vision-language models are zero-shot reward models for reinforcement learning. J Rocamonde, V Montesinos, E Nava, E Perez, D Lindner, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Vision-language models as success detectors. Y Du, K Konyushkova, M Denil, A Raju, J Landon, F Hill, N Freitas, S Cabi, Conference on Lifelong Learning Agents. PMLR2023</p>
<p>Robocat: A self-improving generalist agent for robotic manipulation. K Bousmalis, G Vezzani, D Rao, C Devin, A X Lee, M Bauza, T Davchev, Y Zhou, A Gupta, A Raju, A Laurens, C Fantacci, V Dalibard, M Zambelli, M Martins, R Pevceviciute, M Blokzijl, M Denil, N Batchelor, T Lampe, E Parisotto, K Żołna, S Reed, S G Colmenarejo, J Scholz, A Abdolmaleki, O Groth, J.-B Regli, O Sushkov, T Rothörl, J E Chen, Y Aytar, D Barker, J Ortiz, M Riedmiller, J T Springenberg, R Hadsell, F Nori, N Heess, 2023</p>
<p>MuJoCo: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 10.1109/IROS.2012.63861092012 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012</p>
<p>A Abdolmaleki, J T Springenberg, Y Tassa, R Munos, N Heess, M Riedmiller, Maximum a posteriori policy optimisation. 2018</p>
<p>T Lampe, A Abdolmaleki, S Bechtle, S H Huang, J T Springenberg, M Bloesch, O Groth, R Hafner, T Hertweck, M Neunert, M Wulfmeier, J Zhang, F Nori, N Heess, M Riedmiller, Mastering stacking of diverse shapes with large-scale iterative reinforcement learning on real robots. 2023</p>
<p>CoCa: Contrastive captioners are image-text foundation models. J Yu, Z Wang, V Vasudevan, L Yeung, M Seyedhosseini, Y Wu, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
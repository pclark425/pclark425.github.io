<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1539 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1539</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1539</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-7b35984b9b0bde78290cf92d8176682b65e59f54</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7b35984b9b0bde78290cf92d8176682b65e59f54" target="_blank">GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes using GPU-accelerated RL simulations as an alternative to CPU ones for speeding up Deep RL training, and shows promising speed-ups of learning various continuous-control, locomotion tasks.</p>
                <p><strong>Paper Abstract:</strong> Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using 10-1000x fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1539.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1539.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPU-based physics engine (NVIDIA Flex / in-house)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NVIDIA Flex / in-house GPU-accelerated physics engine (used in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPU-accelerated rigid-body physics simulator implemented with CUDA (referred to as NVIDIA Flex and an in-house GPU-based physics engine) used to concurrently simulate hundreds to thousands of robot agents for RL locomotion tasks; uses implicit time-stepping and a non-smooth Newton rigid-body solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>NVIDIA Flex / in-house GPU-based physics engine</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>GPU-accelerated rigid-body physics engine (CUDA) built with a non-smooth Newton solver and maximal-coordinate formulation, supporting torque control, speculative collision/contact detection, Coulomb friction, and implicit time-discretization for time-stepping; designed to simulate many agents concurrently on a single GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (rigid-body dynamics, contact mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity rigid-body dynamics with explicit contact modeling for locomotion; tuned to produce realistic humanoid gaits.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Non-smooth Newton rigid-body solver; maximal-coordinate formulation; unilateral constraint functions for contacts with smooth isotropic Coulomb friction (sliding friction coefficient = 1.0); restitution = 0.0; gravity = 9.8 m/s^2; implicit time-discretization; time step = 1/120 s; sparse iterative Krylov linear solver with minimal linear iterations chosen for stability; agents can interact (inter-agent collisions enabled).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Ant; Humanoid (28-DoF); Humanoid Flagrun Harder (HFH); HFH on Complex Terrain</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep RL agents trained with a variation of Proximal Policy Optimization (PPO); policy and value are feed-forward neural networks (SELU activations) sharing architecture; training runs synchronized distributed PPO variants with Horovod/NCCL for multi-GPU.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learning continuous-control locomotion behaviors: run forward, change direction toward targets, recover from falls (stand-up), and traverse uneven/complex terrains with a height map.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Trained humanoid running task in <20 minutes on a single machine (1 GPU + 1 CPU core); e.g., 16 minutes to run using 512 agents; multi-GPU HFH: 1 GPU reached reward 4000 in ~2 hours, 8 GPUs in ~30 minutes, 16 GPUs in ~15 minutes. (All values reported in paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Potential sim-to-real transfer to real robots (paper discusses potential but does not perform transfer); cites prior sim-to-real successes using Bullet for real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper emphasizes solver choices that affect realism: Krylov iterative methods gave sufficient stiffness for realistic humanoid gaits while relaxation methods (e.g., Projected Gauss-Seidel) were less effective; uses contact modeling (Coulomb friction), implicit time-stepping, and a fine timestep (1/120 s). The authors do not present a formal minimum-fidelity study for transfer, but note these solver/parameter choices were important for achieving realistic locomotion.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors report that relaxation solvers (Projected Gauss-Seidel) were less effective at producing realistic humanoid gaits compared to Krylov-based solvers; they also caution that differences in physics simulators (solvers, iterations) make direct performance comparisons difficult. The paper does not report explicit sim-to-real failure cases for this engine.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1539.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1539.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo: A Physics Engine for Model-Based Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used CPU-based physics simulator for continuous-control RL, known for efficient simulation and relatively stable contact models; used extensively in prior locomotion benchmarks and cited here for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MuJoCo: A Physics Engine for Model-Based Control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>CPU-based rigid-body physics engine popular in RL research for continuous control benchmarks (e.g., humanoid and ant locomotion environments); provides efficient simulation and contact handling.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (rigid-body dynamics, contact mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity rigid-body dynamics with stable contact models (commonly treated as the standard for RL locomotion benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Efficient CPU simulation with stable contact modelling; used as the reference simulator for many continuous-control RL tasks; paper measures MuJoCo 1.5 single-core simulation times to compare CPU vs GPU scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Standard MuJoCo locomotion agents (e.g., Humanoid environments, Ant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Simulated agents/environments used by prior RL works (not trained within this paper using MuJoCo, but MuJoCo timings were measured for comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Continuous-control locomotion tasks (benchmark environments for policy learning).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Not directly trained in this paper; authors reference prior works that trained humanoid policies using MuJoCo (e.g., Salimans et al. trained MuJoCo humanoid with Evolution Strategies in ~10 minutes on large CPU clusters). Within this paper MuJoCo 1.5 single-core simulation times were measured and plotted (Appendix F).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes that simulation differences (solvers, number of iterations) across engines (MuJoCo vs GPU engine) make rewards and behaviors not directly comparable, but does not prescribe a minimal fidelity for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper does not report MuJoCo-specific transfer failures; it does report that MuJoCo single-core simulation time increases as number of concurrent humanoids increases (scaling limitation) compared to GPU simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1539.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1539.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bullet Physics Library</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source physics engine often used in robotics and sim-to-real work; cited here as the engine used by several recent successful sim-to-real studies that trained policies in simulation and deployed them on real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Bullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open-source rigid-body physics engine used in robotics and manipulation research; commonly used to generate simulated data for training policies intended for real-world deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (rigid-body dynamics, contact mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity rigid-body/contact dynamics suitable for policy training and sim-to-real pipelines (used in cited sim-to-real successes).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Approximates rigid-body contacts and friction; commonly used with domain randomization and other techniques to enable sim-to-real transfer in manipulation and locomotion tasks (as cited). Specific solver/config details are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Policies for grasping in clutter, quadruped locomotion, dexterous in-hand manipulation (as reported in cited works)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RL policies trained in Bullet simulation (details in cited works); typically deep neural network policies used for control.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Robotic manipulation and locomotion tasks where policies trained in simulation are transferred to real robots (sim-to-real), e.g., bin picking, quadruped locomotion, dexterous manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robots (sim-to-real)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>The paper states that cited works using Bullet were able to generate simulation data to train policies that worked in the real world; no numeric transfer metrics are reported in this paper (transfer performance details are in the cited references).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper does not discuss minimal fidelity for Bullet specifically; it only notes prior successful sim-to-real examples that used Bullet.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases for Bullet are reported in this paper; authors only cite Bullet-based sim-to-real successes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1539.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1539.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Roboschool / DeepMind Control Suite</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Roboschool and DeepMind Control Suite (simulation environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternative simulation environment suites referenced for locomotion tasks: Roboschool (OpenAI) and DeepMind Control Suite; used in prior RL benchmarks for locomotion and control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Roboschool; DeepMind Control Suite</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Environment suites built on physics engines (Roboschool historically wrapped alternatives to MuJoCo; DeepMind Control Suite provides standardized continuous-control tasks). They provide benchmark tasks (e.g., humanoid run) used by prior RL works.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (continuous control benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity depending on backend physics engine; used as standard RL benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provide observation spaces and task definitions; underlying physics fidelity depends on implementation (Roboschool and Control Suite differ in observation composition and contact modelling as noted in the paper's observation comparison table).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Humanoid (Roboschool/Control Suite variants), other benchmark agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Simulated benchmark agents/environments used in prior RL work; policies are typically neural-network controllers trained with RL.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Continuous-control locomotion and related benchmark tasks used for training and evaluating RL agents.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper provides a comparison table (Appendix B) enumerating differences in observation dimensionality across MuJoCo, Roboschool, Control Suite, and their own environments, highlighting implementation-driven differences rather than explicit fidelity ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure cases reported; paper notes that reward function and implementation differences across simulators affect comparability of results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>MuJoCo: A Physics Engine for Model-Based Control <em>(Rating: 2)</em></li>
                <li>Learning deep policies for robot bin picking by simulating robust grasping sequences <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation <em>(Rating: 2)</em></li>
                <li>Emergence of locomotion behaviours in rich environments <em>(Rating: 2)</em></li>
                <li>Implicit nonlinear complementarity: A new approach to contact dynamics <em>(Rating: 2)</em></li>
                <li>GPU-accelerated red blood cells simulations with transport dissipative particle dynamics <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1539",
    "paper_id": "paper-7b35984b9b0bde78290cf92d8176682b65e59f54",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "GPU-based physics engine (NVIDIA Flex / in-house)",
            "name_full": "NVIDIA Flex / in-house GPU-accelerated physics engine (used in this paper)",
            "brief_description": "A GPU-accelerated rigid-body physics simulator implemented with CUDA (referred to as NVIDIA Flex and an in-house GPU-based physics engine) used to concurrently simulate hundreds to thousands of robot agents for RL locomotion tasks; uses implicit time-stepping and a non-smooth Newton rigid-body solver.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "NVIDIA Flex / in-house GPU-based physics engine",
            "simulator_description": "GPU-accelerated rigid-body physics engine (CUDA) built with a non-smooth Newton solver and maximal-coordinate formulation, supporting torque control, speculative collision/contact detection, Coulomb friction, and implicit time-discretization for time-stepping; designed to simulate many agents concurrently on a single GPU.",
            "scientific_domain": "mechanics / robotics (rigid-body dynamics, contact mechanics)",
            "fidelity_level": "medium-to-high fidelity rigid-body dynamics with explicit contact modeling for locomotion; tuned to produce realistic humanoid gaits.",
            "fidelity_characteristics": "Non-smooth Newton rigid-body solver; maximal-coordinate formulation; unilateral constraint functions for contacts with smooth isotropic Coulomb friction (sliding friction coefficient = 1.0); restitution = 0.0; gravity = 9.8 m/s^2; implicit time-discretization; time step = 1/120 s; sparse iterative Krylov linear solver with minimal linear iterations chosen for stability; agents can interact (inter-agent collisions enabled).",
            "model_or_agent_name": "Ant; Humanoid (28-DoF); Humanoid Flagrun Harder (HFH); HFH on Complex Terrain",
            "model_description": "Deep RL agents trained with a variation of Proximal Policy Optimization (PPO); policy and value are feed-forward neural networks (SELU activations) sharing architecture; training runs synchronized distributed PPO variants with Horovod/NCCL for multi-GPU.",
            "reasoning_task": "Learning continuous-control locomotion behaviors: run forward, change direction toward targets, recover from falls (stand-up), and traverse uneven/complex terrains with a height map.",
            "training_performance": "Trained humanoid running task in &lt;20 minutes on a single machine (1 GPU + 1 CPU core); e.g., 16 minutes to run using 512 agents; multi-GPU HFH: 1 GPU reached reward 4000 in ~2 hours, 8 GPUs in ~30 minutes, 16 GPUs in ~15 minutes. (All values reported in paper.)",
            "transfer_target": "Potential sim-to-real transfer to real robots (paper discusses potential but does not perform transfer); cites prior sim-to-real successes using Bullet for real-world transfer.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper emphasizes solver choices that affect realism: Krylov iterative methods gave sufficient stiffness for realistic humanoid gaits while relaxation methods (e.g., Projected Gauss-Seidel) were less effective; uses contact modeling (Coulomb friction), implicit time-stepping, and a fine timestep (1/120 s). The authors do not present a formal minimum-fidelity study for transfer, but note these solver/parameter choices were important for achieving realistic locomotion.",
            "failure_cases": "Authors report that relaxation solvers (Projected Gauss-Seidel) were less effective at producing realistic humanoid gaits compared to Krylov-based solvers; they also caution that differences in physics simulators (solvers, iterations) make direct performance comparisons difficult. The paper does not report explicit sim-to-real failure cases for this engine.",
            "uuid": "e1539.0",
            "source_info": {
                "paper_title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo: A Physics Engine for Model-Based Control",
            "brief_description": "A widely used CPU-based physics simulator for continuous-control RL, known for efficient simulation and relatively stable contact models; used extensively in prior locomotion benchmarks and cited here for comparison.",
            "citation_title": "MuJoCo: A Physics Engine for Model-Based Control",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo",
            "simulator_description": "CPU-based rigid-body physics engine popular in RL research for continuous control benchmarks (e.g., humanoid and ant locomotion environments); provides efficient simulation and contact handling.",
            "scientific_domain": "mechanics / robotics (rigid-body dynamics, contact mechanics)",
            "fidelity_level": "high-fidelity rigid-body dynamics with stable contact models (commonly treated as the standard for RL locomotion benchmarks).",
            "fidelity_characteristics": "Efficient CPU simulation with stable contact modelling; used as the reference simulator for many continuous-control RL tasks; paper measures MuJoCo 1.5 single-core simulation times to compare CPU vs GPU scaling.",
            "model_or_agent_name": "Standard MuJoCo locomotion agents (e.g., Humanoid environments, Ant)",
            "model_description": "Simulated agents/environments used by prior RL works (not trained within this paper using MuJoCo, but MuJoCo timings were measured for comparison).",
            "reasoning_task": "Continuous-control locomotion tasks (benchmark environments for policy learning).",
            "training_performance": "Not directly trained in this paper; authors reference prior works that trained humanoid policies using MuJoCo (e.g., Salimans et al. trained MuJoCo humanoid with Evolution Strategies in ~10 minutes on large CPU clusters). Within this paper MuJoCo 1.5 single-core simulation times were measured and plotted (Appendix F).",
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes that simulation differences (solvers, number of iterations) across engines (MuJoCo vs GPU engine) make rewards and behaviors not directly comparable, but does not prescribe a minimal fidelity for transfer.",
            "failure_cases": "Paper does not report MuJoCo-specific transfer failures; it does report that MuJoCo single-core simulation time increases as number of concurrent humanoids increases (scaling limitation) compared to GPU simulator.",
            "uuid": "e1539.1",
            "source_info": {
                "paper_title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Bullet",
            "name_full": "Bullet Physics Library",
            "brief_description": "An open-source physics engine often used in robotics and sim-to-real work; cited here as the engine used by several recent successful sim-to-real studies that trained policies in simulation and deployed them on real robots.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Bullet",
            "simulator_description": "Open-source rigid-body physics engine used in robotics and manipulation research; commonly used to generate simulated data for training policies intended for real-world deployment.",
            "scientific_domain": "mechanics / robotics (rigid-body dynamics, contact mechanics)",
            "fidelity_level": "medium-fidelity rigid-body/contact dynamics suitable for policy training and sim-to-real pipelines (used in cited sim-to-real successes).",
            "fidelity_characteristics": "Approximates rigid-body contacts and friction; commonly used with domain randomization and other techniques to enable sim-to-real transfer in manipulation and locomotion tasks (as cited). Specific solver/config details are not provided in this paper.",
            "model_or_agent_name": "Policies for grasping in clutter, quadruped locomotion, dexterous in-hand manipulation (as reported in cited works)",
            "model_description": "RL policies trained in Bullet simulation (details in cited works); typically deep neural network policies used for control.",
            "reasoning_task": "Robotic manipulation and locomotion tasks where policies trained in simulation are transferred to real robots (sim-to-real), e.g., bin picking, quadruped locomotion, dexterous manipulation.",
            "training_performance": null,
            "transfer_target": "Real-world robots (sim-to-real)",
            "transfer_performance": "The paper states that cited works using Bullet were able to generate simulation data to train policies that worked in the real world; no numeric transfer metrics are reported in this paper (transfer performance details are in the cited references).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "The paper does not discuss minimal fidelity for Bullet specifically; it only notes prior successful sim-to-real examples that used Bullet.",
            "failure_cases": "No specific failure cases for Bullet are reported in this paper; authors only cite Bullet-based sim-to-real successes.",
            "uuid": "e1539.2",
            "source_info": {
                "paper_title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Roboschool / DeepMind Control Suite",
            "name_full": "Roboschool and DeepMind Control Suite (simulation environments)",
            "brief_description": "Alternative simulation environment suites referenced for locomotion tasks: Roboschool (OpenAI) and DeepMind Control Suite; used in prior RL benchmarks for locomotion and control.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Roboschool; DeepMind Control Suite",
            "simulator_description": "Environment suites built on physics engines (Roboschool historically wrapped alternatives to MuJoCo; DeepMind Control Suite provides standardized continuous-control tasks). They provide benchmark tasks (e.g., humanoid run) used by prior RL works.",
            "scientific_domain": "mechanics / robotics (continuous control benchmarks)",
            "fidelity_level": "medium-to-high fidelity depending on backend physics engine; used as standard RL benchmarks.",
            "fidelity_characteristics": "Provide observation spaces and task definitions; underlying physics fidelity depends on implementation (Roboschool and Control Suite differ in observation composition and contact modelling as noted in the paper's observation comparison table).",
            "model_or_agent_name": "Humanoid (Roboschool/Control Suite variants), other benchmark agents",
            "model_description": "Simulated benchmark agents/environments used in prior RL work; policies are typically neural-network controllers trained with RL.",
            "reasoning_task": "Continuous-control locomotion and related benchmark tasks used for training and evaluating RL agents.",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper provides a comparison table (Appendix B) enumerating differences in observation dimensionality across MuJoCo, Roboschool, Control Suite, and their own environments, highlighting implementation-driven differences rather than explicit fidelity ranking.",
            "failure_cases": "No specific failure cases reported; paper notes that reward function and implementation differences across simulators affect comparability of results.",
            "uuid": "e1539.3",
            "source_info": {
                "paper_title": "GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "MuJoCo: A Physics Engine for Model-Based Control",
            "rating": 2
        },
        {
            "paper_title": "Learning deep policies for robot bin picking by simulating robust grasping sequences",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation",
            "rating": 2
        },
        {
            "paper_title": "Emergence of locomotion behaviours in rich environments",
            "rating": 2
        },
        {
            "paper_title": "Implicit nonlinear complementarity: A new approach to contact dynamics",
            "rating": 2
        },
        {
            "paper_title": "GPU-accelerated red blood cells simulations with transport dissipative particle dynamics",
            "rating": 1
        }
    ],
    "cost": 0.015962749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GPU-Accelerated Robotic Simulation for Distributed Reinforcement Learning</h1>
<p>Jacky Liang ${ }^{\dagger \ddagger 1}$<br>Robotics Institute<br>Carnegie Mellon University<br>Nuttapong Chentanez ${ }^{2}$<br>NVIDIA</p>
<p>Viktor Makoviychuk ${ }^{\dagger 2}$<br>NVIDIA<br>Ankur Handa ${ }^{\dagger 2}$<br>NVIDIA<br>Dieter Fox ${ }^{2}$<br>NVIDIA<br>University of Copenhagen<br>${ }^{1}$ jackyliang@cmu.edu<br>${ }^{2}{$ vmakoviychuk, ahanda, nchentanez, mmacklin, dieterf}@nvidia.com</p>
<h4>Abstract</h4>
<p>Most Deep Reinforcement Learning (Deep RL) algorithms require a prohibitively large number of training samples for learning complex tasks. Many recent works on speeding up Deep RL have focused on distributed training and simulation. While distributed training is often done on the GPU, simulation is not. In this work, we propose using GPU-accelerated RL simulations as an alternative to CPU ones. Using NVIDIA Flex, a GPU-based physics engine, we show promising speed-ups of learning various continuous-control, locomotion tasks. With one GPU and CPU core, we are able to train the Humanoid running task in less than 20 minutes, using $10-1000 \times$ fewer CPU cores than previous works. We also demonstrate the scalability of our simulator to multi-GPU settings to train more challenging locomotion tasks.</p>
<p>Keywords: Deep Reinforcement Learning, GPU Acceleration, Simulation</p>
<h2>1 Introduction</h2>
<p>Model-free Deep RL has seen impressive achievements [1, 2, 3] in recent years, but many methods and tasks require enormous amount of compute due to the large sample complexity of exploration in high-dimensional state and action spaces. One approach to overcome exploration is by using human demonstrations [4, 5], but collecting human demonstrations remain challenging for many tasks, and it is difficult to scale. Another approach is to vastly scale up RL simulation and training to distributed settings, so large amounts of data can be obtained in a relatively short amount of time. Many recent works of this approach have seen scaling benefits by performing policy training on the GPU while scaling up environment simulation on many CPUs. In this work, we propose using a GPU-accelerated RL simulator to bring the benefits of GPU's parallelism to RL simulation as well.
Using Flex, a GPU-based physics engine developed with CUDA, we implement an OpenAI Gymlike interface to perform RL experiments for continuous control locomotion tasks. We benchmark our simulator on ant and humanoid running tasks as well as their more challenging variations, inspired by ones proposed in OpenAI Roboschool and the Deepmind Parkour environments. They include learning to run toward changing target locations, recovering from falls, and running on complex, uneven terrains. Our choice of tasks is driven by their popularity and the challenges they offer to various Deep RL algorithms. Although our training results are not directly comparable to those obtained in physics simulators used in prior work (e.g. MujoCo, Bullet) due to differences in physics simulation, we have endeavoured to do head-to-head comparisons wherever possible. Using</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GPU-Accelerated RL Simulation. We use an in-house GPU-accelerated physics simulator, to concurrently simulate hundreds to thousands of robots for Deep RL of continuous control locomotion tasks. Here we show the Ant, Humanoid, and Humanoid Flagrun Harder on Complex Terrain tasks benchmarked in our work. Using a single machine (1 GPU and CPU core), we are able to train humanoids to run in less than 20 minutes.</p>
<p>Our GPU-accelerated RL framework to simulate and train hundreds to thousands of agents at once on a single GPU, we were able to achieve faster training results than previous works which used large CPU clusters. In addition, the scale and speed-ups achieved through our simulator, especially in the more challenging tasks, make GPU-accelerated RL simulation a viable alternative to CPU ones.</p>
<p>We summarize our key contributions below:</p>
<ol>
<li>A GPU-accelerated RL simulator built with an in-house GPU-based physics engine. We plan to release our simulator in the near future to facilitate fast RL training to the community.</li>
<li>Experiments on massively distributed simulations of hundreds to thousands of locomotion environments on single and multiple GPU settings.</li>
<li>Improvements in training speed for various challenging locomotion tasks, learning the humanoid running task in less than 20 minutes on a single machine. See our trained policies at https://sites.google.com/view/accelerated-gpu-simulation/home.</li>
</ol>
<p>We note that in this paper our focus is on the application of our GPU-based physics engine to RL simulation, and not on comparisons and benchmarks of the physics engine itself.</p>
<h1>2 Related Works</h1>
<h3>2.1 Distributed Deep RL</h3>
<p>Many prior works have explored parallelizing environment simulation and policy training on CPUs. Nair et al. [6] proposed the first massively parallelized method for training RL agents. Their method, Gorila DQN, has separate learners, actors, and parameter servers. Simulating and training with hundreds of CPU cores it was able to achieve superhuman performance in most Atari games in a few days. Following Gorila DQN, Mnih et al. [1] proposed the Asynchronous-Actor-Critic-Agents (A3C) algorithm, and with 16 CPU cores they were able to compete with Gorila DQN in Atari games with about the same training time.</p>
<p>Babaeizadeh et al. [7] extended A3C by proposing a CPU/GPU hybrid variation of A3C, GA3C, which moves the policy to the GPU. This enabled GPU-accelerated policy inference and training. Their algorithm dynamically adjusts the number of parallel agents, trainers, and parameter servers to maximize training speed. On a single machine with 16 CPU cores and 1 Tesla K40 GPU GA3C achieved more than 4 speed-up over a CPU-only implementation of A3C. Adamski et al. [8] furthered scaled up A3C training by using larger batch sizes and a well-tuned Adam Optimizer. Their method allowed them to learn many Atari games with hundreds of CPU cores, with no GPU, in less than an hour (e.g., Breakout with 768 CPU cores in 21 minutes).</p>
<p>Salimans et al. [9] explored using evolutionary strategies (ES) for RL. The authors scaled up ES to use as much as 720 CPU cores to learn Atari games in about one hour. They also performed learning experiments with MuJoCo locomotion tasks, and with 1440 CPU cores they were able to train a humanoid to walk in 10 minutes. Such et al. [10] applied Genetic Algorithms (GA), a</p>
<p>gradient-free optimization method, to RL. GA is also amenable to parallelization, and it was able to learn Atari games also in one hour with 720 CPU cores. In humanoid locomotion tasks however, the algorithm trained much slower than ES, and was not able to achieve comparable performance in the same time frame.</p>
<p>Recent advances in parallel computation tools such as Horovod [11] and Ray have enabled researchers to easily scale up machine learning to distributed settings. Ray RLLib [12] is a distributed RL library using the Ray framework. In their benchmarks, the authors were able to scale ES with Ray RLLib to more than 8000 CPU cores, learning the humanoid walking task in just 3.7 minutes. Mania et al. [13] also used Ray but for their proposed algorithm, Augmented Random Search (ARS). ARS learned the humanoid walking task in 21 minutes with 48 CPU cores, while using $15 \times$ less CPU time than ES.</p>
<p>Other previous works aimed to improve the efficiency of off-policy learning from the large amount of data generated by many parallel actors. Espeholt et al. [14] proposed IMPALA, which applies large-scale, distributed learning systems to solving multi-task RL problems. IMPALA is inspired by A3C, but the actors don't compute and send policy gradients to the learners - they send the sampled trajectories instead. The authors scaled IMPALA to use 500 CPU actors and 8 GPU learners, and it learned the benchmarked tasks (DMLab-30, a suite of multi-task video game-like environments) in less than a day. Horgan et al. [15] introduced Distributed Prioritized Experience Replay, an offpolicy RL algorithm that uses a novel technique to sample more important trajectories in its replay buffer for learning. This work uses many CPU cores for simulating the RL environment, and 1 GPU for training. With 360 actors, the method learned most Atari games in a few hours, and with 32 actors it trained a humanoid to walk in 1 hour, run in 4 hours. Stooke and Abbeel [16] explored optimizing existing RL algorithms for fast performance on a multi-GPU system. The authors used an entire NVIDIA DGX-1, which contains 8 NVIDIA Tesla V100 GPUs. Running 256 parallel simulations on 40 CPU cores and performing training on all 8 V100s, the authors report being able to train many Atari games in less than 10 minutes. Recently, OpenAI massively scaled up Proximal Policy Optimization [3] to use more than 6000 CPU cores to simulate in-hand manipulation tasks [17] and more than 100,000 for playing Dota ${ }^{1}$.</p>
<h1>2.2 Locomotion and Physics Simulation</h1>
<p>We focus our attention to continuous control locomotion tasks first proposed in MuJoCo [18], a popular CPU-based physics simulator. DART and Bullet are other notable alternatives, but MuJoCo remains by far the most popular physics simulator in the Deep RL community [9, 10, 12, 13, 15, 19, 3, 20] for its efficient simulation and relatively stable contact models. Duan et al. [19] first benchmarked different RL algorithms on various continuous control tasks such as cartpole swing-up and humanoid walking forward. Later, Schulman et al. [3] introduced more complex locomotion tasks, such as humanoid flagrun where the agent must learn to change and run toward different directions. Heess et al. [20] take this one step further and train humanoid agents to walk and run on uneven and dynamic terrains. Taking inspiration from these works, we use the humanoid running task and its more challenging variations for benchmarking. Owing to the humanoid's high degree of freedom control space, its tasks require most Deep RL algorithms to use a significant number of samples to learn, which provide opportunities for improving learning speed via reduction in simulation time.
Many previous works on distributed RL have focused on discrete control problems such as Atari games, which do not require physics simulation. Moreover, the works in continuous control tasks have only used CPU-based simulations. While GPU-accelerated physics simulations have been applied in scientific computing [21, 22] and healthcare [23, 24], they have yet to be applied in robotics. To achieve state-of-the-art performance, previous works often had to scale environment simulation to hundreds, if not thousands of CPU cores. In our work, we explore using GPU-accelerated simulation as an alternative to CPU-based ones. Using a single GPU, we can simulate hundreds to thousands of robots and achieve state-of-the-art results in locomotion tasks on a single machine, learning the humanoid running task in less than 20 minutes.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: GPU Simulation Speed. We measure the speed of GPU simulation for the humanoid task as we increase the number of concurrent humanoids simulated. The total simulations per second peaks at around 60 KHz for 750 humanoids, and the best mean GPU simulation frame time per agent is less than 0.02 ms . The simulation time grows much slower than the number of humanoids because of the constant CUDA kernels launch overhead, which dominates in total step time when only a few humanoids are available.</p>
<h1>3 GPU-Accelerated Robotics Simulation</h1>
<h3>3.1 GPU-based Physics Engine</h3>
<p>Our in-house GPU-based physics engine uses a non-smooth Newton method for its rigid body solver and a maximal coordinate formulation. Like with environments in MuJoCo and Bullet, we use torque control as the actuation model. Potential collisions and contacts among bodies are detected speculatively and are modeled through unilateral constraint functions with a smooth isotropic Coulomb friction model. We use sliding friction coefficient of 1.0, the same as in MuJoCo [25]. Restitution coefficient is 0.0 and gravity is $9.8 \frac{\mathrm{m}}{\mathrm{s}^{2}}$ downward. For time-stepping we use an implicit time-discretization also like [25], and the time step used is $\frac{1}{120} \mathrm{~s}$. Each Newton iteration is solved by a sparse iterative Krylov linear solver with the minimum number of linear iterations such that simulation is stable for our experiments. We found Krylov methods allowed sufficient stiffness to achieve realistic humanoid gaits, while relaxation methods like Projected Gauss-Seidel were less effective, especially when paired with a maximal coordinate representation.
We develop a GPU-accelerated robotics simulation framework for RL that can simulate many robot agents in parallel for a variety of tasks. To simulate multiple robots performing the same task in parallel, we load all robots and task-related objects into the same simulation. This is unlike previous works that parallelize simulation by using multiple CPU processes or threads, each simulating an individual environment. The parallelism and the speed-up in our simulation is achieved by performing the physics computations on the GPU. We note that in our simulations, agents are able to interact with each other, which is not possible for running multiple simulation processes of 1 agent each.</p>
<h3>3.2 GPU Simulation Speed</h3>
<p>To illustrate the typical performance of our simulator on a single-machine setting, we measured the GPU simulation frame time for the humanoid task as we increase the number of humanoids concurrently simulated in the environment. The results are obtained on an NVIDIA Tesla V100 GPU. The GPU simulation frame time does not include the time needed for calculating rewards, applying actions, and transfer RL-related data back and forth from the Python client, as that speed varies based on implementation of the RL framework. In Figure 2 we report two plots, the total simulation frames generated per second, calculated by multiplying the number of agents by the frames per second of the entire simulation, and the GPU frame time per agent. We note that both values converge with around 750 simulated humanoids, where it can generate 60 K frames per second, and the mean frame time per agent is below 0.02 ms .
We observe in our learning experiments that although the total simulation frames generated per second peaks around 750 agents, this is not the optimal number of agents for minimizing learning speed. The number of agents used affects how the learning algorithm and policy explores the state and action spaces, and the threshold for optimizing learning speed depends on the specific task.</p>
<p>4 Experiments</p>
<p>To evaluate the performance of our GPU-accelerated physics simulator for RL, we perform a set of learning experiments on various locomotion tasks. We first measure the learning performance on a single GPU as we vary the number of parallel agents simulated on that GPU. Then we measure how well learning performance scales with multiple GPUs and nodes as we fix the number of agents simulated per GPU.</p>
<h3>4.1 Tasks</h3>
<p>We perform learning experiments on the following 4 tasks, 3 of which are shown in Figure 1.</p>
<p>Ant. We use the ant model commonly found in MuJoCo locomotion environments. It has 4 legs and 8 controllable joints that form the action space. The goal of the Ant task is to have the agent move forward as fast as possible. Ant is relatively easy to learn, because the initial state of the task is stable. This makes it a useful task for sanity checks and debugging.</p>
<p>Humanoid. Like [20, 26], we use the humanoid model with 28 degrees of freedom and 21 actuated joints. This humanoid is more complex than the 24-DoF humanoid model with 17 actuated joints used in [3, 9, 12, 13]. The 28-DoF humanoid has 4 additional joints to control the ankle angles of the robot, whereas the 24-DoF one has ball-shaped feet that cannot be rotated. We choose the more complex humanoid for benchmarking, because the additional ankle joints allow the humanoid to learn more realistic running and turning motions. The goal of the Humanoid task is to have the agent move forward as fast as possible. This task is often used in the literature for benchmarking locomotion learning performance.</p>
<p>The observations for Ant and Humanoid include the agents height, velocity, and joint angles, among others. See Appendix B for a detailed comparison of observations used in our and previous works.</p>
<p>Humanoid Flagrun Harder (HFH). In the HFH task, a humanoid must learn to not only run forward but also to turn and run toward different target locations. The agent must also learn how to recover from falling by standing up. This is a much more challenging task than vanilla Humanoid, and it takes more time to train. The action space of this task is the same as that in the Humanoid task. We observe that training a humanoid for HFH leads to more robust and symmetric walking gaits e.g. humanoids can maintain their stand-up and running skills even with as much as 50% higher or lower gravity.</p>
<p>Humanoid Flagrun Harder on Complex Terrain. In this task, the agent must learn to run and change directions on uneven terrain with static rectangular obstacles. The dimensions, location, and orientation of the obstacles are randomly sampled from a uniform distribution. The action space of this task is the same as that in the Humanoid task. To help the humanoid navigate complex terrain and overcome obstacles, we augment the observation space with a 2D, $15 \times 11$ rectangular height map that follows the humanoids center of mass. Similar to [20], our height map is denser near the humanoid.</p>
<p>For Ant and Humanoid, an episode terminates if the agents fall below a threshold. For the two HFH tasks, we allow the agents to fall below a threshold for a certain time period (160 frames) before terminating the agent. This enables the agents to learn to stand up and recover from a fall. The Flagrun targets for the HFH tasks change every 200 frames to a random location within 100m of the agent, or earlier if the humanoid reaches within 1m of the target. For all tasks, the maximum episode length for both training and evaluation is 1000 frames.</p>
<p>Rewards. Similar to previous works, the reward function for all tasks reward the current speed toward the desired targets and penalize excessive torque applied to the joints. Our reward functions however, are not immediately comparable to previous works, due to a smaller alive bonus and the addition of other terms that we empirically found to lead to more natural, symmetric running gaits. We note that the locomotion rewards used in MuJoCo and Bullet are also different, arising from the vagaries of implementation details, such as the solver and the number of iterations used for optimisation. See Appendix C, D for the exact rewards used and a comparison of our rewards with those used in previous work.</p>
<p>A common reward threshold for solving the humanoid running forward task is 6000 [9, 12, 13], but this threshold is for the 24-DoF humanoid, and to our knowledge there is no widely used reward</p>
<p>threshold for the 28-DoF humanoid and for the HFH tasks. For the Humanoid task, we chose a reward threshold of 3000 for walking and 5000 for running. 3000 roughly corresponds to a forward moving speed of $2 \mathrm{~m} / \mathrm{s}$, and 5000 for $4 \mathrm{~m} / \mathrm{s}$, which is about the same speed as Roboschool's example 24-DoF humanoid running policy. For Ant, we use 3000 as the reward threshold for running, and 7000 for final reward. For the HFH task, we use 2500 as an intermediary reward threshold, around which the agents first learn to stand up from the ground, and 4000 as the final reward.</p>
<p>Initial State and Perturbations. Unlike parallel simulations on CPUs that simulate multiple agents in their own environments, usually one environment per CPU core, we simulate all agents in one environment on the GPU. The initial positions, velocities, joint angles, and joint angular velocities of agents are perturbed slightly during initialization. We also exert random forces onto the agents for all 4 tasks every 200 to 300 frames, for a few Newtons each time. These external perturbations help the agent to learn more robust policies. We also enabled inter-agent collisions for the HFH tasks. The initial spacing of the humanoids affect the collision frequency, and the occasional collisions help the agents to explore states where they must learn to balance and recover from falls, leading to more robust policies.</p>
<h1>4.2 Learning Algorithm</h1>
<p>We use a variation of Proximal Policy Optimization (PPO) [3] for all our experiments to benchmarks locomotion tasks. We adapted the open source OpenAI Baseline ${ }^{2}$ implementation of PPO to work with our simulation environment, where a single environment step simulates multiple agents concurrently. Similar to [3, 20], we also use an adaptive-learning rate based on the KL divergence between the current and previous policies. Additionally, for stability we whiten the current observations by maintaining online statistics of mean and standard deviation from the history of past observations.</p>
<p>Our policy and value functions share the same feed-forward network architectures. As in the Baselines implementation of PPO, we use scaled exponential linear units (SELU [27]) for the activation function. SELU implicitly encourages normalized activations, and in our hyperparameter search, policies with SELU learned faster and achieved higher rewards than those with ReLU and Tanh.</p>
<p>For our multi-GPU benchmarks, we implemented two variants of Distributed PPO. Our first variant uses multiple GPUs to generate rollouts but trains the policy and value function on a single GPU. This is often the case with CPU based implementations, where each CPU worker generates rollouts, and one master CPU trains. Our second variant is similar to Distributed PPO [20]. Both are synchronized algorithms, where at every iteration, gradients from all workers are applied to a central policy, and the worker policies are updated with the new policy parameters and this is what we use for all our experiments. We found that the first variant was not scalable to multiple nodes. We also experimented with averaging parameters, but we found that it performed significantly worse than averaging gradients. We use Horovod ${ }^{3}$ for distributed simulation and training across multiple GPUs and nodes where each GPU runs its own simulation and training instance in Tensorflow. The weight parameters are updated by averaging gradients across multiple GPU workers using efficient allreduce by NCCL ${ }^{4}$ and broadcasting from a master GPU. Importantly, since the env.step() function is implemented directly on GPU for multiple agents, we are able to leverage the parallelism offered by GPUs to obtain observation-action pairs concurrently for hundreds to thousands of agents.</p>
<h3>4.3 Hardware</h3>
<p>All experiments were done using NVIDIA Tesla V100 GPUs on NVIDIA's internal compute cluster. Each single-GPU experiment uses 1 CPU core of a 20-Core Intel Xeon E5-2698 v4 processor running at 2.2 GHz . For multi-GPU experiments, we scale the number of CPU cores used to match the number of GPUs used.</p>
<h3>4.4 Single-GPU Simulation and Training</h3>
<p>We first performed hyperparameter grid-search on the number of frames used per PPO optimization iteration and the network architectures for training 1024 parallel agents on the Ant and Humanoid</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>tasks. For both tasks, we found the best policy and value network architectures to have 3 hidden layers decreasing in size. See Appendix E for the specific architectures and hyperparameters used.</p>
<p>We found the best frames per iteration within our searched values for Ant and Humanoid to be 32 frames per iteration $\times 1024$ agents $=32768$ frames. This number is kept constant as we scale the number of parallel agents simulated up and down. For example, for 512 agents, we use 64 frames per iteration, and for 2048 we use 16. Keeping the frames per iteration constant helps us to show differences in learning speed as caused by improvements in simulation speed, and not by performance of the learning algorithm. We note that for high agent counts, the small number of frames used still enable learning, because our tasks use dense rewards.</p>
<p>We report the time needed to reach certain reward thresholds for the Ant, Humanoid, and HFH tasks as we vary the number of agents in Figure 3. Because we fix the amount of experience used per PPO update constant, we are able to observe a trade-off between increasing the number of agents but collecting less frames per agent and decreasing the number of agents but collecting more frames per agent. The point of diminishing return varies across task and reward thresholds.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Single GPU Experiments. We show the reward curves and wall time needed for training various tasks using increasing numbers of simulated agents to reach certain reward thresholds. The number of agents we evaluated vary in the powers of 2. The reward thresholds were chosen for significant behavior changes. For example - at around 2500 reward the Humanoid Flagrun Harder agents learn to stand up from sitting positions and begin walking. We keep the amount of experience used per PPO update iteration constant across evaluations by decreasing the frames used per agent as the number of agents increase. Using 512 agents we were able to train the Humanoid agent to run in about 16 minutes. All experiments were running against the same set of seeds for consistent comparison.</p>
<p>We also note the short time needed to learn these tasks using GPU-accelerated simulations. We list training times and resources used for the humanoid task in prior works in Table 1, all of which used CPU-based physics simulation. With one GPU and CPU core, the Humanoid agents were able to run in less than 20 minutes while using 10 to a 1000 less CPU cores than previous works [13, 9].</p>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>CPU Cores</th>
<th>GPUs</th>
<th>Time (mins)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Evolution Strategies [9]</td>
<td>1440</td>
<td>-</td>
<td>10</td>
</tr>
<tr>
<td>Augmented Random Search [13]</td>
<td>48</td>
<td>-</td>
<td>21</td>
</tr>
<tr>
<td>Distributed Prioritized Experience Replay [15]</td>
<td>32</td>
<td>1</td>
<td>240</td>
</tr>
<tr>
<td>Proximal Policy Optimization w/ GPU Simulation (Ours)</td>
<td>1</td>
<td>1</td>
<td>16</td>
</tr>
</tbody>
</table>
<p>Table 1: Resources and Times for Training a Humanoid to Run. Prior works all used CPU-based physics simulations. In this table, we do not include the original PPO paper [3] - it used 128 CPU cores for the humanoid task but did not report training time. We also did not include the Distributed PPO paper [20] - their humanoid training took more than 40 hours to converge, but they did not report the number of CPU cores used.</p>
<p>4.5 Multi-GPU Distributed Simulation and Training</p>
<p>We extend our method to distribute GPU simulations across multiple GPU workers to see how learning speed can be improved on the Humanoid, HFH, and HFH on Complex Terrain tasks. For these experiments, we run a simulation and training instance on each GPU, and we use Horovod for distributed gradients averaging. We also normalize the advantage estimates across all GPUs and distribute them back to each GPU worker at every iteration, ensuring that advantages across all GPUs share a consistent global mean and standard deviation. The number of agents simulated per GPU for Humanoid and HFH is 1024. We use a smaller number of 512 agents per GPU for HFH on Complex Terrain to keep memory usage and simulation speed reasonable, as the addition of the height map significantly increases the dimensionality of the observations. Results are reported in Figure 4. We observe only limited scaling effects for the Humanoid task, which hit diminishing returns after 4 GPUs. In the more complex tasks however, we observed noticeable speed-ups. For the HFH task, the 1 GPU run reached 4000 rewards in about 2 hours, while the 8 GPU run reached it in about 30 minutes, and 16 GPUs in about 15. For the HFH on Complex Terrain task, we observe more apparent scaling benefits with multiple GPU simulation and training. On average, the 16 and 32 GPU runs learn the task faster than the 2, 4, and 8 GPU runs, while the large overlap in standard deviations for 8 GPUs with 16 and 32 shows the diminishing returns of using more agents in learning.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Multi-GPU Simulation and Training. We show how our GPU-accelerated RL simulation can be scaled to simulation and training with multiple GPUs. The overlap of standard deviations indicates that there are little scaling effects for the Humanoid task, and the benefit of multi-GPU simulation and training is only apparent for more complex tasks and with a greater difference in the number of GPUs used. All experiments were running against the same set of seeds for consistent comparison.</p>
<h2>5 Conclusion and Future Work</h2>
<p>In this work, we used an in-house GPU-accelerated physics simulator to concurrently simulate hundreds to thousands of robots for Deep RL of continuous-control, locomotion tasks. In contrast to prior works that trained locomotion tasks on CPU clusters, with some using hundreds to thousands of CPU cores, we are able to train a humanoid to run in less than 20 minutes on a single machine with 1 GPU and CPU core, making GPU-accelerated RL simulation a viable alternative to CPU-based ones. Our RL simulation framework can also be scaled to multi-GPU and multi-node settings, and we observed that multi-GPU simulation and training shows greater learning speed improvements for more complex locomotion tasks. Given the recent successes of sim2real transfer learning, from grasping in clutter [28], quadruped locomotion [29], to dexterous manipulation [30], all of which used Bullet to generate simulation data to train policies that worked in the real world, we believe our simulator can provide valuable speed-ups for similar applications in the future.</p>
<p>In future work, we plan to experiment with more complex humanoid environments by allowing the humanoid to actively control the orientation of the rays used to generate the height map. This may enable the humanoids to navigate dynamic obstacles and obstacles in mid-air. We also plan to use our simulator for manipulation tasks with robots such as the Fetch, Baxter, and YuMi. In this work, we've considered locomotion tasks with full state information. For many tasks in manipulation and navigation however, training from vision data is preferred. For such tasks, we note the potential of zero-copy training - directly feeding simulation data generated by a GPU-based simulator and the task's states and rewards into a deep learning framework without the data leaving the GPU. Zero-copy training eliminates the need to communicate data from the GPU to the CPU, and can further improve training speed.</p>
<h1>Acknowledgments</h1>
<p>We thank Phil Rogers, Vikrama Ditya, Christopher Lamb, Nathan Luehr, David Addison, Hari Sundararajan, Sivakumar Arayandi Thottakara, Julie Bernauer and many others who manage the NVIDIA GPU infrastructure for all the kind help they provided in carrying out the experiments on the GPU clusters.</p>
<h2>References</h2>
<p>[1] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.
[2] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of go with deep neural networks and tree search. Nature, 529: 484-503, 2016. URL http://www.nature.com/nature/journal/v529/n7587/full/ nature16961.html.
[3] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[4] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, A. Sendonaris, G. DulacArnold, I. Osband, J. Agapiou, J. Z. Leibo, and A. Gruslys. Learning from demonstrations for real world reinforcement learning. CoRR, abs/1704.03732, 2017.
[5] A. Rajeswaran, V. Kumar, A. Gupta, J. Schulman, E. Todorov, and S. Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. CoRR, abs/1709.10087, 2017.
[6] A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. De Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, et al. Massively parallel methods for deep reinforcement learning. arXiv preprint arXiv:1507.04296, 2015.
[7] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz. Reinforcement learning through asynchronous advantage actor-critic on a gpu. arXiv preprint arXiv:1611.06256, 2016.
[8] I. Adamski, R. Adamski, T. Grel, A. Jedrych, K. Kaczmarek, and H. Michalewski. Distributed deep reinforcement learning: Learn how to play atari games in 21 minutes. arXiv preprint arXiv:1801.02852, 2018.
[9] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
[10] F. P. Such, V. Madhavan, E. Conti, J. Lehman, K. O. Stanley, and J. Clune. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning. arXiv preprint arXiv:1712.06567, 2017.
[11] A. Sergeev and M. Del Balso. Horovod: fast and easy distributed deep learning in tensorflow. arXiv preprint arXiv:1802.05799, 2018.
[12] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, J. Gonzalez, K. Goldberg, and I. Stoica. Ray rllib: A composable and scalable reinforcement learning library. arXiv preprint arXiv:1712.09381, 2017.
[13] H. Mania, A. Guy, and B. Recht. Simple random search provides a competitive approach to reinforcement learning. arXiv preprint arXiv:1803.07055, 2018.
[14] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.</p>
<p>[15] D. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hessel, H. van Hasselt, and D. Silver. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933, 2018.
[16] A. Stooke and P. Abbeel. Accelerated methods for deep reinforcement learning. arXiv preprint arXiv:1803.02811, 2018.
[17] OpenAI. Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2017.
[18] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033, Oct 2012. doi:10.1109/IROS.2012.6386109.
[19] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. In International Conference on Machine Learning, pages $1329-1338,2016$.
[20] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017.
[21] C. Freniere, A. Pathak, M. Raessi, and G. Khanna. The feasibility of amazon's cloud computing platform for parallel, gpu-accelerated, multiphase-flow simulations. Computing in Science \&amp; Engineering, 18(5):68-77, 2016.
[22] J. Spiechowicz, M. Kostur, and L. Machura. Gpu accelerated monte carlo simulation of brownian motors dynamics with cuda. Computer Physics Communications, 191:140-149, 2015.
[23] A. L. Blumers, Y.-H. Tang, Z. Li, X. Li, and G. E. Karniadakis. Gpu-accelerated red blood cells simulations with transport dissipative particle dynamics. Computer physics communications, 217:171-179, 2017.
[24] J. Wu, C. K. Chui, and C. L. Teo. A software component approach for gpu accelerated physicsbased blood flow simulation. In Systems, Man, and Cybernetics (SMC), 2015 IEEE International Conference on, pages 2465-2470. IEEE, 2015.
[25] E. Todorov. Implicit nonlinear complementarity: A new approach to contact dynamics. In Robotics and Automation (ICRA), 2010 IEEE International Conference on, pages 2322-2329. IEEE, 2010.
[26] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.
[27] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter. Self-normalizing neural networks. In Advances in Neural Information Processing Systems, pages 972-981, 2017.
[28] J. Mahler and K. Goldberg. Learning deep policies for robot bin picking by simulating robust grasping sequences. In Conference on Robot Learning, pages 515-524, 2017.
[29] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018.
[30] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation, 2018.</p>
<h1>A Rewards vs Frames</h1>
<p>We plot the reward vs frames curves for single-GPU experiments in Figure 5 and multi-GPU experiments in Figure 6. A zoomed-in version of each plot is shown on the second row. The difference in the number of total frames for different number of agents is due to the fact that we stop training based on a fixed amount of time.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Reward vs Frames for Single GPU Experiments.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Reward vs Frames for Multi-GPU Experiments.</p>
<h1>B Comparison of Observations</h1>
<p>Table 2 compares the different observations for the Humanoid running task used in MuJoCo, Roboschool, Control Suite, and our environments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MuJoCo ${ }^{5}$</th>
<th style="text-align: left;">Roboschool ${ }^{6}$</th>
<th style="text-align: left;">Control Suite ${ }^{7}$</th>
<th style="text-align: left;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Root Body Height</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Root Body Rotation</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Root Body Velocity</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">Root Body Angular Velocity</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">Root Body Heading Direction</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Joint Angles</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">21</td>
<td style="text-align: left;">21</td>
</tr>
<tr>
<td style="text-align: left;">Joint Angle Velocities</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">17</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">21</td>
</tr>
<tr>
<td style="text-align: left;">Positions of Hands and Feet</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$4 \times 3$</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Velocities of Bodies</td>
<td style="text-align: left;">$14 \times 3$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">27</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Angular Velocities of Bodies</td>
<td style="text-align: left;">$14 \times 3$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Inertia Tensor and Mass</td>
<td style="text-align: left;">$14 \times 10$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Actuator Forces</td>
<td style="text-align: left;">23</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">21</td>
</tr>
<tr>
<td style="text-align: left;">External Forces on Bodies</td>
<td style="text-align: left;">$14 \times 6$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
<tr>
<td style="text-align: left;">Whether Feet in Contact w/ Ground</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">376</td>
<td style="text-align: left;">44</td>
<td style="text-align: left;">67</td>
<td style="text-align: left;">76</td>
</tr>
</tbody>
</table>
<p>Table 2: Observation and dimensionality comparison for Humanoid running task. The Root Body Heading Direction for Roboschool and our agents are represented by the sine and cosine values of the angular difference between our agent's heading and the direction to the target (which for the humanoid running task is just forwards from the starting location). The Root Body Rotation is represented as quaternions for MuJoCo, roll and pitch angles for Roboschool and ours, and the z-projection of the rotation matrix for Control Suite.</p>
<h2>C Rewards</h2>
<p>The reward function we used for all four tasks are as follows:</p>
<p>$$
\begin{gathered}
R=R_{\text {alive }}+S+0.5 R_{\text {heading }}+0.05 R_{\text {standing }}-4 \frac{1}{\tau_{\max }}||\tau||<em 2="2">{1}-0.5||u||</em> \
R_{\text {heading }}= \begin{cases}1 &amp; \cos \left(\theta_{\text {target }}\right)&gt;0.8 \
\cos \left(\theta_{\text {target }}\right) / 0.8 &amp; \cos \left(\theta_{\text {target }}\right)&lt;=0.8\end{cases} \
R_{\text {standing }}=\mathbb{1}{\cos \left(\theta_{\text {vertical }}\right)&gt;0.93}
\end{gathered}
$$}^{2}-0.2 N_{\text {joints }}-N_{\text {feet }</p>
<p>$\theta_{\text {target }}$ is the angle from the robot's current heading to the angle toward the target location. $\theta_{\text {vertical }}$ is the angle of the robot's torso from the vertical-axis (i.e. if the humanoid is standing up right, this angle would be 0 ). $R_{\text {alive }}$ is the alive bonus, and it is 0.5 for Ant and 2 for humanoids. $S$ is the speed toward the current target. $\tau$ is the vector of motor torques applied at each joint, with $\tau_{\max }$ being the maximum that can be applied. $u$ is the current action. $N_{\text {joints }}$ is the number of joints at joint limits, and $N_{\text {feet }}$ is the number of feet that is in collision with ground.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>D Comparison of Reward Functions</h1>
<p>Table 3 compares the coefficients for the summands in the reward function for the Humanoid running task used in MuJoCo, Roboschool, and our environments.</p>
<p>We don't list the reward function coefficients for the humanoid walking task in Deepmind Control Suite, as it is very different in structure - it is the product of the running speed with two coefficients that depend on the magnitude of the controls and how upright the humanoid is. See here ${ }^{8}$ for details.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MuJoCo $^{9}$</th>
<th style="text-align: left;">Roboschool ${ }^{10}$</th>
<th style="text-align: left;">Ours</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alive Bonus</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Running Speed Bonus</td>
<td style="text-align: left;">0.25</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Heading Bonus</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Standing Bonus</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Control Cost</td>
<td style="text-align: left;">-0.1</td>
<td style="text-align: left;">$-\frac{0.1}{N_{d}}$</td>
<td style="text-align: left;">-0.5</td>
</tr>
<tr>
<td style="text-align: left;">Electricity (Torque) Cost</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-4.25</td>
<td style="text-align: left;">$-\frac{4}{-0.5}$</td>
</tr>
<tr>
<td style="text-align: left;">Joints at Limits Cost</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-0.2</td>
<td style="text-align: left;">-0.2</td>
</tr>
<tr>
<td style="text-align: left;">Feet Contact Cost</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-1</td>
<td style="text-align: left;">-1</td>
</tr>
<tr>
<td style="text-align: left;">External Forces Cost</td>
<td style="text-align: left;">$-5 \times 10^{-6}$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Reward function comparison for Humanoid Running task. $N_{d}$ is the dimension of the controls. The External Forces Cost for MuJoCo is the multiplier of the sum of squares of external forces on all bodies.</p>
<h2>E Hyperparameters</h2>
<p>In table 4 we give the hyperparameters used during training for Ant, Humanoid, and Humanoid Flagrun Harder tasks. The timesteps per batch is given relative to a specific amount of parallel agents simulated - 1024, and this is scaled across different experiments. For example, if the timesteps per batch is 32 , then we used 64 with the experiment that has 512 agents, and 16 with the experiment that has 2048 agents. The desired KL specifies the target KL value used for adapting the Adam step size at each iteration.</p>
<p>The value and policy networks in our PPO algorithm have the same feed-forward architectures, and they are specified in a list format, where the $n$th value gives the size of the $n$th layer.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: left;">Ant</th>
<th style="text-align: left;">Humanoid</th>
<th style="text-align: left;">HFH</th>
<th style="text-align: left;">HFH Terrain</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Timesteps per Batch</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">64</td>
<td style="text-align: left;">64</td>
</tr>
<tr>
<td style="text-align: left;">Num. Epochs</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Minibatch Size per Agent</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">32</td>
<td style="text-align: left;">8</td>
</tr>
<tr>
<td style="text-align: left;">Desired KL</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">0.01</td>
<td style="text-align: left;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Neural Net Architecture</td>
<td style="text-align: left;">$[128,64,32]$</td>
<td style="text-align: left;">$[256,128,64]$</td>
<td style="text-align: left;">$[512,256,128]$</td>
<td style="text-align: left;">See Caption</td>
</tr>
</tbody>
</table>
<p>Table 4: Hyperparameters used in different tasks. For the HFH Terrain neural network, we pass the $15 \times 11$ height map into two fully-connected (FC) layers of sizes $[256,128]$, the other observations through one layer of size 512, then finally pass their concatenated outputs through two more FC layers of sizes $[256,128]$ before outputting the controls.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>F MuJoCo Simulation Times</h1>
<p>In Figure 7 we report plots similar to those in Figure 2 where we evaluate the total frames per second and frame time per agent on MuJoCo 1.5 as the number of concurrent humanoids simulated in the scene increases. We measured MuJoCo's single-core CPU simulation time in a similar setup as we did with our GPU simulation time - at every time step random actions are given to the 28 -DoF humanoids lying on the floor. The CPU used is an Intel Core i9-7960X running at 2.80 GHz . At the time of writing MuJoCo 2.0 has just been released, but it is not yet supported by mujoco.py. We plot the projected curves for MuJoCo 2.0 by reducing the simulation time of MuJoCo 1.5 by $40 \%$, as reported here ${ }^{11}$.
We note that while MuJoCo performs well on one core with one humanoid, the simulation time increases as the number of humanoids in the scene increases. This is in contrast to GPU simulations, where having more concurrent simulations with more contacts, which enable large-scale inter-agent interactions, reduces the simulation time per agent.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: MuJoCo Simulation Speed.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{11}$ https://www.roboti.us/index.html#mujoco200&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
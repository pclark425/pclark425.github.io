<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6866 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6866</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6866</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-271516332</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18897v1.pdf" target="_blank">Small Molecule Optimization with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent advancements in large language models have opened new possibilities for generative molecular drug design. We present Chemlactica and Chemma, two language models fine-tuned on a novel corpus of 110M molecules with computed properties, totaling 40B tokens. These models demonstrate strong performance in generating molecules with specified properties and predicting new molecular characteristics from limited samples. We introduce a novel optimization algorithm that leverages our language models to optimize molecules for arbitrary properties given limited access to a black box oracle. Our approach combines ideas from genetic algorithms, rejection sampling, and prompt optimization. It achieves state-of-the-art performance on multiple molecular optimization benchmarks, including an 8% improvement on Practical Molecular Optimization compared to previous methods. We publicly release the training corpus, the language models and the optimization algorithm.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6866.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6866.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemlactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemlactica (125M parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 125M-parameter decoder-only language model fine-tuned on a JSONL corpus of ~110M PubChem molecules and computed properties, used for SMILES-conditioned property prediction and conditional SMILES generation and deployed as the generator in a population-based molecular optimization loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemlactica-125M</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine-tuned model (causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>125M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining / fine-tuning on a novel JSONL corpus derived from PubChem: ~110M molecules, computed molecular properties (QED, SAS, MW, TPSA, CLogP, H-bond donors/acceptors, ring counts), similarity pairs (sampled from related-molecule pairs recalculated with ECFC4), totaling ~40B tokens in the corpus generation process; SMILES canonicalized with RDKit and encoded with tag-based template tokens (e.g. [QED]value[/QED], [START_SMILES]).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct conditional SMILES generation via property-tagged prompts and special tokens; techniques used during sampling include vanilla temperature sampling (dynamic temperature schedule from 1 to 1.5), repetition penalty, undesired-token suppression to force generation of [START_SMILES], and a chain-of-thought trick (omitting [START_SMILES] in prompt to encourage intermediate property generation). Also used in-loop fine-tuning on high-performing molecules (explicit oracle modeling) and as the generator in an LLM-enhanced population-based optimizer (replacing classic crossover/mutation).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES (canonicalized via RDKit) with additional property and tag tokens in JSONL text format</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Small-molecule drug discovery and molecular optimization tasks (Practical Molecular Optimization (PMO) benchmark tasks, multi-property docking optimization against DRD2, MK2, AChE, QED maximization with similarity constraints, ADMET/property prediction tasks and MoleculeNet regression tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Task-specific constraints encoded in prompts or oracle: QED thresholds (e.g., target QED ≥ 0.9), similarity constraints (e.g., sim ≥ 0.4 or specified values), molecular weight limits (≤ 500 Da in docking tasks), composite oracle reward functions combining docking score, QED and MW; note: synthetic accessibility was measured (SAS) in training data but the optimization algorithm does not explicitly enforce synthetic accessibility or retrosynthetic feasibility as a general constraint.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used for SMILES parsing, canonicalization and computing computed properties (QED, SAS, TPSA, CLogP, MW); ECFC4 fingerprints for similarity recalculation; docking experiments used the conformer generation + docking pipeline and aggregate reward function adopted from the Beam Enumeration implementation (conformational generation and docking software / docking oracle described in Beam Enumeration paper); PyTorch ecosystem (FSDP) and FlashAttention used for model training/serving.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem-derived JSONL corpus (~110M molecules), PMO benchmark (23 tasks from Gao et al. 2022) for optimization evaluation, MoleculeNet tasks and ADMET datasets (Fang et al.) for property prediction/fine-tuning, docking target protein structures for docking benchmarks (DRD2, MK2, AChE).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Property prediction: RMSE, corrected RMSE (invalid SMILES replaced by dataset mean), MAPE, Pearson correlation; Calibration: binned accuracy vs predicted probability; Conditional generation: RMSE between target and generated property values; Optimization benchmarks: AUC Top-10 (Top-10 average property vs number of oracle calls) for PMO, generative yield (number of unique molecules above reward threshold per fixed oracle calls), oracle burden (number of oracle calls needed to generate N unique molecules above threshold), success rate for similarity-constrained QED optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Property prediction: QED RMSE 0.016 (Chemlactica-125M, Table 1); Conditional generation: property-conditioned RMSEs reported per property (see Table 1). PMO optimization: AUC Top-10 = 17.170 (Chemlactica-125M) vs prior best Genetic-guided GFlowNets 16.213 (average across 23 tasks, Table 5/10). QED similarity-constrained optimization: Success rate = 99.0% (Chemlactica-125M) vs RetMol 94.5% using up to 10,000 QED evaluations (Table 7). MoleculeNet regression tasks: comparable RMSEs (e.g. ESOL 0.270 ± 0.011, Table 3). Docking tasks: Chemlactica-125M shows favorable oracle-burden metrics (detailed per-target numbers in Table 6); full numeric breakdowns provided in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Operates only on SMILES (no 3D coordinate modeling), limited understanding of proteins/other biological entities; synthetic accessibility and retrosynthetic feasibility not enforced in optimization loop; optimization algorithm hyperparameters not exhaustively tuned; sensitivity to numerical precision (bfloat16 mixed precision degrades multi-round fine-tuning performance and causes cascading sub-optimal generations), batching/padding numerical effects can change generated molecules; possible unfair advantage if model leverages properties that overlap with oracle internals (mitigated in some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6866.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6866.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemlactica-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemlactica (1.3B parameter variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.3B-parameter variant of the Chemlactica family trained on the same PubChem-derived corpus, evaluated for property prediction, conditional SMILES generation, and used as a generator in their optimization algorithm showing improved exploitation performance relative to the 125M variant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemlactica-1.3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine-tuned model (causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.3B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same PubChem-derived JSONL corpus as Chemlactica-125M (≈110M molecules with computed properties and similarity pairs), trained with same tag-based template formatting and tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional SMILES generation from property-tagged prompts, used in the LLM-enhanced population optimizer; sampling with temperature, repetition penalties, undesired-token suppression; used in optimization with dynamic fine-tuning when pool stagnates.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES with tag tokens (same as other Chemlactica models)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular optimization tasks (PMO benchmark), docking optimization, property prediction/fine-tuning tasks (MoleculeNet/ADMET)</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same task-specific constraints as other models: QED/MW windows, similarity constraints, composite oracle functions; no general synthetic-accessibility enforcement during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for properties and SMILES standardization; docking pipeline for docking benchmarks; uses PyTorch FSDP and FlashAttention for training.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem-derived JSONL corpus, PMO benchmark, MoleculeNet, ADMET datasets, docking target datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as Chemlactica-125M: RMSE/corrected RMSE for prediction/generation, AUC Top-10 for PMO, generative yield and oracle burden for docking optimization, Pearson correlation for ADMET regression tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PMO AUC Top-10 = 17.284 (Chemlactica-1.3B) improving slightly over the 125M variant; MoleculeNet/ADMET tasks show competitive RMSE/Pearson values listed in Tables 3 and 4. In docking tasks, 1.3B variant improves generative yields on some targets (e.g., DRD2) consistent with larger models trading exploration for exploitation (detailed per-target numbers in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same limitations as other Chemlactica models: SMILES-only representation, limited protein understanding, no explicit synthetic accessibility enforcement in optimization loop, hyperparameter sensitivity, and precision-related degradation in multi-round fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6866.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6866.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chemma-2B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chemma (2B parameter variant, Gemma continued pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2B-parameter language model (Chemma-2B) derived by continued pretraining of a Gemma checkpoint and further trained on the PubChem-based corpus; used for property prediction, conditional SMILES generation, and as the generator in the LLM-enhanced optimization algorithm and shows the best generative yield in docking benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Chemma-2B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, fine-tuned model (causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Continued pretraining on the same PubChem JSONL corpus (≈110M molecules) with added [START_SMILES]/[END_SMILES] tokens and property tags; tokenizers adapted from Gemma with added chemistry-specific tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Conditional SMILES generation via prompted property tags and similarity contexts; uses sampling techniques (temperature, repetition penalty, undesired token suppression); leveraged inside the population-based optimizer; fine-tuning on high-performing molecules for explicit oracle modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES with tagging schema (JSONL template), same as Chemlactica models</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecular optimization and de novo drug design tasks, docking optimization (DRD2, MK2, AChE), PMO benchmark, property prediction/fine-tuning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same task-specific constraints encoded via prompts: QED targets, MW max 500 for docking tasks, similarity constraints in lead optimization; no general enforcement of synthetic accessibility or retrosynthetic constraints across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for SMILES canonicalization and computing properties; docking pipeline from Beam Enumeration for docking oracle and conformer generation; ECFC4 fingerprints for similarity calculations; standard PyTorch tooling for training/serving.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>PubChem-derived JSONL corpus, PMO benchmark tasks, MoleculeNet/ADMET datasets, docking target datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RMSE/corrected RMSE for property prediction/conditional generation, AUC Top-10 for PMO, generative yield and oracle burden for docking experiments, calibration curves for multiple-choice property prediction, Pearson correlation for ADMET tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>PMO AUC Top-10 = 17.534 for Chemma-2B (highest among their models, Table 5/10), outperforming prior baselines; conditional generation and property prediction RMSEs comparable to Chemlactica variants (Table 1); in docking benchmarks Chemma-2B 'consistently achieves the highest generative yield across all evaluated receptors' (Table 6) — Chemlactica variants tend to have better oracle-burden metrics while Chemma shows better yield/exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Same SMILES-only limitation and limited protein knowledge; sensitivity to model precision (bfloat16 vs FP32) in multi-round optimization; potential negative feedback from fine-tuning on sub-optimal generated molecules when precision is low; no wet-lab synthesis validation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6866.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6866.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-enhanced optimizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Population-based molecular optimization algorithm leveraging LLM generation and in-loop fine-tuning (LLM-enhanced genetic algorithm + explicit oracle modeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel optimization algorithm that replaces classic genetic crossover/mutation with LLM generation of SMILES conditioned on pools of similar molecules and fine-tunes the language model on high-performing molecules (explicit oracle modeling), designed to maximize arbitrary black-box oracle scores with limited oracle budget.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Uses Chemlactica / Chemma models as generators (Chemlactica-125M, Chemlactica-1.3B, Chemma-2B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>algorithmic framework combining LLM generation, population-based selection, rejection-sampling style fine-tuning and prompt-engineering</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>uses LLMs of sizes 125M, 1.3B, and 2B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Does not introduce new training data beyond model corpora; uses training samples formed dynamically from pool molecules and their generated similar molecules (prompts created by molecules2prompt function using [SIMILAR] tags and optionally [PROPERTY] oracle scores), and fine-tunes LLM on high-performing samples drawn during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Iterative loop: maintain a pool P of top molecules, for each generation draw S similar molecules for prompts, create N generation prompts and sample N unique SMILES from the LLM; add new molecules to pool and keep top-P by oracle score; if stagnation for K iterations, fine-tune the LLM on high-performing molecules (explicit oracle modeling prompts include [PROPERTY]O(m)[/PROPERTY][START_SMILES]m[END_SMILES]); uses rejection-sampling ideas and LM-in-the-loop prompt optimization; sampling includes temperature scheduling, repetition penalty, undesired token suppression and chain-of-thought omission trick.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES in prompt-tagged textual format (e.g., [SIMILAR]...[/SIMILAR],[PROPERTY]...[/PROPERTY],[START_SMILES]...[END_SMILES])</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Black-box molecular optimization problems: PMO benchmark (23 tasks), multi-property docking optimization for drug discovery (DRD2, MK2, AChE), similarity-constrained QED maximization (lead optimization), and general property-driven molecule design.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Optimization enforces task-specific constraints via oracles/prompts: similarity thresholds (e.g., sim ≥ 0.4), QED thresholds (e.g., ≥0.9), MW ≤ 500 in docking tasks, composite rewards combining docking energy and other properties; the algorithm itself does not inherently enforce synthetic accessibility or provide retrosynthesis filtering, unless encoded into the oracle.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Relies on RDKit for property computation and SMILES validation/standardization; uses external black-box oracles including docking pipelines (Beam Enumeration conformer/docking setup) and any user-specified oracle; integrates with LLM training stacks (PyTorch FSDP, FlashAttention) for in-loop fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluated on PMO benchmark task suite (Gao et al. 2022), docking case studies using docking oracle and protein targets (DRD2, MK2, AChE), QED similarity dataset (800 lead molecules selected with QED in [0.7,0.8] following Wang et al. 2023), MoleculeNet/ADMET for property prediction subsystems.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Optimization performance: AUC Top-10 (PMO), generative yield, oracle burden (number of oracle calls needed for N molecules above threshold), success rate for constrained tasks; for property prediction components: RMSE/corrected RMSE, Pearson correlation for regression tasks, calibration plots for predicted probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>The LLM-enhanced optimizer with Chemlactica-125M achieved PMO AUC Top-10 = 17.170, outperforming Genetic-guided GFlowNets baseline (16.213). Using stronger generator models improved performance (Chemlactica-1.3B = 17.284, Chemma-2B = 17.534). In QED similarity-constrained lead optimization, the approach achieved 99.0% success rate with 10k QED evaluations versus RetMol 94.5% (Table 7). Docking experiments showed Chemma-2B with the optimizer achieved the highest generative yields across receptors while Chemlactica-125M minimized oracle burden (detailed per-target metrics in Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires many oracle evaluations (though constrained in benchmarks); sensitive hyperparameters (pool size P, S similar molecules, K finetuning tolerance, lr) and requires per-model tuning; numeric precision issues (bfloat16/mixed precision) can degrade multi-round fine-tuning and lead to cascading poor generations; batching/padding during batched generation changes matrix sizes and numerical behavior causing small errors accumulating and producing lower-quality outputs; does not enforce synthetic accessibility/retrosynthesis by default; in-context learning attempted but abandoned due to lack of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6866.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6866.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubChem JSONL corpus (Chemlactica corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubChem-derived JSONL molecular corpus with tags and computed properties (110M molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A structured JSONL corpus constructed from PubChem dumps containing ~110M molecules, computed properties (QED, SAS, MW, TPSA, CLogP, H-bond counts, ring counts), similarity pairs sampled and recalculated with ECFC4, and templated tag-based text formatting used to train and fine-tune the Chemlactica and Chemma LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>training corpus used for Chemlactica and Chemma models</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>textual JSONL corpus with specialized tag tokens for properties and SMILES</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈110 million molecules represented as JSONL; corpus generation produced ~40B tokens used across model training</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Derived from PubChem dumps (cutoff Jan 26, 2023); molecules parsed and canonicalized with RDKit; computed molecular properties added (QED, SAS, MW, TPSA, CLogP, H-bond donors/acceptors, ring counts); related-molecule pairs sampled (~4 billion sampled from PubChem related pairs, similarities recalculated with ECFC4); each molecule represented as a JSON object with tag-delimited fields (e.g., [QED]0.84[/QED], [START_SMILES]...[/END_SMILES]) and randomized property order to improve model versatility.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Not a generative model itself; corpus was used to continue-pretrain and fine-tune LLMs for conditional generation (models learn to emit SMILES conditioned on property tags and to predict properties from SMILES).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES canonicalized by RDKit stored within JSONL, plus property tags and auxiliary textual fields (synonyms, CID, similar molecules).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Pretraining and fine-tuning data for LLMs aimed at molecular property prediction, property-conditioned generation, and molecular optimization tasks in drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Data filtered to include only RDKit-parsable SMILES; similarity pairs selected with Tanimoto >= 0.8 originally then recalculated and sampled; molecules that failed RDKit MolFromSmiles were discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit used to compute properties and canonicalize SMILES; ECFC4 fingerprints for similarity recalculation; this corpus interfaces with LLM training pipelines (tokenizers augmented with chemistry-specific tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Constructed from PubChem; used alongside benchmark datasets (PMO, MoleculeNet, ADMET) for evaluation but is itself the primary training corpus for models in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not applicable directly; downstream evaluation of models trained on the corpus used RMSE, AUC Top-10, generative yield, oracle burden, calibration metrics, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Corpus enabled training of models that achieve property prediction RMSEs (e.g., QED RMSE 0.016 for Chemlactica-125M) and strong optimization results (PMO AUC Top-10 improvements reported); the authors publicly release the corpus (link in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>SMILES-only representation means 3D/structural information not present; any biases or errors in PubChem or RDKit canonicalization propagate into the corpus; reliance on computed properties (RDKit) rather than experimental values for many entries could introduce noise relative to experimental labels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Molecule Optimization with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>BARTSmiles: Generative masked language models for molecular representations <em>(Rating: 2)</em></li>
                <li>REINVENT 2.0: an ai tool for de novo drug design <em>(Rating: 2)</em></li>
                <li>Sample efficiency matters: A benchmark for practical molecular optimization <em>(Rating: 2)</em></li>
                <li>Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design <em>(Rating: 2)</em></li>
                <li>Genetic-guided gflownets: Advancing in practical molecular optimization benchmark <em>(Rating: 2)</em></li>
                <li>EvoPrompting: Language models for code-level neural architecture search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6866",
    "paper_id": "paper-271516332",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "Chemlactica-125M",
            "name_full": "Chemlactica (125M parameter variant)",
            "brief_description": "A 125M-parameter decoder-only language model fine-tuned on a JSONL corpus of ~110M PubChem molecules and computed properties, used for SMILES-conditioned property prediction and conditional SMILES generation and deployed as the generator in a population-based molecular optimization loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemlactica-125M",
            "model_type": "decoder-only LLM, fine-tuned model (causal LM)",
            "model_size": "125M parameters",
            "training_data_description": "Continued pretraining / fine-tuning on a novel JSONL corpus derived from PubChem: ~110M molecules, computed molecular properties (QED, SAS, MW, TPSA, CLogP, H-bond donors/acceptors, ring counts), similarity pairs (sampled from related-molecule pairs recalculated with ECFC4), totaling ~40B tokens in the corpus generation process; SMILES canonicalized with RDKit and encoded with tag-based template tokens (e.g. [QED]value[/QED], [START_SMILES]).",
            "generation_method": "Direct conditional SMILES generation via property-tagged prompts and special tokens; techniques used during sampling include vanilla temperature sampling (dynamic temperature schedule from 1 to 1.5), repetition penalty, undesired-token suppression to force generation of [START_SMILES], and a chain-of-thought trick (omitting [START_SMILES] in prompt to encourage intermediate property generation). Also used in-loop fine-tuning on high-performing molecules (explicit oracle modeling) and as the generator in an LLM-enhanced population-based optimizer (replacing classic crossover/mutation).",
            "chemical_representation": "SMILES (canonicalized via RDKit) with additional property and tag tokens in JSONL text format",
            "target_application": "Small-molecule drug discovery and molecular optimization tasks (Practical Molecular Optimization (PMO) benchmark tasks, multi-property docking optimization against DRD2, MK2, AChE, QED maximization with similarity constraints, ADMET/property prediction tasks and MoleculeNet regression tasks).",
            "constraints_used": "Task-specific constraints encoded in prompts or oracle: QED thresholds (e.g., target QED ≥ 0.9), similarity constraints (e.g., sim ≥ 0.4 or specified values), molecular weight limits (≤ 500 Da in docking tasks), composite oracle reward functions combining docking score, QED and MW; note: synthetic accessibility was measured (SAS) in training data but the optimization algorithm does not explicitly enforce synthetic accessibility or retrosynthetic feasibility as a general constraint.",
            "integration_with_external_tools": "RDKit used for SMILES parsing, canonicalization and computing computed properties (QED, SAS, TPSA, CLogP, MW); ECFC4 fingerprints for similarity recalculation; docking experiments used the conformer generation + docking pipeline and aggregate reward function adopted from the Beam Enumeration implementation (conformational generation and docking software / docking oracle described in Beam Enumeration paper); PyTorch ecosystem (FSDP) and FlashAttention used for model training/serving.",
            "dataset_used": "PubChem-derived JSONL corpus (~110M molecules), PMO benchmark (23 tasks from Gao et al. 2022) for optimization evaluation, MoleculeNet tasks and ADMET datasets (Fang et al.) for property prediction/fine-tuning, docking target protein structures for docking benchmarks (DRD2, MK2, AChE).",
            "evaluation_metrics": "Property prediction: RMSE, corrected RMSE (invalid SMILES replaced by dataset mean), MAPE, Pearson correlation; Calibration: binned accuracy vs predicted probability; Conditional generation: RMSE between target and generated property values; Optimization benchmarks: AUC Top-10 (Top-10 average property vs number of oracle calls) for PMO, generative yield (number of unique molecules above reward threshold per fixed oracle calls), oracle burden (number of oracle calls needed to generate N unique molecules above threshold), success rate for similarity-constrained QED optimization.",
            "reported_results": "Property prediction: QED RMSE 0.016 (Chemlactica-125M, Table 1); Conditional generation: property-conditioned RMSEs reported per property (see Table 1). PMO optimization: AUC Top-10 = 17.170 (Chemlactica-125M) vs prior best Genetic-guided GFlowNets 16.213 (average across 23 tasks, Table 5/10). QED similarity-constrained optimization: Success rate = 99.0% (Chemlactica-125M) vs RetMol 94.5% using up to 10,000 QED evaluations (Table 7). MoleculeNet regression tasks: comparable RMSEs (e.g. ESOL 0.270 ± 0.011, Table 3). Docking tasks: Chemlactica-125M shows favorable oracle-burden metrics (detailed per-target numbers in Table 6); full numeric breakdowns provided in paper tables.",
            "experimental_validation": false,
            "challenges_or_limitations": "Operates only on SMILES (no 3D coordinate modeling), limited understanding of proteins/other biological entities; synthetic accessibility and retrosynthetic feasibility not enforced in optimization loop; optimization algorithm hyperparameters not exhaustively tuned; sensitivity to numerical precision (bfloat16 mixed precision degrades multi-round fine-tuning performance and causes cascading sub-optimal generations), batching/padding numerical effects can change generated molecules; possible unfair advantage if model leverages properties that overlap with oracle internals (mitigated in some experiments).",
            "uuid": "e6866.0",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chemlactica-1.3B",
            "name_full": "Chemlactica (1.3B parameter variant)",
            "brief_description": "A 1.3B-parameter variant of the Chemlactica family trained on the same PubChem-derived corpus, evaluated for property prediction, conditional SMILES generation, and used as a generator in their optimization algorithm showing improved exploitation performance relative to the 125M variant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemlactica-1.3B",
            "model_type": "decoder-only LLM, fine-tuned model (causal LM)",
            "model_size": "1.3B parameters",
            "training_data_description": "Same PubChem-derived JSONL corpus as Chemlactica-125M (≈110M molecules with computed properties and similarity pairs), trained with same tag-based template formatting and tokens.",
            "generation_method": "Conditional SMILES generation from property-tagged prompts, used in the LLM-enhanced population optimizer; sampling with temperature, repetition penalties, undesired-token suppression; used in optimization with dynamic fine-tuning when pool stagnates.",
            "chemical_representation": "SMILES with tag tokens (same as other Chemlactica models)",
            "target_application": "Molecular optimization tasks (PMO benchmark), docking optimization, property prediction/fine-tuning tasks (MoleculeNet/ADMET)",
            "constraints_used": "Same task-specific constraints as other models: QED/MW windows, similarity constraints, composite oracle functions; no general synthetic-accessibility enforcement during optimization.",
            "integration_with_external_tools": "RDKit for properties and SMILES standardization; docking pipeline for docking benchmarks; uses PyTorch FSDP and FlashAttention for training.",
            "dataset_used": "PubChem-derived JSONL corpus, PMO benchmark, MoleculeNet, ADMET datasets, docking target datasets.",
            "evaluation_metrics": "Same as Chemlactica-125M: RMSE/corrected RMSE for prediction/generation, AUC Top-10 for PMO, generative yield and oracle burden for docking optimization, Pearson correlation for ADMET regression tasks.",
            "reported_results": "PMO AUC Top-10 = 17.284 (Chemlactica-1.3B) improving slightly over the 125M variant; MoleculeNet/ADMET tasks show competitive RMSE/Pearson values listed in Tables 3 and 4. In docking tasks, 1.3B variant improves generative yields on some targets (e.g., DRD2) consistent with larger models trading exploration for exploitation (detailed per-target numbers in Table 6).",
            "experimental_validation": false,
            "challenges_or_limitations": "Same limitations as other Chemlactica models: SMILES-only representation, limited protein understanding, no explicit synthetic accessibility enforcement in optimization loop, hyperparameter sensitivity, and precision-related degradation in multi-round fine-tuning.",
            "uuid": "e6866.1",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Chemma-2B",
            "name_full": "Chemma (2B parameter variant, Gemma continued pretraining)",
            "brief_description": "A 2B-parameter language model (Chemma-2B) derived by continued pretraining of a Gemma checkpoint and further trained on the PubChem-based corpus; used for property prediction, conditional SMILES generation, and as the generator in the LLM-enhanced optimization algorithm and shows the best generative yield in docking benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Chemma-2B",
            "model_type": "decoder-only LLM, fine-tuned model (causal LM)",
            "model_size": "2B parameters",
            "training_data_description": "Continued pretraining on the same PubChem JSONL corpus (≈110M molecules) with added [START_SMILES]/[END_SMILES] tokens and property tags; tokenizers adapted from Gemma with added chemistry-specific tokens.",
            "generation_method": "Conditional SMILES generation via prompted property tags and similarity contexts; uses sampling techniques (temperature, repetition penalty, undesired token suppression); leveraged inside the population-based optimizer; fine-tuning on high-performing molecules for explicit oracle modeling.",
            "chemical_representation": "SMILES with tagging schema (JSONL template), same as Chemlactica models",
            "target_application": "Molecular optimization and de novo drug design tasks, docking optimization (DRD2, MK2, AChE), PMO benchmark, property prediction/fine-tuning tasks.",
            "constraints_used": "Same task-specific constraints encoded via prompts: QED targets, MW max 500 for docking tasks, similarity constraints in lead optimization; no general enforcement of synthetic accessibility or retrosynthetic constraints across experiments.",
            "integration_with_external_tools": "RDKit for SMILES canonicalization and computing properties; docking pipeline from Beam Enumeration for docking oracle and conformer generation; ECFC4 fingerprints for similarity calculations; standard PyTorch tooling for training/serving.",
            "dataset_used": "PubChem-derived JSONL corpus, PMO benchmark tasks, MoleculeNet/ADMET datasets, docking target datasets.",
            "evaluation_metrics": "RMSE/corrected RMSE for property prediction/conditional generation, AUC Top-10 for PMO, generative yield and oracle burden for docking experiments, calibration curves for multiple-choice property prediction, Pearson correlation for ADMET tasks.",
            "reported_results": "PMO AUC Top-10 = 17.534 for Chemma-2B (highest among their models, Table 5/10), outperforming prior baselines; conditional generation and property prediction RMSEs comparable to Chemlactica variants (Table 1); in docking benchmarks Chemma-2B 'consistently achieves the highest generative yield across all evaluated receptors' (Table 6) — Chemlactica variants tend to have better oracle-burden metrics while Chemma shows better yield/exploitation.",
            "experimental_validation": false,
            "challenges_or_limitations": "Same SMILES-only limitation and limited protein knowledge; sensitivity to model precision (bfloat16 vs FP32) in multi-round optimization; potential negative feedback from fine-tuning on sub-optimal generated molecules when precision is low; no wet-lab synthesis validation reported.",
            "uuid": "e6866.2",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLM-enhanced optimizer",
            "name_full": "Population-based molecular optimization algorithm leveraging LLM generation and in-loop fine-tuning (LLM-enhanced genetic algorithm + explicit oracle modeling)",
            "brief_description": "A novel optimization algorithm that replaces classic genetic crossover/mutation with LLM generation of SMILES conditioned on pools of similar molecules and fine-tunes the language model on high-performing molecules (explicit oracle modeling), designed to maximize arbitrary black-box oracle scores with limited oracle budget.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Uses Chemlactica / Chemma models as generators (Chemlactica-125M, Chemlactica-1.3B, Chemma-2B)",
            "model_type": "algorithmic framework combining LLM generation, population-based selection, rejection-sampling style fine-tuning and prompt-engineering",
            "model_size": "uses LLMs of sizes 125M, 1.3B, and 2B in experiments",
            "training_data_description": "Does not introduce new training data beyond model corpora; uses training samples formed dynamically from pool molecules and their generated similar molecules (prompts created by molecules2prompt function using [SIMILAR] tags and optionally [PROPERTY] oracle scores), and fine-tunes LLM on high-performing samples drawn during optimization.",
            "generation_method": "Iterative loop: maintain a pool P of top molecules, for each generation draw S similar molecules for prompts, create N generation prompts and sample N unique SMILES from the LLM; add new molecules to pool and keep top-P by oracle score; if stagnation for K iterations, fine-tune the LLM on high-performing molecules (explicit oracle modeling prompts include [PROPERTY]O(m)[/PROPERTY][START_SMILES]m[END_SMILES]); uses rejection-sampling ideas and LM-in-the-loop prompt optimization; sampling includes temperature scheduling, repetition penalty, undesired token suppression and chain-of-thought omission trick.",
            "chemical_representation": "SMILES in prompt-tagged textual format (e.g., [SIMILAR]...[/SIMILAR],[PROPERTY]...[/PROPERTY],[START_SMILES]...[END_SMILES])",
            "target_application": "Black-box molecular optimization problems: PMO benchmark (23 tasks), multi-property docking optimization for drug discovery (DRD2, MK2, AChE), similarity-constrained QED maximization (lead optimization), and general property-driven molecule design.",
            "constraints_used": "Optimization enforces task-specific constraints via oracles/prompts: similarity thresholds (e.g., sim ≥ 0.4), QED thresholds (e.g., ≥0.9), MW ≤ 500 in docking tasks, composite rewards combining docking energy and other properties; the algorithm itself does not inherently enforce synthetic accessibility or provide retrosynthesis filtering, unless encoded into the oracle.",
            "integration_with_external_tools": "Relies on RDKit for property computation and SMILES validation/standardization; uses external black-box oracles including docking pipelines (Beam Enumeration conformer/docking setup) and any user-specified oracle; integrates with LLM training stacks (PyTorch FSDP, FlashAttention) for in-loop fine-tuning.",
            "dataset_used": "Evaluated on PMO benchmark task suite (Gao et al. 2022), docking case studies using docking oracle and protein targets (DRD2, MK2, AChE), QED similarity dataset (800 lead molecules selected with QED in [0.7,0.8] following Wang et al. 2023), MoleculeNet/ADMET for property prediction subsystems.",
            "evaluation_metrics": "Optimization performance: AUC Top-10 (PMO), generative yield, oracle burden (number of oracle calls needed for N molecules above threshold), success rate for constrained tasks; for property prediction components: RMSE/corrected RMSE, Pearson correlation for regression tasks, calibration plots for predicted probabilities.",
            "reported_results": "The LLM-enhanced optimizer with Chemlactica-125M achieved PMO AUC Top-10 = 17.170, outperforming Genetic-guided GFlowNets baseline (16.213). Using stronger generator models improved performance (Chemlactica-1.3B = 17.284, Chemma-2B = 17.534). In QED similarity-constrained lead optimization, the approach achieved 99.0% success rate with 10k QED evaluations versus RetMol 94.5% (Table 7). Docking experiments showed Chemma-2B with the optimizer achieved the highest generative yields across receptors while Chemlactica-125M minimized oracle burden (detailed per-target metrics in Table 6).",
            "experimental_validation": false,
            "challenges_or_limitations": "Requires many oracle evaluations (though constrained in benchmarks); sensitive hyperparameters (pool size P, S similar molecules, K finetuning tolerance, lr) and requires per-model tuning; numeric precision issues (bfloat16/mixed precision) can degrade multi-round fine-tuning and lead to cascading poor generations; batching/padding during batched generation changes matrix sizes and numerical behavior causing small errors accumulating and producing lower-quality outputs; does not enforce synthetic accessibility/retrosynthesis by default; in-context learning attempted but abandoned due to lack of improvement.",
            "uuid": "e6866.3",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PubChem JSONL corpus (Chemlactica corpus)",
            "name_full": "PubChem-derived JSONL molecular corpus with tags and computed properties (110M molecules)",
            "brief_description": "A structured JSONL corpus constructed from PubChem dumps containing ~110M molecules, computed properties (QED, SAS, MW, TPSA, CLogP, H-bond counts, ring counts), similarity pairs sampled and recalculated with ECFC4, and templated tag-based text formatting used to train and fine-tune the Chemlactica and Chemma LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "training corpus used for Chemlactica and Chemma models",
            "model_type": "textual JSONL corpus with specialized tag tokens for properties and SMILES",
            "model_size": "≈110 million molecules represented as JSONL; corpus generation produced ~40B tokens used across model training",
            "training_data_description": "Derived from PubChem dumps (cutoff Jan 26, 2023); molecules parsed and canonicalized with RDKit; computed molecular properties added (QED, SAS, MW, TPSA, CLogP, H-bond donors/acceptors, ring counts); related-molecule pairs sampled (~4 billion sampled from PubChem related pairs, similarities recalculated with ECFC4); each molecule represented as a JSON object with tag-delimited fields (e.g., [QED]0.84[/QED], [START_SMILES]...[/END_SMILES]) and randomized property order to improve model versatility.",
            "generation_method": "Not a generative model itself; corpus was used to continue-pretrain and fine-tune LLMs for conditional generation (models learn to emit SMILES conditioned on property tags and to predict properties from SMILES).",
            "chemical_representation": "SMILES canonicalized by RDKit stored within JSONL, plus property tags and auxiliary textual fields (synonyms, CID, similar molecules).",
            "target_application": "Pretraining and fine-tuning data for LLMs aimed at molecular property prediction, property-conditioned generation, and molecular optimization tasks in drug discovery.",
            "constraints_used": "Data filtered to include only RDKit-parsable SMILES; similarity pairs selected with Tanimoto &gt;= 0.8 originally then recalculated and sampled; molecules that failed RDKit MolFromSmiles were discarded.",
            "integration_with_external_tools": "RDKit used to compute properties and canonicalize SMILES; ECFC4 fingerprints for similarity recalculation; this corpus interfaces with LLM training pipelines (tokenizers augmented with chemistry-specific tokens).",
            "dataset_used": "Constructed from PubChem; used alongside benchmark datasets (PMO, MoleculeNet, ADMET) for evaluation but is itself the primary training corpus for models in the paper.",
            "evaluation_metrics": "Not applicable directly; downstream evaluation of models trained on the corpus used RMSE, AUC Top-10, generative yield, oracle burden, calibration metrics, etc.",
            "reported_results": "Corpus enabled training of models that achieve property prediction RMSEs (e.g., QED RMSE 0.016 for Chemlactica-125M) and strong optimization results (PMO AUC Top-10 improvements reported); the authors publicly release the corpus (link in paper).",
            "experimental_validation": null,
            "challenges_or_limitations": "SMILES-only representation means 3D/structural information not present; any biases or errors in PubChem or RDKit canonicalization propagate into the corpus; reliance on computed properties (RDKit) rather than experimental values for many entries could introduce noise relative to experimental labels.",
            "uuid": "e6866.4",
            "source_info": {
                "paper_title": "Small Molecule Optimization with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2,
            "sanitized_title": "chemformer_a_pretrained_transformer_for_computational_chemistry"
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "BARTSmiles: Generative masked language models for molecular representations",
            "rating": 2,
            "sanitized_title": "bartsmiles_generative_masked_language_models_for_molecular_representations"
        },
        {
            "paper_title": "REINVENT 2.0: an ai tool for de novo drug design",
            "rating": 2,
            "sanitized_title": "reinvent_20_an_ai_tool_for_de_novo_drug_design"
        },
        {
            "paper_title": "Sample efficiency matters: A benchmark for practical molecular optimization",
            "rating": 2,
            "sanitized_title": "sample_efficiency_matters_a_benchmark_for_practical_molecular_optimization"
        },
        {
            "paper_title": "Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design",
            "rating": 2,
            "sanitized_title": "beam_enumeration_probabilistic_explainability_for_sample_efficient_selfconditioned_molecular_design"
        },
        {
            "paper_title": "Genetic-guided gflownets: Advancing in practical molecular optimization benchmark",
            "rating": 2,
            "sanitized_title": "geneticguided_gflownets_advancing_in_practical_molecular_optimization_benchmark"
        },
        {
            "paper_title": "EvoPrompting: Language models for code-level neural architecture search",
            "rating": 1,
            "sanitized_title": "evoprompting_language_models_for_codelevel_neural_architecture_search"
        }
    ],
    "cost": 0.0185485,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Molecule Optimization with Large Language Models
26 Jul 2024</p>
<p>Philipp Guevorguian 
Menua Bedrosian 
Yerevann Tigran Fahradyan 
Chilingaryan Gayane 
Hrant Yerevann 
Khachatrian 
Armen Aghajanyan </p>
<p>YerevaNN Yerevan State University</p>
<p>YerevaNN American University
Armenia</p>
<p>YerevaNN Yerevan State University</p>
<p>Small Molecule Optimization with Large Language Models
26 Jul 20246418AFBD9AB07352845857871C036FE6arXiv:2407.18897v1[cs.LG]
Recent advancements in large language models have opened new possibilities for generative molecular drug design.We present Chemlactica and Chemma, two language models fine-tuned on a novel corpus of 110M molecules with computed properties, totaling 40B tokens.These models demonstrate strong performance in generating molecules with specified properties and predicting new molecular characteristics from limited samples.We introduce a novel optimization algorithm that leverages our language models to optimize molecules for arbitrary properties given limited access to a black box oracle.Our approach combines ideas from genetic algorithms, rejection sampling, and prompt optimization.It achieves stateof-the-art performance on multiple molecular optimization benchmarks, including an 8% improvement on Practical Molecular Optimization compared to previous methods.We publicly release the training corpus, the language models and the optimization algorithm.</p>
<p>Introduction</p>
<p>Molecular optimization is a cornerstone of drug discovery, involving the complex task of identifying compounds with specific desirable properties.This process traditionally requires extensive laboratory experimentation, making it time-consuming and costly.Computational methods have emerged as powerful tools to accelerate this process, yet they often need help with the vast and discrete nature of chemical space [Wu et al., 2018].</p>
<p>Large language models (LLMs) have recently demonstrated remarkable capabilities across various domains, from natural language processing to code generation [Brown et al., 2020, OpenAI, 2023].While there have been initial attempts to apply LLMs to chemical tasks [Irwin et al., 2022, Edwards et al., 2022, Chilingaryan et al., 2024], these efforts have often been limited in scope or performance.Our work represents a significant leap forward, leveraging the full power of LLMs to revolutionize molecular optimization for drug discovery.</p>
<p>We present a novel approach that harnesses LLMs to generate and optimize small molecules with unprecedented efficiency and accuracy.Our method uniquely combines LLMs' generative capabilities with evolutionary strategies, enabling more effective exploration of chemical space than traditional graph-based or SMILES-based models.Our training corpus, models and code can be found at https://github.com/yerevann/chemlactica.</p>
<p>Our research makes several contributions to the field:</p>
<p>Preprint.Under review.</p>
<p>Related Work</p>
<p>Language Models for Molecular Representation While graph-based representations are common for molecules, string-based representations, particularly Simplified Molecular Input Line Entry System (SMILES) [Weininger, 1988], have gained traction due to their compatibility with language models.This approach leverages the power of pre-trained language models and enables efficient processing of molecular data.Notable examples include ChemFormer [Irwin et al., 2022], MolT5 [Edwards et al., 2022], and BARTSmiles [Chilingaryan et al., 2024], which adapt traditional language model architectures to chemical tasks.These models demonstrate the potential of applying natural language processing techniques to molecular design and property prediction.</p>
<p>Molecular Optimization Techniques Molecular optimization, a key challenge in drug discovery, involves navigating a vast combinatorial space of potential drugs while satisfying multiple constraints.Traditional approaches include genetic algorithms adapted for molecular graphs, often incorporating domain-specific heuristics [Jensen, 2019].More recent methods leverage machine learning, particularly deep learning techniques.For instance, variational autoencoders [Kingma and Welling, 2013] have been applied to generate and optimize molecules in latent space.The GFlowNets [Bengio et al., 2021] represents a novel approach designed to sample compositional objects (like molecules) with reward-proportional probability, making it well-suited for optimization tasks.Extensions of GFlowNets [Kim et al., 2024] incorporating genetic search have shown promising results in molecular optimization.</p>
<p>Recurrent Neural Networks in Molecular Design Recurrent neural networks (RNNs) have also been applied to molecular optimization.A notable example is REINVENT [Olivecrona et al., 2017], which uses policy-based reinforcement learning to generate molecules with desired properties.Recent enhancements to REINVENT, such as augmented memory and beam enumeration [Guo and Schwaller, 2023b], have further improved its performance.These approaches combine molecular diversity filters, experience replay mechanisms, and substructure filtering to increase sample efficiency in molecular optimization tasks.</p>
<p>Large Language Models in Optimization The success of large language models (LLMs) has led to their application in various optimization tasks beyond text generation.For instance, Chen et al. [2023] combined prompt tuning with evolutionary algorithms to design neural network architectures, outperforming human experts on specific tasks.Similarly, EvoPrompt [Guo et al., 2023] developed a general evolutionary algorithm using language models, optimizing task-specific prompts for various downstream applications.These studies demonstrate the potential of LLMs in complex optimization problems, paving the way for their application in molecular design and optimization.</p>
<p>Our work builds upon these foundations, uniquely combining the strengths of large language models with evolutionary strategies for molecular optimization.We extend the application of LLMs beyond simple property prediction or generation, developing a comprehensive framework for navigating the complex landscape of molecular design.</p>
<p>Training Corpus</p>
<p>Molecular Database from PubChem We constructed a comprehensive SQL database using PubChem dumps, encompassing information on molecules, similar molecule pairs, experimental properties, and bioassays.Using rdkit [Landrum et al., 2013], we computed key molecular properties, including synthesizability score (SAS), quantitatively estimated drug-likeness (QED), molecular weight (MW), total polar surface area (TPSA), partition coefficient (CLogP), and various structural features such as hydrogen donors/acceptors and ring counts.Due to differences in SMILES canonicalization between PubChem and rdkit, we standardized all SMILES strings using rdkit's implementation.</p>
<p>Our dataset's cutoff date is January 26th, 2023, excluding any subsequent additions or modifications to PubChem.To ensure data integrity, molecules that failed rdkit's MolFromSmiles parsing were discarded.</p>
<p>To incorporate similarity information, we utilized PubChem's related molecule data, which includes pairs with Tanimoto similarity ≥0.8 based on PubChem fingerprints.From the resulting 200 billion pairs, we sampled 4 billion and recalculated their similarities using the ECFC4 fingerprint for improved accuracy and consistency with widely used methods.</p>
<p>JSONL Corpus Generation</p>
<p>We transformed our database into a corpus of JSONL files, with each molecule represented as a single JSON object.Below is an abbreviated example for aspirin: This representation includes molecular identifiers, computed properties, similarity data, synonyms, experimental properties, and the PubChem compound identifier (CID).</p>
<p>Text Generation Template</p>
<p>We developed a template system using paired tags to delimit each property and data point.For instance, a molecule's QED value is represented as
[QED]0.84[/QED].
To enhance the model's versatility in both property prediction and property-conditioned molecular generation, we randomized the property order and alternated the position of the primary molecule (start vs. in-between other tags) with equal probability.This carefully curated and structured corpus forms the foundation for training our language models, enabling them to learn complex relationships between molecular structures and properties.</p>
<p>Model Training and Evaluation</p>
<p>Selection of Pretrained Language Models We chose models for continued pretraining based on their general-purpose performance and domain-specific knowledge.At its release, Galactica outperformed models like OPT, Chinchilla, and BLOOM on tasks such as BIG-bench, MMLU, and TruthfulQA [Taylor et al., 2022].Its pretraining included two million PubChem molecules, SMILES-specific tagging, and a scientific corpus, making it well-suited for molecular data.Gemma, while not explicitly trained on molecular data, underwent extensive pretraining (2 trillion tokens for Gemma-2B) and demonstrated state-of-the-art performance on benchmarks like MMLU, HellaSwag, and Human eval, comparable to larger models like LLaMA 2 and Mistral [Team et al., 2024].</p>
<p>Tokenization and Sample Preparation</p>
<p>We utilized the original tokenizers from Gemma and Galactica, adding chemistry-specific tokens [START_SMILES] and [END_SMILES] to Gemma's tokenizer for consistency.To optimize training efficiency, we included all opening and closing tags as special tokens (e.g., [QED]).Samples of varying lengths were tokenized and grouped into blocks of 2048 tokens, separated by model-specific separator tokens (EOS "</s>" for Chemlactica, BOS "<bos>" for Chemma).</p>
<p>Training Methodology Both Chemma and Chemlactica were trained using the Adam optimizer [Kingma and Ba, 2014] with cross-entropy loss and a causal language modeling objective.We applied dropout only to Chemlactica, maintaining consistency with the original model architectures.Chemma-2B was trained in full bfloat16 for computational efficiency.We leveraged PyTorch's [Paszke et al., 2019] Fully Sharded Data Parallel (FSDP) [Zhao et al., 2023] and Flash Attention [Dao, 2024] for optimized training.The training was conducted locally at Yerevan State University (Chemlactica-125M: 306 A100 hours) and on Nebius.aicloud (Chemma-2B: 488 H100 GPU hours, Chemlactica-1.3B: 288 H100 GPU hours).Preparatory work before the final training runs consumed multiple thousands of A100 hours.</p>
<p>Evaluation of Computed Property Prediction and Conditional Generation</p>
<p>To assess our models' proficiency in learning computed properties, we conducted two comprehensive experiments:</p>
<p>Property Prediction We randomly sampled a fixed set of 100 molecules from the validation set.For each property, we prompted the models with
[START_SMILES]M i [END_SMILES][QED],
where M i represents the SMILES string of the molecule.We then calculated the Root Mean Square Error (RMSE) between predicted and actual property values to evaluate performance.</p>
<p>Conditional Generation</p>
<p>For each property, we sampled 100 values v i from the distribution of PubChem molecules.We then prompted the models to generate molecules with
[QED]v i [/QED][START_SMILES].
Using rdkit, we computed the actual property values of the generated SMILES and calculated the RMSE against the target v i .</p>
<p>Table 1 presents the results for both Property Prediction (PP) and Conditional Generation (CG) across various properties for our three model variants.For Chemma-2B, we provide evaluations at different training data volumes, including a compute-controlled run with 2.1B tokens to ensure fair comparison with Chemlactica-125M.</p>
<p>To account for potential invalid generations, we compute a corrected RMSE by substituting the property values of invalid SMILES with the mean value of the respective property's distribution in our dataset.</p>
<p>Our generation process incorporates several techniques to improve output quality:</p>
<p>• Chain-of-Thought (CoT): We omit [START_SMILES] from the initial prompt, enabling the model to generate more property values before the molecule itself.</p>
<p>• Repetition Penalty: Applied to discourage repetitive outputs [Keskar et al., 2019].</p>
<p>• Undesired Token Suppression: Employed to ensure the model eventually generates
[START_SMILES].
Table 2 provides an ablation study of these sampling components across our three models, demonstrating their individual and combined impacts on generation quality.Surprisingly, the best combinations of hyperparameters coincide for all three models.</p>
<p>These experiments comprehensively show our models' capabilities in predicting molecular properties and generating molecules with specified properties.These are crucial tasks in computational drug discovery and molecular design.Model calibration in language modeling refers to the alignment between a model's predicted probabilities for generating specific text and the actual likelihood of that text being correct.To assess the calibration of our models, we developed a suite of multiple-choice property prediction questions based on our training data format.</p>
<p>We generated 2000 questions for each computed property, resulting in 10,000 responses.Each question presented a SMILES string as input:</p>
<p>[START_SMILES]<SMILES> [END_SMILES] followed by five potential continuations, with only one being correct.This methodology is inspired by the calibration analysis in the GPT-4 technical report [OpenAI, 2023], which highlights calibration as a key indicator of high-quality pretraining.</p>
<p>For each response, we calculated the model's predicted probability based on the perplexity of the text, normalizing it against other responses for the same question.These probabilities were then aggregated and sorted into 10 equal-width bins.We plotted the fraction of correct responses for each bin, allowing us to visualize the relationship between the model's confidence and accuracy.</p>
<p>Results</p>
<p>Figures 1a and 1b present the calibration plots for Chemma-2B and Chemlactica-125M, respectively.The x-axis represents the 10 probability bins, while the left y-axis shows the correct response fraction.</p>
<p>The right y-axis and red bars indicate the number of occurrences within each bin.</p>
<p>Chemlactica and Chemma models demonstrate robust calibration, as evidenced by the near-linear relationship between assigned probabilities and correct outcomes across all computed properties.This relationship closely follows the diagonal grey line, which represents perfect calibration.</p>
<p>These results suggest that the perplexity scores generated by our models serve as reliable confidence indicators for molecular data predictions (averaged over a set of molecules), provided the data falls within the distribution of the training corpus.This calibration is crucial for practical applications, as it allows users to accurately gauge the reliability of the models' outputs in various molecular prediction and generation tasks.</p>
<p>Property Prediction</p>
<p>Supervised fine-tuning recipe.We designed and implemented a fine-tuning strategy to evaluate our model's adaptability to novel tasks not present in the initial training corpus.To this end, we fined-tuned our models on 6 tasks introduced by Fang et al.</p>
<p>[2023a] and 3 others by MoleculeNet Wu et al. [2018].Inspired by instruction tuning methodologies, we generated a specialized training corpus formatted as follows:
[START_SMILES]m smiles [END_SMILES][PROPERTY]<VALUE>[/PROPERTY].
We only trained the model on generated responses following the [PROPERTY] tag during the finetuning process.Our initial experiments indicated that a general fine-tuning recipe of 15 epochs yielded satisfactory results with a peak learning rate of 10e − 4 with 3 epochs of warmup and a NEFTune noise [Jain et al., 2023] of 5.However, we observed that our models could significantly benefit from a more rigorous hyperparameter optimization process.Consequently, we conducted an extensive hyperparameter tuning study, exploring a grid of values within the following ranges: Learning rate: [0.00001, 0.00005, 0.0001, 0.0002], Number of epochs: [10,15,20], Warmup epoch ratios: [0, 0.4, 1], NEFTune noise : [0.0, 5.0, 10.0].The results presented in Table 3 and 4 showcase the abilities of our models after the hyperparameter tuning stage.The details of hyperparameters selected per task and model can be found in the Appendix A.1.</p>
<p>Molecular Optimization Algorithm</p>
<p>We present a novel population-based algorithm for molecular optimization that leverages our trained language models.The algorithm addresses the challenging task of navigating the vast chemical space to find molecules with desired properties, subject to a limited evaluation budget.Formally, we define the molecular optimization problem as:
m * = arg max m∈M O(m)
where m represents a molecule, M is the constraint set of valid molecules (typically very large), and O : M → R is a black-box oracle function that evaluates molecular properties.This oracle could represent complex processes such as lab experiments or quantum simulations.</p>
<p>Our approach maintains a pool of P high-performing molecules and iteratively generates new candidates using a language model.It is built on three key innovations:</p>
<p>LLM-enhanced genetic algorithm We leverage our language models to generate molecules similar to the current pool.This can be viewed as a genetic algorithm where traditional crossover/mutation operations are replaced by language model generation.For S randomly selected molecules from the pool, we generate a new molecule using the prompt:</p>
<p>[SIMILAR]m if the best molecule (in terms of oracle score) has not improved for K iterations then 5. Take all the molecules from the P ool with their corresponding similar molecules (using which they have been generated), m i , (m i,1 , m i,2 , . . ., m i,S ), i = 1, . . ., P respectively.</p>
<p>train_samples i ← molecules2prompt((m i,1 , m i,2 , . . ., m i,S ), m i ), i = 1, . . ., P 6. Train LM on train_samples i , i = 1, . . ., P .end if until optim.problem stopping condition This approach allows for more intelligent exploration of the chemical space compared to traditional mutation operators.</p>
<p>Explicit oracle modeling Inspired by the rejection sampling technique [Bai et al., 2022, Touvron et al., 2023], we incorporate oracle feedback directly into the language model by fine-tuning on high-performing molecules.This is done using prompts of the form:
[PROPERTY]O(m)[/PROPERTY][START_SMILES]m smiles [END_SMILES]
This explicit modeling allows the language model to learn the relationship between molecular structure and oracle scores, enabling more targeted generation.</p>
<p>In-context learning</p>
<p>In early experiments we tried to use in-context learning during generation and fine-tuning by making our prompts shorter than the model's context length.This did not improve the results, and we abandoned the idea in further experiments.Note that there was no explicit training for in-context learning during the pretraining phase.</p>
<p>Algorithm 1 presents our complete optimization procedure, which includes initialization of an empty molecule pool, iterative generation of new molecules using the language model, evaluation of new molecules using the oracle function, updating the pool to maintain the top-P molecules, and periodic fine-tuning of the language model when progress stagnates.Algorithm 2 details our prompt construction process, which is crucial for effective molecule generation and model fine-tuning.</p>
<p>We employ a dynamic fine-tuning strategy to adapt the language model throughout the optimization process.Fine-tuning is triggered if the best molecule doesn't improve for K consecutive iterations, with the maximum number of fine-tuning rounds limited by the oracle budget.We use a learning rate scheduler with warm-up steps, and each fine-tuning step consists of multiple epochs with a portion of data reserved for validation to prevent overfitting.</p>
<p>Given the complexity of our algorithm, we adopt a focused hyperparameter tuning strategy, prioritizing the most sensitive parameters while keeping others fixed.This approach balances computational By combining these elements, our algorithm effectively leverages the power of large language models for molecular optimization, demonstrating strong performance across a range of tasks as detailed in Section 6.</p>
<p>Experiments</p>
<p>Practical Molecular Optimization</p>
<p>Problem formulation.Inspired by real-world molecular design setting Gao et al. [2022] propose a practical molecular optimization (PMO) benchmark consisting of 23 molecular optimization problems.PMO focuses on sample efficiency, generalizability to different optimization objectives, and robustness to hyperparameter selection of the molecular optimization algorithms.To assess the optimization ability and sample efficiency, Gao et al. [2022] put a limit on the number of oracle calls for each task to be 10000 and report the area under the curve (AUC) of the top-10 average property value versus the number of oracle calls as the performance metric.AUC values are calculated after every 100 oracle call, then combined and normalized to map the [0, 1] range.</p>
<p>Our approach.Using our proposed optimization algorithm we evaluate Chemlactica-125M, Chemlactica-1.3B and Chemma-2B models.The hyperparameters for the optimization algorithm are tuned for each model separately according to the hyperparameter tuning methodology.For this benchmark, we use the bfloat16 data type for the language model's parameters.</p>
<p>Results.Our method performs strongly, surpassing the existing approaches.Our algorithm powered by the smallest Chemlactica-125M model already improves over the state-of-the-art by a significant margin, with an AUC Top-10 of 17.170 (Chemlactica-125M) vs 16.213 (Genetic-guided GFlowNets).Additionally, strengthening the generator model improves the performance.Chemlactica-1.3B and Chemma-2B achieve AUC Top-10 of 17.284 and 17.534, respectively.For a more comprehensive Note that, unlike most of the other methods, our language models can leverage additional information about the oracle if the oracle internally calculates common molecular properties.These properties can be explicitly written in the prompts used in the optimization loop.In Appendix A.4 we show that such rich prompts can significantly improve the metrics on several PMO tasks.</p>
<p>Multi-property Optimization with Docking</p>
<p>Problem formulation.This benchmark, initially proposed in the REINVENT paper [Blaschke et al., 2020], evaluates a model's capability to generate viable molecules for practical drug discovery.Specifically, it assesses the model's ability to generate plausible molecules that optimize docking scores (minimize docking energy) against specified protein targets.The benchmark focuses on three targets with extensive real-world applications: the dopamine type 2 receptor (DRD2), MK2-kinase, and acetylcholinesterase.To ensure the generation of realistic molecules, the oracle reward function incorporates additional constraints, including the maximization of QED and a molecular weight limit of 500 Da.</p>
<p>The primary objective is to maximize the reward function with minimal oracle calls, emphasizing sample efficiency.We quantify this efficiency using two metrics: oracle burden and generative yield.Oracle burden measures the number of oracle calls required to generate N unique molecules above a predefined reward threshold.At the same time, generative yield represents the number of unique molecules generated above a reward threshold for a fixed number of oracle calls.To maintain consistency with recent implementations, we adopt the molecular preprocessing, conformational generation, docking parameters, and aggregate reward function from the Beam Enumeration paper [Guo and Schwaller, 2023b], specifically comparing our results with the beam structure 15 methods, which demonstrated superior average-case performance.</p>
<p>Results.We used the exact same hyperparameters as those selected in the PMO experiment.Table 6 presents our approach's performance on this benchmark, simulating real-world drug design scenarios.Chemma-2B consistently achieves the highest performance for the generative yield metric across all evaluated receptors.Conversely, Chemlactica-125M demonstrates superior performance in terms of oracle burden, except for MK2 at oracle burden 1, where Chemma outperforms it.Notably, Chemlactica-1.3B achieved even better yield scores on the DRD2 target.Appendix A.7 shows the set of molecules generated at the beginning and at the end of the optimization trajectory for DRD2 docking.</p>
<p>These results suggest that model size is crucial in balancing exploration and exploitation of the molecular space.Smaller models appear more adept at initial space exploration, while larger models excel in exploiting the reward space.This trade-off between oracle burden and generative yield could have significant implications for applied drug design, particularly when access to oracle functions is limited or costly.</p>
<p>Our findings validate the effectiveness of our approach, demonstrating that our models can leverage pre-training information and selective fine-tuning to optimize complex reward functions, even with limited data unseen during pre-training.Furthermore, the successful transfer of training parameters and sampling strategies from the molecular optimization benchmark to this task underscores our method's flexibility and robustness.This adaptability suggests that our approach could be particularly valuable in scenarios where extensive hyperparameter tuning is impractical or undesirable.</p>
<p>QED Maximization with Similarity Constrained Molecular Design</p>
<p>Problem formulation.The objective of this optimization problem is to generate a molecule that has a high QED and is similar to some given molecule.More formally, given a molecule M , the objective of the problem is to generate a new molecule M ′ such that sim(M ′ , M ) ≥ 0.4 and qed(M ′ ) ≥ 0.9.</p>
<p>Following Wang et al. [2023] 800 molecules are selected with QED in the range [0.7, 0.8] as the inputs to the optimization problem, and the performance is measured by the percentage of the molecules that have been optimized (satisfy the QED and similarity constraints).In addition, a maximum number of QED evaluations is chosen to optimize each lead molecule.</p>
<p>Our approach.Since this is a lead optimization problem, we add the lead molecule to all prompts in addition to the molecules added from the pool.The lead molecule is added by enclosing it in [SIMILAR] tag.For this task, we design an oracle function by combining the QED value of the generated molecule with the similarity value of the lead molecule and the generated molecule.Additionally, we decreased the maximum number of QED evaluations to 10000, compared to the baselines, which used 50000.</p>
<p>Results.For this task, we only evaluate the Chemlactica-125M model, which achieves better success rates compared to the best existing approaches, 99.0% (Chemlactica-125M) versus 94.6% (RetMol), while being constrained to use 5 times less QED evaluations at maximum.Since the performance of the Chemlactica-125M is very close to perfect, we have not evaluated other models for this task.Table 7 illustrates the performance of different algorithms.</p>
<p>Conclusion</p>
<p>This paper presents three language models: Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B.These models were trained on a novel corpus encompassing over 100 million molecules and their properties.We demonstrate the efficacy of these models on multiple tasks in chemistry research, with a particular focus on molecular optimization.Our proposed optimization algorithm combines the capabilities of language models with concepts from genetic algorithms.This approach has shown strong performance across various benchmarks, indicating its potential for addressing complex molecular design challenges.We publicly release our training corpus, pretrained models, optimization algorithm, and associated training recipes to support reproducibility and further research in this area.</p>
<p>While our work demonstrates promising results in molecular optimization and related tasks, we acknowledge that it represents an early step in applying language models to chemical research.We hope our contributions will provide a valuable foundation for future work in this domain, potentially enabling new molecular design and analysis approaches.</p>
<p>Limitations</p>
<p>The language models introduced in this paper operate only on SMILES representations and do not support 3D coordinates of atoms, limiting their reliability in scenarios where 3D conformation is critical.Furthermore, the models have very limited understanding of other biological entities like proteins, which constrains their practical applicability in certain areas of biochemistry and drug discovery.While effective, the optimization algorithms presented in this paper have not been exhaustively tuned, suggesting potential room for improvement.Additionally, our current approach does not account for synthetic accessibility or other practical considerations in drug design, which may limit its immediate applicability in real-world drug discovery pipelines.</p>
<p>Broader Impact</p>
<p>The molecular optimization models presented in this work have the potential for both positive and negative societal impacts.On the positive side, these models could significantly benefit the drug discovery and healthcare industries by accelerating the development of new therapeutic compounds.This acceleration may lead to faster responses to emerging health challenges and potentially reduce the cost of drug development.</p>
<p>However, as with many dual-use technologies, there is a risk that sufficiently advanced versions of these models could lower the barriers for malicious actors attempting to develop chemical or biological weapons.This risk underscores the importance of responsible development and deployment of such technologies.</p>
<p>Given these potential impacts, we recommend that future work in this area include rigorous evaluation of these algorithms and language models in designing potentially harmful substances to better understand and mitigate risks.Additionally, developing safeguards and ethical guidelines for using and disseminating molecular optimization models is crucial.Collaboration with experts in biosecurity and ethics will be essential to ensure that the development of these technologies proceeds in a manner that maximizes benefits while minimizing the potential for harm.</p>
<p>A Appendix</p>
<p>A.1 Hyperparameters Table 8 lists the hyperparameters we used for pretraining the language models.</p>
<p>For supervised fine-tuning we did a grid search over the following hyperparameters: peak learning rate, number of epochs, warmup steps and the amount of Neftune noise.Table 9 shows the best values for all tasks and models.Warmup steps are written as a ratio of the total training steps here.</p>
<p>Methodology for Hyperparameter Tuning of the Optimization Algorithm Given the large number of hyperparameters in our optimization algorithm, we adopt a two-step approach.First, we identify and freeze the hyperparameters that empirically show less sensitivity to the algorithm's performance.Then, we focus on tuning the more sensitive hyperparameters using grid search.</p>
<p>For tuning, we utilize the perindopril_mpo and zaleplon_mpo tasks from the PMO benchmark, following the methodology in [Gao et al., 2022].We report the AUC Top-10 metric from three independent runs with different seeds for each hyperparameter configuration.The best-performing configuration is then applied across all benchmarks in our evaluation.Notably, we tune the hyperparameters separately for Chemlactica-125M, Chemlactica-1.3B, and Chemma-2B to account for model-specific optimal settings.</p>
<p>A key hyperparameter, N , which determines the number of molecules generated before updating the pool, is set to 200.We employ vanilla temperature sampling for molecule generation throughout the optimization process.To address the need for generating thousands of unique molecules in many optimization benchmarks, we implement a dynamic temperature scheduling strategy.The sampling temperature starts at 1 and linearly increases to 1.5 as the number of oracle evaluations grows.This gradual temperature increase promotes the generation of more diverse molecules over time, reducing repetition and encouraging exploration of the chemical space.</p>
<p>Grid search.We perform grid search on P (pool size), S (number of similar molecules), K (finetuning tolerance level) and lr (fine-tuning peak learning rate) with the following grid:
• P = [10, 30, 50] • S = [0, 1, 2, 5] • K = [3, 5, 7] • lr = [10 −4 , 10 −5 ]</p>
<p>A.2 Detailed Results for Practical Molecular Optimization</p>
<p>Table 10 shows the evaluations of Chemlactica-125M, Chemlactica-1.3B and Gemma-2B, along with other methods on 23 tasks of the PMO benchmark.There is no method that uniformly beats all others The reason is that the oracle has a binary multiplier term that is usually equal to zero, so there is no supervision signal for the entire generation process.</p>
<p>A.3 Ablation Study on the Optimization Algorithm</p>
<p>A key component of our proposed optimization algorithm is the fine-tuning step, which is activated when the algorithm's progress stagnates.To assess the impact of this fine-tuning step, we conducted a comparative analysis of optimization processes both with and without this feature.For this evaluation, we selected four representative tasks from the PMO benchmark: jnk3, median1, sitagliptin_mpo, and scaffold_hop.These tasks were chosen to provide a diverse set of challenges and to be representative of the broader benchmark.</p>
<p>Table 11 presents the quantitative results of these experiments.To provide a more comprehensive understanding of the fine-tuning effect, we visualize the optimization trajectories in Figures 6 through 8.These visualizations aggregate data from five independent runs, offering insights into both the mean performance and its variance across different initializations.</p>
<p>This ablation study allows us to isolate the impact of the fine-tuning step and understand its contribution to the overall performance of our optimization algorithm across different types of molecular optimization tasks.</p>
<p>A.4 Leveraging Known Molecular Properties in Optimization Tasks</p>
<p>Our language models possess knowledge of various molecular properties such as QED, CLogP, and TPSA.However, we deliberately avoid utilizing this information in Algorithm 1 to maintain fair comparison with other methods.This decision stems from the fact that our models have been trained on properties that are components of the oracle functions we optimize against (e.g., those in PMO).</p>
<p>Exploiting this partial oracle information could potentially give our method an unfair advantage.</p>
<p>We conducted a separate set of experiments to explore the models' capacity to utilize additional information in solving optimization problems.We selected four tasks from the PMO benchmark: jnk3, median1, sitagliptin_mpo, and scaffold_hop.For these tasks, we modified Algorithm 2 to incorporate relevant known properties into the prompt p between steps 2 and 3.</p>
<p>Table 12 presents a performance comparison between our standard approach and this propertyaugmented version.The specific syntax used for adding these properties to the prompts is detailed in Table 13.Notably, no additional properties were added for the jnk3 task as our models lack specific knowledge about its oracle function.</p>
<p>The results demonstrate a significant performance improvement across all models when these additional properties are incorporated.This finding suggests that our models can effectively leverage their pre-existing knowledge of molecular properties to enhance their performance in molecular design tasks.However, it's important to note that while this approach showcases the potential of our      Challenges in Batched Generation Molecular optimization pipelines require repeated model calls for generation, followed by oracle function scoring.While batched processing accelerates this process through GPU parallelization, it introduces complications.The necessary padding for batch processing alters matrix sizes, affecting multiply-accumulate operations within the model.These small errors accumulate as they propagate through the model's layers.Lower precision exacerbates these errors, leading to larger discrepancies in logit values and, consequently more significant impacts on the generated molecules.</p>
<p>Cascading Effects of Sub-optimal Generations In our approach, high-scoring generated molecules are used for both additional fine-tuning and identifying similar molecules to guide optimization.However, when lower precision leads to sub-optimal molecule generation, it creates a negative feedback loop.The model is fine-tuned on and guided by these lower-quality molecules, hindering the generation of higher-scoring molecules in subsequent iterations.This causal relationship between successive generations underlies the particularly adverse effects of low precision in molecular optimization pipelines.</p>
<p>Precision Ablation Study To quantify the impact of numerical precision on the optimization process, we conducted an ablation study comparing 32-bit floating point precision with bfloat16 precision.Table 14 presents the results of this comparison across all drug discovery case studies described in Section 6.2.Despite the potential computational costs, these results demonstrate the critical importance of maintaining higher numerical precision in molecular optimization tasks.</p>
<p>A.6 Visualization of the Model Outputs on Property Prediction and Conditional Generation Tasks</p>
<p>[</p>
<p>WEIGHT]180.16[/WEIGHT][TPSA]63.60[/TPSA][CLOGP]1.31[/CLOGP][START_SMILES]CC(=O)OC1=CC=CC=C1C(=O)O[END_SMILES] [SAS]1.58[/SAS][QED]0.92[/QED][SIMILAR]O=C(Oc1ccccc1C(=O)O)c1ccccc1O 0.59[/SIMILAR] [SYNONYM]aspirin[/SYNONYM] [PROPERTY]Vapor Pressure 2.52X10-5 mm Hg at 25 °C (calc)[/PROPERTY] [CID]2244[/CID]</p>
<p>Figure 1 :
1
Figure 1: Model calibration on synthetic multiple choice question where y=x represents perfect calibration.</p>
<p>m 1 , m 2 , . . ., m S ), m 1. Check if the outcome should be a molecule generation prompt or a training sample.if m is null then 1.1.Sample similarity values for molecules in the prompt, desirable oracle score and set the suffix for a molecule generation.v sim i ∼ U(0.4,0.9), i = 1, . . ., S v max ← the maximum oracle score achieved at this moment v prop ∼ U(v max , oracle_max) suf f ix ← [START_SMILES] else 1.3.Compute the correct similarity values for the molecules in the prompt and the correct oracle score, set the suffix for a training sample.v sim i = similar(m i , m), i = 1, . . ., S v prop = O(m) suf f ix ← [START_SMILES]m smiles [END_SMILES]eos end if 2. Concatenate all molecules in the prompt with their similarity values.p ← bos[SIMILAR]m smiles 1 if at least one fine-tuning has been performed then 2.1.Add the oracle score to the prompt.p ← concat(p, [PROPERTY]v prop [/PROPERTY]) end if 3. Add the appropriate suffix.return concat(p, suf f ix) efficiency with optimization performance.Detailed methodology and results of our hyperparameter tuning experiments are provided in Appendix A.1.</p>
<p>Figures</p>
<p>Figures 2e-2eshow the performance of Chemma-2B for property prediction and conditional molecular generations tasks.Each dot in the scatter plot corresponds to one molecule.The histogram in the background is the actual distribution of those properties in the database.The purple line shows RMSE error for the given value of the property.</p>
<p>(greedy sampling) 0/100 invalid SMILES, 7/100 from PubChem rmse 0.415 rmse_c 0.415 mape 0.105 corr: 0.786 (c) SAS-conditioned generation of molecules.(greedy sampling) 0/100 invalid SMILES, 15/100 from PubChem rmse 6.942 rmse_c 6.942 mape 0.054 corr: 0.985 (d) TPSA-conditioned generation of molecules.invalid SMILES rmse 0.140 mape 0.234 corr: 0.874 (f) Similarity-conditioned generation of molecules.</p>
<p>Figure 2 :
2
Figure 2: Illustration of errors made by Chemma-2B during property prediction and conditional generation for various properties.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Optimization process visualization using Chemlactica-125M model for sitagliptin_mpo task with four different seeds.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Optimization process visualization using Chemma-2B model for sitagliptin_mpo task with four different seeds.</p>
<p>Table 1 :
1
RMSE (RSME corrected for mean) ↓ for Property Prediction and Conditional Generation for different tasks and models.
QEDSIMSASPPCGPPCGPPCGChemlactica-125M 0.016 0.101 (0.108) 0.0460.1830.0780.315 (0.379)Chemlactica-1.3B0.004 0.050 (0.050) 0.0430.1670.0660.400 (0.400)Chemma-2B-2.1B0.016 0.100 (0.100) 0.0490.1260.0730.384 (0.382)Chemma-2B-39B0.004 0.075 (0.075) 0.0460.1400.0370.415 (0.415)CLOGPTPSAWEIGHTPPCGPPCGPPCGChemlactica-125M 0.106 0.568 (0.568) 1.322 5.216 (5.244) 9.350 30.276 (30.276)Chemlactica-1.3B0.100 0.405 (0.405) 0.893 5.543 (15.640) 3.576 16.877 (16.877)Chemma-2B-2.1B0.137 1.675 (1.675) 1.638 7.077 (7.077) 8.962 39.695 (41.109)Chemma-2B-39B0.034 0.461 (0.461) 0.959 6.942 (6.942) 1.931 18.933 (20.395)4.2 Model Calibration4.2.1 Methodology</p>
<p>Table 2 :
2
Ablation study on Conditional Generation hyperparameters.Each row represents one combination of Chain-of-Thought (CoT), repetition penalty (rep.), and suppression (supp.).All experiments are done on the molecular weight prediction task.
Chemlactica-125MChemlactica-1.3BChemma-2BCoT rep. supp.RMSE (c) ↓ Invalids ↓RMSE (c) ↓ Invalids ↓RMSE (c) ↓ Invalids ↓No 1.0No70.02 (70.02)0/10015.41 (65.22)1/10016.56 (65.58)1/100No 1.0No70.11 (70.11)0/10015.81 (65.32)1/10012.15 (64.54)1/100Yes 1.0No 112.52 (112.52)0/100 187.26 (187.26)0/100 198.48 (191.89)46/100Yes 1.010 No82.28 (82.28)0/100 137.19 (137.19)0/100 170.02 (170.02)0/100Yes 1.0 Yes33.46 (33.46)0/10018.53 (25.22)1/10031.98 (31.85)1/100Yes 1.005 Yes34.52 (34.52)0/10017.14 (17.14)0/10029.71 (29.71)0/100Yes 1.010 Yes30.27 (30.27)0/10016.87 (16.87)0/10018.93 (20.39)1/100Yes 1.015 Yes30.27 (30.27)0/10018.07 (19.61)1/10018.99 (20.44)1/100Yes 1.020 Yes31.17 (31.17)1/10016.33 (18.03)1/10024.16 (25.27)1/100Yes 1.050 Yes45.38 (45.38)1/10016.49 (34.48)1/100 74.78 (130.11)63/100Yes 1.100 Yes35.20 (35.20)0/10016.61 (32.37)1/100 740.28 (488.73)59/1001.01750000.2 0.4 0.6 0.8 P(correct)25000 50000 75000 100000 125000 150000 Number of Occurences0.0Bin Ranges (0.3, 0.4] (0.2, 0.3] (0.1, 0.2] (-0.001, 0.1] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]0</p>
<p>Table 3 :
3
Regression tasks from MoleculeNet, all values are RMSE ↓.
ESOLFreeSolvLipophilicityAvgMoleculeNet GC0.9701.4000.6551.008Chemformer0.6331.2300.5980.820MoLFormer-XL0.2790.2310.5290.346GROVER large0.8311.5440.5600.978MolCLR1.1102.2000.6501.320iMolCLR1.1302.0900.6401.287BARTSmiles0.3080.3380.5400.395Chemlactica-125M 0.270 ± 0.011 0.306 ± 0.011 0.533 ± 0.009 0.369 ± 0.000Chemlactica-1.3B0.281 ± 0.005 0.356 ± 0.009 0.557 ± 0.021 0.403 ± 0.013Chemma-2B0.298 ± 0.014 0.359 ± 0.040 0.563 ± 0.004 0.406 ± 0.012</p>
<p>Table 4 :
4
Regression tasks from the ADMET benchmark.All numbers are Pearson correlation ↑.
HLMMDR1-MDCK ERSolubilityMPNN2 (from the original paper)0.680.780.59Chemlactica-125M0.68 ± 0.0110.77 ± 0.0120.57 ± 0.035Chemlactica-1.3B0.68 ± 0.0040.77 ± 0.0090.54 ± 0.043Chemma-2B0.67 ± 0.0040.78 ± 0.0090.53 ± 0.024RLMhPPBrPPBMPNN2 (from the original paper)0.740.770.70Chemlactica-125M0.71 ± 0.0040.73 ± 0.0040.60 ± 0.098Chemlactica-1.3B0.65 ± 0.0040.74 ± 0.0010.62 ± 0.017Chemma-2B0.68 ± 0.0050.75 ± 0.0040.60 ± 0.030
Fang et al. [2023b] the results for three regression tasks from MoleculeNet[Wu et al., 2018].Fang et al. [2023b]introduces a new dataset for six ADMET targets.The authors provided training/test split but no validation set.We used a random 20% of the training set as a validation set to pick the best hyperparameters.Table4shows the results.</p>
<p>Initialize an empty P ool ← {} repeat 1. Generate prompts for molecule generation.for i = 1 to N do (m i,1 , m i,2 , . . ., m i,S ) ← random_subset(P ool) p i ← molecules2prompt((m i,1 , m i,2 , . . ., m i,S ), null) end for 2. Generate N new and unique molecules with the language model.m i ← LM (p i ), i = 1, . . ., N 3. Update the pool with m i s and keep only the top-P molecules.P ool ← P ool ∪ {m 1 , . . ., m N } P ool ← top-P (P ool) 4. Fine-tune if necessary.</p>
<p>smiles 1 0.8[/SIMILAR]...[SIMILAR]m smiles S 0.8[/SIMILAR][START_SMILES] Algorithm 1 molecular_optimization Input: P , S, N , K</p>
<p>Table 5 :
5
Kim et al. [2024] [2023a]ctica-125M, Chemlactica-1.3B and Chemma-2B in comparison with other methods.REINVENT results are taken fromGao et al. [2022], Augmented memory is taken fromGuo and Schwaller [2023a], and Genetic-guided (GG) GFlowNets are taken fromKim et al. [2024].Values are the average of 5 runs with different seeds, metric is Top-10 AUC ↑ ±
standard deviationjnk3median1scaffold_hop sitagliptin_mposum of 4sum of 23REINVENT0.783 ± 0.023 0.356 ± 0.009 0.560 ± 0.019 0.021 ± 0.0031.72014.196Augmented memory 0.739 ± 0.110 0.326 ± 0.013 0.567 ± 0.008 0.284 ± 0.0501.91615.002GG GFlowNets0.764 ± 0.069 0.379 ± 0.010 0.615 ± 0.100 0.634 ± 0.0392.39216.213Chemlactica-125M0.881 ± 0.058 0.359 ± 0.060 0.626 ± 0.016 0.649 ± 0.051 2.515 ± 0.119 17.170 ± 0.424Chemlactica-1.3B0.866 ± 0.021 0.382 ± 0.047 0.673 ± 0.080 0.586 ± 0.062 2.506 ± 0.155 17.284 ± 0.284Chemma-2B0.891 ± 0.032 0.382 ± 0.022 0.669 ± 0.110 0.613 ± 0.018 2.555 ± 0.099 17.534 ± 0.214understanding of the optimization dynamics, Figures 3-5 illustrate visualizations of the optimizationprocesses for sitagliptin_mpo task with different seeds for different models.</p>
<p>Table 6 :
6
Drug discovery case studies via docking function reward optimization.All experiments were run with a maximum oracle budget of 5000 oracle calls.Note that both oracle burden and generative yield values are reward-threshold dependent, and mean values from the reported baseline works are reported.The parentheses for oracle burden indicate how many unique molecules need to be generated for consideration.The best performance on each task-metric combination is bolded.Note that the hyperparameters of our models are not tuned for this task; instead, we used the best-performing hyperparameters on the PMO benchmark.
MetricTargetReinventBeam Chemlactica ChemlacticaChemmaBaseline Structure 15125M1.3B2BGenerative Yield 0.7 ↑DRD21879 ± 16 3474 ± 1583733 ± 512 3659 ± 2883848 ± 98MK2879 ± 10 3127 ± 1383772 ± 578 3660 ± 535 3578 ± 452AChE2437 ± 53 3824 ± 1624108 ± 67 4193 ± 128 4092 ± 284DRD2102 ± 6 1780 ± 4392827 ± 510 2621 ± 614 2985 ± 194Generative Yield 0.8 ↑MK22 ± 0987 ± 211 2569 ± 1156 2216 ± 522 1058 ± 465AChE147 ± 11 2059 ± 3273246 ± 168 3652 ± 349 3096 ± 372DRD2168 ± 149126 ± 9020 ± 2911 ± 1074 ± 62Oracle burden 0.8 (1) ↓MK21724 ± 802736 ± 166345 ± 31278 ± 125189 ± 278AChE83 ± 29105 ± 2922 ± 2815 ± 2374 ± 72DRD2883 ± 105582 ± 83114 ± 08160 ± 130240 ± 11Oracle burden 0.8 (10) ↓MK2Failed 1122 ± 154493 ± 418248 ± 261440 ± 548AChE481 ± 108462224 ± 1791 ± 103168 ± 94DRD24595 ± 01120 ± 25364 ± 119430 ± 250518 ± 41Oracle burden 0.8 (100) ↓MK2Failed 2189 ± 181865 ± 533486 ± 346934 ± 918AChE 3931 ± 286 1110 ± 265497 ± 58333 ± 131433 ± 143</p>
<p>Table 7 :
7
Performance comparison of different algorithms on QED and Similarity constrained molecular optimization problem.
Success Rate (%) ↑QMO92.8RetMol94.5Chemlactica-125M99.0</p>
<p>Table 8 :
8
Hyperparameters of our language models.All cross-entropy losses use mean reduction.None of our (and many other) methods get non-zero result on valsartan_smarts.
Chemlactica-125M Chemlactica-1.3B Chemma-2BPeak learning rate1.4e-31.0e-41.0e-3Warmup steps500500500Context length204820482048ADAM β 10.90.90.9ADAM β 20.950.950.95ADAM ϵ1e-81e-81e-8Weight Decay0.10.10.1Dropout0.10.1NoneAttention Dropout0.10.1NonePrecisionMixedMixedBF16Loss FunctionCE LossCE LossCE LossVocabulary Size5006650066256000Gradient Clipping1.01.01.0</p>
<p>Table 9 :
9
Selected hyperparameters for property prediction tasks as a result of the grid search.We report learning rate (LR), warmup ratio (WU), number of epochs (Ep.) and Neftune noise (Nef.).
Chemlactica-125MChemlactica-1BChemma-2BTaskLRWU Ep. Nef.LRWU Ep. Nef.LRWU Ep. Nef.</p>
<p>Table 10 :
10
Comparision of different methods on PMO.The values represent the AUC Top-10 ↑ metric averaged over five independent runs with different seeds.
OracleREINVENTAugmentedGeneticChemlacticaChemlacticaChemmaMemoryGFN125M1.3B2Balbuterol_similarity0.882 ± 0.006 0.913 ± 0.009 0.949 ± 0.010 0.951 ± 0.0110.947 ± 0.0120.951 ± 0.009amlodipine_mpo0.635 ± 0.035 0.691 ± 0.047 0.761 ± 0.019 0.772 ± 0.0910.769 ± 0.0830.766 ± 0.107celecoxib_rediscover0.713 ± 0.067 0.796 ± 0.008 0.802 ± 0.029 0.906 ± 0.0460.911 ± 0.0130.920 ± 0.011deco_hop0.666 ± 0.044 0.658 ± 0.024 0.733 ± 0.109 0.801 ± 0.1010.836 ± 0.1170.831 ± 0.123drd20.945 ± 0.007 0.963 ± 0.006 0.974 ± 0.006 0.965 ± 0.0070.968 ± 0.0050.972 ± 0.006fexofenadine_mpo0.784 ± 0.006 0.859 ± 0.009 0.856 ± 0.039 0.881 ± 0.0310.891 ± 0.0390.931 ± 0.014gsk30.865 ± 0.043 0.881 ± 0.021 0.881 ± 0.042 0.926 ± 0.0220.916 ± 0.0270.928 ± 0.021isomers_c7h8n2o20.852 ± 0.036 0.853 ± 0.087 0.969 ± 0.003 0.951 ± 0.0120.933 ± 0.0170.947 ± 0.009isomers_c9h10n2o2pf2cl 0.642 ± 0.054 0.736 ± 0.051 0.897 ± 0.007 0.927 ± 0.0060.929 ± 0.0120.914 ± 0.017jnk30.783 ± 0.023 0.739 ± 0.110 0.764 ± 0.069 0.881 ± 0.0580.866 ± 0.0210.891 ± 0.032median10.356 ± 0.009 0.326 ± 0.013 0.379 ± 0.010 0.359 ± 0.0600.382 ± 0.0470.382 ± 0.022median20.276 ± 0.008 0.291 ± 0.008 0.294 ± 0.007 0.328 ± 0.0320.329 ± 0.0160.366 ± 0.018mestranol_similarity0.618 ± 0.048 0.750 ± 0.049 0.708 ± 0.057 0.896 ± 0.0640.850 ± 0.0510.926 ± 0.023osimertinib_mpo0.837 ± 0.009 0.855 ± 0.004 0.860 ± 0.008 0.907 ± 0.0150.892 ± 0.0130.879 ± 0.016perindopril_mpo0.537 ± 0.016 0.613 ± 0.015 0.595 ± 0.014 0.709 ± 0.0520.755 ± 0.0660.711 ± 0.062qed0.941 ± 0.000 0.942 ± 0.000 0.942 ± 0.000 0.942 ± 0.0000.942 ± 0.0000.941 ± 0.000ranolazine_mpo0.760 ± 0.009 0.801 ± 0.006 0.819 ± 0.018 0.864 ± 0.0140.883 ± 0.0170.868 ± 0.015scaffold_hop0.560 ± 0.019 0.567 ± 0.008 0.615 ± 0.100 0.626 ± 0.0160.673 ± 0.0800.669 ± 0.110sitagliptin_mpo0.021 ± 0.003 0.284 ± 0.050 0.634 ± 0.039 0.649 ± 0.0510.586 ± 0.0620.613 ± 0.018thiothixene_rediscovery 0.534 ± 0.013 0.550 ± 0.041 0.583 ± 0.034 0.624 ± 0.1020.693 ± 0.1190.698 ± 0.121troglitazone_rediscovery 0.441 ± 0.032 0.540 ± 0.048 0.511 ± 0.054 0.734 ± 0.1300.765 ± 0.1380.824 ± 0.049valsartan_smarts0.178 ± 0.358 0.000 ± 0.000 0.135 ± 0.271 0.000 ± 0.0000.000 ± 0.0000.000 ± 0.000zaleplon_mpo0.358 ± 0.062 0.394 ± 0.026 0.552 ± 0.033 0.569 ± 0.0470.569 ± 0.0200.608 ± 0.055sum14.19615.00216.21317.170 ± 0.424 17.284 ± 0.284 17.534 ± 0.214</p>
<p>Table 11 :
11
Illustration of the results of ablation study on the fine-tuning step in the optimization algorithm.The values represent AUC Top-10 ↑ obtained from five independent runs.</p>
<p>Table 13 :
13
The descriptions of tasks used in the prompts in the extended version of our optimization algorithm.The results are in Table12.See Section A.4 for details.
the syntax of additional properties added to the promptsjnk3(nothing added)median1[SIMILAR]camphor_smiles 0.55[/SIMILAR][SIMILAR]menthol_smiles 0.55[/SIMILAR]scaffold_hop[SIMILAR]pharmacophor_smiles 0.80[/SIMILAR]sitagliptin_mpo [SIMILAR]sitagliptin_smiles 0.99[/SIMILAR][CLOGP]2.02[/CLOGP][TPSA]77.04[/TPSA]</p>
<p>Table 14 :
14
Impact of numerical precision on multi-property optimization with docking task.
MetricTarget Chemlactica-125M Chemlactica-125MBF16 FP32Generative Yield 0.7 ↑DRD23501 ± 2523733 ± 512MK23000 ± 803772 ± 578AChE4337 ± 1334108 ± 67DRD22574 ± 1032827 ± 510Generative Yield 0.8 ↑MK21223 ± 5192569 ± 1156AChE3877 ± 2723246 ± 168DRD2156 ± 10020 ± 29Oracle burden 0.8 (1) ↓MK2320 ± 83345 ± 312AChE10 ± 822 ± 28DRD2283 ± 61114 ± 08Oracle burden 0.8 (10) ↓MK2631 ± 100493 ± 418AChE123 ± 119224 ± 17DRD2577 ± 71364 ± 119Oracle burden 0.8 (100) ↓MK21134 ± 178865 ± 533AChE350 ± 137497 ± 58
AcknowledgementsWe would like to thank Garik Petrosyan and Zaven Navoyan for insightful discussions.We appreciate Nebius.aifor granting us access to their GPU cloud and providing excellent support.Philipp Guevorguian's research is supported by the Yandex Armenia fellowship.Chemlactica-125MChemlactica-1.3B Chemma-2B fine-tuning no fine-tuning fine-tuning no fine-tuning fine-tuning no fine-tuning jnk3 0.881 ± 0.058 0.878 ± 0.040 0.866 ± 0.021 0.867 ± 0.036 0.891 ± 0.032 0.869 ± 0.033 median1 0.359 ± 0.060 0.371 ± 0.006 0.382 ± 0.047 0.395 ± 0.027 0.382 ± 0.022 0.380 ± 0.034 scaffold_hop 0.626 ± 0.016 0.648 ± 0.017 0.673 ± 0.080 0.721 ± 0.121 0.669 ± 0.110 0.700 ± 0.122 sitagliptin_mpo 0.649 ± 0.051 0.607 ± 0.051 0.586 ± 0.062 0.576 ± 0.082 0.613 ± 0.018 0.563 ± 0.059 sum 2.515 ± 0.119 2.504 ± 0.068 2.506 ± 0.155 2.559 ± 0.062 2.555 ± 0.099 2.512 ± 0.160 models, it may not provide a fair comparison with methods that don't have access to such property information.A.5 The Impact of Floating Point Precision on Molecular OptimizationNumerical Precision in Model Training Lower precision training, including mixed and halfprecision methods, is commonly used to increase training throughput.These techniques, employed during our models' pretraining stages, typically have negligible impact on performance and may even provide a regularizing effect.However, in the context of molecular optimization involving multiple rounds of fine-tuning, lower numerical precision leads to significantly degraded performance.Several factors contribute to this phenomenon in the specific case of molecular optimization with language models.Table12: The performance of the extended version of our optimization algorithm on selected PMO tasks.The prompts used in the optimization contain the description of the tasks in the format our language models has seen during pretraining.See Table13for the additional tags used in the prompts.
Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Flow network based generative models for non-iterative diverse candidate generation. E Bengio, M Jain, M Korablyov, D Precup, Y Bengio, Advances in Neural Information Processing Systems. 202134</p>
<p>Reinvent 2.0: an ai tool for de novo drug design. T Blaschke, J Arús-Pous, H Chen, C Margreitter, C Tyrchan, O Engkvist, K Papadopoulos, A Patronov, Journal of chemical information and modeling. 60122020</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, arXiv:2005.141652020arXiv preprint</p>
<p>Evoprompting: Language models for code-level neural architecture search. A Chen, D Dohan, D R So, ArXiv, abs/2302.148382023257232765</p>
<p>Bartsmiles: Generative masked language models for molecular representations. G Chilingaryan, H Tamoyan, A Tevosyan, N Babayan, K Hambardzumyan, Z Navoyan, A Aghajanyan, H Khachatrian, L Khondkaryan, 10.1021/acs.jcim.4c00512Journal of Chemical Information and Modeling. 2024</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. T Dao, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Translation between molecules and natural language. C N Edwards, T Lai, K Ros, G Honke, H Ji, ArXiv, abs/2204.118172022248376906</p>
<p>Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective. C Fang, Y Wang, R Grater, S Kapadnis, C Black, P Trapa, S Sciabola, Journal of Chemical Information and Modeling. 63112023a</p>
<p>Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective. C Fang, Y Wang, R Grater, S Kapadnis, C Black, P Trapa, S Sciabola, Journal of Chemical Information and Modeling. 63112023b</p>
<p>Sample efficiency matters: A benchmark for practical molecular optimization. W Gao, T Fu, J Sun, C W Coley, ArXiv, abs/2206.124112022</p>
<p>Augmented memory: Capitalizing on experience replay to accelerate de novo molecular design. J Guo, P Schwaller, ArXiv, abs/2305.161602023a</p>
<p>Beam enumeration: Probabilistic explainability for sample efficient selfconditioned molecular design. J Guo, P Schwaller, ArXiv, abs/2309.139572023b</p>
<p>Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. Q Guo, R Wang, J Guo, B Li, K Song, X Tan, G Liu, J Bian, Y Yang, T University, M Research, ArXiv, abs/2309.085322023</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>N Jain, P -Y. Chiang, Y Wen, J Kirchenbauer, H.-M Chu, G Somepalli, B R Bartoldson, B Kailkhura, A Schwarzschild, A Saha, arXiv:2310.05914Noisy embeddings improve instruction finetuning. 2023arXiv preprint</p>
<p>A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. J H Jensen, Chemical science. 10122019</p>
<p>N S Keskar, B Mccann, L R Varshney, C Xiong, R Socher, arXiv:1909.05858Ctrl: A conditional transformer language model for controllable generation. 2019arXiv preprint</p>
<p>Genetic-guided gflownets: Advancing in practical molecular optimization benchmark. H.-S Kim, M Kim, S Choi, J Park, abs/2402.059612024</p>
<p>Pubchem substance and compound databases. S Kim, P A Thiessen, E E Bolton, J Chen, G Fu, A Gindulyte, L Han, J He, S He, B A Shoemaker, J Wang, B Yu, J Zhang, S H Bryant, Nucleic Acids Research. 442015</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Auto-encoding variational bayes. P Kingma, M Welling, CoRR, abs/1312.61142013</p>
<p>Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. G Landrum, 2013</p>
<p>Molecular de-novo design through deep reinforcement learning. M Olivecrona, T Blaschke, O Engkvist, H Chen, Journal of Cheminformatics. 929783112017</p>
<p>OpenAI. Gpt-4 technical report. 2023</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 201932</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Retrieval-based controllable molecule generation. Z Wang, W Nie, Z Qiao, C Xiao, R Baraniuk, A Anandkumar, International Conference on Learning Representations. 2023</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 2811988</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 922018</p>
<p>Pytorch fsdp: Experiences on scaling fully sharded data parallel. Y Zhao, A Gu, R Varma, L Luo, C.-C Huang, M Xu, L Wright, H Shojanazeri, M Ott, S Shleifer, A Desmaison, C Balioglu, P Damania, B Nguyen, G Chauhan, Y Hao, A Mathews, S Li, 10.14778/3611540.3611569Proc. VLDB Endow. VLDB Endowaug 202316</p>
<p>Cc1cccc(N2CCC3(CCC3C (=O)Nc3nnc(CC4CCCCC4 )s3)C2=O). 1</p>
<p>COCC1C(C(=O)Nc2nnc(C 3CCCCC3)s2)CC(=O)N1c. 1ccccn1 Score: 0.7572</p>
<p>O=C(Nc1nnc(CC2CCCCC2 )s1)C1CCc2c(ncccc2=O )C1. Score: 0.8523</p>
<p>=O)NCc2c cc3c(c2)OCO3)c2c1C=C (F)C(C. A.7.2 MK2 CO.Cc1ccc(CC2</p>
<p>)(C)=O) c(Nc2nc3ccccc3nc2NS( =O)(=O)c2cn(C)cn2)c1. A.7.3 AChE CO.Cc1ccc(P(CC2Cl Score: 0.0000 I.O=C1CC(NC(=O</p>
<p>C=CC3c4ccccc4C=CC23) =NCN1 Score: 0.0000 COCc1c2ccc(C3OCCO3)c c2c(C)c2cc(S(=O). =O</p>
<p>N3CCC4(CC3)OC(=O). C4</p>
<p>O=C(CCC1=CC=C2C(c3cc ccc3)=C3C(=CCc4ccccc 43)C2S1)NC1=NCCO1 Score: 0.7877 Cc1ccc2c(C)c(S(=O)(= O)C3CCC4(CC3)CC. 4</p>
<p>Cc1ccc2cc(S(=O)(=O)C 3(O)CCC4(CC3)NC. Score: 0.79754</p>
<p>=O)(=O)c3cc c4cc(C)ccc4c3C)=CC1). Score: 0.8045 C=C1NC(=O)CC12[CH]CC 1(C=CC(S(</p>
<p>. C( =o, Score: 0.8255C2</p>
<p>Score: 0.8295CC1CN=C(NC(=O)C2CCc3 ccccc3C23c2ccccc2C3C )O1 Score: 0.8338 Cc1ccc2cc(S(=O)(=O)C 3CCC4(CC3)NC(=O)OC43 C=C3. </p>
<p>Score: 0.8412Cc1cc(N)c2cc(S(=O). </p>
<p>O)c3=ccc4 , C=C3)CC(=O )NC4C. </p>
<p>CSC1=NC(NC(=O)Cc2ccc c(-c3cccc4c3C3=CC=C4C3) c2)COC1 Score: 0.8572 Cc1ccc2c(C)c(S(=O). </p>
<p>. O)c3=ccc4 , CC3)NC(=O) CC43C=C3)ccc2c1</p>            </div>
        </div>

    </div>
</body>
</html>
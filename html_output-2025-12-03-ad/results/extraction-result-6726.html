<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6726 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6726</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6726</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-dac3a172b504f4e33c029655e9befb3386e5f63a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a" target="_blank">Emergent Abilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Trans. Mach. Learn. Res.</p>
                <p><strong>Paper TL;DR:</strong> This paper discusses an unpredictable phenomenon that is referred to as emergent abilities of large language models, an ability to be emergent if it is not present in smaller models but is present in larger models.</p>
                <p><strong>Paper Abstract:</strong> Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6726.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6726.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits explicit intermediate reasoning steps from a language model before producing a final answer, enabling multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>68B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math word problems (BIG-Bench / assorted math datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step arithmetic / math word-problem reasoning requiring intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard prompting without intermediate steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports that CoT only yields a positive performance gap over standard prompting when scaled to ~1.3e23 training FLOPs (~68B params for LaMDA); prior smaller models do not get benefit. Authors note emergence: the method is only effective above a scale threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6726.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting variant that elicits chain-of-thought style reasoning in a zero-shot setting (no few-shot exemplars), often by adding a phrase like 'Let's think step by step'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Arithmetic / reasoning benchmarks (e.g., BIG-Bench variants)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning elicited in zero-shot prompts to generate intermediate steps and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard zero-shot prompting (no CoT cue)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper cites Kojima et al. (2022) showing zero-shot CoT emerges at large scale (table entry: ~3.1e23 FLOPs, 175B for GPT-3). The surveyed paper emphasizes that zero-shot CoT's effectiveness is scale-dependent (emergent).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6726.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style decoding method that samples multiple chain-of-thought reasoning traces and aggregates final answers (e.g., by majority vote) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>68B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Math / reasoning benchmarks (chain-of-thought tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve accuracy on multi-step reasoning by sampling diverse reasoning paths and aggregating answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer) / solve rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>single chain-of-thought greedy decoding</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports self-consistency shows emergent benefit at ~1.3e23 FLOPs (68B). Method operates by producing diverse reasoning traces and combining them; the paper frames this as leveraging diversity of reasoning traces to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6726.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scratchpad / Intermediate-Output Fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning or prompting approach that trains models to emit intermediate computation steps (a scratchpad) which improves multi-step program-like computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Show your work: Scratchpads for intermediate computation with language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (or models evaluated by Nye et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>40M</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Scratchpad (intermediate outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>8-digit addition (program-execution arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Execute multi-step arithmetic/program-like computation requiring stepwise intermediate results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>solve rate / exact-match on numeric output</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard fine-tuning without scratchpad / direct-output model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports that enabling scratchpad yields benefit only for models of ~8.9e19 FLOPs (40M params) or larger; i.e., scratchpad is emergent above a small scale. Survey highlights scratchpad helps program execution and multi-step computation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6726.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that decomposes a complex problem into an ordered sequence of simpler subproblems solved sequentially, then composes subanswers for the final solution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Complex reasoning tasks (various chain-of-thought benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decompose and solve multi-step reasoning tasks by ordering subproblems from easiest to hardest.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / solve rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey lists least-to-most prompting as an augmented prompting method that is observed to be effective at large model scales (table: emergence at ~3.1e23 FLOPs, 175B). The paper frames it as a decomposition strategy enabling complex reasoning when scale permits.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6726.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-Finetuning (FLAN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Finetuning (FLAN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning a language model on a mixture of tasks phrased as natural-language instructions to enable better zero-shot or few-shot instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finetuned language models are zero-shot learners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>68B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Instruction-following Finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>conditioning / fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multi-task / instruction-following evaluations (generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Respond appropriately to novel tasks described by instructions without exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>aggregate task accuracy / task-specific metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>no instruction finetuning (base pre-trained model)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports instruction-finetuning harmed performance for models <=7e21 FLOPs (~8B) and only improved performance when scaled to ~1e23 FLOPs (~100B / table reports 68B for FLAN). The paper also notes subsequent work (Sanh et al.) induced instruction-following in smaller encoder-decoder models, showing architecture and data matter.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6726.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Calibration P(True)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Calibration via P(True) (True/False technique)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A calibration technique where the model first proposes an answer then separately estimates the probability that its proposed answer is true, used to better predict when it will answer correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models (mostly) know what they know.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Anthropic model (as reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>52B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>P(True) calibration</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>post-hoc calibration / confidence estimation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Calibration evaluation (general QA calibration tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure whether models can predict correctness of their own answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>calibration effectiveness (e.g., calibration error, superior method frequency)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard calibration methods (probability of correct answer among options)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey cites Kadavath et al. (2022) showing that P(True) technique's superiority only emerges at largest model scale (~2.6e23 FLOPs, 52B params).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6726.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-book / Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-book / Retrieval-Augmented Use of External Knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting model responses with explicit retrieval or open-book knowledge to improve factuality and fact-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scaling language models: Methods, analysis & insights from training Gopher.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7.1B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Open-book / retrieval-augmented prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Fact-checking / open-book QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use an external knowledge source at inference to verify or produce factual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / fact-checking success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>closed-book baseline (no retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey notes that using open-book knowledge helps for fact checking and that benefit emerges at ~1.3e22 FLOPs (7.1B Gopher). This is an example of augmenting model with external memory to enable tasks at smaller scale than would otherwise be needed.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6726.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Differentiable Search Index</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer Memory as a Differentiable Search Index</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that treats transformer memory as a learnable differentiable index to retrieve relevant contexts, effectively a learned retrieval mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformer memory as a differentiable search index.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Differentiable Search Index</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>retrieval-based / memory-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Tasks requiring long-range context / retrieval-augmented performance (various)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Leverage learned memory/index for lookup of relevant information to assist generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>task-specific accuracy / retrieval-augmented metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>standard model without differentiable index</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey lists differentiable search index emergence at ~3.3e22 FLOPs (11B T5). This is presented as an augmented prompting/architecture strategy to improve capabilities beyond pure scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6726.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Leveraging Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Explanations in Prompting (post-answer explanations / in-context explanations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting few-shot prompting with explanations associated with examples (either before or after the final answer) to improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can language models learn from explanations in context?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>280B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Prompting with Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Various BIG-Bench / reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve task performance by providing explanatory context in prompts accompanying examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / exact match / task-specific metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>few-shot prompting without explanations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey reports augmentation with explanations had emergent positive effect only at very large scale (e.g., emergence at ~5.0e23 FLOPs, 280B for Gopher). Authors highlight that explanation-based augmentation can be scale-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6726.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multilingual CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multilingual Chain-of-Thought Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Applying chain-of-thought prompting across multiple languages to elicit intermediate reasoning in non-English languages and solve multilingual reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are multilingual chain-of-thought reasoners.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>62B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Multilingual Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Multilingual reasoning / BIG-Bench multilingual tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve reasoning tasks presented in various languages using chain-of-thought prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / solve rate across languages</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>monolingual CoT or standard prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Listed in table as emergent for PaLM at ~2.9e23 FLOPs (62B). Survey notes that both model scale and training data composition (multilingual data) contribute to emergence in multilingual settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6726.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6726.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruthfulQA result</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TruthfulQA benchmark emergent performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed emergent improvement in TruthfulQA (measuring truthfulness) for large Gopher models at high scale.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TruthfulQA: Measuring how models mimic human falsehoods.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gopher</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>280B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Few-shot prompting (baseline) / model scaling</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>none (scale effect)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>TruthfulQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure tendency of models to produce truthful answers vs. mimic human falsehoods on adversarially-curated questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>performance above random (percentage points above random baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>smaller Gopher and GPT-3 models (scaled baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Survey states Gopher at 5.0e23 FLOPs (280B) showed a jump to more than 20 percentage points above random on TruthfulQA, whereas smaller models did not perform above random. This is presented as an emergent ability tied to scale.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emergent Abilities of Large Language Models', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners. <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 2)</em></li>
                <li>Finetuned language models are zero-shot learners. <em>(Rating: 2)</em></li>
                <li>Scaling language models: Methods, analysis & insights from training Gopher. <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Can language models learn from explanations in context? <em>(Rating: 2)</em></li>
                <li>Language models are multilingual chain-of-thought reasoners. <em>(Rating: 2)</em></li>
                <li>BIG-Bench: Beyond the imitation game: Measuring and extrapolating the capabilities of language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6726",
    "paper_id": "paper-dac3a172b504f4e33c029655e9befb3386e5f63a",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique that elicits explicit intermediate reasoning steps from a language model before producing a final answer, enabling multi-step problem solving.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "LaMDA",
            "model_size": "68B",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Math word problems (BIG-Bench / assorted math datasets)",
            "task_description": "Multi-step arithmetic / math word-problem reasoning requiring intermediate steps.",
            "performance_metric": "accuracy (final-answer)",
            "performance_value": null,
            "comparison_target_method": "standard prompting without intermediate steps",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper reports that CoT only yields a positive performance gap over standard prompting when scaled to ~1.3e23 training FLOPs (~68B params for LaMDA); prior smaller models do not get benefit. Authors note emergence: the method is only effective above a scale threshold.",
            "ablation_study_present": null,
            "uuid": "e6726.0",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Zero-shot CoT",
            "name_full": "Zero-shot Chain-of-Thought Prompting",
            "brief_description": "A prompting variant that elicits chain-of-thought style reasoning in a zero-shot setting (no few-shot exemplars), often by adding a phrase like 'Let's think step by step'.",
            "citation_title": "Large language models are zero-shot reasoners.",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_size": "175B",
            "reasoning_method_name": "Zero-shot Chain-of-Thought",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Arithmetic / reasoning benchmarks (e.g., BIG-Bench variants)",
            "task_description": "Multi-step reasoning elicited in zero-shot prompts to generate intermediate steps and final answer.",
            "performance_metric": "accuracy (final-answer)",
            "performance_value": null,
            "comparison_target_method": "standard zero-shot prompting (no CoT cue)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper cites Kojima et al. (2022) showing zero-shot CoT emerges at large scale (table entry: ~3.1e23 FLOPs, 175B for GPT-3). The surveyed paper emphasizes that zero-shot CoT's effectiveness is scale-dependent (emergent).",
            "ablation_study_present": null,
            "uuid": "e6726.1",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency Decoding",
            "brief_description": "An ensemble-style decoding method that samples multiple chain-of-thought reasoning traces and aggregates final answers (e.g., by majority vote) to improve robustness.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models.",
            "mention_or_use": "use",
            "model_name": "LaMDA",
            "model_size": "68B",
            "reasoning_method_name": "Self-Consistency",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "Math / reasoning benchmarks (chain-of-thought tasks)",
            "task_description": "Improve accuracy on multi-step reasoning by sampling diverse reasoning paths and aggregating answers.",
            "performance_metric": "accuracy (final-answer) / solve rate",
            "performance_value": null,
            "comparison_target_method": "single chain-of-thought greedy decoding",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey reports self-consistency shows emergent benefit at ~1.3e23 FLOPs (68B). Method operates by producing diverse reasoning traces and combining them; the paper frames this as leveraging diversity of reasoning traces to improve performance.",
            "ablation_study_present": null,
            "uuid": "e6726.2",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Scratchpad",
            "name_full": "Scratchpad / Intermediate-Output Fine-tuning",
            "brief_description": "Fine-tuning or prompting approach that trains models to emit intermediate computation steps (a scratchpad) which improves multi-step program-like computations.",
            "citation_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "mention_or_use": "use",
            "model_name": "LaMDA (or models evaluated by Nye et al.)",
            "model_size": "40M",
            "reasoning_method_name": "Scratchpad (intermediate outputs)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "8-digit addition (program-execution arithmetic)",
            "task_description": "Execute multi-step arithmetic/program-like computation requiring stepwise intermediate results.",
            "performance_metric": "solve rate / exact-match on numeric output",
            "performance_value": null,
            "comparison_target_method": "standard fine-tuning without scratchpad / direct-output model",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper reports that enabling scratchpad yields benefit only for models of ~8.9e19 FLOPs (40M params) or larger; i.e., scratchpad is emergent above a small scale. Survey highlights scratchpad helps program execution and multi-step computation.",
            "ablation_study_present": null,
            "uuid": "e6726.3",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting",
            "brief_description": "A prompting strategy that decomposes a complex problem into an ordered sequence of simpler subproblems solved sequentially, then composes subanswers for the final solution.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_size": "175B",
            "reasoning_method_name": "Least-to-Most Prompting",
            "reasoning_method_type": "sequential / decomposition",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Complex reasoning tasks (various chain-of-thought benchmarks)",
            "task_description": "Decompose and solve multi-step reasoning tasks by ordering subproblems from easiest to hardest.",
            "performance_metric": "accuracy / solve rate",
            "performance_value": null,
            "comparison_target_method": "standard few-shot prompting",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey lists least-to-most prompting as an augmented prompting method that is observed to be effective at large model scales (table: emergence at ~3.1e23 FLOPs, 175B). The paper frames it as a decomposition strategy enabling complex reasoning when scale permits.",
            "ablation_study_present": null,
            "uuid": "e6726.4",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Instruction-Finetuning (FLAN)",
            "name_full": "Instruction Finetuning (FLAN)",
            "brief_description": "Fine-tuning a language model on a mixture of tasks phrased as natural-language instructions to enable better zero-shot or few-shot instruction following.",
            "citation_title": "Finetuned language models are zero-shot learners.",
            "mention_or_use": "use",
            "model_name": "FLAN",
            "model_size": "68B",
            "reasoning_method_name": "Instruction-following Finetuning",
            "reasoning_method_type": "conditioning / fine-tuning",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Multi-task / instruction-following evaluations (generalization)",
            "task_description": "Respond appropriately to novel tasks described by instructions without exemplars.",
            "performance_metric": "aggregate task accuracy / task-specific metrics",
            "performance_value": null,
            "comparison_target_method": "no instruction finetuning (base pre-trained model)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey reports instruction-finetuning harmed performance for models &lt;=7e21 FLOPs (~8B) and only improved performance when scaled to ~1e23 FLOPs (~100B / table reports 68B for FLAN). The paper also notes subsequent work (Sanh et al.) induced instruction-following in smaller encoder-decoder models, showing architecture and data matter.",
            "ablation_study_present": null,
            "uuid": "e6726.5",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Calibration P(True)",
            "name_full": "Calibration via P(True) (True/False technique)",
            "brief_description": "A calibration technique where the model first proposes an answer then separately estimates the probability that its proposed answer is true, used to better predict when it will answer correctly.",
            "citation_title": "Language models (mostly) know what they know.",
            "mention_or_use": "use",
            "model_name": "Anthropic model (as reported)",
            "model_size": "52B",
            "reasoning_method_name": "P(True) calibration",
            "reasoning_method_type": "post-hoc calibration / confidence estimation",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Calibration evaluation (general QA calibration tasks)",
            "task_description": "Measure whether models can predict correctness of their own answers.",
            "performance_metric": "calibration effectiveness (e.g., calibration error, superior method frequency)",
            "performance_value": null,
            "comparison_target_method": "standard calibration methods (probability of correct answer among options)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey cites Kadavath et al. (2022) showing that P(True) technique's superiority only emerges at largest model scale (~2.6e23 FLOPs, 52B params).",
            "ablation_study_present": null,
            "uuid": "e6726.6",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Open-book / Retrieval",
            "name_full": "Open-book / Retrieval-Augmented Use of External Knowledge",
            "brief_description": "Augmenting model responses with explicit retrieval or open-book knowledge to improve factuality and fact-checking.",
            "citation_title": "Scaling language models: Methods, analysis & insights from training Gopher.",
            "mention_or_use": "use",
            "model_name": "Gopher",
            "model_size": "7.1B",
            "reasoning_method_name": "Open-book / retrieval-augmented prompting",
            "reasoning_method_type": "retrieval-based",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Fact-checking / open-book QA",
            "task_description": "Use an external knowledge source at inference to verify or produce factual answers.",
            "performance_metric": "accuracy / fact-checking success rate",
            "performance_value": null,
            "comparison_target_method": "closed-book baseline (no retrieval)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey notes that using open-book knowledge helps for fact checking and that benefit emerges at ~1.3e22 FLOPs (7.1B Gopher). This is an example of augmenting model with external memory to enable tasks at smaller scale than would otherwise be needed.",
            "ablation_study_present": null,
            "uuid": "e6726.7",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Differentiable Search Index",
            "name_full": "Transformer Memory as a Differentiable Search Index",
            "brief_description": "An approach that treats transformer memory as a learnable differentiable index to retrieve relevant contexts, effectively a learned retrieval mechanism.",
            "citation_title": "Transformer memory as a differentiable search index.",
            "mention_or_use": "use",
            "model_name": "T5",
            "model_size": "11B",
            "reasoning_method_name": "Differentiable Search Index",
            "reasoning_method_type": "retrieval-based / memory-augmented",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Tasks requiring long-range context / retrieval-augmented performance (various)",
            "task_description": "Leverage learned memory/index for lookup of relevant information to assist generation.",
            "performance_metric": "task-specific accuracy / retrieval-augmented metrics",
            "performance_value": null,
            "comparison_target_method": "standard model without differentiable index",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey lists differentiable search index emergence at ~3.3e22 FLOPs (11B T5). This is presented as an augmented prompting/architecture strategy to improve capabilities beyond pure scaling.",
            "ablation_study_present": null,
            "uuid": "e6726.8",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Leveraging Explanations",
            "name_full": "Using Explanations in Prompting (post-answer explanations / in-context explanations)",
            "brief_description": "Augmenting few-shot prompting with explanations associated with examples (either before or after the final answer) to improve generalization.",
            "citation_title": "Can language models learn from explanations in context?",
            "mention_or_use": "use",
            "model_name": "Gopher",
            "model_size": "280B",
            "reasoning_method_name": "Prompting with Explanations",
            "reasoning_method_type": "sequential / augmentation",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "Various BIG-Bench / reasoning tasks",
            "task_description": "Improve task performance by providing explanatory context in prompts accompanying examples.",
            "performance_metric": "accuracy / exact match / task-specific metrics",
            "performance_value": null,
            "comparison_target_method": "few-shot prompting without explanations",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey reports augmentation with explanations had emergent positive effect only at very large scale (e.g., emergence at ~5.0e23 FLOPs, 280B for Gopher). Authors highlight that explanation-based augmentation can be scale-dependent.",
            "ablation_study_present": null,
            "uuid": "e6726.9",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Multilingual CoT",
            "name_full": "Multilingual Chain-of-Thought Reasoning",
            "brief_description": "Applying chain-of-thought prompting across multiple languages to elicit intermediate reasoning in non-English languages and solve multilingual reasoning tasks.",
            "citation_title": "Language models are multilingual chain-of-thought reasoners.",
            "mention_or_use": "use",
            "model_name": "PaLM",
            "model_size": "62B",
            "reasoning_method_name": "Multilingual Chain-of-Thought",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "Multilingual reasoning / BIG-Bench multilingual tasks",
            "task_description": "Solve reasoning tasks presented in various languages using chain-of-thought prompting.",
            "performance_metric": "accuracy / solve rate across languages",
            "performance_value": null,
            "comparison_target_method": "monolingual CoT or standard prompting",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Listed in table as emergent for PaLM at ~2.9e23 FLOPs (62B). Survey notes that both model scale and training data composition (multilingual data) contribute to emergence in multilingual settings.",
            "ablation_study_present": null,
            "uuid": "e6726.10",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "TruthfulQA result",
            "name_full": "TruthfulQA benchmark emergent performance",
            "brief_description": "Observed emergent improvement in TruthfulQA (measuring truthfulness) for large Gopher models at high scale.",
            "citation_title": "TruthfulQA: Measuring how models mimic human falsehoods.",
            "mention_or_use": "use",
            "model_name": "Gopher",
            "model_size": "280B",
            "reasoning_method_name": "Few-shot prompting (baseline) / model scaling",
            "reasoning_method_type": "none (scale effect)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "TruthfulQA",
            "task_description": "Measure tendency of models to produce truthful answers vs. mimic human falsehoods on adversarially-curated questions.",
            "performance_metric": "performance above random (percentage points above random baseline)",
            "performance_value": null,
            "comparison_target_method": "smaller Gopher and GPT-3 models (scaled baseline)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Survey states Gopher at 5.0e23 FLOPs (280B) showed a jump to more than 20 percentage points above random on TruthfulQA, whereas smaller models did not perform above random. This is presented as an emergent ability tied to scale.",
            "ablation_study_present": null,
            "uuid": "e6726.11",
            "source_info": {
                "paper_title": "Emergent Abilities of Large Language Models",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners.",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 2
        },
        {
            "paper_title": "Finetuned language models are zero-shot learners.",
            "rating": 2
        },
        {
            "paper_title": "Scaling language models: Methods, analysis & insights from training Gopher.",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Can language models learn from explanations in context?",
            "rating": 2
        },
        {
            "paper_title": "Language models are multilingual chain-of-thought reasoners.",
            "rating": 2
        },
        {
            "paper_title": "BIG-Bench: Beyond the imitation game: Measuring and extrapolating the capabilities of language models.",
            "rating": 2
        }
    ],
    "cost": 0.0185135,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Emergent Abilities of Large Language Models</h1>
<p>Jason Wei ${ }^{1}$<br>Yi Tay ${ }^{1}$<br>Rishi Bommasani ${ }^{2}$<br>Colin Raffel ${ }^{3}$<br>Barret Zoph ${ }^{1}$<br>Sebastian Borgeaud ${ }^{4}$<br>Dani Yogatama ${ }^{4}$<br>Maarten Bosma ${ }^{1}$<br>Denny Zhou ${ }^{1}$<br>Donald Metzler ${ }^{1}$<br>Ed H. Chi ${ }^{1}$<br>Tatsunori Hashimoto ${ }^{2}$<br>Oriol Vinyals ${ }^{4}$<br>Percy Liang ${ }^{2}$<br>Jeff Dean ${ }^{1}$<br>William Fedus ${ }^{1}$<br>jasonwei@google.com<br>yitay@google.com<br>nlprishi@stanford.edu<br>craftel@gmail.com<br>barretzoph@google.com<br>sborgeaud@deepmind.com<br>dyogatama@deepmind.com<br>bosma@google.com<br>dennyzhou@google.com<br>metzler@google.com<br>edchi@google.com<br>thashim@stanford.edu<br>vinyals@deepmind.com<br>pliang@stanford.edu<br>jeff@google.com<br>liamfedus@google.com</p>
<p>${ }^{1}$ Google Research ${ }^{2}$ Stanford University ${ }^{3}$ UNC Chapel Hill ${ }^{4}$ DeepMind</p>
<p>Reviewed on OpenReview: https://openreview.net/forum?id=yzkSU5zdwD</p>
<h4>Abstract</h4>
<p>Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence raises the question of whether additional scaling could potentially further expand the range of capabilities of language models.</p>
<h2>1 Introduction</h2>
<p>Language models have revolutionized natural language processing (NLP) in recent years. It is now well-known that increasing the scale of language models (e.g., training compute, model parameters, etc.) can lead to better performance and sample efficiency on a range of downstream NLP tasks (Devlin et al., 2019; Brown et al., 2020, inter alia). In many cases, the effect of scale on performance can often be methodologically predicted via scaling laws - for example, scaling curves for cross-entropy loss have been shown to empirically span more than seven orders of magnitude (Kaplan et al., 2020; Hoffmann et al., 2022). On the other hand, performance for certain downstream tasks counterintuitively does not appear to continuously improve as a function of scale, and such tasks cannot be predicted ahead of time (Ganguli et al., 2022).</p>
<p>In this paper, we will discuss the unpredictable phenomena of emergent abilities of large language models. Emergence as an idea has been long discussed in domains such as physics, biology, and computer science (Anderson, 1972; Hwang et al., 2012; Forrest, 1990; Corradini \&amp; O'Connor, 2010; Harper \&amp; Lewis, 2012, inter</p>
<p>alia). We will consider the following general definition of emergence, adapted from Steinhardt (2022) and rooted in a 1972 essay called "More Is Different" by Nobel prize-winning physicist Philip Anderson (Anderson, 1972):</p>
<p>Emergence is when quantitative changes in a system result in qualitative changes in behavior.</p>
<p>Here we will explore emergence with respect to model scale, as measured by training compute and number of model parameters. Specifically, we define emergent abilities of large language models as abilities that are not present in smaller-scale models but are present in large-scale models; thus they cannot be predicted by simply extrapolating the performance improvements on smaller-scale models (2). ${ }^{1}$ We survey emergent abilities as observed in a range of prior work, categorizing them in settings such as few-shot prompting (3) and augmented prompting strategies (4). Emergence motivates future research on why such abilities are acquired and whether more scaling will lead to further emergent abilities, which we highlight as important questions for the field (5).</p>
<h1>2 Emergent Abilities Definition</h1>
<p>As a broad concept, emergence is often used informally and can be reasonably interpreted in many different ways. In this paper, we will consider a focused definition of emergent abilities of large language models:</p>
<p>An ability is emergent if it is not present in smaller models but is present in larger models.</p>
<p>Emergent abilities would not have been directly predicted by extrapolating a scaling law (i.e. consistent performance improvements) from small-scale models. When visualized via a scaling curve ( $x$-axis: model scale, $y$-axis: performance), emergent abilities show a clear pattern-performance is near-random until a certain critical threshold of scale is reached, after which performance increases to substantially above random. This qualitative change is also known as a phase transition-a dramatic change in overall behavior that would not have been foreseen by examining smaller-scale systems (Huberman \&amp; Hogg, 1987).</p>
<p>Today's language models have been scaled primarily along three factors: amount of computation, number of model parameters, and training dataset size (Kaplan et al., 2020; Hoffmann et al., 2022). In this paper, we will analyze scaling curves by plotting the performance of different models where training compute for each model is measured in FLOPs on the $x$-axis (Hoffmann et al., 2022). Because language models trained with more compute tend to also have more parameters, we additionally show plots with number of model parameters as the $x$-axis in Appendix D (see Figure 11 and Figure 12, as well as Figure 4 and Figure 10). Using training FLOPs or model parameters as the $x$-axis produces curves with similar shapes due to the fact that most dense Transformer language model families have scaled training compute roughly proportionally with model parameters (Kaplan et al., 2020).</p>
<p>Training dataset size is also an important factor, but we do not plot capabilities against it because many language model families use a fixed number of training examples for all model sizes (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022). Although we focus on training computation and model size here, there is not a single proxy that adequately captures all aspects of scale. For example, Chinchilla (Hoffmann et al., 2022) has one-fourth as many parameters as Gopher (Rae et al., 2021) but uses similar training compute; and sparse mixture-of-expert models have more parameters per training/inference compute than dense models (Fedus et al., 2021; Du et al., 2021). Overall, it may be wise to view emergence as a function of many correlated variables. For example, later in Figure 4 we will also plot emergence as a function of WikiText103 perplexity (Merity et al., 2016), which happens to closely correlate with training computation for Gopher/ Chinchilla (though this correlation may not hold in the long-run).</p>
<p>Note that the scale at which an ability is first observed to emerge depends on a number of factors and is not an immutable property of the ability. For instance, emergence may occur with less training compute</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>or fewer model parameters for models trained on higher-quality data. Conversely, emergent abilities also crucially depend on other factors such as not being limited by the amount of data, its quality, or the number of parameters in the model. Today's language models are likely not trained optimally (Hoffmann et al., 2022), and our understanding of how to best train models will evolve over time. Our goal in this paper is not to characterize or claim that a specific scale is required to observe emergent abilities, but rather, we aim to discuss examples of emergent behavior in prior work.</p>
<h1>3 Few-Shot Prompted Tasks</h1>
<p>We first discuss emergent abilities in the prompting paradigm, as popularized by GPT-3 (Brown et al., 2020). ${ }^{2}$ In prompting, a pre-trained language model is given a prompt (e.g. a natural language instruction) of a task and completes the response without any further training or gradient updates to its parameters. Brown et al. (2020) proposed few-shot prompting, which includes a few input-output examples in the model's context (input) as a preamble before asking the model to perform the task for an unseen inference-time example. An example prompt is shown in Figure 1.</p>
<p>The ability to perform a task via few-shot prompting is emergent when a model has random performance until a certain scale, after which performance increases to well-above random. Figure 2 shows eight such emergent abilities spanning five language model families from various work.</p>
<p>BIG-Bench. Figure 2A-D depicts four emergent few-shot prompted tasks from BIG-Bench, a crowd-sourced suite of over 200 benchmarks for language model evaluation (BIG-Bench, 2022). Figure 2A shows an arithmetic benchmark that tests 3-digit addition and subtraction, as well as 2-digit multiplication. GPT-3 and LaMDA (Thoppilan et al., 2022) have close-to-zero performance for several orders of magnitude of training compute, before performance jumps to sharply above random at $2 \cdot 10^{22}$ training FLOPs ( 13 B parameters) for GPT-3, and $10^{23}$ training FLOPs ( 68 B parameters) for LaMDA. Similar emergent behavior also occurs at around the same model scale for other tasks, such as transliterating from the International Phonetic Alphabet (Figure 2B), recovering a word from its scrambled letters (Figure 2C), and Persian question-answering (Figure 2D). Even more emergent abilities from BIG-Bench are given in Appendix E.</p>
<p>TruthfulQA. Figure 2E shows few-shot prompted performance on the TruthfulQA benchmark, which measures the ability to answer questions truthfully (Lin et al., 2021). This benchmark is adversarially curated against GPT-3 models, which do not perform above random, even when scaled to the largest model size. Small Gopher models also do not perform above random until scaled up to the largest model of $5 \cdot 10^{23}$ training FLOPs ( 280 B parameters), for which performance jumps to more than $20 \%$ above random (Rae et al., 2021).</p>
<p>Grounded conceptual mappings. Figure 2F shows the task of grounded conceptual mappings, where language models must learn to map a conceptual domain, such as a cardinal direction, represented in a textual grid world (Patel \&amp; Pavlick, 2022). Again, performance only jumps to above random using the largest GPT-3 model.</p>
<p>Multi-task language understanding. Figure 2G shows the Massive Multi-task Language Understanding (MMLU) benchmark, which aggregates 57 tests covering a range of topics including math, history, law, and more (Hendrycks et al., 2021a). For GPT-3, Gopher, and Chinchilla, models of $\sim 10^{22}$ training FLOPs ( $\sim 10 \mathrm{~B}$ parameters) or smaller do not perform better than guessing on average over all the topics, scaling up to 3-5 $\cdot 10^{23}$ training FLOPs ( $70 \mathrm{~B}-280 \mathrm{~B}$ parameters) enables performance to substantially surpass random. This result is striking because it could imply that the ability to solve knowledge-based questions spanning a large collection of topics might require scaling up past this threshold (for dense language models without retrieval or access to external memory).</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model. The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale, after which performance significantly increases to well-above random. Note that models that used more training compute also typically have more parameters-hence, we show an analogous figure with number of model parameters instead of training FLOPs as the $x$-axis in Figure 11. A-D: BIG-Bench (2022), 2-shot. E: Lin et al. (2021) and Rae et al. (2021). F: Patel \&amp; Pavlick (2022). G: Hendrycks et al. (2021a), Rae et al. (2021), and Hoffmann et al. (2022). H: Brown et al. (2020), Hoffmann et al. (2022), and Chowdhery et al. (2022) on the WiC benchmark (Pilehvar \&amp; Camacho-Collados, 2019).</p>
<p>Word in Context. Finally, Figure 2H shows the Word in Context (WiC) benchmark (Pilehvar \&amp; CamachoCollados, 2019), which is a semantic understanding benchmark. Notably, GPT-3 and Chinchilla fail to achieve one-shot performance of better than random, even when scaled to their largest model size of $\sim 5 \cdot 10^{23}$ FLOPs. Although these results so far may suggest that scaling alone may not enable models to solve WiC, above-random performance eventually emerged when PaLM was scaled to $2.5 \cdot 10^{24}$ FLOPs ( 540 B parameters), which was much larger than GPT-3 and Chinchilla.</p>
<h1>4 Augmented Prompting Strategies</h1>
<p>Although few-shot prompting is perhaps currently the most common way of interacting with large language models, recent work has proposed several other prompting and finetuning strategies to further augment the abilities of language models. If a technique shows no improvement or is harmful when compared to the baseline of not using the technique until applied to a model of a large-enough scale, we also consider the technique an emergent ability.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Specialized prompting or finetuning methods can be emergent in that they do not have a positive effect until a certain model scale. A: <em>Wei et al. (2022b)</em>. B: <em>Wei et al. (2022a)</em>. C: <em>Nye et al. (2021)</em>. D: <em>Kadavath et al. (2022)</em>. An analogous figure with number of parameters on the $x$-axis instead of training FLOPs is given in Figure 12. The model shown in A-C is LaMDA <em>(Thoppilan et al., 2022)</em>, and the model shown in D is from Anthropic.</p>
<p>Multi-step reasoning. Reasoning tasks, especially those involving multiple steps, have been challenging for language models and NLP models more broadly <em>(Rae et al., 2021; Bommasani et al., 2021; Nye et al., 2021)</em>. A recent prompting strategy called chain-of-thought prompting enables language models to solve such problems by guiding them to produce a sequence of intermediate steps before giving the final answer <em>(Cobbe et al., 2021; Wei et al., 2022b; Suzgun et al., 2022)</em>. As shown in Figure 3A, chain of thought prompting only surpasses standard prompting without intermediate steps when scaled to $10^{23}$ training FLOPs ( 100B parameters). A similar emergence in performance gain was also observed when augmenting few-shot prompting with explanations that came after the final answer <em>(Lampinen et al., 2022)</em>.</p>
<p>Instruction following. Another growing line of work aims to better enable language models to perform new tasks simply by reading instructions describing the task (without few-shot exemplars). By finetuning on a mixture of tasks phrased as instructions, language models have been shown to respond appropriately to instructions describing an unseen task <em>(Ouyang et al., 2022; Wei et al., 2022a; Sanh et al., 2022; Chung et al., 2022)</em>. As shown in Figure 3B, <em>Wei et al. (2022a)</em> found that this instruction-finetuning technique hurts performance for models of $7 \cdot 10^{21}$ training FLOPs (8B parameters) or smaller, and only improves performance when scaled to $10^{23}$ training FLOPs ( 100B parameters) (though <em>Sanh et al. (2022)</em> found shortly after that this instruction-following behavior could be also induced by finetuning smaller encoder-decoder T5 models).</p>
<p>Program execution. Consider computational tasks involving multiple steps, such as adding large numbers or executing computer programs. <em>Nye et al. (2021)</em> show that finetuning language models to predict intermediate outputs (scratchpad) enables them to successfully execute such multi-step computations. As shown in Figure 3C, on 8-digit addition, using a scratchpad only helps for models of $\sim 9 \cdot 10^{19}$ training FLOPs (40M parameters) or larger.</p>
<p>Model calibration. Finally, an important direction for deployment of language models studies is <em>calibration</em>, which measures whether models can predict which questions they will be able to answer correctly. <em>Kadavath et al. (2022)</em> compared two ways of measuring calibration: a True/False technique, where models first propose answers and then evaluate the probability P(True) that their answers are correct, and more-standard methods of calibration, which use the probability of the correct answer compared with other answer options. As shown in Figure 3D, the superiority of the True/False technique only emerges when scaled to the largest model scale of $\sim 3 \cdot 10^{23}$ training FLOPs (52B parameters).</p>
<p>Table 1: List of emergent abilities of large language models and the scale (both training FLOPs and number of model parameters) at which the abilities emerge.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Emergent scale</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train. FLOPs</td>
<td style="text-align: center;">Params.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Few-shot prompting abilities</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Brown et al. (2020)</td>
</tr>
<tr>
<td style="text-align: center;">- Addition/subtraction (3 digit)</td>
<td style="text-align: center;">$2.3 \mathrm{E}+22$</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- Addition/subtraction (4-5 digit)</td>
<td style="text-align: center;">$3.1 \mathrm{E}+23$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Hendrycks et al. (2021a)</td>
</tr>
<tr>
<td style="text-align: center;">- MMLU Benchmark (57 topic avg.)</td>
<td style="text-align: center;">$3.1 \mathrm{E}+23$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Rae et al. (2021)</td>
</tr>
<tr>
<td style="text-align: center;">- Toxicity classification (CivilComments)</td>
<td style="text-align: center;">$1.3 \mathrm{E}+22$</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- Truthfulness (Truthful QA)</td>
<td style="text-align: center;">$5.0 \mathrm{E}+23$</td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- MMLU Benchmark (26 topics)</td>
<td style="text-align: center;">$5.0 \mathrm{E}+23$</td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Patel \&amp; Pavlick (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Grounded conceptual mappings</td>
<td style="text-align: center;">$3.1 \mathrm{E}+23$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- MMLU Benchmark (30 topics)</td>
<td style="text-align: center;">$5.0 \mathrm{E}+23$</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">Chinchilla</td>
<td style="text-align: center;">Hoffmann et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Word in Context (WiC) benchmark</td>
<td style="text-align: center;">$2.5 \mathrm{E}+24$</td>
<td style="text-align: center;">540B</td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">Chowdhery et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Many BIG-Bench tasks (see Appendix E)</td>
<td style="text-align: center;">Many</td>
<td style="text-align: center;">Many</td>
<td style="text-align: center;">Many</td>
<td style="text-align: center;">BIG-Bench (2022)</td>
</tr>
<tr>
<td style="text-align: center;">Augmented prompting abilities</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">- Instruction following (finetuning)</td>
<td style="text-align: center;">$1.3 \mathrm{E}+23$</td>
<td style="text-align: center;">68B</td>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">Wei et al. (2022a)</td>
</tr>
<tr>
<td style="text-align: center;">- Scratchpad: 8-digit addition (finetuning)</td>
<td style="text-align: center;">$8.9 \mathrm{E}+19$</td>
<td style="text-align: center;">40 M</td>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">Nye et al. (2021)</td>
</tr>
<tr>
<td style="text-align: center;">- Using open-book knowledge for fact checking</td>
<td style="text-align: center;">$1.3 \mathrm{E}+22$</td>
<td style="text-align: center;">7.1B</td>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;">Rae et al. (2021)</td>
</tr>
<tr>
<td style="text-align: center;">- Chain-of-thought: Math word problems</td>
<td style="text-align: center;">$1.3 \mathrm{E}+23$</td>
<td style="text-align: center;">68B</td>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">Wei et al. (2022b)</td>
</tr>
<tr>
<td style="text-align: center;">- Chain-of-thought: StrategyQA</td>
<td style="text-align: center;">$2.9 \mathrm{E}+23$</td>
<td style="text-align: center;">62B</td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">Chowdhery et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Differentiable search index</td>
<td style="text-align: center;">$3.3 \mathrm{E}+22$</td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">Tay et al. (2022b)</td>
</tr>
<tr>
<td style="text-align: center;">- Self-consistency decoding</td>
<td style="text-align: center;">$1.3 \mathrm{E}+23$</td>
<td style="text-align: center;">68B</td>
<td style="text-align: center;">LaMDA</td>
<td style="text-align: center;">Wang et al. (2022b)</td>
</tr>
<tr>
<td style="text-align: center;">- Leveraging explanations in prompting</td>
<td style="text-align: center;">$5.0 \mathrm{E}+23$</td>
<td style="text-align: center;">280B</td>
<td style="text-align: center;">Gopher</td>
<td style="text-align: center;">Lampinen et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Least-to-most prompting</td>
<td style="text-align: center;">$3.1 \mathrm{E}+23$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Zhou et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Zero-shot chain-of-thought reasoning</td>
<td style="text-align: center;">$3.1 \mathrm{E}+23$</td>
<td style="text-align: center;">175B</td>
<td style="text-align: center;">GPT-3</td>
<td style="text-align: center;">Kojima et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Calibration via P(True)</td>
<td style="text-align: center;">$2.6 \mathrm{E}+23$</td>
<td style="text-align: center;">52B</td>
<td style="text-align: center;">Anthropic</td>
<td style="text-align: center;">Kadavath et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Multilingual chain-of-thought reasoning</td>
<td style="text-align: center;">$2.9 \mathrm{E}+23$</td>
<td style="text-align: center;">62B</td>
<td style="text-align: center;">PaLM</td>
<td style="text-align: center;">Shi et al. (2022)</td>
</tr>
<tr>
<td style="text-align: center;">- Ask me anything prompting</td>
<td style="text-align: center;">$1.4 \mathrm{E}+22$</td>
<td style="text-align: center;">6B</td>
<td style="text-align: center;">EleutherAI</td>
<td style="text-align: center;">Arora et al. (2022)</td>
</tr>
</tbody>
</table>
<h1>5 Discussion</h1>
<p>We have seen that a range of abilities - in the few-shot prompting setup or otherwise - have thus far only been observed when evaluated on a sufficiently large language model. Hence, their emergence cannot be predicted by simply extrapolating performance on smaller-scale models. Emergent few-shot prompted tasks are also unpredictable in the sense that these tasks are not explicitly included in pre-training, and we likely do not know the full scope of few-shot prompted tasks that language models can perform. This raises the question of whether further scaling could potentially endow even-larger language models with new emergent abilities. Tasks that language models cannot currently do are prime candidates for future emergence; for instance, there are dozens of tasks in BIG-Bench for which even the largest GPT-3 and PaLM models do not achieve above-random performance (see Appendix E.4).</p>
<p>The ability for scale to unpredictably enable new techniques is not just theoretical. Consider the Word in Context (WiC) benchmark (Pilehvar \&amp; Camacho-Collados, 2019) shown in Figure 2H, as a historical example. Here, scaling GPT-3 to around $3 \cdot 10^{23}$ training FLOPs ( 175 B parameters) failed to unlock above-random one-shot prompting performance. ${ }^{3}$ Regarding this negative result, Brown et al. (2020) cited the model architecture of GPT-3 or the use of an autoregressive language modeling objective (rather than using a denoising training objective) as potential reasons, and suggested training a model of comparable size with bidirectional architecture as a remedy. However, later work found that further scaling a decoder-only language model was actually enough to enable above-random performance on this task. As is shown in Figure 2H, scaling PaLM (Chowdhery et al., 2022) from $3 \cdot 10^{23}$ training FLOPs ( 62 B parameters) to $3 \cdot 10^{24}$ training</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>FLOPs (540B parameters) led to a significant jump in performance, without the significant architectural changes suggested by Brown et al. (2020).</p>
<h1>5.1 Potential explanations of emergence</h1>
<p>Although there are dozens of examples of emergent abilities, there are currently few compelling explanations for why such abilities emerge in the way they do. For certain tasks, there may be natural intuitions for why emergence requires a model larger than a particular threshold scale. For instance, if a multi-step reasoning task requires $l$ steps of sequential computation, this might require a model with a depth of at least $O(l)$ layers. It is also reasonable to assume that more parameters and more training enable better memorization that could be helpful for tasks requiring world knowledge. ${ }^{4}$ As an example, good performance on closed-book question-answering may require a model with enough parameters to capture the compressed knowledge base itself (though language model-based compressors can have higher compression ratios than conventional compressors (Bellard, 2021)).</p>
<p>It is also important to consider the evaluation metrics used to measure emergent abilities (BIG-Bench, 2022). For instance, using exact string match as the evaluation metric for long-sequence targets may disguise compounding incremental improvements as emergence. Similar logic may apply for multi-step or arithmetic reasoning problems, where models are only scored on whether they get the final answer to a multi-step problem correct, without any credit given to partially correct solutions. However, the jump in final answer accuracy does not explain why the quality of intermediate steps suddenly emerges to above random, and using evaluation metrics that do not give partial credit are at best an incomplete explanation, because emergent abilities are still observed on many classification tasks (e.g., the tasks in Figure 2D-H).</p>
<p>As an alternative evaluation, we measure cross-entropy loss, which is used in scaling laws for pre-training, for the six emergent BIG-Bench tasks, as detailed in Appendix A. This analysis follows the same experimental setup from BIG-Bench (2022) and affirms their conclusions for the six emergent tasks we consider. Namely, cross-entropy loss improves even for small model scales where the downstream metrics (exact match, BLEU, and accuracy) are close to random and do not improve, which shows that improvements in the log-likelihood of the target sequence can be masked by such downstream metrics. However, this analysis does not explain why downstream metrics are emergent or enable us to predict the scale at which emergence occurs. Overall, more work is needed to tease apart what enables scale to unlock emergent abilities.</p>
<h3>5.2 Beyond scaling</h3>
<p>Although we may observe an emergent ability to occur at a certain scale, it is possible that the ability could be later achieved at a smaller scale - in other words, model scale is not the singular factor for unlocking an emergent ability. As the science of training large language models progresses, certain abilities may be unlocked for smaller models with new architectures, higher-quality data, or improved training procedures. For example, there are 14 BIG-Bench tasks ${ }^{5}$ for which LaMDA 137B and GPT-3 175B models perform at near-random, but PaLM 62B in fact achieves above-random performance, despite having fewer model parameters and training FLOPs. While there is not an empirical study ablating every difference between PaLM 62B and prior models (the computational cost would be too high), potential reasons for the better performance of PaLM could include high-quality training data (e.g., more multilingual and code data than LaMDA) and architectural differences (e.g., split digit-encodings; see Section 2 in Chowdhery et al. (2022)). Another potentially way of unlocking emergence is through a different pre-training objective - it was shown in Tay et al. (2022c) that a computationally-efficient continued pre-training stage on a mixture-of-denoisers objective (Tay et al., 2022a) enabled emergent performance on several BIG-Bench tasks.</p>
<p>Moreover, once an ability is discovered, further research may make the ability available for smaller scale models. Consider the nascent direction of enabling language models to follow natural language instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022, inter alia). Although Wei et al. (2022a) initially found that instruction-based finetuning only worked for 68B parameter or larger decoder-only</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>models, [Sanh et al. (2022)] induced similar behavior in a 11B model with an encoder-decoder architecture, which typically has higher performance after finetuning than decoder-only architectures [Wang et al. (2022a)]. As another example, <em>Ouyang et al. (2022)</em> proposed a finetuning and reinforcement learning from human feedback approach for the InstructGPT models, which enabled a 1.3B model to outperform much larger models in human-rater evaluations on a broad set of use cases.</p>
<p>There has also been work on improving the general few-shot prompting abilities of language models [Gao et al. (2021); Schick &amp; Schtze (2021, inter alia)]. Theoretical and interpretability research [Wei et al. (2021a); Saunshi et al. (2021)] on why a language modeling objective facilitates certain downstream behavior could in turn have implications on how to enable emergence beyond simply scaling. For instance, certain features of pre-training data (e.g., long-range coherence, having many rare classes) have also been shown to correlate with emergent few-shot prompting and could potentially enable it in smaller models [Xie et al. (2022); Chan et al. (2022)], and few-shot learning can require certain model architectures in some scenarios [Chan et al. (2022)]. Computational linguistics work has further shown how threshold frequencies of training data can activate emergent syntactic rule-learning when model parameters and training FLOPs are held constant [Wei et al. (2021b)], which has even been shown to have striking aha moments similar to those in the psycholinguistics literature [Abend et al. (2017); Zhang et al. (2021)]. As we continue to train language models, lowering the scale threshold for emergent abilities will become more important for making research on such abilities to available to the community more broadly [Bommasani et al. (2021); Ganguli et al. (2022); Liang et al. (2022)].</p>
<p>Naturally, there are limitations to a program consisting only of increasing scale (training compute, model parameters, and dataset size). For instance, scaling may eventually be bottle-necked by hardware constraints, and some abilities may not have emerged at this point. Other abilities may never emergefor instance, tasks that are far out of the distribution of even a very large training dataset might not ever achieve any significant performance. Finally, an ability could emerge and then plateau; in other words, there is no guarantee that scaling enables an ability to reach the desired level.</p>
<h3>5.3 Another view of emergence</h3>
<p>While scale (e.g., training FLOPs or model parameters) has been highly correlated with language model performance on many downstream metrics so far, scale need not be the only lens to view emergent abilities. For example, the emergence of task-specific abilities can be analyzed as a function of the language models perplexity on a general text corpus such as WikiText103 [Merity et al. (2016)]. Figure 4 shows such a plot with WikiText103 perplexity of the language model on the $x$-axis and performance on the MMLU benchmark on the $y$-axis, side-by-side with plots of training FLOPs and model parameters on the $x$-axis.</p>
<p>Because WikiText103 perplexity and training FLOPs happen to be highly correlated for the models considered here (Gopher and Chinchilla), the plots of emergent abilities look similar for both. However, this correlation between WikiText103 perplexity and scale may not hold in the future as new techniques beyond vanilla dense Transformer models are developed (e.g., retrieval-augmented models may have strong WikiText103 perplexity with less training compute and fewer model parameters [Borgeaud et al. (2021)]). Also note that using WikiText103 perplexity to compare across model families can be complicated due to factors such as differences in training data composition. Overall, emergent abilities should probably be viewed as a function of many correlated variables.</p>
<h3>5.4 Emergent risks</h3>
<p>Importantly, similar to how emergent abilities have been observed in the few-shot prompting setting without explicitly being included in pre-training, risks could also emerge [Bommasani et al. (2021); Steinhardt (2021); Ganguli et al. (2022)]. For instance, societal risks of large language models such as truthfulness, bias, and toxicity are a growing area of research [Weidinger et al. (2021)]. Such risks are important considerations whether or not they can be precisely characterized as emergent based on the definition in 2, and, in some scenarios, do increase with model scale (see the Inverse Scaling Prize). Since work on emergent abilities</p>
<p><sup>6</sup>https://github.com/inverse-scaling/prize</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Top row: the relationships between training FLOPs, model parameters, and perplexity (ppl) on WikiText103 (Merity et al., 2016) for Chinchilla and Gopher. Bottom row: Overall performance on the massively multi-task language understanding benchmark (MMLU; Hendrycks et al., 2021a) as a function of training FLOPs, model parameters, and WikiText103 perplexity.
incentivizes scaling language models, it is important to be aware of risks that increase with model scale even if they are not emergent.</p>
<p>Here, we summarize several prior findings on the relationship between specific social risks and model scale. On WinoGender (Rudinger et al., 2017), which measures gender bias in occupations such as "nurse" or "electrician," scaling has improved performance so far (Du et al., 2021; Chowdhery et al., 2022), though BIG-Bench (2022) found in BBQ bias benchmark (Parrish et al., 2022) that bias can increase with scaling for ambiguous contexts. As for toxicity, Askell et al. (2021) found that while larger language models could produce more toxic responses from the RealToxicityPrompts dataset (Gehman et al., 2020), this behavior could be mitigated by giving models prompts with examples of being "helpful, harmless, and honest." For extracting training data from language models, larger models were found to be more likely to memorize training data (Carlini et al., 2021; 2022), though deduplication methods have been proposed and can simultaneously reduce memorization while improving performance (Kandpal et al., 2022; Lee et al., 2022a). The TruthfulQA benchmark (Lin et al., 2021) showed that GPT-3 models were more likely to mimic human falsehoods as they got larger, though Rae et al. (2021) later showed on a multiple-choice version that scaling Gopher to 280B enabled emergent performance substantially better than random.</p>
<p>Beyond the above, emergent risks also include phenomena that might only exist in future language models or that have not yet been characterized in current language models. Some such behaviors, as discussed in detail in Hendrycks et al. (2021b), could be backdoor vulnerabilities, inadvertent deception, or harmful content synthesis. Approaches involving data filtering, forecasting, governance, and automatically discovering harmful behaviors have been proposed for discovering and mitigating emergent risks (Bender et al., 2021; Weidinger et al., 2021; Steinhardt, 2021; Ganguli et al., 2022; Perez et al., 2022, inter alia). For a more detailed discussion of the risks of large language models, including emergent risks, see Bender et al. (2021); Steinhardt (2021); Bommasani et al. (2021); Ganguli et al. (2022).</p>
<h1>5.5 Sociological changes</h1>
<p>Finally, the emergent abilities discussed here focus on model behavior and are just one of several types of emergence in NLP (Manning et al., 2020; Teehan et al., 2022). Another notable type of qualitative change is sociological, in which increasing scale has shifted how the community views and uses language models. For instance, NLP has historically focused on task-specific models (Jurafsky \&amp; Martin, 2009). Recently, scaling has led to an explosion in research on and development of models that are "general purpose" in that they are single models that aim to perform a range of tasks not explicitly encoded in the training data (e.g., GPT-3, Chinchilla, and PaLM) (Manning, 2022).</p>
<p>One key set of results in the emergent sociological shift towards general-purpose models is when scaling enables a few-shot prompted general-purpose model to outperform prior state of the art held by finetuned task-specific models. As a few examples, GPT-3 175B achieved new state of the art on the TriviaQA and PiQA question-answering benchmarks (Brown et al., 2020); PaLM 540B achieved new state of the art on three arithmetic reasoning benchmarks (Chowdhery et al., 2022); and the multimodal Flamingo 80B model achieved new state of the art on six visual question answering benchmarks (Alayrac et al., 2022). In all of these cases, state-of-the-art performance was achieved by few-shot prompting a language model of unprecedented scale (scaling curves for these examples are shown in Appendix Figure 13). These abilities are not necessarily emergent since they have smooth, predictable scaling curves-however, they do underscore an emergent sociological shift towards general-purpose models in the NLP community.</p>
<p>The ability for general-purpose models to perform unseen tasks given only a few examples has also led to many new applications of language models outside the NLP research community. For instance, language models have been used via prompting to translate natural language instructions into actions executable by robots (Ahn et al., 2022; Huang et al., 2022), interact with users (Coenen et al., 2021; Wu et al., 2021; 2022a; Lee et al., 2022b), and facilitate multi-modal reasoning (Zeng et al., 2022; Alayrac et al., 2022). Large language models have also been deployed in the real-world both in products, such as GitHub CoPilot, ${ }^{7}$ and directly as services themselves, such as OpenAI's GPT-3 API. ${ }^{8}$</p>
<h3>5.6 Directions for future work</h3>
<p>Future work on emergent abilities could involve train more-capable language models, as well as methods for better enabling language models to perform tasks. Some potential directions include but are not limited to the following.</p>
<p>Further model scaling. Further scaling up models has so far appeared to increase the capabilities of language models, and is a straightforward direction for future work. However, simply scaling up language models is computationally expensive and requires solving substantial hardware challenges, and so other approaches will likely play a key role in the future of the emergent abilities of large language models.</p>
<p>Improved model architectures and training. Improving model architecture and training procedures may facilitate high-quality models with emergent abilities while mitigating computational cost. One direction is using sparse mixture-of-experts architectures (Lepikhin et al., 2021; Fedus et al., 2021; Artetxe et al., 2021; Zoph et al., 2022), which scale up the number of parameters in a model while maintaining constant computational costs for an input. Other directions for better computational efficiency could involve variable amounts of compute for different inputs (Graves, 2016; Dehghani et al., 2018), using more localized learning strategies than backpropagation through all weights in a neural network (Jaderberg et al., 2017), and augmenting models with external memory (Guu et al., 2020; Borgeaud et al., 2021; Wu et al., 2022b, inter alia). These nascent directions have already shown promise in many settings but have not yet seen widespread adoption, which will likely require further work.</p>
<p>Data scaling. Training long enough on a large-enough dataset has been shown to be key for the ability of language models to acquire syntactic, semantic, and other world knowledge (Zhang et al., 2021; Wei et al., 2021b; Razeghi et al., 2022). Recently, Hoffmann et al. (2022) argued that prior work (Kaplan et al., 2020)</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>underestimated the amount of training data needed to train a compute-optimal model, underscoring the importance of training data. Collecting large datasets so that models can be trained for longer could allow a greater range of emergent abilities under a fixed model size constraint.</p>
<p>Better techniques for and understanding of prompting. Although few-shot prompting Brown et al., 2020) is simple and effective, general improvements to prompting may further expand the abilities of language models. For instance, simple modifications such as calibrating output probabilities Zhao et al., 2021; Holtzman et al., 2021) or using a noisy channel Min et al., 2022a) have improved performance on a range of tasks. Augmenting few-shot exemplars with intermediate steps Reynolds and McDonell, 2021; Nye et al., 2021; Wei et al., 2022b) has also enabled models to perform multi-step reasoning tasks not possible in the standard prompting formulation from Brown et al. (2020). Moreover, better exploration of what makes prompting successful Wei et al., 2021a; Xie et al., 2022; Min et al., 2022b; Olsson et al., 2022) could lead to insights on how to elicit emergent abilities at a smaller model scale. Sufficient understanding of why models work generally lags the development and popularization of techniques such as few-shot prompting, and it is also likely that the best practices for prompting will change as more-powerful models are developed over time.</p>
<p>Frontier tasks. Although language models can perform a wide range of tasks, there are still many tasks that even the largest language models to date cannot perform with above-random accuracy. Dozens of such tasks from BIG-Bench are enumerated in Appendix E.4; these tasks often involve abstract reasoning (e.g., playing Chess, challenging math, etc). Future research could potentially investigate why these abilities have not yet emerged, and how to enable models to perform these tasks. Looking forward, another growing direction could be multilingual emergence; results on multilingual BIG-Bench tasks indicate that both model scale and training data play a role in emergence (e.g., Figure 2D shows that both using PaLM's training dataset and scaling to 62B parameters is required for question-answering in Persian). Other frontier tasks could include prompting in multiple modalities Alayrac et al., 2022; Ramesh et al., 2022).</p>
<p>Understanding emergence. Beyond research on unlocking further emergence, an open question for future research is how and why emergent abilities occur in large language models. This paper conducted initial analyses regarding scaling of the cross-entropy loss on BIG-Bench Appendix A.1), different metrics for generative tasks Appendix A.2), and which types of tasks emergence occurs Appendix A. 3 and Appendix B). These analyses did not provide complete answers to why emergence occurs or how to predict it. Future research could potentially analyze emergence in new ways (e.g., analyze the relationship between emergent tasks and similar data in training; create a synthetic task that requires multiple compositional sub-tasks and evaluate how each of those sub-tasks improve with scale and unlock emergence when combined). Overall, understanding emergence is an important direction because it could potentially allow us predict what abilities future models may have, as well as provide new insights into how to train more-capable language models.</p>
<h1>6 Conclusions</h1>
<p>We have discussed emergent abilities of language models, for which meaningful performance has only been thus far observed at a certain computational scale. Emergent abilities can span a variety of language models, task types, and experimental scenarios. Such abilities are a recently discovered outcome of scaling up language models, and the questions of how they emerge and whether more scaling will enable further emergent abilities seem to be important future research directions for the field of NLP.</p>
<h2>Broader Impact Statement</h2>
<p>In this paper, we surveyed results in the existing literature, without proposing new methods or models. As discussed in (5), emergent abilities are unpredictable in several ways, and include emergent risks (5.4). We believe these phenomena warrant careful study and raise important questions for the field.</p>
<h2>Acknowledgments</h2>
<p>We thank Charles Sutton, Slav Petrov, Douglas Eck, Jason Freidenfelds, Jascha Sohl-Dickstein, Ethan Dyer, Dale Schuurmans, and Xavier Garcia for useful discussions and feedback on the manuscript.</p>
<h1>References</h1>
<p>Omri Abend, Tom Kwiatkowski, Nathaniel J Smith, Sharon Goldwater, and Mark Steedman. Bootstrapping language acquisition. Cognition, 164:116-143, 2017. URL https://homepages.inf.ed.ac.uk/ sgwater/papers/cognition17-bootstrapping.pdf.</p>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. URL https://arxiv.org/ abs/2204.01691.</p>
<p>Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022. URL https://arxiv.org/abs/2204.14198.</p>
<p>Philip W. Anderson. More is different: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393-396, 1972. URL http://www.lanais.famaf.unc.edu.ar/cursos/ em/Anderson-MoreDifferent-1972.pdf.</p>
<p>Simran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher R. Ask me anything: A simple strategy for prompting language models. arXiv preprint arXiv:2210.02441, 2022. URL https://arxiv.org/abs/2210.02441.</p>
<p>Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021. URL https://arxiv.org/abs/2112.10684.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. URL https://arxiv.org/abs/2112.00861.</p>
<p>Fabrice Bellard. gpt2tc: Text completion and compression using GPT-2, 2021. URL https://bellard. org/libnc/gpt2tc.html. Accessed Apr. 26, 2022.</p>
<p>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 2021. URL https://dl.acm.org/doi/pdf/10.1145/3442188. 3445922 .</p>
<p>BIG-Bench. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. URL https://arxiv.org/abs/2206.04615.</p>
<p>Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL https://arxiv.org/abs/2108. 07258 .</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. arXiv preprint arXiv:2112.04426, 2021. URL https: //arxiv.org/abs/2112.04426.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. URL https://papers.nips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. USENIX Security, 2021. URL https://www.usenix.org/conference/usenixsecurity21/ presentation/carlini-extracting.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022. URL https://arxiv.org/abs/2202.07646.</p>
<p>Stephanie C.Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh, Pierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022. URL https://arxiv.org/abs/2205.05055.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, et al. PaLM: Scaling language modeling with Pathways. arXiv preprint arXiv:2204.02311, 2022. URL https://arxiv.org/abs/2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. URL https://arxiv.org/abs/2210.11416.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. Wordcraft: A human-AI collaborative editor for story writing. arXiv preprint arXiv:2107.07430, 2021. URL https://arxiv.org/abs/2107. 07430 .</p>
<p>Antonella Corradini and Timothy O'Connor. Emergence in science and philosophy, volume 6. Routledge, 2010. URL https://books.google.com/books?hl=en\&amp;lr=\&amp;id=55RaBwAAQBAJ\&amp;oi= fnd\&amp;pg=PP1\&amp;dq=Emergence+in+science+and+philosophy\&amp;ots=2_8VNDXLfv\&amp;sig=1aisq_ WouF95Cx58WWMZ0Gq3RNk.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. URL https://arxiv.org/abs/1807.03819.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. URL https://aclanthology.org/ N19-1423.</p>
<p>Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language models with mixture-of-experts. ICML, 2021. URL https://arxiv.org/abs/2112.06905.</p>
<p>William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. URL https://arxiv.org/ abs/2101.03961.</p>
<p>Stephanie Forrest. Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks. Physica D: Nonlinear Phenomena, 42(1-3):1-11, 1990. URL https://www.sciencedirect.com/science/article/abs/pii/0167278990900630.</p>
<p>Deep Ganguli, Danny Hernandez, Liane Lovitt, Nova DasSarma, Tom Henighan, Andy Jones, Nicholas Joseph, Jackson Kernion, Ben Mann, Amanda Askell, et al. Predictability and surprise in large generative models. arXiv preprint arXiv:2202.07785, 2022. URL https://arxiv.org/abs/2202.07785.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. ACL, 2021. doi: 10.18653/v1/2021.acl-long.295. URL https://aclanthology.org/2021.acl-long. 295.</p>
<p>Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of EMNLP, 2020. doi: 10.18653/v1/ 2020.findings-emnlp.301. URL https://aclanthology.org/2020.findings-emnlp.301.</p>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. URL https://arxiv.org/abs/1603.08983.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ICML, 2020. URL https://arxiv.org/abs/2002.08909.</p>
<p>David A. Harper and Paul A. Lewis. New perspectives on emergence in economics. New Perspectives on Emergence in Economics, pp. 2-3, 2012. URL https://www.sciencedirect. com/science/article/pii/S0167268112000200?casa_token=fLs2nCYo_64AAAAA: H2sSpSygJmEqXgmpM4jLyeppph3C4TgEsaSXm5RkOpTOr4q2A1x9Su3u4uycK4sIC6a8NdLiSw.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ICLR, 2021a. URL https://openreview.net/ forum?id=d7KBjmI3GmQ.</p>
<p>Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ML safety. arXiv preprint arXiv:2109.13916, 2021b. URL https://arxiv.org/abs/2109.13916.</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. NeurIPS, 2022. URL https://arxiv.org/abs/2203.15556.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isn't always right. EMNLP, 2021. URL https://aclanthology. org/2021.emnlp-main.564.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022. URL https://arxiv.org/pdf/2201.07207.</p>
<p>Bernardo A. Huberman and Tad Hogg. Phase transitions in artificial intelligence systems. Artificial Intelligence, 33(2):155-171, 1987. URL https://www.sciencedirect.com/science/article/ abs/pii/0004370287900336.</p>
<p>Harold Y. Hwang, Yoh Iwasa, Masashi Kawasaki, Bernhard Keimer, Naoto Nagaosa, and Yoshinori Tokura. Emergent phenomena at oxide interfaces. Nature Materials, 11(2):103-113, 2012. URL https://www. nature.com/articles/nmat3223.</p>
<p>Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver, and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. ICML, 2017. URL https://arxiv.org/abs/1608.05343.</p>
<p>Dan Jurafsky and James H. Martin. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall series in Artificial Intelligence. Pearson Prentice Hall, 2009. ISBN 9780131873216. URL https://books.google.com/books?id= fZmj5UNK8AQC.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. URL https://arxiv.org/abs/2207.05221.</p>
<p>Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. ICML, 2022. URL https://arxiv.org/abs/2202.06539.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916.</p>
<p>Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. Can language models learn from explanations in context? Findings of EMNLP, 2022. URL https://arxiv.org/abs/2204.02329.</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. ACL, 2022a. URL https://arxiv.org/abs/2107.06499.</p>
<p>Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-AI collaborative writing dataset for exploring language model capabilities. CHI, 2022b. URL https://arxiv.org/abs/2201.06796.</p>
<p>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. ICLR, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.</p>
<p>Percy Liang. Semi-supervised learning for natural language. PhD thesis, Massachusetts Institute of Technology, 2005. URL https://www-cs.stanford.edu/ pliang/papers/meng-thesis.pdf.</p>
<p>Percy Liang, Rishi Bommasani, Kathleen A. Creel, and Rob Reich. The time is now to develop community norms for the release of foundation models, 2022. URL https://crfm.stanford.edu/2022/05/17/ community-norms.html.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. URL https://arxiv.org/abs/2109.07958.</p>
<p>Christopher D. Manning. Human language understanding \&amp; reasoning. Daedalus, 151(2):127-138, 2022. URL https://www.amacad.org/publication/human-language-understanding-reasoning.</p>
<p>Christopher D. Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the National Academy of Sciences, 117(48):30046-30054, 2020. URL https://www.pnas.org/doi/10.1073/pnas. 1907367117 .</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. URL https://arxiv. org/abs/1806.08730.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. URL https://huggingface.co/datasets/wikitext.</p>
<p>Scott Miller, Jethran Guinness, and Alex Zamanian. Name tagging with word clusters and discriminative training. In NAACL, 2004. URL https://aclanthology.org/N04-1043.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. ACL, 2022a. URL https://arxiv.org/abs/2108.04106.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022b. URL https://arxiv.org/abs/2202.12837.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. URL https://openreview.net/forum?id=iedYJm92o0a.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, et al. In-context learning and induction heads. Transformer Circuits, 2022. URL https: //transformer-circuits.pub/2022/in-context-learning-and-induction-heads/ index.html.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://arxiv.org/abs/2203.02155.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: a method for automatic evaluation of machine translation. In ACL, 2002. URL https://aclanthology.org/P02-1040.pdf.</p>
<p>Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question answering. In Findings of $A C L, 2022$. URL https://arxiv.org/abs/2110.08193.</p>
<p>Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. ICLR, 2022. URL https://openreview.net/forum?id=gJcEM8sxHK.</p>
<p>Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. arXiv preprint arXiv:2202.03286, 2022. URL https://arxiv.org/abs/2202.03286.</p>
<p>Mohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. NAACL, 2019. URL https://aclanthology.org/ N19-1128.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8), 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_ are_unsupervised_multitask_learners.pdf.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446, 2021. URL https://arxiv.org/abs/ 2112.11446 .</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020. URL https://jmlr.org/papers/v21/20-074.html.</p>
<p>Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. URL https://arxiv.org/ abs/2204.06125.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. URL https://arxiv.org/ abs/2202.07206.</p>
<p>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, 2021. URL https://arxiv.org/abs/2102.07350.</p>
<p>Rachel Rudinger, Chandler May, and Benjamin Van Durme. Social bias in elicited natural language inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, 2017. URL https://aclanthology.org/W17-1609.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. ICLR, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.</p>
<p>Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language models help solve downstream tasks. ICLR, 2021. URL https://arxiv.org/abs/2010.03648.</p>
<p>Timo Schick and Hinrich Schtze. It's not just size that matters: Small language models are also few-shot learners. NAACL, June 2021. URL https://aclanthology.org/2021.naacl-main.185.</p>
<p>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. URL https://arxiv.org/abs/ 2210.03057 .</p>
<p>Jacob Steinhardt. On the risks of emergent behavior in foundation models, October 2021. URL https://bounded-regret.ghost.io/ on-the-risks-of-emergent-behavior-in-foundation-models/. Accessed Apr 13, 2022.</p>
<p>Jacob Steinhardt. Future ml systems will be qualitatively different, 2022. URL https://bounded-regret. ghost.io/future-ml-systems-will-be-qualitatively-different/. Accessed May 20, 2022 .</p>
<p>Mirac Suzgun, Nathan Scales, Nathaneal Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny ZHou, and Jason Wei. Challenging BIG-Bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. URL https: //arxiv.org/abs/2210.09261.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022a. URL https://arxiv.org/abs/2205.05131.</p>
<p>Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. arXiv preprint arXiv:2202.06991, 2022b. URL https://arxiv.org/abs/2202.06991.</p>
<p>Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scaling laws with $0.1 \%$ extra compute. arXiv preprint arXiv:2210.11399, 2022c. URL https://arxiv.org/abs/2210.11399.</p>
<p>Ryan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar Mirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large language models. In ACL Big Science Workshop, 2022. URL https://aclanthology.org/2022.big Science-1.11/.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. URL https://arxiv.org/abs/2201.08239.</p>
<p>Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847, 2018. URL https://arxiv.org/abs/1806.02847.</p>
<p>Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining objective work best for zero-shot generalization? ICML, 2022a. URL https://arxiv.org/abs/2204.05832.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b. URL https: //arxiv.org/abs/2203.11171.</p>
<p>Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks? An analysis of head and prompt tuning. NeurIPS, 2021a. URL https://openreview.net/ forum?id=MDMV2SxCboX.</p>
<p>Jason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick. Frequency effects on syntactic rule learning in transformers. EMNLP, 2021b. doi: 10.18653/v1/2021.emnlp-main.72. URL https://aclanthology. org/2021.emnlp-main.72.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ICLR, 2022a. URL https: //openreview.net/forum?id=gEZrGCozdqR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. NeurIPS, 2022b. URL https://arxiv. org/abs/2201.11903.</p>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359, 2021. URL https://arxiv.org/abs/2112.04359.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie J. Cai. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. arXiv preprint arXiv:2110.01691, 2021. URL https://arxiv.org/abs/2110.01691.</p>
<p>Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J. Cai. PromptChainer: Chaining large language model prompts through visual programming. arXiv preprint arXiv:2203.06566, 2022a. URL https://arxiv.org/abs/2203.06566.</p>
<p>Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022b.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. ICLR, 2022. URL https://arxiv.org/abs/2111.02080.</p>
<p>Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. URL https://arxiv. org/abs/2204.00598.</p>
<p>Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need billions of words of pretraining data? In $A C L, 2021$. doi: $10.18653 / \mathrm{v} 1 / 2021$.acl-long. 90 . URL https://aclanthology. org/2021.acl-long. 90 .</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. ICML, 2021. URL https://arxiv.org/abs/2102.09690.</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs/2205.10625.</p>
<p>Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022. URL https: //arxiv.org/abs/2202.08906.</p>
<h1>A BIG-Bench analysis</h1>
<h2>A. 1 Cross-entropy loss analysis</h2>
<p>Here we study how scaling curves may appear differently depending on the evaluation metric used to measure performance. We will focus on the six few-shot prompted BIG-Bench tasks that we consider emergent for LaMDA models. Three of these tasks are generative and use Exact Match (EM) or BLEU (Papineni et al., 2002) as the evaluation metric. The other three tasks are classification and use accuracy (acc) as the evaluation metric.</p>
<p>In the scaling curves for these tasks, peformance in EM/BLEU/acc is close to random for small models ( $\leq 10^{22}$ FLOPs / $\leq 27$ B params). We will compare these scaling curves against alternative plots that have a different $y$-axis measured by cross-entropy loss. Cross-entropy loss differs from EM/BLEU/acc in that it captures improvements in performance (the predicted distribution getting closer to ground truth) even when the EM/BLEU/acc is random. For example, if two examples are both wrong as measured by EM/BLEU/acc, one example may be closer to the ground truth in terms of probabilities, and this information is captured by the cross-entropy loss.</p>
<p>These plots are expected to look like one of the following:</p>
<ul>
<li>Outcome 1: For the model scales where EM/BLEU/acc is random, cross-entropy loss also does not improve as scale increases. This outcome implies that for these scales, the model truly does not get any better at the tasks.</li>
<li>Outcome 2: For the model scales where EM/BLEU/acc is random, cross-entropy loss does improve. This outcome implies that the models do get better at the task, but these improvements are not reflected in the downstream metric of interest. The broader implication is that scaling small models improves the models in a way that is not reflected in EM/BLEU/Acc, and that there is some critical model scale where these improvements enable the downstream metric to increase to above random as an emergent ability.</li>
</ul>
<p>We find that all six BIG-Bench tasks fall under Outcome 2, and detail this analysis below. Overall, the conclusion from this analysis is that small models do improve in some ways that downstream metrics that EM/BLEU/Acc do not capture. However, these tasks are still considered emergent, and this analysis does not provide any straightforward indicators of how to predict such emergent behaviors.</p>
<h2>A.1.1 Generative tasks</h2>
<p>Figure 5 shows the cross-entropy loss on the three generative BIG-Bench tasks (modified arithmetic, IPA transliterate, and word unscramble) alongside the downstream evaluation metrics used in Figure 2. For all three tasks, notice that while the error rate is nearly $100 \%$ for small models ( $\leq 10^{22}$ FLOPs / $\leq 27$ B params), the cross-entropy loss does actually improve for these model sizes. At the point of emergence as measured by error rate, we also see an "elbow" in performance improvement for cross-entropy loss.</p>
<h2>A.1.2 Classification tasks</h2>
<p>Figure 6 (middle row) shows the cross-entropy loss of the three classification BIG-Bench tasks. Similar to the generative tasks, when the error rate is close to random, cross-entropy loss consistently still improves for models trained with more compute. This again shows that performance as computed by accuracy can mask consistent improvements in the likelihood of the target sequences.</p>
<p>We also perform an additional analysis of the multiple choice emergent tasks in Figure 6 (bottom row), which shows the log probabilities of the correct response and incorrect response(s). We find that the crossentropy loss decreases for both the correct and incorrect responses in the three emergent multiple choice tasks. Counterintuitively, both log-probabilities can decrease in tandem even when the probability across all available multiple choice responses is normalized. The reason is that larger models produce less-extreme probabilities (i.e., values approaching 0 or 1 ) and therefore the average log-probabilities have fewer extremely</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Adjacent plots for error rate and cross-entropy loss on three emergent generative tasks in BIG-Bench for LaMDA. We show error rate for both greedy decoding $(T=0)$ as well as random sampling $(T=1)$. Error rate is ( 1 - exact match score) for modified arithmetic and word unscramble, and ( 1 - BLEU score) for IPA transliterate.
small values. However, we note that for each of these three tasks, that the average log-probability of the correct and incorrect responses eventually deviates at a certain scale, during which performance on the task increases substantially.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Adjacent plots for error rate, cross-entropy loss, and log probabilities of correct and incorrect responses on three classification tasks on BIG-Bench that we consider to demonstrate emergent abilities. Logical arguments only has 32 samples, which may contribute to noise. Error rate is ( 1 - accuracy).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://copilot.github.com/
${ }^{8}$ https://beta.openai.com/docs/introduction&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
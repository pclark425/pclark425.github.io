<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8804 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8804</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8804</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-220047360</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2020.acl-main.67.pdf" target="_blank">Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks</a></p>
                <p><strong>Paper Abstract:</strong> Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the first-order adjacency information. 2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8804.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8804.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LineGraph+Concept/Relation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Line Graph-based decomposition into concept graph and relation (edge) graph</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforms an AMR input graph by turning labeled edges into nodes of a line graph, producing two unlabeled sub-graphs: a concept graph (original nodes, labels removed on edges) and a relation graph (the line graph L(G) whose nodes are original edges). These two graphs are encoded jointly with cross-graph attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Line graph decomposition (concept graph + relation graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Given an AMR graph G_a=(V_a,E_a), construct: (1) concept graph G_c with V_c=V_a and E_c = E_a with labels removed (unlabeled directed edges between concept nodes); (2) relation graph G_e = L(G_a) (the line graph) where each node represents an original labeled edge and two relation-nodes are adjacent iff their corresponding edges share a concept node in G_a; directions in the original directed graph are preserved in L(G). Redundant parallel edges in L(G) are removed. Alignments between relation nodes and concept nodes are created via endpoints of original edges.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted directed semantic graphs with labeled edges)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute line graph L(G_a) from the original AMR; produce concept graph by removing edge labels; optionally split word nodes into subword nodes (BPE) and link subwords with a special 'subword' edge; produce masking matrix M to align relation-nodes to their endpoint concept-nodes for masked cross-attention during encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence natural language generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Using this representation with Mix-Order GAT encoder and Transformer decoder: BLEU on LDC2015E86 = 30.58 (K=4), Meteor = 35.81; BLEU on LDC2017T10 = 32.46 (K=4), Meteor = 36.78. Comparisons shown for K=1 (BLEU 28.64/29.96), K=2 (29.62/31.06).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to prior best Structural Transformer (Zhu et al., 2019) (BLEU 29.66 / 31.54), LineGraph+MixGAT (K=4) improves BLEU by +0.92 on both datasets. Ablation: removing relation-graph structure (i.e. treating relation nodes as isolated K_e=0) causes BLEU drops of 1.69 (LDC2015E86) and 1.38 (LDC2017T10), showing explicit edge relations help.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly models relations among labeled edges (edge-to-edge topology) in addition to concept-to-concept relations; helps disambiguate lexical realizations that depend on edge context; improves handling of reentrancies and larger graphs; both subgraphs have unlabeled edges enabling efficient encoding by MixGAT.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases the number of graph nodes (adds relation nodes corresponding to edges) which enlarges the encoded graph structure (paper does not quantify runtime cost). The paper does not report explicit compute or memory overhead comparisons versus simpler encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When the relation graph is not used or relation nodes do not interact (K_e=0), the model can fail to recover correct lexical distinctions (example: generates 'competition' instead of 'competitors') and mis-handle parallel structures; these failure cases are demonstrated in the paper's case studies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8804.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8804.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MixGAT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mix-Order Graph Attention Network (Mix-Order GAT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-attention encoder that integrates multi-hop (1..K) neighborhood information in a single layer by computing per-order attention and concatenating projected attentive features across orders to obtain node representations; supports directed graphs and is combined with dual-graph (reversed edges) updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Mix-Order Graph Attention encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each node x_i and for each order k=1..K, define R_k(x_i) as nodes reachable within k hops (R_1 includes self). Compute attention weights α^k_ij over nodes in R_k(x_i) with per-order projection matrices W^k, compute the attentive summary for each order, then concatenate the K order-specific summaries and apply nonlinearity and feed-forward nets. Residual connections, layer normalization and FFN applied; dual (reversed-edge) graph updates are merged to capture bidirectional flow.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Graphs with unlabeled directed edges (used here on concept graph and relation/line graph derived from AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a text-conversion method per se; MixGAT is an encoder that converts graph-structured node features into context-aware node representations by combining multi-hop neighborhood information in a single layer (avoids deep stacking to reach distant nodes). It is used together with masked cross-attention to combine concept and relation representations, and the decoder generates text from the concept node encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph-to-sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Encoder with K=4 neighborhoods yields final system BLEU: LDC2015E86 = 30.58, LDC2017T10 = 32.46 (Meteor 35.81 / 36.78). Ablation: K from 1→4 increased BLEU by +1.94 (LDC2015E86) and +2.50 (LDC2017T10); K=1 baseline (MixGAT but only 1-hop) gives BLEU 28.64/29.96.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MixGAT (K=4) outperforms Transformer-based Structural Transformer (Zhu et al., 2019) by +0.92 BLEU on both datasets; the paper contrasts MixGAT's explicit multi-order graph encoding with Transformers that model arbitrary pairs but ignore edge topological structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Captures higher-order (multi-hop) dependencies in a single layer rather than relying on many stacked layers; improves performance especially on larger graphs and graphs with many reentrancies; integrates naturally with line-graph decomposition to encode edge relations.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires choosing neighborhood order K (performance sensitive to K); increases number of learnable projection matrices (one per order) which may increase parameter count. The paper does not provide explicit training-time or memory cost trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When K is too small (e.g., K=1) MixGAT fails to capture distant dependencies, leading to lower BLEU, and the performance gap widens for larger AMR graphs. No other explicit failure modes for MixGAT are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8804.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8804.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph linearization (DFS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization via depth-first traversal (sequence-to-sequence linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Early AMR-to-text approaches linearize the AMR graph into a token sequence (e.g., via depth-first traversal) and apply sequence-to-sequence models to generate text, but this risks losing graph structural information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Sequence linearization (depth-first traversal)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the AMR graph (commonly depth-first) to produce a linearized token sequence that encodes nodes and edges (and sometimes edge labels) in order; feed the sequence into a seq2seq encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted directed semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal / serialization of graph into a linear token sequence; sometimes augmented with syntactic information or sub-word splitting.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported baseline: Seq2Seq BLEU on LDC2015E86 = 22.00 (paper's Table 1). Sequence-based models perform worse than graph-based encoders in the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Sequence linearization baselines are outperformed by graph-based encoders (GCN, GGNN) and by transformer-based structural encoders; the paper reports much lower BLEU for seq2seq linearization than for MixGAT + line graph.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple and compatible with existing seq2seq infrastructure; straightforward to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization may lose or obscure graph topology and labeled-edge structure, harming generation quality; performs worst among compared approaches in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Per paper results, linearized seq2seq models fail to capture structural dependencies and obtain substantially lower BLEU, especially on sentences whose realizations depend on non-local graph structure.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8804.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8804.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structural Transformer (relation-aware self-attn)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modeling graph structure in transformer for better AMR-to-text generation (Structural Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based encoder that augments self-attention with relation-aware structure encodings so arbitrary concept pairs can interact regardless of direct connectivity; it models structural label sequences between concept pairs but does not preserve edge topological structure for edges themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure in transformer for better AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relation-aware self-attention (Structural Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Extend Transformer self-attention with structural features encoding paths/relations between concept pairs (relation-aware attention) so any pair of concepts has attention informed by graph-structure-derived features or label-sequence encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (concept-level encoding with structural relations)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute structural relation encodings (e.g., label sequences or path features) for each concept pair and incorporate these into Transformer self-attention; no explicit line-graph construction.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in paper as baseline: BLEU on LDC2015E86 = 29.66, LDC2017T10 = 31.54; Meteor 35.45 / 36.02.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Structural Transformer is the previous state-of-the-art cited; LineGraph+MixGAT (K=4) improves BLEU by +0.92 on both datasets. Authors argue Structural Transformer models arbitrary concept pairs but ignores topological structures of edges (i.e., edge-to-edge relations).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Directly models arbitrary concept pairs and indirect relations without deep graph stacking; strong prior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>According to this paper, it still ignores explicit topological relationships among labeled edges (edge structures), which the line-graph decomposition addresses.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper argues it may be less effective than line-graph+MixGAT when edge-to-edge relations and reentrancies are critical; no case-level failures from Zhu et al. are reproduced beyond aggregate comparisons.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8804.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8804.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BPE + subword nodes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Byte Pair Encoding with subword-node splitting and subword-edge linking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To mitigate data sparsity in AMR node labels and the decoder vocabulary, words in AMR nodes and references are split into subwords via BPE; node splitting is represented by connecting subword nodes with a special 'subword' edge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural machine translation of rare words with subword units</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subword node splitting with 'subword' edges (BPE)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Apply BPE to segment both AMR graph word nodes and target reference text into subword units; if an AMR word node splits into several subword nodes, connect these subword nodes with a special 'subword' edge in the concept graph, then construct line graph and encode as usual. Decoder and encoder share subword vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (word nodes split into subword nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Tokenize words with BPE; replace a single word-node by a small connected chain of subword nodes linked by a 'subword' edge; include these nodes/edges when building concept graph and line graph.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (reducing data sparsity in sequence generation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported as an isolated metric; used as preprocessing in all reported experiments (system scores above include BPE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper follows previous work (Zhu et al., 2019) in using BPE and shared vocabulary; the paper reports that this preprocessing helps sequence generation but does not provide isolated ablation numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Addresses data sparsity and rare word problems; allows sharing vocabulary between encoder and decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases node/edge counts in the graph (more fine-grained nodes), which may increase encoder graph size; no explicit cost/overhead numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases reported for BPE/subword splitting in the paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8804.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8804.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Graph representation (reversed-edge auxiliary graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Maintain both the original directed graph and a dual graph with reversed edge directions to permit top-down and bottom-up information propagation; update node embeddings on both graphs and combine them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing AMR-to-text generation with dual graph representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dual-graph bidirectional propagation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For a directed input graph, also construct a dual graph with the same nodes but reversed edge directions. Encode both graphs (here with MixGAT using different neighborhood info R_K) and combine the resulting node representations (concatenate or project) so that information flows in both directions and unidirectional losses are reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Directed AMR graphs (concept graph and relation graph treated with dual graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create dual neighborhood lists R_K for reversed-edge graph and run MixGAT updates on both original and dual; merge their outputs via projection matrices and FFN per encoding layer.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Dual Graph method cited as prior baseline in Table 1 (reported BLEU and Meteor as a baseline; included in comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper builds on prior idea of dual graphs (Ribeiro et al., 2019) and uses dual updates inside each MixGAT layer; combining dual updates helps retain bidirectional structural information compared to strictly unidirectional GAT updates.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Restores information lost by strictly unidirectional message passing; captures both top-down and bottom-up flows.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires maintaining and encoding two neighborhood views; potential extra computation (paper merges outputs but does not report runtime penalties).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No direct failure cases reported for dual-graph usage; it is used to mitigate the failure mode of unidirectional propagation losing reversed-direction structural info.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Modeling graph structure in transformer for better AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Enhancing AMR-to-text generation with dual graph representations <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for amr-to-text generation <em>(Rating: 2)</em></li>
                <li>Densely connected graph convolutional networks for graph-to-sequence learning <em>(Rating: 1)</em></li>
                <li>Neural machine translation of rare words with subword units <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8804",
    "paper_id": "paper-220047360",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "LineGraph+Concept/Relation",
            "name_full": "Line Graph-based decomposition into concept graph and relation (edge) graph",
            "brief_description": "Transforms an AMR input graph by turning labeled edges into nodes of a line graph, producing two unlabeled sub-graphs: a concept graph (original nodes, labels removed on edges) and a relation graph (the line graph L(G) whose nodes are original edges). These two graphs are encoded jointly with cross-graph attention.",
            "citation_title": "Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks",
            "mention_or_use": "use",
            "representation_name": "Line graph decomposition (concept graph + relation graph)",
            "representation_description": "Given an AMR graph G_a=(V_a,E_a), construct: (1) concept graph G_c with V_c=V_a and E_c = E_a with labels removed (unlabeled directed edges between concept nodes); (2) relation graph G_e = L(G_a) (the line graph) where each node represents an original labeled edge and two relation-nodes are adjacent iff their corresponding edges share a concept node in G_a; directions in the original directed graph are preserved in L(G). Redundant parallel edges in L(G) are removed. Alignments between relation nodes and concept nodes are created via endpoints of original edges.",
            "graph_type": "AMR graphs (rooted directed semantic graphs with labeled edges)",
            "conversion_method": "Compute line graph L(G_a) from the original AMR; produce concept graph by removing edge labels; optionally split word nodes into subword nodes (BPE) and link subwords with a special 'subword' edge; produce masking matrix M to align relation-nodes to their endpoint concept-nodes for masked cross-attention during encoding.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence natural language generation)",
            "performance_metrics": "Using this representation with Mix-Order GAT encoder and Transformer decoder: BLEU on LDC2015E86 = 30.58 (K=4), Meteor = 35.81; BLEU on LDC2017T10 = 32.46 (K=4), Meteor = 36.78. Comparisons shown for K=1 (BLEU 28.64/29.96), K=2 (29.62/31.06).",
            "comparison_to_others": "Compared to prior best Structural Transformer (Zhu et al., 2019) (BLEU 29.66 / 31.54), LineGraph+MixGAT (K=4) improves BLEU by +0.92 on both datasets. Ablation: removing relation-graph structure (i.e. treating relation nodes as isolated K_e=0) causes BLEU drops of 1.69 (LDC2015E86) and 1.38 (LDC2017T10), showing explicit edge relations help.",
            "advantages": "Explicitly models relations among labeled edges (edge-to-edge topology) in addition to concept-to-concept relations; helps disambiguate lexical realizations that depend on edge context; improves handling of reentrancies and larger graphs; both subgraphs have unlabeled edges enabling efficient encoding by MixGAT.",
            "disadvantages": "Increases the number of graph nodes (adds relation nodes corresponding to edges) which enlarges the encoded graph structure (paper does not quantify runtime cost). The paper does not report explicit compute or memory overhead comparisons versus simpler encodings.",
            "failure_cases": "When the relation graph is not used or relation nodes do not interact (K_e=0), the model can fail to recover correct lexical distinctions (example: generates 'competition' instead of 'competitors') and mis-handle parallel structures; these failure cases are demonstrated in the paper's case studies.",
            "uuid": "e8804.0"
        },
        {
            "name_short": "MixGAT",
            "name_full": "Mix-Order Graph Attention Network (Mix-Order GAT)",
            "brief_description": "A graph-attention encoder that integrates multi-hop (1..K) neighborhood information in a single layer by computing per-order attention and concatenating projected attentive features across orders to obtain node representations; supports directed graphs and is combined with dual-graph (reversed edges) updates.",
            "citation_title": "Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks",
            "mention_or_use": "use",
            "representation_name": "Mix-Order Graph Attention encoding",
            "representation_description": "For each node x_i and for each order k=1..K, define R_k(x_i) as nodes reachable within k hops (R_1 includes self). Compute attention weights α^k_ij over nodes in R_k(x_i) with per-order projection matrices W^k, compute the attentive summary for each order, then concatenate the K order-specific summaries and apply nonlinearity and feed-forward nets. Residual connections, layer normalization and FFN applied; dual (reversed-edge) graph updates are merged to capture bidirectional flow.",
            "graph_type": "Graphs with unlabeled directed edges (used here on concept graph and relation/line graph derived from AMR)",
            "conversion_method": "Not a text-conversion method per se; MixGAT is an encoder that converts graph-structured node features into context-aware node representations by combining multi-hop neighborhood information in a single layer (avoids deep stacking to reach distant nodes). It is used together with masked cross-attention to combine concept and relation representations, and the decoder generates text from the concept node encodings.",
            "downstream_task": "AMR-to-text generation (graph-to-sequence)",
            "performance_metrics": "Encoder with K=4 neighborhoods yields final system BLEU: LDC2015E86 = 30.58, LDC2017T10 = 32.46 (Meteor 35.81 / 36.78). Ablation: K from 1→4 increased BLEU by +1.94 (LDC2015E86) and +2.50 (LDC2017T10); K=1 baseline (MixGAT but only 1-hop) gives BLEU 28.64/29.96.",
            "comparison_to_others": "MixGAT (K=4) outperforms Transformer-based Structural Transformer (Zhu et al., 2019) by +0.92 BLEU on both datasets; the paper contrasts MixGAT's explicit multi-order graph encoding with Transformers that model arbitrary pairs but ignore edge topological structure.",
            "advantages": "Captures higher-order (multi-hop) dependencies in a single layer rather than relying on many stacked layers; improves performance especially on larger graphs and graphs with many reentrancies; integrates naturally with line-graph decomposition to encode edge relations.",
            "disadvantages": "Requires choosing neighborhood order K (performance sensitive to K); increases number of learnable projection matrices (one per order) which may increase parameter count. The paper does not provide explicit training-time or memory cost trade-offs.",
            "failure_cases": "When K is too small (e.g., K=1) MixGAT fails to capture distant dependencies, leading to lower BLEU, and the performance gap widens for larger AMR graphs. No other explicit failure modes for MixGAT are reported.",
            "uuid": "e8804.1"
        },
        {
            "name_short": "Graph linearization (DFS)",
            "name_full": "Graph linearization via depth-first traversal (sequence-to-sequence linearization)",
            "brief_description": "Early AMR-to-text approaches linearize the AMR graph into a token sequence (e.g., via depth-first traversal) and apply sequence-to-sequence models to generate text, but this risks losing graph structural information.",
            "citation_title": "Neural AMR: sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "Sequence linearization (depth-first traversal)",
            "representation_description": "Traverse the AMR graph (commonly depth-first) to produce a linearized token sequence that encodes nodes and edges (and sometimes edge labels) in order; feed the sequence into a seq2seq encoder-decoder model.",
            "graph_type": "AMR graphs (rooted directed semantic graphs)",
            "conversion_method": "Depth-first traversal / serialization of graph into a linear token sequence; sometimes augmented with syntactic information or sub-word splitting.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported baseline: Seq2Seq BLEU on LDC2015E86 = 22.00 (paper's Table 1). Sequence-based models perform worse than graph-based encoders in the paper's comparisons.",
            "comparison_to_others": "Sequence linearization baselines are outperformed by graph-based encoders (GCN, GGNN) and by transformer-based structural encoders; the paper reports much lower BLEU for seq2seq linearization than for MixGAT + line graph.",
            "advantages": "Simple and compatible with existing seq2seq infrastructure; straightforward to implement.",
            "disadvantages": "Linearization may lose or obscure graph topology and labeled-edge structure, harming generation quality; performs worst among compared approaches in the experiments.",
            "failure_cases": "Per paper results, linearized seq2seq models fail to capture structural dependencies and obtain substantially lower BLEU, especially on sentences whose realizations depend on non-local graph structure.",
            "uuid": "e8804.2"
        },
        {
            "name_short": "Structural Transformer (relation-aware self-attn)",
            "name_full": "Modeling graph structure in transformer for better AMR-to-text generation (Structural Transformer)",
            "brief_description": "A transformer-based encoder that augments self-attention with relation-aware structure encodings so arbitrary concept pairs can interact regardless of direct connectivity; it models structural label sequences between concept pairs but does not preserve edge topological structure for edges themselves.",
            "citation_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "Relation-aware self-attention (Structural Transformer)",
            "representation_description": "Extend Transformer self-attention with structural features encoding paths/relations between concept pairs (relation-aware attention) so any pair of concepts has attention informed by graph-structure-derived features or label-sequence encodings.",
            "graph_type": "AMR graphs (concept-level encoding with structural relations)",
            "conversion_method": "Compute structural relation encodings (e.g., label sequences or path features) for each concept pair and incorporate these into Transformer self-attention; no explicit line-graph construction.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported in paper as baseline: BLEU on LDC2015E86 = 29.66, LDC2017T10 = 31.54; Meteor 35.45 / 36.02.",
            "comparison_to_others": "Structural Transformer is the previous state-of-the-art cited; LineGraph+MixGAT (K=4) improves BLEU by +0.92 on both datasets. Authors argue Structural Transformer models arbitrary concept pairs but ignores topological structures of edges (i.e., edge-to-edge relations).",
            "advantages": "Directly models arbitrary concept pairs and indirect relations without deep graph stacking; strong prior performance.",
            "disadvantages": "According to this paper, it still ignores explicit topological relationships among labeled edges (edge structures), which the line-graph decomposition addresses.",
            "failure_cases": "Paper argues it may be less effective than line-graph+MixGAT when edge-to-edge relations and reentrancies are critical; no case-level failures from Zhu et al. are reproduced beyond aggregate comparisons.",
            "uuid": "e8804.3"
        },
        {
            "name_short": "BPE + subword nodes",
            "name_full": "Byte Pair Encoding with subword-node splitting and subword-edge linking",
            "brief_description": "To mitigate data sparsity in AMR node labels and the decoder vocabulary, words in AMR nodes and references are split into subwords via BPE; node splitting is represented by connecting subword nodes with a special 'subword' edge.",
            "citation_title": "Neural machine translation of rare words with subword units",
            "mention_or_use": "use",
            "representation_name": "Subword node splitting with 'subword' edges (BPE)",
            "representation_description": "Apply BPE to segment both AMR graph word nodes and target reference text into subword units; if an AMR word node splits into several subword nodes, connect these subword nodes with a special 'subword' edge in the concept graph, then construct line graph and encode as usual. Decoder and encoder share subword vocabulary.",
            "graph_type": "AMR graphs (word nodes split into subword nodes)",
            "conversion_method": "Tokenize words with BPE; replace a single word-node by a small connected chain of subword nodes linked by a 'subword' edge; include these nodes/edges when building concept graph and line graph.",
            "downstream_task": "AMR-to-text generation (reducing data sparsity in sequence generation)",
            "performance_metrics": "Not reported as an isolated metric; used as preprocessing in all reported experiments (system scores above include BPE).",
            "comparison_to_others": "Paper follows previous work (Zhu et al., 2019) in using BPE and shared vocabulary; the paper reports that this preprocessing helps sequence generation but does not provide isolated ablation numbers.",
            "advantages": "Addresses data sparsity and rare word problems; allows sharing vocabulary between encoder and decoder.",
            "disadvantages": "Increases node/edge counts in the graph (more fine-grained nodes), which may increase encoder graph size; no explicit cost/overhead numbers reported.",
            "failure_cases": "No explicit failure cases reported for BPE/subword splitting in the paper.",
            "uuid": "e8804.4"
        },
        {
            "name_short": "Dual Graph",
            "name_full": "Dual Graph representation (reversed-edge auxiliary graph)",
            "brief_description": "Maintain both the original directed graph and a dual graph with reversed edge directions to permit top-down and bottom-up information propagation; update node embeddings on both graphs and combine them.",
            "citation_title": "Enhancing AMR-to-text generation with dual graph representations",
            "mention_or_use": "use",
            "representation_name": "Dual-graph bidirectional propagation",
            "representation_description": "For a directed input graph, also construct a dual graph with the same nodes but reversed edge directions. Encode both graphs (here with MixGAT using different neighborhood info R_K) and combine the resulting node representations (concatenate or project) so that information flows in both directions and unidirectional losses are reduced.",
            "graph_type": "Directed AMR graphs (concept graph and relation graph treated with dual graphs)",
            "conversion_method": "Create dual neighborhood lists R_K for reversed-edge graph and run MixGAT updates on both original and dual; merge their outputs via projection matrices and FFN per encoding layer.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Dual Graph method cited as prior baseline in Table 1 (reported BLEU and Meteor as a baseline; included in comparisons).",
            "comparison_to_others": "Paper builds on prior idea of dual graphs (Ribeiro et al., 2019) and uses dual updates inside each MixGAT layer; combining dual updates helps retain bidirectional structural information compared to strictly unidirectional GAT updates.",
            "advantages": "Restores information lost by strictly unidirectional message passing; captures both top-down and bottom-up flows.",
            "disadvantages": "Requires maintaining and encoding two neighborhood views; potential extra computation (paper merges outputs but does not report runtime penalties).",
            "failure_cases": "No direct failure cases reported for dual-graph usage; it is used to mitigate the failure mode of unidirectional propagation losing reversed-direction structural info.",
            "uuid": "e8804.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Modeling graph structure in transformer for better AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_in_transformer_for_better_amrtotext_generation"
        },
        {
            "paper_title": "Enhancing AMR-to-text generation with dual graph representations",
            "rating": 2,
            "sanitized_title": "enhancing_amrtotext_generation_with_dual_graph_representations"
        },
        {
            "paper_title": "Structural neural encoders for amr-to-text generation",
            "rating": 2,
            "sanitized_title": "structural_neural_encoders_for_amrtotext_generation"
        },
        {
            "paper_title": "Densely connected graph convolutional networks for graph-to-sequence learning",
            "rating": 1,
            "sanitized_title": "densely_connected_graph_convolutional_networks_for_graphtosequence_learning"
        },
        {
            "paper_title": "Neural machine translation of rare words with subword units",
            "rating": 1,
            "sanitized_title": "neural_machine_translation_of_rare_words_with_subword_units"
        }
    ],
    "cost": 0.017190999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks
Association for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020</p>
<p>Yanbin Zhao 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Lu Chen 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Zhi Chen 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Ruisheng Cao 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Su Zhu 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Kai Yu kai.yu@sjtu.edu.cn 
MoE Key Lab of Artificial Intelligence
Department of Computer Science and Engineering Shanghai
AI Institute
Shanghai Jiao Tong University SpeechLab
Jiao Tong University
ShanghaiChina</p>
<p>Line Graph Enhanced AMR-to-Text Generation with Mix-Order Graph Attention Networks</p>
<p>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics
the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 2020732
Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models. This work focuses on AMR-to-text generation -A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR). Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations: 1) The message propagation process in AMR graphs is only guided by the firstorder adjacency information.2) The relationships between labeled edges are not fully considered. In this work, we propose a novel graph encoding framework which can effectively explore the edge relations. We also adopt graph attention networks with higherorder neighborhood information to encode the rich structure in AMR graphs. Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets. The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.</p>
<p>Introduction</p>
<p>Abstract Meaning Representation (Banarescu et al., 2013) is a sentence-level semantic representation formalized by a rooted directed graph, where nodes are concepts and edges are semantic relations. Since AMR is a highly structured meaning representation, it can promote many semantic related tasks such as machine translation (Song et al., 2019) and summarization (Liao et al., 2018). However, the usage of AMR graphs can be challenging, since it is non-trivial to completely capture the rich structural information in the graph-based data, especially when the graph has labeled edges. * Kai Yu is the corresponding author. Generation from AMR aims to translate the AMR semantics into the surface form (natural language). It is a basic Graph-to-sequence task that directly takes AMR as input. Figure 1 (left) gives a standard AMR graph and its corresponding surface form. Early works utilize sequence-to-sequence framework by linearizing the entire graph (Konstas et al., 2017;Cao and Clark, 2019). Such representation may lose useful structural information. In recent studies, graph neural networks (GNNs) have been in a dominant position on this task and achieved state-of-the-art performance (Beck et al., 2018;Song et al., 2018;Guo et al., 2019;Damonte and Cohen, 2019). However, In these GNN-based models, the representation of each concept node is only updated by the aggregated information from its neighbors, which leads to two limitations: 1) The interaction between indirectly connected nodes heavily relies on the number of stacked layers. When the graph size becomes larger, the dependencies between distant AMR concepts cannot be fully explored. 2) They only focus on modeling the relations between concepts while ignoring edge relations and their structures. Zhu et al. (2019) and Cai and Lam (2019) use Transformer to model arbitrary concept pairs no matter whether directly connected or not, but they still ignore the topological structures of the edges in the entire AMR graph.</p>
<p>To address the above limitations, we propose a novel graph-to-sequence model based on graph attention networks (Velickovic et al., 2018). We transform the edge labels into relation nodes and construct a new graph that directly reflects the edge relations. In graph theory, such a graph is called a Line Graph (Harary and Norman, 1960). As illustrated in Figure 1, we thus separate the original AMR graph into two sub-graphs without labeled edges -concept graph and relation graph. The two graphs describe the dependencies of AMR concepts and edges respectively, which is helpful in modeling these relationships (especially for edges). Our model takes these sub-graphs as inputs, and the communications between the two graphs are based on the attention mechanism. Furthermore, for both graphs, we mix the higher-order neighborhood information into the corresponding graph encoders in order to model the relationships between indirectly connected nodes.</p>
<p>Empirical study on two English benchmark datasets shows that our model reaches state-of-theart performance with 30.58 and 32.46 BLEU scores on LDC2015E86 and LDC2017T10, respectively. In summary, our contributions include:</p>
<p>• We propose a novel graph-to-sequence model, which firstly uses the line graph to model the relationships between AMR edges.</p>
<p>• We integrate higher-order neighborhood information into graph encoders to model the relationships between indirectly connected nodes.</p>
<p>• We demonstrate that both higher-order neighborhood information and edge relations are important to graph-to-sequence modeling.</p>
<p>Mix-Order Graph Attention Networks</p>
<p>In this section, we first introduce graph attention networks (GATs) and their mix-order extensions, which are the basis of our proposed model.</p>
<p>Graph Attention Networks</p>
<p>GAT is a special type of networks that operates on graph-structured data with attention mechanisms. Given a graph G = (V, E), where V and E are the set of nodes x i and the set of edges (e ij , e ) 1 , respectively. N (x i ) denote the nodes which are directly connected by x i . N + (x i ) is the set including x i and all its direct neighbors. we have
N + (x i ) = N (x i ) ∪ {x i }.
Each node x i in the graph has an initial feature h 0 i ∈ R d , where d is the feature dimension. The representation of each node is iteratively updated by the graph attention operation. At the l-th step, each node x i aggregates context information by attending over its neighbors and itself. The updated representation h l i is calculated by the weighted average of the connected nodes:
h l i = σ   x j ∈N + (x i ) α ij h l−1 j W l   ,(1)
where attention coefficient α ij is calculated as:
α ij = softmax j h l−1 i W l t1 h l−1 j W l t2 T (2)
where σ is a nonlinear activation function, e.g. ReLU. W l , W l t1 and W l t2 ∈ R d×d are learnable parameters for projections. After L steps, each node will finally have a context-aware representation h L i . In order to achieve a stable training process, we also employ a residual connection followed by layer normalization between two graph attention layers.</p>
<p>Mixing Higher Order Information</p>
<p>The relations between indirectly connected nodes are ignored in a traditional graph attention layer. Mix-Order GAT, however, can explore these relationships in a single-step operation by mixing the higher-order neighborhood information. We first give some notations before describing the details of the Mix-Order GAT. We use R K = R 1 , ...R K to represent neighborhood information from order 
R k (x i ) are reachable for x i within k hops (k ≥ 1). R 1 (x i ) = N + (x i )
, and as illustrated in Figure 2, we can have:
R k (x i ) = x j ∈R k−1 (x i ) N + (x j ).(3)
The K-Mix GAT integrates the neighborhood information R K . At the l-th update step, each x i will interact with its reachable neighbors with different orders and calculate the attentive features independently. The representation h l i is updated by the concatenated features from different orders, i.e.
h l i = MixGAT l (h l−1 i , R K ) = K k=1 σ   x j ∈R k (x i ) α k ij h l−1 j W l k   ,(4)
where represents concatenation, α k ij are the attention weights in the k-th order, and W l k ∈ R d×d/K are learnable weights for projections. We will use MixGAT(·) to denote the Mix-Order GAT layer in the following section.</p>
<p>Method</p>
<p>The architecture of our method is illustrated in Figure 3. As mentioned above, we separate the AMR graph into two sub-graphs without labeled edges. Our model follows the Encoder-Decoder architecture, where the encoder takes the two sub-graphs as inputs, and the decoder generates corresponding text from the encoded information. We first give some detailed explanations about the line graph and input representation.</p>
<p>Line Graph &amp; Input Representation</p>
<p>The line graph of a graph G is another graph L(G) that represents the adjacencies between edges of G. L(G) is defined as:</p>
<p>• Each node of L(G) represents an edge of G • Two nodes of L(G) are adjacent if and only if their corresponding edges share a common node in G.</p>
<p>For directed graphs, the directions are maintained in the corresponding line graphs. Redundant edges between two relation nodes are removed in the line graphs. Figure 4 provides several examples. In our model, we use the line graph to organize labeled edges and transform the original AMR graph into two sub-graphs. Given an AMR graph
G a = (V a , E a ), we separate it into concept graph G c = (V c , E c ) and relation graph G e = (V e , E e ),
where G e = L(G a ). As for concept graph G c , its topological structure is the same with G a , but the edge labels are eliminated, i.e.
V c = V a ; E c =Ê a ,(5)
WhereÊ a is the edge set without label information.</p>
<p>Both G c and G e have no labeled edges, which can be efficiently encoded by Mix-Order GAT. We use R K c and R K e to denote 1 ∼ K orders neighborhood information of G c and G e . We represent each concept node x i ∈ V c with an initial embedding c 0 i ∈ R d , and each relation node y i ∈ V e e1 e2 e1 e2 e1 e2 e1 e2</p>
<p>original graph line graph original graph line graph Figure 4: Examples of finding line graphs. In the left part, e1 and e2 have opposite directions, so each direction is maintained in the line graph. In the right part, e1 and e2 follow the same direction, so there is only one direction in the corresponding line graph.</p>
<p>with an embedding e 0 i ∈ R d . The sets of node embeddings are denoted as
C 0 = {c 0 i } m i=1 and E 0 = {e 0 i } n i=1 ,
where m = |V c | and n = |V e | denote the numbers of concept nodes and relation nodes, respectively. Thus, the inputs of our system can be formulated by
I = C 0 , E 0 , R K c , R K e .</p>
<p>Self Updating</p>
<p>The encoder of our system consists of N stacked graph encoding layers. As illustrated in Figure  3, each graph encoding layer has two parts: selfupdating for each graph and masked cross attention.
For G c and G e , We use C l−1 = {c l−1 i } m i=1 and E l−1 = {e l−1 i } n i=1
to denote the input node embeddings of the l-th encoding layer. The representations of the two graphs are updated independently by mix-order graph attention networks (MixGAT). At the l-th step (layer), we have:
C l self = MixGAT l c1 (C l−1 , R K c ), E l self = MixGAT l e1 (E l−1 , R K e ).(6)
Where C l self and E l self are updated representations according to the mix-order neighborhood information R K c and R K e . One thing should be noticed is that both G c and G e are directed graphs. This implies that the information propagation in the graph is in a top-down manner, following the pre-specified direction. However, unidirectional propagation loses the structural information in the reversed direction. To build communication in both directions, we employ Dual Graph (Ribeiro et al., 2019). Dual graph has the same node representations but reversed edge directions compared to the original graph. For example, if edge A→B is in the original graph, it turns to B→A in the corresponding dual graph. Since dual graphs have the same node representations, we only need to change the neighborhood information. Denote G c and G e as the dual graph of G c and G e . R K c and R K e are the corresponding neighborhood information. We have:
C l self = MixGAT l c2 (C l−1 , R K c ), E l self = MixGAT l e2 (E l−1 , R K e ).(7)
Since we have updated the node embeddings in two directions, the final representations of the independent graph updating process are the combination of the bi-directional embeddings, i.e.
C l self = C l self ; C l self W l c1 , E l self = E l self ; E l self W l e1 ,(8)
where W l c1 and W 1 e1 ∈ R 2d×d are trainable matrix for projections. C l self ∈ R m×d and E l self ∈ R n×d are results of the self-updating process.</p>
<p>Masked Cross Attention</p>
<p>Self updating for G c and G e can model the relationships of AMR concepts and edge respectively. However, it is also necessary to explore the dependencies between concept nodes and relation nodes. As a result, the cross-graph communication between G c and G e is very important. From the structure of the original AMR graph, we can easily build alignment between G c and G e . A relation node y i is directly aligned to a concept node x i if x i is the start-point/end-point of the edge corresponding to y i . As illustrated in Figure 1, ARG0 is the edge between run-02 and he. As a result, node ARG0 in G e is directly connect to run-02 and he in G c .</p>
<p>We apply the attention mechanism to complete the interaction between the two graphs, and use M ∈ R n×m to mask the attention weights of unaligned pairs between G c and G e . For element m ij in M, we let m ij = 0 if y i ∈ V e is aligned to x j ∈ V c , otherwise m ij = −∞. The masked cross attention is employed between the representation sets E l self and C l self , and the matrix of attention weights A l can be calculated as:
A l = E l self W l a1 C l self W l a2 T + M,(9)
where W l a1 and W l a2 ∈ R d×d are learnable projection matrixes. The weight scores of unaligned pairs are set to −∞ according to M. For nodes in E l self , the relevant representation from C l self is identified using A l as:
E l cross = softmax (A l ) C l self ,(10)
where E l cross ∈ R n×d is the masked weighted summation of C l self . The same calculation is performed for nodes in C l self as:
C l cross = softmax(A T l )E l self .(11)
The final outputs of a graph encoding layer are the combination of the original embeddings and the context representations from another graph. We also employ the outputs from previous layer as residual inputs, i.e.
C l = FFN C l self ; C l cross W l c2 + C l−1 , E l = FFN E l self ; E l cross W l e2 + E l−1 ,(12)
where FFN is a feed-forward network consists of two linear transformations. After N -stacked graph encoding layers, The two graphs G c and G e are finally encoded as C N and E N .</p>
<p>Decoder</p>
<p>The decoder of our system is similar to the Transformer decoder. At each generation step, the representation of the output token is updated by multiple rounds of attention with the previously-generated tokens and the encoder outputs. Note that the outputs of our graph encoder have two parts: concept representations C N and the relation representations E N . For generation, concept information is more important, since the concept graph directly contains the natural words. With the multi-step cross attention, C N also caries abundant relation information. For simplicity, we only use C N as the encoder output on the decoder side 2 .</p>
<p>To address the data sparsity issue in sequence generation, we employ the Byte Pair Encoding (BPE) (Sennrich et al., 2016) following the settings of Zhu et al. (2019). We split the word nodes in AMR graphs and reference sentences into subwords, and the decoder vocabulary is shared with the encoder for concept graphs.</p>
<p>Experiments</p>
<p>Settings</p>
<p>Data and preprocessing We conduct our experiments with two benchmark datasets: LDC2015E85 and LDC2017T10. The two datasets contain 16833 and 36521 training samples, and they use a common development set with 1368 samples and a common test set with 1371 samples. We segment natural words in both AMR graphs and references into sub-words. As a result, a word node in AMR graphs may be divided into several sub-word nodes. We use a special edge subword to link the corresponding sub-word nodes. Then, for each AMR graph, we find its corresponding line graph and generate G c and G e respectively. Training details For model parameters, the number of graph encoding layers is fixed to 6, and the representation dimension d is set to 512. We set the graph neighborhood order K = 1, 2 and 4 for both G c and G e . The Transformer decoder is based on Open-NMT (Klein et al., 2018), with 6 layers, 512 dimensions and 8 heads. We use Adam (Kingma and Ba, 2015) as our optimizer and β = (0.9, 0.98). The learning rate is varied over the course of training, similar with Vaswani et al. (2017):
lr = γd −0.5 · min(t −0.5 , t * w −1.5 ),(13)
where t denotes the accumulative training steps, and w indicates the warmup steps. We use w = 16000 and the coefficient γ is set to 0.75. As for batch size, we use 80 for LDC2015E86 and 120 for LDC2017T10. 3</p>
<p>Results</p>
<p>We compare our system with several baselines, including traditional sequence-to-sequence models, several graph-to-sequence models with multiple graph encoders, and transformer-based models. All models are trained on the single dataset without ensemble or additional unlabeled data. For performance evaluation, we use BLEU (Papineni et al., 2002) as our major metric. We also use Meteor (Banerjee and Lavie, 2005), which considers the synonyms between predicted sentences and references.</p>
<p>The experimental results on the test sets of LDC2015E86 and LDC2017T10 are reported in Table 1. As we can see, Sequence-based models perform the worst, since they lose useful structural information in graphs. Graph-based models get better results with varied graph encoders to capture the structural information in graphs. Transformer-based models reach previous state-ofthe-art with structure-aware self-attention approach to better modeling the relations between indirectly connected concepts. Comparing to previous studies, our approach with K = 4 order neighborhood information reaches the best BLEU scores, improving over the state-of-the-art model (Zhu et al., 2019) by 0.92 on both datasets. Similar phenomena can be found on the additional metrics of Meteor.</p>
<p>Analysis</p>
<p>As mentioned above, our system has two critical points: higher-order graph neighborhood information and relationships between AMR edges. To verify the effectiveness of these two settings, we conduct a series of ablation tests based on different characteristics of graphs.  </p>
<p>Ablation Study on Neighborhood information</p>
<p>Higher order neighborhood information includes the relationships between indirectly connected nodes. Table 2 shows the connectivity of the con-cept graphs under different orders. When K = 1, each node can reach 24.91% of the other nodes directly in the graph (LDC2015E86), and it grows to 41.67% when K = 4. As suggested in Table 1, if graph nodes only interact with their direct neighbors (K = 1), it performs worse than previous Transformer-based models. However, significant improvement can be observed when we integrate higher-order neighborhood information. As K grows form 1 to 4, the BLEU score increases 1.94 and 2.50 on LDC2015E86 and LDC2017T10, respectively.  Figure 5: BLEU variation between models with different orders K with respect to AMR graph size.</p>
<p>As mentioned above, if only consider the firstorder neighborhood, the dependencies between distant AMR concepts cannot be fully explored when the graph size becomes larger. To verify this hypothesis, we split the test set into different parts according to the AMR graph size (i.e. number of concepts). We evaluate our models with order  Figure 6: BLEU variation between models with different K e with respect to size of AMR graph and (left) and reentrancy numbers (right). K = 4 and K = 1 on different partitions. All models are trained on LDC2015E86 set. Figure 5 shows the result. The model with K = 4 significantly outperforms the one with K = 1. Furthermore, we can find that the performance gap between the two models increases when the graph gets bigger. As a result, higher-order neighborhood information does play an important role in graph-to-sequence generation, especially for larger AMR graphs.</p>
<p>Ablation Study on Relationships of Labeled Edges</p>
<p>We are the first one to consider the relationships between labeled edges in AMR graph by integrating the line graph (relation graph) G e in our system. This section will deeply analyze the effectiveness of this contribution. In previous settings, the graph neighborhood order K is the same for both G c and G e . To conduct the ablation test, we fix the neighborhood order K c for G c and vary the order K e for relation graph G e . We set K e = 0, 1 and 4, where K e = 0 indicates that the relation nodes in G e can only interact with itself. This means the dependencies between AMR edges are completely ignored, and the edge information is simply combined with the corresponding concepts. We report the results on both test sets in Table 3.  Table 3: Results of models with varied neighborhood orders of relation graph G e . BLEU scores significantly different from the best model is marked with * (p &lt; 0.01), tested by bootstrap resampling (Koehn, 2004).</p>
<p>If we ignore the dependencies between AMR edges (K e = 0), there is a significant performance degradation: 1.69 and 1.38 BLEU score decline on LDC2015E86 and LDC2017T10 respectively. The performance gets better when K e &gt; 0, which means the edge relations do bring benefits to the graph encoding and sequence generation. When K e = 4, the edge relations are fully explored in varied neighborhood orders, and it reaches the best performance on both datasets. Performance test on different partitions of AMR graph size (Figure 6, left) also suggests that relationships of edges are helpful when the graph becomes larger. We also study the effectiveness of edge relations when handling reentrancies. Reentrancies are the nodes with multiple parents. Such structures are identified as very difficult aspects in AMR graph (Damonte and Cohen, 2019). We think the relation graph G e is helpful in exploring different dependencies with the same concept, which can bring benefits to those graphs containing more reentrancies. To test this hypothesis, we also split the test set into different parts according to their numbers of reentrancies and evaluate our models with K e = 4 and K e = 0 on different partitions. As shown in Figure 6 (right), the gap becomes wide when the number of reentrancies grows to 5. Also, compare to the graph size, edge relations are more important in handling graphs with reentrancies.</p>
<p>Case Study</p>
<p>To gain insight into the model performance. Table  4 provides a few examples. The reentrancies in the AMR graphs is marked with bold type.</p>
<p>In Example (a), two different nodes have same conceptcompete, but they have different forms in the corresponding natural language. According to the references, one is for "competitors" and the other is for "competition". Our model with K e = 0 fails to distinguish the difference and generate two (f / feel-02 :ARG0 (h / he) :ARG1 (p / person :quant (m / more) :ARG0-of (c / compete-01) :ARG1-of (n / new-01) :source (c2 / country :poss (w / we))) :ARG0-of (p2 / participate-01 :ARG1 (c3 / compete-01 :mod (t / this)))))</p>
<p>Reference: he felt that , there were more new competitors from our country participating in this competition .</p>
<p>K e = 0: he feels more competition from our country who participate in this competition . K e = 4: he feels that more new competitors from our country who participate in this competition .  "competition" in the output. However, model with K e = 4 successfully recover word "competitors" from the context of the AMR graph.</p>
<p>In Example (b), the concept they has two parents with the same conceptwant. Though our model with K e = 0 successfully finds they is the subject of the both two want, it fails to recognize the parallel relationship between the objects money and face and regard face as a verb. In the contrast, our model with K e = 4 perfectly finds the parallel structure in the AMR graph and reconstructs the correct sentence.</p>
<p>In Example (c), we compare our best model with two baselines: GCNSEQ (Damonte and Cohen, 2019) and Structural Transformer (Denote as ST-Transformer) from Zhu et al. (2019). The AMR graph in Example (b) has two reentrancies, which makes it more difficult to recover the corresponding sentence. As we can see, traditional graph-based model GCNSEQ cannot predict the correct subject of the predicate can. Structural-Transformer uses the correct subject, but the recovered sentence is quite disfluent because of the redundant people. This overgeneration problem is mainly caused by reentrancies (Beck et al., 2018). However, our model can effectively handle this problem and generates a proper sentence with correct semantics.</p>
<p>Related Work</p>
<p>AMR-to-text generation is a typical graph-tosequence task. Early research employs rule-based methods to deal with this problem. Flanigan et al. (2016) use two-stage method by first split the graphs into spanning trees and use multiple tree transducers to generate natural language. Song et al. (2017) use heuristic extraction algorithm to learn graph-to-string rules. More works frame graph-to-sequence as a translation task and use either phrase-based (Ferreira et al., 2017;Pourdamghani et al., 2016) or neural-based (Konstas et al., 2017) models. These methods usually need to linearize the input graphs by means of a depthfirst traversal. Cao and Clark (2019) get a better sequence-based model by leveraging additional syntactic information.</p>
<p>Moving to graph-to-sequence approaches, Marcheggiani and Perez-Beltrachini (2018) first show that graph neural networks can significantly improve the generation performance by explicitly encoding the structure of the graph. Since than, models with variant graph encoders have been proposed in recent years, such as graph LSTM (Song et al., 2018), gated graph neural networks (GGNN) (Beck et al., 2018) and graph convolutional neural networks (Damonte and Cohen, 2019). Guo et al. (2019) introduce dense connectivity to allow the information exchange across different of layers. Ribeiro et al. (2019) learn dual representations capturing top-down and bottom-up adjuvant view of the graph, and reach the best performance in graph-based models.</p>
<p>Despite the great success of graph neural net-works, they all restrict the update of node representation based on only first-order neighborhood and rely on stacked layers to model the relationships between indirectly connected nodes. To solve this problem, recent studies extend the Transformer (Vaswani et al., 2017) to encode the graph structure. Zhu et al. (2019) and Cai and Lam (2019) use relation-aware self-attention to encode structural label sequences of concept pairs, which can model arbitrary concept pairs no matter whether directly connected or not. With several mechanisms such as sub-word (Sennrich et al., 2016) and shared vocabulary, Zhu et al. (2019) achieved state-of-theart performance on this task. Our model follows the same spirit of exploring the relations between indirectly connected nodes, but our method is substantially different: (1) we use a graph-based method integrated with higherorder neighborhood information while keeping the explicit structure of graphs.</p>
<p>(2) we first consider the relations between labeled edges by introducing line graphs.</p>
<p>Conclusion and Future Work</p>
<p>In this work, we presented a novel graph-tosequence approach which uses line graph to model the relationships between labeled edges from the original AMR graph. The mix-order graph attention networks are found effective when handling indirectly connected nodes. The ablation studies also demonstrate that exploring edge relations brings benefits to graph-to-sequence modeling. Furthermore, our framework can be efficiently applied to other graph-to-sequence tasks such as WebNLG (Gardent et al., 2017) and syntax-based neural machine translation (Bastings et al., 2017). In future work we would like to do several experiments on other related tasks to test the versatility of our framework. Also, we plan to use large-scale unlabeled data to improve the performance further.</p>
<p>Figure 1 :
1An AMR graph (left) for sentence "He runs as fast as the wind." and its concept graph and relation graph (line graph). Two graphs are aligned with each other based on the node-edge relations in the original graph.</p>
<p>Figure 2 :
2Neighborhood information in different orders.</p>
<p>1 eFigure 3 :
13is the edge label which are not considered in the GAT layer An overview of our proposed model 1 to order K. R k (x i ) denotes the k-th order neighborhood, which means all nodes in</p>
<p>(t / they) :ARG1 (m / money)) :ARG2 (w2 / want-01 :polarity -:ARG0 (t / they) :ARG1 (f / face))) Reference: they want money , not the face K e = 0: they want money but do n't want to face . K e = 4: they want the money , not the face . (b) (p / possible-01 :ARG1 (h / help-01 :ARG0 (p2 / person) :ARG1 (y / you)) :condition (t / tell-01 :ARG0 (y / you) :ARG2 (p2 / person))) Reference: if you tell people they can help you, GCNSEQ: if you tell them , you can help you ! ST-Transformer: if you tell people , people can help you ! Ours (K e = 4): people can help you if you tell them ... (c)</p>
<p>Table 1: Main results of our approaches and several baselines on the test sets of LDC2015E86 and LDC2017T10Models 
LDC2015E86 
LDC2017T10 
BLEU Meteor BLEU Meteor </p>
<p>Sequence-Based Model 
Seq2Seq (Konstas et al., 2017) 
22.00 
-
-
-
Syntax+S2S (Cao and Clark, 2019) 
23.50 
-
26.80 
-</p>
<p>Graph-Based Model 
Graph LSTM (Song et al., 2018) 
23.30 
-
-
-
GCNSEQ (Damonte and Cohen, 2019) 
24.40 
23.60 
24.54 
24.07 
Dual Graph (Ribeiro et al., 2019) 
24.32 
30.53 
27.87 
33.21 
DCGCN (Guo et al., 2019) 
25.70 
31.50 
27.60 
34.00 </p>
<p>Transformer-Based Model 
Transformer (Zhu et al., 2019) 
25.50 
33.16 
27.43 
34.62 
Graph Transformer (Cai and Lam, 2019) 
27.40 
32.90 
29.80 
35.10 
Structural Transformer (SA) (Zhu et al., 2019) 
29.66 
35.45 
31.54 
36.02 </p>
<p>Our Approach 
Line Graph + MixGAT, K = 1 
28.64 
34.51 
29.96 
35.15 
Line Graph + MixGAT, K = 2 
29.62 
35.38 
31.06 
36.13 
Line Graph + MixGAT, K = 4 
30.58 
35.81 
32.46 
36.78 </p>
<p>Table 2 :
2The connectivity of the concept graphs under different orders.</p>
<p>Table 4 :
4Examples comparison between (a) Our approach with different K e . (b) Our approach and several baselines.
We also implement a version which considers both C N and E N , and achieve similar results
Our code is available at https://github.com/ ybz79/AMR2text
AcknowledgmentsWe thank the anonymous reviewers for their thoughtful comments.This work has been supported by the National Key Research and Development Program of China (Grant No.  2017YFB1002102)  and Shanghai Jiao Tong University Scientific and Technological Innovation Funds (YG2020YQ01).
Abstract meaning representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, LAW@ACL. The Association for Computer LinguisticsLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In LAW@ACL, pages 178-186. The Association for Computer Linguistics.</p>
<p>METEOR: an automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Association for Computational Linguistics. IEE-valuation@ACLSatanjeev Banerjee and Alon Lavie. 2005. METEOR: an automatic metric for MT evaluation with im- proved correlation with human judgments. In IEE- valuation@ACL, pages 65-72. Association for Com- putational Linguistics.</p>
<p>Graph convolutional encoders for syntax-aware neural machine translation. Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, Khalil Sima, EMNLP. Association for Computational LinguisticsJoost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. 2017. Graph convolutional encoders for syntax-aware neural ma- chine translation. In EMNLP, pages 1957-1967. As- sociation for Computational Linguistics.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, ACL (1). Association for Computational LinguisticsDaniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In ACL (1), pages 273-283. Association for Computational Linguistics.</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, arXiv:1911.07470arXiv preprintDeng Cai and Wai Lam. 2019. Graph transformer for graph-to-sequence learning. arXiv preprint arXiv:1911.07470.</p>
<p>Factorising AMR generation through syntax. Kris Cao, Stephen Clark, NAACL-HLT (1). Association for Computational LinguisticsKris Cao and Stephen Clark. 2019. Factorising AMR generation through syntax. In NAACL-HLT (1), pages 2157-2163. Association for Computational Linguistics.</p>
<p>Structural neural encoders for amr-to-text generation. Marco Damonte, Shay B Cohen, abs/1903.11410CoRRMarco Damonte and Shay B. Cohen. 2019. Structural neural encoders for amr-to-text generation. CoRR, abs/1903.11410.</p>
<p>Linguistic realisation as machine translation: Comparing different MT models for amr-to-text generation. Iacer Thiago Castro Ferreira, Sander Calixto, Emiel Wubben, Krahmer, INLG. Association for Computational LinguisticsThiago Castro Ferreira, Iacer Calixto, Sander Wubben, and Emiel Krahmer. 2017. Linguistic realisation as machine translation: Comparing different MT mod- els for amr-to-text generation. In INLG, pages 1-10. Association for Computational Linguistics.</p>
<p>Generation from abstract meaning representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime G Carbonell, HLT-NAACL. The Association for Computational LinguisticsJeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime G. Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In HLT-NAACL, pages 731-739. The Association for Computational Linguistics.</p>
<p>The webnlg challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, Association for Computational Linguistics. INLGClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The webnlg challenge: Generating text from RDF data. In INLG, pages 124-133. Association for Computational Lin- guistics.</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, TACL. 7Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional net- works for graph-to-sequence learning. TACL, 7:297- 312.</p>
<p>Some properties of line digraphs. Frank Harary, Robert Z Norman, Rendiconti del Circolo Matematico di Palermo. 92Frank Harary and Robert Z Norman. 1960. Some properties of line digraphs. Rendiconti del Circolo Matematico di Palermo, 9(2):161-168.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.</p>
<p>Opennmt: Neural machine translation toolkit. Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent Nguyen, Jean Senellart, Alexander M Rush, AMTA (1). Association for Machine Translation in the AmericasGuillaume Klein, Yoon Kim, Yuntian Deng, Vincent Nguyen, Jean Senellart, and Alexander M. Rush. 2018. Opennmt: Neural machine translation toolkit. In AMTA (1), pages 177-184. Association for Ma- chine Translation in the Americas.</p>
<p>Statistical significance tests for machine translation evaluation. Philipp Koehn, EMNLP. Association for Computational LinguisticsPhilipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP, pages 388-395. Association for Computational Linguis- tics.</p>
<p>Neural AMR: sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, ACL (1). Association for Computational LinguisticsIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: sequence-to-sequence models for parsing and gener- ation. In ACL (1), pages 146-157. Association for Computational Linguistics.</p>
<p>Abstract meaning representation for multi-document summarization. Kexin Liao, Logan Lebanoff, Fei Liu, Association for Computational Linguistics. COLINGKexin Liao, Logan Lebanoff, and Fei Liu. 2018. Ab- stract meaning representation for multi-document summarization. In COLING, pages 1178-1190. As- sociation for Computational Linguistics.</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, INLG. Association for Computational LinguisticsDiego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for struc- tured data to text generation. In INLG, pages 1-9. Association for Computational Linguistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, ACL. ACLKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In ACL, pages 311- 318. ACL.</p>
<p>Generating english from abstract meaning representations. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, The Association for Computer Linguistics. INLGNima Pourdamghani, Kevin Knight, and Ulf Herm- jakob. 2016. Generating english from abstract mean- ing representations. In INLG, pages 21-25. The As- sociation for Computer Linguistics.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, EMNLP-IJCNLP. Association for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In EMNLP- IJCNLP. Association for Computational Linguistics.</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, ACL. The Association for Computer Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL. The Association for Com- puter Linguistics.</p>
<p>Semantic neural machine translation using AMR. Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, Jinsong Su, TACL. 7Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic neural ma- chine translation using AMR. TACL, 7:19-31.</p>
<p>Amr-to-text generation with synchronous node replacement grammar. Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, Daniel Gildea, ACL (2). Association for Computational LinguisticsLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. Amr-to-text gener- ation with synchronous node replacement grammar. In ACL (2), pages 7-13. Association for Computa- tional Linguistics.</p>
<p>A graph-to-sequence model for amrto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, ACL (1). Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for amr- to-text generation. In ACL (1), pages 1616-1626. Association for Computational Linguistics.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998-6008.</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, ICLR. Open-Review. netPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In ICLR. Open- Review.net.</p>
<p>Modeling graph structure in transformer for better AMR-to-text generation. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, Guodong Zhou, EMNLP-IJCNLP. Association for Computational Linguistics. Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text gen- eration. In EMNLP-IJCNLP. Association for Com- putational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
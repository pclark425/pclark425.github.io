<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4741 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4741</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4741</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-1b6e810ce0afd0dd093f789d2b2742d047e316d5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5" target="_blank">Chain of Thought Prompting Elicits Reasoning in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.</p>
                <p><strong>Paper Abstract:</strong> We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4741.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4741.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that supplies a few in-context exemplars of 〈input, natural-language intermediate reasoning steps, output〉 so that the model generates step-by-step natural-language rationales before the final answer, eliciting multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (PaLM, GPT-3, LaMDA, Codex, UL2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to off-the-shelf autoregressive transformer LLMs of different sizes (PaLM: 8B/62B/540B; GPT-3 variants up to ~175B; LaMDA up to 137B; Codex code-davinci-002; UL2 20B). Models were used with greedy decoding for reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS) — multi-step arithmetic, multi-digit, algebraic word problems and subsets (SingleOp, SingleEq, AddSub, MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Providing intermediate natural-language steps (CoT) helps the model decompose semantic problem statements into sub-steps and equations, allocating token-level computation to intermediate reasoning and improving translation from language to arithmetic operations; this capability reliably emerges only at large model scale (~100B+ parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical accuracy gains on multi-step arithmetic and symbolic tasks for large models (PaLM 540B, GPT-3 175B, Codex) compared to standard prompting; ablations show that (a) equation-only prompting and (b) variable-compute-only (dots) do not match CoT, and (c) placing CoT after the answer does not match CoT-before-answer; manual inspection found generated CoTs that are logically correct when final answers are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT often fails or harms performance for small models (<~10B): they produce fluent but illogical CoTs; CoT sometimes yields correct answers via coincidentally wrong chains; CoT gives little or negative gains on simple one-step problems; gains depend on model scale and exemplar quality/style.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Key examples (accuracy %): PaLM 540B: standard prompting GSM8K 17.9 → CoT 56.9 (+ext calc 58.6); SVAMP 69.4 → 79.0 (+ext 79.8); MAWPS 79.2 → 93.3. GPT-3 175B (text-davinci-002): GSM8K 15.6 → 46.9 (+ext 49.6); SVAMP 65.7 → 68.9; MAWPS 72.7 → 87.1. Codex (code-davinci-002): GSM8K 19.7 → 63.1 (+ext 65.4). LaMDA 137B: GSM8K 6.5 → 14.3 (+ext 17.8). UL2 20B: GSM8K 4.1 → 4.4 (+ext 6.9). Improvements are largest on harder multi-step datasets (e.g., GSM8K) and smaller or negative on trivial single-step subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation experiments: (1) Equation-only prompting (prompt to output only a math equation) provided little benefit on semantically complex datasets (GSM8K) though it helped some short-step datasets; (2) variable-compute-only (dots) matched baseline, indicating extra token budget alone isn't the cause; (3) CoT-after-answer matched baseline, indicating the sequential reasoning path matters; (4) applying a post-hoc external calculator to evaluate arithmetic in generated CoTs improves accuracy (see ext. calc. numbers). Manual error analysis categorized errors (semantic misunderstanding, one-step-missing, other) and showed scaling fixes many errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Failure modes include arithmetic computation mistakes (simple calculation errors), semantic misunderstanding of problem text, missing intermediate steps, symbol-mapping errors, hallucinations, repetitive/unterminating outputs, and variance due to exemplar wording/order; CoT emergence requires large models (~100B+), making deployment costly; CoTs are not guaranteed faithful or correct and can be persuasive while wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>CoT is an emergent capability: small models (<~10B) do not benefit or are harmed; medium (~10–100B) show mixed gains; large (~100B+) show large gains. PaLM 540B and Codex show strongest CoT gains; GPT-3 175B shows substantial gains; LaMDA 137B and UL2 20B show modest or small gains. External calculator post-processing benefits multiple models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4741.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-540B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (540 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large autoregressive transformer language model evaluated in this paper; exhibited the strongest chain-of-thought gains and achieved new state-of-the-art results on several math benchmarks with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM 540B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive transformer language model (PaLM family) with 540B parameters; evaluated with greedy decoding; other PaLM sizes tested: 8B and 62B.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS subsets including multi-step MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>At large scale, PaLM can internalize semantic-to-operation mappings and follow natural-language decomposition (elicited by CoT) to produce stepwise solutions — chain-of-thought acts as an explicit scaffold that the large model can follow to perform arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large empirical accuracy jumps with CoT: GSM8K 17.9% → 56.9% (CoT); SVAMP 69.4% → 79.0%; MAWPS 79.2% → 93.3%; manual error analyses comparing 62B→540B show scale fixes many semantic and one-step-missing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT still produces arithmetic computation errors (handled partially by external calculator); CoT can be sensitive to exemplar selection and style; on some datasets (e.g., trivial single-step problems) gains are minimal.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>PaLM (540B) accuracies (standard → CoT → +ext calc where available): GSM8K 17.9 → 56.9 → 58.6; SVAMP 69.4 → 79.0 → 79.8; ASDiv 72.1 → 73.9; AQuA 25.2 → 35.8; MAWPS 79.2 → 93.3 → 93.5.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Scaling ablation (62B→540B) fixed many categories of error; ablation showing equation-only prompting underperforms CoT on semantically complex sets; external calculator post-hoc evaluation slightly improves CoT scores by correcting numeric mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Requires large compute and capacity; CoTs may still include arithmetic errors and semantic mistakes; exemplar sensitivity remains; not all progress explained (open question whether true algorithmic reasoning occurs).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Outperformed other evaluated models under CoT in several benchmarks (e.g., new SOTA on GSM8K vs prior finetuned methods); Codex and GPT-3 also improve with CoT but PaLM 540B generally achieves the largest absolute accuracy on hardest benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4741.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3-175B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 (text-davinci-002, ~175B parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive language model (Instruct-style variant text-davinci-002) evaluated with and without chain-of-thought prompting; shows large improvements on multi-step arithmetic tasks when given CoT exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 175B (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approx. 175B-parameter Instruct-style GPT-3 variant (text-davinci-002) used via API, with greedy decoding in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>CoT provides intermediate reasoning tokens that guide the pretrained language model's latent patterns toward multi-step arithmetic decomposition; model leverages pretraining knowledge and token-level autoregression to implement stepwise calculation and mapping from problem text to arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Substantial gains on GSM8K: standard 15.6% → CoT 46.9% (→ +ext calc 49.6%); improvements also on AQuA and MAWPS. Ablations show equation-only and variable-compute-only not matching CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Smaller gains on some datasets (SVAMP, ASDiv) relative to other models; still susceptible to arithmetic mistakes that external calculator can partially correct; exemplar sensitivity and occasional incorrect but fluent CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracies (standard → CoT → +ext calc): GSM8K 15.6 → 46.9 → 49.6; SVAMP 65.7 → 68.9 → 70.3; ASDiv 70.3 → 71.3; AQuA 24.8 → 35.8; MAWPS 72.7 → 87.1 → 87.5.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Post-hoc external evaluation of arithmetic expressions improves accuracy modestly; ablation studies (equation-only, dot-sequence, CoT-after-answer) point to the sequential decomposition as a causal factor; limited manual error analysis indicates many CoTs are logically correct when final answer is correct.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Produces arithmetic miscalculations, semantic errors, and faulty chains; gains depend on prompt exemplars and model scale; CoT not a panacea for small/medium models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Substantial improvements relative to standard prompting; Codex and PaLM 540B often achieve higher absolute CoT accuracies on hardest datasets; GPT-3 175B shows clear benefit from CoT but is not the top performer in all benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4741.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized transformer model variant evaluated in arithmetic tasks; shows very strong gains when using chain-of-thought prompting, possibly due to code pretraining's stronger symbolic/numeric capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A code-trained large language model (OpenAI Codex variant) used in the same prompting experiments; exact parameter count unspecified in paper but a code-specialized model.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Code pretraining imparts stronger ability to emit structured symbolic/sketch-like computations; when guided by CoT, Codex robustly produces equations and intermediate symbolic steps that can be executed/checked.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large measured gains: GSM8K standard 19.7% → CoT 63.1% (+ext 65.4); SVAMP 69.9% → 76.4%; ASDiv 74.0% → 80.4%; AQuA 29.5% → 45.3%; MAWPS 78.7% → 92.6% (+ext 93.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>When CoT contains incorrect symbolic structure (not just arithmetic mistakes), external calculators cannot fix it; exemplar and prompt sensitivity remain.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Accuracies (standard → CoT → +ext calc): GSM8K 19.7 → 63.1 → 65.4; SVAMP 69.9 → 76.4 → 77.0; ASDiv 74.0 → 80.4 → 80.0; AQuA 29.5 → 45.3; MAWPS 78.7 → 92.6 → 93.3.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>External-calculator post-processing yields further gains, indicating many CoT failures are numeric-computation errors; ablations show equation-only helps some simple datasets but CoT remains superior for semantically complex problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Relies on correct structural translation of semantics into equations; still produces arithmetic and semantic errors; requires heuristics to extract and execute equations for post-hoc correction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Codex with CoT often matches or surpasses GPT-3 175B and approaches or exceeds PaLM 540B on several benchmarks, suggesting code pretraining confers advantages for symbolic/numeric reasoning under CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4741.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaMDA-137B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaMDA (137 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large dialogue-oriented transformer model family tested at multiple scales (up to 137B); shows moderate improvements with CoT but lower absolute performance on some arithmetic benchmarks compared to PaLM and Codex.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA 137B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LaMDA family transformer model evaluated at various scales (422M, 2B, 8B, 68B, 137B) with greedy decoding; reported results averaged over exemplar order seeds for some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>CoT elicits stepwise reasoning in LaMDA but the model's pretraining and scale determine how reliably those steps map to correct arithmetic; larger sizes show emergent ability but at lower absolute levels than top-performing models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>CoT improved LaMDA 137B on GSM8K 6.5% → 14.3% (+ext 17.8%), SVAMP 29.5% → 37.5% (+ext 42.1%), ASDiv 40.1% → 46.6% (+ext 53.4%), MAWPS 43.2% → 57.9% (+ext 69.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>On AQuA CoT decreased performance in the reported setting (25.5% → 20.6%); smaller LaMDA sizes produced fluent but illogical CoTs; exemplar ordering had less variance for LaMDA but overall accuracy remains lower than top models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LaMDA 137B accuracies (standard → CoT → +ext calc where given): GSM8K 6.5 → 14.3 → 17.8; SVAMP 29.5 → 37.5 → 42.1; ASDiv 40.1 → 46.6 → 53.4; AQuA 25.5 → 20.6; MAWPS 43.2 → 57.9 → 69.3.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Averaging over exemplar permutations reported; error analyses on LaMDA-generated CoTs: of 50 correct-answer examples nearly all chains were logically correct; among 50 incorrect-answer examples many had major semantic/coherence errors (54%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Lower base arithmetic ability compared to PaLM/Codex; CoT can reduce performance on some tasks; significant fraction of incorrect CoTs have major semantic errors; arithmetic/calculation errors persist.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>LaMDA 137B benefits from CoT but achieves lower absolute scores than PaLM 540B and Codex; shows the same qualitative emergent-scale trend but at lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4741.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UL2-20B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UL2 (20 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 20B-parameter model evaluated as part of the model suite; exhibits very small gains or low absolute arithmetic performance with CoT prompting, illustrating that CoT improvements are scale-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UL2 20B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>UL2 family transformer tuned with mixture-of-denoising objectives; a 20B-parameter instance was evaluated with greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>At this smaller scale CoT fails to reliably produce logically correct decomposition — models can output fluent text but lack the internal capacities to map those steps to correct arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirically small improvements: GSM8K 4.1% → 4.4% (→ +ext calc 6.9%); SVAMP 10.1% → 12.5% (→ +ext calc 28.3%); some ext. calc. gains indicate numeric computation failures that can be partially corrected when extractable.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>CoT sometimes harmed or negligibly affected performance; many generated CoTs are fluent but incorrect; overall demonstrates scale threshold for CoT effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>UL2 20B accuracies (standard → CoT → +ext calc): GSM8K 4.1 → 4.4 → 6.9; SVAMP 10.1 → 12.5 → 28.3; ASDiv 16.0 → 16.9 → 34.3; AQuA 20.5 → 23.6; MAWPS 16.6 → 19.1 → 42.7.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Post-hoc external calculation produced large relative gains on some datasets (e.g., ASDiv, MAWPS), implying many failures are numeric rather than structural when the model outputs extractable equations; overall ablations show CoT ineffective at this scale.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Small-scale model failures: fluent but illogical CoTs, inability to generalize symbol manipulations, low arithmetic competence; CoT can sometimes reduce performance relative to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Performs substantially worse than large models (PaLM 540B, Codex, GPT-3 175B) under CoT; highlights the emergence-of-scale phenomenon.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4741.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Equation-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Equation-only Prompting (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that prompts the model to output only the mathematical equation corresponding to the problem (no natural-language intermediate steps) before providing the final answer, used to test whether CoT gains are merely from producing an equation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (evaluated primarily on PaLM and LaMDA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same model family instances as main experiments; equation-only is a prompting variant rather than a separate model.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same math word-problem benchmarks; especially informative on short-step vs. semantically complex multi-step problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Producing an explicit equation might be sufficient to improve arithmetic performance by converting semantics to executable math directly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Equation-only prompting improves performance on simple one-step or two-step datasets (e.g., some MAWPS subsets) where the equation directly follows from semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Equation-only prompting fails on semantically complex datasets (GSM8K); qualitative example shows equation-only generated an incorrect equation while CoT produced the correct decomposition and answer, concluding equation-only is insufficient for complex semantic understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as part of ablation: for GSM8K equation-only did not help much; for easy one- or two-step datasets equation-only gave measurable improvements (details in Appendix; overall CoT outperforms equation-only on complex data).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Ablation contrast demonstrates CoT's benefit is not solely from exposing an equation; natural-language intermediate reasoning contributes to correct semantic decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Equation-only requires the model to correctly parse semantics in one translation step; fails when mapping semantics→equation is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Equation-only helps small-step datasets across models but underperforms CoT on harder tasks for LaMDA, PaLM, GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4741.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4741.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExtCalc</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External Calculator (post-hoc arithmetic evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-processing intervention that extracts arithmetic expressions from model-generated chains-of-thought and evaluates them with a reliable external calculator (e.g., Python eval), to correct numeric computation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (PaLM, GPT-3, Codex, LaMDA, UL2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as a post-hoc step to outputs of the evaluated autoregressive models: parse equations appearing in generated CoTs and replace the model's computed numeric results with externally computed values.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same math word-problem benchmarks; particularly targets arithmetic/computation errors within generated CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Many CoT failures are due to numeric execution mistakes rather than structural reasoning errors; executing extracted expressions externally can fix these arithmetic mistakes and boost final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Tabled results show consistent accuracy improvements when external calculator is applied (e.g., PaLM 540B GSM8K 56.9 → 58.6; GPT-3 175B 46.9 → 49.6; Codex 63.1 → 65.4; UL2 and LaMDA show larger relative gains on some datasets), indicating a sizable fraction of errors are simple calculation mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>External calculator only helps when the CoT contains extractable and structurally-correct equations; it cannot correct semantic mapping errors or structurally incorrect equations; requires heuristics for equation extraction and propagation across chained equations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Representative gains: PaLM 540B GSM8K 56.9 → 58.6; GPT-3 175B 46.9 → 49.6; Codex 63.1 → 65.4; LaMDA 137B GSM8K 14.3 → 17.8; UL2 20B SVAMP 12.5 → 28.3 (large improvement where arithmetic extraction was possible).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Applying an external calculator is an explicit intervention demonstrating that part of model failure is numeric execution — this intervention systematically increases performance across models and datasets where extraction is feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Depends on reliably extracting correct expressions; fails for semantic/structural errors; ad-hoc propagation rules (e.g., string matching between equations) are brittle; not a general remedy for non-numeric reasoning mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>External calculator helps all tested models to varying degrees; larger models tend to benefit less in absolute terms (because they commit fewer numeric mistakes), while smaller/medium models show larger relative gains when equations are extractable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain of Thought Prompting Elicits Reasoning in Large Language Models', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Program induction by rationale generation: Learning to solve and explain algebraic word problems <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Reasoning like program executors <em>(Rating: 2)</em></li>
                <li>Neural execution engines: Learning to execute subroutines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4741",
    "paper_id": "paper-1b6e810ce0afd0dd093f789d2b2742d047e316d5",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting method that supplies a few in-context exemplars of 〈input, natural-language intermediate reasoning steps, output〉 so that the model generates step-by-step natural-language rationales before the final answer, eliciting multi-step reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (PaLM, GPT-3, LaMDA, Codex, UL2)",
            "model_description": "Applied to off-the-shelf autoregressive transformer LLMs of different sizes (PaLM: 8B/62B/540B; GPT-3 variants up to ~175B; LaMDA up to 137B; Codex code-davinci-002; UL2 20B). Models were used with greedy decoding for reported results.",
            "arithmetic_task_type": "Math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS) — multi-step arithmetic, multi-digit, algebraic word problems and subsets (SingleOp, SingleEq, AddSub, MultiArith).",
            "mechanism_hypothesis": "Providing intermediate natural-language steps (CoT) helps the model decompose semantic problem statements into sub-steps and equations, allocating token-level computation to intermediate reasoning and improving translation from language to arithmetic operations; this capability reliably emerges only at large model scale (~100B+ parameters).",
            "evidence_for_mechanism": "Large empirical accuracy gains on multi-step arithmetic and symbolic tasks for large models (PaLM 540B, GPT-3 175B, Codex) compared to standard prompting; ablations show that (a) equation-only prompting and (b) variable-compute-only (dots) do not match CoT, and (c) placing CoT after the answer does not match CoT-before-answer; manual inspection found generated CoTs that are logically correct when final answers are correct.",
            "evidence_against_mechanism": "CoT often fails or harms performance for small models (&lt;~10B): they produce fluent but illogical CoTs; CoT sometimes yields correct answers via coincidentally wrong chains; CoT gives little or negative gains on simple one-step problems; gains depend on model scale and exemplar quality/style.",
            "performance_metrics": "Key examples (accuracy %): PaLM 540B: standard prompting GSM8K 17.9 → CoT 56.9 (+ext calc 58.6); SVAMP 69.4 → 79.0 (+ext 79.8); MAWPS 79.2 → 93.3. GPT-3 175B (text-davinci-002): GSM8K 15.6 → 46.9 (+ext 49.6); SVAMP 65.7 → 68.9; MAWPS 72.7 → 87.1. Codex (code-davinci-002): GSM8K 19.7 → 63.1 (+ext 65.4). LaMDA 137B: GSM8K 6.5 → 14.3 (+ext 17.8). UL2 20B: GSM8K 4.1 → 4.4 (+ext 6.9). Improvements are largest on harder multi-step datasets (e.g., GSM8K) and smaller or negative on trivial single-step subsets.",
            "probing_or_intervention_results": "Ablation experiments: (1) Equation-only prompting (prompt to output only a math equation) provided little benefit on semantically complex datasets (GSM8K) though it helped some short-step datasets; (2) variable-compute-only (dots) matched baseline, indicating extra token budget alone isn't the cause; (3) CoT-after-answer matched baseline, indicating the sequential reasoning path matters; (4) applying a post-hoc external calculator to evaluate arithmetic in generated CoTs improves accuracy (see ext. calc. numbers). Manual error analysis categorized errors (semantic misunderstanding, one-step-missing, other) and showed scaling fixes many errors.",
            "limitations_and_failure_modes": "Failure modes include arithmetic computation mistakes (simple calculation errors), semantic misunderstanding of problem text, missing intermediate steps, symbol-mapping errors, hallucinations, repetitive/unterminating outputs, and variance due to exemplar wording/order; CoT emergence requires large models (~100B+), making deployment costly; CoTs are not guaranteed faithful or correct and can be persuasive while wrong.",
            "comparison_to_other_models": "CoT is an emergent capability: small models (&lt;~10B) do not benefit or are harmed; medium (~10–100B) show mixed gains; large (~100B+) show large gains. PaLM 540B and Codex show strongest CoT gains; GPT-3 175B shows substantial gains; LaMDA 137B and UL2 20B show modest or small gains. External calculator post-processing benefits multiple models.",
            "uuid": "e4741.0",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "PaLM-540B",
            "name_full": "PaLM (540 billion parameters)",
            "brief_description": "A very large autoregressive transformer language model evaluated in this paper; exhibited the strongest chain-of-thought gains and achieved new state-of-the-art results on several math benchmarks with CoT prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM 540B",
            "model_description": "Autoregressive transformer language model (PaLM family) with 540B parameters; evaluated with greedy decoding; other PaLM sizes tested: 8B and 62B.",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS subsets including multi-step MultiArith).",
            "mechanism_hypothesis": "At large scale, PaLM can internalize semantic-to-operation mappings and follow natural-language decomposition (elicited by CoT) to produce stepwise solutions — chain-of-thought acts as an explicit scaffold that the large model can follow to perform arithmetic reasoning.",
            "evidence_for_mechanism": "Large empirical accuracy jumps with CoT: GSM8K 17.9% → 56.9% (CoT); SVAMP 69.4% → 79.0%; MAWPS 79.2% → 93.3%; manual error analyses comparing 62B→540B show scale fixes many semantic and one-step-missing errors.",
            "evidence_against_mechanism": "CoT still produces arithmetic computation errors (handled partially by external calculator); CoT can be sensitive to exemplar selection and style; on some datasets (e.g., trivial single-step problems) gains are minimal.",
            "performance_metrics": "PaLM (540B) accuracies (standard → CoT → +ext calc where available): GSM8K 17.9 → 56.9 → 58.6; SVAMP 69.4 → 79.0 → 79.8; ASDiv 72.1 → 73.9; AQuA 25.2 → 35.8; MAWPS 79.2 → 93.3 → 93.5.",
            "probing_or_intervention_results": "Scaling ablation (62B→540B) fixed many categories of error; ablation showing equation-only prompting underperforms CoT on semantically complex sets; external calculator post-hoc evaluation slightly improves CoT scores by correcting numeric mistakes.",
            "limitations_and_failure_modes": "Requires large compute and capacity; CoTs may still include arithmetic errors and semantic mistakes; exemplar sensitivity remains; not all progress explained (open question whether true algorithmic reasoning occurs).",
            "comparison_to_other_models": "Outperformed other evaluated models under CoT in several benchmarks (e.g., new SOTA on GSM8K vs prior finetuned methods); Codex and GPT-3 also improve with CoT but PaLM 540B generally achieves the largest absolute accuracy on hardest benchmarks.",
            "uuid": "e4741.1",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "GPT-3-175B",
            "name_full": "GPT-3 (text-davinci-002, ~175B parameters)",
            "brief_description": "A large autoregressive language model (Instruct-style variant text-davinci-002) evaluated with and without chain-of-thought prompting; shows large improvements on multi-step arithmetic tasks when given CoT exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 175B (text-davinci-002)",
            "model_description": "Approx. 175B-parameter Instruct-style GPT-3 variant (text-davinci-002) used via API, with greedy decoding in experiments.",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).",
            "mechanism_hypothesis": "CoT provides intermediate reasoning tokens that guide the pretrained language model's latent patterns toward multi-step arithmetic decomposition; model leverages pretraining knowledge and token-level autoregression to implement stepwise calculation and mapping from problem text to arithmetic.",
            "evidence_for_mechanism": "Substantial gains on GSM8K: standard 15.6% → CoT 46.9% (→ +ext calc 49.6%); improvements also on AQuA and MAWPS. Ablations show equation-only and variable-compute-only not matching CoT.",
            "evidence_against_mechanism": "Smaller gains on some datasets (SVAMP, ASDiv) relative to other models; still susceptible to arithmetic mistakes that external calculator can partially correct; exemplar sensitivity and occasional incorrect but fluent CoTs.",
            "performance_metrics": "Reported accuracies (standard → CoT → +ext calc): GSM8K 15.6 → 46.9 → 49.6; SVAMP 65.7 → 68.9 → 70.3; ASDiv 70.3 → 71.3; AQuA 24.8 → 35.8; MAWPS 72.7 → 87.1 → 87.5.",
            "probing_or_intervention_results": "Post-hoc external evaluation of arithmetic expressions improves accuracy modestly; ablation studies (equation-only, dot-sequence, CoT-after-answer) point to the sequential decomposition as a causal factor; limited manual error analysis indicates many CoTs are logically correct when final answer is correct.",
            "limitations_and_failure_modes": "Produces arithmetic miscalculations, semantic errors, and faulty chains; gains depend on prompt exemplars and model scale; CoT not a panacea for small/medium models.",
            "comparison_to_other_models": "Substantial improvements relative to standard prompting; Codex and PaLM 540B often achieve higher absolute CoT accuracies on hardest datasets; GPT-3 175B shows clear benefit from CoT but is not the top performer in all benchmarks.",
            "uuid": "e4741.2",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Codex",
            "name_full": "Codex (code-davinci-002)",
            "brief_description": "A code-specialized transformer model variant evaluated in arithmetic tasks; shows very strong gains when using chain-of-thought prompting, possibly due to code pretraining's stronger symbolic/numeric capabilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Codex (code-davinci-002)",
            "model_description": "A code-trained large language model (OpenAI Codex variant) used in the same prompting experiments; exact parameter count unspecified in paper but a code-specialized model.",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).",
            "mechanism_hypothesis": "Code pretraining imparts stronger ability to emit structured symbolic/sketch-like computations; when guided by CoT, Codex robustly produces equations and intermediate symbolic steps that can be executed/checked.",
            "evidence_for_mechanism": "Large measured gains: GSM8K standard 19.7% → CoT 63.1% (+ext 65.4); SVAMP 69.9% → 76.4%; ASDiv 74.0% → 80.4%; AQuA 29.5% → 45.3%; MAWPS 78.7% → 92.6% (+ext 93.3%).",
            "evidence_against_mechanism": "When CoT contains incorrect symbolic structure (not just arithmetic mistakes), external calculators cannot fix it; exemplar and prompt sensitivity remain.",
            "performance_metrics": "Accuracies (standard → CoT → +ext calc): GSM8K 19.7 → 63.1 → 65.4; SVAMP 69.9 → 76.4 → 77.0; ASDiv 74.0 → 80.4 → 80.0; AQuA 29.5 → 45.3; MAWPS 78.7 → 92.6 → 93.3.",
            "probing_or_intervention_results": "External-calculator post-processing yields further gains, indicating many CoT failures are numeric-computation errors; ablations show equation-only helps some simple datasets but CoT remains superior for semantically complex problems.",
            "limitations_and_failure_modes": "Relies on correct structural translation of semantics into equations; still produces arithmetic and semantic errors; requires heuristics to extract and execute equations for post-hoc correction.",
            "comparison_to_other_models": "Codex with CoT often matches or surpasses GPT-3 175B and approaches or exceeds PaLM 540B on several benchmarks, suggesting code pretraining confers advantages for symbolic/numeric reasoning under CoT.",
            "uuid": "e4741.3",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "LaMDA-137B",
            "name_full": "LaMDA (137 billion parameters)",
            "brief_description": "A large dialogue-oriented transformer model family tested at multiple scales (up to 137B); shows moderate improvements with CoT but lower absolute performance on some arithmetic benchmarks compared to PaLM and Codex.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA 137B",
            "model_description": "LaMDA family transformer model evaluated at various scales (422M, 2B, 8B, 68B, 137B) with greedy decoding; reported results averaged over exemplar order seeds for some experiments.",
            "arithmetic_task_type": "Multi-step math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).",
            "mechanism_hypothesis": "CoT elicits stepwise reasoning in LaMDA but the model's pretraining and scale determine how reliably those steps map to correct arithmetic; larger sizes show emergent ability but at lower absolute levels than top-performing models.",
            "evidence_for_mechanism": "CoT improved LaMDA 137B on GSM8K 6.5% → 14.3% (+ext 17.8%), SVAMP 29.5% → 37.5% (+ext 42.1%), ASDiv 40.1% → 46.6% (+ext 53.4%), MAWPS 43.2% → 57.9% (+ext 69.3%).",
            "evidence_against_mechanism": "On AQuA CoT decreased performance in the reported setting (25.5% → 20.6%); smaller LaMDA sizes produced fluent but illogical CoTs; exemplar ordering had less variance for LaMDA but overall accuracy remains lower than top models.",
            "performance_metrics": "LaMDA 137B accuracies (standard → CoT → +ext calc where given): GSM8K 6.5 → 14.3 → 17.8; SVAMP 29.5 → 37.5 → 42.1; ASDiv 40.1 → 46.6 → 53.4; AQuA 25.5 → 20.6; MAWPS 43.2 → 57.9 → 69.3.",
            "probing_or_intervention_results": "Averaging over exemplar permutations reported; error analyses on LaMDA-generated CoTs: of 50 correct-answer examples nearly all chains were logically correct; among 50 incorrect-answer examples many had major semantic/coherence errors (54%).",
            "limitations_and_failure_modes": "Lower base arithmetic ability compared to PaLM/Codex; CoT can reduce performance on some tasks; significant fraction of incorrect CoTs have major semantic errors; arithmetic/calculation errors persist.",
            "comparison_to_other_models": "LaMDA 137B benefits from CoT but achieves lower absolute scores than PaLM 540B and Codex; shows the same qualitative emergent-scale trend but at lower performance.",
            "uuid": "e4741.4",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "UL2-20B",
            "name_full": "UL2 (20 billion parameters)",
            "brief_description": "A 20B-parameter model evaluated as part of the model suite; exhibits very small gains or low absolute arithmetic performance with CoT prompting, illustrating that CoT improvements are scale-dependent.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "UL2 20B",
            "model_description": "UL2 family transformer tuned with mixture-of-denoising objectives; a 20B-parameter instance was evaluated with greedy decoding.",
            "arithmetic_task_type": "Math word problems (GSM8K, SVAMP, ASDiv, AQuA, MAWPS).",
            "mechanism_hypothesis": "At this smaller scale CoT fails to reliably produce logically correct decomposition — models can output fluent text but lack the internal capacities to map those steps to correct arithmetic.",
            "evidence_for_mechanism": "Empirically small improvements: GSM8K 4.1% → 4.4% (→ +ext calc 6.9%); SVAMP 10.1% → 12.5% (→ +ext calc 28.3%); some ext. calc. gains indicate numeric computation failures that can be partially corrected when extractable.",
            "evidence_against_mechanism": "CoT sometimes harmed or negligibly affected performance; many generated CoTs are fluent but incorrect; overall demonstrates scale threshold for CoT effectiveness.",
            "performance_metrics": "UL2 20B accuracies (standard → CoT → +ext calc): GSM8K 4.1 → 4.4 → 6.9; SVAMP 10.1 → 12.5 → 28.3; ASDiv 16.0 → 16.9 → 34.3; AQuA 20.5 → 23.6; MAWPS 16.6 → 19.1 → 42.7.",
            "probing_or_intervention_results": "Post-hoc external calculation produced large relative gains on some datasets (e.g., ASDiv, MAWPS), implying many failures are numeric rather than structural when the model outputs extractable equations; overall ablations show CoT ineffective at this scale.",
            "limitations_and_failure_modes": "Small-scale model failures: fluent but illogical CoTs, inability to generalize symbol manipulations, low arithmetic competence; CoT can sometimes reduce performance relative to baseline.",
            "comparison_to_other_models": "Performs substantially worse than large models (PaLM 540B, Codex, GPT-3 175B) under CoT; highlights the emergence-of-scale phenomenon.",
            "uuid": "e4741.5",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "Equation-only",
            "name_full": "Equation-only Prompting (ablation)",
            "brief_description": "An ablation that prompts the model to output only the mathematical equation corresponding to the problem (no natural-language intermediate steps) before providing the final answer, used to test whether CoT gains are merely from producing an equation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (evaluated primarily on PaLM and LaMDA variants)",
            "model_description": "Same model family instances as main experiments; equation-only is a prompting variant rather than a separate model.",
            "arithmetic_task_type": "Same math word-problem benchmarks; especially informative on short-step vs. semantically complex multi-step problems.",
            "mechanism_hypothesis": "Producing an explicit equation might be sufficient to improve arithmetic performance by converting semantics to executable math directly.",
            "evidence_for_mechanism": "Equation-only prompting improves performance on simple one-step or two-step datasets (e.g., some MAWPS subsets) where the equation directly follows from semantics.",
            "evidence_against_mechanism": "Equation-only prompting fails on semantically complex datasets (GSM8K); qualitative example shows equation-only generated an incorrect equation while CoT produced the correct decomposition and answer, concluding equation-only is insufficient for complex semantic understanding.",
            "performance_metrics": "Reported as part of ablation: for GSM8K equation-only did not help much; for easy one- or two-step datasets equation-only gave measurable improvements (details in Appendix; overall CoT outperforms equation-only on complex data).",
            "probing_or_intervention_results": "Ablation contrast demonstrates CoT's benefit is not solely from exposing an equation; natural-language intermediate reasoning contributes to correct semantic decomposition.",
            "limitations_and_failure_modes": "Equation-only requires the model to correctly parse semantics in one translation step; fails when mapping semantics→equation is nontrivial.",
            "comparison_to_other_models": "Equation-only helps small-step datasets across models but underperforms CoT on harder tasks for LaMDA, PaLM, GPT-3.",
            "uuid": "e4741.6",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        },
        {
            "name_short": "ExtCalc",
            "name_full": "External Calculator (post-hoc arithmetic evaluator)",
            "brief_description": "A post-processing intervention that extracts arithmetic expressions from model-generated chains-of-thought and evaluates them with a reliable external calculator (e.g., Python eval), to correct numeric computation errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (PaLM, GPT-3, Codex, LaMDA, UL2)",
            "model_description": "Applied as a post-hoc step to outputs of the evaluated autoregressive models: parse equations appearing in generated CoTs and replace the model's computed numeric results with externally computed values.",
            "arithmetic_task_type": "Same math word-problem benchmarks; particularly targets arithmetic/computation errors within generated CoTs.",
            "mechanism_hypothesis": "Many CoT failures are due to numeric execution mistakes rather than structural reasoning errors; executing extracted expressions externally can fix these arithmetic mistakes and boost final accuracy.",
            "evidence_for_mechanism": "Tabled results show consistent accuracy improvements when external calculator is applied (e.g., PaLM 540B GSM8K 56.9 → 58.6; GPT-3 175B 46.9 → 49.6; Codex 63.1 → 65.4; UL2 and LaMDA show larger relative gains on some datasets), indicating a sizable fraction of errors are simple calculation mistakes.",
            "evidence_against_mechanism": "External calculator only helps when the CoT contains extractable and structurally-correct equations; it cannot correct semantic mapping errors or structurally incorrect equations; requires heuristics for equation extraction and propagation across chained equations.",
            "performance_metrics": "Representative gains: PaLM 540B GSM8K 56.9 → 58.6; GPT-3 175B 46.9 → 49.6; Codex 63.1 → 65.4; LaMDA 137B GSM8K 14.3 → 17.8; UL2 20B SVAMP 12.5 → 28.3 (large improvement where arithmetic extraction was possible).",
            "probing_or_intervention_results": "Applying an external calculator is an explicit intervention demonstrating that part of model failure is numeric execution — this intervention systematically increases performance across models and datasets where extraction is feasible.",
            "limitations_and_failure_modes": "Depends on reliably extracting correct expressions; fails for semantic/structural errors; ad-hoc propagation rules (e.g., string matching between equations) are brittle; not a general remedy for non-numeric reasoning mistakes.",
            "comparison_to_other_models": "External calculator helps all tested models to varying degrees; larger models tend to benefit less in absolute terms (because they commit fewer numeric mistakes), while smaller/medium models show larger relative gains when equations are extractable.",
            "uuid": "e4741.7",
            "source_info": {
                "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Reasoning like program executors",
            "rating": 2
        },
        {
            "paper_title": "Neural execution engines: Learning to execute subroutines",
            "rating": 1
        }
    ],
    "cost": 0.021668,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</h1>
<p>Jason Wei<br>Brian Ichter</p>
<p>Xuezhi Wang Dale Schuurmans Maarten Bosma<br>Fei Xia Ed H. Chi Quoc V. Le Denny Zhou<br>Google Research, Brain Team<br>{jasonwei, dennyzhou}@google.com</p>
<h4>Abstract</h4>
<p>We explore how generating a chain of thought-a series of intermediate reasoning steps-significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning processes are highlighted.</p>
<h1>1 Introduction</h1>
<p>The NLP landscape has recently been revolutionized by language models (Peters et al., 2018; Devlin et al., 2019; Brown et al., 2020, inter alia). Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency ( Kaplan et al., 2020; Brown et al., 2020, inter alia). However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning (Rae et al., 2021).</p>
<p>This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer. Prior work has given models the ability to generate natural language intermediate steps by training from scratch (Ling et al., 2017) or finetuning a pretrained model (Cobbe et al., 2021), in addition to neuro-symbolic methods that use formal languages instead of natural language (Roy and Roth, 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Second, large language models offer the exciting prospect of in-context few-shot learning via prompting. That is, instead of finetuning a separate language model checkpoint for each new task, one can simply "prompt" the model with a few input-output exemplars demonstrating the task. Remarkably, this has been successful for a range of simple question-answering tasks (Brown et al., 2020).</p>
<p>Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input-output pairs used in normal machine learning. For the traditional fewshot prompting method used in Brown et al. (2020), it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale (Rae et al., 2021). In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: 〈input, chain of thought, output〉. A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as chain-of-thought prompting. An example prompt is shown in Figure 1.</p>
<p>We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree. Figure 2 illustrates one such result-on the GSM8K benchmark of math word problems (Cobbe et al., 2021), chain-of-thought prompting with PaLM 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance. A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality. This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).</p>
<h2>2 Chain-of-Thought Prompting</h2>
<p>Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem. It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: "After Jane gives 2 flowers to her mom she has $10 \ldots$ then after she gives 3 to her dad she will have $7 \ldots$ so the answer is 7." The goal of this paper is to endow language models with the ability to generate a similar chain of thought-a coherent series of intermediate reasoning steps that lead to the final answer for a problem. We will show that sufficiently large</p>
<p>language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.
Figure 1 shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect. The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come after the final answer (Narang et al., 2020; Wiegreffe et al., 2022; Lampinen et al., 2022, inter alia)).
Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.</p>
<ol>
<li>First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.</li>
<li>Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model's computations that support an answer remains an open question).</li>
<li>Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.</li>
<li>Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.
In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (Section 3), commonsense reasoning (Section 4), and symbolic reasoning (Section 5).</li>
</ol>
<h1>3 Arithmetic Reasoning</h1>
<p>We begin by considering math word problems of the form in Figure 1, which measure the arithmetic reasoning ability of language models. Though simple for humans, arithmetic reasoning is a task where language models often struggle (Hendrycks et al., 2021; Patel et al., 2021, inter alia). Strikingly, chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark (Cobbe et al., 2021).</p>
<h3>3.1 Experimental Setup</h3>
<p>We explore chain-of-thought prompting for various language models on multiple benchmarks.
Benchmarks. We consider the following five math word problem benchmarks: (1) the GSM8K benchmark of math word problems (Cobbe et al., 2021), (2) the SVAMP dataset of math word problems with varying structures (Patel et al., 2021), (3) the ASDiv dataset of diverse math word problems (Miao et al., 2020), (4) the AQuA dataset of algebraic word problems, and (5) the MAWPS benchmark (Koncel-Kedziorski et al., 2016). Example problems are given in Appendix Table 12.
Standard prompting. For the baseline, we consider standard few-shot prompting, popularized by Brown et al. (2020), in which a language model is given in-context exemplars of input-output pairs before outputting a prediction for a test-time example. Exemplars are formatted as questions and answers. The model gives the answer directly, as shown in Figure 1 (left).
Chain-of-thought prompting. Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in Figure 1 (right). As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting-Figure 1 (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix Table 20. (These particular exemplars did not undergo prompt engineering; robustness is studied in Section 3.4 and Appendix A.2.) To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Examples of 〈input, chain of thought, output〉 triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted. Full prompts in Appendix G.
math word problems, we used this single set of eight chain of thought exemplars for all benchmarks except AQuA, which is multiple choice instead of free response. For AQuA, we used four exemplars and solutions from the training set, as given in Appendix Table 21.</p>
<p>Language models. We evaluate five large language models. The first is GPT-3 (Brown et al., 2020), for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters (Ouyang et al., 2022).The second is LaMDA (Thoppilan et al., 2022), which has models of 422M, 2B, 8B, 68B, and 137B parameters. The third is PaLM, which has models of 8B, 62B, and 540B parameters. The fourth is UL2 20B (Tay et al., 2022), and the fifth is Codex (Chen et al., 2021, code-davinci-002 in the OpenAI API). We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations (Wang et al., 2022a)). For LaMDA, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars. As LaMDA experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models.</p>
<h1>3.2 Results</h1>
<p>The strongest results of chain-of-thought prompting are summarized in Figure 4, with all experimental outputs for each model collection, model size, and benchmark shown in Table 2 in the Appendix. There are three key takeaways. First, Figure 4 shows that chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of $\sim 100 \mathrm{~B}$ parameters. We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.</p>
<p>Second, chain-of-thought prompting has larger performance gains for more-complicated problems. For instance, for GSM8K (the dataset with the lowest baseline performance), performance more than doubled for the largest GPT and PaLM models. On the other hand, for SingleOp, the easiest subset of MAWPS which only requires a single step to solve, performance improvements were either negative or very small (see Appendix Table 3).</p>
<p>Third, chain-of-thought prompting via GPT-3 175B and PaLM 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset. Figure 4 shows how PaLM 540B uses chain-ofthought prompting to achieve new state of the art on GSM8K, SVAMP, and MAWPS (though note that standard prompting already passed the prior best for SVAMP). On the other two datasets, AQuA and ASDiv, PaLM with chain-of-thought prompting reaches within $2 \%$ of the state of the art (Appendix Table 2).</p>
<p>To better understand why chain-of-thought prompting works, we manually examined modelgenerated chains of thought by LaMDA 137B for GSM8K. Of 50 random examples where the model returned the correct final answer, all of the generated chains of thought were also logically and mathematically correct except two that coincidentally arrived at the correct answer (see Appendix D.1, and Table 8 for examples of correct model-generated chains of thought). We also randomly examined 50 random samples for which the model gave the wrong answer. The summary of this analysis is that $46 \%$ of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol mapping error, or one reasoning step missing), and that the other $54 \%$ of the chains of thought had major errors in semantic understanding or coherence (see Appendix D.2). To provide a small insight into why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors made by PaLM 62B and whether those errors were fixed by scaling to PaLM 540B. The summary is that scaling PaLM to 540B fixes a large portion of one-step missing and semantic understanding errors in the 62B model (see Appendix A.1).</p>
<h3>3.3 Ablation Study</h3>
<p>The observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting. Figure 5 shows an ablation study with three variations of chain of thought described below.</p>
<p>Equation only. One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. Figure 5 shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought. For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix Table 6).</p>
<p>Variable compute only. Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots ( . . . ) equal to the number of characters in the equation needed to solve the problem. This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.</p>
<p>Chain of thought after answer. Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining. Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer. This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge.</p>
<h3>3.4 Robustness of Chain of Thought</h3>
<p>Sensitivity to exemplars is a key consideration of prompting approaches-for instance, varying the permutation of few-shot exemplars can cause the accuracy of GPT-3 on SST-2 to range from near chance ( $54.3 \%$ ) to near state of the art ( $93.4 \%$ ) (Zhao et al., 2021). In this final subsection, we evaluate robustness to chains of thought written by different annotators. In addition to the results above, which used chains of thought written by an Annotator A, two other co-authors of this paper (Annotators B and C) independently wrote chains of thought for the same few-shot exemplars (shown in Appendix H). Annotator A also wrote another chain of thought that was more concise than the original, following the style of solutions given in Cobbe et al. (2021). ${ }^{1}$</p>
<p>Figure 6 shows these results for LaMDA 137B on GSM8K and MAWPS (ablation results for other datasets are given in Appendix Table 6 / Table 7). Although there is variance among different chain of thought annotations, as would be expected when using exemplar-based prompting (Le Scao and Rush, 2021; Reynolds and McDonell, 2021; Zhao et al., 2021), all sets of chain of thought prompts outperform the standard baseline by a large margin. This result implies that successful use of chain of thought does not depend on a particular linguistic style.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Ablation study for different variations of prompting using LaMDA 137B and PaLM 540B. Results for other datasets are given in Appendix Table 6 and Table 7.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Chain-of-thought prompting has variance for different prompt examples (as expected) but outperforms standard prompting for various annotators as well as for different exemplars.</p>
<p>To confirm that successful chain-of-thought prompting works for other sets of exemplars, we also run experiments with three sets of eight exemplars randomly sampled from the GSM8K training set, an independent</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>source (examples in this dataset already included reasoning steps like a chain of thought). Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.
In addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see Appendix A.2).</p>
<h1>4 Commonsense Reasoning</h1>
<p>Although chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge. Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems (Talmor et al., 2021).</p>
<p>Benchmarks. We consider five datasets covering a diverse range of commonsense reasoning types. The popular CSQA (Talmor et al., 2019) asks commonsense questions about the world involving complex semantics that often require prior knowledge. StrategyQA (Geva et al., 2021) requires models to infer a multi-hop strategy to answer questions. We choose two specialized evaluation sets from the BIG-bench effort (BIG-bench collaboration, 2021): Date Understanding, which involves inferring a date from a given context, and Sports Understanding, which involves determining whether a sentence relating to sports is plausible or implausible. Finally, the SayCan dataset (Ahn et al., 2022) involves mapping a natural language instruction to a sequence of robot actions from a discrete set. Figure 3 shows examples with chain of thought annotations for all datasets.</p>
<p>Prompts. We follow the same experimental setup as the prior section. For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars. The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set. For SayCan, we use six examples from the training set used in Ahn et al. (2022) and also manually composed chains of thought.</p>
<p>Results. Figure 7 highlights these results for PaLM (full results for LaMDA, GPT-3, and different model scales are shown in Table 4). For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B. With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA ( $75.6 \%$ vs $69.4 \%$ ) and outperforming an unaided sports enthusiast on sports understanding ( $95.4 \%$ vs $84 \%$ ). These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Chain-of-thought prompting also improves the commonsense reasoning abilities of language models. The language model shown here is PaLM. Prior best numbers are from the leaderboards of CSQA (Talmor et al., 2019) and StrategyQA (Geva et al., 2021) (single-model only, as of May 5, 2022). Additional results using various sizes of LaMDA, GPT-3, and PaLM are shown in Table 4.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>5 Symbolic Reasoning</h1>
<p>Our final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models. We show that chain-ofthought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.</p>
<p>Tasks. We use the following two toy tasks.</p>
<ul>
<li>Last letter concatenation. This task asks the model to concatenate the last letters of words in a name (e.g., "Amy Brown" $\rightarrow$ "yn"). It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought. ${ }^{3}$ We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data (https://namecensus.com/).</li>
<li>Coin flip. This task asks the model to answer whether a coin is still heads up after people either flip or don't flip the coin (e.g., "A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?" $\rightarrow$ "no").
As the construction of these symbolic reasoning tasks is well-defined, for each task we consider an in-domain test set for which examples had the same number of steps as the training/few-shot exemplars, as well as an out-of-domain (OOD) test set, for which evaluation examples had more steps than those in the exemplars. For last letter concatenation, the model only sees exemplars of names with two words, and then performs last letter concatenation on names with 3 and 4 words. ${ }^{4}$ We do the same for the number of potential flips in the coin flip task. Our experimental setup uses the same methods and models as in the prior two sections. We again manually compose chains of thought for the few-shot exemplars for each task, which are given in Figure 3.</li>
</ul>
<p>Results. The results of these in-domain and OOD evaluations are shown in Figure 8 for PaLM, with results for LaMDA shown in Appendix Table 5. With PaLM 540B, chain-of-thought prompting leads to almost $100 \%$ solve rates (note that standard prompting already solves coin flip with PaLM 540, though not for LaMDA 137B). Note that these in-domain evaluations are "toy tasks" in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example. And yet, small models still fail-the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.
As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale.</p>
<h2>6 Discussion</h2>
<p>We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models. We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (Section 3). Next,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (Section 4). Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (Section 5). In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. No language models were finetuned in the process of writing this paper.
The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme (Wei et al., 2022b). For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves. Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully-in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models. This observation likely raises more questions than it answers-for instance, how much more can we expect reasoning ability to improve with a further increase in model scale? What other prompting methods might expand the range of tasks that language models can solve?
As for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually "reasoning," which we leave as an open question. Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization). Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work (Rashkin et al., 2021; Ye and Durrett, 2022; Wiegreffe et al., 2022, inter alia). Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.</p>
<h1>7 Related Work</h1>
<p>This work is inspired by many research areas, which we detail in an extended related work section (Appendix C). Here we describe two directions and associated papers that are perhaps most relevant.
The first relevant direction is using intermediate steps to solve reasoning problems. Ling et al. (2017) pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason (Roy et al., 2015; Chiang and Chen, 2019; Amini et al., 2019; Chen et al., 2019). Cobbe et al. (2021) extend Ling et al. (2017) by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, Nye et al. (2021) leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.
Naturally, this paper also relates closely to the large body of recent work on prompting. Since the popularization of few-shot prompting as given by Brown et al. (2020), several general approaches have improved the prompting ability of models, such as automatically learning prompts (Lester et al., 2021) or giving models instructions describing a task (Wei et al., 2022a; Sanh et al., 2022; Ouyang et al., 2022). Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.</p>
<h2>8 Conclusions</h2>
<p>We have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves. Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work on language-based approaches to reasoning.</p>
<h1>Acknowledgements</h1>
<p>We thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the paper. We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton, and Aakanksha Chowdhery for helpful discussions. We thank Sid Maxwell for notifying us about a mistake in the manual error analysis in the original manuscript.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operationbased formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. EMNLP.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. 2018. Learning with latent language. NAACL.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.</p>
<p>BIG-bench collaboration. 2021. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. In preparation.</p>
<p>Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg Durrett. 2021. Flexible generation of natural language deductions. EMNLP.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. NeurIPS.</p>
<p>Jonathon Cai, Richard Shin, and Dawn Song. 2017. Making neural programming architectures generalize via recursion. ICLR.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. NeurIPS.</p>
<p>Howard Chen, Jacqueline He, Karthik Narasimhan, and Danqi Chen. 2022. Can rationalization improve robustness? NAACL.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.</p>
<p>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2019. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. ICLR.</p>
<p>Ting-Rui Chiang and Yun-Nung Chen. 2019. Semantically-aligned equation generation for solving and reasoning math word problems. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2656-2668, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. IJCAI.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL.</p>
<p>Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. 2019. Neural logic machines. ICLR.</p>
<p>Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. $A C L$.</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. TACL.</p>
<p>Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark. 2022. DREAM: Uncovering mental models behind language models. NAACL.</p>
<p>Braden Hancock, Paroma Varma, Stephanie Wang, Martin Bringmann, Percy Liang, and Christopher Ré. 2018. Training classifiers with natural language explanations. ACL.</p>
<p>Peter Hase and Mohit Bansal. 2022. When can models learn from explanations? a formal framework for understanding the roles of explanation data. $A C L$.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. EMNLP.</p>
<p>Zhanming Jie, Jierui Li, and Wei Lu. 2022. Learning to reason deductively: Math word problem solving as complex relation extraction. arXiv preprint arXiv:2203.10316.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</p>
<p>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. NAACL.</p>
<p>Andrew K. Lampinen, Ishita Dasgupta, Stephanie C.Y. Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L. McClelland, Jane X. Wang, and Felix Hill. 2022. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329.</p>
<p>Yihuai Lan, Lei Wang, Qiyuan Zhang, Yunshi Lan, Bing Tian Dai, Yan Wang, Dongxiang Zhang, and Ee-Peng Lim. 2021. MWPToolkit: An open-source framework for deep learning-based math word problem solvers. arXiv preprint arXiv:2109.00799.</p>
<p>Teven Le Scao and Alexander Rush. 2021. How many data points is a prompt worth? NAACL.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. EMNLP.</p>
<p>Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. Proceedings of the 2nd Workshop on Text Meaning and Interpretation.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. ACL.</p>
<p>Zhengzhong Liang, Steven Bethard, and Mihai Surdeanu. 2021. Explainable multi-hop verbal reasoning through internal monologue. NAACL.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. ACL.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586.</p>
<p>Bodhisattwa Prasad Majumder, Oana-Maria Camburu, Thomas Lukasiewicz, and Julian McAuley. 2021. Rationale-inspired natural language explanations with commonsense. arXiv preprint arXiv:2106.13876.</p>
<p>Ana Marasović, Iz Beltagy, Doug Downey, and Matthew E Peters. 2022. Few-shot self-rationalization with natural language prompts. NAACL Findings.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In $A C L$.</p>
<p>Shen Yun Miao, Chao Chun Liang, and Keh Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. ACL.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training text-to-text models to explain their predictions. arXiv preprint arXiv:2004.14546.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? NAACL.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. NAACL.</p>
<p>Xinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin, Yan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu Chen. 2022. Reasoning like program executors. arXiv preprint arXiv:2201.11473.</p>
<p>Piotr Piękos, Mateusz Malinowski, and Henryk Michalewski. 2021. Measuring and improving BERT's mathematical abilities by predicting the order of reasoning. $A C L$.</p>
<p>Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models: Methods, analysis \&amp; insights from training Gopher. arXiv preprint arXiv:2112.11446.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</p>
<p>Dheeraj Rajagopal, Vidhisha Balachandran, Eduard H. Hovy, and Yulia Tsvetkov. 2021. SelfExplain: A self-explaining architecture for neural text classifiers. EMNLP.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! Leveraging language models for commonsense reasoning. ACL.</p>
<p>Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehension with numerical reasoning. EMNLP.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870.</p>
<p>Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration. arXiv preprint arXiv:2109.02102.</p>
<p>Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A recipe for arbitrary text style transfer with large language models. ACL.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. EMNLP.
Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about Quantities in Natural Language. TACL.</p>
<p>Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. EMNLP.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. ICLR.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. 2021. Generate \&amp; rank: A multi-task framework for math word problems. In Findings of the Association for Computational Linguistics: EMNLP 2021.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. NAACL.</p>
<p>Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, and Jonathan Berant. 2020. Leap-ofthought: Teaching pre-trained models to systematically reason over implicit knowledge. NeurIPS.</p>
<p>Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021. CommonsenseQA 2.0: Exposing the limits of ai through gamification. NeurIPS Track on Datasets and Benchmarks.</p>
<p>Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, and Donald Metzler. 2022. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131.</p>
<p>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. 2022. LaMDA: Language models for dialog applications. arXiv preprint arXiv:2201.08239.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022a. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 2022b. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv preprint arXiv:2204.07705.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot learners. ICLR.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022b. Emergent abilities of large language models. Transactions on Machine Learning Research.</p>
<p>Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-AI collaboration for generating free-text explanations. NAACL.</p>
<p>Sarah Wiegreffe and Ana Marasović. 2021. Teach me to explain: A review of datasets for explainable NLP. NeurIPS.</p>
<p>Sarah Wiegreffe, Ana Marasović, and Noah A. Smith. 2021. Measuring association between labels and free-text rationales. EMNLP.</p>
<p>Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. PromptChainer: Chaining large language model prompts through visual programming. CHI Extended Abstracts.</p>
<p>Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b. AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. CHI.</p>
<p>Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan, and Milad Hashemi. 2020. Neural execution engines: Learning to execute subroutines. NeurIPS.</p>
<p>Huihan Yao, Ying Chen, Qinyuan Ye, Xisen Jin, and Xiang Ren. 2021. Refining language models with compositional explanations. NeurIPS.</p>
<p>Xi Ye and Greg Durrett. 2022. The unreliability of explanations in few-shot in-context learning. arXiv preprint arXiv:2205.03401.</p>
<p>Yordan Yordanov, Vid Kocijan, Thomas Lukasiewicz, and Oana-Maria Camburu. 2021. Few-shot out-of-domain transfer learning of natural language explanations. arXiv preprint arXiv:2112.06204.</p>
<p>Omar Zaidan, Jason Eisner, and Christine Piatko. 2007. Using "annotator rationales" to improve machine learning for text categorization. NAACL.</p>
<p>Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615.
Eric Zelikman, Yuhuai Wu, and Noah D. Goodman. 2022. STaR: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465.</p>
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. ICML.</p>
<p>Wangchunshu Zhou, Jinyi Hu, Hanlin Zhang, Xiaodan Liang, Maosong Sun, Chenyan Xiong, and Jian Tang. 2020. Towards interpretable natural language understanding with explanations as latent variables. NeurIPS.</p>
<h1>Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 6 and Appendix A.2.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] We don't expect negative societal impacts as a direct result of the contributions in our paper. One consideration, however, is that generated chain of thought is not always factual, which is noted as a limitation in Appendix D. 1 (and note that we do not suggest using such chains of thought in a factual manner or in any real-world scenario).
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We included inputs, outputs, and targets for LaMDA and GPT-3 in the supplementary material. Although we use proprietary models, we GPT-3 results are fully reproducible. Reproducibility is further discussed in Appendix E.1.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Data splits were specified, N/A for hyperparams.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Standard deviation for multiple seeds using LaMDA 137B, where each seed is a different random order of exemplars, is given in Table 6 and Table 7.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Type of resources are described in Appendix E.2, though we did not estimate the total amount of compute.</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] We used two models that we anonymized based on the recommendation of the NeurIPS chairs. These models will be cited in the camera-ready version of the paper.
(b) Did you mention the license of the assets? [Yes] See Appendix E.3.
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] The coinflip and last letter concatenation datasets are the only new assets, and they are given in the Supplementary Materials.
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] No human data collected.
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] No human data collected.</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</li>
</ol>
<h1>A Frequently Asked Questions</h1>
<h2>A. 1 Why does increasing model scale improve chain-of-thought prompting?</h2>
<p>The finding that successful chain-of-thought reasoning predictably emerges only at certain model scales is intriguing. Scaling up language models has been shown to confer benefits such as improved performance and sample efficiency (Kaplan et al., 2020), but chain-of-thought reasoning is emergent in the sense that its success cannot be predicted only by extrapolating the performance of small scale models, as chain of thought actually hurts performance for most models smaller than 10B parameters.
The question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and we made a preliminary attempt to shed insight into it via error analysis. This small analysis involved manually reading 45 errors made by PaLM 62B and categorizing them into semantic understanding ( 20 errors), one step missing ( 18 errors), and other errors ( 7 errors). The "other category" included hallucinations, repetitive outputs, and symbol mapping errors. This categorization is a coarse one borrowed from the initial error analysis done on LaMDA in Appendix D.2, for which categories were conceived based on what improvements were needed to make the chain of thought correct.
As shown in Figure 9, scaling PaLM to 540B parameters fixed a substantial portion of errors in all three categories. Examples of semantic understanding and one-step missing errors that were fixed by scaling PaLM to 540B are given in Figure 10. This result appears consistent with a hypothesis that language models acquire a range of semantic understanding and logical reasoning skills as a function of model scale (though note that model scale is often conflated with other factors, such as amount of training compute).
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Error analysis of 45 problems that PaLM 62B got incorrect. These errors were categorized that semantic understanding, one step missing, and other. The other category includes hallucinations, repetitive outputs, and symbol mapping errors. Scaling PaLM to 540B fixed a substantial portion of errors in all categories.</p>
<p>There are also three notable points regarding why small language models fail. The first observation is that small language models fail at even relatively easy symbol mapping tasks. As demonstrated in Section 5, for even symbolic reasoning tasks that only require generalization to new examples using the same chain of thought logical structure that was given in the few-shot exemplars, small language models still failed. The second observation is that small language models seem to have inherently weaker arithmetic abilities, as shown by Brown et al. (2020), the ability to do simple arithmetic operations (without semantic understanding) requires sufficient model scale. Finally, we noticed qualitatively that small language models often did not generate a final answer that could be parsed, due to either repetitions or logic that never arrived at a final answer.
In summary, the success of chain-of-thought reasoning as a result of model scale is a complicated phenomena that likely involves a variety of emergent abilities (semantic understanding, symbol mapping, staying on topic, arithmetic ability, faithfulness, etc). Future work could more thoroughly investigate what properties of pretraining data, model architecture, and optimization objective causally enable such reasoning capabilities.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 10: Examples of semantic understanding and one-step missing errors that were fixed by scaling PaLM from 62B to 540B.</p>
<h1>A. 2 What is the role of prompt engineering?</h1>
<p>One of the key considerations of prompting is sensitivity to the exact prompt. There is no shortage of work showing that prompts affect language models in unexpected ways (Min et al., 2022). The general way that we created chain of thought annotations was by taking eight exemplars from the training set and decomposing the reasoning process into multiple steps leading to the final answer. Examples of chain of thought annotations are provided in Figure 3, with full prompts given in Appendix G. To analyze how sensitive chain of thought is to prompt engineering, we performed robustness experiments with respect to various factors.</p>
<ul>
<li>Different annotators. We first analyze robustness to three different annotators (Section 3.4 and Figure 6). Although there is notable variance in performance (which we will discuss later), chain of thought performed better than the baseline by a large margin for all three annotators on eight datasets in arithmetic, commonsense, and symbolic reasoning (Table 6 and Table 7). Similar to the annotation process in Cobbe et al. (2021), annotators were not given specific instructions about</li>
</ul>
<p>how to write the chain of thought annotations other than to simply write the step-by-step reasoning process that led to the final answer. Thus, the annotations were written in each annotator's own linguistic "chain of thought" writing style.</p>
<ul>
<li>Annotators without machine learning background. The GSM8K dataset (Cobbe et al., 2021) conveniently provides a training set with reasoning chains written by crowd compute workers, which enables us to investigate whether chain of thought still works with reasoning chains from an independent source without a background in machine learning. So we randomly sampled three sets of eight exemplars with chains of thought from GSM8K. These chain of thought annotations also outperformed the baseline by a large margin for all four arithmetic datasets (Table 6), indicating that chain of thought is not dependent on a particular set of annotators.</li>
<li>Different exemplars. The different GSM8K exemplars experiment above (Table 6) also shows that chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of exemplars on all four arithmetic datasets (instead of picking exemplars from the training set for each dataset), which suggests that the exemplars do not necessarily have to come from the same dataset distribution as the test examples.</li>
<li>Different order of exemplars. Prior work has shown that in some cases (e.g., classification) even the order of prompts matter-varying the permutation of few-shot exemplars can cause the accuracy of GPT-3 on SST-2 to range from near chance (54.3\%) to near SOTA (93.4\%) (Zhao et al., 2021). We show the standard deviation of performance from different exemplars in Table 6 and Table 7. Standard deviations with respect to prompt order are relatively minimal in almost all cases. The one exception is the coin flip task, for which exemplar orders have high standard deviation, likely for the reason cited in Zhao et al. (2021)—for classification, many exemplars of the same category in a row biases the model outputs).</li>
<li>Different number of exemplars. We also found that gains from chain-of-thought prompting generally still held when there was a varying number of few-shot exemplars. This is shown for five datasets in Figure 11 (we did not have the compute to run this for all datasets). We also found in preliminary experiments that further increasing the number of exemplars in standard prompting did not lead to significant gains (e.g., increasing from 8 to 16 exemplars did not improve the performance of standard prompting enough to catch up with chain-of-thought prompting).</li>
<li>Different language models. Another interesting question is whether certain prompts that work better for one model work better for other large language models. We find that with the same prompts, chain-of-thought prompting improves performance across all three models (LaMDA, GPT-3, and PaLM) for all datasets except CSQA and StrategyQA for GPT-3 (Table 1, Table 4, Table 5). The fact that gains from chain of thought did not transfer perfectly among models is a limitation; further work could investigate why how different pre-training datasets and model architectures affect the performance gain from chain-of-thought prompting.</li>
</ul>
<p>Prompt engineering still matters, though. Although the results are relatively robust to the prompt for arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can improve performance significantly in many cases. Though most chain of thought annotations outperform standard prompting, there is large variation in many cases. For instance, for the coin flip task, the performance varied from $99.6 \%$ for Annotator A to $71.4 \%$ for Annotator C, though both were above standard prompting $=50.0 \%$ (see Table 7). There are even tasks where prompt engineering is a requirement for good performance. In preliminary experiments, we tried using chain of thought to enable language models to reverse the order of a list of 5 items. While two co-authors were not able to write chain of thought prompts that solved the task despite their best attempts, a third co-author was able to write a chain of thought that perfectly solved the task.</p>
<p>How to generate chain of thought annotations in a robust fashion could be an interesting direction for future work. For instance, an idea here could be to use a large language model to automatically generate chains of thought via prompting (and potentially optimize this over a validation set).</p>
<h1>A. 3 Will chain-of-thought prompting improve performance for my task of interest?</h1>
<p>While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more helpful for some tasks than others. Based on the experiments in this paper, our intuition is that chain of thought helps the most when three conditions are met: (1) the task is challenging and requires</p>
<p>multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively flat. Conversely, the benefits are smaller when one or more of these conditions are not met.</p>
<p>These intuitions are perhaps supported by the arithmetic reasoning results. The performance gain from chain-of-thought prompting is largest for PaLM 540B on GSM8K (challenging multi-step problems, flat scaling curve), which meets these conditions. The performance gain is small for the subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which PaLM 540B already achieves performance of $90 \%$ or higher (and it is also generally true that there is less headroom for improvement when performance is already strong).</p>
<p>Although in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and symbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a "chain of thought" to solve (at least in principle). We leave the empirical evaluation of chain-of-thought prompting on such diverse tasks (e.g., machine translation, etc.) to future work.</p>
<h1>A. 4 Why is prompting with the equation only not enough for some arithmetic reasoning datasets?</h1>
<p>Prompting with the equation only as an intermediate step does help on many datasets, especially when the datasets only require a few reasoning steps (SVAMP, ASDiv, MAWPS). For GSM8K, however, using the equation only did not improve performance substantially. Based on qualitative analysis, we believe that these questions are too semantically challenging for the model to directly translate them into a math equation. Consider this example from LaMDA 137B:</p>
<p>Question: Mike plays ping pong for 40 minutes. In the first 20 minutes, he scores 4 points. In the second 20 minutes, he scores $25 \%$ more points. How many total points did he score?
Equation ONLY (WRONG ANSWER): $(4+20 * 0.25)=6$. The answer is 6 .
Chain of thought (CORRECT): Mike played ping pong for 40 minutes. In the first 20 minutes, he scored 4 points. In the second 20 minutes, he scored $25 \%$ more points. So he scored $25 \%$ more in the second 20 minutes. $4 \times 1.25=5$. So he scored 5 points in the second 20 minutes. So he scored 9 points in total. The answer is 9 .</p>
<p>It is hard for the model to directly translate all of the semantics into a single equation, but chain of thought allows it to better reason about each part of the question via intermediate steps in natural language.</p>
<h1>B All Experimental Results</h1>
<p>This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs. chain-of-thought prompting.
For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly. A similar observation was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external calculator (using the Python eval function) to all the equations in the generated chain of thought. When there are multiple equations in a chain of thought, we propagate the external calculator results from one equation to the following equations via string matching. As shown in Table 1, we see that adding a calculator significantly boosts performance of chain-of-thought prompting on most tasks.</p>
<p>Table 1: Chain of thought prompting outperforms standard prompting for various large language models on five arithmetic reasoning benchmarks. All metrics are accuracy (\%). Ext. calc.: post-hoc external calculator for arithmetic computations only. Prior best numbers are from the following. $a$ : Cobbe et al. (2021). $b \&amp; e$ : Pi et al. (2022), $c$ : Lan et al. (2021), $d$ : Piękos et al. (2021).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompting</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">MAWPS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prior best</td>
<td style="text-align: center;">N/A (finetuning)</td>
<td style="text-align: center;">$55^{a}$</td>
<td style="text-align: center;">$57.4^{b}$</td>
<td style="text-align: center;">$75.3^{c}$</td>
<td style="text-align: center;">$37.9^{d}$</td>
<td style="text-align: center;">$88.4^{e}$</td>
</tr>
<tr>
<td style="text-align: center;">UL2 20B</td>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chain of thought</td>
<td style="text-align: center;">$4.4(+0.3)$</td>
<td style="text-align: center;">$12.5(+2.4)$</td>
<td style="text-align: center;">$16.9(+0.9)$</td>
<td style="text-align: center;">$23.6(+3.1)$</td>
<td style="text-align: center;">$19.1(+2.5)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ ext. calc</td>
<td style="text-align: center;">6.9</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">23.6</td>
<td style="text-align: center;">42.7</td>
</tr>
<tr>
<td style="text-align: center;">LaMDA 137B</td>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">6.5</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chain of thought</td>
<td style="text-align: center;">$14.3(+7.8)$</td>
<td style="text-align: center;">$37.5(+8.0)$</td>
<td style="text-align: center;">$46.6(+6.5)$</td>
<td style="text-align: center;">$20.6(+4.9)$</td>
<td style="text-align: center;">$57.9(+14.7)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ ext. calc</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">69.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3 175B <br> (text-davinci-002)</td>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chain of thought</td>
<td style="text-align: center;">$46.9(+31.3)$</td>
<td style="text-align: center;">$68.9(+3.2)$</td>
<td style="text-align: center;">$71.3(+1.0)$</td>
<td style="text-align: center;">$35.8(+11.0)$</td>
<td style="text-align: center;">$87.1(+14.4)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ ext. calc</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">Codex <br> (code-davinci-002)</td>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chain of thought</td>
<td style="text-align: center;">$63.1(+43.4)$</td>
<td style="text-align: center;">$76.4(+6.5)$</td>
<td style="text-align: center;">$80.4(+6.4)$</td>
<td style="text-align: center;">$45.3(+15.8)$</td>
<td style="text-align: center;">$92.6(+13.9)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ ext. calc</td>
<td style="text-align: center;">65.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">93.3</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 540B</td>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">79.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Chain of thought</td>
<td style="text-align: center;">$56.9(+39.0)$</td>
<td style="text-align: center;">$79.0(+9.6)$</td>
<td style="text-align: center;">$73.9(+1.8)$</td>
<td style="text-align: center;">$35.8(+10.6)$</td>
<td style="text-align: center;">$93.3(+14.2)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ ext. calc</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">93.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Standard prompting versus chain of thought prompting on five arithmetic reasoning benchmarks. Note that chain of thought prompting is an emergent ability of model scale-it does not positively impact performance until used with a model of sufficient scale.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>GSM8K</th>
<th></th>
<th>SVAMP</th>
<th></th>
<th>ASDiv</th>
<th></th>
<th>AQuA</th>
<th></th>
<th>MAWPS</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
</tr>
<tr>
<td>UL2</td>
<td>20B</td>
<td>4.1</td>
<td>4.4</td>
<td>10.1</td>
<td>12.5</td>
<td>16.0</td>
<td>16.9</td>
<td>20.5</td>
<td>23.6</td>
<td>16.6</td>
<td>19.1</td>
</tr>
<tr>
<td>LaMDA</td>
<td>420M</td>
<td>2.6</td>
<td>0.4</td>
<td>2.5</td>
<td>1.6</td>
<td>3.2</td>
<td>0.8</td>
<td>23.5</td>
<td>8.3</td>
<td>3.2</td>
<td>0.9</td>
</tr>
<tr>
<td></td>
<td>2B</td>
<td>3.6</td>
<td>1.9</td>
<td>3.3</td>
<td>2.4</td>
<td>4.1</td>
<td>3.8</td>
<td>22.9</td>
<td>17.7</td>
<td>3.9</td>
<td>3.1</td>
</tr>
<tr>
<td></td>
<td>8B</td>
<td>3.2</td>
<td>1.6</td>
<td>4.3</td>
<td>3.4</td>
<td>5.9</td>
<td>5.0</td>
<td>22.8</td>
<td>18.6</td>
<td>5.3</td>
<td>4.8</td>
</tr>
<tr>
<td></td>
<td>68B</td>
<td>5.7</td>
<td>8.2</td>
<td>13.6</td>
<td>18.8</td>
<td>21.8</td>
<td>23.1</td>
<td>22.3</td>
<td>20.2</td>
<td>21.6</td>
<td>30.6</td>
</tr>
<tr>
<td></td>
<td>137B</td>
<td>6.5</td>
<td>14.3</td>
<td>29.5</td>
<td>37.5</td>
<td>40.1</td>
<td>46.6</td>
<td>25.5</td>
<td>20.6</td>
<td>43.2</td>
<td>57.9</td>
</tr>
<tr>
<td>GPT</td>
<td>350M</td>
<td>2.2</td>
<td>0.5</td>
<td>1.4</td>
<td>0.8</td>
<td>2.1</td>
<td>0.8</td>
<td>18.1</td>
<td>8.7</td>
<td>2.4</td>
<td>1.1</td>
</tr>
<tr>
<td></td>
<td>1.3B</td>
<td>2.4</td>
<td>0.5</td>
<td>1.5</td>
<td>1.7</td>
<td>2.6</td>
<td>1.4</td>
<td>12.6</td>
<td>4.3</td>
<td>3.1</td>
<td>1.7</td>
</tr>
<tr>
<td></td>
<td>6.7B</td>
<td>4.0</td>
<td>2.4</td>
<td>6.1</td>
<td>3.1</td>
<td>8.6</td>
<td>3.6</td>
<td>15.4</td>
<td>13.4</td>
<td>8.8</td>
<td>3.5</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>15.6</td>
<td>46.9</td>
<td>65.7</td>
<td>68.9</td>
<td>70.3</td>
<td>71.3</td>
<td>24.8</td>
<td>35.8</td>
<td>72.7</td>
<td>87.1</td>
</tr>
<tr>
<td>Codex</td>
<td>-</td>
<td>19.7</td>
<td>63.1</td>
<td>69.9</td>
<td>76.4</td>
<td>74.0</td>
<td>80.4</td>
<td>29.5</td>
<td>45.3</td>
<td>78.7</td>
<td>92.6</td>
</tr>
<tr>
<td>PaLM</td>
<td>8B</td>
<td>4.9</td>
<td>4.1</td>
<td>15.1</td>
<td>16.8</td>
<td>23.7</td>
<td>25.2</td>
<td>19.3</td>
<td>21.7</td>
<td>26.2</td>
<td>30.5</td>
</tr>
<tr>
<td></td>
<td>62B</td>
<td>9.6</td>
<td>29.9</td>
<td>48.2</td>
<td>46.7</td>
<td>58.7</td>
<td>61.9</td>
<td>25.6</td>
<td>22.4</td>
<td>61.8</td>
<td>80.3</td>
</tr>
<tr>
<td></td>
<td>540B</td>
<td>17.9</td>
<td>56.9</td>
<td>69.4</td>
<td>79.0</td>
<td>72.1</td>
<td>73.9</td>
<td>25.2</td>
<td>35.8</td>
<td>79.2</td>
<td>93.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Standard prompting versus chain of thought prompting on the four subsets of the MAWPS benchmark. The point of stratifying the MAWPS benchmark is to show that performance gains are minimal on easy one-step or two-step problems where large language models already achieve high performance (e.g., SingleOp, SingleEq, and AddSub).</p>
<table>
<thead>
<tr>
<th>Model</th>
<th></th>
<th>SingleOp</th>
<th></th>
<th>SingleEq</th>
<th></th>
<th>AddSub</th>
<th></th>
<th>MultiArith</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
<td>standard</td>
<td>CoT</td>
</tr>
<tr>
<td>UL2</td>
<td>20B</td>
<td>24.9</td>
<td>27.2</td>
<td>18.0</td>
<td>20.2</td>
<td>18.5</td>
<td>18.2</td>
<td>5.0</td>
<td>10.7</td>
</tr>
<tr>
<td>LaMDA</td>
<td>420M</td>
<td>2.8</td>
<td>1.0</td>
<td>2.4</td>
<td>0.4</td>
<td>1.9</td>
<td>0.7</td>
<td>5.8</td>
<td>1.5</td>
</tr>
<tr>
<td></td>
<td>2B</td>
<td>4.6</td>
<td>4.1</td>
<td>2.4</td>
<td>3.3</td>
<td>2.7</td>
<td>3.2</td>
<td>5.8</td>
<td>1.8</td>
</tr>
<tr>
<td></td>
<td>8B</td>
<td>8.0</td>
<td>7.0</td>
<td>4.5</td>
<td>4.4</td>
<td>3.4</td>
<td>5.2</td>
<td>5.2</td>
<td>2.4</td>
</tr>
<tr>
<td></td>
<td>68B</td>
<td>36.5</td>
<td>40.8</td>
<td>23.9</td>
<td>26.0</td>
<td>17.3</td>
<td>23.2</td>
<td>8.7</td>
<td>32.4</td>
</tr>
<tr>
<td></td>
<td>137B</td>
<td>73.2</td>
<td>76.2</td>
<td>48.8</td>
<td>58.7</td>
<td>43.0</td>
<td>51.9</td>
<td>7.6</td>
<td>44.9</td>
</tr>
<tr>
<td>GPT</td>
<td>350M</td>
<td>3.2</td>
<td>1.8</td>
<td>2.0</td>
<td>0.2</td>
<td>2.0</td>
<td>1.5</td>
<td>2.3</td>
<td>0.8</td>
</tr>
<tr>
<td></td>
<td>1.3B</td>
<td>5.3</td>
<td>3.0</td>
<td>2.4</td>
<td>1.6</td>
<td>2.3</td>
<td>1.5</td>
<td>2.2</td>
<td>0.5</td>
</tr>
<tr>
<td></td>
<td>6.7B</td>
<td>13.5</td>
<td>3.9</td>
<td>8.7</td>
<td>4.9</td>
<td>8.6</td>
<td>2.5</td>
<td>4.5</td>
<td>2.8</td>
</tr>
<tr>
<td></td>
<td>175B</td>
<td>90.9</td>
<td>88.8</td>
<td>82.7</td>
<td>86.6</td>
<td>83.3</td>
<td>81.3</td>
<td>33.8</td>
<td>91.7</td>
</tr>
<tr>
<td>Codex</td>
<td>-</td>
<td>93.1</td>
<td>91.8</td>
<td>86.8</td>
<td>93.1</td>
<td>90.9</td>
<td>89.1</td>
<td>44.0</td>
<td>96.2</td>
</tr>
<tr>
<td>PaLM</td>
<td>8B</td>
<td>41.8</td>
<td>46.6</td>
<td>29.5</td>
<td>28.2</td>
<td>29.4</td>
<td>31.4</td>
<td>4.2</td>
<td>15.8</td>
</tr>
<tr>
<td></td>
<td>62B</td>
<td>87.9</td>
<td>85.6</td>
<td>77.2</td>
<td>83.5</td>
<td>74.7</td>
<td>78.2</td>
<td>7.3</td>
<td>73.7</td>
</tr>
<tr>
<td></td>
<td>540B</td>
<td>94.1</td>
<td>94.1</td>
<td>86.5</td>
<td>92.3</td>
<td>93.9</td>
<td>91.9</td>
<td>42.2</td>
<td>94.7</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We tested 10 common names using GPT-3 davinci and it got all but one correct.
${ }^{4}$ For names of length longer than 2 words, we concatenate multiple first and last names together.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
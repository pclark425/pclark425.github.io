<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4754 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4754</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4754</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-266573579</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.16702v1.pdf" target="_blank">Rethinking Tabular Data Understanding with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4754.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4754.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct Prompting (textual chain-of-thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot textual reasoning method where GPT-3.5 is prompted to 'think step by step' (chain-of-thought) over a linearized table and produce a final answer directly in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613 / gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 series accessed via OpenAI API (gpt-3.5-turbo variants); used in a black-box zero-shot setting for table question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Direct Prompting (Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-shot chain-of-thought prompting: the model is asked to 'think step by step' and then give a single final answer in a prescribed format. Repeated runs can be aggregated but the base method is a single prompt per attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table question answering benchmark where a natural language question must be answered using a semi-structured Wikipedia table.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy (single attempt) 58.66% (sampled WTQ, with NORM preprocessing reported); with self-consistency (10 outputs) DP achieves ~64.84% (sampled WTQ); DP + Mix Self-Consistency contributes to combined methods achieving 72.40% (sampled) / 73.6% (full WTQ with NORM).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Symbolic PyAgent single attempt 56.87%; PyAgent with SC ~63.49%. Mix Self-Consistency (5 DP + 5 PyAgent) = 72.40% (sampled); Self-Evaluation selection approach ~64.99%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Textual (DP) slightly outperforms symbolic (PyAgent) in single-shot settings on WTQ and is more resilient to structural perturbations; however, both methods benefit substantially from aggregation (self-consistency) and from mixing outputs across reasoning styles (Mix Self-Consistency yields the largest gains and SOTA on WTQ).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DP suffers heavily from table misinterpretation errors (42% of DP errors) and degrades on long tables; DP's single-shot performance is inferior to aggregated/diverse-output variants (self-consistency or mix). In some cases aggregation with only two diverse paths (self-evaluation) matched performance of many-path self-consistency, showing diverse outputs are not always strictly necessary to match gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4754.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4754.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Python Shell Agent (symbolic reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive symbolic reasoning approach where GPT-3.5 generates code (pandas in a python_repl_ast tool) to query and compute answers from the table, using the Python execution environment as an external symbolic tool.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0613 / gpt-3.5-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 series accessed via OpenAI API; employed as a code-generating agent that interacts with a Python REPL to perform symbolic operations on table data.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Python Agent (symbolic/tool-augmented reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single interactive agent run: model generates python code to manipulate a dataframe (df) and derive answers; the method relies on precise table structure and code correctness rather than direct natural-language chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table QA requiring selection/aggregation/localization of cell values from Wikipedia tables.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Accuracy (single attempt) 56.87% (sampled WTQ); with self-consistency (10 outputs) PyAgent achieves ~63.49% (sampled WTQ). Partial-table PyAgent (omitting central rows) = 52.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Direct Prompting single attempt 58.66%; DP with SC ~64.84%; Mix Self-Consistency (5 DP + 5 PyAgent) = 72.40% (sampled).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic reasoning via code generation is competitive but slightly worse than DP in single-shot accuracy; it is more vulnerable to table structural perturbations (especially transposition) and prone to coding/execution errors (38% of PyAgent errors), yet excels at tasks requiring exact computation (counting, column localization) and can handle larger tables by requesting/processing partial views.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PyAgent can fail due to coding bugs, non-observable actions, or misinterpreting special rows; it is significantly impacted by table transposition (large drops) and overall more brittle to structural changes than textual reasoning. In some counting/localization problems PyAgent outperforms DP, indicating similar/diverse methods are task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4754.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4754.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (multi-sample aggregation of chain-of-thought outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An aggregation method that samples multiple reasoning traces/answers from the LLM and selects the most frequent final answer (majority voting) to improve robustness of chain-of-thought reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-3.5 models; multiple stochastic generations (temperature sampling) are collected and aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (multi-sample majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Generate multiple (e.g., 10) independent reasoning outputs by sampling the LLM (introducing diversity via randomness/temperature), then perform majority voting on final answers; diversity arises from sampling variation across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table QA benchmark (see above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>DP with SC (10 outputs): ~64.84% (sampled WTQ); PyAgent with SC (10 outputs): ~63.49% (sampled WTQ). Reported improvements vs. single-shot: DP from 58.66% -> ~64.84%; PyAgent from 56.87% -> ~63.49%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-shot DP 58.66% and single-shot PyAgent 56.87%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling multiple reasoning traces and aggregating (diverse approach) improves accuracy for both textual and symbolic methods; self-consistency provides substantial gains over single-shot (similar) runs.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Aggregation yields variability and ties requiring tie-breaking; a carefully designed selection (self-evaluation) with only two paths sometimes matches the benefit of many-path SC, showing that many samples are not the only way to achieve gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4754.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4754.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mix-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mix Self-Consistency (mixed-path aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid aggregation strategy that generates multiple outputs from different reasoning methods (textual DP and symbolic PyAgent) and then applies majority voting across the pooled set of outputs to exploit complementary strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-3.5 used to produce multiple outputs per reasoning type; hyperparameter sets number of outputs per method (e.g., 5 DP + 5 PyAgent).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Mix Self-Consistency (mixed-method ensemble + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Produce a predetermined number of sampled outputs from each reasoning style (e.g., 5 textual, 5 symbolic), aggregate all outputs, and take the majority-voted answer; diversity arises both from sampling and from using distinct reasoning procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ); TabFact (subsample)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>WTQ: table QA; TabFact: table-based fact verification/claim checking.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Mix Self-Consistency (5 DP + 5 PyAgent) sampled WTQ: 72.40% (sampled set); full WTQ with NORM + Mix-SC: 73.6% (SOTA reported). On a 500-subset of TabFact, Mix-SC achieved 0.885 accuracy, slightly outperforming baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Best single-method SC: DP w/ SC ~64.84%, PyAgent w/ SC ~63.49%; DP single 58.66%, PyAgent single 56.87%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mixing reasoning methods and aggregating diverse outputs yields the largest gains: Mix-SC substantially outperforms both single-method SC and single-shot methods, achieving SOTA on WTQ (73.6%) in zero-shot setting; diversity across method types is particularly beneficial because textual and symbolic errors are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Ablation shows the split of outputs is a hyperparameter: 5+5 gave best min/avg, while 4+6 achieved highest maximum in tests. The method relies on majority voting and can still be sensitive to correlated errors; also DP-derived answers were prioritized in their experiments due to DP's slightly higher base performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4754.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4754.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Eval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation-driven selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decision/prompting prompt that asks the LLM to compare two candidate answers (from different methods) and select which is more likely correct based on question nature and each method's strengths, without directly validating against the table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used to perform meta-evaluation between two candidate answers (one from chain-of-thought DP and one from PyAgent).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Evaluation (meta-decision)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Given two candidate answers (Answer A from DP, Answer B from PyAgent), the model follows a structured evaluation rubric (preliminary evaluation, nature of question, final verdict) to select which answer is likely correct; leverages knowledge of method strengths (e.g., Python for counting) to decide.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Table QA benchmark; same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Self-Evaluation selection approach improves accuracy to ~64.99% (text reports) / reported table value ~64.22%; it matched performance of using 10-path SC for a single method in their experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>DP with 10-path SC ~64.84%; PyAgent with 10-path SC ~63.49%; Mix-SC (5+5) = 72.40%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A lightweight meta-evaluation that selects between two reasoning methods can approximate gains from many-path self-consistency while requiring far fewer LLM calls; it leverages complementary strengths and reduces bias toward one method.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Reported table entries differ slightly (64.22% vs 64.99% in different text passages); while self-evaluation can match some multi-sample SC results, it does not reach Mix-SC SOTA and depends on reliable meta-judgment by the LLM (which can itself err).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4754.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4754.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Table Structure Normalization (NORM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage preprocessing strategy that detects whether a table is a column-table and, if needed, transposes it to a row-table, then optionally reorders rows to produce a well-ordered row-table to reduce structural variance for downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (used as a detector/suggester within NORM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used GPT-3.5 for content-aware transposition determination and row-reordering suggestions as a preprocessing step.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Content-aware transposition + optional resorting (NORM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>other</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Use the model to inspect the first row vs first column and determine headings (to decide transpose), then encourage a readable row ordering. Aims to neutralize structural perturbations (transpose/row-shuffle) prior to reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WikiTableQuestions (WTQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Preprocessing to improve downstream table QA robustness under structural perturbations (transposition, row-shuffle).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Applying NORM before DP and PyAgent mitigates accuracy drops under perturbations; example: NORM + DP keeps/recovers performance close to original (DP single attempt with NORM 58.66% reported), and in aggregate pipelines NORM + Mix-SC led to full WTQ accuracy 73.6% (SOTA). On table-transposition detection tasks, content-aware determinator achieved 97.39% (original) and 94.77% (transposed) for heading choice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Without NORM DP/PyAgent performance degrades substantially under perturbations (see Table 3). Mix-SC without NORM still improves but NORM + Mix-SC gives best full-test results (73.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Normalizing table structure removes a major source of brittleness for both textual and symbolic reasoning; NORM restores perturbed-table accuracies to near-original levels and is an important enabler for aggregation methods to reach high performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The resorting stage of NORM can change answers for sequence-dependent questions (resorting can alter row-index-based answers), so in full evaluation they occasionally skip resorting; NORM decisions (and resorting) can introduce errors if not applied carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rethinking Tabular Data Understanding with Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>StructGPT: A general framework for large language model to reason on structured data <em>(Rating: 2)</em></li>
                <li>Binding language models in symbolic languages <em>(Rating: 2)</em></li>
                <li>Augmented language models: a survey <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4754",
    "paper_id": "paper-266573579",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "DP",
            "name_full": "Direct Prompting (textual chain-of-thought)",
            "brief_description": "A zero-shot textual reasoning method where GPT-3.5 is prompted to 'think step by step' (chain-of-thought) over a linearized table and produce a final answer directly in natural language.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0613 / gpt-3.5-turbo-16k-0613)",
            "model_description": "GPT-3.5 series accessed via OpenAI API (gpt-3.5-turbo variants); used in a black-box zero-shot setting for table question answering.",
            "reasoning_method_name": "Direct Prompting (Chain-of-Thought)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single-shot chain-of-thought prompting: the model is asked to 'think step by step' and then give a single final answer in a prescribed format. Repeated runs can be aggregated but the base method is a single prompt per attempt.",
            "task_name": "WikiTableQuestions (WTQ)",
            "task_description": "Table question answering benchmark where a natural language question must be answered using a semi-structured Wikipedia table.",
            "performance": "Accuracy (single attempt) 58.66% (sampled WTQ, with NORM preprocessing reported); with self-consistency (10 outputs) DP achieves ~64.84% (sampled WTQ); DP + Mix Self-Consistency contributes to combined methods achieving 72.40% (sampled) / 73.6% (full WTQ with NORM).",
            "comparison_with_other_method": true,
            "performance_other_method": "Symbolic PyAgent single attempt 56.87%; PyAgent with SC ~63.49%. Mix Self-Consistency (5 DP + 5 PyAgent) = 72.40% (sampled); Self-Evaluation selection approach ~64.99%.",
            "key_findings": "Textual (DP) slightly outperforms symbolic (PyAgent) in single-shot settings on WTQ and is more resilient to structural perturbations; however, both methods benefit substantially from aggregation (self-consistency) and from mixing outputs across reasoning styles (Mix Self-Consistency yields the largest gains and SOTA on WTQ).",
            "counter_examples_or_negative_results": "DP suffers heavily from table misinterpretation errors (42% of DP errors) and degrades on long tables; DP's single-shot performance is inferior to aggregated/diverse-output variants (self-consistency or mix). In some cases aggregation with only two diverse paths (self-evaluation) matched performance of many-path self-consistency, showing diverse outputs are not always strictly necessary to match gains.",
            "uuid": "e4754.0",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "PyAgent",
            "name_full": "Python Shell Agent (symbolic reasoning)",
            "brief_description": "An interactive symbolic reasoning approach where GPT-3.5 generates code (pandas in a python_repl_ast tool) to query and compute answers from the table, using the Python execution environment as an external symbolic tool.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0613 / gpt-3.5-turbo-16k-0613)",
            "model_description": "GPT-3.5 series accessed via OpenAI API; employed as a code-generating agent that interacts with a Python REPL to perform symbolic operations on table data.",
            "reasoning_method_name": "Python Agent (symbolic/tool-augmented reasoning)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Single interactive agent run: model generates python code to manipulate a dataframe (df) and derive answers; the method relies on precise table structure and code correctness rather than direct natural-language chain-of-thought.",
            "task_name": "WikiTableQuestions (WTQ)",
            "task_description": "Table QA requiring selection/aggregation/localization of cell values from Wikipedia tables.",
            "performance": "Accuracy (single attempt) 56.87% (sampled WTQ); with self-consistency (10 outputs) PyAgent achieves ~63.49% (sampled WTQ). Partial-table PyAgent (omitting central rows) = 52.45%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Direct Prompting single attempt 58.66%; DP with SC ~64.84%; Mix Self-Consistency (5 DP + 5 PyAgent) = 72.40% (sampled).",
            "key_findings": "Symbolic reasoning via code generation is competitive but slightly worse than DP in single-shot accuracy; it is more vulnerable to table structural perturbations (especially transposition) and prone to coding/execution errors (38% of PyAgent errors), yet excels at tasks requiring exact computation (counting, column localization) and can handle larger tables by requesting/processing partial views.",
            "counter_examples_or_negative_results": "PyAgent can fail due to coding bugs, non-observable actions, or misinterpreting special rows; it is significantly impacted by table transposition (large drops) and overall more brittle to structural changes than textual reasoning. In some counting/localization problems PyAgent outperforms DP, indicating similar/diverse methods are task-dependent.",
            "uuid": "e4754.1",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency (multi-sample aggregation of chain-of-thought outputs)",
            "brief_description": "An aggregation method that samples multiple reasoning traces/answers from the LLM and selects the most frequent final answer (majority voting) to improve robustness of chain-of-thought reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo variants)",
            "model_description": "Same GPT-3.5 models; multiple stochastic generations (temperature sampling) are collected and aggregated.",
            "reasoning_method_name": "Self-Consistency (multi-sample majority voting)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Generate multiple (e.g., 10) independent reasoning outputs by sampling the LLM (introducing diversity via randomness/temperature), then perform majority voting on final answers; diversity arises from sampling variation across runs.",
            "task_name": "WikiTableQuestions (WTQ)",
            "task_description": "Table QA benchmark (see above).",
            "performance": "DP with SC (10 outputs): ~64.84% (sampled WTQ); PyAgent with SC (10 outputs): ~63.49% (sampled WTQ). Reported improvements vs. single-shot: DP from 58.66% -&gt; ~64.84%; PyAgent from 56.87% -&gt; ~63.49%.",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-shot DP 58.66% and single-shot PyAgent 56.87%.",
            "key_findings": "Sampling multiple reasoning traces and aggregating (diverse approach) improves accuracy for both textual and symbolic methods; self-consistency provides substantial gains over single-shot (similar) runs.",
            "counter_examples_or_negative_results": "Aggregation yields variability and ties requiring tie-breaking; a carefully designed selection (self-evaluation) with only two paths sometimes matches the benefit of many-path SC, showing that many samples are not the only way to achieve gains.",
            "uuid": "e4754.2",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Mix-SC",
            "name_full": "Mix Self-Consistency (mixed-path aggregation)",
            "brief_description": "A hybrid aggregation strategy that generates multiple outputs from different reasoning methods (textual DP and symbolic PyAgent) and then applies majority voting across the pooled set of outputs to exploit complementary strengths.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo variants)",
            "model_description": "Same GPT-3.5 used to produce multiple outputs per reasoning type; hyperparameter sets number of outputs per method (e.g., 5 DP + 5 PyAgent).",
            "reasoning_method_name": "Mix Self-Consistency (mixed-method ensemble + majority vote)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Produce a predetermined number of sampled outputs from each reasoning style (e.g., 5 textual, 5 symbolic), aggregate all outputs, and take the majority-voted answer; diversity arises both from sampling and from using distinct reasoning procedures.",
            "task_name": "WikiTableQuestions (WTQ); TabFact (subsample)",
            "task_description": "WTQ: table QA; TabFact: table-based fact verification/claim checking.",
            "performance": "Mix Self-Consistency (5 DP + 5 PyAgent) sampled WTQ: 72.40% (sampled set); full WTQ with NORM + Mix-SC: 73.6% (SOTA reported). On a 500-subset of TabFact, Mix-SC achieved 0.885 accuracy, slightly outperforming baselines.",
            "comparison_with_other_method": true,
            "performance_other_method": "Best single-method SC: DP w/ SC ~64.84%, PyAgent w/ SC ~63.49%; DP single 58.66%, PyAgent single 56.87%.",
            "key_findings": "Mixing reasoning methods and aggregating diverse outputs yields the largest gains: Mix-SC substantially outperforms both single-method SC and single-shot methods, achieving SOTA on WTQ (73.6%) in zero-shot setting; diversity across method types is particularly beneficial because textual and symbolic errors are complementary.",
            "counter_examples_or_negative_results": "Ablation shows the split of outputs is a hyperparameter: 5+5 gave best min/avg, while 4+6 achieved highest maximum in tests. The method relies on majority voting and can still be sensitive to correlated errors; also DP-derived answers were prioritized in their experiments due to DP's slightly higher base performance.",
            "uuid": "e4754.3",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-Eval",
            "name_full": "Self-Evaluation-driven selection",
            "brief_description": "A decision/prompting prompt that asks the LLM to compare two candidate answers (from different methods) and select which is more likely correct based on question nature and each method's strengths, without directly validating against the table.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo variants)",
            "model_description": "GPT-3.5 used to perform meta-evaluation between two candidate answers (one from chain-of-thought DP and one from PyAgent).",
            "reasoning_method_name": "Self-Evaluation (meta-decision)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Given two candidate answers (Answer A from DP, Answer B from PyAgent), the model follows a structured evaluation rubric (preliminary evaluation, nature of question, final verdict) to select which answer is likely correct; leverages knowledge of method strengths (e.g., Python for counting) to decide.",
            "task_name": "WikiTableQuestions (WTQ)",
            "task_description": "Table QA benchmark; same as above.",
            "performance": "Self-Evaluation selection approach improves accuracy to ~64.99% (text reports) / reported table value ~64.22%; it matched performance of using 10-path SC for a single method in their experiments.",
            "comparison_with_other_method": true,
            "performance_other_method": "DP with 10-path SC ~64.84%; PyAgent with 10-path SC ~63.49%; Mix-SC (5+5) = 72.40%.",
            "key_findings": "A lightweight meta-evaluation that selects between two reasoning methods can approximate gains from many-path self-consistency while requiring far fewer LLM calls; it leverages complementary strengths and reduces bias toward one method.",
            "counter_examples_or_negative_results": "Reported table entries differ slightly (64.22% vs 64.99% in different text passages); while self-evaluation can match some multi-sample SC results, it does not reach Mix-SC SOTA and depends on reliable meta-judgment by the LLM (which can itself err).",
            "uuid": "e4754.4",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "NORM",
            "name_full": "Table Structure Normalization (NORM)",
            "brief_description": "A two-stage preprocessing strategy that detects whether a table is a column-table and, if needed, transposes it to a row-table, then optionally reorders rows to produce a well-ordered row-table to reduce structural variance for downstream reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (used as a detector/suggester within NORM)",
            "model_description": "Used GPT-3.5 for content-aware transposition determination and row-reordering suggestions as a preprocessing step.",
            "reasoning_method_name": "Content-aware transposition + optional resorting (NORM)",
            "reasoning_method_type": "other",
            "reasoning_method_description": "Use the model to inspect the first row vs first column and determine headings (to decide transpose), then encourage a readable row ordering. Aims to neutralize structural perturbations (transpose/row-shuffle) prior to reasoning.",
            "task_name": "WikiTableQuestions (WTQ)",
            "task_description": "Preprocessing to improve downstream table QA robustness under structural perturbations (transposition, row-shuffle).",
            "performance": "Applying NORM before DP and PyAgent mitigates accuracy drops under perturbations; example: NORM + DP keeps/recovers performance close to original (DP single attempt with NORM 58.66% reported), and in aggregate pipelines NORM + Mix-SC led to full WTQ accuracy 73.6% (SOTA). On table-transposition detection tasks, content-aware determinator achieved 97.39% (original) and 94.77% (transposed) for heading choice.",
            "comparison_with_other_method": true,
            "performance_other_method": "Without NORM DP/PyAgent performance degrades substantially under perturbations (see Table 3). Mix-SC without NORM still improves but NORM + Mix-SC gives best full-test results (73.6%).",
            "key_findings": "Normalizing table structure removes a major source of brittleness for both textual and symbolic reasoning; NORM restores perturbed-table accuracies to near-original levels and is an important enabler for aggregation methods to reach high performance.",
            "counter_examples_or_negative_results": "The resorting stage of NORM can change answers for sequence-dependent questions (resorting can alter row-index-based answers), so in full evaluation they occasionally skip resorting; NORM decisions (and resorting) can introduce errors if not applied carefully.",
            "uuid": "e4754.5",
            "source_info": {
                "paper_title": "Rethinking Tabular Data Understanding with Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "StructGPT: A general framework for large language model to reason on structured data",
            "rating": 2,
            "sanitized_title": "structgpt_a_general_framework_for_large_language_model_to_reason_on_structured_data"
        },
        {
            "paper_title": "Binding language models in symbolic languages",
            "rating": 2,
            "sanitized_title": "binding_language_models_in_symbolic_languages"
        },
        {
            "paper_title": "Augmented language models: a survey",
            "rating": 1,
            "sanitized_title": "augmented_language_models_a_survey"
        }
    ],
    "cost": 0.015928499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Rethinking Tabular Data Understanding with Large Language Models
27 Dec 2023</p>
<p>Tianyang Liu 
Fei Wang 
Muhao Chen muhchen@ucdavis.edu </p>
<p>UC San Diego</p>
<p>UC Davis</p>
<p>Rethinking Tabular Data Understanding with Large Language Models
27 Dec 20236FA253D8C214663F2C169779AF9D356BarXiv:2312.16702v1[cs.CL]
Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area.In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways.We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks.This prompts the proposal of a method for table structure normalization.Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks.Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on WIKITABLEQUESTIONS, representing a substantial advancement over previous existing table processing paradigms of LLMs.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs; Brown et al. 2020;Chowdhery et al. 2022;Zhang et al. 2022;Ope-nAI 2022, 2023a,c;Touvron et al. 2023a,b) have revolutionized the field of NLP, demonstrating an extraordinary ability to understand and reason over rich textual data (Wei et al., 2023;Wang et al., 2023;Zhou et al., 2023;Kojima et al., 2023;Li et al., 2023b).On top of LLMs' existing capabilities for NLP, further bolstering their potential for decision-making by drawing from external knowledge sources remains an exciting research frontier (Nakano et al., 2022;Mialon et al., 2023;Hao et al., 2023;Jiang et al., 2023b).Amongst such knowledge sources, tabular data serve as a ubiqui-Figure 1: A demonstration of the challenges faced by LLMs in comprehending and interpreting table structures.In the first example, despite the LLM correctly identifying table headings, it falters in accurately determining the headings' positions within the table structure.In the second example, the model using Python Shell as an external tool erroneously defaults to interpreting headings (located in first column) as column headers, leading to subsequent mistakes in the generated code.Some logos in this and subsequent figures are generated by OpenAI's DALL-E3 (OpenAI, 2023b).</p>
<p>tous kind due to their expressiveness for relations, properties and statistics, and their being easy to construct by human curators.</p>
<p>Like humans, LLMs can also benefit from reading tabular data accompanying text.However, as indicated in Fig. 1, the structural nature of tables presents unique challenges to these models.Inherently designed to parse and process vast expanses of unstructured textual content, LLMs confront a paradigm shift when facing tabular data.Linearizing tables to suit the LLM paradigm can obscure Figure 2: Illustrative examples sampled from the WIKITABLEQUESTIONS dataset, wherein a comparison is exhibited between textual reasoning via direct prompting and symbolic reasoning through Python Shell interactions.Top: The table and its title.Bottom Left: The first question example where textual reasoning erroneously interprets due to the limitation of precision localization, while symbolic reasoning accurately locates the answer with Python code.Bottom Right: The second question example where textual reasoning successfully identifies the answer, but symbolic reasoning mistakenly treats the special row total row as the final answer.</p>
<p>the inherent structural and relational information, making tasks such as precise localization and complex statistical analyses.Additionally, the design variations in tables, whether 'column tables' with headers in the first row or 'row tables' with headers in the first column, further complicate the interpretation process.Beyond structural concerns, numerical reasoning and aggregation over tabular data present another layer of complexity.While LLMs excel at textual understanding, they occasionally stumble when confronted with tasks necessitating precise numerical computation within tables.Moreover, tables often present a dense amalgamation of textual or numerical data.The sheer volume and intricacy of this information can risk overshadowing crucial details, potentially impeding the LLM's decision-making abilities (Shi et al., 2023).</p>
<p>With the emergence of instruction fine-tuning techniques (Wei et al., 2022;Chung et al., 2022) and the application of Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2022;Gao et al., 2022;Christiano et al., 2017), LLMs have witnessed significant enhancements in their alignment capabilities, paving the way for transitioning from few-shot to zero-shot learning settings (Kojima et al., 2023).In light of these advancements, this paper delves deep into the the challenges and intricacies of tabular understanding and reasoning by LLMs, exemplified in Fig. 2. We organize our exploration around three pivotal re-search questions: (1) How well do LLMs perceive table structures and how can we ensure robustness against structural variations?(2) Comparing textual and symbolic reasoning for table data in LLMs, which prevails in effectiveness, and what advantages and challenges manifest in each strategy?</p>
<p>(3) Will the aggregation of multiple reasoning pathways enhance the accuracy and reliability of tabular data interpretation by LLMs?</p>
<p>In pursuit of answering the aforementioned research questions, we conduct experiments on SOTA LLMs such as GPT-3.5 (OpenAI, 2023a).Our findings in 4 underscore that while LLMs are adept at semantically interpreting tables, their capability to resist structural variance ( 4.1) and understand table structures ( 4.2) is suboptimal.Motivated by these findings, we propose a table structure normalization method to enhance LLMs' resilience against structural table variations in 4.3.Intriguingly, 5.1 reveals that textual reasoning surpasses symbolic reasoning in contexts with limited table content, defying conventional conceptions of symbolic reasoning's dominance in other domains (Mialon et al., 2023).Both textual and symbolic reasoning strategies encompass different advantages and challenges, which is detailed in 5.2.To harness the unique strengths of each, we implement mix self-consistency mechanism ( 6) that remarkably attains SOTA performance on Table QA, exemplifying the synergistic potential when both rea-soning strategies are aggregated.</p>
<p>Related Work</p>
<p>PLMs for Tabular Data Processing.Tabular reasoning presents unique challenges due to the fusion of free-form natural language questions with structured or semi-structured tabular data, for which PLMs jointly trained on tables and text are developed in the past few years, including TaBERT (Yin et al., 2020), TaPas (Herzig et al., 2020), TAPEX (Liu et al., 2022), ReasTAP (Zhao et al., 2022), and PASTA (Gu et al., 2022).Despite these advancements, recent studies have identified generalization issues under table perturbations (Zhao et al., 2023;Chang et al., 2023), raising concerns regarding the robustness of PLMs.Specific efforts like LETA (Zhao et al., 2023) and LATTICE (Wang et al., 2022) have investigated and mitigated the vulnerabilities related to structural perturbations of tabular data, like row/column shuffling and table transpose, through various techniques, including data augmentation and order-invariant graph attention.However, these approaches require whitebox access to the models, limiting their applicability to SOTA LLMs with only blackbox accessibility, a limitation directly addressed in this work.</p>
<p>Tabular Data Processing with LLMs.Recent advancements in LLMs, notably within few-shot learning, have demonstrated their potential for tabular reasoning.Chen (2023) leveraged the Chain-of-Thought (CoT) technique (Wei et al., 2023) to illustrate LLMs' effectiveness in this domain.Building upon CoT, Cheng et al. (2023a) and Ye et al. (2023) introduced frameworks that incorporate symbolic reasoning for improved comprehension, with Ye et al. emphasizing their ability to adeptly decompose both evidence and questions.The advent of aligned models, such as ChatGPT, has enabled zeroshot table reasoning.However, these models often lack sensitivity to table structures, struggling with structural perturbations.StructGPT (Jiang et al., 2023a), while introducing a promising framework for LLMs to efficiently engage with structured data, has its effectiveness limited by not integrating symbolic reasoning, a critical aspect for enhancing the full capabilities of LLMs in tabular reasoning, which is the focal point of this study.Furthermore, while programming-based approaches can mitigate some challenges, they are limited in addressing free-form queries, creating a gap in the landscape.Innovations like AutoGPT (Significant Gravitas, 2023) have sought to address this, spawning the development of tabular agents like LangChain (Chase, 2022), SheetCopilot (Li et al., 2023a), and Data-Copilot (Zhang et al., 2023).These agents offer solutions unattainable through conventional programming but still require rigorous evaluation in various scenarios.In our study, we delve into addressing these challenges for enhancing LLMs' reasoning capabilities within structural perturbations, hence providing insights that facilitate improved accuracy in the current context.</p>
<p>Preliminaries</p>
<p>This section succinctly introduces the foundational aspects of our study over structurally perturbed tabular data.3.1 formally defines the problem, delineating the critical notations and conceptual frameworks, and 3.2 explicates our experimental setup details, elucidating dataset choice, model utilization, and evaluation strategy.</p>
<p>Problem Definition</p>
<p>Question answering (QA) over tabular data, commonly known as the TableQA task, is an important challenge in NLP.In this study, we targets TableQA to explore and enhance the proficiency of LLMs, in reasoning over tabular data.Additionally, we probe the robustness and adaptability of these models by introducing structural perturbations to tables.</p>
<p>Let T represent a table consisting of R rows and C columns, and  represent its title/caption.Each cell in T is denoted by T i,j , where i  [0, R  1] and j  [0, C  1].T 0,j are headers.Given a question Q pertaining to the table, our task is to identify an answer A. This answer is generally a collection of values, denoted as {a 1 , a 2 , . . ., a k }, where k  N + .</p>
<p>Furthermore, to delve deeper into the structural comprehension of LLMs, we introduce structural perturbations, which include:
1 1. Transposed Table (T  ): A table obtained
by converting rows to columns and vice-versa, maintaining the row and column order:
T  i,j = T j,i i  [0, R  1], j  [0, C  1].</p>
<p>Row Shuffled Table (T  ):</p>
<p>A table obtained by randomly shuffling the rows (excluding the headers) with a random permutation function , while keeping the order of columns unchanged:
T  i,j = T (i),j i  [1, R  1], j  [0, C  1]
3. Row Shuffled and Transposed Table (T   ): A table obtained by first randomly shuffling the rows (excluding headers) and then applying transposition:
T   i,j = T j,(i) i  [1, R  1], j  [0, C  1]
Defining our research problem more formally: our primary objective is to investigate the function, f , that can appropriately answer the posed question using the provided table.Specifically, this function will take three arguments: the table variant T   {T , T  , T  , T   }, its title  , and the question Q.It will output an answer A. The entire problem can be formally framed as:
f (T  , , Q)  A, T   {T , T  , T  , T   }</p>
<p>Experimental Setup</p>
<p>This section details the experimental setup adopted in our study, including the datasets employed, model selection, evaluation metrics, reasoning methods, and other details.</p>
<p>Dataset.We used the WIKITABLEQUESTIONS (WTQ; Pasupat and Liang 2015) dataset for our experiments.Models.We employ the GPT-3.5 (OpenAI, 2023a) series for our research.Given that tables usually have extensive data, depending on the prompt length, we dynamically use gpt-3.5-turbo-0613and gpt-3.5-turbo-16k-0613,with a primary aim to optimize cost when querying the API.</p>
<p>Evaluation Metrics.Following prior works (Jiang et al., 2022;Ni et al., 2023;Cheng et al., 2023b;Ye et al., 2023), we employ Exact Match Accuracy as the evaluation metric to validate predictions against ground truths, embedding instructions in prompts for consistent and parseable outputs.</p>
<p>Reasoning Methods.Our evaluation hinges on two distinct zero-shot reasoning approaches:</p>
<p>LLM Robustness to Structural Perturbations</p>
<p>This section explores how LLMs interpret varied table structures in response to our first research question ( 1).We probe the impact of three table perturbations on LLM performance ( 4.1), uncover LLMs' challenges and limitations for direct table transposition and recoganize tranposed tables ( 4.2), and introduce a structure normalization strategy (NORM) to mitigate these issues ( 4.3).(T ), and their combination (T  ).As demonstrated in Tab. 1, both reasoning methods, DP and PyAgent, exhibit significant performance declines, with more pronounced when transposition is applied.DP consistently outperforms PyAgent largely across perturbations, indicating that textual reasoning tends to be more resilient to these structural changes.This resilience can be attributed to LLMs' ability to grasp semantic connections and meanings irrespective of structural shifts.In contrast, symbolic reasoning, exemplified by PyAgent, is heavily reliant on table structure, making it more vulnerable, especially to transposition.</p>
<p>Impacts of</p>
<p>Limitations of Table Transposition with LLMs</p>
<p>To better understand LLMs' capabilities with regards to table structures, we investigate their ability on detecting tables in need of transposition and performing table transposition.</p>
<p>LLMs as Transposition Detectors. Given a table</p>
<p>T , the goal is to detect whether a table should be transposed for better comprehension by LLMs.This is formulated as a binary classification task:
f (T )  0, f (T  )  1,
Where 0 denotes 'no need of transposition' and 1 indicates 'transposition needed'.Tab. 2 shows the results using the prompt in Appx.A.4.GPT-3.5 correctly classified 93.35% of original tables T as not requiring transposition.However, its accuracy dramatically decreased to 32.54% on transposed tables T  .Our observations highlight that LLMs suffer from structural bias in the interpretation of table orientations, predominantly leading to recommendations against transposition.</p>
<p>LLMs as Table Transposers.The objective is to switch between original and transposed table formats.Specifically, the goal is to directly yield T  given T , and vice versa.Formally, the task is:
f (T )  T  , f (T  )  T
We observed that GPT-3.5'sproficiency in this task is limited, with an accuracy of 53.68% transposing row tables and 51.07%for the inverse operation, suggesting that LLMs can not transpose tables precisely.For a detailed error case study and further analysis, refer to the Appx.B.</p>
<p>Table Structure Normalization</p>
<p>In addressing structural variations in tables, our goal is to ensure consistent interpretation and utility across diverse table structures.To normalize various table structures into well-ordered rowtables prior to downstream tasks, we introduce NORM, which is a two-stage normalization strategy: the first stage detects column-tables and transposing them into row-tables, while the second stage sorts the row-tables for enhanced comprehensibility.Through this approach, NORM accommodates for structural perturbations without compromising the understanding of the standardized row-tables.</p>
<p>Content-Aware Transposition Determination</p>
<p>In the straightforward methods mentioned in 4.2, LLMs are affected by the loss of structure information of the table.Our approach aims to reduce this structural dependence by introducing a contentaware determination process, which leverages the semantic reasoning capabilities of LLMs, instead of perceiving the table's structure.Specifically, we analyze the inherent content within the first row (T 0, * ) and the first column (T * ,0 ) of a given table (T ) to decide which is more semantically fitting to serve as the table's heading.This content-aware approach can be mathematically modeled as:
f (T , T 0, * , T * ,0 )  T 0, * f (T  , T 0, * , T * ,0 )  T * ,0
Here, a selection of the first row suggests that the current table structure is preferred, whereas opting for the first column signifies a need for transposition.The prompt detailing this method is provided in Appx.A.5.Results in Tab. 2 highlight capability of    Row Reordering.Upon transposition, our next objective is to ensure the logical coherence of the table data through reordering the rows.We instruct LLMs to suggest improved reordering strategies using the prompts as detailed in Appx.A.6.Due to the subjective nature involved in identifying the most suitable order of a tabular data, and given that there are no widely recognized standards for this process, the effectiveness of the proposed sorting strategy will be evaluated based its downstream impact on the results of table QA task.We notice that when the entire well-ordered table is exposed, GPT-3.5 occasionally suggests alternative sorting strategies, leading to unnecessary complexity.To counteract this tendency and ensure a better sorting proposal, we strategically present the model with only the first three and the last three rows of the</p>
<p>Comparing Textual and Symbolic Reasoning</p>
<p>In this section, we delve into the comparison of textual and symbolic reasoning methods in LLMs for tabular data understanding ( 5.1), further conducting a detailed error analysis ( 5.2) to address the second research question ( 1).We evaluate the performance of each reasoning strategy using GPT-3.5, shedding light on their strengths and challenges.In 4.3, we explored NORM to mitigate structural perturbations, enhancing generalized LLM performance and successfully restoring perturbed tables to accuracy levels similar to their original states.Therefore, subsequent analyses will exclusively consider the original tables (T ).</p>
<p>Results</p>
<p>Tab. 4 showcases the performance of GPT-3.5 when employed for both direct textual reasoning using DP and interactive symbolic reasoning using PyAgent.By instructing the model with the CoT (Wei et al., 2023) reasoning strategy to think step by step, and then give the final answer, as detailed in Appx.A.1, we can achieve an accuracy of 58.66%.This surpasses the StructGPT's Iterative Readingthen-Reasoning method, which concentrates reasoning tasks by continually collecting relevant evidence.For tables with limited tokens, symbolic reasoning via PyAgent offers an accuracy of 56.87%, which is slightly behind the accuracy by DP in a single attempt.A distinct advantage of symbolic reasoning is its ability to only present parts of the table in the prompt.As our experiments revealed, after excluding the central rows and showcasing only the initial and final three rows, we manage to maintain an accuracy of 52.45% with a 4.42% drop compared to the full-table PyAgent results.This makes it possible to deal with larger tables with numerous rows using LLMs with limited context window.In the following sections, we will present a comprehensive analysis of the discrepancies and errors observed across these methods.</p>
<p>Error Analysis</p>
<p>To elucidate the challenges and limitations of DP and PyAgent, this section presents an in-depth error analysis by sampling 50 erroneous outputs for each.Tab. 5 summarizes the predominant error types for DP and PyAgent methods.</p>
<p>Reasoning Aggregation</p>
<p>This section examines how combining multiple reasoning pathways can boost LLMs' accuracy in interpreting tabular data, which is in response to the third research question ( 1).</p>
<p>Methods</p>
<p>Self-Consistency.Previous work has highlighted the advantages of generating multiple outputs from LLMs and adopting the most frequent answer, a mechanism known as self-consistency (SC; Wang et al. 2023).Tab. 4 showcases the notable improvements realized through self-consistency (aggregating 10 outputs), with DP achieving an accuracy of 64.84% and PyAgent attaining 63.49%.</p>
<p>Self-Evaluation.Based on our error analysis in 5.2, different reasoning methods excel at specific tasks.For instance, symbolic reasoning tends to outperform textual reasoning in counting and column localization tasks.To optimize the choice between these methods, we strategically use a prompt (referenced in Appx.A.7), which avoids directly validating answers against tables but guides the LLM to choose between the two reasoning approaches based on the question's nature and each answer's clarity.By weighing the problem against the known strengths and weaknesses of each reasoning strategy, this tactic mitigates potential bias towards textual reasoning by LLMs and enhances answer accuracy.As evidenced by Tab. 4, using self-evaluation boosts accuracy to 64.99%.Impressively, this method, using only two reasoning paths, matches the performance of using 10 paths of DP or PyAgent independently.</p>
<p>Mix Self-Consistency.According to 5.1, symbolic and textual reasoning exhibit distinct focuses but deliver similar performance.Consequently, we introduce Mix Self-Consistency, a method that selects a predetermined number of outputs for each type of inference, aiming for self-consistency.This approach hinges on the idea that multiple outputs can reflect the confidence levels of LLMs in answer generation.In scenarios where LLMs are less proficient, they tend to produce a diverse set The detailed mechanics of how this approach is operationalized within the framework of Mix Self-Consistency, including the aggregation and interpretation of these outputs, are further elucidated in Appx.E.2.Tab. 4 demonstrates that using mix self-consistency (generating 5 outputs per inference type, 3 totaling 10) enhances performance substantially, achieving an impressive accuracy of 72.40%, which achieves SOTA performance on the sampled WTQ data.</p>
<p>Overall Evaluation</p>
<p>To evaluate our method thoroughly, we conduct a comprehensive pass of testing using the complete WTQ test set, integrating both NORM and Mix self-consistency mechanisms.Since re-sorting may change the answers of row index-related questions, we perform NORM without resorting in this evaluation. 4However, it is noteworthy that re-sorting can 3 The choice of generating 5 outputs per inference type (5+5) is a hyperparameter selection influenced by the dataset's distribution.We conducted an ablation study regarding this in Appx.E.1.We use an equal split (5+5) based on observed comparable performance between the two reasoning strategies. 4Originally, the NORM process included a re-sorting step to counteract the row-shuffling perturbation.However, re-sorting may inadvertently alter answers reliant on the initial sequence, as explored in the error analysis ( 5.2) with a detailed case study in Appx.D.6.</p>
<p>Method Accuracy (%)</p>
<p>Fine-tuning Based Models TAPAS (Herzig et al., 2020) 48.8 T5-3B (Xie et al., 2022) 49.3 TAPAX (Liu et al., 2022) 57.5 REASTAP (Zhao et al., 2022) 58.7 OMNITAB (Jiang et al., 2022) 63.3</p>
<p>LLMs Based Methods STRUCTGPT  (Jiang et al., 2023a) 48.4 BINDER  (Cheng et al., 2023b) 55.5 BINDER  (Cheng et al., 2023b) 64.6 LEVER  (Ni et al., 2023) 65.8 DATER  (Ye et al., 2023) 65.9</p>
<p>Ours  73.6</p>
<p>Table 6: Comparison of various methods on all test data of WTQ. denotes methods based on the GPT-3.5 (OpenAI, 2023a);  denotes methods based on the Codex (OpenAI, 2022).</p>
<p>be advantageous for questions not dependent on row indexes, particularly when dealing with tables that are initially unorganized or messy.As illustrated in Tab.6, our proposed method exhibits outstanding efficacy with an accuracy of 73.6%, significantly outperforming existing models to achieve SOTA performance on the complete WTQ test set.Importantly, our approach is conducted in a fully zero-shot manner.For a detailed analysis of how table size impacts method performance, see Appx.C.</p>
<p>In response to the third research question, our findings reveal that reasoning path aggregation significantly enhances LLMs' accuracy in table reasoning tasks.Notably, the Mix Self-Consistency method achieves an accuracy of 73.6% on the WTQ dataset, surpassing the previous SOTA by a considerable margin.The Self-Evaluation strategy also contributes to this remarkable performance by adeptly selecting between reasoning approaches.</p>
<p>Conclusion</p>
<p>This study delved into the proficiency of LLMs in tabular reasoning.We identify that LLMs are sensitive to structural variance of tables, yet the application of a normalization strategy, NORM, can stabilize the table structures, thus reinforcing the resistance to structural perturbations.When it comes to comparing reasoning approaches, textual reasoning demonsrate slight superiority over symbolic reasoning, with each strategy possessing its unique advantages.Moreover, integrating multiple reasoning strategy via mix self-consistency is proved beneficial for overall interpretation accuracy, significantly outperforming previous SOTA results on the WTQ dataset.Collectively, these observations pave the way for refining LLMs' approach to tabular understanding and reasoning.</p>
<p>Limitation</p>
<p>While this study provides insights into tabular data reasoning with LLMs, it is pertinent to acknowledge its limitations.First, the exclusive utilization of GPT-3.5, due to the budgetary constraints, may limit the generalizability of our findings, as exploration with GPT-4 might offer enhanced outcomes.Second, all table data are sourced from Wikipedia, which may introduce potential data leakage or memorization issues, as certain answers might be implicitly available within the LLMs' training data, thus potentially biasing results.Lastly, several perturbation-sensitive table-based questions, especially regarding table perturbations like shuffling, may impact the precision of the reported accuracy, as demonstrated answers may change based on the structural modifications of the table.</p>
<p>Appendices A Prompts</p>
<p>A.1 Prompt of Direct Prompting (DP).</p>
<p>You are an advanced AI capable of analyzing and understanding information within tables.Read the table below regarding "[TITLE]".</p>
<p>[TABLE ] Based on the given table, answer the following question:</p>
<p>[QUESTION]</p>
<p>Let's think step by step, and then give the final answer.Ensure the final answer format is only "Final Answer: AnswerName1, AnswerName2..." form, no other form.And ensure the final answer is a number or entity names, as short as possible, without any explanation.</p>
<p>A.2 Prompt of Python Agent.</p>
<p>You are working with a pandas dataframe in Python.The name of the dataframe is <code>df</code>.Your task is to use <code>python_repl_ast</code>to answer the question posed to you.</p>
<p>Tool description:</p>
<p>-<code>python_repl_ast</code>: A Python shell.Use this to execute python commands.Input should be a valid python command.When using this tool, sometimes the output is abbreviated -ensure it does not appear abbreviated before using it in your answer.</p>
<p>Guidelines:</p>
<p>-<strong>Aggregated Rows</strong>: Be cautious of rows that aggregate data such as 'total', 'sum', or 'average'.Ensure these rows do not influence your results inappropriately.-<strong>Data Verification</strong>: Before concluding the final answer, always verify that your observations align with the original table and question.Notes for final answer: -Ensure the final answer format is only "Final Answer: AnswerName1, AnswerName2..." form, no other form.-Ensure the final answer is a number or entity names, as short as possible, without any explanation.</p>
<p>-Ensure to have a concluding thought that verifies the table, observations and the question before</p>
<p>giving the final answer.</p>
<p>You are provided with a table regarding "[TITLE]".This is the result of <code>print(df.to_markdown())</code>:</p>
<p>[TABLE ] <strong>Note</strong>: All cells in the table should be considered as <code>object</code>data type, regardless of their appearance.</p>
<p>Begin! Question: [QUESTION]</p>
<p>A.3 Prompt of LLMs as</p>
<p>A.5 Prompt of Content-Aware Transposition Determination</p>
<p>You are an advanced AI capable of analyzing and understanding information within tables.Read the table below regarding "[TITLE]".</p>
<p>[TABLE ] Headings of a table are labels or titles given to rows or columns to provide a brief description of the data they contain.</p>
<p>Based on the given table, the headings of the table are more likely to be:
(A) [FIRST_ROW] (B) [FIRST_COLUMN]
(C) None of the above Directly give your choice.Ensure the format is only "Choice: (A)/(B)/(C)" form, no other form, without any explanation.</p>
<p>A.6 Prompt of Resorting</p>
<p>You are an advanced AI capable of analyzing and understanding information within tables.Read the table below regarding "[TITLE]":</p>
<p>[TABLE ] Note: Only selected rows from the beginning and end of the table are displayed for brevity.Intermediate rows are omitted and represented by "..." for clarity.</p>
<p>The table column headings are provided below, separated by semicolons:</p>
<p>[HEADINGS]</p>
<p>In order to optimize the interpretability and readability of the data, follow these guidelines to determine the most suitable sorting method:</p>
<p>Sorting Guidelines:</p>
<ol>
<li>Evaluate columns based on data types such as numerical, alphabetical, chronological, categorical, or other relevant sorting methods.2. Identify any patterns or relationships in the data that would be highlighted by certain sorting methods.3. Consider column position, as those on the left may sometimes have sorting priority.4. If applicable, consider sorting by multiple columns in a prioritized sequence.</li>
</ol>
<p>Provide your decision using one of the following statements:</p>
<p>-For sorting using a single column: "Sort by: [Name of Column]".-For sorting using multiple columns: "Sort by: [Primary Column Name], [Secondary Column Name], ...".</p>
<p>-If no specific sorting seems advantageous: "Sort by: N/A".</p>
<p>Your response should strictly follow the formats provided.</p>
<p>A.7 Prompt of Self-Evaluation</p>
<p>Below is a markdown table regarding "[TITLE]":</p>
<p>[TABLE ] You're tasked with answering the following question:</p>
<p>[QUESTION]</p>
<p>You have 2 answers derived by two different methods.Answer A was derived by prompting the AI to think step-by-step.Answer B was derived by interacting with a Python Shell.
Answer A is [COT_ANSWER]. Answer B is [AGENT_ANSWER].
Your task is to determine which is the correct answer.It is crucial that you strictly adhere to the following evaluation process:</p>
<ol>
<li><strong>Preliminary Evaluation</strong>: Begin by evaluating which of the two answers directly addresses the question in a straightforward and unambiguous manner.A direct answer provides a clear response that aligns closely with the query without introducing additional or extraneous details.If one of the answers is not a direct response to the question, simply disregard it.2. <strong>Nature of the Question</strong>: If both answers appear to be direct answers, then evaluate the nature of the question.For tasks involving computation, counting, and column-locating, especially when for extensive table, the Python Shell (Answer B) might be more precise.However, always remain cautious if the Python Shell's output appears off (e.g., error messages, success notifications, etc.).Such outputs may not be trustworthy for a correct answer.3. <strong>Final Verdict</strong>: Finally, after thorough evaluation and explanation, provide your verdict strictly following the given format:
-Use "[[A]]" if Answer A is correct. -Use "[[B]]" if Answer B is correct.
Note: 1.Each method has its own strengths and weaknesses.Evaluate them with an unbiased perspective.</li>
</ol>
<p>When in doubt, consider the nature of the question and lean towards the method that is most suited for such queries.2. Ensure that your verdict is provided after evaluation, at the end.Fig. 3 illustrates a typical mistake made by LLMs when transposing tables, a problem that becomes more evident when a table has many identical or similar entries.Take, for example, the 'Nation Cup' column shown in the figure, which is filled with numerous ?symbols.LLMs, limited in processing structured data, often mishandle such tables, leading to misplacements or misalignments.This highlights the fundamental difficulties and limitations LLMs face in accurately transposing tables containing repetitive or similar data cells.</p>
<p>B Analysis for LLMs as</p>
<p>B.2 Analysis</p>
<p>A further examination of the results, as shown in Fig. 4, illustrates that transposition accuracy for LLMs as direct table transposer is associated with the table's dimensions.The accuracy in row-to-column transposition (T  T  ) is distinctly sensitive to the original table's row count, whereas column-to-row transposition (T   T ) accuracy is similarly related on the number of columns.This observation can be potentially attributed to the inherent characteristics and organizational structure of table data.In most of the row tables, cells within a given column often display homogeneous data types, such as numerical or temporal values.This homogeneity can pose significant challenges for LLMs, as the models might struggle to differentiate between semantically similar cells during the transposition process, thereby leading to potential misalignments and misplacements, particularly as the number of rows increases.Conversely, in those column tables, cells within a row may exhibit similar data types, introducing analogous challenges and potential errors during transposition.</p>
<p>C Impact of Table Size on WTQ Performance</p>
<p>This section presents an analysis of the impact of table size (quantified by row numbers) on the performance of different methods when applied to the WikiTableQuestions benchmark.Specifically, we examine how the average accuracy of DP, PyAgent, and the combination by applying mix self-consistency is affected by the number of rows in a table.</p>
<p>To systematically evaluate the impact, we segmented the row numbers into 10 ranges, each containing approximately 430 data points, and calculated the average accuracy within these intervals.Fig. 5 visualizes the average accuracy across these ranges for each method.It is evident that there is a shared trend of diminishing accuracy as the number of rows increases.This observation suggests that all methods are subject to decreased efficacy in the context of long tables.</p>
<p>The decline in performance with larger tables can be attributed to the complexity of handling longcontext data and the abundance of potentially interfering information.This complexity often results in an increased error rate.The insights gained from this analysis point towards a need for the development of better symbolic methods for handling long tables, which might be capable of effectively narrowing down the scope of larger tables, either by selective attention to relevant segments or by intelligently summarizing the data, to mitigate the challenges posed by long-context information.</p>
<p>D Error Case Study for WTQ D.1 Table Misinterpretation</p>
<p>D.1.1 Counting Error</p>
<p>Figure 6: Example of a table misinterpretation error by DP, where the LLMs make mistakes attributable to its deficiency in performing counting tasks effectively.</p>
<p>Fig. 6 highlights a typical error related to table comprehension, emphasizing a common problem in LLMs when dealing with tasks that involve statistical analysis.It points to a weakness in LLMs' ability to accurately process and respond to questions based on statistical information without the help of external tools.Fig. 7 showcases a common error in table interpretation associated with LLMs.This error originates from the LLMs' linearization process, which impairs their ability to recognize table structures.Although the model efficiently identifies the highest value, 132, in the 2nd(m) column, it inaccurately associates this value with the 1nd(m) column, assuming it represents the same feature as in the 2nd(m) column.This leads to a misplacement of the value in the table's interpretation.Fig. 8 illustrates a minor issue stemming from a coding mistake.The table shown in the figure features a variety of data under the Team column.Alongside the expected Dallas Cowboy entries, there are cells with a slight variation: Dallas Cowboy  .The Python Shell Agent used failed to recognize these unusual variations.This is evident from the use of the df['Team'] = "Dallas Cowboy" command for calculating occurrences, leading to a discrepancy in the final count and resulting in inaccurate outcomes.Fig. 10 depicts an error where the Python Shell Agent incorrectly includes a special row in its calculations.Specifically, when counting the number of Linux distributions supporting the x86 architecture, the agent erroneously counts a nested heading row.As indicated in the figure, the row indexed at 9 is not a valid data entry but rather serves as a nested heading for the table.This row should have been excluded from the count, resulting in an inaccurate calculation (29  30) of distributions supporting the x86 architecture.Fig. 11 shows an issue that originates from a basic coding mistake.Although the agent correctly grasps the concept, aiming to filter for entries with a maximum velocity of at least 100 km/h, it falters in the implementation stage by using a contains("100 km/h") statement in the code.This error is akin to the Logical Inconsistency error described in (DP), where a mismatch between the understanding of a concept and its practical execution becomes evident.Fig. 12 displays a scenario in which the final answer produced did not align with the specified prompt instructions.In this instance, the Python Shell Agent correctly executed a series of interactions and accurately identified the final answer as 77.However, the response given was Brooks Racing had the number 77..Although the reasoning and the result are correct within their respective contexts, the format of the response impedes the parsing of the correct answer.Fig. 13 shows a DP example where the final answer does not directly address the posed question.Faced with the binary query Are there at least 13 names on the chart?, the anticipated response should be a simple yes or no.However, the LLM responds with 12.Although this answer aligns conceptually with the data in the table and the logic seems accurate, it does not conform to the direct and binary response format required by the question.30, 194730, , to September 4, 1950.Despite this, the interpretation erroneously concludes that neither was in the role in 1934, leading to a contradiction between the information and the final answer.Fig. 17 demonstrates an instance of Execution Issue error made by the Python Shell Agent.In the process of identifying the team that participated in Euro 2000 qualifying for the most consecutive years, the agent faces difficulties in the data processing phase.Initially, an error occurs due to the Date column not containing datetime objects.Then the agent successfully converts the entries into the appropriate format.However, the agent, in trying to compute the number of consecutive participation years for each team, gets stuck in a loop of continually refining its calculation method without arriving at a conclusive answer within the given interaction steps.Fig. 18 illustrates a situation where PyAgent encounters a Non-Observable Action Error.Specifically, the actions given by the PyAgent, for example, consecutive_games_count = len(df_filtered), do not generate any observable output in the Python Shell.Consequently, the agent does not receive any valid observations to aid it in deriving the correct answer, thus, leading to an incorrect count of consecutive games played in the ARCO Arena from January 2nd to January 24th.Fig. 19 shows a case of data inconsistency due to the application of the resorting stage in the NORM procedures.The figure's upper table displays the original format, with Gillig as the manufacturer in the final row.However, after resorting as suggested by LLMs, the lower table in the figure lists New Flyer as the last row's manufacturer.This change, while seemingly minor in the broader context of table comprehension, significantly impacts responses to specific queries like "What is the name of the last manufacturer on the chart?"</p>
<p>D.1.2 Locating Error</p>
<p>D.2 Coding Error</p>
<p>D.2.1 Attribute Noise Error</p>
<p>D.2.2 Special Row Misinterpretation Error</p>
<p>D.2.3 Incorrect Coding</p>
<p>D.3 Misalignment Issue</p>
<p>D.3.1 Answer Format Issue</p>
<p>D.3.2 Answer Deviation Error</p>
<p>D.4.2 Reasoning Mistakes in PyAgent</p>
<p>D.5.2 Non-Observable Action Error</p>
<p>D.6 Resorting Issue</p>
<p>Figure 20: Accuracy results for the Mix Self-Consistency method applied to the sampled WTQ dataset, with varying combinations of DP and PyAgent outputs (depicted as DP vs. PyAgent on the x-axis).The combinations range from 10 DP vs. 0 PyAgent to 0 DP vs. 10 PyAgent.Each data point represents the maximum, minimum, and average accuracies obtained from 100 tests per combination, conducted using random sampling.Note that for the 10 DP vs. 0 PyAgent and 0 DP vs. 10 PyAgent combinations, there is no random sampling of paths.However, variance is observed due to the presence of multiple equally probable answer sets generated by the 10 paths, leading to different possible selections of answers even without sampling, thereby introducing randomness into the results.</p>
<p>E Analysis of Mix Self-Consistency E.1 Ablation Study of Output Selection</p>
<p>This section presents an ablation study conducted to elucidate the effect of various combinations of DP and PyAgent outputs on the performance of the Mix Self-Consistency method.For this experiment, we systematically explored different combinations while keeping the total output count constant at ten.Each combination was tested 100 times through random shuffling.For each test, maximum, minimum, and average accuracies were recorded.</p>
<p>Fig. 20 shows the results of the ablation study.The 5+5 combination (5 DP + 5 PyAgent) consistently gives the highest minimum and average accuracies among all tested combinations, making it a robust and reliable choice for this task.The 4+6 combination (4 DP + 6 PyAgent) secured the highest maximum accuracy in our tests.</p>
<p>Through this ablation study, we aim to provide insights into how different output selections influence the effectiveness of the Mix Self-Consistency method.Importantly, the choice of output combination should be considered as a hyperparameter that is intimately related to the distribution of the dataset being used.Given that different reasoning strategies exhibit unique strengths and weaknesses, it is crucial to tailor the output combination to align with the characteristics of the specific tasks and datasets in question, thereby maximizing the performance of the Mix Self-Consistency method.</p>
<p>E.2 Mechanics of Mix Self-Consistency in Output Selection</p>
<p>The effectiveness of the Mix Self-Consistency method in achieving high accuracy largely stems from its ability to harness the strengths of different reasoning methods.Intuitively, the multiple outputs from certain reasoning method can be interpreted as the confidence score for the generated answers.In scenarios where a method excels, its outputs often tend to converge towards a common answer, signifying higher confidence and reliability.In contrast, a method less suited to the problem at hand tends to produce more diverse results, indicative of a lower level of confidence.By aggregating these outputs from different methods and applying majority voting, the Mix Self-Consistency method refines these variations into a more accurate prediction.As shown in Fig. 21, This process leverages the strengths of the employed reasoning methods, thereby enhancing overall performance.</p>
<p>F Results of Mix Self-Consistency on TabFact</p>
<p>This section presents the additional results of applying the Mix Self-Consistency method to the Tab-Fact dataset, as part of an extended investigation to verify and evaluate the method's adaptability and effectiveness in other related tasks beyond WTQ dataset.</p>
<p>Method Accuracy</p>
<p>StructGPT (Jiang et al., 2023a) 0.708 Dater (Ye et al., 2023) 0.874 Ours 0.885 For TabFact, a subsample of 500 data points was randomly selected from the test set.The experimental setup mirrored that of the WTQ experiments, employing the same parameters such as temperature settings for model inference.The strategy for output selection in the TabFact experiment also follows the 5+5 combination, which proves to be the best for the WTQ dataset, to aggregate the output answers from 5 instances of DP and 5 instances of PyAgent.Additionally, all the prompts (e.g., DP, PyAgent) used in the TabFact experiment were slightly modified to align with the requirements of the fact-checking scenarios.</p>
<p>Tab. 7 summarizes the accuracy results of the Mix Self-Consistency method, StructGPT, and Dater on the TabFact dataset.Mix Self-Consistency can also achieve the highest accuracy, outperforming both StructGPT and Dater in fact-checking.</p>
<p>Strictly follow the given format to respond: Question: the input question you must answer Thought: you should always think about what to do to interact with <code>python_repl_ast ction: can **ONLY** be</code>python_repl_ast ction Input: the input code to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: after verifying the table, observations, and the question, I am confident in the final answer Final Answer: the final answer to the original input question (AnswerName1, AnswerName2...)</p>
<p>Figure 4 :
4
Figure 4: Performance of GPT-3.5 as direct table transposer: from original to transposed tables (T  T  ) and from transposed to original tables (T   T ), with different row and column counts.</p>
<p>Figure 5 :
5
Figure 5: The impact of table size on the accuracy of DP, PyAgent, and Mix-SC on the all the test set of WikiTableQuestions.The x-axis represents the row number ranges, and the y-axis shows the average accuracy for each method.</p>
<p>Figure 7 :
7
Figure 7: Example of a table misinterpretation error by DP, where the model fails accurately locating the specific cell.</p>
<p>Figure 8 :
8
Figure 8: Example of a Coding Issue error by PyAgent, where the abnormal variant data entries leads to inaccurate output.</p>
<p>Figure 9 :
9
Figure 9: Example of a coding error by PyAgent, where PyAgent misinterpretes the special row -Total.</p>
<p>Fig. 9
9
Fig.9displays an error where the Python Shell Agent incorrectly interprets a special row, treating the Total row as a valid season entry.In this case, while calculating the seasons in which Nicols won at least 10 races, the Agent did not properly exclude the cumulative Total row in its code for computation.Consequently, it erroneously reported one season as meeting the criteria.</p>
<p>Figure 10 :
10
Figure 10: Example of another coding error by PyAgent, where the PyAgent misinterpretes the special row which is a nested heading.</p>
<p>Figure 11 :
11
Figure 11: Example of a coding error by PyAgent, where the coding is incorrect</p>
<p>Figure 12 :
12
Figure 12: Example of a misalignment issue by PyAgent, where the answer format does not follow the instruction in the prompt to give a parsable answer.Note that emojis presented in the figure are purely for visual aid and were not incorporated in actual experiments; the same applies to any figures below.</p>
<p>Figure 13 :
13
Figure 13: Example of a misalignment issue by DP, where the final answer does not directly answer the question.</p>
<p>Figure 14 :
14
Figure 14: Example of a misalignment issue by PyAgent, where the final answer does not directly answer the question.</p>
<p>Fig. 14
14
Fig.14illustrates a case with the Python Shell Agent where the final response fails to directly answer the posed question.The question Which team has the same number of playoffs appearances as the St. Louis Bombers? clearly requests the identification of a specific team.Yet, it is noted that the agent prematurely delivers an answer upon finding data related to the playoff appearances of the St. Louis Bombers.While the direction of the python shell agent's reasoning appears correct, the resultant answer ultimately falls short of resolving the question correctly.</p>
<p>Fig. 15
15
Fig. 15 presents an example of a Logical Inconsistency error occurring during the interpretation of tabulated data.The error in reasoning is occurred in determining whether George E. Leach or Kenneth F. Cramer was the Chief of the National Guard Bureau in 1934.The reasoning text accurately states that George E. Leach served from December 1, 1931, to November 30, 1935, and Kenneth F. Cramer served from September30, 194730,  , to September 4, 1950.Despite this, the interpretation erroneously concludes that neither was in the role in 1934, leading to a contradiction between the information and the final answer.</p>
<p>Figure 16 :
16
Figure 16: Example of a logical inconsistency error by PyAgent where the LLMs make mistakes in reasoning on the observations.</p>
<p>Fig. 16
16
Fig. 16 depicts an instance of a Logical Inconsistency Error during the Python Shell interaction.In this case, LLMs successfully the necessary steps to gather information about Tablet 9 and Tablet 10, a misinterpretation of the retrieved data results in flawed reasoning.This leads to an error in the conclusion drawn from the interaction.</p>
<p>Figure 17 :
17
Figure 17: An example of an execution issue by PyAgent, where the agent attempts to fix an coding error but falls into a loop.</p>
<p>Figure 18 :
18
Figure 18: Example of a Execution Issue error by PyAgent, where the agent inputs non-observable actions into Python Shell.</p>
<p>Figure 19 :
19
Figure 19: Example of a Normalization Issue error by DP, where the correct answer changes due to the resorting stage in NORM.</p>
<p>Figure 21 :
21
Figure 21: An illustration of Mix Self-Consistency by aggreagting outputs from multiple reasoning methods to form a unified, high-confidence prediction..</p>
<p>Table Perturbations on LLMs
LLMs AsTask DescriptionAccuracyTransposerf (T )  T  f (T  )  T53.68 51.07Detectorf (T )  0 f (T  )  193.35 32.54Determinatorf (T , T0, * , T * ,0)  T0, *  f (T  , T0, * , T * ,0)  T * ,097.39 94.77Table 2: Evaluation results of GPT-3.5 on the 421distinct tables of WTQ dataset, including three tasks:Transposer involving switching between original (T )and transposed tables (T  ), Detector for identifyingneed for table transposition (0 for no transposition, 1for transposition required), and Determinator to chooseprobable table headings either from the first row (T 0, *  )or the first column (T  * ,0 ).In  3.1, we present three types of structural tableperturbations: transposition (T  ), row-shuffling</p>
<p>in discerning table headings se-
MethodTTT T  DP59.5052.2151.1437.51+NORM58.66 -1.41%58.66 +12.35%58.30 +14.00%57.71 +53.85%PyAgent55.9147.9112.438.96+NORM56.87 +1.72%57.11 +19.20%55.44 +346.02%55.08 +514.73%</p>
<p>Table 3 :
3
Accuracy of GPT-3.5 under different table perturbations for Direct Prompting (DP) and Python Shell Agent (PyAgent) with NORM applied.mantically, with accuracies of 97.39% and 94.77% respectively for original table and tranposed table.</p>
<p>table .
.MethodAccuracy (%)Few-shot Prompting MethodsBINDER  (Cheng et al., 2023b)63.61BINDER  (Cheng et al., 2023b)55.07DATER W/O SC  (Ye et al., 2023)61.75DATER W/ SC  (Ye et al., 2023)68.99Zero-shot Prompting MethodsSTRUCTGPT  (Jiang et al., 2023a)51.77NORM+DP 58.66NORM+PYAGENT 56.87NORM+PYAGENT-OMITTED 52.45NORM+DP&amp;PYAGENT W/ EVAL 64.22DP W/ SC 66.39+NORM 64.10+NORM W/O RESORT 66.99PYAGENT W/ SC 61.39+NORM 63.77+NORM W/O RESORT 62.84DP&amp;PYAGENT W/ MIX-SC 73.06+NORM 72.40+NORM W/O RESORT 73.65This selective exposure typically allows themodel to discern logical ordering patterns withoutbeing influenced by existing table configurations.Tab. 3 underscores the efficacy of NORM whenapplied prior to the two reasoning methods -DPand PyAgent. Demonstrably, NORM robustly mit-igates structural perturbations, optimizing tablecomprehensibility for LLMs. The results illustratethat applying NORM does not detrimentally affectthe original results (T ), and it effectively refinesperturbated data, aligning the outcomes closelywith the original results, and in some instances,even showing slight improvement. This suggeststhat NORM as a preprocessing step for preparingtabular data can enhance robust analysis by LLMs.In addressing our initial research question, theanalysis indicates that LLMs' performance is sen-sitive to table structural variations, with signifi-cant struggles observed in accurately interpretingthe same tabular content under transposition andshuffling. While textual reasoning demonstrates</p>
<p>Table 4 :
4
Performance on the sampled WTQ dataset (T ).</p>
<p> denotes methods based on Codex, while  represents those based on GPT-3.5.The term SC refers to selfconsistency. 2 NORM W/O RESORT means that reordering stage for NORM is not performed.some resilience to structural variations, symbolic reasoning is significantly impacted, particularly with transposed tables.The NORM strategy effectively navigates these challenges by eliminating dependency on table structures, providing consistent interpretation across diverse table structures without compromising the integrity or meaning of the original content.</p>
<p>For SC, results are derived by conducting an average over 100 shuffles to accommodate instances of ties during majority voting.In the Mix-SC method, DP-derived answers are prioritized over PyAgent due to DP's observed superior performance.All experiments regarding SC follow this.gentwithin single attempts.Despite this, PyAgent can handle larger tables by processing partial table views.Notably, DP encounters difficulties in accurate table interpretation, while PyAgent reveals instability in coding capabilities.
Table interpretationerrors significantly afflict the DP method, compris-ing 42% of its total errors, highlighting substantialchallenges for LLMs in accurately interpreting ta-ble data. PyAgent primarily struggles with codingerrors, constituting 38% of its total errors. Theseerrors either originate from misunderstandings oftable content, often overlooking subtle details, ormanifest as inherent deficiencies in coding capabili-ties. These prevalent errors underscore the intrinsicchallenges and limitations each method faces in thereasoning process. Detailed case studies on eacherror type are delineated in Appx.  D.In response to the second research question, theanalysis indicates DP marginally surpasses PyA-
2</p>
<p>Table 5 :
5
Error types of DP and PyAgent methods. This does not imply that PyAgent does not make table interpretation errors; these are included under coding errors to avoid overlapping.Note that the percentages for each reasoning method might not sum up to 100%; the remaining percentage points are attributable to other errors, such as problems with dataset labeling, which are not categorized here.
Error TypesDP PyAgent DescriptionCase StudyTable Misinterpretation 42%- LLMs incorrectly interpret the content in tables.Appx.  D.1.1, Appx.  D.1.2Coding Errors-38%LLMs produce inaccurate code, typically due to is-sues with minor details.Appx.  D.2.1, Appx.  D.2.2, Appx.  D.2.3Misalignment Issue24%28%Outputs are conceptually correct but the answers do not align with the instructions.Appx.  D.3.1, Appx.  D.3.2Logical Inconsistency 20%10%LLMs exhibit failures in reasoning, leading to con-tradictions or inconsistencies.Appx.  D.4.1, Appx.  D.4.2Execution Issue-12%Issues emerge related to the execution of Python code.Appx.  D.5.1, Appx.  D.5.2Resorting Issue10%8%The resorting stage in NORM changes the answers of some sequence-dependent questions.Appx.  D.6
of answers.Conversely, for tasks that LLMs handle adeptly, consistent answers are often generated across multiple reasoning attempts, converging towards one answer.Such convergence allows for the aggregation of model outputs that align with areas where LLMs exhibit stronger reasoning capabilities, thereby substantially improving accuracy.</p>
<p>Table Transposer
Transposer
You are given the following table: [TABLE] Please transpose this table.Maintain the format I give, with each row beginning with '|' and each cell separated by ' | '.Do not change the content of any cell.Your response should solely consist of the transposed table, without any additional text.To enhance readability and facilitate efficient data analysis, it is often suggested that the table headings be horizontally located in the first/topmost row.Please evaluate the table with this consideration in mind, and provide your response in the following format: <strong>Table Headings</strong>: List the headings of the table, separated by commas.<strong>Table Evaluation</strong>: Identify whether the headings listed are horizontally located in the first/topmost row.If not, describe the position.<strong>Transpose Recommended</strong>: Indicate if transposing is recommended.Answer with only "YES" or "NO", without any additional explanation.
A.4 Prompt of LLMs as Table Transposition DetectorPlease examine the provided table:[TABLE]</p>
<p>Table Transposer B
Transposer
.1 Case Study Figure 3: An example error case of content misalignment occurring within cells when leveraging LLMs directly to transpose a table.Blue Table (Top): The original table subjected to a transposition operation.Green Table (Buttom Left): The ground truth table subsequent to transposition.Purple Table (Buttom Right): GPT-3.5'soutput of transposed table.Cells erroneously aligned or displaced are highlighted in red.</p>
<p>Table 7 :
7
Accuracy results of different methods without fine-tuning on the TabFact dataset.</p>
<p>Column shuffling was not employed as the typical number of columns is limited and this shuffling had minimal impact on accuracy(Zhao et al.,<br />
).</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020. 2020. 2020. December 6-12, 2020Language models are few-shot learners</p>
<p>. Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ngand Bing Xiang. 2023. Dr.spider: A diagnostic evaluation benchmark towards text-to-sql robustness</p>
<p>. Harrison Chase, 2022LangChain</p>
<p>Large language models are few(1)-shot table reasoners. Wenhu Chen, 2023</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, 2023a</p>
<p>Binding language models in symbolic languages. Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu, ICLR, abs/2210.028752023b</p>
<p>. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Emily Vinodkumar Prabhakaran, Nan Reif, Ben Du, Reiner Hutchinson, James Pope, Jacob Bradbury, Michael Austin, Guy Isard, Pengcheng Gur-Ari, Toju Yin, Anselm Duke, Sanjay Levskaya, Sunipa Ghemawat, Henryk Dev, Xavier Michalewski, Vedant Garcia, Kevin Misra, Liam Robinson, Denny Fedus, Daphne Zhou, David Ippolito, Hyeontaek Luan, Barret Lim, Alexander Zoph, Ryan Spiridonov, David Sepassi, Shivani Dohan, Mark Agrawal, Omernick, M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas EckJeff Dean, Slav Petrovand Noah Fiedel. 2022. Palm: Scaling language modeling with pathways</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Leo Gao, John Schulman, Jacob Hilton, Scaling laws for reward model overoptimization. 2022</p>
<p>PASTA: Tableoperations aware fact verification via sentence-table cloze pre-training. Zihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiaoman Zhao, Xiaoyong Du, 10.18653/v1/2022.emnlp-main.331Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu, 2023</p>
<p>TaPas: Weakly supervised table parsing via pre-training. Jonathan Herzig, Krzysztof Pawel, Thomas Nowak, Francesco Mller, Julian Piccinno, Eisenschlos, 10.18653/v1/2020.acl-main.398Proceedings of the 58th Annual Meeting of the. the 58th Annual Meeting of theAssociation for Computational Linguistics. Association for Computational Linguistics2020</p>
<p>Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen, Structgpt: A general framework for large language model to reason on structured data. 2023a</p>
<p>OmniTab: Pretraining with natural and synthetic data for few-shot tablebased question answering. Zhengbao Jiang, Yi Mao, Pengcheng He, Graham Neubig, Weizhu Chen, 10.18653/v1/2022.naacl-main.68Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Active retrieval augmented generation. Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, 2023b</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, Zhaoxiang Zhang, Sheetcopilot: Bringing software productivity to the next level through large language models. 2023a</p>
<p>Making large language models better reasoners with stepaware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, 2023b</p>
<p>Tapex: Table pre-training via learning a neural sql executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, 2022</p>
<p>. Grgoire Mialon, Roberto Dess, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozire, Jane Schick, Asli Dwivedi-Yu, Edouard Celikyilmaz, Yann Grave, Thomas Lecun, Scialom, 2023Augmented language models: a survey</p>
<p>Webgpt: Browserassisted question-answering with human feedback. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman2022</p>
<p>Lever: Learning to verify language-to-code generation with execution. Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov, Wen-Tau Yih, Sida I Wang, Xi Victoria, Lin , Proceedings of the 40th International Conference on Machine Learning (ICML'23). the 40th International Conference on Machine Learning (ICML'23)2023</p>
<p>Dalle 3. OpenAI. 2023c. Gpt-4 technical report. Panupong Pasupat and Percy Liang. 2023b. 2015Compositional semantic parsing on semi-structured tables</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schrli, Denny Zhou, 2023</p>
<p>Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. Learning to summarize from human feedback. </p>
<p>Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, ArXiv preprint, abs/2302.13971</p>
<p>Robust (controlled) table-to-text generation with structure-aware equivariance learning. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang ; Wang, Zhewei Xu, Pedro Szekely, Muhao Chen, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom2023b. 2022Llama 2: Open foundation and fine-tuned chat models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, M. Dai, and Quoc V. Le.2022Finetuned language models are zero-shot learners</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Victor Wang, Bailin Zhong, Chengzu Wang, Connor Li, Ansong Boyle, Ziyu Ni, Dragomir Yao, Caiming Radev, Lingpeng Xiong, Rui Kong, Noah A Zhang, Luke Smith, Tao Zettlemoyer, Yu, 2022EMNLP</p>
<p>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, 2023</p>
<p>TaBERT: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen-Tau Yih, Sebastian Riedel, 10.18653/v1/2020.acl-main.745Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer, 2022Opt: Open pretrained transformer language models</p>
<p>Wenqi Zhang, Yongliang Shen, Weiming Lu, Yueting Zhuang, Data-copilot: Bridging billions of data and humans with autonomous workflow. 2023</p>
<p>ReasTAP: Injecting table reasoning skills during pre-training via synthetic reasoning examples. Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev, 10.18653/v1/2022.emnlp-main.615Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>Robut: A systematic study of table qa robustness against human-annotated adversarial perturbations. Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru Tang, Boyu Mi, Dragomir Radev, 2023</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. </p>            </div>
        </div>

    </div>
</body>
</html>
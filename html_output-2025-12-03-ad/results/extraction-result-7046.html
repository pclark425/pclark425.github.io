<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7046 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7046</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7046</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-94beb9f249d6d2f1c00d8edfa2db861633aee6f9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/94beb9f249d6d2f1c00d8edfa2db861633aee6f9" target="_blank">Structured Chain-of-Thought Prompting for Code Generation</a></p>
                <p><strong>Paper Venue:</strong> ACM Transactions on Software Engineering and Methodology</p>
                <p><strong>Paper TL;DR:</strong> Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs and the human evaluation shows human developers prefer programs from SCoT prompting.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive abilities in code generation. Chain-of-Thought (CoT) prompting is the state-of-the-art approach to utilizing LLMs. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, the accuracy of CoT prompting still cannot satisfy practical applications. For example, gpt-3.5-turbo with CoT prompting only achieves 53.29% Pass@1 in HumanEval. In this article, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation named SCoT prompting. Our motivation is that human developers follow structured programming. Developers use three programming structures (i.e., sequential, branch, and loop) to design and implement structured programs. Thus, we ask LLMs to use three programming structures to generate SCoTs (structured reasoning steps) before outputting the final code. Compared to CoT prompting, SCoT prompting explicitly introduces programming structures and unlocks the structured programming thinking of LLMs. We apply SCoT prompting to two LLMs (i.e., gpt-4-turbo, gpt-3.5-turbo, and DeepSeek Coder-Instruct- \(\{\) 1.3B, 6.7B, 33B \(\}\) ) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). SCoT prompting outperforms CoT prompting by up to 13.79% in Pass@1. SCoT prompting is robust to examples and achieves substantial improvements. The human evaluation also shows human developers prefer programs from SCoT prompting.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7046.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7046.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Chain-of-Thought Prompting for Code Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that forces LLMs to first produce structured intermediate reasoning steps (SCoTs) composed of program structures — sequence, branch, loop — plus an input-output specification, and then generate code from that structure; applied to ChatGPT and Codex and evaluated on function-level code benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT; Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting technique applied to transformer-based auto-regressive LLMs: ChatGPT (instruction-tuned with RLHF) and Codex (code-specialized completion model trained on natural language and large code corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Codex: 175B; ChatGPT: not specified</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (auto-regressive); prompting augmentation: two-step SCoT -> code pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not newly trained; applied to models pre-trained on large corpora of natural language and code (paper notes Codex trained on natural language + billions of lines of code; ChatGPT trained on extensive text and code plus RLHF).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Structured Chain-of-Thought (SCoT): generate an explicit input-output block and intermediate steps organized into program structures (sequence/branch/loop), optionally nested; two-step generation where multiple SCoTs are sampled then deterministic code generation per SCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval; MBPP; MBCPP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Function-level code generation benchmarks with English requirements, function signatures, and unit tests; HumanEval (164 problems, ~7.7 tests each, Python), MBPP (974 problems, 3 tests each, Python), MBCPP (848 problems, 3 tests each, C++).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level code generation (synthesis of programs from natural-language requirements) evaluated by unit tests.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased Pass@k (k in {1,3,5}) computed from 20 sampled programs per problem then aggregated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ChatGPT SCoT Pass@1: HumanEval 60.64%, MBPP 46.98%, MBCPP 57.06%; Codex SCoT Pass@1: HumanEval 49.82%, MBPP 38.29%, MBCPP 48.34%. (Full Pass@3/5 numbers reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Relative to CoT prompting (best prior prompting baseline): ChatGPT Pass@1 improvements up to +13.79% (HumanEval), +12.31% (MBPP), +6.63% (MBCPP); Codex Pass@1 improvements up to +13.77% (HumanEval), +7.38% (MBPP), +5.57% (MBCPP). SCoT also improves over few-shot and zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Constraining intermediate reasoning to program-structured steps substantially improves code-generation correctness (notably Pass@1), yields outputs preferred by human developers (correctness, lower code smell, maintainability), is robust to example seed and writing style, and is effective across models and languages (Python and C++).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires manually authored example SCoTs for prompt construction (though shown robust to different seeds/styles); generated SCoTs can contain errors/noise (mitigated by asking models to double-check and via two-step pipeline); potential dataset leakage concerns for underlying models; two-step pipeline may introduce extra sampling cost; does not use external execution or formal theorem provers; SCoTs are sometimes close to pseudocode but often more abstract (~26% close to pseudocode).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7046.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7046.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique where LLMs are asked to generate natural-language intermediate reasoning steps (a chain of thought) before producing final answers; used as the state-of-the-art baseline for eliciting reasoning behavior and adapted here as a baseline for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT; Codex (used as baseline prompting style on same base models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>CoT is a prompt engineering / inference-time method applied to transformer LLMs to elicit stepwise natural-language reasoning; not a separate model architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Prompting technique for transformer autoregressive models</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generate free-form natural language intermediate steps (CoTs) then final output; single-step generation in this paper's baseline setup (longer maximum generation length to allow CoT+code).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval; MBPP; MBCPP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same function-level code generation benchmarks used to evaluate SCoT and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased Pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ChatGPT CoT Pass@1: HumanEval 53.29%, MBPP 41.83%, MBCPP 53.51%; Codex CoT Pass@1: HumanEval 43.79%, MBPP 35.66%, MBCPP 45.79%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>CoT provides only slight improvements over few-shot prompting for code generation (e.g., up to ~2% on HumanEval Pass@1 in this paper) and is substantially outperformed by SCoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While CoT elicits intermediate reasoning and helps for many natural-language reasoning tasks, natural-language-only CoTs are less effective for code generation because they ignore program structure and can be ambiguous about control-flow scopes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT's free-form natural-language steps can be ambiguous for program structure, leading to lower code-generation accuracy and scope misunderstandings; in this work CoT produced smaller gains compared to SCoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7046.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7046.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo-0301, instruction-tuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned transformer LLM trained with reinforcement learning from human feedback (RLHF), used as a base model in experiments for code generation under zero-shot, few-shot, CoT, and SCoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (referred to as ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned, RLHF-trained transformer model optimized for following human instructions and multi-domain tasks including code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (auto-regressive), instruction-tuned via RLHF</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Extensive natural language text and code files; further instruction tuning with human feedback (as stated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Evaluated with prompting methods including zero-shot, few-shot, Chain-of-Thought, Structured CoT (SCoT); SCoT sampled structured intermediate plans then deterministic code generation per plan.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval; MBPP; MBCPP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Function-level code-generation benchmarks (see SCoT entry).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased Pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Under SCoT prompting ChatGPT Pass@1: HumanEval 60.64%, MBPP 46.98%, MBCPP 57.06%; Under CoT prompting ChatGPT Pass@1: HumanEval 53.29%, MBPP 41.83%, MBCPP 53.51%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>SCoT prompting improved ChatGPT Pass@1 by up to +13.79% relative to CoT prompting (HumanEval) and produced higher human-evaluation scores (correctness, code smell, maintainability).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ChatGPT benefits substantially from SCoT prompting for code generation; structured intermediate representations yield better test-passing rates and human-preferred code.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model size not specified in paper; underlying pretraining could contain benchmark data (data leakage concern). Performance depends on prompt examples (though SCoT shown robust).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7046.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7046.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-specialized auto-regressive transformer model (completion-focused) trained on large corpora of natural language and billions of lines of code; used as a base model to evaluate prompting techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based language model fine-tuned/optimized for code completion and synthesis, trained on mixed natural-language and very large-scale code data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (auto-regressive) for code completion</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Natural language + billions of lines of code (as reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tested with zero-shot, few-shot, CoT, and SCoT prompting; SCoT used two-step structured plan sampling then deterministic code generation per plan.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval; MBPP; MBCPP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Function-level code-generation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level code generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased Pass@k</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Codex SCoT Pass@1: HumanEval 49.82%, MBPP 38.29%, MBCPP 48.34%; Codex CoT Pass@1: HumanEval 43.79%, MBPP 35.66%, MBCPP 45.79%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>SCoT prompting increased Codex Pass@1 by up to +13.77% relative to CoT prompting on HumanEval and produced consistent gains across MBPP and MBCPP.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Codex, like ChatGPT, benefits from SCoT prompting though absolute numbers are lower than ChatGPT in this study; structured intermediate reasoning yields notable relative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pretraining on public code creates potential benchmark leakage; no external formal reasoning tools or provers used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7046.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7046.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks (HumanEval/MBPP/MBCPP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HumanEval; MBPP; MBCPP function-level code generation benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three representative datasets for evaluating code generation: HumanEval (hand-written Python problems), MBPP (Many Back-Programming Problems, Python), and MBCPP (C++ problems); each provides requirements, signatures, and unit tests used to compute Pass@k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval; MBPP; MBCPP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>HumanEval: 164 hand-written Python problems (~7.7 tests each). MBPP: 974 Python problems (3 tests each). MBCPP: 848 C++ problems (3 tests each). Used for unit-test-based correctness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level program synthesis evaluated by unit tests</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Unbiased Pass@k (k=1,3,5) measured from 20 generated candidates</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported Pass@k for various prompting techniques and models (see SCoT, CoT, ChatGPT, Codex entries for numeric results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as common evaluation across prompting techniques; SCoT outperforming CoT and other baselines on these benchmarks as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Unit-test-based Pass@1 is a strict metric where structured reasoning improvements were most pronounced; SCoT exhibits consistent gains across these datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Benchmarks may appear in LLM pretraining corpora (potential leakage); HumanEval lacks a train split (authors reused MBPP examples for HumanEval prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7046.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7046.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT (rank technique)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT: Code Generation with Generated Tests</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rank/re-ranking technique that samples many program candidates from an LLM, executes them on auto-generated test cases, and reranks candidates by execution results to pick better solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CodeT: Code Generation with Generated Tests</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>sampling + execution-based reranking (post-generation pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Not a logical-reasoning augmentation but a reranking approach using execution feedback (generated tests) to select best program candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Executes sampled candidate programs on auto-generated test cases and uses execution outcomes to rerank; requires execution environment and test-generation module.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MBPP (used in exploratory complementarity experiments in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>MBPP: Python code generation benchmark with 3 test cases per problem; used in exploratory combined evaluation with SCoT in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Function-level code synthesis with execution-based reranking</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@k (improvement observed when combined with SCoT in an exploratory experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Numerical improvements shown in illustrative figure for progressive addition of CodeT and SCoT on MBPP, but exact numeric table values are not reported in-text for this combined experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Described as complementary to SCoT prompting: SCoT improves candidate quality, CodeT improves selection; combining both yields further gains in exploratory experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Rank techniques like CodeT are complementary to SCoT prompting; they require executable environments and thus have limited applicability where execution is infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dependence on execution environment and generated test quality; not directly comparable as they solve different parts of the pipeline (generation vs. selection).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Structured Chain-of-Thought Prompting for Code Generation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained in Code <em>(Rating: 2)</em></li>
                <li>CodeT: Code Generation with Generated Tests <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7046",
    "paper_id": "paper-94beb9f249d6d2f1c00d8edfa2db861633aee6f9",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "SCoT prompting",
            "name_full": "Structured Chain-of-Thought Prompting for Code Generation",
            "brief_description": "A prompting technique that forces LLMs to first produce structured intermediate reasoning steps (SCoTs) composed of program structures — sequence, branch, loop — plus an input-output specification, and then generate code from that structure; applied to ChatGPT and Codex and evaluated on function-level code benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT; Codex (code-davinci-002)",
            "model_description": "Prompting technique applied to transformer-based auto-regressive LLMs: ChatGPT (instruction-tuned with RLHF) and Codex (code-specialized completion model trained on natural language and large code corpora).",
            "model_size": "Codex: 175B; ChatGPT: not specified",
            "architecture_type": "Transformer (auto-regressive); prompting augmentation: two-step SCoT -&gt; code pipeline",
            "training_data": "Not newly trained; applied to models pre-trained on large corpora of natural language and code (paper notes Codex trained on natural language + billions of lines of code; ChatGPT trained on extensive text and code plus RLHF).",
            "reasoning_method": "Structured Chain-of-Thought (SCoT): generate an explicit input-output block and intermediate steps organized into program structures (sequence/branch/loop), optionally nested; two-step generation where multiple SCoTs are sampled then deterministic code generation per SCoT.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "HumanEval; MBPP; MBCPP",
            "benchmark_description": "Function-level code generation benchmarks with English requirements, function signatures, and unit tests; HumanEval (164 problems, ~7.7 tests each, Python), MBPP (974 problems, 3 tests each, Python), MBCPP (848 problems, 3 tests each, C++).",
            "task_type": "Function-level code generation (synthesis of programs from natural-language requirements) evaluated by unit tests.",
            "performance_metric": "Unbiased Pass@k (k in {1,3,5}) computed from 20 sampled programs per problem then aggregated.",
            "performance_value": "ChatGPT SCoT Pass@1: HumanEval 60.64%, MBPP 46.98%, MBCPP 57.06%; Codex SCoT Pass@1: HumanEval 49.82%, MBPP 38.29%, MBCPP 48.34%. (Full Pass@3/5 numbers reported in paper tables.)",
            "comparison_with_baseline": "Relative to CoT prompting (best prior prompting baseline): ChatGPT Pass@1 improvements up to +13.79% (HumanEval), +12.31% (MBPP), +6.63% (MBCPP); Codex Pass@1 improvements up to +13.77% (HumanEval), +7.38% (MBPP), +5.57% (MBCPP). SCoT also improves over few-shot and zero-shot prompting.",
            "key_findings": "Constraining intermediate reasoning to program-structured steps substantially improves code-generation correctness (notably Pass@1), yields outputs preferred by human developers (correctness, lower code smell, maintainability), is robust to example seed and writing style, and is effective across models and languages (Python and C++).",
            "limitations": "Requires manually authored example SCoTs for prompt construction (though shown robust to different seeds/styles); generated SCoTs can contain errors/noise (mitigated by asking models to double-check and via two-step pipeline); potential dataset leakage concerns for underlying models; two-step pipeline may introduce extra sampling cost; does not use external execution or formal theorem provers; SCoTs are sometimes close to pseudocode but often more abstract (~26% close to pseudocode).",
            "uuid": "e7046.0",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CoT prompting",
            "name_full": "Chain-of-Thought Prompting",
            "brief_description": "A prompting technique where LLMs are asked to generate natural-language intermediate reasoning steps (a chain of thought) before producing final answers; used as the state-of-the-art baseline for eliciting reasoning behavior and adapted here as a baseline for code generation.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGPT; Codex (used as baseline prompting style on same base models)",
            "model_description": "CoT is a prompt engineering / inference-time method applied to transformer LLMs to elicit stepwise natural-language reasoning; not a separate model architecture.",
            "model_size": null,
            "architecture_type": "Prompting technique for transformer autoregressive models",
            "training_data": null,
            "reasoning_method": "Generate free-form natural language intermediate steps (CoTs) then final output; single-step generation in this paper's baseline setup (longer maximum generation length to allow CoT+code).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "HumanEval; MBPP; MBCPP",
            "benchmark_description": "Same function-level code generation benchmarks used to evaluate SCoT and baselines.",
            "task_type": "Function-level code generation",
            "performance_metric": "Unbiased Pass@k",
            "performance_value": "ChatGPT CoT Pass@1: HumanEval 53.29%, MBPP 41.83%, MBCPP 53.51%; Codex CoT Pass@1: HumanEval 43.79%, MBPP 35.66%, MBCPP 45.79%.",
            "comparison_with_baseline": "CoT provides only slight improvements over few-shot prompting for code generation (e.g., up to ~2% on HumanEval Pass@1 in this paper) and is substantially outperformed by SCoT prompting.",
            "key_findings": "While CoT elicits intermediate reasoning and helps for many natural-language reasoning tasks, natural-language-only CoTs are less effective for code generation because they ignore program structure and can be ambiguous about control-flow scopes.",
            "limitations": "CoT's free-form natural-language steps can be ambiguous for program structure, leading to lower code-generation accuracy and scope misunderstandings; in this work CoT produced smaller gains compared to SCoT.",
            "uuid": "e7046.1",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT (gpt-3.5-turbo-0301)",
            "name_full": "ChatGPT (gpt-3.5-turbo-0301, instruction-tuned model)",
            "brief_description": "An instruction-tuned transformer LLM trained with reinforcement learning from human feedback (RLHF), used as a base model in experiments for code generation under zero-shot, few-shot, CoT, and SCoT prompting.",
            "citation_title": "ChatGPT",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (referred to as ChatGPT)",
            "model_description": "Instruction-tuned, RLHF-trained transformer model optimized for following human instructions and multi-domain tasks including code generation.",
            "model_size": null,
            "architecture_type": "Transformer (auto-regressive), instruction-tuned via RLHF",
            "training_data": "Extensive natural language text and code files; further instruction tuning with human feedback (as stated in paper).",
            "reasoning_method": "Evaluated with prompting methods including zero-shot, few-shot, Chain-of-Thought, Structured CoT (SCoT); SCoT sampled structured intermediate plans then deterministic code generation per plan.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "HumanEval; MBPP; MBCPP",
            "benchmark_description": "Function-level code-generation benchmarks (see SCoT entry).",
            "task_type": "Function-level code generation",
            "performance_metric": "Unbiased Pass@k",
            "performance_value": "Under SCoT prompting ChatGPT Pass@1: HumanEval 60.64%, MBPP 46.98%, MBCPP 57.06%; Under CoT prompting ChatGPT Pass@1: HumanEval 53.29%, MBPP 41.83%, MBCPP 53.51%.",
            "comparison_with_baseline": "SCoT prompting improved ChatGPT Pass@1 by up to +13.79% relative to CoT prompting (HumanEval) and produced higher human-evaluation scores (correctness, code smell, maintainability).",
            "key_findings": "ChatGPT benefits substantially from SCoT prompting for code generation; structured intermediate representations yield better test-passing rates and human-preferred code.",
            "limitations": "Model size not specified in paper; underlying pretraining could contain benchmark data (data leakage concern). Performance depends on prompt examples (though SCoT shown robust).",
            "uuid": "e7046.2",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Codex (code-davinci-002)",
            "name_full": "Codex (code-davinci-002)",
            "brief_description": "A code-specialized auto-regressive transformer model (completion-focused) trained on large corpora of natural language and billions of lines of code; used as a base model to evaluate prompting techniques.",
            "citation_title": "Codex",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex)",
            "model_description": "Transformer-based language model fine-tuned/optimized for code completion and synthesis, trained on mixed natural-language and very large-scale code data.",
            "model_size": "175B",
            "architecture_type": "Transformer (auto-regressive) for code completion",
            "training_data": "Natural language + billions of lines of code (as reported in paper).",
            "reasoning_method": "Tested with zero-shot, few-shot, CoT, and SCoT prompting; SCoT used two-step structured plan sampling then deterministic code generation per plan.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "HumanEval; MBPP; MBCPP",
            "benchmark_description": "Function-level code-generation benchmarks.",
            "task_type": "Function-level code generation",
            "performance_metric": "Unbiased Pass@k",
            "performance_value": "Codex SCoT Pass@1: HumanEval 49.82%, MBPP 38.29%, MBCPP 48.34%; Codex CoT Pass@1: HumanEval 43.79%, MBPP 35.66%, MBCPP 45.79%.",
            "comparison_with_baseline": "SCoT prompting increased Codex Pass@1 by up to +13.77% relative to CoT prompting on HumanEval and produced consistent gains across MBPP and MBCPP.",
            "key_findings": "Codex, like ChatGPT, benefits from SCoT prompting though absolute numbers are lower than ChatGPT in this study; structured intermediate reasoning yields notable relative gains.",
            "limitations": "Pretraining on public code creates potential benchmark leakage; no external formal reasoning tools or provers used.",
            "uuid": "e7046.3",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Benchmarks (HumanEval/MBPP/MBCPP)",
            "name_full": "HumanEval; MBPP; MBCPP function-level code generation benchmarks",
            "brief_description": "Three representative datasets for evaluating code generation: HumanEval (hand-written Python problems), MBPP (Many Back-Programming Problems, Python), and MBCPP (C++ problems); each provides requirements, signatures, and unit tests used to compute Pass@k.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "HumanEval; MBPP; MBCPP",
            "benchmark_description": "HumanEval: 164 hand-written Python problems (~7.7 tests each). MBPP: 974 Python problems (3 tests each). MBCPP: 848 C++ problems (3 tests each). Used for unit-test-based correctness evaluation.",
            "task_type": "Function-level program synthesis evaluated by unit tests",
            "performance_metric": "Unbiased Pass@k (k=1,3,5) measured from 20 generated candidates",
            "performance_value": "Reported Pass@k for various prompting techniques and models (see SCoT, CoT, ChatGPT, Codex entries for numeric results).",
            "comparison_with_baseline": "Used as common evaluation across prompting techniques; SCoT outperforming CoT and other baselines on these benchmarks as reported.",
            "key_findings": "Unit-test-based Pass@1 is a strict metric where structured reasoning improvements were most pronounced; SCoT exhibits consistent gains across these datasets.",
            "limitations": "Benchmarks may appear in LLM pretraining corpora (potential leakage); HumanEval lacks a train split (authors reused MBPP examples for HumanEval prompting).",
            "uuid": "e7046.4",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "CodeT (rank technique)",
            "name_full": "CodeT: Code Generation with Generated Tests",
            "brief_description": "A rank/re-ranking technique that samples many program candidates from an LLM, executes them on auto-generated test cases, and reranks candidates by execution results to pick better solutions.",
            "citation_title": "CodeT: Code Generation with Generated Tests",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "model_size": null,
            "architecture_type": "sampling + execution-based reranking (post-generation pipeline)",
            "training_data": null,
            "reasoning_method": "Not a logical-reasoning augmentation but a reranking approach using execution feedback (generated tests) to select best program candidates.",
            "external_tool_used": true,
            "external_tool_description": "Executes sampled candidate programs on auto-generated test cases and uses execution outcomes to rerank; requires execution environment and test-generation module.",
            "benchmark_name": "MBPP (used in exploratory complementarity experiments in paper)",
            "benchmark_description": "MBPP: Python code generation benchmark with 3 test cases per problem; used in exploratory combined evaluation with SCoT in paper.",
            "task_type": "Function-level code synthesis with execution-based reranking",
            "performance_metric": "Pass@k (improvement observed when combined with SCoT in an exploratory experiment)",
            "performance_value": "Numerical improvements shown in illustrative figure for progressive addition of CodeT and SCoT on MBPP, but exact numeric table values are not reported in-text for this combined experiment.",
            "comparison_with_baseline": "Described as complementary to SCoT prompting: SCoT improves candidate quality, CodeT improves selection; combining both yields further gains in exploratory experiment.",
            "key_findings": "Rank techniques like CodeT are complementary to SCoT prompting; they require executable environments and thus have limited applicability where execution is infeasible.",
            "limitations": "Dependence on execution environment and generated test quality; not directly comparable as they solve different parts of the pipeline (generation vs. selection).",
            "uuid": "e7046.5",
            "source_info": {
                "paper_title": "Structured Chain-of-Thought Prompting for Code Generation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Evaluating Large Language Models Trained in Code",
            "rating": 2
        },
        {
            "paper_title": "CodeT: Code Generation with Generated Tests",
            "rating": 2
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "rating": 2
        }
    ],
    "cost": 0.016912749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Structured Chain-of-Thought Prompting for Code Generation</h1>
<p>Jia Li o*<br>lijia@stu.pku.edu.cn<br>Peking University<br>Beijing, China<br>Yongmin Li<br>Peking University<br>Beijing, China<br>liyongmin@pku.edu.cn</p>
<h2>ABSTRACT</h2>
<p>Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-theart prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation.</p>
<p>In this paper, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation, named SCoT prompting. Our motivation is source code contains rich structural information and any code can be composed of three program structures (i.e., sequence, branch, and loop structures) [3]. Intuitively, structured intermediate reasoning steps make for structured source code. Thus, we ask LLMs to use program structures to build CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs. Compared to CoT prompting, SCoT prompting explicitly constraints LLMs to think about how to solve requirements from the view of source code and further the performance of LLMs in code generation. We apply SCoT prompting to two LLMs (i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline - CoT prompting by up to $13.79 \%$ in Pass@1. (2) Human evaluation shows human developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to examples and achieves substantial improvements.</p>
<h2>ACM Reference Format:</h2>
<p>Jia Li o*, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured Chain-of-Thought Prompting for Code Generation. In Proceedings of ACM Conference (Con-ference'17). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/ nnnnnnnn.nnnnnnn</p>
<h2>1 INTRODUCTION</h2>
<p>Code generation aims to automatically generate a program that satisfies a given natural language requirement [13, 14, 38]. Large</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Ge Li <br> Peking University <br> Beijing, China <br> lige@pku.edu.cn</h2>
<h2>Zhi Jin <br> Peking University <br> Beijing, China <br> zhijin@pku.edu.cn</h2>
<ol>
<li>Initialize a result with -999999</li>
<li>Iterate through the list of lists</li>
<li>Initialize a sum with @</li>
<li>Iterate through the list</li>
<li>Add the element to the sum</li>
<li>Update result with the maximum of sum and result</li>
<li>Divide the result by $K$</li>
<li>Return the result
(a) Chain-of-Thought</li>
</ol>
<p>Input: arry: list[list], K: int
Output: result: int or float
Loop
1: Initialize a result with -999999
Structure
2: for list in the list of lists:
3: Calculate the sum of the list
4: If the sum is great than result:
Branch
5: Update the result
Structure:
6: Divide result by $K$
Sequence
7: return result
structure
(b) Structured Chain-of-Thought</p>
<p>Figure 1: The comparison of a Chain-of-Thoughts (CoT) and our Structured Chain-of-Thought (SCoT).</p>
<p>Language Models (LLMs) have recently shown impressive performance in code generation, such as ChatGPT [18], and CodeGen [17]. During the inference, LLMs take a prompt as input that consists of several examples (e.g., <requirement, code> pairs) and a new requirement. LLMs learn code generation from examples and analogously generate a new program. The performance of LLMs heavily relies on the prompt [39]. Nowadays, how to make an effective prompt (i.e., Prompting technique) for code generation is still an open question.</p>
<p>Chain-of-Thought (CoT) prompting [35] is the state-of-the-art (SOTA) prompting technique. CoT Prompting asks LLMs first to generate a CoT and then output the code. A CoT is several intermediate natural language reasoning steps that describe how to write code step by step. Figure 1 (a) shows a CoT on code generation. However, CoT prompting brings slight improvements in code generation. For example, it only improves ChatGPT by 0.82 points in Pass@1 upon a real-world benchmark [7].</p>
<p>In this paper, we propose a Structured CoT for code generation. Our motivation is that code generation aims to convert a natural language requirement to source code. Different from natural languages, source code contains rich structural information [22, 30, 37]. For example, source code contains three basic structures</p>
<p>[3], including sequence, branch, and loop structures. Intuitively, intermediate reasoning steps leading to the structured code should also be structured. Consider a human developer's thought process when solving a requirement (e.g., find the maximum number in a list). It is typical to come up with a solving process with program structures: "Initialize a result with -inf; for each number in the list; if the number is greater than result: Update result with the number ...". Our idea is to enable LLMs to generate similar structured CoTs a coherent series of intermediate reasoning steps constructed by program structures. Besides, LLMs' training data contains lots of code data, so they have the ability to generate program structures. However, standard CoT ignores the program structures and has low accuracy in code generation. Thus, it is necessary to design a structured CoT to unlock the reasoning ability of LLMs in code generation.</p>
<p>Figure 1 (b) shows a SCoT. The design of our SCoT has two inspirations. First, existing work [3] proved that any simple or complex program can be composed of three basic structures, i.e., sequence structure, branch structure, and loop structure. Thus, we introduce three basic structures and constrain LLMs to use them to generate CoTs. As shown in Figure 1 (b), the SCoT uses a loop structure to clearly describe an iteration in line 2. While in the CoT, the scopes of two iterations in lines 2 and 4 are ambiguous. It shows the superiority of SCoT in code generation. Besides, every program contains a required input-output structure, which includes the input-output parameters and their types (e.g., Input: array: list[list]; Output: result in Figure 1 (b)). By generating the input-output structure, LLMs are asked to analyze requirements and determine the entry and exit of the code, which benefits the following solving process.</p>
<p>Based on the SCoT, we present a new prompting technique named SCoT prompting. It asks LLMs first to generate a SCoT using program structures and then implement the code. Compared to CoT prompting, SCoT prompting explicitly introduces program structures into intermediate reasoning steps and constraints LLMs to think about how to solve requirements from the view of programming languages. It further unlocks the reasoning ability of LLMs in code generation, thus achieving higher accuracy.</p>
<p>We apply SCoT prompting to two popular LLMs (i.e., ChatGPT [18] and Codex [7]) and evaluate it on three representative benchmarks (i.e., HumanEval [7], MBPP [2], and MBCPP [1]). We use unit tests to measure the correctness of generated programs and report the Pass@k $(k \in[1,3,5])$ [7]. Based on experimental results, we obtain four findings. (1) SCoT prompting significantly improves the accuracy of LLMs on code generation. Compared to the SOTA baseline - Chain-of-Thought prompting, in terms of Pass@1, SCoT prompting outperforms it by up to $13.79 \%$ in HumanEval, 12.31\% in MBPP, and 6.63\% in MBCPP. (2) Human evaluation shows that human developers prefer programs generated by SCoT prompting. (3) SCoT prompting is effective for different LLMs and different programming languages. In terms of Pass@1, it improves ChatGPT by up to $13.79 \%$ and Codex by up to $13.77 \%$. Besides, SCoT prompting is language-agnostic and effective in multiple languages (e.g., Python and $\mathrm{C}++$ ). (4) We explore the robustness of SCoT prompting to examples. Results show that SCoT prompting does not depend on specific examples or writing styles.</p>
<p>We summarize our contributions in this paper as follows.</p>
<ul>
<li>We propose a Structured Chain-of-Thought (SCoT), which utilizes program structures to build the intermediate reasoning steps.</li>
<li>We propose a novel prompting technique for code generation, named SCoT Prompting. It prompts large language models first to generate a SCoT and then implement the code.</li>
<li>We conduct extensive experiments on three benchmarks. Qualitative and quantitative experiments show that SCoT prompting significantly outperforms SOTA baselines (e.g., Chain-of-Thought prompting).</li>
<li>We discuss the contributions of different program structures and the robustness of SCoT prompting.
Data Availability. We open source our replication package [? ], including the datasets and the source code of SCoT prompting, to facilitate other researchers and practitioners to repeat our work and verify their studies.</li>
</ul>
<h2>2 METHODOLOGY</h2>
<p>In this section, we propose a Structured Chain-of-Thought (SCoT). A SCoT denotes several intermediate reasoning steps constructed by program structures. Then, we present a novel prompting technique for code generation named SCoT prompting. SCoT prompting asks LLMs first to generate a SCoT and then output the final code. In the subsections, we first describe the design of our SCoT and further show the details of SCoT prompting.</p>
<h3>2.1 Structured Chain-of-Thought</h3>
<p>Standard Chain-of-Thought (CoT) is several intermediate natural language reasoning steps that lead to the final answer [35]. The CoT is initially designed for natural language generation (e.g., commonsense reasoning [26]). Thus, the CoT only uses natural languages to sequentially describe how to solve a problem step by step. Figure 1 (a) shows a CoT on code generation. A limitation is that CoT brings slight improvements in code generation. For example, adding the CoT only improves ChatGPT by 0.82 points in Pass@1 upon a real-world benchmark - HumanEval [7].</p>
<p>In this paper, we propose a Structured CoT. Our motivation is that, unlike natural language generation, the goal of code generation is highly structured code. Source code solves a problem through special structures, including sequence structures, branch structures, and loop structures. For example, given a requirement - reading text from a given file, imagine a human developer's thought process. The developer will use program structures to design an initial idea: "if the given file exists: read text from the file; else: raise an error;". The program structures clearly show the solving process and benefit the following code implementation. Thus, intermediate reasoning steps leading to the code should also be structured.</p>
<p>Figure 2 shows some examples of SCoT. Compared to the CoT, our SCoT explicitly introduces program structures. Existing work [3] proved that any simple or complex program can be composed of three basic structures, i.e., sequence structure, branch structure, and loop structure Thus, we introduce three basic structures, whose details are shown as follows.</p>
<ul>
<li>Sequence Structure. The intermediate steps are sequentially placed and all steps are at the same level.</li>
<li>Branch Structure. It starts with a condition and places different intermediate steps for different results of the condition. In this</li>
</ul>
<p>Input: paren_string: str
Output: list_of_int: List[int]
1: Initialize list_of_int to an empty list
2: for each string in paren_string do
3: Initialize depth to 0
4: for each character in string do
5: if character is '(' then
6: $\quad \mid$ depth $+=1$
7: $\quad$ elif character is ')' then
8: $\quad$ depth $-=1$
9: append depth to list_of_int
10: return list_of_int
(a)</p>
<p>Input: string: str, substring: str
Output: count: int Loop Structure
1: Initialize count to 0
2: while substring is not found in string do
3: if string is empty then
4: return 0
5: increment count
6: remove the first character of string
7: return count
(b)</p>
<p>Figure 2: Examples of SCoT in code generation.
paper, branch structures contain three formats, i.e., if ..., if ... else, and if ... elif ... else.</p>
<ul>
<li>Loop Structure. A set of intermediate steps are repeatedly conducted until given conditions are not met. In this paper, loop structures contain two basic formats, including the for loop and the while loop.
We allow the nesting between different program structures. It allows LLMs to design more complex SCoT for some difficult requirements. As shown in Figure 2, the SCoT flexibly uses various program structures to build a solving process.</li>
</ul>
<p>Besides three basic structures, we add the input-output structure, which contains input-output parameters and their types. Our motivation is that an input-output structure is required for a program, which indicates the entry and exit. Generating the input-output structure is beneficial to clarify requirements and generate the following solving process.</p>
<h3>2.2 SCoT prompting</h3>
<p>Based on the SCoT, we propose a new prompting technique for code generation, named SCoT prompting. It asks LLMs first to generate a SCoT and then output the final code.</p>
<p>To implement SCoT prompting, we design two special prompts. The first prompt is used to generate a SCoT, and an example of the prompt is shown in Figure 3. The prompt starts with several examples (i.e., <requirement, SCoTx>). These examples cover three basic program structures and the input-output structure. Next, the italic sentences are instructions for LLMs, which indicate the goal of LLMs and related constraints. Finally, the prompt ends with a new requirement and is fed into LLMs. We expect LLMs to learn from examples and generate a SCoT for the new requirement.</p>
<p>After generating a SCoT, we design the second prompt for generating the final code. An example of the prompt is shown in Figure 4.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 3: A prompt for generating a SCoT.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">first_Repeated_Char</span><span class="p">(</span><span class="nf">str</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">Write</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">python</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">first</span><span class="w"> </span><span class="n">repeated</span>
<span class="k">character</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">string</span><span class="p">.</span>
<span class="w">    </span><span class="k">Input</span><span class="err">:</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">string</span>
<span class="w">    </span><span class="k">Output</span><span class="err">:</span><span class="w"> </span><span class="nl">ch</span><span class="p">:</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">repeated</span><span class="w"> </span><span class="k">character</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">str</span>
<span class="w">    </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="k">each</span><span class="w"> </span><span class="k">character</span><span class="w"> </span><span class="n">ch</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span>
<span class="w">    </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">ch</span><span class="w"> </span><span class="n">appears</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="k">than</span><span class="w"> </span><span class="n">once</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span>
<span class="w">    </span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">ch</span>
<span class="w">    </span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span>
<span class="w">    </span><span class="o">*</span><span class="n">Please</span><span class="w"> </span><span class="k">check</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">above</span><span class="w"> </span><span class="n">solving</span><span class="w"> </span><span class="n">process</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">write</span><span class="w"> </span><span class="n">a</span>
<span class="n">code</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">it</span><span class="p">.</span><span class="w"> </span><span class="n">Note</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">solving</span><span class="w"> </span><span class="n">process</span><span class="w"> </span><span class="n">may</span><span class="w"> </span><span class="n">contain</span>
<span class="n">errors</span><span class="p">.</span>
<span class="w">    </span><span class="n">h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">{}</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">ch</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nf">str</span><span class="err">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">ch</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">h</span><span class="p">:</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="n">ch</span><span class="p">;</span>
<span class="w">            </span><span class="k">else</span><span class="err">:</span>
<span class="w">                </span><span class="n">h</span><span class="o">[</span><span class="n">ch</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">None</span>
</code></pre></div>

<p>(more examples...)
def text_lowercase_underscore(text):
Write a function to find sequences of lowercase letters joined with an underscore.
$\qquad$
pass
Please understand the requirement and write a rough solving process. It starts with a input-output structure. You should use three basic structures to build the solving process, including sequences, branches, and loops. The necessary details should be written in natural languages.</p>
<p>Figure 4: A prompt for generating the code.</p>
<p>The prompt starts with several examples (i.e., <requirement, SCoT, code>). The italic sentences are instructions. We consider the SCoT as a soft template and ask LLMs to implement a program. Finally,</p>
<p>the prompt ends with a new requirement and its SCoT, and is input into LLMs. By learning from examples, LLMs generate a new program based on the requirement and SCoT.</p>
<p>Related work [25] has found that generative models may be negatively affected by error accumulation. Similarly, in SCoT prompting, the generated SCoT may contain noises (e.g., errors or missing steps). These noises will further negatively affect code implementation. In this paper, we utilize two approaches to alleviating error accumulation. First, as shown in Figure 4, we ask LLMs to doublecheck the SCoT and fix possible noises. It allows LLMs to adaptively refer to the SCoT and filter out noises. Second, SCoT prompting utilizes a two-step generation pipeline. It provides a window of opportunity to debug where the SCoT goes wrong. In practice, human developers can first check the generated SCoT and fix possible errors. Then, the SCoT is used to generate code.</p>
<h3>2.3 Implementation Details</h3>
<p>SCoT prompting is a prompting technique for code generation, which does not rely on specific LLMs. In this paper, we consider ChatGPT as the default LLM. We select a few (e.g., three) <requirement, code> pairs from real-world benchmarks (i.e., training data) as example seeds. Then, we manually write the SCoT for seeds and obtain examples - <requirement, SCoT, code> triples, which are used to make prompts in Figure 3 and 4. A prompt contains three examples by default. The examples and prompt templates are available in our replication package. In the future, users can flexibly apply our approach to more powerful LLMs in a plug-and-play fashion.</p>
<h2>3 STUDY DESIGN</h2>
<p>To assess SCoT prompting, we conduct a large-scale study to answer four research questions. In this section, we present the details of our study, including datasets, evaluation metrics, comparison baselines, and implementation details.</p>
<h3>3.1 Research Questions</h3>
<p>Our study aims to answer the following research questions (RQ).
RQ1: How does SCoT prompting perform in terms of accuracy compared to baselines? This RQ aims to verify that SCoT prompting has a higher accuracy than existing prompting techniques on code generation. We apply three existing prompting techniques and SCoT prompting to two LLMs. Then, we use unit tests to measure the correctness of programs generated by different approaches and report the Pass@k.</p>
<p>RQ2: Do developers prefer programs generated by SCoT prompting? The ultimate goal of code generation is to assist human developers in writing code. In this RQ, we hire 10 developers (including industry employees and academic researchers) to manually review the programs generated by SCoT prompting and baselines. We measure the quality of programs in three aspects, including correctness, code smell, and maintainability.</p>
<p>RQ3: Is SCoT prompting robust to examples? Prompting techniques may be sensitive to example [39]. In this RQ, we measure the robustness of SCoT prompting to examples. Specifically, we measure the performance of SCoT prompting with different example seeds and different example writing styles.</p>
<p>Table 1: Statistics of the datasets in our experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistics</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;">MBCPP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Language</td>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">Python</td>
<td style="text-align: center;">C++</td>
</tr>
<tr>
<td style="text-align: left;"># Train</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">474</td>
<td style="text-align: center;">413</td>
</tr>
<tr>
<td style="text-align: left;"># Test</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">435</td>
</tr>
<tr>
<td style="text-align: left;">Avg. tests per sample</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>RQ4: What are the contributions of different program structures in SCoT prompting? As stated in Section 2.1, SCoT prompting introduces three basic structures and the input-output structure. This RQ is designed to analyze the contributions of different structures. We select an LLM as the base model. Then, we individually remove a program structure and report the fluctuations in performance.</p>
<h3>3.2 Benchmarks</h3>
<p>Following previous studies [6, 7, 17, 40], we conduct experiments on three representative code generation benchmarks, including the HumanEval in Python, MBPP in Python, and MBCPP in C++. The details of the benchmarks are described as follows.</p>
<ul>
<li>HumanEval [7] is a Python function-level code generation benchmark, which contains 164 hand-written programming problems. Each programming problem consists of an English requirement, a function signature, and several test cases, with an average of 7.7 test cases per problem.</li>
<li>MBPP [2] is a Python function-level code generation benchmark. It contains 974 programming problems that involve simple numeric manipulations or basic usage of standard libraries. Each problem contains an English requirement, a function signature, and three manually written test cases for checking functions.</li>
<li>MBCPP [1] is a C++ function-level code generation benchmark. It consists of 848 programming problems that are collected by crowd-sourcing. Each problem contains an English description, a function signature, and three test cases for checking the correctness of functions.
We follow the original splits of three datasets. The statistics of the benchmarks are shown in Table 1. We randomly pick several samples from training data to make examples in prompts (Section 2.3). Then, we measure the performance of different approaches on test data. Because HumanEval does not contain train data, we reuse examples from MBPP in HumanEval.</li>
</ul>
<h3>3.3 Evaluation Metrics</h3>
<p>Following previous code generation studies [6, 7, 17, 40], we use Pass@ $k$ as our evaluation metrics. Specifically, given a requirement, a code generation model is allowed to generate $k$ programs. The requirement is solved if any generated programs pass all test cases. We compute the percentage of solved requirements in total requirements as Pass@ $k$. For Pass@ $k$, a higher value is better. In our experiments, $k$ is set to 1,3 , and 5 , because we think that developers mainly use Top-5 outputs in real-world scenarios.</p>
<p>Previous work [1, 6, 7] found that standard Pass@ $k$ has high variance and proposed an unbiased Pass@ $k$. We follow previous</p>
<p>work and employ the unbiased Pass@k. Specifically, we generate $n \geq k$ programs per requirement (in this paper, we use $n=20$, $k \in[1,3,5])$, count the number of solved requirements $c$, and calculate the unbiased Pass@k:</p>
<p>$$
\operatorname{Pass} @ k:=\underset{\text { Problems }}{\mathbb{E}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
$$</p>
<p>We also notice that previous code generation studies use text-similarity-based metrics (e.g., BLEU [21]). These metrics are initially designed for natural language generation and are poor in measuring the correctness of programs [7]. Thus, we omit these metrics in our experiments.</p>
<h3>3.4 Comparison Baselines</h3>
<p>This paper proposes a new prompting technique for code generation. To assess the effectiveness of our approach, we select three mainstream prompting techniques as baselines.</p>
<ul>
<li>Zero-shot prompting [7] directly feeds the requirement into LLMs without examples. Then, it extracts a generated program from LLMs' outputs.</li>
<li>Few-shot prompting [7] randomly selects several &lt; requirement, code&gt; pairs as examples. Given a requirement, it concatenates examples and the requirement together, making a prompt. Then, the prompt is fed into LLMs, and LLMs predict a new program.</li>
<li>Chain-of-Thought (CoT) prompting [35] is a variant of fewshot prompting. CoT prompting produces a special prompt consisting of <requirement, CoT, code> triples as examples. A CoT is several intermediate natural language reasoning steps.
To ensure the fairness of comparison, all baselines and SCoT prompting have the same number of examples (i.e., three examples) and example seeds.</li>
</ul>
<p>The criteria for selecting baselines are three-fold. (1) SCoT prompting is a prompting technique for code generation. Thus, we directly compare it to existing prompting techniques for code generation. We also notice some emerging prompting techniques in other fields, such as Self-Consistency [31] and Least-to-Most [41]. But these approaches are designed for specific tasks (e.g., Arithmetic reasoning) and can not be directly applied to code generation. Thus, we omit them in this paper. (2) Our approach is to augment LLMs and can be flexibly applied to different LLMs. Thus, we do not directly compare LLMs to our approach. (3) We also omit some rank techniques for code generation [6]. They first use LLMs to generate many candidates and then leverage test cases or neural networks to rerank candidates. We think our work and these rank techniques are complementary. Users can use our approach to generate programs and then use post-processing techniques to select the final output. We further discuss the complementarity through experiments in Section 5.2.</p>
<h3>3.5 Base Large Language Models</h3>
<p>There are many available LLMs for source code. Our motivation is that existing LLMs can be divided into two categories: standard
language models and instruction-tuned models. For each category, we pick a representative model as the base model.
(1) Standard language models are pre-trained on a large-scale corpus with the next-token prediction objective. They are mainly used to continually complete the given content, such as code completion. Thus, we pick the state-of-the-art completion model for code - Codex [7] as a base model.</p>
<p>Codex [7] is a powerful language model for code generation, which supports a commercial application - GitHub Copilot [9]. Codex's training data contains both natural language and billions of lines of code. We use OpenAI's APIs to access the latest version of Codex with 175 billion parameters, i.e., code-davinci-002 [19].
(2) Instruction-tuned models refer to LLMs after instruction tuning. Instruction tuning trains LLMs to understand human users' instructions and perform tasks based on the instructions. We select the state-of-the-art instruction-tuned model - ChatGPT [18] as a base model.</p>
<p>ChatGPT [18] is the state-of-the-art LLM for code generation. ChatGPT is trained with extensive natural language text and code files. Then, it is trained with reinforcement learning and learns to follow human instructions. We use OpenAI's APIs to access the ChatGPT, i.e., gpt-3.5-turbo-0301 [18].</p>
<p>Our approach does not rely on specific LLMs and can be applied to different LLMs in a plus-and-play fashion. In the future, we will explore its usage on more powerful LLMs.</p>
<h3>3.6 Sampling Settings</h3>
<p>Following previous studies [7, 17, 40], we use nucleus sampling [11] to decode programs from LLMs. To ensure the fairness of experiments, all baselines and SCoT prompting generate 20 programs per requirement. The details of sampling settings are shown as follows.</p>
<p>Baselines. The temperature is 0.8 and the top- $p$ is 0.95 . For zeroshot and few-shot prompting, the maximum generated length is 300 tokens. The maximum generated length of CoT prompting is 600 tokens. Our motivation is that CoT prompting needs to generate intermediate reasoning steps and code. Thus, it requires a larger generation length.</p>
<p>SCoT prompting. In the first step, we sample 20 individual SCoTs from LLMs per requirement. The temperature is 0.8 and the top- $p$ is 0.95 . The maximum generated length is 300 tokens. Then, for each SCoT, we use LLMs to generate a corresponding program. The temperature is 0 and the maximum generated length is 300 tokens. Finally, we obtain 20 programs for each requirement. The total generation length of two steps is the same as CoT prompting.</p>
<h2>4 RESULTS AND ANALYSIS</h2>
<h3>4.1 RQ1: How does SCoT prompting perform in terms of accuracy compared to baselines?</h3>
<p>In the first research question, we apply SCoT prompting and baselines to three benchmarks and use unit tests to measure the correctness of generated programs.</p>
<p>Setup. We apply baselines and SCoT prompting to two LLMs (Section 3.5). Then, we measure the performance of different approaches on three code generation benchmarks (Section 3.2) using the Pass@k (Section 3.3).</p>
<p>Table 2: The Pass@k (\%) of SCoT prompting and baselines on three code generation benchmarks. The numbers in red denote SCoT prompting's relative improvements compared to the SOTA baseline - CoT prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Base Model</th>
<th style="text-align: center;">Prompting Technique</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBCPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">Zero-shot prompting</td>
<td style="text-align: center;">49.73</td>
<td style="text-align: center;">66.07</td>
<td style="text-align: center;">71.54</td>
<td style="text-align: center;">37.07</td>
<td style="text-align: center;">43.54</td>
<td style="text-align: center;">48.58</td>
<td style="text-align: center;">47.53</td>
<td style="text-align: center;">60.09</td>
<td style="text-align: center;">64.22</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot prompting</td>
<td style="text-align: center;">52.47</td>
<td style="text-align: center;">69.32</td>
<td style="text-align: center;">74.10</td>
<td style="text-align: center;">40.00</td>
<td style="text-align: center;">49.82</td>
<td style="text-align: center;">53.13</td>
<td style="text-align: center;">52.58</td>
<td style="text-align: center;">63.03</td>
<td style="text-align: center;">66.11</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT prompting</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">41.83</td>
<td style="text-align: center;">51.04</td>
<td style="text-align: center;">54.57</td>
<td style="text-align: center;">53.51</td>
<td style="text-align: center;">63.84</td>
<td style="text-align: center;">67.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT Prompting</td>
<td style="text-align: center;">60.64</td>
<td style="text-align: center;">73.53</td>
<td style="text-align: center;">77.32</td>
<td style="text-align: center;">46.98</td>
<td style="text-align: center;">55.31</td>
<td style="text-align: center;">58.36</td>
<td style="text-align: center;">57.06</td>
<td style="text-align: center;">65.70</td>
<td style="text-align: center;">68.70</td>
</tr>
<tr>
<td style="text-align: center;">Relative Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$13.79 \%$</td>
<td style="text-align: center;">$5.40 \%$</td>
<td style="text-align: center;">$2.38 \%$</td>
<td style="text-align: center;">$12.31 \%$</td>
<td style="text-align: center;">$8.37 \%$</td>
<td style="text-align: center;">$6.95 \%$</td>
<td style="text-align: center;">$6.63 \%$</td>
<td style="text-align: center;">$2.91 \%$</td>
<td style="text-align: center;">$2.49 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">Zero-shot prompting</td>
<td style="text-align: center;">40.20</td>
<td style="text-align: center;">61.78</td>
<td style="text-align: center;">68.11</td>
<td style="text-align: center;">27.07</td>
<td style="text-align: center;">43.81</td>
<td style="text-align: center;">47.93</td>
<td style="text-align: center;">40.25</td>
<td style="text-align: center;">54.17</td>
<td style="text-align: center;">60.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot prompting</td>
<td style="text-align: center;">42.93</td>
<td style="text-align: center;">62.96</td>
<td style="text-align: center;">70.10</td>
<td style="text-align: center;">33.17</td>
<td style="text-align: center;">45.72</td>
<td style="text-align: center;">49.62</td>
<td style="text-align: center;">44.12</td>
<td style="text-align: center;">57.65</td>
<td style="text-align: center;">62.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoT prompting</td>
<td style="text-align: center;">43.79</td>
<td style="text-align: center;">63.41</td>
<td style="text-align: center;">71.56</td>
<td style="text-align: center;">35.66</td>
<td style="text-align: center;">46.57</td>
<td style="text-align: center;">50.11</td>
<td style="text-align: center;">45.79</td>
<td style="text-align: center;">58.92</td>
<td style="text-align: center;">62.56</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SCoT Prompting</td>
<td style="text-align: center;">49.82</td>
<td style="text-align: center;">66.56</td>
<td style="text-align: center;">75.14</td>
<td style="text-align: center;">38.29</td>
<td style="text-align: center;">50.74</td>
<td style="text-align: center;">53.16</td>
<td style="text-align: center;">48.34</td>
<td style="text-align: center;">60.77</td>
<td style="text-align: center;">64.19</td>
</tr>
<tr>
<td style="text-align: center;">Relative Improvement</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$13.77 \%$</td>
<td style="text-align: center;">$4.97 \%$</td>
<td style="text-align: center;">$5.00 \%$</td>
<td style="text-align: center;">$7.38 \%$</td>
<td style="text-align: center;">$8.95 \%$</td>
<td style="text-align: center;">$6.09 \%$</td>
<td style="text-align: center;">$5.57 \%$</td>
<td style="text-align: center;">$3.14 \%$</td>
<td style="text-align: center;">$2.61 \%$</td>
</tr>
</tbody>
</table>
<p>Results. The Pass@k $(k \in[1,3,5])$ of different approaches are shown in Table 2. The numbers in red denote SCoT prompting's relative improvements compared to the SOTA baseline - CoT prompting.</p>
<p>Analyses. (1) SCoT prompting achieves the best results among all baselines. Table 2 shows that SCoT prompting can generate more correct programs than baselines on three benchmarks. Compared to the SOTA baseline - CoT prompting, in terms of Pass@1, SCoT prompting outperforms it by up to $13.79 \%$ in HumanEval, $12.31 \%$ in MBPP, and $6.63 \%$ in MBCPP. Pass@1 is a strict metric and it is difficult to improve. The results show that SCoT prompting can significantly improve the accuracy of LLMs on code generation and is more promising than existing prompting techniques. (2) SCoT prompting is effective in different LLMs and programming languages. SCoT prompting is effective in different LLMs. Compared to CoT prompting, in terms of Pass@1, SCoT prompting further improves ChatGPT by up to $13.79 \%$ and Codex by up to $13.77 \%$. Besides, SCoT prompting is language-agnostic and can be applied to different programming languages. As shown in Table 2, SCoT prompting brings substantial improvements in Python (i.e., Hu manEval and MBPP) and C++ (i.e., MBCPP). (3) SCoT prompting unlocks the reasoning ability of LLMs on code generation. LLMs can benefit from generating intermediate reasoning steps. The baseline - CoT prompting utilizes natural language steps but only brings slight improvements. In terms of Pass@1, CoT prompting improves few-shot prompting by up to $2 \%$ in HumanEval, $7.51 \%$ in MBPP, and $3.79 \%$ in MBCPP. In this paper, we introduce program structures into intermediate reasoning steps and propose a Structured Chain-of-Thought (SCoT). The SCoT constrains LLMs to use program structures to generate intermediate steps, moving in the direction of code. In terms of Pass@1, SCoT prompting improves few-shot prompting by up to $16.05 \%$ in HumanEval, $17.45 \%$ in MBPP, and $9.56 \%$ in MBCPP. The improvements show that SCoT prompting further unlocks the reasoning ability of LLMs in code generation.</p>
<p>Table 3: The results of human evaluation in three aspects. The numbers in red denote SCoT prompting's relative improvements compared to the SOTA baseline - CoT prompting. All the $p$-values are substantially smaller than 0.05 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Correctness</th>
<th style="text-align: center;">Code Smell</th>
<th style="text-align: center;">Maintainability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Zero-shot prompting</td>
<td style="text-align: center;">1.012</td>
<td style="text-align: center;">1.523</td>
<td style="text-align: center;">1.372</td>
</tr>
<tr>
<td style="text-align: left;">Few-shot prompting</td>
<td style="text-align: center;">1.119</td>
<td style="text-align: center;">1.653</td>
<td style="text-align: center;">1.552</td>
</tr>
<tr>
<td style="text-align: left;">CoT prompting</td>
<td style="text-align: center;">1.225</td>
<td style="text-align: center;">1.689</td>
<td style="text-align: center;">1.616</td>
</tr>
<tr>
<td style="text-align: left;">SCoT prompting</td>
<td style="text-align: center;">$\mathbf{1 . 4 1 2}$</td>
<td style="text-align: center;">$\mathbf{1 . 8 6 9}$</td>
<td style="text-align: center;">$\mathbf{1 . 8 7 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Relative Improvement</td>
<td style="text-align: center;">$\mathbf{1 5 . 2 7 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 . 6 6 \%}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 9 0 \%}$</td>
</tr>
</tbody>
</table>
<p>Answer to RQ1: SCoT prompting achieves higher accuracy than baselines on three benchmarks. In terms of Pass@1, SCoT prompting outperforms the SOTA baseline by up to $13.79 \%$ in HumanEval, $12.31 \%$ in MBPP, and $6.63 \%$ in MBCPP. The significant improvements show the effectiveness of SCoT prompting in code generation.</p>
<h3>4.2 RQ2: Do developers prefer programs generated by SCoT prompting?</h3>
<p>The ultimate goal of code generation is to assist developers in writing programs. In this RQ, we hire 10 developers (including industry employees and academic researchers) to manually review the programs generated by SCoT prompting and baselines.</p>
<p>Setup. To ensure the fairness of evaluation, we follow settings of human evaluation in previous studies [10, 14]. We have carefully checked the evaluation settings and think our settings are reliable. Specifically, we manually evaluate generated programs in the following aspects:</p>
<ul>
<li>Correctness (whether the program satisfies the requirement). 0 point: the program is totally inconsistent with the requirement. 1 point: the program is implemented, but misses some details. 2 points: the program is correctly implemented.</li>
<li>Code Smell (whether the program contains bad code smell). 0 point: There is a serious code smell. 1 point: some details are not</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Two programs generated by few-shot prompting and SCoT prompting, respectively.
in place. There is code smell of low severity. 2 points: the details are in place. No obviously better code in terms of performance exists. If possible, resources are released accordingly. No obvious code smell.</p>
<ul>
<li>Maintainability (whether the implementation is standardized and has good readability). 0 point: the program does not follow a consistent specification, or there are many meaningless names in variable naming, or there are certain repetitions and redundant code. 1 point: the program implementation meets certain specifications. But some variable names can be further refined. 2 points: the program implementation is relatively standardized. The variable naming is basically semantically straightforward, and the readability is good.
We explain the above aspects to evaluators through some examples. We also discuss with evaluators and set the score of each aspect to an integer, ranging from 0 to 2 (from bad to good). For SCoT prompting and baselines, we select a fixed LLM as the base model (i.e., ChatGPT) and collect 200 generated programs per approach. Finally, we obtain 800 programs for evaluation. We invite 10 developers with 3-5 years of development experience to evaluate the programs in the form of a questionnaire. The evaluators include industry employees and academic researchers that are not co-authors of this paper. The 800 programs are divided into 5 groups, with each questionnaire containing one group. The programs are randomly shuffled and anonymously reviewed by evaluators. Each group is evaluated by two evaluators, and the final score is the average of two evaluators' scores. Evaluators are allowed to search the Internet for unfamiliar concepts.</li>
</ul>
<p>Results. The results of the human evaluation are shown in Table 3. The numbers in red denote SCoT prompting's relative improvements compared to the SOTA baseline - CoT prompting. All the $p$-values are substantially smaller than 0.05 .</p>
<p>Analyses. SCoT prompting achieves the highest scores in all three aspects among all baselines. Specifically, SCoT prompting outperforms the SOTA baseline - CoT prompting by $15.27 \%$ in correctness, $10.66 \%$ in code smell, and $15.90 \%$ in maintainability.</p>
<p>We attribute the improvements to our proposed SCoT. The SCoT constrains LLMs to use program structures to generate intermediate reasoning steps. It allows LLMs to explore diverse solutions with three basic structures, improving the correctness of the code. Then, based on the SCoT, LLMs focus on implementing a program in a standardized way. Thus, the generated programs contain fewer code smells than ones from baselines.</p>
<p>Figure 5 shows two programs generated by SCoT prompting and few-shot prompting, respectively. Both programs pass unit tests. But the program from few-shot prompting contains a very complex statement highlighted in Figure 5). Developers have to spend lots of effort to understand and maintain this program. In contrast, the program from SCoT prompting has good readability, and the SCoT clearly explains the behavior of the code. Developers can further use the SCoT as comments of the program for future maintenance.</p>
<p>Answer to RQ2: Human developers prefer programs generated by SCoT prompting. Specifically, SCoT prompting outperforms the SOTA baseline by $19.93 \%$ in correctness, $11.25 \%$ in code smell, and $16.17 \%$ in maintainability. A case study also shows the program from SCoT prompting is easy to read and maintain.</p>
<h3>4.3 RQ3: Is SCoT prompting robust to examples?</h3>
<p>As stated in Section 2.3, SCoT prompting requires manually written examples to make prompts. In practice, people may write different examples, which makes the performance of SCoT prompting varies. Thus, in this RQ, we explore the robustness of SCoT prompting to examples.</p>
<p>Setup. As stated in Section 2.3, we select some <requirement, code> pairs as example seeds and manually write SCoTs for them, obtaining examples in prompts. In this RQ, we measure the robustness of SCoT prompting to examples in two aspects, i.e., seed selection and writing style.
(1) Seed Selection. It aims to validate SCoT prompting does not rely on specific seeds. We select three groups of <requirement, code> pairs as seeds and ask an annotator to write SCoTs for them. Then, we obtain three groups of examples. We measure the performance of SCoT prompting with different groups of examples. (2) Writing Style. People have different writing styles. It aims to validate that SCoT prompting does not rely on specific writing styles. We hire three annotators to independently write SCoTs for the same example seed, and obtain three groups of examples. Annotator A is a Ph.D. student in software engineering. Annotator B is a product manager from the industry. Annotator C is a developer from the industry. Then, we measure the performance of SCoT prompting with different annotators.</p>
<p>For comparison, we also measure the robustness of CoT prompting in the above settings. We select ChatGPT as the base model and conduct evaluations in HumanEval.</p>
<p>Results. The results are shown in Table 5 and 6, respectively.
Analyses. SCoT prompting is robust to examples. As shown in Table 5 and 6, SCoT prompting substantially outperforms CoT prompting when using different example seeds or annotators. It validates that SCoT prompting does not depend on specific seeds or writing styles. It also shows that the improvements of SCoT</p>
<p>Table 4: The results of ablation study. The base model is ChatGPT.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompting Technique</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBCPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
</tr>
<tr>
<td style="text-align: center;">CoT prompting</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">41.83</td>
<td style="text-align: center;">51.04</td>
<td style="text-align: center;">54.57</td>
<td style="text-align: center;">53.51</td>
<td style="text-align: center;">63.84</td>
<td style="text-align: center;">67.03</td>
</tr>
<tr>
<td style="text-align: center;">SCoT prompting</td>
<td style="text-align: center;">60.64</td>
<td style="text-align: center;">73.53</td>
<td style="text-align: center;">77.32</td>
<td style="text-align: center;">46.98</td>
<td style="text-align: center;">55.31</td>
<td style="text-align: center;">58.36</td>
<td style="text-align: center;">57.06</td>
<td style="text-align: center;">65.70</td>
<td style="text-align: center;">68.70</td>
</tr>
<tr>
<td style="text-align: center;">w/o Basic structures</td>
<td style="text-align: center;">55.67</td>
<td style="text-align: center;">70.94</td>
<td style="text-align: center;">76.13</td>
<td style="text-align: center;">43.36</td>
<td style="text-align: center;">53.64</td>
<td style="text-align: center;">56.57</td>
<td style="text-align: center;">54.79</td>
<td style="text-align: center;">64.32</td>
<td style="text-align: center;">67.77</td>
</tr>
<tr>
<td style="text-align: center;">w/o IO structure</td>
<td style="text-align: center;">59.65</td>
<td style="text-align: center;">72.79</td>
<td style="text-align: center;">77.12</td>
<td style="text-align: center;">46.13</td>
<td style="text-align: center;">54.76</td>
<td style="text-align: center;">57.88</td>
<td style="text-align: center;">56.61</td>
<td style="text-align: center;">65.01</td>
<td style="text-align: center;">68.42</td>
</tr>
</tbody>
</table>
<p>Table 5: The Pass@k of CoT prompting and SCoT prompting with different example seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Seed</th>
<th style="text-align: center;">CoT prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SCoT prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
</tr>
<tr>
<td style="text-align: left;">Seed A</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">$\mathbf{6 0 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5 3}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Seed B</td>
<td style="text-align: center;">52.81</td>
<td style="text-align: center;">68.97</td>
<td style="text-align: center;">74.55</td>
<td style="text-align: center;">$\mathbf{6 0 . 2 7}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 1 1}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 1 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Seed C</td>
<td style="text-align: center;">51.36</td>
<td style="text-align: center;">67.44</td>
<td style="text-align: center;">73.62</td>
<td style="text-align: center;">$\mathbf{5 9 . 3 6}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 8 8}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 7 9}$</td>
</tr>
</tbody>
</table>
<p>Table 6: The Pass@k of CoT prompting and SCoT prompting with different annotators.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator</th>
<th style="text-align: center;">CoT prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SCoT prompting</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
</tr>
<tr>
<td style="text-align: left;">Annotator A</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">$\mathbf{6 0 . 6 4}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 5 3}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 3 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Annotator B</td>
<td style="text-align: center;">51.43</td>
<td style="text-align: center;">67.92</td>
<td style="text-align: center;">73.44</td>
<td style="text-align: center;">$\mathbf{5 9 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 1 6}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Annotator C</td>
<td style="text-align: center;">52.18</td>
<td style="text-align: center;">68.45</td>
<td style="text-align: center;">74.71</td>
<td style="text-align: center;">$\mathbf{6 0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{7 3 . 1 5}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 2 4}$</td>
</tr>
</tbody>
</table>
<p>prompting come from the program structures instead of specific details in examples.</p>
<p>We also notice that there are slight variances in the performance of SCoT prompting under different settings. It is expected for prompting techniques using examples. Similar variances can be found in SCoT prompting, and SCoT prompting still outperforms CoT prompting in different settings.</p>
<p>Answer to RQ3: SCoT prompting is robust to examples. Under different example seeds or writing styles, SCoT prompting substantially outperforms the SOTA baseline - CoT prompting.</p>
<h3>4.4 RQ4: What are the contributions of different program structures in SCoT prompting?</h3>
<p>As stated in Section 2.1, SCoT prompting introduces basic structures (i.e., sequence, branch, and loop) and the input-output structure. This RQ is designed to analyze the contributions of different program structures.</p>
<p>Setup. We select ChatGPT as the base model. Then, we conduct an ablation study by independently removing basic structures and the input-output (IO) structure. When removing basic structures, we use a CoT with an IO structure as the intermediate steps. When removing the IO structure, the SCoT only contains a solving process with basic structures. We select ChatGPT as the base model.</p>
<p>Results. The results are shown in Table 4. "w/o" is the abbreviation of without.</p>
<p>Analyses. (1) Three basic structures are beneficial to design a feasible solving process. In Table 4, after removing basic structures,</p>
<p>SCOT prompting without basic structures:
Input: arry: list[list]
Output: result: int or float</p>
<ol>
<li>Initialize a result with -999999</li>
<li>Iterate through the list of lists</li>
<li>Calculate the sum of the list</li>
<li>Update the result with the maximum of sum and result</li>
<li>Return the result</li>
</ol>
<p>SCOT prompting:
Input: arry: list[list]
Output: result: int or float
1: Initialize a result with -999999
2: for _list in the list of lists:
3: Calculate the sum of the _list
4: Update the result with the maximum of sum and result
5: return the result</p>
<p>Figure 6: The comparison of SCoT prompting and SCoT prompting without basic structures.
the performance of SCoT prompting drops obviously. We carefully inspect failed cases and find that LLMs benefit from using basic structures to clearly write a solving process. Figure 6 shows the intermediate steps of SCoT prompting and SCoT prompting without basic structures. SCoT prompting without basic structures uses CoTs, which sequentially describe how to write the code line by line and contain many ambiguities. For example, the scopes of two iterations on lines 2 and 4 are unclear. LLMs are likely to misunderstand the CoT and generate incorrect code. In contrast, SCoT prompting uses three basic structures to describe the solving process. The SCoT is clear and is similar to code, benefiting the following code implementation.
(2) The IO structure benefits the requirement understanding. In Table 4, after deleting the IO structure, the performance of SCoT prompting has a slight decrease. We analyze failed cases and think the IO structure benefits the requirement understanding. Figure 7 shows two programs from SCoT prompting and SCoT prompting without the IO structure. We can see that SCoT prompting without the IO structure wrongly understands the output format and generates an incorrect program. After adding the IO structure, LLMs first reason about the input-output format and correctly return a boolean value.</p>
<p>Table 7: The comparison of SCoT-P prompting and SCoT prompting. The numbers in red denote SCoT prompting's relative improvements compared to SCoT-P prompting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Approach</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBCPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
<td style="text-align: center;">Pass@1</td>
<td style="text-align: center;">Pass@3</td>
<td style="text-align: center;">Pass@5</td>
</tr>
<tr>
<td style="text-align: center;">CoT prompting</td>
<td style="text-align: center;">53.29</td>
<td style="text-align: center;">69.76</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">41.83</td>
<td style="text-align: center;">51.04</td>
<td style="text-align: center;">54.57</td>
<td style="text-align: center;">53.51</td>
<td style="text-align: center;">63.84</td>
<td style="text-align: center;">67.03</td>
</tr>
<tr>
<td style="text-align: center;">SCoT-P prompting</td>
<td style="text-align: center;">55.23</td>
<td style="text-align: center;">70.33</td>
<td style="text-align: center;">75.94</td>
<td style="text-align: center;">43.28</td>
<td style="text-align: center;">52.16</td>
<td style="text-align: center;">55.77</td>
<td style="text-align: center;">54.25</td>
<td style="text-align: center;">64.09</td>
<td style="text-align: center;">67.78</td>
</tr>
<tr>
<td style="text-align: center;">SCoT prompting</td>
<td style="text-align: center;">60.64</td>
<td style="text-align: center;">73.53</td>
<td style="text-align: center;">77.32</td>
<td style="text-align: center;">46.98</td>
<td style="text-align: center;">55.31</td>
<td style="text-align: center;">58.36</td>
<td style="text-align: center;">57.06</td>
<td style="text-align: center;">65.70</td>
<td style="text-align: center;">68.70</td>
</tr>
<tr>
<td style="text-align: center;">Relative Improvement</td>
<td style="text-align: center;">9.80\%</td>
<td style="text-align: center;">4.55\%</td>
<td style="text-align: center;">1.82\%</td>
<td style="text-align: center;">8.55\%</td>
<td style="text-align: center;">6.04\%</td>
<td style="text-align: center;">4.64\%</td>
<td style="text-align: center;">5.18\%</td>
<td style="text-align: center;">2.51\%</td>
<td style="text-align: center;">1.36\%</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: The comparison of SCoT prompting and SCoT prompting without the IO structure.</p>
<p>Answer to RQ3: The input-output structure helps LLMs understand requirements and improves ChatGPT by up to $6.37 \%$ in Pass@1. Three basic structures are beneficial to clearly describe a solving process and improve ChatGPT by up to $12.73 \%$ in Pass@1.</p>
<h2>5 DISCUSSION</h2>
<h3>5.1 SCoT vs. Pseudocode</h3>
<p>We notice that the SCoT is similar to the pseudocode. The SCoT and pseudocode both contain an input-output structure and a solving process. We randomly select 100 generated SCoTs and manually review them. We find that $26 \%$ of SCoTs are very close to the pseudocode. On one hand, we think the similarity enhances the usability of our approach. For example, users can quickly know the behavior of a program based on its SCoT. The SCoT also can be inserted into the comment and benefits future maintenance. On the other hand, the majority of SCoTs ( $74 \%$ ) are different from the pseudocode because they are more abstract. Specifically, SCoTs tend to use natural languages to summarize an operation, e.g., cal aluate the sum of list1. But the pseudocode contains more implementation details, e.g., sum $\leftarrow 0$; for i in list1: sum $\leftarrow$ sum $+i$;</p>
<p>Compared to the pseudocode, we think the SCoT is a better choice for intermediate steps. Because a SCoT naturally decomposes code generation into two steps. LLMs first focus on exploring diverse solutions and then implement a program in a standardized way. To validate this point, we design a variant of SCoT prompting,
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: The complementarity between CodeT and SCoT prompting.
named SCoT-P Prompting. It is similar to SCoT prompting, but considers the pseudocode as intermediate steps. We apply SCoT-P Prompting and SCoT prompting to ChatGPT and measure their accuracy. The results are shown in Table 7. SCoT prompting substantially outperforms SCoT-P Prompting on three benchmarks. The improvements show the superiority of our SCoT.</p>
<h3>5.2 SCoT prompting vs. Rank Techniques</h3>
<p>Some recent studies [6, 12] propose rank techniques to improve the performance of LLMs on code generation. Given a requirement, they first sample many programs from LLMs and then use test cases or neural networks to rerank sampled programs. For example, CodeT [6] is a popular rank technique. CodeT does large-scale sampling and executes sampled programs on auto-generated test cases. Based on execution results, the programs are reranked. In this paper, we do not directly compare our approach to rank techniques due to two reasons.
(1) SCoT prompting and rank techniques have different focuses, and they are complementary. Our work aims to design a new prompting technique and improve the accuracy of LLMs in code generation. Rank techniques do not care about LLMs and aim to pick the best one from LLMs' multiple outputs. In practice, users can use SCoT prompting to generate many programs and then use rank techniques to pick a final output.</p>
<p>To verify the complementarity between SCoT prompting and rank techniques, we conduct an exploratory experiment. We select ChatGPT as a base model and progressively introduce CodeT and SCoT prompting. The results on MBPP are shown in Figure 8. We</p>
<p>can see that the performance of ChatGPT is continually improved by adding CodeT and SCoT prompting.
(2) Rank techniques approaches rely on execution environments. Rank techniques require executing programs on test cases and using execution results to rerank programs. In many realistic programming scenarios, users want to get code suggestions for an unfinished project. It is infeasible to execute auto-generated programs. Thus, we think rank techniques have limited application scenarios and make additional use of the execution results. Our approach works in a general scenario and does not use execution results. Thus, it is unfair to directly compare SCoT prompting to rank techniques.</p>
<h3>5.3 Threats to Validity</h3>
<p>There are three main threats to the validity of our work.
(1) The generalizability of experimental results. To mitigate this threat, we carefully select the benchmarks, metrics, and baselines. Following previous studies [1, 2, 7], we pick three representative code generation benchmarks. They are hand-written or collected from real-world programming communities, and cover two popular languages (i.e., Python and C++ ). For evaluation metrics, we select a widely used metric - Pass@k, which utilizes test cases to check the correctness of programs. We use the unbiased Pass@ $k$ which is more reliable [7]. For comparison baselines, we select the SOTA prompting techniques and conduct a comprehensive comparison in Section 4. SCoT prompting and baselines have the same example seeds and maximum generation lengths.
(2) The impact of the two-step pipeline. CoT prompting generates a CoT and the code in one step. Our SCoT prompting generates the code in two steps. It first generates SCoTs and then generates the code. It is possible that the improvements come from the two-step pipeline. To solve this threat, we have two considerations. First, LLMs in our experiments are auto-regressive language models. For an auto-regressive language model, a one-step pipeline and a two-step pipeline are theoretically equivalent. Second, we conduct an ablation study in Section 4.4. We keep the two-step pipeline unchanged and remove program structures. The results in Table 4 show that SCoT prompting without prompt structures has a significant drop in the Pass@k. It shows that the improvements of SCoT prompting are brought by program structures instead of the two-step pipeline.
(3) The data leakage. Existing LLMs are trained with extensive code files from open-source communities. It is possible that their training data contains the experimental benchmarks, leading to data leakage. But we think that it does not affect the fairness of our experiments. In this paper, we select a specific LLM (e.g., ChatGPT) as the base model and apply different prompting techniques to it. Thus, the reported relative improvements between baselines and our approach are credible. In the future, we will add the latest benchmarks to alleviate this threat.</p>
<h2>6 RELATED WORK</h2>
<p>Large language models (LLMs) for Source Code are large-scale neural networks that are pre-trained with a large corpus consisting of natural language text and source code. Nowadays, LLMs for source code have been expanding and can be divided into two categories: standard language models and instruction-tuned models.</p>
<p>Standard Language models are pre-trained on a large-scale corpus with the next-token prediction objective. They are mainly used to continually complete the given context, such as code completion. After the success of GPT series [4, 23, 24] in NLP, OpenAI fine-tunes GPT models on code to produce closed-source Codex [7]. There follow many open-source replication attempts, e.g., CodeParrot [29], CodeGen [17], CodeGeeX [40], InCoder [8], StarCoder [15] and CodeT5+ [33].</p>
<p>Instruction-tuned models are models after instruction tuning [34]. Instruction tuning trains models to understand human users' instructions and perform tasks by following instructions. ChatGPT [18] is trained with human feedback [20], powerful on both natural language tasks and programming tasks. Many attempt to train an "open-source ChatGPT". Alpaca [27] is LLaMA [28] tuned using self-instruct [32] and ChatGPT feedback. Code Alpaca [5] is LLaMA tuned using self-instruct and ChatGPT feedback, with instructions focusing on programming tasks. WizardCoder [16] is StarCoder [15] tuned using Evol-Instruct [36] and ChatGPT feedback with Code Alpaca's dataset as seed dataset. InstructCodeT5+ [33] is CodeT5+ [33] tuned on Code Alpaca's dataset.</p>
<p>Prompting Techniques. With the enormous number of parameters (e.g., Codex: 175 billion parameters), it is hard to directly fine-tune LLMs on code generation. Prompting techniques are a popular approach, which leverages LLMs to generate code by inputting a special prompt.</p>
<p>Early, researchers proposed zero-shot prompting and few-shot prompting. Zero-shot prompting concatenates a task instruction (e.g., please generate a program based on the requirement) and a requirement together, making a prompt. Based on the zeroshot prompting, few-shot prompting further adds several (requirement, code) pairs to the prompts, so that LLMs can learn code generation from given examples.</p>
<p>The Chain-of-Thought (CoT) prompting [35] is a recently proposed prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the final code. It allows LLMs to first design a solving process that leads to the code. CoT prompting has achieved the SOTA results in natural language generation and sparked lots of follow-up research, such as self-consistency prompting [31], least-to-most prompting [41]. But these prompting techniques are designed for natural language generation and bring slight improvements in code generation.</p>
<p>In this paper, we propose a novel prompting technique named Structured Chain-of-Thought (SCoT) prompting. Different from standard CoT prompting, SCoT prompting explicitly introduces program structures and asks LLMs to generate intermediate reasoning steps with program structures. We compare CoT prompting and SCoT prompting in Section 4. The results show that SCoT prompting significantly outperforms CoT prompting in three benchmarks.</p>
<h2>7 CONCLUSION AND FUTURE WORK</h2>
<p>Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting is the state-of-the-art (SOTA) approach to generating code. It first generates a CoT and then outputs the code. A CoT is several intermediate natural language reasoning steps. However, CoT prompting still has low accuracy in code generation. This paper</p>
<p>proposes a Structured CoT (SCoT) and presents a new prompting technique for code generation, named SCoT prompting. SCoT prompting asks LLMs to generate a SCoT using program structures (i.e., sequence, branch, and loop structures). Then, LLMs generate the code based on the SCoT. A large-scale study on three benchmarks shows that SCoT prompting significantly outperforms CoT prompting in Pass@k and human evaluation. Besides, SCoT prompting is robust to examples and obtains stable improvements.</p>
<p>In the future, we will explore new prompting techniques for code generation. For example, source code can be represented by a tree (e.g., abstract syntax tree). We can design a tree-based prompting technique, which uses LLMs to generate a tree.</p>
<h2>REFERENCES</h2>
<p>[1] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wan Uddin Ahmad, Shisji Wang, Qing Sun, Mingyue Shang, et al. 2022. Multi-lingual Evaluation of Code Generation Models. arXiv preprint arXiv:2210.14868 (2022).
[2] Jacob Austin, Augustine Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).
[3] Corrado Böhm and Giuseppe Jacopini. 1966. Flow diagrams, turing machines and languages with only two formation rules. Commun. ACM 9, 5 (1966), 366-371. https://doi.org/10.1145/355592.365646
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulia Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
[5] Sahil Chaudhary. 2025. Code Alpaca: An Instruction-following LLaMA model for code generation. https://github.com/sahil280114/codealpaca.
[6] Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022. CodeT: Code Generation with Generated Tests. https://doi.org/10.48550/ARXIV.2207.10397
[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qining Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fottos Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murali, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained in Code. (2021). arXiv:2107.03374 [cs.LG]
[8] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Riusji Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=bQwbB36dEL
[9] GitHub. 2022. GitHub Copilot. https://github.com/features/copilot.
[10] Yiyang Hao, Ge Li, Yongqiang Liu, Xiaowei Miao, He Zong, Siyuan Jiang, Yang Liu, and He Wei. 2022. AixBench: A Code Generation Benchmark Dataset. CoRR abs/2206.13179 (2022). https://doi.org/10.48550/arXiv.2206.13179 arXiv:2206.13179
[11] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The Curious Case of Neural Text Degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=rygGQyzFvH
[12] Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnación, Shizendu Lahiri, Madanlal Musuvathi, and Jianfeng Gao. 2022. Faultaware neural code rankers. Advances in Neural Information Processing Systems 35 (2022), 13419-13432.
[13] Jia Li, Ge Li, Zhuo Li, Zhi Jin, Xing Hu, Kechi Zhang, and Zhiyi Fu. 2023. CodeEditor: Learning to Edit Source Code with Pre-Trained Models. ACM Trans. Softw. Eng. Methodol. (may 2023). https://doi.org/10.1145/3597207 Just Accepted.
[14] Jia Li, Yongmin Li, Ge Li, Zhi Jin, Yiyang Hao, and Xing Hu. 2023. SkCoder: A Sketch-based Approach for Automatic Code Generation. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023. IEEE, 2124-2135. https://doi.org/10.1109/ICSE48619.2023.00179
[15] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. StarCoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023).
[16] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Hu, Qingwei Lin, and Dazze Jiang. 2023. WizardCoder: Empowering Code Large Language Models with Evol-Instruct. arXiv preprint arXiv:2306.08568 (2023).
[17] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv preprint (2022).
[18] OpenAI. 2022. ChatGPT. https://openai.com/blog/chatgpt.
[19] OpenAI. 2022. Codex. https://beta.openai.com/docs/api-reference/completions.
[20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems 35 (2022), 27730-27744.
[21] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311-318.
[22] Han Peng, Ge Li, Wenhan Wang, Yunfei Zhao, and Zhi Jin. 2021. Integrating Tree Path in Transformer for Code Representation. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 9345-9354. https://proceedings.neuropa.cc/paper/2021/ hash/4e0223a87610176ef0d24ef6d2dcde3a-Abstract.html
[23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. (2018).
[24] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[25] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence Level Training with Recurrent Neural Networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.06732
[26] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies, NAACLHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, 4149-4158. https://doi.org/10.18653/v1/n19-1421
[27] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatno-lab/stanford_ alpaca.
[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971 (2023).
[29] Lewis Tunstall, Leandro Von Weera, and Thomas Wolf. 2022. Natural language processing with transformers. ${ }^{\circ}$ CI Reilly Media, Inc. ${ }^{\circ}$
[30] Wenhan Wang, Ge Li, Sijie Shen, Xin Xia, and Zhi Jin. 2020. Modular Tree Network for Source Code Representation Learning. 29, 4, Article 31 (sep 2020), 23 pages. https://doi.org/10.1145/3409331
[31] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id=1PLiNfMMrw
[32] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, 13484-13508. https://aclanthology.org/2023.acl-long. 754
[33] Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hsu. 2023. CodeO+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922 (2023).
[34] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. https://openreview.net/forum?id=gEZrGCozdqR
[35] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun</p>
<p>Cho (Eds.). https://openreview.net/forum?id=_VjQlMeSB_J
[36] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Greng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).
[37] Jian Zhang, Xu Wang, Hongyu Zhang, Haifong Sun, Kaixuan Wang, and Xudong Liu. 2019. A novel neural source code representation based on abstract syntax tree. In Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, Joanne M. Atlee, Terifik Bułtan, and Jon Whittle (Eds.). IEEE / ACM, 783-794. https://doi.org/10.1109/ICSE.2019.00086
[38] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023. Self-Edit: Fault-Aware Code Editor for Code Generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naouki Okazaki (Eds.). Association for Computational Linguistics, 769-787.
https://doi.org/10.18653/v1/2023.acl-long. 45
[39] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning. PMLR, 12697-12706.
[40] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023. CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. arXiv:2303.17568 [cs.LG]
[41] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H. Chi. 2023. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/pdf?id= WZH7099tgfM</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
Conference'17, July 2017, Washington, DC, USA
(C) 2023 Association for Computing Machinery.</p>
<p>ACM ISBN 978-x-xxxx-xxxx-x/YY/MM... \$15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
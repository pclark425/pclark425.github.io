<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4183 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4183</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4183</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-280416781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.00033v2.pdf" target="_blank">GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized. This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the \textit{ParShift} library, and synthetic data generation and clustering using \textit{pyclugen} and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts specifying detailed requirements but omitting in-context examples. Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails. Results show that only a small subset of models consistently generate correct, executable code. GPT-4.1 achieved a 100\% success rate across all runs in both experimental tasks, whereas most other models succeeded in fewer than half of the runs, with only Grok-3 and Mistral-Large approaching comparable performance. In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs. Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4183.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4183.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-CoI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-CoI</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that integrates external structured and unstructured domain-specific knowledge into an LLM pipeline to support more reliable scientific hypothesis generation and reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving scientific hypothesis generation with knowledge grounded large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KG-CoI (knowledge-grounded LLM augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates external structured and unstructured domain-specific knowledge sources into an LLM's generation process and includes hallucination-detection mechanisms to improve the factual accuracy and relevance of automatically generated hypotheses; described as a knowledge-grounding augmentation rather than a novel LLM architecture. The paper describes KG-CoI at a high level (knowledge integration + hallucination detection) but does not specify detailed model architectures, parameter counts, or exact prompting templates in the cited text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific research / hypothesis generation (applied to domain-specific tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Hypothesis/principle generation and knowledge-grounded relationships (qualitative hypotheses rather than explicit quantified laws described in the citation)</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Grounding an LLM with external structured and unstructured domain knowledge (knowledge graph or other curated resources) and using the grounded model to synthesize hypotheses and relevant relationships from the provided knowledge context.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Reported evaluation of generated hypotheses using measures of factuality and relevance; the paper states KG-CoI reduces factual errors and increases relevance of generated hypotheses (comparative evaluation against ungrounded LLM behavior is implied in the description).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative claims in the text: 'significantly reduces factual errors and increases the relevance of generated hypotheses.' No numerical accuracy/precision/recall values are provided in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Described limitations include LLM hallucinations and difficulty reliably linking hypotheses to evidence without external knowledge grounding; the citation is discussed as a mitigation for these failure modes but does not claim extraction of explicit quantitative laws from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively against unaugmented/standard LLM generation (an ungrounded LLM baseline); reported improvements in factuality and relevance versus that baseline (no precise numeric baseline reported in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4183.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4183.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PiFlow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PiFlow (Principle-aware multi-agent discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM framework that integrates domain-specific scientific principles into the discovery/hypothesis generation process to improve solution quality in scientific discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Piflow: Principle-aware scientific discovery with multi-agent collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PiFlow (multi-agent, principle-aware LLM framework)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A multi-agent framework where multiple LLM agents collaborate and are guided by encoded domain principles (domain-specific scientific principles) to produce hypotheses/solutions; the system emphasizes integrating scientific principles into the LLM-driven discovery pipeline to better connect hypotheses with evidence. The paper summary reports a multi-agent LLM orchestration and principle integration approach rather than low-level architecture details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Applied scientific discovery (examples given: nanomaterials and biomolecular research)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Principle- and hypothesis-level scientific relationships (principle-aware discovery); described as improving discovery of domain-relevant relationships rather than explicitly producing closed-form quantitative laws in the cited summary.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Multi-agent LLM collaboration augmented with encoded domain principles; agents generate, critique, and refine hypotheses guided by principle encodings and evidence sources from the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Empirical comparison of solution quality against a single LLM performing the discovery process independently; reported domain-specific application tests (nanomaterials and biomolecular research) used to evaluate gains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: 'PiFlow achieved a 94% improvement in solution quality compared to a single LLM conducting the discovery process independently' (as stated in the paper's background summary). No detailed precision/recall/F1 numbers for law extraction are provided in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported improvement metric: 94% improvement in solution quality vs. single-LLM baseline (the paper summary does not provide an explicit percent 'success rate' for extracted laws).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Noted broader limitations of LLMs include difficulty consistently linking hypotheses with evidence; PiFlow is presented as a mitigation but the summary does not claim general extraction of explicit quantitative laws from literature and does not enumerate failure modes for PiFlow itself in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to a single-LLM baseline performing the discovery task; the cited result reports a 94% improvement in 'solution quality' over that baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4183.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4183.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system/toolkit that uses off-the-shelf LLMs to generate literature reviews from an abstract, aiming to reduce human time and effort in summarizing scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Litllm: A toolkit for scientific literature review</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LitLLM (literature-review generation toolkit)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An approach that takes an abstract as input and uses one or more off-the-shelf LLMs to synthesize a literature review; the system automates distillation of key points across literature to produce a coherent review. The cited description emphasizes process automation and time savings rather than low-level model architecture or equation-extraction pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Scientific literature across domains (tool described as general-purpose for literature review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Distillation of patterns, themes, and synthesized findings from literature (qualitative summarization); the cited description does not state explicit extraction of quantitative laws or numeric relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Natural-language summarization using off-the-shelf LLMs driven from an abstract input to generate a literature review; method focuses on text-summarization and synthesis rather than parsing numeric tables or extracting formal equations as described in the background summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Reported reduction in time and effort compared to traditional literature review methods (empirical user-time / labor comparisons implied in summary); no detailed quantitative law-validation protocol described in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative claim: 'LitLLM substantially reduces time and effort for literature review compared to traditional methods.' No accuracy/precision/recall metrics for quantitative extraction are reported in the summary.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The summary does not claim LitLLM extracts formal quantitative laws; implicit limitations include risk of hallucination and the need for human oversight in literature synthesis. No explicit failure-mode statistics are provided in the paper summary.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to traditional manual literature review in terms of time and effort; reported to substantially reduce required time/effort but no numeric evaluation of quantitative-relationship extraction is given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Improving scientific hypothesis generation with knowledge grounded large language models <em>(Rating: 2)</em></li>
                <li>Piflow: Principle-aware scientific discovery with multi-agent collaboration <em>(Rating: 2)</em></li>
                <li>Litllm: A toolkit for scientific literature review <em>(Rating: 2)</em></li>
                <li>LLM4SR: A survey on large language models for scientific research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4183",
    "paper_id": "paper-280416781",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "KG-CoI",
            "name_full": "KG-CoI",
            "brief_description": "A system that integrates external structured and unstructured domain-specific knowledge into an LLM pipeline to support more reliable scientific hypothesis generation and reduce hallucinations.",
            "citation_title": "Improving scientific hypothesis generation with knowledge grounded large language models",
            "mention_or_use": "mention",
            "system_name": "KG-CoI (knowledge-grounded LLM augmentation)",
            "system_description": "Integrates external structured and unstructured domain-specific knowledge sources into an LLM's generation process and includes hallucination-detection mechanisms to improve the factual accuracy and relevance of automatically generated hypotheses; described as a knowledge-grounding augmentation rather than a novel LLM architecture. The paper describes KG-CoI at a high level (knowledge integration + hallucination detection) but does not specify detailed model architectures, parameter counts, or exact prompting templates in the cited text.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "General scientific research / hypothesis generation (applied to domain-specific tasks)",
            "number_of_papers": null,
            "law_type": "Hypothesis/principle generation and knowledge-grounded relationships (qualitative hypotheses rather than explicit quantified laws described in the citation)",
            "law_examples": "",
            "extraction_method": "Grounding an LLM with external structured and unstructured domain knowledge (knowledge graph or other curated resources) and using the grounded model to synthesize hypotheses and relevant relationships from the provided knowledge context.",
            "validation_approach": "Reported evaluation of generated hypotheses using measures of factuality and relevance; the paper states KG-CoI reduces factual errors and increases relevance of generated hypotheses (comparative evaluation against ungrounded LLM behavior is implied in the description).",
            "performance_metrics": "Qualitative claims in the text: 'significantly reduces factual errors and increases the relevance of generated hypotheses.' No numerical accuracy/precision/recall values are provided in this paper's summary.",
            "success_rate": "",
            "challenges_limitations": "Described limitations include LLM hallucinations and difficulty reliably linking hypotheses to evidence without external knowledge grounding; the citation is discussed as a mitigation for these failure modes but does not claim extraction of explicit quantitative laws from literature.",
            "comparison_baseline": "Compared qualitatively against unaugmented/standard LLM generation (an ungrounded LLM baseline); reported improvements in factuality and relevance versus that baseline (no precise numeric baseline reported in this paper's summary).",
            "uuid": "e4183.0",
            "source_info": {
                "paper_title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "PiFlow",
            "name_full": "PiFlow (Principle-aware multi-agent discovery)",
            "brief_description": "A multi-agent LLM framework that integrates domain-specific scientific principles into the discovery/hypothesis generation process to improve solution quality in scientific discovery tasks.",
            "citation_title": "Piflow: Principle-aware scientific discovery with multi-agent collaboration",
            "mention_or_use": "mention",
            "system_name": "PiFlow (multi-agent, principle-aware LLM framework)",
            "system_description": "A multi-agent framework where multiple LLM agents collaborate and are guided by encoded domain principles (domain-specific scientific principles) to produce hypotheses/solutions; the system emphasizes integrating scientific principles into the LLM-driven discovery pipeline to better connect hypotheses with evidence. The paper summary reports a multi-agent LLM orchestration and principle integration approach rather than low-level architecture details.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Applied scientific discovery (examples given: nanomaterials and biomolecular research)",
            "number_of_papers": null,
            "law_type": "Principle- and hypothesis-level scientific relationships (principle-aware discovery); described as improving discovery of domain-relevant relationships rather than explicitly producing closed-form quantitative laws in the cited summary.",
            "law_examples": "",
            "extraction_method": "Multi-agent LLM collaboration augmented with encoded domain principles; agents generate, critique, and refine hypotheses guided by principle encodings and evidence sources from the domain.",
            "validation_approach": "Empirical comparison of solution quality against a single LLM performing the discovery process independently; reported domain-specific application tests (nanomaterials and biomolecular research) used to evaluate gains.",
            "performance_metrics": "Reported: 'PiFlow achieved a 94% improvement in solution quality compared to a single LLM conducting the discovery process independently' (as stated in the paper's background summary). No detailed precision/recall/F1 numbers for law extraction are provided in this summary.",
            "success_rate": "Reported improvement metric: 94% improvement in solution quality vs. single-LLM baseline (the paper summary does not provide an explicit percent 'success rate' for extracted laws).",
            "challenges_limitations": "Noted broader limitations of LLMs include difficulty consistently linking hypotheses with evidence; PiFlow is presented as a mitigation but the summary does not claim general extraction of explicit quantitative laws from literature and does not enumerate failure modes for PiFlow itself in detail.",
            "comparison_baseline": "Compared directly to a single-LLM baseline performing the discovery task; the cited result reports a 94% improvement in 'solution quality' over that baseline.",
            "uuid": "e4183.1",
            "source_info": {
                "paper_title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "LitLLM",
            "name_full": "LitLLM",
            "brief_description": "A system/toolkit that uses off-the-shelf LLMs to generate literature reviews from an abstract, aiming to reduce human time and effort in summarizing scientific literature.",
            "citation_title": "Litllm: A toolkit for scientific literature review",
            "mention_or_use": "mention",
            "system_name": "LitLLM (literature-review generation toolkit)",
            "system_description": "An approach that takes an abstract as input and uses one or more off-the-shelf LLMs to synthesize a literature review; the system automates distillation of key points across literature to produce a coherent review. The cited description emphasizes process automation and time savings rather than low-level model architecture or equation-extraction pipelines.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Scientific literature across domains (tool described as general-purpose for literature review generation)",
            "number_of_papers": null,
            "law_type": "Distillation of patterns, themes, and synthesized findings from literature (qualitative summarization); the cited description does not state explicit extraction of quantitative laws or numeric relationships.",
            "law_examples": "",
            "extraction_method": "Natural-language summarization using off-the-shelf LLMs driven from an abstract input to generate a literature review; method focuses on text-summarization and synthesis rather than parsing numeric tables or extracting formal equations as described in the background summary.",
            "validation_approach": "Reported reduction in time and effort compared to traditional literature review methods (empirical user-time / labor comparisons implied in summary); no detailed quantitative law-validation protocol described in this paper's summary.",
            "performance_metrics": "Qualitative claim: 'LitLLM substantially reduces time and effort for literature review compared to traditional methods.' No accuracy/precision/recall metrics for quantitative extraction are reported in the summary.",
            "success_rate": "",
            "challenges_limitations": "The summary does not claim LitLLM extracts formal quantitative laws; implicit limitations include risk of hallucination and the need for human oversight in literature synthesis. No explicit failure-mode statistics are provided in the paper summary.",
            "comparison_baseline": "Compared to traditional manual literature review in terms of time and effort; reported to substantially reduce required time/effort but no numeric evaluation of quantitative-relationship extraction is given.",
            "uuid": "e4183.2",
            "source_info": {
                "paper_title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Improving scientific hypothesis generation with knowledge grounded large language models",
            "rating": 2,
            "sanitized_title": "improving_scientific_hypothesis_generation_with_knowledge_grounded_large_language_models"
        },
        {
            "paper_title": "Piflow: Principle-aware scientific discovery with multi-agent collaboration",
            "rating": 2,
            "sanitized_title": "piflow_principleaware_scientific_discovery_with_multiagent_collaboration"
        },
        {
            "paper_title": "Litllm: A toolkit for scientific literature review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "LLM4SR: A survey on large language models for scientific research",
            "rating": 1,
            "sanitized_title": "llm4sr_a_survey_on_large_language_models_for_scientific_research"
        }
    ],
    "cost": 0.01550325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries
15 Sep 2025</p>
<p>Nuno Fachada 
Lusófona University
Campo Grande376, 1749-024Copelabs, LisboaPortugal</p>
<p>Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI)
2829-516CaparicaPortugal</p>
<p>Daniel Fernandes 
Lusófona University
Campo Grande376, 1749-024Copelabs, LisboaPortugal</p>
<p>Carlos M Fernandes 
Lusófona University
Campo Grande376, 1749-024Copelabs, LisboaPortugal</p>
<p>Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI)
2829-516CaparicaPortugal</p>
<p>Bruno D Ferreira-Saraiva 
Lusófona University
Campo Grande376, 1749-024Copelabs, LisboaPortugal</p>
<p>CICANT
Lusófona University
Campo Grande376, 1749-024LisboaPortugal</p>
<p>João P Matos-Carvalho 
Center of Technology and Systems (UNINOVA-CTS) and Associated Lab of Intelligent Systems (LASI)
2829-516CaparicaPortugal</p>
<p>Departamento de Informática
Faculdade de Ciências
LASIGE
Universidade de Lisboa
1749-016LisboaPortugal</p>
<p>GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries
15 Sep 2025A9246EFF675E4C2B612510F78FF1BC5A10.3390/fi17090412arXiv:2508.00033v2[cs.SE]Large Language ModelsCode generationPython libraries
Large Language Models (LLMs) have advanced rapidly as tools for automating code generation in scientific research, yet their ability to interpret and use unfamiliar Python APIs for complex computational experiments remains poorly characterized.This study systematically benchmarks a selection of state-of-the-art LLMs in generating functional Python code for two increasingly challenging scenarios: conversational data analysis with the ParShift library, and synthetic data generation and clustering using pyclugen and scikit-learn.Both experiments use structured, zeroshot prompts specifying detailed requirements but omitting in-context examples.Model outputs are evaluated quantitatively for functional correctness and prompt compliance over multiple runs, and qualitatively by analyzing the errors produced when code execution fails.Results show that only a small subset of models consistently generate correct, executable code.GPT-4.1 achieved a 100% success rate across all runs in both experimental tasks, whereas most other models succeeded in fewer than half of the runs, with only Grok-3 and Mistral-Large approaching comparable performance.In addition to benchmarking LLM performance, this approach helps identify shortcomings in third-party libraries, such as unclear documentation or obscure implementation bugs.Overall, these findings highlight current limitations of LLMs for end-to-end scientific automation and emphasize the need for careful prompt design, comprehensive library documentation, and continued advances in language model capabilities.</p>
<p>Introduction</p>
<p>The automation of computational experiments is a critical component of modern scientific research, enabling efficient exploration of hypotheses, reproducible analysis pipelines, and scalable data-driven discovery.In recent years, large language models (LLMs) have emerged as powerful tools capable of generating executable code from natural language descriptions [1][2][3][4], raising the possibility of automating significant portions of the experimental workflow with minimal human intervention [5][6][7].These advancements hold particular promise for researchers and practitioners</p>
<p>Background</p>
<p>This section reviews relevant literature and provides essential context on how LLMs are being used to support scientific research.It begins by describing a range of scientific tasks where LLMs have already proven useful, such as forming hypotheses, conducting peer-review and performing literature review.Then, it examines frameworks that convert natural-language protocols into executable workflows, manage experimental variables and constraints, and generate ready-torun code.The final subsection highlights advances in crafting effective prompts to steer model behavior and ensure consistent and reproducible outputs in scientific experimentation.</p>
<p>LLM in Research and Development</p>
<p>The integration of LLMs into scientific research introduces new workflows in research and development, wherein natural language processing systems assist or autonomously execute a range of scientific tasks, including protocol optimization, data analysis, and experimental design.</p>
<p>A key application of LLMs is automating hypothesis generation.In [13], for instance, Xiong et al. introduce KG-CoI, a system that integrates external structured and unstructured domainspecific knowledge from scientific resources into LLMs, including hallucination detection, to support more reliable scientific exploration.KG-CoI significantly reduces factual errors and increases the relevance of generated hypotheses.In [14], the authors propose PiFlow, a multi-agent LLM framework that integrates domain-specific scientific principles into hypothesis generation to overcome LLMs' difficulty in consistently linking hypotheses with evidence.PiFlow achieved a 94% improvement in solution quality compared to a single LLM conducting the discovery process independently, across applications in nanomaterials and biomolecular research.</p>
<p>Jin et al. [15] present AgentReview, a simulation framework where LLMs emulate the roles of reviewers, authors, and area chairs to analyze biases and decision variability in scientific peer review.By modeling latent sociological factors like authority bias and altruism fatigue, the study reveals that LLM-based agents can offer insights into improving the fairness and robustness of scientific evaluation systems.</p>
<p>In [16], Agarwal et al. introduce LitLLM, a system for generating literature reviews from an abstract using off-the-shelf LLMs.The authors show that LitLLM substantially reduces time and effort for literature review compared to traditional methods.</p>
<p>As the studies reviewed above attest, LLMs are already reshaping key phases of the scientific workflow.However, translating ideas into experiments still depends on detailed protocol planning and execution.In the following section, we discuss how LLM-based agents can draft detailed experimental protocols, and adapt workflows in real time, effectively turning high-level research designs into implementable laboratory action.</p>
<p>LLMs for Experiment Planning and Implementation</p>
<p>LLMs are increasingly being integrated into computational experiment design workflows, particularly through Python and its rich set of libraries.These pipelines automate repetitive tasks, shorten iteration times, and improve reproducibility [17].Recently, Charness et al. [7] argued that LLMs can or at least could soon be integrated into the design, implementation and analysis of social and economic experiments.The remainder of this subsection examines representative approaches to harnessing these capabilities for robust and scalable experiment planning and implementation.</p>
<p>One of the already established abilities of LLMs in assisting scientific research is code generation, either by producing ready-to-run code or by supporting researchers in the iterative development of scripts, pipelines, and analytical workflows.Over time, LLMs have moved beyond simple code snippets to domain-specific pipelines, enabling more sophisticated prototyping and documentation.</p>
<p>In [18], the authors adapt LLMs trained on code completion for writing robot policy code according to natural language prompts.The generated robot policies exhibit spatial-geometric reasoning and are able to prescribe precise values to ambiguous descriptions.By relying on a hierarchical prompting strategy, their approach is able to write more complex code and solve 39.8% of the problems on the HumanEval [1] benchmark.</p>
<p>Luo et al. [19] use LLMs to generate robot control programs, testing and optimizing the output in a simulation environment.After a number of optimization rounds, the robot control codes are deployed on a real robot for construction assembly tasks.The experiments show that their approach can improve the quality of the generated code, thus simplifying the robot control process and facilitating the automation of construction tasks.</p>
<p>In [4], the authors present a comparative evaluation of 16 LLMs in generating Python code for UAV placement and signal power calculations in LoRaWAN environments, demonstrating that state-of-the-art models like DeepSeek-V3 and GPT-4 consistently produce accurate solutions, while smaller, locally executable models such as Phi-4 and LLaMA-3.3 also show strong performance, highlighting the viability of lightweight alternatives.The study demonstrates the importance of domain-specific fine-tuning, offering valuable insights into the practical deployment of LLMs for specialized engineering tasks.</p>
<p>LLMs can write individual scripts, but full experimental design demands additional abstraction, planning, and domain expertise.It requires integrating hypotheses, protocol structure, data collection strategies, ethical considerations, and analysis plans, a complex orchestration far beyond simple script generation.</p>
<p>A method for evaluating LLMs' ability to generate and interpret experimental protocols is proposed in [5].The authors describe BioPlanner, a framework that converts natural language protocols into pseudocode and then assesses a model's ability to reconstruct those protocols using a predefined set of admissible pseudofunctions.This approach enables robust, automated evaluation of long-horizon planning tasks in biology, reducing reliance on manual review and paving the way for scalable scientific automation.</p>
<p>Hong et al. [6] introduce Data Interpreter, an LLM-based agent designed to automate endto-end data science workflows, addressing the limitations of existing LLMs in handling complex and dynamic data science tasks.The system decomposes complex data science tasks into graphbased subproblems and dynamically adapts workflows via programmable node generation.It embeds a confidence-driven verifier to flag and correct logical inconsistencies in generated code.In benchmarks like InfiAgent-DABench and MATH, it achieves roughly 25-26% gains over open source baselines.This is where our proposal takes shape: it evaluates how well an LLM can parse APIs and assemble complete computational experiments from a single prompt, without further human guidance.While this setup is not agentic in nature, it does bear a superficial resemblance to recent work on LLM-based agents, such as Hong et al.'s Data Interpreter [6], ReAct [20], or Auto-GPT [21].These frameworks emphasize multi-step reasoning, interactive planning, and iterative tool use, often allowing models to execute, verify, and refine their own outputs.In contrast, the methodology used in this paper deliberately isolates and benchmarks a model's intrinsic ability to interpret unfamiliar APIs and return fully functional code in a single pass, guided only by a Table 1: Minimal example of unstructured and structured prompts for the same programming task.The unstructured prompt is informal and open-ended, while the structured prompt specifies function details and expected input/output, illustrating the difference in detail and reproducibility.</p>
<p>Prompt Type Prompt Example</p>
<p>Unstructured How do I sum a list of whole numbers in Python?</p>
<p>Structured Write a Python function <code>sum_numbers(numbers: list[int]) -&gt; int</code>that returns the sum of a list of integers.</p>
<p>zero-shot structured prompt.In this sense, our study is complementary to agent research: by establishing a baseline of core code generation competence, it provides a foundation for understanding the prerequisites upon which more complex agentic systems ultimately depend.Prompt design thus becomes pivotal to the scope of this investigation, and accordingly, the next section focuses on prompt engineering and its foundational principles.</p>
<p>Prompt Design</p>
<p>The piece of text or set of instructions that a user provides to an LLM to elicit a specific response is referred to as a prompt.Designing effective prompts has therefore become essential to harness the full capabilities of LLMs, and in recent years this craft has evolved into a distinct field of research and development [8].Advanced prompting techniques-such as in-context learning [22], chain-of-thought [12,19], and ReAct [20]-are frequently employed in studies to improve the reliability and precision of experimental planning within LLM-assisted workflows.</p>
<p>Prompting strategies are often described along multiple dimensions.One common distinction is between structured and unstructured prompts.Structured prompts use precise formulations, with clearly defined inputs, outputs, and constraints, and tend to yield more consistent resultsparticularly in tasks like code generation.However, structured prompts often demand detailed knowledge of both the problem space and the model's behavior, which can hinder accessibility for non-experts.Unstructured prompting, in contrast, adopts a more conversational style, making it easier to use in real-world scenarios where users might lack technical expertise.Yet this flexibility comes at a cost, as it can lead to output inconsistencies due to the inherent ambiguity of informal language.Table 1 illustrates a minimal example of this structured vs. unstructured prompting dimension.</p>
<p>Prompts can also be distinguished by the number of examples they provide to guide the model: zero-shot prompts offer no examples, one-shot prompts include a single instance, and few-shot prompts present several illustrations.Each style carries its own trade-offs, as supported by empirical findings-for instance, Liang et al. [18] showed that structured prompts containing code consistently outperform those written in plain language for tasks involving robotic reasoning.That said, ongoing advancements in LLM capabilities continue to narrow this gap, with recent studies demonstrating promising results for natural-language-based prompting in specialized domains like robotics [23].</p>
<p>Complementing existing techniques, our method employs a fully structured, zero-shot prompt that embeds the relevant API docstrings and specifies, without examples, every step needed to assemble a complete computational experiment.This design tests the model's ability to interpret unfamiliar Python APIs, assemble complete computational workflows-from data extraction and analysis to multi-step data generation, clustering, and evaluation-and return fully functional solutions in a single pass.</p>
<p>Materials and Methods</p>
<p>This section details the methodology adopted to assess the ability of state-of-the-art LLMs to interpret and use Python APIs for automated computational experiment design.The following subsections describe the experimental scenarios and prompt construction (3.1), evaluation pipeline (3.2), selection of LLMs (3.3), experimental protocol (3.4), and data analysis approach (3.5).</p>
<p>Scenarios and Prompts</p>
<p>To evaluate the performance of the selected LLMs (discussed in Section 3.3), two experimental scenarios were prepared, each associated with a carefully designed, structured zeroshot prompt.The zero-shot approach, which does not provide examples within the prompt, was chosen to maximize reproducibility, minimize potential bias from hand-crafted demonstrations, and more closely reflects typical initial queries issued by domain experts to LLMs for technical tasks [8,12].Zero-shot prompting also facilitates direct benchmarking of models' intrinsic capabilities in code generation and comprehension.</p>
<p>Both prompts employ a highly structured format, specifying requirements such as the function name, input and output types, the set of allowable libraries, coding standards (e.g., indentation and style), and the necessity for the function to be self-contained.This structure serves to minimize ambiguity, streamline post-processing, and ensure fair and consistent evaluation between models and runs [8,12].Moreover, by explicitly detailing the APIs to use, including relevant docstrings, the prompts challenge models to demonstrate genuine understanding and integration of unfamiliar libraries, rather than relying on memorized patterns from training data.However, since the documentation of the tested libraries is publicly available since 2023, a degree of prior exposure cannot be fully ruled out.</p>
<p>The two experimental scenarios were designed to reflect progressively increasing complexity and to probe distinct aspects of LLM-driven code synthesis using recent, peer-reviewed research software.Both the ParShift [10] and pyclugen [11] libraries, although not widely known, are open source, fully unit tested, extensively documented, and readily available via the Python Package Index, thus providing a reliable and transparent foundation for the study.</p>
<p>The first experiment, using ParShift, requires LLMs to generate a Python function, do_parshift_exp(), that processes a list of CSV files containing conversational data, extracting the proportion of utterances pairs labeled as "Turn Usurping" from each file, and returning the results as a list of floats.This scenario is hypothesized to be relatively straightforward, involving a linear workflow that primarily tests basic API usage, data parsing, and compliance with explicit prompt constraints.The prompt, with API docstrings omitted for space considerations, is detailed in Table 2.</p>
<p>The second experiment employs pyclugen to significantly increase task complexity.Here, LLMs must generate a function, do_clustering_exp(), that conducts a full clustering analysis by generating multiple synthetic datasets in three dimensions (with varying cluster dimensions along a general direction), applying several clustering algorithms available in scikit-learn [25], evaluating clustering quality using the V-measure [26], and collating detailed results (including algorithm, parameter values, and seed) in a Pandas data frame.This multi-step pipeline tests the LLMs' ability to integrate unfamiliar APIs, manage parameterization, and synthesize robust, structured code.Potential error points include misuse of APIs (both from pyclugen and scikit-learn), mishandling of pyclugen output, and inconsistent output formatting.The prompt is provided in Table 3-the API docstrings are again omitted for brevity.</p>
<p>Table 2: Prompt used in Experiment 1 (ParShift).The table shows the full prompt excluding API docstrings; the complete prompt, including all referenced docstrings, is available in the supplementary material [24].</p>
<p>Prompt 1</p>
<p>Create a Python function named <code>do_parshift_exp()</code>that analyses CSV files containing conversations as described below.This function accepts a list of file paths (<code>List[str]</code>), each pointing to a CSV file, and returns a corresponding list of floats.</p>
<p>Specifically, the analysis of these conversations should be done with the <strong>parshift</strong> library, whose API documentation is provided below in the form the respective source docstrings: <code>`python {{DOCSTRINGS HERE}}</code>T he <code>do_parshift_exp()</code>function must analyse the conversations in the CSV files as follows:</p>
<p>1.For each CSV file specified in the list given as the function's argument, extract the proportion of the "Turn Usurping" class (as a float).Here, proportion is the sum of the "Frequency" divided by all frequencies.</p>
<ol>
<li>Collect all the proportions of 'Turn Usurping' into a list of floats.3.Return the list of floats, with one entry per file.</li>
</ol>
<p>The <code>do_parshift_exp()</code>function should strictly follows these requirements:</p>
<p>-The function must be self-contained.In other words, all variables, constants, and/or helper functions must be defined within the <code>do_parshift_exp()</code>function.</p>
<p>-Beyond the Python standard library, only the <strong>parshift</strong>, <strong>pandas</strong>, and <strong>numpy</strong> libraries are allowed.</p>
<p>-The function should be compatible with Python 3.9 and above and follow the PEP 8 style guidelines, with 4-space indentation.</p>
<p>-Each input file uses a semicolon (;) as a delimiter and contains conversational data as expected by the parshift library.</p>
<p>-Do not include print statements, plots, or additional output.</p>
<p>Please respond with a single code block only.Inline comments within the code are fine, but do not include any explanation or text outside the code block.</p>
<p>The two scenarios were first implemented and validated by the authors to establish the baseline results used for comparison with the LLM-generated functions.Both the baseline implementations and their results are provided in the supplementary material [24].</p>
<p>Evaluation Pipeline</p>
<p>The process of evaluating LLM-generated Python code is depicted in Fig. 1.For each experiment, all combinations of predefined LLMs, random seeds, and prompts are systematically iterated.Each prompt is submitted to the respective LLM, and the resulting output is stored.</p>
<p>Function extraction from LLM responses proceeds in up to two steps: (1) code is extracted from the first Markdown code block, specifically from the text enclosed between the <code>python opening delimiter and the closing</code>(that is, the code is expected to be located within these code fences); or (2) if the previous step fails, the entire response is assessed as potential Python code.If neither approach yields valid code, the attempt is logged and assigned a score of 1, as illustrated in Fig. 1.Both prompts explicitly instruct the LLMs to output a single code block containing only the function implementation; nonetheless, these extraction steps provide tolerance for minor deviations from reply formatting.Upon successful extraction, the function is saved as a Python file for subsequent execution.</p>
<p>Extracted code is then executed in a controlled environment.In Experiment 1 (ParShift), the extracted do_parshift_exp() function is tested using three CSV files, collecting the respective Table 3: Prompt used in Experiment 2 (pyclugen).The table presents the prompt text without the included API docstrings; the complete version with all API docstrings can be found in the supplementary material [24].</p>
<p>Prompt 2</p>
<p>Create a Python function named <code>do_clustering_exp()</code>that runs a comprehensive clustering experiment.This function accepts a single <code>n_samples</code>argument of type <code>int</code>, and returns a pandas DataFrame.Specifically, the experiment involves generating synthetic clustered datasets using the <code>clugen</code>function from the <strong>pyclugen</strong> library, whose API documentation is provided below in the form the respective source docstrings:</p>
<p><code>`python {{DOCSTRINGS HERE}}</code>T he computational experiment requirements are as follows:</p>
<p>-The dimensionality (<code>num_dims</code>) should be set to 3.</p>
<p>-The number of clusters (<code>num_clusters</code>) should be 10.</p>
<p>-Each dataset should contain exactly 10,000 points (<code>num_points</code>).-The average cluster direction vector (<code>direction</code>) is set to a 3-dimensional vector of ones.</p>
<p>-The standard deviation of cluster direction angles (<code>angle_disp</code>) should be pi/16 radians.</p>
<p>-The average separation between cluster centers (<code>cluster_sep</code>) should be fixed at 50 for each dimension.</p>
<p>Additionally, perform a parameter sweep over different values of the average cluster-supporting line length (<code>llength</code>):</p>
<p>-The average lengths (<code>llength</code>) should range from 0 to 800, inclusive, with steps of 25 (i.e., <code>[0, 25, 50, ..., 800]</code>).</p>
<p>Finally:</p>
<p>-The length dispersion of cluster-supporting lines (<code>llength_disp</code>) should be set to 0.2 * llength.</p>
<p>-The cluster lateral dispersion (<code>lateral_disp</code>) should be set to 0.5 * llength.</p>
<p>For each dataset generated, apply the following clustering algorithms from scikit-learn:</p>
<p>-K-means++ (<code>KMeans</code>with <code>init='k-means++'</code>) -Gaussian Mixture Model clustering (<code>GaussianMixture</code>) -Agglomerative Hierarchical Clustering (<code>AgglomerativeClustering</code>) with the following linkage methods:</p>
<p>-Ward linkage -Single linkage -Complete linkage -Average linkage (with Euclidean distance)</p>
<p>Please note that in recent version of scikit-learn, the <code>AgglomerativeClustering</code>class no longer accepts the <code>affinity</code>parameter.This parameter is now called <code>metric</code>which is set to <code>euclidean</code>by default.</p>
<p>Evaluate the clustering quality for each run using the V-measure (<code>v_measure_score</code>) provided by scikit-learn, comparing predicted cluster labels to the ground truth labels provided by <code>clugen</code>.</p>
<p>For every run, record the following into a pandas DataFrame:</p>
<p>-<code>run</code>: Sample run index (from 0 to <code>n_samples</code>-1) -<code>algorithm</code>: Name of the clustering algorithm (e.g., <code>"kmeans++"</code>, <code>"em"</code>, <code>"ahc-ward"</code>, <code>"ahc-single"</code>, <code>"ahc-complete"</code>, <code>"ahc-avg"</code>) -<code>length</code>: The current average line length (<code>llength</code>) -<code>vmeas</code>: Calculated V-measure score -<code>seed</code>: Random seed used for generating the dataset</p>
<p>The <code>do_clustering_exp()</code>function should return this pandas DataFrame.</p>
<p>The <code>do_clustering_exp()</code>function should strictly follows these requirements:</p>
<p>-The function must be self-contained.In other words, all variables, constants, and/or helper functions must be defined within the <code>do_clustering_exp()</code>function.</p>
<ul>
<li>-The function should be compatible with Python 3.9 and above and follow the PEP 8 style guidelines, with 4-space indentation.</li>
</ul>
<p>-Do not include print statements, plots, or additional output.</p>
<p>Please respond with a single code block only.Inline comments within the code are fine, but do not include any explanation or text outside the code block.turn-usurping proportions.In Experiment 2, do_clustering_exp() is executed with n = 30 samples, with results collected as a Pandas data frame.If execution fails due to syntax or runtime errors, the error in question is logged and a score of 2 is assigned.Successful execution leads to an automated validation of the function's output type and format.For Experiment 1, the output must be a list of three floats; for Experiment 2, the expected output is a data frame with specific columns and dimensions.Mismatches in type or structure result in a score of 3. If the output format is correct, values are compared to pre-computed baselines: a 1% numerical tolerance is allowed for Experiment 1, while statistical indistinguishability is required for Experiment 2 (see Section 3.5 for details).Outputs deviating from the baseline receive a score of 4; otherwise, a perfect score of 5 is assigned.</p>
<p>This evaluation procedure is summarized as follows:  5. Code runs and returns the correct, baseline-matching result (score 5).</p>
<p>All outcomes, scores, and error logs are recorded for further analysis.This systematic, automated pipeline enables robust and reproducible benchmarking of the LLM-generated code.</p>
<p>LLMs Considered</p>
<p>The language models evaluated in this work, summarized in Table 4, were selected for their impact in Artificial Intelligence research, diverse methodological approaches, and demonstrated strengths in code generation, reasoning, and efficiency.Geographic diversity was also prioritized to ensure broad representation of current LLM development.Model sizes (parameters, tokens, etc.) are indicated in millions (M), billions (B), or trillions (T), with uppercase letters used throughout for consistency.</p>
<p>Claude 3.7 Sonnet [27], developed by Anthropic, is a state-of-the-art multimodal LLM capable of processing both textual and visual input.The model supports both rapid-response and extended-reasoning modes and is optimized for handling complex instruction-following, including coding tasks and natural language understanding.Claude 3.7 Sonnet is a representative ad-vanced general-purpose LLM, demonstrating strong performance in both standard and zero-shot Python code synthesis tasks.</p>
<p>DeepSeek Coder-V2 [28], R1 [29], and V3 [30] belong to DeepSeek's open source LLM lineup.Coder-V2 is an Mixture-of-Experts (MoE) model specialized for code and mathematics, supporting 338 programming languages.V3 is DeepSeek's 671B MoE general-purpose model, employing multi-head latent attention and reinforcement learning (RL) from human feedback for multilingual reasoning.R1 is a dense reasoning model derived from V3 using multi-stage RL-initially via pure RL and later with cold-start supervised data-to improve math, logic, and code reasoning.The inclusion of these models demonstrates how conditional computation (via MoE) and scale benefit Python code generation, especially in zero-shot scenarios with sparse online code examples.</p>
<p>CodeGemma [31] and Gemma3 [32] are based on Google's lightweight Gemma architecture.CodeGemma (7B) is fine-tuned specifically for code completion and delivers strong performance on Python benchmarks, while Gemma3 (27B) is a general-purpose, multimodal model featuring long context windows and instruction tuning.These models allow examination of how compact, optimized open weight architectures perform in zero-shot code generation, especially when integrating lesser-known library APIs.</p>
<p>GPT-4o [33] and GPT-4.1 [34] are successive generations of OpenAI's flagship LLMs.GPT-4o ("omni") is a unified multimodal model for text, image, audio, and video, trained endto-end and achieving leading text and code performance with improved multilingual and latency characteristics.GPT-4.1 is oriented towards developer applications, with a million-token context window and substantial improvements in code generation.Including both enables direct comparison between generalist multimodal (GPT-4o) and developer-optimized (GPT-4.1)architectures in zero-shot Python code synthesis with uncommon libraries.</p>
<p>Grok 3 [35], developed by xAI, is notable for its exceptionally large context window, officially claimed to support up to 1M tokens.This extended context capacity enables the model to capture long-range dependencies, which is valuable for complex code generation tasks.The model's advanced reasoning abilities and long-context handling make it a relevant benchmark for zero-shot application of novel libraries.</p>
<p>Llama 3.3 [36] and CodeLlama [37], both from Meta, share a decoder-only transformer backbone but diverge in specialization.Llama 3.3 is a multilingual, instruction-tuned model with strong general-language and reasoning capabilities, while CodeLlama is fine-tuned for codeparticularly Python-demonstrating strong performance at infilling and instruction-following.Their comparison illustrates the effect of generalist versus code-specialized tuning on Python code generation with rarely used APIs.</p>
<p>Codestral [38] and Mistral Large 2.1 [39], from Mistral AI, represent recent advances in both code-focused and general-purpose transformer models from Europe.Codestral is a compact (22B parameter) model designed specifically for code generation, supporting over 80 programming languages and offering competitive performance and latency on code benchmarks.Mistral Large 2.1 is a large (123B parameter), dense model with a 128K token context window, demonstrating strong reasoning and code generation capabilities.Including both allows assessment of architectural diversity and regional competitiveness in LLM-based code generation.</p>
<p>Olmo2 [40] (13B), from the Allen Institute for AI, is a fully open source dense transformer trained on approximately 5T tokens.Its rigorous engineering and transparent training pipeline distinguish it among open models, supporting robust reasoning and general language understanding.Olmo2 offers a benchmark for transparent, research-driven LLMs in zero-shot Python code synthesis with less-common libraries.</p>
<p>Phi-4 [41], a 14B parameter model from Microsoft Research, exemplifies a data-centric training approach, leveraging high-quality synthetic data throughout pretraining and post-training to advance reasoning, mathematical problem-solving, and code generation.Phi-4's strong performance relative to larger models-as also observed in a previous study from our group [4]underlines the influence of training methodology over sheer parameter count, and its compact size makes it well-suited for resource-or latency-constrained deployment.</p>
<p>Qwen2.5-Coder [42] and Qwen3 [43], developed by Alibaba Cloud, expand the study's geographic and architectural breadth.Qwen2.5-Coder, a dense model with a 128K-token context window, is reported to perform well in code generation tasks through synthetic and code-centric pretraining.Qwen3 offers both dense and MoE variants, dual-mode inference, adaptive compute, and multilingual reasoning, achieving competitive results in code and logic tasks.Together with the DeepSeek models, the inclusion of Alibaba's LLMs provides insight into the capabilities of open-weight LLMs from Asia in Python code generation when guided by prompts that include explicit API docstrings.</p>
<p>While every effort was made to include a broad range of state-of-the-art models, certain recent releases were excluded due to practical limitations in time, budget, hardware, or technical access.For example, Gemini models were omitted due to payment system issues with Google during the study period, and Llama4 exceeded available local hardware capacity.The rapid proliferation of LLMs precludes exhaustive coverage in a single study.Nonetheless, the selection here captures a representative cross-section of contemporary architectures, parameter scales, technical strategies, and geographic origins, providing a robust foundation for evaluating Python code generation-particularly in the context of novel Python libraries.</p>
<p>Experimental Setup</p>
<p>To evaluate the capabilities of the models described in Subsection 3.3, the prompts detailed in Subsection 3.1 were submitted to each LLM in six separate runs, using distinct pseudorandom number generator seeds where supported.While seed control is intended to improve reproducibility, not all LLMs provide this functionality (e.g., Claude 3.7 Sonnet did not support seeding at the time of writing), and even when seeding is available, strict reproducibility cannot be assured.Sources of non-determinism include parallel computation on heterogeneous hardware (such as multi-GPU configurations), small discrepancies in floating-point arithmetic across hardware types and architectures, and potential variability arising from system drivers or firmware.Further, instability in the software environment-including differences in library versions, system configuration, and computational resource allocation-can also impact reproducibility [44,45].Accordingly, this study focuses on assessing the general statistical consistency of model outputs over six runs per configuration, rather than pursuing exact reproducibility throughout all models and conditions.As a consequence, random seeds are used on a best-effort basis, with the recognition that exact reproducibility may not be achievable under these conditions.</p>
<p>The temperature parameter for sampling was set to 10% of each model's maximum supported temperature (e.g., T = 0.1 if T max = 1.0), as this is generally considered adequate to allow for some stochasticity while still maintaining accurate problem-solving capabilities in code generation [4,46].It should be noted, however, that not all models expose or respect temperature controls.For example, DeepSeek R1 (accessed via its online API) ignored temperature settings at the time of experimentation [47].Furthermore, the documentation for some models is ambiguous regarding the maximum temperature value.For instance, Mistral's official sampling guide illustrates temperatures up to 3.2 [48], but in the present study a value of 0.2 was used, following Mistral's recommendations and to maintain consistency with other models employing a 0.0-2.0range.</p>
<p>Other model parameters, such as top_p (which restricts next-token sampling to the most probable candidates summing to a specified cumulative probability), were left at their default values.Offline models were executed locally via Ollama [49] on the authors' institutional infrastructure, while online models were accessed through their official APIs.</p>
<p>For models supporting external tools-such as sandboxed code execution or web search (e.g., GPT-4o and GPT-4.1)-thesefunctionalities were not enabled or activated.All models were therefore evaluated solely on their intrinsic capabilities in processing the prompt and associated API docstrings, without access to additional external information sources beyond what was explicitly provided within each prompt.</p>
<p>Data Collection and Analysis</p>
<p>For each experiment, the scores assigned to each LLM-generated function (ranging from 1 to 5) were analyzed to provide an overview of model performance over runs with different random seeds.Score distributions were visualized using histograms, allowing for a clear depiction of the relative frequency of each outcome per model over six runs.In addition to these distributions, the percentage of top scores (i.e., the proportion of runs yielding a perfect score of 5) was recorded, as this directly reflects the frequency with which each model succeeded in generating a fully correct solution.</p>
<p>To compare the proportion of correct answers between models, pairwise statistical testing was performed using Fisher's exact test [50].For each model, its proportion of perfect scores (score equal to 5) was compared against that of each other model, and the number of statistically significant "wins" was recorded.Statistical significance is defined as a p-value below the conventional threshold of α = 0.05.To account for multiple comparisons, p-values were adjusted using the Benjamini-Hochberg procedure for controlling the false discovery rate (FDR) [51], applied per model (i.e., to each model's set of pairwise comparisons).</p>
<p>In Experiment 2, to further investigate the quality of runnable LLM-generated functionsspecifically, those able to return properly formatted outputs-their results were statistically compared to those of the baseline pyclugen experiment.Following the output comparison approach described in [52], the outputs of each function (V-measure as a function of average cluster length for each clustering algorithm, with centered and scaled results concatenated by algorithm) were subjected to principal component analysis (PCA).Statistical similarity was assessed using the Mann-Whitney U test [53] on the first principal component scores.In addition, PCA score plots were visualized to provide a qualitative assessment of potential differences between LLMgenerated and baseline outputs.</p>
<p>In addition to functional correctness, the subset of fully correct code snippets (score 5) was further analyzed for code quality using four software engineering metrics: cyclomatic complexity (c c ), maintainability index (m i ), number of type errors per 100 source lines of code (e t /100), and number of F errors per 100 source lines of code (e F /100).Source lines of code (s loc ), defined as the number of non-comment, non-blank lines, were also calculated to characterize code size and to normalize error counts; these values were obtained using Radon [54], a static analysis tool for Python.Cyclomatic complexity, also computed with Radon, quantifies the number of independent paths through a program and provides a measure of structural complexity [55], which is particularly relevant for assessing whether generated code remains simple and usable.The maintainability index, m i , likewise measured with Radon, is a composite indicator on a 0-100 scale (higher is better) derived from c c , s loc , percentage of comment lines, and various measures of operators and operands.In this context it provides an aggregate view of readability and long-term maintainability.Type errors were identified using mypy [56], a static type checker for Python, with results expressed as e t /100 to account for code size and allow comparison between models; these errors indicate violations of type annotations and potential runtime failures.F errors were determined using Ruff [57], a high-performance Python linter that extends and replaces Flake8 [58], detecting a broad range of issues including unused imports, undefined variables, duplicate definitions, and standard style violations.These issues are categorized under the traditional F error codes, reflecting their role in capturing logical and correctness problems beyond type checking or complexity analysis.As with type errors, results were normalized as e F /100 to mitigate differences in code size.For all metrics, emphasis was placed on median values, which are more robust to skewed distributions and outliers, while box plots were used to visualize overall distributions and highlight variability.This enabled a quantitative summary of correct LLM-generated code quality, allowing comparisons not only between models but also across experiments, providing insights into both model-specific tendencies and differences in code quality associated with task complexity.Although there is some overlap between the metrics employed (e.g., cyclomatic complexity is one of the components of the maintainability index, and certain F errors may coincide with type-related issues), their joint use nevertheless provides a deeper assessment of code quality than any single metric alone.</p>
<p>Additional analyses are provided in the supplementary material [24] for interested readers.These include further descriptive statistics (e.g., mean, standard deviation, among others), as well as pairwise model comparisons of full score distributions using the Mann-Whitney U test (note that the main statistical analysis in the paper addresses the proportion of fully correct responses, score = 5, not the full score distribution).In addition, the supplementary material contains non-aggregated results and individual counts of type and F errors, allowing a more finegrained inspection of model behavior.These supplementary analyses are not further discussed in the main text since: 1) descriptive statistics such as means or standard deviations may obscure important aspects of discrete, skewed, or multi-modal distributions; 2) while overall scores are useful to understand how and when models fail, emphasizing statistically significant differences in the complete score distribution between models may distract from the primary criterion of interest: the consistent generation of fully correct code; and, 3) detailed per-instance code quality results, although useful for in-depth inspection, extend beyond the central focus of this work on functional correctness.</p>
<p>Data analysis was conducted primarily in Python, using several established scientific computing libraries.Pandas [59] and NumPy [60] were employed for general data manipulation and numerical computations, while SciPy [61] provided statistical testing functionalities.The statsmodels package [62] was used to perform p-value corrections.Additionally, some analyses were performed in the R environment [63].In particular, the micompr package [64] was used for statistically comparing multidimensional outputs from the LLM-generated functions in Experiment 2 against a baseline, enabling differentiation between functions scoring 4 and those scoring 5.The specific versions of these packages are listed in Table A.1.</p>
<p>Results</p>
<p>Table 5 summarizes the aggregated evaluation results for the 17 LLMs tested in the two experiments. Detailed pairwise model comparisons are provided in Table A.2 for Experiment 1 and Table A.3 for Experiment 2. Additionally, to distinguish between scores of 4 and 5 for</p>
<p>Table 5: Distribution and significance of scores (1-5) for the 17 tested models in the two experiments.'Hist.' shows the histogram (score distribution); 'Top' reports the percentage of top scores (i.e., scores of 5); and 'Sig.' indicates the number of models for which this model had a statistically significant higher proportion of top scores (Fisher's exact test [50], α &lt; 0.05, with p-values corrected for FDR using the Benjamini-Hochberg method [51]; additional test details in Table A To clarify the nature of execution failures that resulted in a score of 2, the corresponding error messages were grouped into three categories (Table 6).Logic errors refer to basic Python or reasoning flaws, such as missing imports or referencing variables before assignment.Established API errors arise from the incorrect use of widely adopted libraries such as Pandas or scikit-learn.Finally, novel API errors correspond to issues in handling the libraries under evaluation, specifically ParShift in Experiment 1 and pyclugen in Experiment 2. This categorization condenses a wide range of raw error messages into a small set of recurring patterns, highlighting the main reasons for LLM-generated code failure in these experiments.
-3.7-sonnet - - 4 2 - - codegemma - - 6 - 6 - codellama - - 4 - 1 5 codestral - - 2 - 6 - deepseek-coder-v2 - - 6 - 6 - deepseek-r1 - - 4 2 - - deepseek-v3 1 - 1 - - - gemma3 - - - - 6 - gpt-4.1 - - - - - - gpt-4o - - 2 - 1 - grok-3-beta - - - - 1 - llama3.3 - - 6 - 6 - mistral-large - - 4 - - - olmo2 6 - - - 6 - phi4 - - 4 - 1 - qwen2.5-coder - - 6 - 6 - qwen3 - -3 2 - 2
To complement the functional correctness analysis, code quality indicators for the subset of runs that produced fully correct code (score 5) are also reported.Table 7 summarizes, for each model and experiment, the medians of s loc , c c (cyclomatic complexity), m i (maintainability index), e t /100 (type errors per 100 s loc ), and e F /100 (F errors per 100 s loc ), with n indicating the number of successful instances.The last row ("All instances") reports experiment-wide medians (and total n). Figure 2 complements this table by displaying standard box plots of these metrics per model and experiment, providing a clearer view of distributional spread and potential outliers.</p>
<p>Complete experimental data, including all generated outputs, extracted functions, and error logs, are available in the supplementary material [24].</p>
<p>Experiment 1: ParShift</p>
<p>The first task involved generating Python code using the ParShift library to compute the proportion of utterance pairs classified as "Turn Usurping" from CSV conversation files.As indicated in Table 5, the models exhibited a wide range of outcomes.GPT-4.1 and Grok 3 received a perfect score of 5 in every evaluated run, generating executable Python code that returned the expected output in all cases.These two models also achieved a statistically significant higher proportion of perfect scores than other models in 12 out of 16 pairwise comparisons (see Table A.2).</p>
<p>Other high-performing models include Codestral, DeepSeek-V3, and GPT-4o, which generated correct solutions in four out of six runs.Claude 3.7 Sonnet, DeepSeek-R1, and Mistral Large achieved score 5 results in two out of six runs each.All large models executed online achieved Left column: Experiment 1; right column: Experiment 2 (note that axis scales differ substantially between experiments and should not be compared directly).No box plot is shown for Codestral in Experiment 2, since this model did not generate working code.Rows correspond to different metrics: s loc -source lines of code, i.e., non-comment lines of code; c c -cyclomatic complexity, a measure of independent code paths (lower is better); m i -maintainability index, a composite software quality measure (0-100, higher is better); e t /100-type errors per 100 s loc , reflecting type inconsistencies; and, e F /100-F errors per 100 s loc , reflecting potential code quality issues.Table 7: Median code quality metrics per model for each experiment for runs that produced fully correct code (score 5): s loc -source lines of code, i.e., non-comment lines of code; c c -cyclomatic complexity, a measure of independent code paths (lower is better); m i -maintainability index, a composite software quality measure (0-100, higher is better); e t /100-type errors per 100 s loc , reflecting type inconsistencies; and, e F /100-F errors per 100 s loc , reflecting potential code quality issues.'All instances' shows experiment-wide medians of each metric across all models, with n equal to the total number of successful instances.at least one perfect result.Among offline models, only Codestral reached a perfect score, doing so in 66.7% of runs.Other offline models did not produce any perfect scores in this experiment.Interestingly, Gemma3 always produced code that executed without errors, but returned an incorrect-although properly formatted-result in every run (score 4).As summarized in Table 6, most execution failures (score 2) were novel API errors.These typically manifested as KeyErrors when attempting to access non-existent ParShift fields, observed in models such as Llama 3.3, Phi-4, Qwen2.5-Coder,DeepSeek-R1, Mistral Large, and Codestral.In several cases, exceptions surfaced from Pandas within ParShift's I/O pipeline (mainly in DeepSeek Coder-V2), but these were triggered by incorrect use of ParShift's API and were therefore classified as novel API errors.A secondary source of failures involved logic errors, including missing or incorrect imports, as was always the case in Olmo2.There were no direct established API errors in this experiment.</p>
<p>As can be observed in Table 7 and Fig. 2, the characteristics of the LLM-generated code that achieved score 5 differed substantially across models in Experiment 1. Regarding size, Codestralgenerated code was concise and uniform (median s loc = 13, without variance), while code from Grok 3 was comparatively longer (typically &gt; 30 lines).Structural complexity was low for code produced by Claude 3.7 Sonnet and Mistral Large (median c c = 2), while code from Grok 3 and DeepSeek-V3 showed higher branching (median c c &gt; 5), including an outlier for DeepSeek-V3 with c c = 12.Maintainability was high for code generated by Claude 3.7 Sonnet, Codestral, and GPT-4o (median m i &gt; 90), whereas code from DeepSeek-R1 showed lower maintainability (median m i ≈ 60).Type checking outcomes varied: several models yielded zero or near-zero e t /100, but code from Claude 3.7 Sonnet and DeepSeek-R1 exhibited higher medians (&gt; 20 type errors per 100 s loc ).F errors were mostly related with unused inputs and a few inconsequential type redefinitions.Code from Grok 3 had the lowest median (≈ 12.5 per 100 s loc ), while code from Claude 3.7 Sonnet and Codestral showed higher medians (roughly 28-31 per 100 s loc ).Focusing on models with 100% success rate, GPT-4.1 stands out by always being on the average of other models in all these metrics, and Grok 3 by producing somewhat verbose and complex code, although with very few type and F errors.</p>
<p>Experiment 2: pyclugen</p>
<p>The second experiment required the generation of a function to perform a clustering experiment using the pyclugen and scikit-learn libraries and return a Pandas data frame with computed clustering metrics.We hypothesize that this task is more complex than that of Experiment 1 due to the higher dimensionality of the data and the need to properly apply multiple clustering algorithms.</p>
<p>The GPT-4.1 and Mistral Large models produced correct, executable code that returned the expected output in every run, obtaining a perfect score of 5 in all cases.The Grok 3 model produced correct output in five out of six runs, while Claude 3.7 Sonnet and DeepSeek-R1 succeeded in four out of six runs.These five models showed a statistically significant higher proportion of top scores relative to most others (details in Table A.3).</p>
<p>The remaining online models, GPT-4o and DeepSeek-V3, produced correct solutions in some runs, but none of the smaller, offline models succeeded in generating correct code in any evaluated run.This observation is in line with our hypothesis that Experiment 2 involves greater complexity than Experiment 1.</p>
<p>The Phi-4 model consistently produced code that executed successfully and returned a valid data frame in the correct format, but the results were statistically different from those provided by the reference implementation in all tested runs (see Table A .4).This was due to the generated code applying clustering algorithms to the point projections (which pyclugen uses as an intermediate step to generate the final points) instead of the final data points themselves, resulting in structurally correct but semantically incorrect outputs.</p>
<p>As shown in Table 6, the most common code execution failures in Experiment 2 (score 2) were established API errors, mainly from incorrect use of scikit-learn.A recurring issue in the code generated by several models (CodeGemma, Codestral, DeepSeek Coder-V2, Gemma3, Llama3.3,Olmo2, and Qwen2.5-Coder) was importing GaussianMixture from sklearn.cluster instead of the correct sklearn.mixturemodule.Additional problems included passing deprecated/invalid parameters in scikit-learn methods (CodeLlama) and accessing unavailable attributes in scikit-learn objects (Grok 3), as well as Pandas usage incompatible with recent versions (GPT-4o and Phi-4).Novel API errors were less frequent but present, reflecting misuse of pyclugen, such as calling the clugen() function with missing required positional arguments (CodeLlama), or assuming non-existent functions or attributes (CodeLlama and Qwen3).Finally, a smaller set of logic errors was observed, including missing imports or symbols (Claude 3.7 Sonnet and DeepSeek-R1) and failure to generate the experiment's function with the specified name (Qwen3).</p>
<p>In addition to these execution failures, some models produced outputs with the wrong format (score 3), in particular returning a data frame with the column name "ahc-average" rather than the required "ahc-avg" (DeepSeek-V3 and GPT-4o).</p>
<p>Focusing only on fully functional generated code (score 5), broadly similar profiles were observed between models, with a few interesting differences, as shown in Table 7 and Fig. 2. In terms of size, code from Claude 3.7 Sonnet was markedly more verbose (median s loc = 95), whereas other models clustered around 56-62 lines.Cyclomatic complexity was stable (median c c = 5) for all models except Claude 3.7 Sonnet (median c c = 2); the combination of higher s loc and lower c c suggests longer but more linear implementations with fewer branching constructs.An isolated outlier was observed for GPT-4.1 with c c = 7. Maintainability indices were generally similar between models (medians in the 70s), with lower values for code from DeepSeek-R1 (median m i ≈ 55) and an occasional low outlier for Mistral Large (below 55).Type checking results were uniformly strong: medians of e t /100 were low for all models, including a zero median for Mistral Large; code from Grok 3 showed the highest median among models (≈ 3.5 per 100 s loc ), though still small in absolute terms.F errors per 100 s loc (e F /100) were also minimal, and consistently zero for DeepSeek-V3, GPT-4o, and Mistral Large.The few observed F errors were related to undefined types (with no impact on runtime execution) and some assigned variables that were never used.Code from Grok 3 showed the highest median (≈ 1.75 per 100 s loc ).Among models achieving perfect correctness across all runs, code from GPT-4.1 tended again to align with group medians across metrics, whereas code from Mistral Large frequently reached zero e t /100 and e F /100.</p>
<p>Aggregate Observations</p>
<p>In both experiments, GPT-4.1 was the only model to achieve a perfect score in every evaluated run.Grok 3 also performed strongly, with only one instance of a non-executable output in Experiment 2. At the other end of the scale, Qwen3 was the only model to produce score 1 outputs, failing to generate a Python function at all in some cases.Concurrently, score 4 was commonly assigned to Gemma3 (in Experiment 1) and Phi-4 (in Experiment 2) due to correct function execution but semantically incorrect results.</p>
<p>In both experiments, most outputs clustered around either score 2 (runtime or syntax error) or score 5 (correct result).Scores of 3 (e.g., incorrect return type or number of elements) and 4 (incorrect results with valid output) were less frequent.Within the score 2 outputs, Experiment 1 failures were largely due to novel API errors (mainly from incorrectly accessing ParShift's data structures), whereas Experiment 2 was dominated by established API errors (misuse or hallucination of methods, mainly in scikit-learn).</p>
<p>Considering the last row of Table 7, both experiments yielded the same number of successful instances (n = 30).Code size increased substantially from Experiment 1 to Experiment 2 (median s loc from 21 to 58).Median cyclomatic complexity rose only modestly (median c c from 4 to 5) and was notably more stable in Experiment 2, with values tightly concentrated around 5. The maintainability index decreased between experiments (median m i from 86 to 73.5), consistent with the greater breadth of the second task.Error rates per 100 s loc were lower in Experiment 2: the median e t /100 was roughly halved (1.84 vs. 3.60), and the median e F /100 dropped from approximately 18 to 0.</p>
<p>Discussion</p>
<p>The results obtained from the two experiments provide insight into the capabilities and limitations of LLMs in generating Python code using third-party libraries, particularly when dealing with less commonly known APIs.Several important observations can be drawn from the analysis of results.</p>
<p>In Experiment 1, most LLMs encountered difficulties with the ParShift library, resulting predominantly from key errors when accessing its internal data structures.These failures can be traced in part to limitations in the clarity of the existing ParShift documentation.Indeed, during the preliminary phase of this study, the authors themselves faced challenges in clearly formulating the final prompt due to ambiguities and complexities in the library's documentation.A post-experiment assessment of ParShift's documentation confirms that clearer structure and more explicit examples would likely reduce such errors.Consequently, improvements to ParShift's documentation are planned, motivated directly by findings from this study.</p>
<p>In Experiment 2, the incorrect import of GaussianMixture was a common failure point for many models, reflecting a broader class of import and attribute errors.This issue highlights a fundamental characteristic of LLMs: their reliance on statistical patterns in training data to predict plausible tokens rather than verifying correctness through explicit validation.Since GaussianMixture appears contextually similar to algorithms within the sklearn.clustermodule, models often incorrectly associated it with this module.This behavior resembles human guesswork based on contextual association rather than verified knowledge, reflecting a known limitation of current generative models [65].In contrast, models such as GPT-4.1 and Mistral Large consistently avoided this mistake, suggesting that more rigorous fine-tuning with validated code samples, stronger internal consistency mechanisms, or filtering of incorrect token sequences for critical libraries such as scikit-learn may mitigate such errors.</p>
<p>Code quality results demonstrated that top performers are not identical once maintainability and static issues are considered.GPT-4.1 tended to sit near group medians across metricsa "steady default" that consistently produced working code, albeit not always optimizing for maintainability or strict style conformance.Its counterpart, GPT-4o, was also relatively average across metrics but showed higher maintainability when it produced correct outputs.Grok 3 was notably verbose in Experiment 1 yet exhibited very few type and F errors; conversely, in Experiment 2 it was among the less verbose models while showing comparatively higher static error rates among successful runs.A plausible explanation is that the simpler task in Experiment 1 involved a more idiosyncratic API (ParShift) that several models found harder to parse, whereas the more complex Experiment 2 may have benefited from clearer API affordances (e.g., pyclugen documentation), allowing some models to adapt structure to the increased task complexity, with only a modest reduction in maintainability.Importantly, Grok 3 almost always produced working code (score 5) across both tasks, trailing only GPT-4.1 in overall reliability.Claude 3.7 Sonnet presented the lowest and most stable c c between experiments.It achieved this with relatively few s loc in Experiment 1 but with substantially higher s loc in Experiment 2 (approximately 95 versus 56-62 for peers), yielding longer, more linear code that may be easier to read, while maintaining reasonable m i and a relatively higher static error rate in Experiment 1. Codestral, although only successful in Experiment 1, produced concise code with low c c , high m i , and zero type errors.However, it exhibited the highest median e F /100, largely attributable to unused (often type-related) imports-suggesting a conservative import strategy or remnants of partially pruned templates.DeepSeek-R1 showed consistently lower m i relative to peers in both experiments, indicating a higher refactoring burden even when outputs were correct.Its nonreasoning counterpart DeepSeek-V3 generated code with higher maintainability and fewer static issues, indicating that R1 did not surpass V3 in this study.Finally, while Mistral Large struggled in Experiment 1 (both in success rate and some quality indicators), it excelled in Experiment 2, producing correct code in all runs with uniformly strong quality metrics (e.g., zero type and F errors).</p>
<p>With the exception of Codestral, models that produced working code in one experiment also did so in the other.Across successful runs, Experiment 2 generally yielded longer code (higher s loc ), only slightly higher c c , and a modest decrease in m i -patterns consistent with increased task complexity.Interestingly, type and F errors were markedly fewer in Experiment 2 among correct solutions, which, together with the error taxonomy, strengthens the hypothesis that ParShift's API in Experiment 1 posed interpretability challenges for several models.This observation complements the earlier conclusion regarding the value of improved documentation for ParShift.</p>
<p>During prompt development for Experiment 2, a separate issue arose when some LLMs provided np.int64 types for the seed parameter in the clugen() function from the pyclugen li-brary.Although conceptually correct, this caused runtime crashes due to type incompatibility with the previous implementation of pyclugen.To address this, pyclugen was updated prior to the experiment to accept any integer-compatible type, ensuring robustness to typical coding practices.This experience highlights a broader utility of employing LLMs for testing assumptions and identifying edge cases that might not surface even under comprehensive unit testing.In this particular instance, despite 100% unit test coverage, the practical use of LLM-generated code facilitated discovery and rectification of a subtle but impactful type-checking oversight.</p>
<p>Contrasting with the findings from a previous study [4], where smaller offline models, particularly Phi-4 and Llama3.3,performed comparably to larger models, the current results revealed significantly lower performance from these smaller models in generating code for computational experiments.The exception among offline models was Codestral, which achieved a few instances of correct results in Experiment 1.The diminished performance of Phi-4 and Llama3.3compared to previous findings indicates that the complexity and novelty of the APIs used in the present study significantly affect the effectiveness of these models.</p>
<p>Among all evaluated models, GPT-4.1 demonstrated a clear and consistent advantage, achieving perfect scores in both experiments and showing a strong capacity to interpret unfamiliar APIs and return syntactically and semantically correct code.This suggests that recent improvements in the GPT-4 family, such as improved reasoning, instruction following, and error correction mechanisms, translate directly into more reliable code synthesis, especially for complex or lesserknown libraries.Both Grok 3 and Mistral Large also performed at a high level, ranking among the few models able to solve both tasks with high consistency.These results indicate that, at present, only a handful of top-tier models can reliably perform advanced code generation tasks that involve integrating novel APIs in end-to-end experiment pipelines.</p>
<p>It is also relevant that DeepSeek-R1, a model explicitly marketed for improved reasoning, did not clearly outperform its generalist counterpart, DeepSeek-V3, in these scenarios, especially in Experiment 1.Both models achieved similar levels of performance and operated in a comparable range to Claude 3.7 Sonnet and GPT-4o, which are also positioned as high-capacity but general-purpose LLMs.This outcome suggests that, despite recent advances in LLM architectures and training strategies for specialized reasoning, practical gains in challenging code synthesis tasks may remain limited unless accompanied by targeted fine-tuning and extensive exposure to domain-specific code samples.The high variance observed even among top models further underlines the need for ongoing evaluation and benchmarking as LLM technology continues to evolve.</p>
<p>For settings where multiple models achieve similar top-score rates, code quality metrics can guide selection according to context: models with lower e t /100 and e F /100 may be preferable when static typing and automated code checks (continuous integration linting gates that block errors before code is merged) are critical.Higher s loc with lower c c may be acceptable (or even desirable) when readability by less experienced developers is prioritized; and "median-leaning" profiles can be advantageous when consistency across tasks and codebases is valued.</p>
<p>Overall, these findings highlight the importance of rigorous API documentation and modelspecific fine-tuning when employing LLMs for automated generation of computational experiment code.Additionally, these results demonstrate the utility of such automated code generation tasks not only for evaluating model capabilities but also for exposing assumptions and deficiencies within libraries themselves, thus contributing positively to their further development and robustness.The integration of code quality metrics highlights that, beyond attaining functional correctness, the sustainability and maintainability of LLM-generated code should be factored into model selection and deployment decisions.</p>
<p>Limitations</p>
<p>While the present study provides important insights into the capabilities of current LLMs for the automated synthesis of computational experiment code using less common Python libraries, several limitations should be acknowledged.</p>
<p>First, the evaluation was limited to two specific computational experiments involving the ParShift and pyclugen libraries.While these libraries are representative of real-world, lesserknown scientific software, the generalizability of findings to other domains or libraries cannot be guaranteed.It is possible that the observed failure patterns or success rates may differ when different APIs, programming languages, or experimental paradigms are used.Additionally, because the prompts included real docstrings from the target libraries, we cannot fully exclude the possibility that these materials were part of some LLMs' pretraining corpora, potentially influencing their performance.</p>
<p>Second, all models were evaluated under a zero-shot prompt setting, which, while arguably reflecting realistic scenarios faced by many users, does not capture the potential benefits of incontext learning, prompt tuning, or multi-turn interactions.Many recent studies show that LLMs can substantially improve code quality when provided with examples, clarifying questions, or iterative feedback [1][2][3].The single-pass prompt structure employed here, while controlled, does not fully exploit these capabilities.</p>
<p>Third, the evaluation infrastructure did not simulate the availability of external resources (e.g., internet search, code validation tools, or documentation lookup) that are increasingly integrated into commercial LLMs and coding assistants.All models were evaluated using only their pretrained knowledge.Therefore, the results may underestimate the real-world performance of models when used in modern coding environments.</p>
<p>Fourth, model variability was assessed by running each prompt with several seeds and temperature slightly above zero to guarantee some randomness; however, this does not exhaustively capture the full space of model stochasticity, especially in models for which temperature or seed control was not available or did not function as expected.Thus, some sources of non-determinism and rare failure modes may not have been observed.</p>
<p>Finally, the study included a limited set of models, selected for breadth and relevance but necessarily omitting some recent or proprietary releases due to practical access constraints.The results may not reflect the capabilities of models released after the completion of this study, nor those of specialized, commercial, or domain-adapted variants.</p>
<p>These limitations point to important avenues for future research, such as expanding the diversity of tested libraries and tasks, incorporating iterative and in-context prompting, and benchmarking LLMs in environments augmented with external tools and resources.</p>
<p>Conclusions</p>
<p>This study systematically evaluated the ability of state-of-the-art LLMs to generate functional Python code for realistic computational experiments involving less widely known scientific libraries.By presenting models with structured, zero-shot prompts using the ParShift and pyclugen libraries, the experiments probed both code synthesis accuracy and robustness when integrating novel APIs.</p>
<p>The results reveal that only a small subset of current models, particularly GPT-4.1,Grok 3, and Mistral Large, consistently produced correct and executable code across both experiments.These models demonstrated strong capabilities not only in generating syntactically correct code, but also in interpreting prompt requirements and accurately handling library-specific details.Among runs that achieved perfect functional scores, analysis of code quality indicators (source lines of code, cyclomatic complexity, maintainability index, and static error rates) further differentiated models and exposed practical trade-offs in readability and maintainability, highlighting the value of assessing quality beyond pass/fail correctness.Interestingly, the specialized reasoning model DeepSeek-R1 did not clearly surpass the generalist DeepSeek-V3, and both performed similarly to Claude 3.7 Sonnet and GPT-4o.Conversely, the majority of smaller or open weight offline models struggled, often failing at basic integration steps or producing code with semantic errors.</p>
<p>Beyond model assessment, this study also highlighted the role of LLM-based experiments in uncovering both documentation gaps (as seen with ParShift) and latent software bugs (as demonstrated by the type handling issue in pyclugen), suggesting the mutual benefit of LLM evaluation for both model developers and scientific software authors.</p>
<p>While the results are encouraging regarding the best-performing LLMs, the observed limitations and failure cases indicate that reliably automating scientific experiment pipelines with LLMs remains challenging-particularly for models lacking targeted fine-tuning or advanced context handling.To broaden the applicability of LLMs in specialized research domains, continued progress is needed not only in model training and prompt engineering, but also in improving the documentation of third-party libraries.Clearer and more complete API documentation can help LLMs interpret unfamiliar tools more accurately, reducing errors and improving overall performance.</p>
<p>Future work should extend these benchmarks to a wider range of libraries, languages, and interaction modes, as well as to real-world use cases requiring code maintenance, interpretability, and safe deployment.As LLMs continue to advance, systematic and transparent evaluation will remain critical for their responsible and effective integration into scientific research workflows.[50] comparing the proportion of top scores between models (rows vs. columns).The null hypothesis H 0 states that the model in the row does not have a significantly higher proportion of top scores than the model in the column.p-values below the 0.05 significance threshold (highlighted in light grey) indicate rejection of H 0 , suggesting that the row model does have a significantly higher proportion of top scores.Multiple comparisons are corrected per row using the Benjamini-Hochberg procedure [51]   -0.404 0.404 1.000 0.404 1.000 1.000 0.404 1.000 1.000 1.000 0.404 1.000 0.404 0.404 0.404 0.404 codegemma 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 codellama 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 codestral 0.378 0.054 0.054 -0.054 0.378 0.831 0.054 1.000 0.831 1.000 0.054 0.378 0.054 0.054 0.054 0.054 deepseek-coder-v2 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 deepseek-r1 1.000 0.404 0.404 1.000 0.404 -1.000 0.404 1.000 1.000 1.000 0.404 1.000 0.404 0.404 0.404 0.404 deepseek-v3 0.378 0.054 0.054 0.831 0.054 0.378 -0.054 1.000 0.831 1.000 0.054 0.378 0.054 0.054 0.054 0.054 gemma3 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 gpt-4.1 0.040 &lt;0.01 &lt;0.01 0.242 &lt;0.01 0.040 0.242 &lt;0.01 -0.242 1.000 &lt;0.01 0.040 &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 gpt-4o 0.378 0.054 0.054 0.831 0.054 0.378 0.831 0.054 1.000 -1.000 0.054 0.378 0.054 0.054 0.054 0.054 grok-3-beta 0.040 &lt;0.01 &lt;0.01 0.242 &lt;0.01 0.040 0.242 &lt;0.01 1.000 0.242 -&lt;0.01 0.040 &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 llama3.31.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 mistral-large 1.000 0.404 0.404 1.000 0.404 1.000 1.000 0.404 1.000 1.000 1.000 0.404 -0.404 0.404 0.404 0.404 olmo2 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 phi4 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 qwen2.5-coder1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 qwen3 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -Table A.3: Experiment 2: p-values from Fisher's exact test [50] comparing the proportion of top scores between models (rows vs. columns).The null hypothesis H -0.048 0.048 0.048 0.048 0.895 0.412 0.048 1.000 0.667 1.000 0.048 1.000 0.048 0.048 0.048 0.048 codegemma 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 codellama 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 codestral 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 deepseek-coder-v2 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 deepseek-r1 0.895 0.048 0.048 0.048 0.048 -0.412 0.048 1.000 0.667 1.000 0.048 1.000 0.048 0.048 0.048 0.048 deepseek-v3 1.000 0.364 0.364 0.364 0.364 1.000 -0.364 1.000 1.000 1.000 0.364 1.000 0.364 0.364 0.364 0.364 gemma3 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 gpt-4.1 0.260 &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 0.260 0.044 &lt;0.01 -0.121 0.533 &lt;0.01 1.000 &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 gpt-4o 1.000 0.145 0.145 0.145 0.145 1.000 0.727 0.145 1.000 -1.000 0.145 1.000 0.145 0.145 0.145 0.145 grok-3-beta 0.571 0.012 0.012 0.012 0.012 0.571 0.176 0.012 1.000 0.364 -0.012 1.000 0.012 0.012 0.012 0.012 llama3.31.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 1.000 1.000 mistral-large 0.260 &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 0.260 0.044 &lt;0.01 1.000 0.121 0.533 &lt;0.01 -&lt;0.01 &lt;0.01 &lt;0.01 &lt;0.01 olmo2 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 1.000 phi4 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 1.000 qwen2.5-coder1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -1.000 qwen3 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 -Table A.4: Statistical comparisons of Experiment 2 baseline versus outputs from LLM-generated functions.The table shows the adjusted p-values ('p-val.')resulting from the Mann-Whitney U test [53] applied to the first principal component, obtained via PCA, of the concatenated algorithm outputs (V-measure per average cluster length, centered and scaled).The p-values are adjusted for FDR per-model (across seeds) using the Benjamini-Hochberg procedure [51].Significant results (at α = 0.05) indicating statistical differences between baseline and LLM-generated outputs are highlighted with a light grey background.A significant difference implies assigning a score of 4 (statistically different), whereas non-significant results indicate assigning a score of 5 (statistically indistinguishable).The 'sp' columns contain corresponding stylized score plots displaying the first two principal components to assist in evaluating statistical similarities (red dots correspond to the baseline, black dots to the LLM-generated function output).An × indicates cases where the generated function was either not runnable (scores 1-2) or did not return a properly structured output (score 3); hence, no comparison was possible for that seed.Models that only generated such functions are not shown.Identical score plots across different seeds indicate that the respective model produced functions with identical algorithmic processes and internal seeding behavior for two or more seeds.</p>
<p>Beyond the Python standard library, only the <strong>pyclugen</strong>, <strong>numpy</strong>, <strong>pandas</strong>, <strong>sklearn</strong>, and <strong>scipy</strong> libraries are allowed.</p>
<p>Figure 1 :
1
Figure 1: Evaluation pipeline for assessing LLM-generated Python code.Each prompt is submitted to an LLM with a random number generator seed, followed by code extraction, execution, and validation.Outputs are scored based on success at each stage, from parsing to result accuracy, enabling reproducible and structured performance assessment.</p>
<p>⋆</p>
<p>Not publicly disclosed.</p>
<p>Figure 2 :
2
Figure 2: Standard box of code quality metrics per model per experiment, restricted to code achieving score = 5.Left column: Experiment 1; right column: Experiment 2 (note that axis scales differ substantially between experiments and should not be compared directly).No box plot is shown for Codestral in Experiment 2, since this model did not generate working code.Rows correspond to different metrics: s loc -source lines of code, i.e., non-comment lines of code; c c -cyclomatic complexity, a measure of independent code paths (lower is better); m i -maintainability index, a composite software quality measure (0-100, higher is better); e t /100-type errors per 100 s loc , reflecting type inconsistencies; and, e F /100-F errors per 100 s loc , reflecting potential code quality issues.</p>
<p>the row does not have a significantly higher proportion of top scores than the model in the column.p-values below the 0.05 significance threshold (highlighted in light grey) indicate rejection of H 0 , suggesting that the row model does have a significantly higher proportion of top scores.Multiple comparisons are corrected per row using the Benjamini-Hochberg procedure [51] over 16 comparisons.</p>
<p>Table 4 :
4
LLMs tested in this study.'Patch/Date' indicates the model's exact release patch, if available, otherwise displaying the patch release date.'Size' indicates the number of parameters in billions (B), when available.'Mode' shows whether the model was executed offline via Ollama in the author's institution infrastructure, or if it was experimented with online through the respective parent company's infrastructure.'Tag' corresponds to how each specific model is mentioned in the figures and tables of the results section.
FamilyModel/Version Ref. Patch/Date SizeModeTagClaudeSonnet 3.7[27] 20250219⋆ Online claude-3.7-sonnetDeepSeek coder-v2[28] 2024-09-0616B Offline deepseek-coder-v2R1[29] 2025-01-20 671B Online deepseek-r1V3[30] 0324671B Online deepseek-v3Gemmacode[31] 2024-07-187B Offline codegemma3.0[32] 2025-04-1827B Offline gemma3GPT4o[33] 2024-08-06⋆ Online gpt-4o4.1[34] 2025-04-14⋆ Online gpt-4.1Grok3-beta[35] 2025-02-17⋆ Online grok-3-betaLLaMA3.3[36] 2024-12-0670B Offline llama3.3code[37] 2024-07-1870B Offline codellamaMistralcodestral 0.1[38] 2024-09-0322B Offline codestralLarge 2.1[39] 24.11123B Online mistral-largeOlmo2[40] 2025-01-1113B Offline olmo2</p>
<p>.2 for Experiment 1, and Table A.3 for Experiment 2).
Experiment 1Experiment 2ModelHist. TopSig. Hist. TopSig.claude-3.7-sonnet33.3%066.7%10codegemma0.0%00.0%0codellama0.0%00.0%0codestral66.7%00.0%0deepseek-coder-v20.0%00.0%0deepseek-r133.3%066.7%10deepseek-v366.7%033.3%0gemma30.0%00.0%0gpt-4.1100.0%12100.0%11gpt-4o66.7%050.0%0grok-3-beta100.0%1283.3%10llama3.30.0%00.0%0mistral-large33.3%0100.0%11olmo20.0%00.0%0phi40.0%00.0%0qwen2.5-coder0.0%00.0%0qwen30.0%00.0%0Experiment 2, Table A.4 provides the statistical comparison of outputs from LLM-generatedfunctions against a predefined baseline.</p>
<p>Table 6 :
6
Source of errors when code fails to run due to syntax or runtime errors (score 2) for the 17 tested models in the two experiments.'Logic' errors include basic Python or reasoning errors such as missing imports or use of variable before assignment; 'Est.API' errors are due to incorrect use of established APIs such as Pandas or scikit-learn; finally, 'Nov.API' errors occur when misusing the novel API under test, i.e., ParShift in Experiment 1 and pyclugen in Experiment 2.
Experiment 1Experiment 2ModelLogic Est. API Nov. API Logic Est. API Nov. APIclaude</p>
<p>Table A .
A
1: Software versions used for executing LLM-generated code (column 'Execution') and performing the data analysis, including static code quality assessment (column 'Analysis').
Table A.2: Experiment 1: p-values from Fisher's exact testSoftwareExecution AnalysisPython3.10.123.12.3mypy-1.17.1NumPy1.26.42.3.2Pandas2.2.32.3.1ParShift1.0.1-pyclugen1.1.4-Radon-6.0.1Ruff-0.12.9scikit-learn1.4.2-SciPy-1.16.1statsmodels-0.14.5R-4.5.1micompr-1.2.0rmarkdown-2.29</p>
<p>over 16 comparisons.</p>
<p>AcknowledgementsThis research was partially funded by the Fundação para a Ciência e a Tecnologia (FCT, https://ror.org/00snfqn58)under Grants UIDB/04111/2020, UIDB/00066/2020, UIDB/00408/2020, CEECINST/00002/2021/CP2788/CT0001 and the LASIGE Research Unit, ref.UID/000408/2025, as well as by the Instituto Lusófono de Investigação e Desenvolvimento (ILIND), Portugal, under Project COFAC/ILIND/COPELABS/1/2024.Data AvailabilityThe data generated by this study and its respective analysis are available at https://doi.org/10.5281/zenodo.16582736under the CC-BY license.Appendix A. Supplementary TablesThis appendix contains four supplementary tables.TableA.1 provides the versions of the software used for running the LLM-generated code and performing the data analysis.The remaining tables provide detailed statistical analyses of the experimental results.Specifically, Table A.2 presents pairwise p-values for top-score proportions in Experiment 1, TableA
No Python code could be extracted (score 1). </p>
<p>Code fails to run due to syntax or runtime errors (score 2). </p>
<p>Code runs but returns an output of incorrect type or structure. score 3</p>
<p>Code runs and returns the correct type/structure, but with incorrect results (numerically or statistically different from baseline. score 4</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv preprint (2021) 2107.03374Cs.LGEvaluating large language models trained on code. </p>
<p>Jigsaw: large language models meet program synthesis. N Jain, S Vaidyanath, A Iyer, N Natarajan, S Parthasarathy, S Rajamani, R Sharma, 10.1145/3510003.3510203Proceedings of the 44th International Conference on Software Engineering, ICSE '22. the 44th International Conference on Software Engineering, ICSE '22New York, NY, USAAssociation for Computing Machinery2022</p>
<p>On the effectiveness of large language models in domain-specific code generation. X Gu, M Chen, Y Lin, Y Hu, H Zhang, C Wan, Z Wei, Y Xu, J Wang, 10.1145/3697012ACM Transactions on Software Engineering and MethodologyJust Accepted. 2024</p>
<p>DeepSeek-V3, GPT-4, Phi-4, and LLaMA-3.3 generate correct code for LoRaWAN-related engineering tasks. D Fernandes, J P Matos-Carvalho, C M Fernandes, N Fachada, 10.3390/electronics14071428Electronics. 14714282025</p>
<p>Rodriques, Bioplanner: Automatic evaluation of llms on protocol planning in biology. O O'donoghue, A Shtedritski, J Ginger, R Abboud, A E Ghareeb, J Booth, S G , 10.48550/arXiv.2310.10632arXiv:2310.106322023</p>
<p>S Hong, Y Lin, B Liu, B Liu, B Wu, C Zhang, C Wei, D Li, J Chen, J Zhang, 10.48550/arXiv.2402.18679Data interpreter: An LLM agent for data science. 20242402arXiv preprint</p>
<p>The next generation of experimental research with llms. G Charness, B Jabarian, J A List, 10.1038/s41562-025-02137-1Nature Human Behaviour. 932025</p>
<p>X Amatriain, 10.48550/arXiv.2401.14423Prompt design and engineering: Introduction and advanced methods. 20242401arXiv preprint</p>
<p>Calibration and correctness of language models for code. C Spiess, D Gros, K S Pai, M Pradel, M R I Rabin, A Alipour, S Jha, P Devanbu, T Ahmed, 2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE). 202526</p>
<p>. 10.1109/ICSE55347.2025.00040April-5 May 2025Ottawa, ON, Canada</p>
<p>Parshift: a python package to study order and differentiation in group conversations. B D Ferreira-Saraiva, J P Matos-Carvalho, N Fachada, M Pita, 10.1016/j.softx.2023.101554SoftwareX. 241015542023</p>
<p>Generating multidimensional clusters with support lines. N Fachada, D De Andrade, 10.1016/j.knosys.2023.110836Knowledge-Based Systems. 2771108362023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2022</p>
<p>Improving scientific hypothesis generation with knowledge grounded large language models. G Xiong, E Xie, A H Shariatmadari, S Guo, S Bekiranov, A Zhang, arXiv:2411.023822024</p>
<p>Piflow: Principle-aware scientific discovery with multi-agent collaboration. Y Pu, T Lin, H Chen, arXiv:2505.150472025</p>
<p>Agentreview: Exploring peer review dynamics with llm agents. Y Jin, Q Zhao, Y Wang, H Chen, K Zhu, Y Xiao, J Wang, arXiv:2406.127082024</p>
<p>Litllm: A toolkit for scientific literature review. S Agarwal, G Sahu, A Puri, I H Laradji, K D Dvijotham, J Stanley, L Charlin, C , arXiv:2402.017882025</p>
<p>Llm4sr: A survey on large language models for scientific research. Z Luo, Z Yang, Z Xu, W Yang, X Du, arXiv:2501.043062025</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 10.1109/ICRA48891.2023.101605912023 IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Large language model-based code generation for the control of construction assembly robots: A hierarchical generation approach, Developments in the. H Luo, J Wu, J Liu, M F Antwi-Afari, 10.1016/j.dibe.2024.100488Built Environment. 191004882024</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292023</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. H Yang, S Yue, Y He, 10.48550/arXiv.2306.02224arXiv:2306.022242023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 34th Conference on Neural Information Processing Systems. 202033</p>
<p>S Vemprala, R Bonatti, A Bucker, A Kapoor, 10.48550/arXiv.2306.17582ChatGPT for robotics: Design principles and model abilities. 20232306arXiv preprint</p>
<p>Supplementary material for "gpt-4.1 sets the standard in automated experiment design using novel python. N Fachada, D Fernandes, C M Fernandes, B Ferreira-Saraiva, J Matos-Carvalho, 10.5281/zenodo.165827362025</p>
<p>Scikit-learn: Machine learning in Python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 122011</p>
<p>V-measure: A conditional entropy-based external cluster evaluation measure. A Rosenberg, J Hirschberg, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)prague, Czech RepublicAssociation for Computational Linguistics2007. June 2007</p>
<p>Claude 3.7 sonnet system card. Feb. 2025Tech. rep., Anthropic PBC</p>
<p>Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. Q Zhu, D Guo, Z Shao, D Yang, P Wang, R Xu, Y Wu, Y Li, H Gao, S Ma, 10.48550/arXiv.2406.11931arXiv:2406.119312024arXiv preprint</p>
<p>D Guo, D Yang, H Zhang, J Song, R Zhang, R Xu, Q Zhu, S Ma, P Wang, X Bi, 10.48550/arXiv.2501.12948DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, 10.48550/arXiv.2412.19437DeepSeek-V3 technical report. 2024. 19437Cs2412arXiv preprint</p>
<p>H Zhao, J Hui, J Howland, N Nguyen, S Zuo, A Hu, C A Choquette-Choo, J Shen, J Kelley, 10.48550/arXiv.2406.11409arXiv:2406.11409Codegemma: Open code models based on gemma. 2024arXiv preprint</p>
<p>A Kamath, J Ferret, S Pathak, N Vieillard, R Merhej, S Perrin, T Matejovicova, A Ramé, M Rivière, 10.48550/arXiv.2503.19786arXiv:2503.19786Gemma 3 technical report. 2025arXiv preprint</p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, 10.48550/arXiv.2410.21276arXiv:2410.21276GPT-4o system card. 2024arXiv preprint</p>
<p>OpenAI, Introducing gpt-4.1 model family. Apr. 2025</p>
<p>Grok 3 beta -the age of reasoning agents. Feb. 2025</p>
<p>A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, 10.48550/arXiv.2407.21783The llama 3 herd of models. 20242407arXiv preprint</p>
<p>B Roziere, J Gehring, F Gloeckle, S Sootla, I Gat, X E Tan, Y Adi, J Liu, R Sauvestre, T Remez, 10.48550/arXiv.2308.12950arXiv:2308.12950Code llama: Open foundation models for code. 2023arXiv preprint</p>
<p>Mistral AI team. May 2024Codestral</p>
<p>ac- cessed: 2025-07-09Mistral AI team, Large enough. Jul. 2024</p>
<p>. P Walsh, L Soldaini, D Groeneveld, K Lo, S Arora, A Bhagia, Y Gu, S Huang, M Jordan, 10.48550/arXiv.2501.00656arXiv:2501.006562024arXiv preprint2 olmo 2 furious</p>
<p>. M Abdin, J Aneja, H Behl, S Bubeck, R Eldan, S Gunasekar, M Harrison, R J Hewett, M Javaheripi, P Kauffmann, 10.48550/arXiv.2412.08905Phi-4 technical report. 2024arXiv preprint</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Lu, K Dang, Y Fan, Y Zhang, A Yang, R Men, F Huang, 10.48550/arXiv.2409.12186Qwen2.5-coder technical report. </p>
<p>A Yang, A Li, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Gao, C Huang, C Lv, 10.48550/arXiv.2505.09388arXiv:2505.09388Qwen3 technical report. 2025arXiv preprint</p>
<p>H Semmelrock, T Ross-Hellauer, S Kopeinik, D Theiler, A Haberl, S Thalmann, D Kowald, 10.1002/aaai.70002Reproducibility in machine-learning-based research: Overview, barriers, and drivers. 202546e70002</p>
<p>Assessing the macro and micro effects of random seeds on fine-tuning large language models. H Zhou, G Savova, L Wang, 10.48550/arXiv.2503.07329arXiv:2503.073292025</p>
<p>The effect of sampling temperature on problem solving in large language models. M Renze, E Guven, 10.18653/v1/2024.findings-emnlp.432Findings of the Association for Computational Linguistics, EMNLP 2024. Y Al-Onaizan, M Bansal, Y.-N Chen, Miami, FL, USAAssociation for Computational Linguistics2024. 12-16 November 2024</p>
<p>Reasoning model (deepseek-reasoner) | deepseek api docs. Inc Deepseek, Jun. 25, 2025</p>
<p>A I Mistral, Sampling guide | mistral ai documentation. 12 Jun. 2025</p>
<p>Ollama: Get up and running with large language models, GitHub, last access. J Morgan, M Chiang, Feb. 10, 2025 (2023</p>
<p>On the interpretation of χ 2 from contingency tables, and the calculation of P. R A Fisher, 10.2307/2340521Journal of the Royal Statistical Society. 8511922</p>
<p>Y Benjamini, Y Hochberg, 10.1111/j.2517-6161.1995.tb02031.xControlling the false discovery rate: a practical and powerful approach to multiple testing. 199557</p>
<p>Model-independent comparison of simulation output, Simulation Modelling Practice and Theory. N Fachada, V V Lopes, R C Martins, A C Rosa, 10.1016/j.simpat.2016.12.013201772</p>
<p>On a test of whether one of two random variables is stochastically larger than the other. H B Mann, D R Whitney, 10.1214/aoms/1177730491Annals of Mathematical Statistics. 1811947</p>
<p>. M Lacchia, Radon , Aug. 24. 20122025</p>
<p>A complexity measure. T J Mccabe, 10.1109/TSE.1976.233837IEEE Transactions on Software Engineering. 41976</p>
<p>J Lehtosalo, G Van Rossum, I Levkivskyi, M J Sullivan, mypy -optional static typing for Python. Aug. 24. 20142025</p>
<p>. Astral Team, Ruff, Aug. 24. 20222025</p>
<p>T Ziadé, A Sottile, I Cordasco, Flake8: Your tool for style guide enforcement. Aug. 24. 20162025</p>
<p>pandas: a foundational python library for data analysis and statistics. W Mckinney, Python for High Performance and Scientific Computing, PyHPC '11. 2011. 18 November 2011</p>
<p>Array programming with NumPy. C R Harris, K J Millman, S J Van Der Walt, R Gommers, P Virtanen, D Cournapeau, E Wieser, J Taylor, S Berg, N J Smith, 10.1038/s41586-020-2649-2Nature. 58578252020</p>
<p>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. P Virtanen, R Gommers, T E Oliphant, M Haberland, T Reddy, D Cournapeau, E Burovski, P Peterson, W Weckesser, J Bright, 10.1038/s41592-019-0686-2Nature Methods. 172020</p>
<p>Statsmodels: Econometric and statistical modeling with Python. S Seabold, J Perktold, 10.25080/Majora-92bf1922-011Proceedings of the 9th Python in Science Conference. the 9th Python in Science ConferenceTX, USA2010. 28 June-3 July 2010</p>
<p>Team Core, R: A Language and Environment for Statistical Computing, R Foundation for Statistical Computing. Vienna, Austria2025</p>
<p>micompr: An R Package for Multivariate Independent Comparison of Observations. N Fachada, J Rodrigues, V V Lopes, R C Martins, A C Rosa, 10.32614/RJ-2016-055The R Journal. 822016</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. E M Bender, A Koller, 10.18653/v1/2020.acl-main.463doi:10.18653Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. D Jurafsky, J Chai, N Schluter, J Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020. 5-10 July 2020</p>            </div>
        </div>

    </div>
</body>
</html>
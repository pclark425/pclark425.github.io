<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6517 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6517</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6517</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-267658165</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.08756v1.pdf" target="_blank">Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models</a></p>
                <p><strong>Paper Abstract:</strong> When LLMs perform zero-shot inference, they typically use a prompt with a task specification, and generate a completion. However, there is no work to explore the possibility of the reverse - going from completion to task specification. In this paper, we employ both directions to perform cycle-supervised learning entirely in-context. Our goal is to create a forward map f : X ->Y (e.g. image ->generated caption), coupled with a backward map g : Y ->X (e.g. caption ->generated image) to construct a cycle-consistency"loss"(formulated as an update to the prompt) to enforce g(f(X)) ~= X. The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the prompt. Importantly, CyclePrompt reinforces model performance without expensive fine-tuning, without training data, and without the complexity of external environments (e.g. compilers, APIs). We demonstrate CyclePrompt in two domains: code generation and image captioning. Our results on the HumanEval coding benchmark put us in first place on the leaderboard among models that do not rely on extra training data or usage of external environments, and third overall. Compared to the GPT4 baseline, we improve accuracy from 80.5% to 87.2%. In the vision-language space, we generate detailed image captions which outperform baseline zero-shot GPT4V captions, when tested against natural (VQAv2) and diagrammatic (FigureQA) visual question-answering benchmarks. To the best of our knowledge, this is the first use of self-supervised learning for prompting.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6517.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6517.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyclePrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CyclePrompt (Specification → Completion → Specification cycle-consistency prompt refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, in-context prompt refinement method that uses a forward generator f and a backward generator g to form a specification→completion→specification cycle; a discriminator compares the original input and the round-tripped output to produce textual hints that update the prompt and improve downstream outputs without fine-tuning or external environment supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 / GPT4V (as black-box forward/backward components)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cycle-consistency iterative prompt refinement (CyclePrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative self-refinement / cycle-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate correct Python code from a natural-language task specification and verify by round-trip description of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>pass@1-like exact task accuracy (HumanEval accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>87.2</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>GPT-4 baseline (zero-shot / standard prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.7</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CyclePrompt exploits the high sensitivity of code generation to precise language instructions; the forward generator's responsiveness is critical, the discriminator defines the refinement ceiling, and the backward generator is least critical. The method achieves large gains on HumanEval without external tools because language edits reliably change generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6517.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyclePrompt-VQAv2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CyclePrompt applied to image → caption → image cycles evaluated on VQAv2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uses GPT4V as forward (image→text) and DALL·E 3 as backward (text→image) with a discriminator (GPT4V) comparing original and generated images to refine captions over cycles; captions are evaluated by text-based QA accuracy and DA-Score alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4V (forward & discriminator) + DALL·E 3 (backward)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cycle-consistency iterative prompt refinement (CyclePrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative self-refinement / cycle-consistency (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VQAv2</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce detailed image captions that preserve image information useful for answering VQA questions when only captions are available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>text QA accuracy (using caption as input to text QA model) and DA-Score (image-caption alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.652</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot GPT4V captions (baseline zero-shot captioning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.02</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CyclePrompt captions yielded higher text-QA accuracy (0.652 vs 0.632) but mixed DA-Score results (slightly lower positive-assertion DA-Score, and lower when negative assertions included). Authors attribute limits to weaker backward generator and the asymmetric complexity between image (high complexity) and caption (low complexity); nonetheless iterative discriminator feedback collects additional descriptive details across cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6517.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CyclePrompt-FigureQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CyclePrompt applied to image → caption → image cycles evaluated on FigureQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same CyclePrompt multimodal pipeline applied to diagrammatic images (FigureQA); captions are evaluated by text-based QA accuracy and DA-Score alignment, showing improvements over zero-shot GPT4V captions on diagrammatic visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4V (forward & discriminator) + DALL·E 3 (backward)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Cycle-consistency iterative prompt refinement (CyclePrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative self-refinement / cycle-consistency (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FigureQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce captions of diagrammatic figures that preserve information needed to answer diagram-specific questions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>text QA accuracy (using caption as input to text QA model) and DA-Score (image-caption alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.512</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot GPT4V captions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>0.035</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CyclePrompt improved text-QA accuracy on diagrammatic images (0.512 vs 0.477) and improved DA-Scores (both positive-only and with negatives), suggesting the iterative cycle helps collect diagram-specific details; the authors note image-generation backward model weakness but discriminator still provides useful semantic signals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6517.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 baseline (HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (zero-shot / standard prompting baseline on HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The off-the-shelf GPT-4 zero-shot baseline used by the authors for comparison on HumanEval code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-pass prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Code generation from natural language problem statements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>80.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt (GPT-4 with cycle-consistency prompt refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.7</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Used as the primary baseline; CyclePrompt raises accuracy from 80.5% to 87.2% by iterative prompt refinement, showing that in-context cycle supervision can substitute for external execution feedback in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6517.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4V zero-shot caption</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4V (zero-shot detailed image captioning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot GPT4V captions used as a baseline for caption-based QA; prompts asked GPT4V to describe images in detail without cycle-based refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>zero-shot image captioning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-pass prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>VQAv2 (and FigureQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce detailed captions of images for downstream QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>text QA accuracy (using caption as input to text QA model)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.632</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt captions (GPT4V + CyclePrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.02</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Zero-shot GPT4V captions are the baseline; CyclePrompt captions achieved slightly higher QA accuracy on VQAv2 and higher accuracy on FigureQA, indicating benefit of iterative refinement for captioning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6517.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4V zero-shot caption (FigureQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4V (zero-shot detailed image captioning baseline) on FigureQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Zero-shot GPT4V caption baseline evaluated on diagrammatic FigureQA subset for comparison with CyclePrompt captions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT4V</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>zero-shot image captioning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>single-pass prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>FigureQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce detailed captions of diagrams for downstream QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>text QA accuracy (using caption as input to text QA model)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0.477</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt captions (GPT4V + CyclePrompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-0.035</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CyclePrompt captions outperform zero-shot GPT4V captions on diagrammatic QA, suggesting iterative cycle refines diagram-specific content.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6517.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DALL·E 3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DALL·E 3 (text→image generator, used as backward generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-image generative model used by the authors as the backward generator g in Image→Text→Image cycles to synthesize candidate images from captions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DALL·E 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>text-to-image generation (used as backward step in cycles)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>generative model (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate images that correspond to textual captions during the CyclePrompt image-caption-image cycle.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Used as the backward generator; authors note the backward generator can be weak (image generation adds details) but still samples in a neighboring semantic space that the discriminator can use to refine captions. This asymmetry affects convergence in Text→Image→Text cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6517.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior iterative refinement method that uses feedback derived from external environments (compilers, APIs) converted into textual feedback added to prompts to improve subsequent outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Reflexion (verbal reinforcement from external environment)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative self-reflection with external environment/tool feedback</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Iteratively refine agent behavior by converting external environment feedback into textual hints.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper positions Reflexion as related work that requires external environment feedback; CyclePrompt claims to achieve self-reflection without external tooling, distinguishing itself from Reflexion.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6517.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELF-REFINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative in-context refinement technique that uses few-shot examples of (input, output, feedback) to have an LLM generate feedback and refine its outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative self-refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve model outputs through self-generated feedback given in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors contrast CyclePrompt with SELF-REFINE: CyclePrompt requires no training data or hand-crafted in-context (input,output,feedback) examples and extends naturally to multimodal cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6517.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting method that elicits step-by-step intermediate reasoning chains from LLMs to improve complex problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential chain reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decompose complex reasoning tasks into stepwise chains of thought to improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned as related literature (and as a building block for techniques like self-consistency); not used in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6517.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency for Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding strategy for CoT that samples multiple reasoning paths and aggregates to select the most consistent answer across sampled chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-consistency (ensemble over sampled reasoning paths)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / sampling</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve reasoning by aggregating answers across diverse sampled chains of thought.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt (conceptually different: uses cycle-consistency not sampling aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited in related work; contrasts sampling-based diversity/ensemble approaches with CyclePrompt's iterative semantic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6517.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning + Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that interleaves reasoning traces with actions that query external environments or tools, enabling grounded decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ReAct (reasoning + tool use)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented sequential reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Perform interactive reasoning that issues actions (tool calls) guided by chain-of-thought style reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned as related work that leverages external environments; CyclePrompt distinguishes itself by operating without external tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6517.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e6517.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that explores multiple intermediate 'thoughts' arranged in a tree structure and performs search (e.g., MCTS) over them for improved problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / planning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Explore and evaluate multiple candidate intermediate reasoning states to find high-quality final solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CyclePrompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited as related work that uses structured search over diverse reasoning paths; CyclePrompt uses iterative single-threaded semantic updates rather than tree search.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Unpaired image-to-image translation using cycle-consistent adversarial networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6517",
    "paper_id": "paper-267658165",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "CyclePrompt",
            "name_full": "CyclePrompt (Specification → Completion → Specification cycle-consistency prompt refinement)",
            "brief_description": "An iterative, in-context prompt refinement method that uses a forward generator f and a backward generator g to form a specification→completion→specification cycle; a discriminator compares the original input and the round-tripped output to produce textual hints that update the prompt and improve downstream outputs without fine-tuning or external environment supervision.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 / GPT4V (as black-box forward/backward components)",
            "model_size": "",
            "reasoning_method_name": "Cycle-consistency iterative prompt refinement (CyclePrompt)",
            "reasoning_method_type": "iterative self-refinement / cycle-consistency",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "HumanEval (code generation)",
            "task_description": "Generate correct Python code from a natural-language task specification and verify by round-trip description of generated code.",
            "performance_metric": "pass@1-like exact task accuracy (HumanEval accuracy)",
            "performance_value": 87.2,
            "comparison_target_method": "GPT-4 baseline (zero-shot / standard prompting)",
            "performance_difference": 6.7,
            "statistical_significance": null,
            "analysis_notes": "CyclePrompt exploits the high sensitivity of code generation to precise language instructions; the forward generator's responsiveness is critical, the discriminator defines the refinement ceiling, and the backward generator is least critical. The method achieves large gains on HumanEval without external tools because language edits reliably change generated code.",
            "ablation_study_present": false,
            "uuid": "e6517.0",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CyclePrompt-VQAv2",
            "name_full": "CyclePrompt applied to image → caption → image cycles evaluated on VQAv2",
            "brief_description": "Uses GPT4V as forward (image→text) and DALL·E 3 as backward (text→image) with a discriminator (GPT4V) comparing original and generated images to refine captions over cycles; captions are evaluated by text-based QA accuracy and DA-Score alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4V (forward & discriminator) + DALL·E 3 (backward)",
            "model_size": "",
            "reasoning_method_name": "Cycle-consistency iterative prompt refinement (CyclePrompt)",
            "reasoning_method_type": "iterative self-refinement / cycle-consistency (multimodal)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "VQAv2",
            "task_description": "Produce detailed image captions that preserve image information useful for answering VQA questions when only captions are available.",
            "performance_metric": "text QA accuracy (using caption as input to text QA model) and DA-Score (image-caption alignment)",
            "performance_value": 0.652,
            "comparison_target_method": "Zero-shot GPT4V captions (baseline zero-shot captioning)",
            "performance_difference": 0.02,
            "statistical_significance": null,
            "analysis_notes": "CyclePrompt captions yielded higher text-QA accuracy (0.652 vs 0.632) but mixed DA-Score results (slightly lower positive-assertion DA-Score, and lower when negative assertions included). Authors attribute limits to weaker backward generator and the asymmetric complexity between image (high complexity) and caption (low complexity); nonetheless iterative discriminator feedback collects additional descriptive details across cycles.",
            "ablation_study_present": false,
            "uuid": "e6517.1",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CyclePrompt-FigureQA",
            "name_full": "CyclePrompt applied to image → caption → image cycles evaluated on FigureQA",
            "brief_description": "Same CyclePrompt multimodal pipeline applied to diagrammatic images (FigureQA); captions are evaluated by text-based QA accuracy and DA-Score alignment, showing improvements over zero-shot GPT4V captions on diagrammatic visual reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT4V (forward & discriminator) + DALL·E 3 (backward)",
            "model_size": "",
            "reasoning_method_name": "Cycle-consistency iterative prompt refinement (CyclePrompt)",
            "reasoning_method_type": "iterative self-refinement / cycle-consistency (multimodal)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "FigureQA",
            "task_description": "Produce captions of diagrammatic figures that preserve information needed to answer diagram-specific questions.",
            "performance_metric": "text QA accuracy (using caption as input to text QA model) and DA-Score (image-caption alignment)",
            "performance_value": 0.512,
            "comparison_target_method": "Zero-shot GPT4V captions",
            "performance_difference": 0.035,
            "statistical_significance": null,
            "analysis_notes": "CyclePrompt improved text-QA accuracy on diagrammatic images (0.512 vs 0.477) and improved DA-Scores (both positive-only and with negatives), suggesting the iterative cycle helps collect diagram-specific details; the authors note image-generation backward model weakness but discriminator still provides useful semantic signals.",
            "ablation_study_present": false,
            "uuid": "e6517.2",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 baseline (HumanEval)",
            "name_full": "GPT-4 (zero-shot / standard prompting baseline on HumanEval)",
            "brief_description": "The off-the-shelf GPT-4 zero-shot baseline used by the authors for comparison on HumanEval code generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "",
            "reasoning_method_name": "zero-shot prompting",
            "reasoning_method_type": "single-pass prompting",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "HumanEval",
            "task_description": "Code generation from natural language problem statements.",
            "performance_metric": "accuracy (HumanEval)",
            "performance_value": 80.5,
            "comparison_target_method": "CyclePrompt (GPT-4 with cycle-consistency prompt refinement)",
            "performance_difference": -6.7,
            "statistical_significance": null,
            "analysis_notes": "Used as the primary baseline; CyclePrompt raises accuracy from 80.5% to 87.2% by iterative prompt refinement, showing that in-context cycle supervision can substitute for external execution feedback in this domain.",
            "ablation_study_present": false,
            "uuid": "e6517.3",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT4V zero-shot caption",
            "name_full": "GPT4V (zero-shot detailed image captioning baseline)",
            "brief_description": "Zero-shot GPT4V captions used as a baseline for caption-based QA; prompts asked GPT4V to describe images in detail without cycle-based refinement.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4V",
            "model_size": "",
            "reasoning_method_name": "zero-shot image captioning",
            "reasoning_method_type": "single-pass prompting",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "VQAv2 (and FigureQA)",
            "task_description": "Produce detailed captions of images for downstream QA tasks.",
            "performance_metric": "text QA accuracy (using caption as input to text QA model)",
            "performance_value": 0.632,
            "comparison_target_method": "CyclePrompt captions (GPT4V + CyclePrompt)",
            "performance_difference": -0.02,
            "statistical_significance": null,
            "analysis_notes": "Zero-shot GPT4V captions are the baseline; CyclePrompt captions achieved slightly higher QA accuracy on VQAv2 and higher accuracy on FigureQA, indicating benefit of iterative refinement for captioning.",
            "ablation_study_present": false,
            "uuid": "e6517.4",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT4V zero-shot caption (FigureQA)",
            "name_full": "GPT4V (zero-shot detailed image captioning baseline) on FigureQA",
            "brief_description": "Zero-shot GPT4V caption baseline evaluated on diagrammatic FigureQA subset for comparison with CyclePrompt captions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT4V",
            "model_size": "",
            "reasoning_method_name": "zero-shot image captioning",
            "reasoning_method_type": "single-pass prompting",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "FigureQA",
            "task_description": "Produce detailed captions of diagrams for downstream QA",
            "performance_metric": "text QA accuracy (using caption as input to text QA model)",
            "performance_value": 0.477,
            "comparison_target_method": "CyclePrompt captions (GPT4V + CyclePrompt)",
            "performance_difference": -0.035,
            "statistical_significance": null,
            "analysis_notes": "CyclePrompt captions outperform zero-shot GPT4V captions on diagrammatic QA, suggesting iterative cycle refines diagram-specific content.",
            "ablation_study_present": false,
            "uuid": "e6517.5",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DALL·E 3",
            "name_full": "DALL·E 3 (text→image generator, used as backward generator)",
            "brief_description": "A text-to-image generative model used by the authors as the backward generator g in Image→Text→Image cycles to synthesize candidate images from captions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DALL·E 3",
            "model_size": "",
            "reasoning_method_name": "text-to-image generation (used as backward step in cycles)",
            "reasoning_method_type": "generative model (multimodal)",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "Generate images that correspond to textual captions during the CyclePrompt image-caption-image cycle.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Used as the backward generator; authors note the backward generator can be weak (image generation adds details) but still samples in a neighboring semantic space that the discriminator can use to refine captions. This asymmetry affects convergence in Text→Image→Text cycles.",
            "ablation_study_present": false,
            "uuid": "e6517.6",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A prior iterative refinement method that uses feedback derived from external environments (compilers, APIs) converted into textual feedback added to prompts to improve subsequent outputs.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Reflexion (verbal reinforcement from external environment)",
            "reasoning_method_type": "iterative self-reflection with external environment/tool feedback",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "",
            "task_description": "Iteratively refine agent behavior by converting external environment feedback into textual hints.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "CyclePrompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper positions Reflexion as related work that requires external environment feedback; CyclePrompt claims to achieve self-reflection without external tooling, distinguishing itself from Reflexion.",
            "ablation_study_present": null,
            "uuid": "e6517.7",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "SELF-REFINE",
            "name_full": "Self-Refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative in-context refinement technique that uses few-shot examples of (input, output, feedback) to have an LLM generate feedback and refine its outputs.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Self-Refine (iterative self-feedback)",
            "reasoning_method_type": "iterative self-refinement",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "Improve model outputs through self-generated feedback given in-context examples.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "CyclePrompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Authors contrast CyclePrompt with SELF-REFINE: CyclePrompt requires no training data or hand-crafted in-context (input,output,feedback) examples and extends naturally to multimodal cycles.",
            "ablation_study_present": null,
            "uuid": "e6517.8",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought (CoT) prompting",
            "brief_description": "Prompting method that elicits step-by-step intermediate reasoning chains from LLMs to improve complex problem solving.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential chain reasoning",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "Decompose complex reasoning tasks into stepwise chains of thought to improve reasoning.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned as related literature (and as a building block for techniques like self-consistency); not used in the paper's experiments.",
            "ablation_study_present": null,
            "uuid": "e6517.9",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency for Chain-of-Thought",
            "brief_description": "A decoding strategy for CoT that samples multiple reasoning paths and aggregates to select the most consistent answer across sampled chains.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Self-consistency (ensemble over sampled reasoning paths)",
            "reasoning_method_type": "ensemble / sampling",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "",
            "task_description": "Improve reasoning by aggregating answers across diverse sampled chains of thought.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "CyclePrompt (conceptually different: uses cycle-consistency not sampling aggregation)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited in related work; contrasts sampling-based diversity/ensemble approaches with CyclePrompt's iterative semantic updates.",
            "ablation_study_present": null,
            "uuid": "e6517.10",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning + Acting)",
            "brief_description": "An approach that interleaves reasoning traces with actions that query external environments or tools, enabling grounded decision-making.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "ReAct (reasoning + tool use)",
            "reasoning_method_type": "tool-augmented sequential reasoning",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "",
            "task_description": "Perform interactive reasoning that issues actions (tool calls) guided by chain-of-thought style reasoning.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "CyclePrompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned as related work that leverages external environments; CyclePrompt distinguishes itself by operating without external tooling.",
            "ablation_study_present": null,
            "uuid": "e6517.11",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Tree-of-Thoughts",
            "name_full": "Tree of Thoughts (ToT)",
            "brief_description": "A framework that explores multiple intermediate 'thoughts' arranged in a tree structure and performs search (e.g., MCTS) over them for improved problem solving.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "mention_or_use": "mention",
            "model_name": "",
            "model_size": "",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search / planning",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "",
            "task_description": "Explore and evaluate multiple candidate intermediate reasoning states to find high-quality final solutions.",
            "performance_metric": "",
            "performance_value": null,
            "comparison_target_method": "CyclePrompt",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited as related work that uses structured search over diverse reasoning paths; CyclePrompt uses iterative single-threaded semantic updates rather than tree search.",
            "ablation_study_present": null,
            "uuid": "e6517.12",
            "source_info": {
                "paper_title": "Learning How To Ask: Cycle-Consistency Refines Prompts in Multimodal Foundation Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "rating": 1,
            "sanitized_title": "unpaired_imagetoimage_translation_using_cycleconsistent_adversarial_networks"
        }
    ],
    "cost": 0.0160445,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LEARNING HOW TO ASK: CYCLE-CONSISTENCY REFINES PROMPTS IN MULTIMODAL FOUNDATION MODELS
13 Feb 2024</p>
<p>Maurice Diesendruck mdiesendruck@microsoft.com 
Microsoft Research</p>
<p>Jianzhe Lin jianzhelin@microsoft.com 
Microsoft Research</p>
<p>Shima Imani shimaimani@microsoft.com 
Microsoft Research</p>
<p>Gayathri Mahalingam gmahalingam@microsoft.com 
Microsoft Research</p>
<p>Mingyang Xu mingyangxu@microsoft.com 
Microsoft Research</p>
<p>Jie Zhao zhaojie@microsoft.com 
Microsoft Research</p>
<p>LEARNING HOW TO ASK: CYCLE-CONSISTENCY REFINES PROMPTS IN MULTIMODAL FOUNDATION MODELS
13 Feb 2024778F2BE4D80E47BFFC91957EBCB7A82FarXiv:2402.08756v1[cs.CL]
When LLMs perform zero-shot inference, they typically use a prompt with a task specification, and generate a completion.However, there is no work to explore the possibility of the reverse -going from completion to task specification.In this paper, we employ both directions to perform cycle-supervised learning entirely in-context.Our goal is to create a forward map f : X → Y (e.g.image → generated caption), coupled with a backward map g : Y → X (e.g.caption → generated image) to construct a cycle-consistency "loss" (formulated as an update to the prompt) to enforce g(f (X)) ≈ X.The technique, called CyclePrompt, uses cycle-consistency as a free supervisory signal to iteratively craft the prompt.Importantly, CyclePrompt reinforces model performance without expensive fine-tuning, without training data, and without the complexity of external environments (e.g.compilers, APIs).We demonstrate CyclePrompt in two domains: code generation and image captioning.Our results on the HumanEval coding benchmark put us in first place on the leaderboard among models that do not rely on extra training data or usage of external environments, and third overall.Compared to the GPT4 baseline, we improve accuracy from 80.5% to 87.2%.In the vision-language space, we generate detailed image captions which outperform baseline zero-shot GPT4V captions, when tested against natural (VQAv2) and diagrammatic (FigureQA) visual question-answering benchmarks.To the best of our knowledge, this is the first use of self-supervised learning for prompting.</p>
<p>Introduction</p>
<p>Prompting is the main way people interact with large language models (LLMs).With prompting, people can accomplish various tasks, e.g.code generation, question answering, and context summarization.Typically, the prompt consists of two parts, a task specification and the data to which the task applies; and given how costly it can be to fine-tune LLMs, modifying the prompt is often the best option for controlling model performance.One method, called "Reflexion" [1], lets LLMs refine a prompt through verbal reinforcement, based on learning from prior failings.Specifically, Reflexion converts feedback from an external environment (e.g.compilers, APIs) into verbal feedback in the form of a text summary, which is added as additional context to the task specification of the prompt.In doing this, the updated prompt yields better results.However, a limitation of Reflexion is the availability of an external environment, which can be seen as providing extra training data.To achieve verbal reinforcement in the absence of external environments, we propose a free self-supervision based on the concept of cycle-consistency.This approach is inspired by cycle-consistency in translation tasks, where translating a sentence into another language and back is expected to yield the original sentence.Of course, cycle-consistency is well-known in machine learning, and has been a component of loss functions in image generation [2] and in object tracking [3], among others.We for the first time extend this concept to prompting for LLMs, and demonstrate its effectiveness.</p>
<p>Figure 1: The AutoEncoder-like structure for CyclePrompt.In the example, the first cycle caption is: "A person in a red jacket with a denim sleeve is holding a hot dog with yellow mustard in a bun.They are wearing a purple scarf and a red hat with a flower on it.There is a glimpse of another person in a red jacket in the background.It appears to be nighttime, and there's a white canopy in the background, possibly indicating an outdoor event.",and the generated hint is: "The person's stance should be slightly bent forward, and the hot dog should be held in both hands with a napkin wrapped around it".The hint refines the prompt to improve the generation of the following cycle.</p>
<p>Approach</p>
<p>Reasoning Self-reflection External environments MultiModality CoT [4] ✓ × × × ReAct [5] ✓ × × × ToT [6] ✓ × × × RAP [7] ✓ × × × Self-Refine [8] ✓ × × × Beam Search [9] ✓ × × × Reflexion [1] ✓ ✓ ✓ × LATS [10] ✓ ✓ ✓ × CyclePrompt(Ours) ✓ ✓ × ✓ Table 1: A general comparison among state of the arts.CyclePrompt is the first work to achieve self-reflection without the support of external environments.Also, CyclePrompt achieve the reflection across different modalities.</p>
<p>To make the cycle-consistency possible, we propose the "Specification-Completion-Specification" cycle.Two examples are the Text-Code-Text and Image-Text-Image cycles, which correspond to the code generation and image captioning tasks, respectively.Formally, a cycle consists of a forward function f : X → Y (e.g.text → generated code), and a backward function g : Y → X (e.g.code → generated text).As can be expected, most pairings of f and g are imperfect, so to improve the cycle, we formulate a cycle-consistency "loss".This loss compares x and g(f (X)) and refines the prompt with the aim of reducing inconsistency.This routine is inspired by the autoencoder, though instead of generating a discriminative feature, it uses the cycle as a free supervisory signal to refine the prompt.We call the technique CyclePrompt, and visualize its flow in Fig. 1 and application to code generation and image captioning in Fig. 2.</p>
<p>In our experiments, when applied to code generation, CyclePrompt achieves third place on the leaderboard of the HumanEval benchmark [11], and achieves first place among unassisted models.When applied in the vision-language space, CyclePrompt produces highly detailed image captions that outperform baseline zero-shot GPT4V captions.These results underscore the potential of in-context, cycle-based reflection and refinement as a simple and powerful tool for optimizing model use.</p>
<p>Our main contributions are as follows.We:</p>
<p>• Introduce the "Specification → Completion → Specification" cycle for prompt refinement.</p>
<p>• Introduce a "semantic" cycle-consistency loss that extracts higher model performance without training data, expertise, or external systems.• Demonstrate our approach in code generation, achieving state-of-the-art results on the HumanEval dataset; and in image captioning, where question-answering ability of our captions exceeds that of zero-shot GPT4V captions.</p>
<p>• Provide insights into when and why our approach succeeds, including discussion on sensitivity and precision of cycle functions, and discussion about failure modes that present intriguing future directions for characterizing model misalignment.</p>
<p>CyclePrompt</p>
<p>Our approach involves three major components: a forward generator, a backward generator, and a discriminator.These components work together in an iterative refinement process to improve model performance.</p>
<p>The forward generator, denoted as f , is responsible for generating an output y given an input data x.Note that an input is made of two parts, the task specification t and the input data s, yielding f (x) = y where x = t + s.In the code generation task, the input data is a coding task and the output is a piece of code; in the image captioning task, the input data is an image and the output is a text caption.The backward generator, denoted as g, translates the output of the forward generator back into the original space as s ′ .</p>
<p>The discriminator plays a crucial role in the refinement process, and works iteratively from cycle to cycle.This function takes as input the original data s, mapped value y, and output from backward generator s ′ = g(y) = g(f (t + s)), and produces a hint, hint = d(s, s ′ , y).The hint is used to update the current value of s for the next round.The entire cycle procedure is visualized in Fig. 2. Note that the hint is always generated based on the inconsistency between the currently generated data s ′ and the original data s.</p>
<p>This process is repeated for N cycles to achieve cycle-consistency, i.e.Consistent(s, s ′ ) == True.The goal is to minimize inconsistency between the original data s and the translated output s ′ , thereby improving the performance of the model.The ultimate goal is to get final output y N .</p>
<p>While a convergence measure could be used to determine when to stop the iterative process, we leave the exploration of such a measure for future work.In this work, we focus on demonstrating the effectiveness of the CyclePrompt approach through a fixed number of cycles (we empirically use 4 cycles for both code generation and image captioning).</p>
<p>Mathematical Formulation</p>
<p>The following mathematical formulation more closely specifies the function inputs and outputs, noting the cycle indices.We define forward generator f : X → Y , backward generator g : Y → X, and discriminator d :
X × X × Y → X.
Consider for cycle i, we have task specification t, original data s 0 , current data s i , current input x i = t + s i , and current</p>
<p>Algorithm 1 CyclePrompt</p>
<p>Input: original data s 0 , task specification t, input x i = t + s i number of cycles N , forward generator f , backward generator g, discriminator d Apply initial cycle g(f
(x 0 )) = g(y 1 ) = x 1 . for i = 1 to N − 1 do Apply f to x i−1 to get y i = f (x i−1 )
Apply g to y i to get g(f
(x i )) = s i Apply d to (s 0 , s i , y i ) to get hint i+1 Update x i+1 = t + s i + hint i+1 if Consistent(s 0 , s i+1 ) then Break end if end for output y i = f (x i−1 )
. The discriminator compares s 0 and s i to produce a hint, i.e. d(s 0 , s i , y i ) → hint i .This hint is used for two updates: we update current data s i+1 = s i + hint i , and update current input
x i+1 = t + s i+1 .
The iterative refinement process is performed N times, or until s 0 and s i are consistent.This is formalized as:
x 0 = t + s 0 (1) y i = f (x i−1 ) = f (t + s i−1 )(2)s i = g(y i ) (3) x i = t + s i (4) hint i+1 = d(s 0 , s i , y i )(5)s i+1 = s i + hint i+1 (6) x i+1 = t + s i+1(7)
Algorithm 1 demonstrates the approach in pseudocode.</p>
<p>Datasets</p>
<p>The selection of datasets for our experiments was guided by the need to test our approach across different domains and tasks.We chose three datasets: HumanEval, VQAv2, and FigureQA.These datasets cover a range of tasks, including code generation and vision-language tasks, allowing us to evaluate the versatility and effectiveness of our method.</p>
<p>HumanEval</p>
<p>HumanEval [11] is a dataset created by OpenAI for the purpose of evaluating the performance of AI models in generating code.It consists of a series of tasks that require the model to generate Python code to solve a given problem.The tasks are designed to be solvable by a competent programmer in a few minutes and cover a wide range of topics, including string manipulation, data structures, algorithms, and more.The dataset was created as part of the HumanEval competition, where models are ranked based on their performance on these tasks.</p>
<p>In our experiments, we use the HumanEval dataset to test our approach on code generation tasks.We use the tasks in the dataset as inputs to our model, and evaluate the quality of our generated code using the evaluation methods provided by the benchmark authors.</p>
<p>VQAv2</p>
<p>The Visual Question Answering version 2 (VQAv2) dataset [12] is a large-scale dataset for vision-language tasks.It contains open-ended questions about images, which require an understanding of the content of the image to answer.The dataset was created to improve upon the limitations of the original VQA dataset, with a particular focus on reducing biases in the data.</p>
<p>In our experiments, we use the VQAv2 dataset to test our approach on vision-language tasks.We use the images in the dataset as inputs to our model, and evaluate the quality of our generated captions, using the ground truth question-answer pairs provided by the authors.In our experiments, we use the FigureQA dataset to further test our approach on vision-language tasks, knowing that models sometimes perform worse in subdomains that are different from natural images.Similar to with VQAv2, we evaluate the quality of our generated captions using the ground truth question-answer pairs provided by the authors.</p>
<p>Experiment: Code Generation</p>
<p>To perform code generation, we run CyclePrompt on the Text-Code-Text cycle.Here, we begin with a language description of our coding task, and translate it to Python code (this is the forward generator).Then, we describe that generated code, yielding a new description (this is the backward generator).Finally, we observe three things -original description, new description, and generated code -and by comparing the two descriptions, we determine a better task description that would produce code whose description matches the original (this is the discriminator).Intuitively, if the description of our generated code matches our original task description, then we have accomplished the task faithfully.This cycle-consistency can be done entirely within a single model (GPT4, in our case), and yields precise code and descriptions after several cycles.</p>
<p>CyclePrompt performs well on code generation tasks by exploiting two important dynamics: (1) the model has good understanding of code (in this case Python), and (2) the code generation step is sensitive to precise changes described in language.Together, this means that refinements to task descriptions at each cycle do actually improve code generation in subsequent rounds.We will see that such sensitivity does not exist in the vision-language space, which hampers performance there.</p>
<p>Table 2 demonstrates the results on the HumanEval code generation benchmark.Our CyclePrompt method reaches third place overall on the leaderboard, with accuracy of 87.2%, beaten only by methods that utilize outside tooling.Among methods that are purely prompt-based, without the use of external tools, our method is first.</p>
<p>We provide exact prompts used in Appendix A.</p>
<p>Experiment: Image Captioning</p>
<p>To perform image captioning, we run CyclePrompt on the Image-Text-Image cycle.Here, we begin with an image input, and generate a caption for it (this is the forward generator).Then, we use that caption to generate a new image (this is the backward generator).Finally, we observe three things -original image, new image, and generated captionand by comparing the two images, we create an updated caption [and subsequently generated image], whose caption would generate an image that matches the original (this is the discriminator).If these generated images match our original image, then we have accomplished the task faithfully.</p>
<p>We share examples of discriminator input and image generation outputs for VQAv2 and FigureQA in Fig. 5 and Fig. 8, respectively, and provide corresponding caption generation outputs in Appendix B.3.</p>
<p>Evaluation metrics</p>
<p>To test whether our generated captions are useful, we measure accuracy on well-known question-answer (QA) datasets.</p>
<p>As an upper limit of performance, we compute QA accuracy of a VQA model which see the image itself, and then we measure for our approach, i.e.QA accuracy given only generated captions.We also employ the DA-Score method of [12] to measure vision-language alignment.</p>
<p>In summary, considering original image I, caption C, question Q, and answer A, we measure with the following tools, for various settings:
Visual QA Model(I, Q) → A (8) Text QA Model(C, Q) → A DA-Score(I, Q) → A
Our suite of metrics aims to measure whether our generated captions are (1) as useful for question-answering as the original image, and (2) correct in content and details.</p>
<p>Visual QA is used to test how well a model can answer question based on the original image itself.This is considered the "gold standard" performance, since the model has full information.We use GPT4V as a VQA model, with image and question as input, and generate an answer.The answer is then compared to the ground truth answer.</p>
<p>Text QA is our primary metric, focusing on the question-answering ability of a piece of text.We want to know: "Can the CyclePrompt-generated caption provide as much information for question-answering as the original image?"For this case, we use GPT4 as a text QA model, with caption and question as input, and generate an answer.The answer is then compared to the ground truth answer.Here, we seek a comparable baseline, and use captions provided in a zero-shot manner from GPT4V, when asked to produce a detailed caption.</p>
<p>DA-Score: The metrics discussed so far are unidirectional, i.e. they check whether information from the image exists in the caption.But what if our candidate caption introduces spurious details?To account for this, we use the Decompositional-Alignment-Score [12].This score is given a complex text, and decomposes it into a set of disjoint assertions.The alignment of each assertion with images is then measured using a BLIP VQA model.Finally, alignment scores for different assertions are combined a posteriori to give the final text-to-image alignment score.</p>
<p>Experimental setup</p>
<p>We use GPT4V as our forward generator to map images to text, and DALL•E 3 as our backward generator to map text to images.To compare images in our discriminator function, we use an independent instance of GPT4V.For each dataset, we fix 200 random samples for all experimental settings.We find this subsample size large enough to contain diverse examples, while small enough to accommodate compute time and budget.In Table 3, the methods compared are: (1) GPT4V(image) where GPT4V answers benchmark questions directly from the image, (2) GPT4V(our caption) where accuracy and DA-Score are computed for questions generated from CyclePrompt captions, (3) GPT4V(0-shot caption) where accuracy and DA-Score are computed for questions generated by zero-shot GPT4V captions, and (4) DA-baseline where DA-Score is evaluated based on image and benchmark questions.</p>
<p>We provide exact prompts used in Appendix B.</p>
<p>Captioning Performance</p>
<p>Table 3 includes performance metrics for CyclePrompt, compared to various baselines.ACC bench is accuracy on benchmark questions, and DA-score measures image-caption alignment.</p>
<p>In VQAv2 results, GPT4V(image) performs best, with accuracy value of 0.820 for benchmark questions.This means that 82% of questions were answered correctly, when given the image and benchmark question directly.As expected, accuracy is lower when only captions are available, but CyclePrompt captions perform on par with or better than zero-shot GPT4V captions, when comparing accuracy (0.652 versus 0.632) and DA-Scores (0.682 versus 0.699 [positive assertions only] and 0.479 versus 0.509 [negative assertions included]).</p>
<p>For the diagrammatic images in FigureQA, GPT4V also gives highest accuracy (0.584), but again CyclePrompt achieves higher accuracy compared to zero-shot GPT4V (0.512 versus 0.477).We believe that CyclePrompt allows the model to collect details that would not have been described in a single pass.DA-Scores for CyclePrompt (0.557, 0.237) are also higher than those of zero-shot GPT4V (0.553, 0.231) indicating that the assertions generated from CyclePrompt captions were more aligned to the original image than assertions generated from zero-shot GPT4V captions.The DA-Scores for DA-baseline (0.637, 0.391) are the highest, possibly due to the relative simplicity of benchmark questions, compared to questions from highly-detailed captions, which often contained technical or aesthetic details. Figure 5: CyclePrompt inputs and outputs.Final caption: "Several bright green apples with a smooth, shiny texture are placed in a white bowl with a wide rim.The apples are unblemished, except for one in the foreground that has a small, dark indentation near the stem.The bowl sits on a dark surface, and the background is a blurred, dark brown, providing a stark contrast to the vibrant green of the apples.The apples are closely packed together, with one apple prominently in the foreground, slightly obscuring the apples behind it.The lighting is soft and diffused, highlighting the apples' texture and color.The apples have visible white speckles, and the bowl has a subtle shadow cast on the right side.The apples appear more matte than glossy, and the bowl's rim is thick and slightly curved outward."</p>
<p>Case Study: Image Generation</p>
<p>We observe different dynamics in the image generation case, i.e. the Text-Image-Text cycle.This is due to the relatively higher complexity of the target image space, making the cycle unbounded.As a reminder of the opposite, recall the image captioning case.There, our input is an image, which is higher-complexity and detail-rich; and our output is a caption, which is lower-complexity and naturally summarizing.This dynamic is stable, because generated captions are always less detailed than the image.In contrast, when generating images from text, generated images continuously introduce new details, all of which can be consistent with the original text.</p>
<p>Figure 12 provides an extreme example to illustrate.If we begin from text prompt "a happy day", and perform a cycle by generating an image and then captioning it; then our updated caption will express details introduced by our image generator.Normally, this is acceptable, but because text is naturally lower-complexity than images, the image generator continues to introduce new details, while still complying with the original input text.This is shown in Fig. 9, where generated images change gradually as the cycles progress (from wedding photoshoot to generic garden walk), all while satisfying the original input "a happy day".</p>
<p>To demonstrate the inverse relationship of complexity (where we would seek to use CyclePrompt), we show in Fig. 10 what happens if we start the cycle with an image.Here, we think of the image as more prescriptive than text, thereby keeping even long sequences of cycles constrained and converged.Figure 11 provides another example of this convergence, for 20 cycles.</p>
<p>Discussion</p>
<p>CyclePrompt can give state-of-the-art performance without external tooling, but its utility obviously depends on the strength of the forward, backward, and discriminator functions.</p>
<p>We find that the forward generator is most important, followed by the discriminator, and finally the backward generator.</p>
<p>The forward direction is most critical because without a strong forward generator, the system does not have the ability to execute on feedback.In the Text-Code-Text cycle, we see that our model (GPT4) is extremely responsive to changes in our input text, and it faithfully translates text instructions into code.In the Image-Text-Image cycle, we similarly see that our model (GPT4V) reliably provides accurate descriptions for images.In short, without a good forward generator, the system could not improve, even with a good signal.</p>
<p>The discriminator is of secondary importance, because it defines the upper limit of how much refinement CyclePrompt can deliver.Because instructions are pass via text, we benefit from the excellent language understanding of our model.</p>
<p>Finally, a strong backward generator is indeed helpful, but CyclePrompt can still produce useful results when it is weak.This is most evident in the Image-Text-Image cycle, where image generation is weaker, i.e. generated images frequently do not comply with highly detailed captions.Even still, the fact that it samples in a neighboring semantic space, paired with a good discriminator, can inform how to more precisely caption the original image.</p>
<p>Related Work</p>
<p>The field of LLMs has made significant strides in recent years [14,15,16,17,18,19,20,21].These advancements not only expanded the application of LLMs, but also highlighted the importance of effective interaction with these     When the input space is lower-complexity (e.g.text to image, in 9), the output space can comply with the input while continuously changing.When the input space is higher-complexity (e.g.image to text, in 10,11), both spaces are constrained and converge.</p>
<p>models.Substantial research has focused on prompt engineering to elicit specific responses from Large Language Models [22,23], and numerous approaches to prompt engineering have been explored, including context learning, chain-of-thought, and multi-turn prompting techniques [22,24,6,5,8].These strategies enhance our interaction with LLMs, enabling users to obtain more accurate and relevant responses from models.</p>
<p>In the subfield of context learning, few-shot examples in the prompt were shown to enhance model performance [15].But the technique is not flawless, particularly when addressing more intricate reasoning tasks.To address this, [22] proposed Chain-of-Thought Prompting (CoT), which provides LLMs with step-by-step reasoning examples instead of standard question-answer examples.This helps models to generate a reasoning path that breaks down complex reasoning into multiple, more manageable steps [25,22].</p>
<p>To further enhance CoT Prompting, [4] introduced a decoding strategy called self-consistency, which replaces the naive greedy decoding used in CoT Prompting.Self-consistency employs a set of manually written chain-of-thought examples, and samples a collection of candidate outputs from the language model's decoder.Finally, answers are combined by considering the sampled reasoning paths and choosing the most consistent answer among the generated options [4].Another related work that expands CoT prompting technique is the Tree of Thoughts (ToT) framework [6], which facilitates the investigation of cohesive text segments, known as 'thoughts'.These thoughts act as intermediary stages in the process of solving problems.</p>
<p>Another work that enhances CoT reasoning is ReAct [5].ReAct integrates reasoning and acting with language models to address language reasoning and decision-making tasks.This method enables the model to carry out dynamic reasoning for creating, maintaining, and adjusting high-level action plans, while interacting with external environments such as Wikipedia to incorporate supplementary information.This is an improvement over the CoT reasoning, which is a static black box and relies on its internal representations to generate thoughts and is not grounded in the external world.</p>
<p>In the space of translation, Iterative Domain-Repaired Back-Translation [26] focuses on addressing the challenges of domain-specific translation in low-resource settings by utilizing in-domain monolingual data through the backtranslation method.The authors propose a framework that incorporates a Domain-Repair (DR) model to enhance the quality of translations in synthetic bilingual data.The DR model is trained on round-trip translated monolingual sentences and aims to jointly optimize both the DR and Neural Machine Translation (NMT) models.This innovative approach demonstrates the potential of the DR model in refining translations and improving domain adaptation in scenarios with limited resources.</p>
<p>There has also been significant work in "prompt learning" (also known as "prompt tuning", "context optimization", and "soft prompting") where optimization is performed in the embedding space of the prompt, rather than in the semantic space [27,28,29,30].These methods require additional training, as well as access to internals of vision-language models, to pass in custom prompt embeddings.Our method, by contrast, is an exclusive black-box model user.</p>
<p>Most relevant to our work is SELF-REFINE [8], which also employs iterative prompting.This technique aims to improve the initial outputs of LLMs by employing ongoing feedback and refinement processes.The fundamental idea consists of generating an initial output with an LLM, then having the same LLM provide feedback on its output and iteratively use this feedback for self-enhancement.SELF-REFINE performs in-context learning with few-shot examples of (input, output, feedback) triples, in order to predict feedback for refinement.Our work differs in two important ways:</p>
<p>(1) Our approach requires no training data or expertise to create in-context examples, and (2) our approach is well-suited for multimodal applications, where a map [and its inverse] are commonly developed between a new modality and language, thus enabling the desired cycle-supervision.</p>
<p>Finally, the notion of feedback as a "semantic" or textual gradient is also explored in [9] and [1], but both require ground truth data or an external environment to score the value of potential changes.</p>
<p>Future Work</p>
<p>Our work with CyclePrompt has opened several paths for future research.</p>
<p>First, the method can be compared to and combined with multi-agent systems with access to external environments.A hybrid of these approaches might combine their strengths to produce a problem-solving system that is both efficient and high-performing.</p>
<p>Second, we note that the success of CyclePrompt is tied to specific dynamics, including the quality of the forward function, the precision of the discriminator, and to a lesser extent, the quality of the backward function.Future research could further investigate these dynamics to characterize modality misalignment; and recognition of such asymmetries in sensitivity, precision, and complexity could lead to improved understanding of model knowledge.</p>
<p>Finally, an intriguing question is how to formalize the notion of "semantic gradient descent".What are the necessary conditions for the discriminator to enable long-range optimization in the semantic space?What stopping criteria, limits, or convergence properties can be derived?Machine learning models perform millions of updates to numerical parameters.Can thousands or millions of semantic updates be run, instead of less than ten, as is typical in the existing research?Can monotonic improvement be guaranteed?</p>
<p>We believe this work provides a starting point for exploring these topics, demonstrating both interesting open questions about foundation model knowledge, as well as practical strategies for extracting state-of-the-art performance from existing systems.</p>
<p>A Code Generation</p>
<p>Below are the prompts used for code generation.</p>
<p>A.1 Forward Generator</p>
<p>Given the code below :</p>
<p>[ code ] please conclude and describe the task of the code .</p>
<p>A.2 Backward Generator</p>
<p>You are a professional programmer .You will be given a coding task .</p>
<p>Please use python to write the code .</p>
<p>Your response should include only python code .no code comment , no description , no commentary , no docstring , just the python code .</p>
<p>Attention : NO code comment .</p>
<p>A.3 Discriminator</p>
<p>We have two procedures , roughly corresponding to : 1. " go from task description to code " , and 2. " go from code to task description ".</p>
<p>We can achieve a cycle consistency , e .g .description -&gt; generated code -&gt; description , if the original task description and concluded task descriptions are equivalent .This cycle consistency can be achieved only if the generated code is correct .If the code is wrong , the concluded task description from the code will be different from original task description .</p>
<p>The original task description is : [ Task description ]</p>
<p>The generated code is :
[ code ]
The concluded task description is : [ Conclusion ].Our ultimate goal is to generate the correct code .Please try to find the potential errors / mistakes in the generated code , by observing and reflecting on the differences between the original task description and the concluded task description .Then advise on : 1. how to avoid potential mistakes / errors in the code ; 2. how to simplify the code .The entire response should focus on the specific advice to improve the code quality .</p>
<p>A few additional points :</p>
<p>(1) If you find inconsistency in the cycle , respond using the template below , where some example notes are provided in parentheses :</p>
<hr />
<p>The original task is xxx ( e .g ., write a function counting from 0 to 10) , while the concluded task is xxx ( e .g ., write a function counting from 0 to infinity ) .</p>
<p>The difference is xxx ( e .g ., the range was changed ) .</p>
<p>The cause of the inconsistency is the generated code xxx ( e .g ., fail to set max value of range in for loop ) .</p>
<p>Therefore , my advice is :</p>
<p>xxx ( e .g ., ensure that the endpoints of the range are set correctly ) .</p>
<hr />
<p>(2) If you find that the cycle consistency has been achieved , respond using the template below :</p>
<h2>-</h2>
<p>The cycle is consistent , and I have no more advice .</p>
<p>B Vision-Language B.1 Discriminator Function</p>
<p>The following Python code snippet is the implementation of the discriminator or update function used in our CyclePrompt approach.This function, named update_description, compares the original image and the generated image, and updates the description of the original image based on the differences observed.</p>
<p>This function is called during each cycle of the CyclePrompt process, specifically in the reflection and refinement step.It plays a crucial role in achieving cycle-consistency and improving the performance of the model.</p>
<p>def u pd at e_description (</p>
<p>""" Compare the original image and the generated image .""" # Create composite image with original image on left and generated image on right c o m p o site_image_path = os .path .join ( save_dir , f " composite_ { cycle_index }. png " ) c r e a t e _ c o m p o s i t e _ i m a g e _ p l t ( original_image_path , generated_image_path , c o mposite_image_path ) # Define prompt to compare images and create new description prompt = f """ We are machine learning scientists , who are experimenting with cycle consistency in image generation .</p>
<p>The cycle we are testing is as follows .Given a reference image : 1. Generate a description of the reference image .</p>
<ol>
<li>
<p>Use the description to generate a candidate image .</p>
</li>
<li>
<p>Compare the candidate image to the reference image .4. Write an updated description of the reference image .The key to this step is that whatever differences are detected , they represent things that should be in the reference description , so that the new image is generated correctly ( i .e .as close as possible to the reference ) . 5. Go back to step 2 , and repeat the cycle .</p>
</li>
</ol>
<p>We are currently doing steps 3 and 4 , and we need your help .</p>
<p>The REFERENCE image is on the LEFT SIDE , and the CANDIDATE image is on the RIGHT .The current description of the reference image is : { description } Think about how the reference image is different from the candidate image , and write a new description of the reference image that takes into account those differences .</p>
<p>For example , suppose you have : reference image : photo of black cat on brown leather sofa candidate image : illustration of black cat on cloth sofa description : " black cat on sofa " Then the updated description would be something like :</p>
<p>" photograph of black cat on brown leather sofa " because the reference image is a photograph , not an illustration , and the sofa is brown leather , not cloth .</p>
<p>Here are some tips on how to compare two images :</p>
<p>-</p>
<p>B.3 Sample Cycle Descriptions</p>
<p>Application of CyclePrompt for image captioning yields detailed descriptions.In each cycle, the discriminator provides feedback about differences between the original image, and the current cycle's image generation.Based on those differences, the system refines the caption to become more and more descriptive.Below are descriptions generated for the examples shown in Figs. 5 and 8.</p>
<p>Figure 5 VQAv2 Cycle</p>
<p>{" cycle ": 0 , " text ": " There are several green apples in a white bowl .The apples have small brown spots and the one in the front has a small indentation near the stem .The background is dark and out of focus ."}{" cycle ": 1 , " text ": " Several bright green apples with a smooth , shiny texture are placed in a white bowl with a wide rim .The apples are unblemished , except for one in the foreground that has a small , dark indentation near the stem .</p>
<p>The bowl sits on a dark surface , and the background is a blurred , dark brown , providing a stark contrast to the vibrant green of the apples ."}{" cycle ": 2 , " text ": " Several bright green apples with a smooth , shiny texture are placed in a white bowl with a wide rim .The apples are unblemished , except for one in the foreground that has a small , dark indentation near the stem .</p>
<p>The bowl sits on a dark surface , and the background is a blurred , dark brown , providing a stark contrast to the vibrant green of the apples .The apples are closely packed together , with one apple prominently in the foreground , slightly obscuring the apples behind it .The lighting is soft and diffused , highlighting the apples ' texture and color ."}{" cycle ": 3 , " text ": " Several bright green apples with a smooth , shiny texture are placed in a white bowl with a wide rim .The apples are unblemished , except for one in the foreground that has a small , dark indentation near the stem .</p>
<p>The bowl sits on a dark surface , and the background is a blurred , dark brown , providing a stark contrast to the vibrant green of the apples .The apples are closely packed together , with one apple prominently in the foreground , slightly obscuring the apples behind it .The lighting is soft and diffused , highlighting the apples ' texture and color .The apples have visible white speckles , and the bowl has a subtle shadow cast on the right side ."}{" cycle ": 4 , " text ": " Several bright green apples with a smooth , shiny texture are placed in a white bowl with a wide rim .The apples are unblemished , except for one in the foreground that has a small , dark indentation near the stem .</p>
<p>The bowl sits on a dark surface , and the background is a blurred , dark brown , providing a stark contrast to the vibrant green of the apples .The apples are closely packed together , with one apple prominently in the foreground , slightly obscuring the apples behind it .The lighting is soft and diffused , highlighting the apples ' texture and color .The apples have visible white speckles , and the bowl has a subtle shadow cast on the right side .The apples appear more matte than glossy , and the bowl ' s rim is thick and slightly curved outward ."}When the input space is lower-complexity (e.g.text to image, in Fig. 13), the output space can comply with the input while continuously changing.When the input space is higher-complexity (e.g.image to text, in Fig. 14), both spaces are constrained and converge.</p>
<p>Figure 2 :
2
Figure 2: A general flowchart for CyclePrompt, with applications for code generation and image captioning.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Composite input to discriminator</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Composite input to discriminator</p>
<p>Figure 8 :
8
Figure 8: CyclePrompt inputs and outputs.Final caption: "A horizontal bar graph with eight bars in distinct colors, each labeled with a color name on the left side.The bars are arranged from top to bottom in the order of Dark Cyan, Sky Blue, Deep Sky Blue, Chocolate, Deep Pink, Dim Gray, Medium Periwinkle, and Rebecca Purple.The x-axis is labeled 'xaxis label' with a scale from 0 to 100, and the y-axis is labeled 'yaxis label.'The graph has a title at the top that reads 'title.'The bars have varying lengths representing different values on the x-axis, with Dark Cyan being the longest and Rebecca Purple being the shortest.The graph is a clear, 2D representation with no grid lines, and the bars are solid with no patterns or textures.The title, axis labels, and color labels are all clearly legible.The graph background is white, and the bars are not stacked."</p>
<p>FigureQA.*Reported on subset, due to compute time constraints.</p>
<p>Figure 9 :Figure 10 :
910
Figure 9: Text-Image-Text cycle for "A happy day"</p>
<p>Figure 11 :
11
Figure 11: Twenty cycles in Image-Text-Image.</p>
<p>Figure 12 :
12
Figure 12: Comparison of Text-Image-Text (image generation) to Image-Text-Image (image captioning).When the input space is lower-complexity (e.g.text to image, in 9), the output space can comply with the input while continuously changing.When the input space is higher-complexity (e.g.image to text, in 10,11), both spaces are constrained and converge.</p>
<p>Figure 8
8
Figure 8 FigureQA Cycle</p>
<p>Figure 13 :Figure 14 : 13 B. 4
1314134
Figure 13: Text-Image-Text cycle for "Standing at the edge"</p>
<p>Table 2 :
2
Code Generation Results on HumanEval Benchmark
METHODACC.SUPPORT INFRASTRUCTUREGPT480.5-PARSEL85.1CODEX, CONSTRAINT SOLVERMETAGPT85.9INTERPRETER, WEB, PUBSUBANPL86.6USER INTERACTIONOCTOPACK86.6FINE-TUNECP (OURS)87.2-REFLEXION91.0COMPILER, INTERPRETERLATS94.4MCTS3.3 FigureQAFigureQA [13] is a dataset created by Microsoft Research for the task of visual reasoning on diagrammatic and statisticalimages. It contains questions and answers about synthetically generated figures, including bar graphs, line graphs, andpie charts. The dataset was created to push the boundaries of what models can achieve in the realm of visual reasoning,with a focus on understanding and interpreting visual data.</p>
<p>Table 3 :
3
Comparison of VQA Accuracies
METHODAcc bench DA-SCORE (P, N)GPT4V(IMAGE)0.820-GPT4(OUR CAPTION)0.6520.682, 0.479GPT4(0-SHOT CAPTION)0.6320.699, 0.509DA-BASELINE-0.467, 0.102</p>
<p>Feature Correspondence : Do distinct features ( edges , corners , textures , etc .) in one image correspond to the same features in the other image ?differences exist , describe the REFERENCE in terms of those features .-Geometric Consistency : Do the spatial relationships between features within the images remain consistent .For example , if one image has large trees to the right of a tent , does the other image also have large trees to the right of the tent , or are the spatial relationships swapped or different ?Or if the subject is facing one direction in one image , is it facing the same direction in the other image ?If differences exist , describe the REFERENCE in terms of those relationships .-Photometric Consistency : Do the images have consistent appearance in terms of lighting , color , and intensity .If differences exist , describe the REFERENCE in terms of those differences in appearance .-Style Consistency : Are the image styles ( photograph , painting , drawing , diagram , medical image ) the same ?If differences exist , describe the REFERENCE in terms of those differences in style .-Semantic Consistency : Do objects and their parts maintain their identity and meaning across the images .For instance , a wheel of a bicycle should still be identifiable as a wheel of a bicyle in the other image .If differences exist , describe the REFERENCE in terms of those differences in semantics .The above tips are useful for natural images .For graphical , statistical , or diagrammatic images , focus on the data itself and what kind of reasoning is being conveyed .-Keep overall response to about 130 words or less .Feel free to shorter phrases or incomplete sentences , if it helps to include important details .
)logging . info ( f " \ n \ nCYCLE ␣ { cycle_index } ␣ COMPLETE " )logging . info ( f " Compared ␣ { original_image_path } ␣ with ␣ { generated_image_path } " )logging . info ( f " Current ␣ description : ␣ { description } " )logging . info ( f " Updated ␣ description : ␣ { updated_description } " )return updated_descriptionB.2 Zero-shot GPT4V Image Caption PromptDescribe this image in detail . Don ' t refer to ' This image ' or ' This picture '.Just describe what you see in short , simple terms , but be as specific aspossible . Consider the following categories while describing :-Feature Correspondence : Distinct features ( edges , corners , textures , etc .)-Geometric Consistency : Spatial relationships between features-Photometric Consistency : Appearance in terms of lighting , color , and intensity-Style Consistency : Image styles ( photograph , painting , drawing , diagram ,medical image )-Semantic Consistency : Objects and their parts maintaining their identity andmeaning-Structural Integrity : Overall structure of the objects in the imagesNote :-IMPORTANT :-Structural Integrity : Is the overall structure of the objects inthe images preserved across the images ? There should be nounnatural distortions or warping that compromise the object 'srecognizability . If differences exist , describe the REFERENCE interms of those differences in structure .NOTES :-IMPORTANT : The above tips are useful for natural images . Forgraphical , statistical , or diagrammatic images , focus on the dataitself and what kind of reasoning is being conveyed .-Make sure to retain the major components or reasoning of theREFERENCE image .-In the new description NEVER mention the reference or candidateimages , i . e . DO NOT include a header or preamble like ' Thereference image ... ' or ' The image on the left ... '.-ONLY output the new description . No other text , other than the newdescription .-If the candidate image misses something , or contradicts thereference image in any of the ways described in the tips above ( orotherwise ) , then EMPHASIZE this thing in the new referencedescription .-Keep overall response to about 130 words or less . Feel free toshorter phrases or incomplete sentences , if it helps to includeimportant details .The new description of the reference image is :"""# Run GPT4V on composite image , to get an updated descriptionu pd a t ed_description = self . gpt4v_service . call (composite_image_path , prompt , self . config [ " gpt4v_retries " ] , verbose = True</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017</p>
<p>Learning correspondence from the cycle-consistency of time. Xiaolong Wang, Allan Jabri, Alexei A Efros, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.176512023arXiv preprint</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Language agent tree search unifies reasoning acting and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, arXiv:2310.044062023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Conference on Computer Vision and Pattern Recognition (CVPR). 2017</p>
<p>Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, Yoshua Bengio, arXiv:1710.07300Figureqa: An annotated figure dataset for visual reasoning. 2017arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, arXiv:2304.137122023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, arXiv:2306.116442023arXiv preprint</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, arXiv:2009.071182020arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Iterative domain-repaired back-translation. Hao-Ran, Zhirui Wei, Boxing Zhang, Weihua Chen, Luo, arXiv:2010.024732020arXiv preprint</p>
<p>Improving generalization of image captioning with unsupervised prompt learning. Hongchen Wei, Zhenzhong Chen, arXiv:2308.028622023arXiv preprint</p>
<p>Prompt-aligned gradient for prompt tuning. Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, Hanwang Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Conditional prompt learning for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>A systematic survey of prompt engineering on vision-language foundation models. Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao, Yao Qin, Philip Volker Tresp, Torr, arXiv:2307.129802023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
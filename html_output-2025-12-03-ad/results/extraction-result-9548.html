<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9548 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9548</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9548</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-166.html">extraction-schema-166</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using LLMs or related models to distill, extract, or discover quantitative laws, equations, or mathematical relationships from large collections of scholarly papers, including methods, results, challenges, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268297267</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.05156v2.pdf" target="_blank">On Protecting the Data Privacy of Large Language Models (LLMs): A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language. They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks. When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy. This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding. Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs. Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9548",
    "paper_id": "paper-268297267",
    "extraction_schema_id": "extraction-schema-166",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.006382499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>On Protecting the Data Privacy of Large Language Models (LLMs): A Survey
14 Mar 2024</p>
<p>Biwei Yan 
Kun Li 
Minghui Xu 
Yueyan Dong 
Yue Zhang 
Zhaochun Ren 
Xiuzhen Cheng 
On Protecting the Data Privacy of Large Language Models (LLMs): A Survey
14 Mar 2024C1B43CE6B5154B5A050A447375C38986arXiv:2403.05156v2[cs.CR]Large Language Models (LLMs)SecurityData PrivacyPrivacy ProtectionSurvey
Large language models (LLMs) are complex artificial intelligence systems capable of understanding, generating and translating human language.They learn language patterns by analyzing large amounts of text data, allowing them to perform writing, conversation, summarizing and other language tasks.When LLMs process and generate large amounts of data, there is a risk of leaking sensitive information, which may threaten data privacy.This paper concentrates on elucidating the data privacy concerns associated with LLMs to foster a comprehensive understanding.Specifically, a thorough investigation is undertaken to delineate the spectrum of data privacy threats, encompassing both passive privacy leakage and active privacy attacks within LLMs.Subsequently, we conduct an assessment of the privacy protection mechanisms employed by LLMs at various stages, followed by a detailed examination of their efficacy and constraints.Finally, the discourse extends to delineate the challenges encountered and outline prospective directions for advancement in the realm of LLM privacy protection.</p>
<p>I. INTRODUCTION</p>
<p>In recent years, Large Language Models (LLMs) have emerged as pivotal players in the realm of artificial intelligence, revolutionizing various fields such as natural language processing [1], [2], embodied AI [3]- [5], AIgenerated content (AIGC) [6], [7].LLMs, trained on massive datasets, possess the remarkable ability to generate humanlike text, answer complex queries, and undertake a myriad of language-related tasks with unprecedented accuracy and fluency.However, amidst the excitement surrounding the capabilities of LLMs, concerns about data privacy have garnered increasing attention [8].</p>
<p>On one hand, LLMs may be subject to passive privacy leakage.Users can inadvertently expose sensitive data to ChatGPT if they input such information into the chat interface.For example, Samsung Electronics experienced inadvertent leakage of sensitive company data through ChatGPT in three distinct occurrences.Besides, LLMs often rely on vast amounts of data for training, including text scraped from the internet, publicly available datasets, or proprietary sources.This data aggregation process can raise significant data privacy B. Yan, K. Li, M. Xu, Y. Dong, X. Cheng are with the School of Computer and Science and Technology, Shandong University.Email: {bwyan, kli, mhxu, xzcheng}@sdu.edu.cnZ. Ren is with Leiden University.Email: {z.ren@liacs.leidenuniv.nl}Y. Zhang is with the Department of Computer Science, Drexel University.Email: {yz899@drexel.edu}concerns, especially when dealing with sensitive or personally identifiable information (PII) [9].LLMs have been shown to have the potential for memorization of training data, raising concerns about inadvertent leakage of sensitive information during inference [10].Even with techniques such as differential privacy or federated learning, which aim to mitigate privacy risks during training, residual traces of sensitive data may still persist within the model's parameters [11].</p>
<p>On the other hand, LLMs may be vulnerable to active privacy attacks.The deployment of fine-tuned LLMs in various applications introduces additional security challenges.Finetuning or adapting pre-trained LLMs to specific tasks may inadvertently expose them to the exploitation of vulnerabilities, potentially compromising the confidentiality, integrity, or availability of sensitive information [12].For example, to bypass the model's inherent alignment, a prompting strategy was devised that induces GPT-3.5-turbo to "diverge" from producing conventional responses, instead emit training data [13].Pre-existing vulnerabilities such as backdoor attacks, membership inference attacks, and model inversion attacks can be leveraged against pre-trained or fine-tuned models with the objective of illicitly acquiring sensitive data.</p>
<p>To portray the current situation, we outline the present state of research concerning privacy safeguards for LLMs in Fig. 1.Taking into account academic papers on privacy protection and the model list from Hugging Face, we have compiled a list of popular LLMs in the figure.The timeline axis represents the release dates of models, while the vertical axis indicates the size of parameters.Blue data points signify LLMs that have received limited attention in the literature regarding privacy protection, while black data points indicate models studied alongside privacy safeguards.Currently, scholarly focus on data privacy in LLMs primarily revolves around well-known models of relatively smaller scale, like pre-2020 versions of the GPT-2 [14] and BERT [15] series.In contrast, recent releases of LLMs with larger parameter sizes have not been adequately scrutinized due to some models not being publicly available, and privacy protection technology lagging behind the rapid development of LLMs.</p>
<p>In this paper, we extensively investigate data privacy concerns within Large LLMs, specifically examining potential privacy threats from two folds: privacy leakage and privacy attacks.Besides, we delve into the corresponding countermeasures by providing a comprehensive review from the three major stages of developing LLMs: pre-training, fine-tuning and inference.Our contributions are summarized as follows:</p>
<p>• We undertook a comprehensive investigation into the scholarly literature concerning privacy threats within LLMs, categorizing them into two distinct groups: privacy leakage and privacy attacks.• Our examination encompasses an analysis of privacy protection methodologies applied to LLMs, which we categorized based on developmental stages.We categorize privacy protections into three groups based on their location: pre-training, fine-tuning, and inferences.Within each category, we introduce techniques at a high level, explain their application in LLMs, and provide a detailed literature review.The goal of our survey is to provide guidance for LLM developers on implementing cuttingedge techniques to safeguard LLMs.</p>
<p>II. RELATED WORK</p>
<p>In this section, we first introduce existing surveys about the development and evaluation of LLMs.Then, we further elaborate the most related work addressing the privacy and security issues in LLMs, and finally summarize the research of our survey.</p>
<p>A. Surveys on LLM Evaluation</p>
<p>Currently, some works have surveyed the development and evaluation of LLMs.These studies typically cover architectural improvements of LLMs (such as the GPT series, BERT, Transformers [16]- [22]).For example, Li et al. [16] focused on integrating LLM with intelligent personal assistants (IPAs) to improve personal assistance capabilities.It delves into the architecture, capabilities, efficiency, and security aspects of these agents.Zhao et al. [18] focused on four key aspects of LLMs: pre-training, adaptation tuning, utilization, and capacity evaluation.It provides a thorough background on LLMs, including terminologies and techniques.Naveed et al. [21] provided an extensive analysis of LLMs, covering their architecture, training, applications, and challenges.It dives into detailed aspects of LLMs like pre-training, finetuning, and evaluation, while also discussing various LLM applications in different fields.Hadi et al. [22] introduced a thorough overview of LLMs, discussing their history, training, and applications in various fields like medicine, education, finance, and engineering.It examines the technical aspects, challenges, and future potential of LLMs, including ethical considerations and computational requirements.</p>
<p>To understand the capabilities and limitations of LLMs in various applications, some works have conducted comprehensive measurements on these LLMs [17], [23], [24].Chang et al. [17] offered a comprehensive analysis of the methods and criteria for evaluating LLMs.It discusses various aspects including tasks to evaluate, datasets, benchmarks, and evaluation techniques.Guo et al. [23] emphasized the need for a comprehensive evaluation of LLMs in various dimensions, such as knowledge and capability evaluation, alignment evaluation, security considerations, and applications in the specialized domain.In [24], Liu et al. examined the alignment of LLMs with human values and social norms.It proposes a detailed taxonomy to evaluate LLM trustworthiness on various dimensions such as reliability, safety, fairness, resistance to misuse, explainability, adherence to social norms, and robustness.</p>
<p>B. Surveys on LLM Security and Privacy</p>
<p>Since the training of LLMs relies on a substantial amount of data, which usually includes sensitive information.Therefore, LLMs face challenges in handling privacy and security issues [8], [25]- [32].Yao et al. [8] comprehensively investigated the security and privacy of LLMs, and conducted an extensive review of the literature on LLMs from three aspects: beneficial security applications (such as vulnerability detection, secure code generation), adverse effects (e.g., phishing attacks, social engineering) and vulnerabilities (e.g., jailbreak attacks, prompt attacks), as well as corresponding defense measures.Li et al. [25] delved into privacy concerns in LLMs, categorizing privacy attacks and detailing defense strategies.It also explores future research directions for enhancing privacy in LLMs.</p>
<p>Neel et al. [26] explored the privacy risks associated with LLMs, focusing on issues such as the memory of sensitive data and various privacy attacks.It reviews mitigation techniques and highlights the current state of privacy research in LLMs.However, they mainly focus on work that red-teams models to highlight privacy attacks.</p>
<p>Marshall et al. [27] and Al-Hawawreh et al. [28] explored the role of ChatGPT in the field of cybersecurity.Their discussions emphasized its real-world uses, such as enhancing code security and detecting malware.Qammar et al. [29] provided an extensive overview of the evolution of chatbots to ChatGPT and their role in cybersecurity, highlighting vulnerabilities and potential attacks.However, it may lack depth in specific cybersecurity solutions and preventive measures against identified vulnerabilities and attacks.Schwinn et al. [30] offered a comprehensive analysis of both old and new threats in LLMs, providing insight into evolving adversarial attacks and defenses.But The focus on a broad range of threats might overlook in-depth details on specific attack methodologies or defense mechanisms.Derner et al. [31] investigated specific security risks associated with ChatGPT, contributing to a better understanding of its vulnerabilities.However, it may not provide a comprehensive comparison with other models or systems, limiting its scope to ChatGPT only.Shayegani et al. [32] thoroughly examined the vulnerabilities in LLMs exposed by adversarial attacks, offering valuable insights for future model improvements.Nonetheless, the focus on adversarial attacks might lead to less emphasis on other types of vulnerabilities or broader security issues.</p>
<p>In contrast to existing surveys, our research concentrates on addressing data privacy issues within LLMs, providing a comprehensive literature reviwe of privacy threats and privacy protection techniques.We thoroughly examine the countermeasures employed to mitigate privacy threats at different stages, and engage in an in-depth discussion on the current challenges and future research directions in LLM data privacy, aiming to offer guidance and reference for this field.</p>
<p>III. BACKGROUND ON LARGE LANGUAGE MODELS (LLMS)</p>
<p>LLMs are super-large deep learning models pre-trained on vast amounts of data, containing tens of billions to trillions of parameters.They construct extensive unsupervised training based on these parameters, enabling them to more accurately learn patterns and structures of natural language, thereby understanding and generating natural language texts.Compared to traditional NLP models, LLMs demonstrate better proficiency in understanding and generating natural texts, and also exhibit certain logical thinking and reasoning abilities, which is widely in programming [33], vulnerability detection [34], and medical text analysis [35].In 2017, Vaswani et al. [36] introduced the Transformer architecture, which uses parallel processing and attention mechanisms to provide an effective method for processing sequential data (especially text).This significantly enhances the efficiency of dealing with sequential data and supports more efficient training on large datasets, fostering the rapid development of LLMs such as the GPT series, BERT, and Transformer models.The training of LLMs primarily includes two key stages: pre-training and fine-tuning.</p>
<p>• Pre-training: At this stage, the model is typically trained on a very large and diverse dataset.These datasets may include texts from a variety of sources such as the Internet, books and news, or large text datasets published by many organizations and research institutions for academic research.E.g. general text corpora, social media data, user-generated content, and dialogue data).For example, GPT-3, developed by OpenAI, was pre-trained using CommonCrawl, constituting 45TB of compressed plaintext before filtering [37].Regarding multimodal LLMs, CLIP's training dataset encompasses 400 million pairs of images and text, while Stable Diffusion was trained on a dataset consisting of two billion examples sourced from LAION-2B [38].The purpose of pre-training is to enable the model to learn a wide range of language patterns, structures, and knowledge.Through this process, the model acquires a broad ability to understand language, including understanding vocabulary, grammar, and even some common sense.This stage does not focus on any specific task but rather provides a general foundation for language understanding.• Fine-tuning: The fine-tuning stage is carried out on the basis of a pre-trained model, with the goal of better adapting the model to specific tasks or domains.During this phase, the model is trained on a smaller, more specific dataset that is closely related to the target task or domain.</p>
<p>The datasets are usually sourced from websites and forums of specific professional fields, such as the medical, legal, technological, and other professional communities, and mainly consist of labeled demonstration data such as labeled datasets, human-labeled datasets, and LLMgenerated datasets.The datasets available for fine-tuning may be relatively small, typically ranging from a few hundred to a few thousand text samples.Through finetuning, the model learns the characteristics and details specific to the task.</p>
<p>The advantage of this two-stage training method is that it combines the breadth of general language understanding (through pre-training) with the depth of adaptability to specific tasks (through fine-tuning).This enables the model to exhibit higher accuracy and efficiency when dealing with a variety of complex, domain-specific tasks.After the model has been trained and fine-tuned, the inference stage can be performed.</p>
<p>• Inference: In this phase, the trained model is used to make prediction or decision.This includes processing input data (such as users' prompts), using the model to compute outputs, and possibly post-processing to fit specific application needs.The primary purpose of inference is to leverage the knowledge learned by the model to solve real-world problems, such as automated responses, image recognition, or other forms of data analysis.IV.SCOPE, METHODOLOGY, AND OVERVIEW A. Scope Our paper is dedicated to conducting a comprehensive literature review in the field of data privacy for LLMs, organizing and reviewing existing research.We conduct a comprehensive and in-depth privacy analysis, including privacy leakage and privacy attacks in LLMs, as well as privacy protection methods at different stages of privacy inference within LLMs.Our focus is not only on the implementation details of these technologies but also on a deep exploration of their effectiveness in protecting privacy, as well as their potential limitations.</p>
<p>B. Methodology</p>
<p>Data Collection: To comprehensively understand the landscape of data privacy concerns in LLMs, we executed a structured literature search on Google Scholar.The results are summarized in Fig. 3, wherein we categorized the retrieved literature into distinct themes.From the 91 collected papers, we identified 33 that specifically highlight the privacy threats confronting LLMs.Within this subset, a division reveals that 5 papers focus on privacy leakage, while the remaining 28 delve into various privacy attacks.Additionally, we found 58 papers dedicated to exploring privacy protection strategies for LLMs.We classified them according to different phases: 11 during pre-training, 23 during fine tuning, and 24 in inference phase.An analysis of publication trends shows that the majority of these papers, representing 58.57%, were published in 2023, with only 30 released in between 2021 and 2022, indicating a significant recent interest in the topic.Notably, there are also 5 cutting-edge studies from 2024, which underscores the ongoing and dynamic nature in this crucial area of research.</p>
<p>Structuring and Analysis: Fig. 4 presents the organizational structure of this study, which outlines the current privacy threats faced by LLMs and its corresponding protections as well as the relevance between privacy threats and defense technologies.In the section on privacy threats, this paper reviews existing research from two dimensions: privacy attacks and privacy leakage, detailing common attack methods and instances of privacy leakage in LLMs.Regarding privacy protection approaches, we systematically summarize them according to the three stages of LLMs: pre-training, finetuning, and inference.And we summarize the key privacy protection technologies, including data sanitization, federated learning, differential privacy, homomorphic encryption, and secure multi-party computation.Finally, we establish a connection between these key technologies and the privacy threats they may defend against, providing a framework for understanding the data privacy in LLMs.• Privacy Threats ( §V): We first conduct a literature review on privacy threats against LLMs.Based on whether the attackers are active or passive, we further categorize the threats into two groups: privacy leakage, where the attackers passively collect sensitive information due to vulnerabilities, and privacy attacks, where the attackers actively break LLMs to access sensitive information.• Privacy Protections ( §VI &amp; §VII): Based on where privacy protection is located, we can group them into three categories: privacy protection in Pre-Training ( §VI-A), privacy protection in fine-tuning ( §VI-B), and privacy inferences ( §VII).Among them, privacy protection in inferences can be further grouped based on the methods adopted (e.g., whether it is a cryptography-based approach).In each of these protections, we first introduce the techniques at a high level; then, we explain how they can be used in LLMs (see those Tech Tips), and finally, we provide a detailed literature review.</p>
<p>C. Overview</p>
<p>Privacy Protection</p>
<p>V. PRIVACY LEAKAGE AND PRIVACY ATTACKS IN LLMS</p>
<p>We undertake a literature review focusing on privacy threats against LLMs.We categorize these threats into two groups based on the attackers' activity: privacy leakage, wherein attackers passively collect sensitive information due to vulnerabilities, and privacy attacks, wherein attackers actively breach LLMs to access sensitive information A. Privacy Leakage (Passive) 1) Sensitive Query: Users may input queries containing sensitive or personally identifiable information (PII) into LLMs.For example, asking questions about medical conditions, financial situations, or personal relationships could reveal private details about the user's life.If users input sensitive information as prompts, there arise concerns regarding data privacy [39], [40].For example, Samsung Electronics staff provided sensitive corporate data when interacting with ChatGPT.Besides, various LLM plugins also raise privacy concerns of user's sensitive data.Iqbal et al. [41] proposed a systematic framework to evaluate the security, privacy, and safety of third-party plugins integrated into LLM platforms, focusing on OpenAI's ChatGPT ecosystem.Some plugins were found to collect excessive user data, including personal and sensitive information.Some plugins did not provide clear details on how they use user data, potentially violating privacy policies.</p>
<p>2) Contextual Leakage: Even seemingly innocuous queries could indirectly reveal sensitive information about the user when combined with other contextual factors.For instance, asking about nearby landmarks or local events could inadvertently disclose the user's location or activities.Over time, repeated interactions with the model could lead to the accumulation of enough information to uniquely identify the user, posing a risk to privacy.The study [10] focuses on the capabilities of LLMs to infer personal attributes from text, particularly in the context of privacy concerns and the threat of privacy-invasive chatbots.They evaluated LLMs' ability to infer personal attributes (like location, occupation, age, gender, etc.) from text on the PersonalReddit dataset, containing 520 profiles with 5814 comments.They evaluated 9 state-of-the-art LLMs on the PR dataset, with GPT-4 achieving top-1 accuracy of 84.6% and top-3 accuracy of 95.1%.</p>
<p>3) Personal Preferences Leakage: LLMs may infer personal preferences, interests, or characteristics of users based on their queries and interactions.This could result in targeted advertisements, personalized recommendations, or other tailored content that may reveal private aspects of the user's life.For example, LLMs represent a significant asset to recommender systems, offering advantages in delivering personalized recommendations [42].Besides, these models have the potential to refine or establish new methodologies for sequential recommendation [43], which could inadvertently reveal users' personal preferences, thereby raising privacy concerns.</p>
<p>During the utilization of LLMs, individuals may unintentionally disclose their privacy, whether through direct or indirect means.Beyond the direct provision of sensitive information, providers of services can extrapolate intricate user attributes and preferences, thereby gaining access to sensitive data via data analysis methods.</p>
<p>B. Privacy Attacks (Active)</p>
<p>1) Backdoor Attacks (Data Poisoning Attacks) on Pre-Training: During the pre-training phase, the adversary manipulates the training data, introducing poison into the dataset.Subsequently, this tainted training data is disseminated on the internet, where unwitting developers procure and employ it for training their models.Consequently, the models become infused with covert backdoors, thereby compromising their integrity and security.Adversaries can exploit backdoors to exfiltrate sensitive or private information processed by the LLMs [44].This could include personal data, confidential documents, or proprietary information, leading to privacy breaches and potential violations of data protection regulations.Backdoors allow adversaries to manipulate the output of LLMs, potentially leading to the generation of misleading or harmful content.This can have detrimental effects on users' privacy, particularly if the manipulated content contains false information or malicious intent.Yang et al [45] shed light on a critical security vulnerability in NLP models, introducing a data-free backdoor attack that could subvert the integrity of word embeddings by altering a single embedding vector.POISONPROMPT [46] emerges as a novel backdoor attack strategy, demonstrating to its capability to compromise both hard and soft prompt-based LLMs.Furthermore, Huang et al [47] introduced a stealthy Composite Backdoor Attack (CBA) that scatters multiple trigger keys across different prompt components.CBA ensures activation only when all triggers are present, demonstrating high effectiveness in NLP and multimodal tasks while maintaining model accuracy.</p>
<p>2) Backdoor Attacks (Data Poisoning Attacks) on Fine Tuning: Adversaries may inject poisoned or adversarial examples into the fine-tuning dataset to manipulate the behavior of LLMs.These poisoned examples could introduce biases or vulnerabilities into the model, leading to compromised performance or biased outputs that violate privacy and fairness principles.Research by Wan et al [48] has revealed that instruction-tuned LMs, such as ChatGPT, are vulnerable to backdoor attacks where adversaries can manipulate model behavior by tainting training datasets with malicious examples.These poisoned models then exhibit erroneous behavior when exposed to specific trigger phrases, leading them to produce a predetermined target label in classification tasks.In a similar vein, Xu et al [49] demonstrated that attackers can subvert model behavior by interspersing legitimate data with malicious instructions, achieving high success rates of exploitation across various NLP datasets.Furthermore, these attacks can be engineered to elicit targeted or even harmful responses on specific topics.For example, Yan et al [50] have shown that it is possible for adversaries to implant Virtual Prompt Injection (VPI) backdoors into models through tainted instruction tuning data, granting them the capability to finely control the model's outputs in response to carefully chosen triggers.</p>
<p>3) Membership Inference Attacks on Pre-Training: In a membership inference attack [51], [52], an adversary attempts to determine whether a specific individual's data was included in the training dataset used to train an LLM.By analyzing the model's outputs or responses to queries, the attacker can infer whether certain data samples were part of the training data.This can lead to privacy breaches if sensitive information about individuals is inferred from the model's behavior.A study by Mireshghallah et al [53] has highlighted the high susceptibility of Masked Language Models (MLMs) to privacy attacks, demonstrating this through likelihood ratio membership inference attacks that utilize an additional reference MLM.However, considering the unrealistic assumption of reference-based models, Mattern et al [54] proposed an alternative method known as neighbourhood attacks, which compare scores with synthetic texts.In another development, Shi et al [55] introduced WIKIMIA benchmark and MIN-K PROB method, which they claimed improved detection by 7.4% over previous methods.Despite these advancements, Duan et al [56] evaluated membership inference attacks on the pre-training data of LLMs trained on the Pile and found that the success rates of the aforementioned attack methods were limited due to the combination of large datasets and few training iterations, as well as a fuzzy boundary between members and non-members.</p>
<p>4) Membership Inference Attacks on Fine Tuning: Membership Inference Attacks aim to reveal whether specific data samples have been incorporated into the training set of the model.In the context of LLM fine-tuning, adversaries may discern patterns that suggest whether those inputs were part of the training data by meticulously analyzing the model's responses to certain inputs.Accurate execution of such an inference by attackers could lead to the compromise of the model's training data confidentiality.Mireshghallah et al. [57] conducted an empirical investigation that examined significant variation in vulnerability to membership inference of different fine-tuning methods for LLMs.Their findings indicated finetuning model heads proves most susceptible, while using smaller adapters shows reduced attack susceptibility.Moreover, Jagannatha et al. [58] focused on fine-tuned clinical language models (CLMs) and their exposure to MIAs.They demonstrated that the scale of the model plays a crucial role in its privacy risks, with smaller models generally exhibiting lower vulnerability compared to larger architectures.Building on these insights, Fu et al. [59] introduced a novel approach to MIA in fine-tuned LLMs.Their proposed method, Self-calibrated Probabilistic Variation (SPV-MIA), leverages memorization rather than overfitting as a reliable indicator of membership.Additionally, they presented a self-prompt strategy for constructing a comparable dataset for the reference model, aiming to enhance the practicality and effectiveness of MIAs against fine-tuned LLMs.</p>
<p>5) Model Inversion (Data Reconstruction) Attacks: In a model inversion attack, an adversary attempts to reconstruct or reverse-engineer the training data used to train an LLM based on its outputs or internal representations.By analyzing the model's parameters, gradients, or generated text, the attacker aims to recover sensitive information contained in the training data, such as personal communications, financial records, or proprietary documents.Song et al [60]  6) Attribute Inference Attacks: Attribute inference attacks involve inferring sensitive attributes or characteristics of individuals from fine-tuned LLMs.For example, an attacker may attempt to infer demographic information, such as age, gender, or ethnicity, based on the language patterns or topics discussed in the model's generated text [64].This can lead to privacy violations and discrimination against individuals based on inferred attributes.In a comprehensive study, Pan et al. [65] systematically examined the privacy risks associated with 8 state-of-the-art language models.Their examination is anchored on 4 diverse case studies that focus on the threat of attribute inference attacks.The findings are compelling: these state-of-the-art models are indeed susceptible to revealing sensitive details, which include personal identifiers such as identity, genetic information, health data, and geographical locations.This vulnerability stems from the potential for adversaries to reverse-engineer the embeddings within these models.Building on this concern, Staab et al. [10] employed Reddit profiles to showcase that LLMs can accurately infer a variety of personal attributes.Remarkably, these models surpass human performance in terms of both efficiency and speed, underscoring the urgent need for effective privacy safeguards in model development.</p>
<p>7) Model Stealing Attacks: Adversaries may attempt to steal or replicate fine-tuned models trained on proprietary or sensitive datasets.By querying the model and observing its responses, adversaries can extract information about the model's parameters or internal representations, enabling them to reconstruct or replicate the model without access to the original training data.Krishna et al. [66] demonstrated the feasibility of model stealing attacks in NLP, showing that adversaries can reconstruct victim models using only random word sequences and task-specific heuristics, without requiring real training data.This exploit is enabled by the widespread use of transfer learning methods in NLP.And then, Truong et al. [67] advanced the field with their proposal of datafree model stealing techniques.These methods overcome the need for surrogate datasets, enabling accurate replication of valuable models with limited queries.Besides, Sha et al. [68] introduced a novel prompt stealing attack against LLMs, leveraging generated answers to reconstruct well-designed prompts.It involves a two-pronged approach: a parameter extractor dissects prompt types and characteristics, while a prompt reconstructor generates reverse-engineered prompts with notable efficacy.</p>
<p>VI. PRIVACY PROTECTION IN PRE-TRAINING AND FINE-TUNING</p>
<p>Privacy protection in pre-training and fine-tuning of LLMs is paramount in safeguarding sensitive data while ensuring model effectiveness.Incorporating techniques such as differential privacy, data cleaning, and federated learning can mitigate privacy risks.</p>
<p>A. Privacy Protection in Pre-Training 1) Data Cleaning: Data cleaning enhances data quality by rectifying errors and inconsistencies, serving as a foundational step that also plays a critical role in privacy protection by implementing anonymization, data minimization, and secure practices to safeguard sensitive information.To be more specific, we can remove or generalize personally identifiable information (PII) such as names, addresses, social security numbers, etc., to make it harder to identify individuals in the dataset (e.g., we can also mask sensitive information by replacing it with non-sensitive placeholders or pseudonyms while still preserving the structure and relationships within the dataset); we can aggregate data at a higher level to reduce the risk of re-identification.For example, instead of storing individual inference query details, and aggregate queries by day or week.</p>
<p>Tech Tips: When utilizing data cleaning techniques for privacy protection in LLMs, it's essential to prioritize thorough data sanitization before fine-tuning the model for specific tasks.Anonymizing or pseudonymizing sensitive information, and aggregating data to reduce granularity are key strategies to safeguard individual privacy.</p>
<p>OpenAI [37] underscores the thorough measures implemented to elevate the quality and security of their training data.They utilize filtering and fuzzy deduplication techniques to remove personally identifiable information from the corpora utilized for model training.This methodology not only purifies the data but also secures a heightened level of privacy protection.These measures are also employed in [69].Anthropic [70] adopts a strategic approach in their training methodologies, focusing on the exclusive use of beneficial human feedback data to develop AI assistants.This selective data utilization guarantees the creation of assistants that are intrinsically helpful and non-harmful, founded on a foundation of entirely positive interactions.Additionally, their commitment to fostering AI behavior that aligns with constitutional and ethical standards is further highlighted in [71].Kandpal et al. [72] demonstrated that removing duplicated sequences from training data significantly reduces the vulnerability of language models to privacy attacks, such as those allowing adversaries to recover memorized information [61].Through empirical analysis, the authors show that duplication in training data is a key factor contributing to these privacy risks.By deduplicating the training sets, the models become less likely to regenerate sensitive or specific information, hence improving their security against such attacks without compromising the model's performance.</p>
<p>2) Federated Learning: Federated learning revolutionizes machine learning by decentralizing the training process, enabling model training across multiple edge devices or servers while preserving data privacy.Initially, a global model is distributed to participating devices, which independently train the model using their local data.Instead of sending raw data to a central server, only model updates are transmitted, ensuring user privacy as data remains localized.These updates are aggregated at the central server to refine the global model iteratively, leading to continuous improvement without compromising privacy.Federated learning thus offers a paradigm shift, promoting collaborative machine learning in privacy-sensitive environments by leveraging distributed data processing and maintaining data locality.</p>
<p>Tech Tips: In the pre-training of LLMs, federated learning offers a privacy-centric approach by eliminating the need for centralized data storage.Training occurs on local devices, with only model parameters or updates sent to a central server for aggregation.This method keeps personal data on its original device, drastically reducing data breach risks and addressing privacy and security concerns associated with centralized storage.</p>
<p>Chen et al. [73] introduced a federated learning framework for LLMs that focuses on privacy without sacrificing performance, incorporating federated pre-training to securely utilize decentralized data for improved privacy, security, and model generalization.Yu et al. [74] developed Federated Foundation Models to enhance privacy in collaborative learning, focusing on the entire lifecycle of foundation models with federated learning.They tackle privacy, performance, and scalability, paving the way for future research on privacy-preserving, personalized models.</p>
<p>Finding: Federated learning is not enough Federated learning protects data privacy across various participants by decentralizing the training process, where data remains on users' devices and only model updates are shared.However, it's not entirely secure against privacy breaches; malicious servers could potentially extract private user data from shared gradients.To bolster security, federated learning often integrates additional privacy-preserving techniques such as differential privacy, secure multi-party computation, homomorphic encryption, and adversarial training.These methods collectively enhance the robustness of privacy protection in federated learning frameworks.</p>
<p>3) Differential Privacy: Differential privacy is a technique for protecting data privacy, particularly in the fields of statistical release and data analysis.Its purpose is to allow researchers to extract useful statistical information from an entire dataset without revealing any individual data.Differential privacy achieves this by adding a certain amount of random noise to the data, ensuring that even if attackers have complete background knowledge except for the target dataset, they cannot determine whether the dataset contains information about a specific individual.We can define differential privacy as follows: Definition 6.1: Given two datasets D 1 and D 2 , that differ by only one element (i.e., they are "adjacent datasets"), a randomized algorithm A satisfies ϵ-differential privacy if and only if for all output sets S from the algorithms on D 1 and D 2 , the following holds:
Pr [A (D 1 ) ∈ S] Pr [A (D 2 ) ∈ S] ≤ e ϵ(1)
where Pr [A (D 1 ) ∈ S] represents the probability that the result of running algorithm A on dataset D 1 falls within the set S. ϵ is a non-negative parameter known as the privacy budget.</p>
<p>The smaller the ϵ, the higher the level of privacy protection, but this may reduce the utility of the data.e is the base of the natural logarithm, approximately equal to 2.71828.Since the algorithm A is random, differential privacy can ensure that for adjacent datasets (i.e., datasets that differ by only one element), the output of an algorithm is "almost identical."This means that it is nearly impossible to infer any specific information about an individual from the output.By adjusting the value of ϵ, a trade off can be realized between data privacy protection and data utility.Hoory et al. [75] examined the application of differential privacy to pre-trained language models.It focuses on evaluating and enhancing the performance of these models under privacy constraints.Du et al. [76] focused on providing differential privacy in forward propagation for large-scale models.It addresses the challenge of protecting data privacy while performing forward propagation in large models.Li et al. [77] argued that LLMs can be effective learners under differential privacy constraints.It explores techniques to optimize model performance while adhering to privacy standards.</p>
<p>B. Privacy Protection in Fine Tuning</p>
<p>1) Federated Learning: Federated learning transcends its initial application in pre-training, proving equally effective in the fine-tuning phase.This expanded application not only extends its utility but also underscores its versatility in bolstering privacy protection.By operating across data, models, and commands, federated learning presents a holistic solution, showcasing its comprehensive applicability and potential for addressing privacy concerns in diverse contexts.</p>
<p>Tech Tips: Similarly, in the fine-tuning phase, federated learning is employed by initially distributing the pre-trained global model to edge devices or local servers where finetuning tasks are performed.On each device or server, the global model is fine-tuned using locally held data pertinent to the specific task.</p>
<p>Xu et al. [78] and Zhang et al. [79] integrated federated learning into the fine-tuning of LLMs to significantly enhance privacy protection.Their approaches focus on keeping sensitive data on the user's device, eliminating the need for direct data transmission and sharing.By employing advanced privacy-preserving techniques such as differential privacy, secure aggregation, and homomorphic encryption, they ensure that user privacy is safeguarded during the fine-tuning process.Sun et al. [80] introduced FedBPT, a federated learning framework for privacy-preserving prompt tuning in language models, optimizing prompts locally and sharing only updates to minimize communication overhead and ensure data privacy.This method facilitates secure, collaborative model enhancement without exposing sensitive data.Zhao et al. [11] enhanced privacy in model fine tuning across decentralized nodes by aggregating local updates into a central model without centralizing data, effectively keeping sensitive information local and mitigating data breach risks while leveraging collaborative learning benefits.Fan et al. [81] presented an approach that combines federated learning with knowledge distillation and parameter-efficient fine-tuning in LLMs to ensure privacy.They also introduce secure aggregation for safely merging model updates, enabling collaborative, privacypreserving learning across different organizations.</p>
<p>Finding: Federated Learning in Pre-Training V.S. in Fine-Tuning In federated learning, pre-training employs extensive, general datasets for foundational language comprehension through distributed learning, emphasizing data privacy.Fine-tuning, however, focuses on specialized tasks using targeted datasets, prioritizing personalized optimization and stricter privacy on local devices.The technical needs for privacy protection distinctly vary between these stages.However, most research on addressing privacy issues in LLMs through federated learning focuses on optimizing the computational and communication overhead.These studies either claim applicability to both pre-training and fine-tuning phases or claim relevance to a specific phase without making targeted adjustments or designs for that stage.This highlights a gap: the need for precise, stagespecific optimization and design in federated learning for LLMs, essential for improving privacy protection's effectiveness and efficiency at different stages.</p>
<p>2) Differential Privacy: The approaches primarily employ differential privacy techniques to handle privacy-sensitive tuning data, thereby enabling secure and private inference.These approaches focus on balancing the data utility in model tuning with the data privacy [64], [75]- [77], [82]- [87].Behnia et al. [82] introduced EW-Tune, a framework for fine-tuning LLMs with differential privacy guarantees.EW-Tune employed the Edgeworth accountant method, offering finite-sample privacy guarantees suitable for the fine-tuning context.It solves the problem of how to fine-tune LLMs on private data without compromising privacy.Shi et al. [83] presented a framework for enhancing the privacy of LLMs without significantly compromising their utility.The proposed approach, Just Fine-tune Twice (JFT), focuses on selectively applying differential privacy (SDP) to only the sensitive parts of data, based on a policy function.This is achieved through a two-phase fine-tuning process: first with redacted data and then with original data using a privacy-preserving mechanism.This method is shown to be effective for transformer-based models and addresses limitations of prior SDP applications.Wu et al. [84] designed an Adaptive Differential Privacy (ADP) framework for language model training.It estimates the privacy probability of linguistic items without resorting to the prior privacy information and designs a novel Adam algorithm to adaptively adjust the degree of differential privacy noise, potentially improving model utility while maintaining privacy.Li et al. [64] explored a method for prompt tuning LLMs in a privacy-preserving manner.This approach seeks to leverage the power of large models while safeguarding user privacy.</p>
<p>3) Knowledge Unlearning: Knowledge unlearning, also known as machine unlearning, is a strategy aimed at bolstering privacy within machine learning models, especially LLMs [88].When a machine learning model is trained on data, it learns patterns and correlations present in that data.However, sometimes these patterns may inadvertently encode sensitive information about individuals.If the model retains this information, it can pose privacy risks when the model is deployed in real-world applications, especially in scenarios where the model may be exposed to sensitive data.Knowledge unlearning techniques aim to mitigate these risks by selectively forgetting or removing sensitive information from the model.</p>
<p>Tech Tips: In the fine-tuning stage, it functions by ensuring that the model does not hold onto or disclose sensitive details learned during its initial training phases.This process involves retraining the model to eliminate its memory of certain information, effectively reducing the risk of privacy breaches while maintaining or enhancing the model's performance.</p>
<p>Zhang et al. [89] analyzed the Right to be Forgotten in LLMs, identifying the unique legal and technological hurdles and proposing solutions like differential privacy and machine unlearning to balance privacy with technological progress.Chen et al. [90] introduced an efficient unlearning technique for LLMs using unlearning layers within transformers, enabling precise data removal without retraining and effectively managing sequential deletion requests with minimal performance loss.Jang et al. [91] proposed a targeted unlearning method for LMs through gradient ascent on specific sequences, offering an efficient way to erase sensitive information while preserving overall model performance.Eldan et al. [92] detailed a novel unlearning approach for LLMs by fine-tuning on datasets modified to omit targeted knowledge, employing reinforcement bootstrapping to forget information without compromising model integrity.</p>
<p>4) Offsite Tuning: Offsite tuning, detailed by Xiao et al. [93], refines the adaptability of models to specific tasks, prioritizing data privacy through the deployment of lightweight adapters and compressed emulators for localized adjustments.</p>
<p>Tech Tips: This innovative method transmits only essential components to the data owner for offsite tuning, thereby avoiding the exposure of the entire model and ensuring that sensitive data remains under the data owner's control.This significantly lowers the risk of privacy breaches.The adapter, fine-tuned with local data, is updated without direct data exposure and seamlessly reintegrated into the foundation model, effectively safeguarding data privacy throughout the adaptation process.</p>
<p>VII. PRIVACY PROTECTION IN INFERENCE</p>
<p>During the inference process of LLMs, the issue of privacy leakage has garnered widespread attention.To address this issue, researchers have developed numerous strategies to ensure privacy security during the inference phase.In this section, we summarize the privacy protection approaches for the inference stage of LLMs, focusing on various approaches including encryption-based privacy protection approaches, privacy protection approaches through detection, and hardwarebased approaches.</p>
<p>A. Cryptography-based Approaches 1) Homomorphic Encryption: Homomorphic encryption [108] is a cryptographic technique that allows for computations to be performed on ciphertexts, ensuring that the result, when decrypted, is identical to the result of the same operations performed on the plaintext.This encryption method is key in enabling data to be processed while maintaining its encrypted state, adding a new dimension to data privacy and security.Homomorphic encryption is primarily categorized into three types:</p>
<p>• Partial Homomorphic Encryption (PHE): Supports one type of operation (usually addition or multiplication) on ciphertexts.</p>
<p>• Somewhat Homomorphic Encryption (SWHE): Allows a limited number of operations on ciphertexts.• Fully Homomorphic Encryption (FHE): The most powerful, supporting an unlimited number of both addition and multiplication operations on ciphertexts.To better understand homomorphic encryption algorithms, we provide the following definition.</p>
<p>Definition 7.1: An encryption scheme is considered homomorphic over an operation • if it satisfies a specific mathematical property.Specifically, it supports the following equation:
E(m 1 ) • E(m 2 ) = E(m 1 • m 2 ), ∀m 1 , m 2 ∈ M (2)
Here, E represents the encryption algorithm, M denotes the set of all possible messages that can be encrypted, and m 1 and m 2 are any two messages in the scheme.The operation ⋆ can be any binary operation (e.g.addition or multiplication).</p>
<p>Tech Tips: Homomorphic encryption safeguards privacy during the inference stage by encrypting both the model parameters and input data.With HE, computations can be performed directly on encrypted data, allowing the model to make predictions without decrypting sensitive information.This process ensures that neither the raw data nor the model architecture is exposed in their unencrypted form, preserving privacy throughout the inference process.Decryption of the results is only done by trusted parties possessing the decryption key, maintaining the confidentiality of the information.Additionally, HE facilitates secure outsourcing of computations to untrusted servers, enabling organizations to utilize external resources without compromising data privacy.</p>
<p>We now introduce privacy inference approaches based on HE [94]- [98].The THE-X [94] presented a novel approach for enabling privacy-preserving inference on pre-trained transformer models using homomorphic encryption, in which they utilized ReLU to replace GELU and used approximation methods for SoftMax and LayerNorm to support the fully HE operations.However, THE-X may lead to privacy leakages as it poses intermediate results to the client during the computing of ReLU.Iron [95] focused on enhancing privacy in clientserver settings, where clients have private inputs and servers hold proprietary models.It introduces several new homomorphic encryption-based protocols for matrix multiplication and complex non-linear functions (like Softmax, GELU activations, and LayerNorm) which are crucial in Transformer-based models.Bumblebee [96] optimized homomorphic encryptionbased protocols for large matrix multiplication and efficient, accurate protocols for non-linear activation functions in transformers, enhancing data privacy during inference.Zimerman et al. [97] explored secure transformer models tailored for HE, which converts the operators to their polynomial equivalent.Liu et al. [98] proposed a framework to enhance the efficiency of private inference on transformer-based models.It focuses on replacing computation-intensive operators (e.g., ReLU, GELU) in transformers with privacy-computing-friendly alternatives.The framework achieves significant reductions in private inference time and communication overhead while maintaining near-identical model accuracy.</p>
<p>2) Multi-Party Computation: Multi-Party Computation [109], [110] is a cryptographic protocol that enables allows multiple parties (often mutually distrusting) to collaboratively perform a computation task while keeping their individual data private.This means that even though the parties are working together to compute a result, none of them can see the other parties' private data.The objective of secure multiparty Computation is to construct a secure protocol that allows multiple mistrustful participants to jointly compute a target function on their private inputs, while ensuring the accuracy of the output, and protecting and controlling their private inputs even in the presence of dishonest behavior.SMPC can be † Note that the CKKS homomorphic encryption scheme might be vulnerable to passive attacks.[107] formally described as follows: Consider n parties, denoted as P 1 , P 2 , ..., P n .Each party P i holds a private input X i .</p>
<p>There is a predefined function f that takes n inputs.This function is of the form f : (X 1 , X 2 , ..., X n ) → Y , where X i represents the input for party P i and Y is the output using the secret data of all parties.Then, the parties compute the result
Y = (Y 1 , Y 2 , .., Y n ) based on the function f (X 1 , X 2 , ..., X n )
such that each party learns Y (or a portion of Y relevant to them) but learns nothing about the inputs X i of the other parties, for all j ̸ = i.</p>
<p>Tech Tips: MPC enables secure aggregation of model updates in federated learning setups, allowing parties to collaboratively train a shared model.MPC ensures privacy during model inference by performing computations on encrypted data, shielding sensitive information from central servers.MPC facilitates secure data labeling by allowing multiple parties to label data collaboratively without exposing raw labels, thus maintaining the confidentiality of sensitive information throughout the process.</p>
<p>Similar to HE, MPC is another crucial method that can be used to protect model privacy [99]- [103].Wang et al. [99] focused on the challenges and solutions for private inference in transformer models using MPC.While it advances the field of privacy-preserving inference, the complexity of MPC might affect practicality and efficiency.Hou et al. [100] presented a framework CipherGPT for secure GPT model inference in a two-party setting.It introduces optimized cryptographic protocols for operations like matrix multiplication and GELU activation, essential for GPT models.The framework focuses on preserving privacy while ensuring the efficiency of the inference process.However, the specific focus on two-party settings may limit the framework's applicability in more diverse operational environments.Ding et al. [101] proposed a communication-efficient protocol called East for activation functions like GELU and tanh, as well as optimized protocols for softmax and Layer Normalization (LN).These protocols are designed to enhance performance by reducing runtime and communication overhead, ensuring the security of the scheme.Akimoto et al. [102] presented a MPC-based approach to secure inference of Transformer models in natural language processing using ReLU functions.This method addresses the challenge of computing the Transformer's attention mechanism efficiently and securely in an MPC setting.Dong et al. [103] introduced PUMA, a framework for efficient and secure inference on Transformer models using replicated secret sharing.PUMA offers approximations for expensive nonlinear functions (e.g., GeLU and softmax), which can also evaluate the large models like LLaMA-7B efficiently under MPC.</p>
<p>3) Functional Secret Sharing: Function Secret Sharing (FSS) [111] involves dividing an original secret into multiple shares using a mathematical function (such as a polynomial), encoding the secret into each share in such a way that each is independent and insufficient to reveal the entire secret.These parts are then distributed to different participants, who can independently execute predetermined functions, such as arithmetic or logical operations, on their portion of the secret.These computations are carried out on secret shares that are in an encrypted or hidden state, preventing participants from obtaining any information about the original secret from their share alone.The results obtained by each participant are then aggregated, and when a sufficient number of shares are combined and computed, the outcome of executing the function on the entire secret is recovered.The security of this process lies in the fact that each share does not contain enough information to reveal the secret by itself; hence, even if some shares are compromised or participants are dishonest, the secret remains secure.The original secret's information is only revealed when the predetermined threshold is reached, that is, when a certain number of shares are correctly combined.</p>
<p>Tech Tips: In FSS, the LLM or function is partitioned into shares using cryptographic methods, with each party holding a share.During computation, parties perform operations on their shares using their private data, ensuring that individual inputs remain undisclosed.After computation, the parties collaboratively combine their shares to reconstruct the result of the function, maintaining privacy while revealing the final output.</p>
<p>As far as we know, there has been only one secure privacy inference approach based on Function Secret Sharing (FSS), which was proposed by Gupta et al. [104].The approach discussed a system named SIGMA for secure inference of transformer-based models, specifically focusing on Generative Pre-trained Transformers.SIGMA is designed to be efficient in terms of latency and communication overhead while maintaining standard 2-party computation (2PC) security by leveraging FSS.It introduces new FSS-based protocols for complex machine learning functionalities like Softmax and GeLU and optimizes them for GPU acceleration.SIGMA claims significant improvements in latency over state-of-the-art systems and demonstrates scalability to large GPT models.However, the paper does not explicitly outline specific disadvantages, which typically in such systems could include complexity of implementation, computational resource requirements, or potential limitations in the types of models or data that can be securely processed.</p>
<p>4) Differential Privacy in Inference: Similarly, differential privacy can also be applied in the inference stage of LLM, providing a crucial layer of privacy protection during the generation of model predictions or outputs.</p>
<p>Tech Tips: In the inference stages of LLMs, DP can introduce noise to model outputs to safeguard individual data privacy while preserving prediction accuracy.Parameters are adjusted to manage the privacy budget effectively, with continuous monitoring ensuring a balance between privacy and utility over time.</p>
<p>Majmudar et al. [85] presented a method for ensuring differential privacy in the decoding process of LLMs.This approach aims to protect privacy during text generation.Du et al. [86] proposed a method for fine-tuning and inference in language models while maintaining differential privacy during the forward pass.It tackles the challenge of protecting privacy during both fine-tuning and inference phases.Mai et al. [87] introduced the Split-and-Denoise method, combining local differential privacy with a denoising technique to protect privacy in large language model inference.Zhou et al. [105] introduced a method for privacy-preserving inference in pretrained models using token fusion.The advantage is maintaining privacy during inference, but it could impact the inference accuracy or efficiency.Yuan et al. [106] detailed a three-party protocol for secure Transformer model inference, safeguarding both model parameters and user data.It applies permutation instead of complex encryption, offering strong security with practical feasibility for global matrix multiplication-based layers.</p>
<p>Finding: Cryptography-based Private Inference</p>
<p>Privacy protection techniques grounded in Homomorphic Encryption (HE), Multi-Party Computation (MPC), and Functional Secret Sharing (FSS) offer demonstrable security assurances within rigorously defined threat models, as indicated in Table I.Nevertheless, limitations in performance and efficiency present obstacles to their near-term adoption by prominent model service providers.Even though these techniques have enhanced the efficiency of critical components, their experimental results demonstrate that deploying HE, MPC, and FSS might lead to degraded performance.Alternative approaches often rely on principles of obfuscation, yet their levels of randomness and security are weaker than cryptography-based solutions, and they typically consider specific attacks.</p>
<p>B. Detection-based Approaches</p>
<p>In existing research on Language Models (LMs), some efforts focuses on detecting privacy leaks [112]- [115].These studies predominantly examine whether the content generated by LMs directly exposes data privacy or if such privacy can be inferred through contextual associations.This approach is equally applicable to LLMs, suggesting a viable pathway for assessing and mitigating privacy risks in more advanced linguistic computational models.</p>
<p>Tech Tips: Detection-based methods for protecting the privacy of LLM involve identifying and mitigating potential privacy risks in the text generated by these models which two main strategies: (i) Direct detection methods involve directly examining the text generated by LLMs to identify privacy leaks.(ii) Contextual inference detection methods focus on identifying privacy breaches that may not be explicitly evident in the generated text but can be deduced through contextual correlations.</p>
<p>1) Direct Detection: Kim et al. [116] developed a blackbox probing method to evaluate privacy risks in LLMs by using crafted prompts to elicit Personally Identifiable Information (PII) from model outputs.This approach assesses the likelihood of LLMs inadvertently revealing PII, offering a targeted strategy for understanding privacy vulnerabilities in generated text.Phute et al. [117] unveiled a zero-shot defense strategy for LLMs aimed at curbing harmful content generation.By deploying a harm classifier from the same LLM, this method significantly reduces the efficacy of adversarial attacks, eliminating the need for fine-tuning.Chen et al. [118] developed a moving target defense system for LLMs to counter adversarial attacks, using N-Gram models and naive Bayes classification for evaluating responses and BERT for assessing question-answer coherence, effectively distinguishing between beneficial and malicious content.</p>
<p>2) Contextual Inference Detection: Mireshghallah et al. [119] introduced CONFAIDE, a benchmark that evaluates LLMs' privacy reasoning across four complexity levels, revealing notable deficiencies in models like GPT-4 and ChatGPT in terms of privacy preservation and social reasoning.Huang et al. [120] proposed a framework to assess PLMs' risk of privacy leakage, focusing on email addresses.Their approach, which analyzes memorization and association, highlights vulnerabilities in how models might unintentionally disclose or link email addresses to individuals.</p>
<p>Finding: Detection-based Approaches</p>
<p>Due to the inherent complexity and variability of text data, scrutinizing the outputs of LLMs in practical applications has its limitations.Attackers can exploit these limitations by crafting impermissible outputs from seemingly permissible ones [121].This underscores the necessity for advanced and dynamic security measures, beyond simple output filtering or static rules, to effectively counteract sophisticated manipulation techniques and ensure the integrity and safety of LLMs applications.</p>
<p>C. Hardware-based Approaches</p>
<p>Hardware-based approaches for protecting the privacy of LLM focus on leveraging specialized hardware features and technologies to establish secure execution environments and safeguard data during processing.</p>
<p>Tech Tips: Hardware-based Approaches such as Trusted Execution Environments (TEEs), hardware virtualization, secure enclaves, hardware Root of Trust (RoT), and encrypted processing, aim to ensure the confidentiality, integrity, and privacy of both the model parameters and the data being processed.</p>
<p>1) Data Locality: PrivateLoRA [122] leveraged edge devices' storage for private data and personalized parameters, while utilizing the cloud for computational enhancement.It splits model parameters across the cloud and edge devices and transmits only unreadable activations and gradients to maintain data locality.The method integrates three sequential low-rank matrices for weight adaptation and reduces communication overhead through Low Rank Residual Transmission.It ensures data locality by keeping personalized parameters on edge devices and raw data derivatives on the cloud.The model targets query, key, and value projections in self-attention for adaptation to minimize communication overhead.PrivateLoRA is a paradigm that powers a heterogeneously distributed inference and training cycle, achieving high throughput and performance on smart phones.</p>
<p>2) Confidential Computing with Trusted Execution Environment (TEE): Confidential computing aims to address this gap by safeguarding data even while it is being processed.One key technology used in confidential computing is Trusted Execution Environments (TEEs).A TEE is a secure area of a computer's processor that ensures code and data loaded inside it are protected from unauthorized access or modification, even from the operating system or hypervisor.TEEs provide a secure environment where sensitive computations can be performed, ensuring the confidentiality and integrity of the data being processed [123]- [130].</p>
<p>The NVIDIA H100 GPU, featuring support for confidential computing, enhances data privacy by establishing a secure execution environment through hardware virtualization and a TEE [131].This environment ensures that data and code are processed securely during training or inference, preventing unauthorized access or modification by unauthorized users.By anchoring security measures in an on-die hardware root of trust (RoT), NVIDIA ensures the integrity of the GPU's boot sequence and establishes a chain of trust through cryptographic attestation.Furthermore, NVIDIA continues to enhance the security and integrity by incorporating features such as encrypted firmware, firmware revocation, and fault injection countermeasures.The TEEs applied in [132] protect privacy by securely executing custodial operations, encrypting and controlling access to data, and facilitating encrypted transmission of user queries and prompts.Huang et al. [133] introduced a method deploying TEEs on both client and server sides, implementing secure communication and split fine-tuning of a language model to maintain accuracy.</p>
<p>VIII. CHALLENGES AND FUTURE DIRECTIONS</p>
<p>A. Difficulties in Understanding Black-Box LLMs 1) Challenges: Pre-trained LLMs are often treated as black box models [134], [135], meaning that their internal workings and decision-making processes are not fully transparent or interpretable.This opacity makes it challenging to analyze and understand how these models handle sensitive information and whether they inadvertently leak privacy.In addition, LLMs are trained on vast amounts of diverse data, which may include sensitive or personally identifiable information.Understanding how these models process and retain such data without compromising privacy is inherently complex, especially given the intricate relationships between input data and model outputs.Language is dynamic and context-dependent, leading to challenges in predicting how LLMs will behave in various real-world scenarios.Privacy risks may vary depending on the context in which the model is deployed, making it difficult to generalize findings across different applications or domains.</p>
<p>2) Future Directions: Developing techniques to interpret and explain the decisions of pre-trained LLMs can shed light on their privacy implications.This may involve analyzing model activations, attention mechanisms, or other internal representations to identify potential privacy vulnerabilities.Conducting adversarial testing to evaluate the robustness of pre-trained LLMs against privacy attacks.For example, adversarial examples can be generated to probe the model's behavior and identify weaknesses that may lead to privacy breaches [136].Besides, we can focus on developing finetuning techniques that explicitly consider privacy concerns, such as differential privacy-aware optimization or adversarial training with privacy objectives.These techniques aim to mitigate privacy risks during the fine-tuning process.</p>
<p>B. Privacy in Multimodal LLMs</p>
<p>1) Challenges: The majority of research on LLMs has focused on purely textual models such as GPT and BERT.As a result, there may be a tendency for researchers to prioritize investigating the privacy implications of these models, leaving less attention on Multimodal LLMs.Multimodal LLMs, which integrate both textual and visual information, are a relatively recent development compared to their purely textual counterparts [137], [138].As such, there has been less time for researchers to explore and investigate their privacy implications thoroughly.Multimodal LLMs process a more diverse range of data types, including text, images, and possibly other modalities such as audio or video.Analyzing the privacy implications of such complex and heterogeneous data poses additional challenges compared to purely textual data, which may deter some researchers from delving into this area.</p>
<p>2) Future Directions: Redefining privacy in Multimodal LLMs is necessary to address the increased data complexity, unique privacy risks, intermodal interactions, user expectations, and regulatory considerations associated with multimodal data processing.Developing techniques to fuse different modalities while preserving user privacy is an important research direction.This could involve exploring encryption methods, differential privacy techniques, or novel privacypreserving machine learning algorithms tailored to multimodal data.Conducting adversarial analyses to identify potential vulnerabilities and privacy risks in Multimodal LLMs.This could involve exploring adversarial attacks and defenses specific to multimodal data, such as perturbing images or textual inputs to compromise privacy.</p>
<p>C. Privacy in Personalized LLMs</p>
<p>1) Challenges: Personalized LLMs may store and process sensitive user data, such as personal conversations, search queries, or browsing history.If not adequately protected, this data could be vulnerable to unauthorized access or misuse, leading to privacy breaches and potential harm to individuals.Personalized LLMs have the capacity to infer personal information about users based on their interactions with the model.This includes sensitive attributes such as health status, political views, financial situation, or intimate preferences.Such inferences could be unintentionally revealed through model responses or recommendations, compromising user privacy.Numerous small-scale enterprises offer users specialized largescale model services tailored to vertical domains, encompassing sectors such as judiciary, education, and finance.These expansive models entail a greater incorporation of domainspecific personal data.However, owing to the comparatively limited privacy safeguarding capabilities inherent in smallscale enterprises, the susceptibility to user privacy breaches is heightened, potentially precipitating irreversible ramifications.</p>
<p>2) Future Directions: To safeguard personalized finetuning of LLMs from privacy leakage, we need to explore architectures specifically designed with privacy [139].In addition, we can develop a combination of techniques.This includes implementing differential privacy methods to add noise during training, utilizing federated learning to train models locally on user devices, employing secure multiparty computation to jointly train models without sharing private data directly, introducing data perturbation to prevent memorization of sensitive information.We can also apply regularization methods to prevent overfitting, and exploring privacy-preserving architectures designed specifically for protecting sensitive data during fine-tuning.D. Privacy Protection Throughout the Entire Creation Process of LLMs 1) Challenges: Given the intricate complexity involved in training LLMs, privacy protection research tends to dissect various phases of LLM development and deployment, including pre-training, prompt tuning, and inference.Nevertheless, each segment within the LLM lifecycle harbors its own set of privacy vulnerabilities, and these stages do not operate in isolation [140].For instance, privacy breaches detected during the inference phase might originate from potential backdoors introduced during pre-training.Thus, safeguarding privacy comprehensively across large models demands concurrent scrutiny of multiple stages, a task that also introduces complexities and challenges into privacy protection efforts.</p>
<p>2) Future Directions: Protecting the privacy of LLMs throughout their creation process is paramount and requires a multifaceted approach.Firstly, during data collection, minimizing the collection of sensitive information and obtaining informed consent from users are critical steps.Data should be anonymized or pseudonymized to mitigate re-identification risks.Secondly, in data preprocessing and model training, techniques such as federated learning, secure multiparty computation, and differential privacy can be employed to train LLMs on decentralized data sources while preserving individual privacy.Additionally, conducting privacy impact assessments and adversarial testing during model evaluation ensures potential privacy risks are identified and addressed before deployment.In the deployment phase, privacy-preserving APIs and access controls can limit access to LLMs, while transparency and accountability measures foster trust with users by providing insight into data handling practices.Ongoing monitoring and maintenance, including continuous monitoring for privacy breaches and regular privacy audits, are essential to ensure compliance with privacy regulations and the effectiveness of privacy safeguards.By implementing these measures comprehensively throughout the LLM creation process, developers can mitigate privacy risks and build trust with users, thereby leveraging the capabilities of LLMs while safeguarding individual privacy.</p>
<p>E. Hardware-assisted Privacy Protection 1) Future Directions: NVIDIA Confidential Computing provides a comprehensive suite of privacy-enhancing features and technologies that safeguard LLM data and operations against unauthorized access, manipulation, and breaches, thereby ensuring the confidentiality and integrity of sensitive information throughout the LLM lifecycle.In the future, we can integrate confidential computing capabilities into LLM workflows, ensuring comprehensive privacy protection across the entire lifecycle, while continued innovation in GPU security features, such as encrypted firmware and fault injection countermeasures, reinforces the company's commitment to advancing data privacy safeguards for sensitive workloads.</p>
<p>IX. CONCLUSION</p>
<p>In this paper, we thoroughly investigates the data privacy concerns associated with LLMs, focusing on privacy leakage, privacy attacks, and the pivotal technologies for privacy protection during various stages of LLM privacy inference, including federated learning, differential privacy, knowledge unlearning, and hardware-assisted privacy protection.By conducting a detailed analysis of the strengths and weaknesses of existing approaches, this study highlights the challenges and limitations in LLMs and proposes directions for future work.This research is of significant importance for deepening our understanding of data privacy issues in LLMs and promoting further exploration and improvement in LLMs.</p>
<p>Fig. 1 .
1
Fig. 1.The current state of research on privacy protection for LLMs is depicted.The horizontal axis represents the time of LLM releases, while the vertical axis represents the size of model parameters.Blue dots signify LLM instances not addressed in literature pertaining to privacy protection, whereas black dots indicate those that have been examined in such literature.The green backdrop delineates the central cluster zone of LLMs with the potential to facilitate privacy protection.</p>
<p>Fig. 2 .
2
Fig. 2. The process of data propagation during both the training and inference stages of LLMs.</p>
<p>Fig. 3 .
3
Fig. 3.The distribution of research papers concerning the data privacy in LLMs."PT" and "FT" represent abbreviations for Pre-Training and Fine-Tuning, respectively.</p>
<p>Figure 4
4
Figure 4 offers an intricate portrayal of privacy concerns, encapsulating both privacy leaks and attacks, alongside the tailored defensive technologies deployed at various phases of LLMs lifecycle, including pre-training, fine-tuning, and inference stages.</p>
<p>Fig. 4 .
4
Fig. 4. Privacy threats, protection, and their defensive correlations.</p>
<p>Tech Tips:</p>
<p>Integrating differential privacy into the pretraining process of LLMs involves adding noise to the training data or model updates to safeguard individual privacy while maintaining effective model training.This can be achieved by injecting random noise into training data or perturbing gradients during backpropagation.Adaptive noise mechanisms dynamically adjust noise levels based on data sensitivity and privacy budgets.Careful management of the privacy budget ensures desired privacy levels are maintained.</p>
<p>Sensitive Query Contextual Leakage Personal Preferences Leakage Backdoor Attacks Membership Inference Attacks Model Inversion Attacks Attribute Inference Attacks Model Stealing Attacks Data Cleaning Differential Privacy Knowledge Unlearning Offsite Tuning Federated Learning Homomorphic Encryption Multi-Party Computation Functional Secret Sharing Direct Detection Contextual Inference Detection Data Locality Trusted Execution Environment Privacy Threats Privacy Leakage (Passive) Privacy Attacks (Active)</p>
<p>demonstrated this through the development of such an attack.And the study by Carlini et al [61] on GPT-2 demonstrates that adversaries can extract individual training examples through training data extraction attacks.Following this, Lehman et al [62] further investigated the risk of model inversion attacks on a BERT model trained on sensitive EHR data.Surprisingly, they found that simple probing methods failed to extract sensitive information, indicating a potential safety margin for releasing such model weights.However, Text Revealer [63] was designed by Zhang et al, which is the first model inversion attack specifically designed for reconstructing private texts from transformer-based text classification models.Its attack leverages external datasets and GPT-2 to generate fluent, domain-specific text, optimizing perturbations to the hidden state based on feedback from the target model.</p>
<p>Llm-based nlg evaluation: Current status and challenges. M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.013832024arXiv preprint</p>
<p>Translating natural language to planning goals with large-language models. Y Xie, C Yu, T Zhu, J Bai, Z Gong, H Soh, arXiv:2302.051282023arXiv preprint</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>A survey on robotics with foundation models: toward embodied ai. Z Xu, K Wu, J Wen, J Li, N Liu, Z Che, J Tang, arXiv:2402.023852024arXiv preprint</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, H Zhu, C Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt. Y Cao, S Li, Y Liu, Z Yan, Y Dai, P S Yu, L Sun, arXiv:2303.042262023arXiv preprint</p>
<p>Ai-generated content (aigc): A survey. J Wu, W Gan, Z Chen, S Wan, H Lin, arXiv:2304.066322023arXiv preprint</p>
<p>Y Yao, J Duan, K Xu, Y Cai, E Sun, Y Zhang, arXiv:2312.02003A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. 2023arXiv preprint</p>
<p>Detecting personal information in training corpora: an analysis. N Subramani, S Luccioni, J Dodge, M Mitchell, Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing. the 3rd Workshop on Trustworthy Natural Language ProcessingTrustNLP 2023. 2023</p>
<p>Beyond memorization: Violating privacy via inference with large language models. R Staab, M Vero, M Balunović, M Vechev, arXiv:2310.072982023arXiv preprint</p>
<p>Privacy-preserving fine-tuning of artificial intelligence (ai) foundation models with federated learning, differential privacy, offsite tuning, and parameter-efficient fine-tuning (peft). J Zhao, Authorea Preprints. 2023</p>
<p>Taiyi: A bilingual fine-tuned large language model for diverse biomedical tasks. L Luo, J Ning, Y Zhao, Z Wang, Z Ding, P Chen, W Fu, Q Han, G Xu, Y Qiu, arXiv:2311.116082023arXiv preprint</p>
<p>Scalable extraction of training data from (production) language models. M Nasr, N Carlini, J Hayase, M Jagielski, A F Cooper, D Ippolito, C A Choquette-Choo, E Wallace, F Tramèr, K Lee, arXiv:2311.170352023arXiv preprint</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Y Li, H Wen, W Wang, X Li, Y Yuan, G Liu, J Liu, W Xu, X Wang, Y Sun, arXiv:2401.05459Personal llm agents: Insights and survey about the capability, efficiency and security. 2024arXiv preprint</p>
<p>A survey on evaluation of large language models. Y Chang, X Wang, J Wang, Y Wu, K Zhu, H Chen, L Yang, X Yi, C Wang, Y Wang, arXiv:2307.031092023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>A survey on llm-gernerated text detection: Necessity, methods, and future directions. J Wu, S Yang, R Zhan, Y Yuan, D F Wong, L S Chao, arXiv:2310.147242023arXiv preprint</p>
<p>Eight things to know about large language models. S R Bowman, arXiv:2304.006122023arXiv preprint</p>
<p>H Naveed, A U Khan, S Qiu, M Saqib, S Anwar, M Usman, N Barnes, A Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M B Shaikh, N Akhtar, J Wu, S Mirjalili, 2023Authorea Preprints</p>
<p>Evaluating large language models: A comprehensive survey. Z Guo, R Jin, C Liu, Y Huang, D Shi, L Yu, Y Liu, J Li, B Xiong, D Xiong, arXiv:2310.197362023arXiv preprint</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models' alignment. Y Liu, Y Yao, J.-F Ton, X Zhang, R G H Cheng, Y Klochkov, M F Taufiq, H Li, arXiv:2308.053742023arXiv preprint</p>
<p>H Li, Y Chen, J Luo, Y Kang, X Zhang, Q Hu, C Chan, Y Song, arXiv:2310.10383Privacy in large language models: Attacks, defenses and future directions. 2023arXiv preprint</p>
<p>S Neel, P Chang, arXiv:2312.06717Privacy issues in large language models: A survey. 2023arXiv preprint</p>
<p>What effects do large language models have on cybersecurity. J Marshall, 2023</p>
<p>Chatgpt for cybersecurity: practical applications, challenges, and future directions. M Al-Hawawreh, A Aljuhani, Y Jararweh, Cluster Computing. 2662023</p>
<p>Chatbots to chatgpt in a cybersecurity space: Evolution, vulnerabilities, attacks, challenges, and future recommendations. A Qammar, H Wang, J Ding, A Naouri, M Daneshmand, H Ning, arXiv:2306.092552023arXiv preprint</p>
<p>Adversarial attacks and defenses in large language models: Old and new threats. L Schwinn, D Dobre, S Günnemann, G Gidel, arXiv:2310.197372023arXiv preprint</p>
<p>Beyond the safeguards: Exploring the security risks of chatgpt. E Derner, K Batistič, arXiv:2305.080052023arXiv preprint</p>
<p>Survey of vulnerabilities in large language models revealed by adversarial attacks. E Shayegani, M A A Mamun, Y Fu, P Zaree, Y Dong, N Abu-Ghazaleh, arXiv:2310.108442023arXiv preprint</p>
<p>Low-code llm: Visual programming over llms. Y Cai, S Mao, W Wu, Z Wang, Y Liang, T Ge, C Wu, W You, T Song, Y Xia, arXiv:2304.081032023arXiv preprint</p>
<p>Large language models effectively leverage document-level context for literary translation, but critical errors persist. M Karpinska, M Iyyer, arXiv:2304.032452023arXiv preprint</p>
<p>Large language models in medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature medicine. 2982023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>Datacomp: In search of the next generation of multimodal datasets. S Y Gadre, G Ilharco, A Fang, J Hayase, G Smyrnis, T Nguyen, R Marten, M Wortsman, D Ghosh, J Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Cybercrime and privacy threats of large language models. N Kshetri, IT Professional. 2532023</p>
<p>Why johnny can't prompt: how non-ai experts try (and fail) to design llm prompts. J Zamfirescu-Pereira, R Y Wong, B Hartmann, Q Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Llm platform security: Applying a systematic evaluation framework to openai's chatgpt plugins. U Iqbal, T Kohno, F Roesner, arXiv:2309.102542023arXiv preprint</p>
<p>Llm-rec: Personalized recommendation via prompting large language models. H Lyu, S Jiang, H Zeng, Y Xia, J Luo, arXiv:2307.157802023arXiv preprint</p>
<p>Leveraging large language models for sequential recommendation. J Harte, W Zorgdrager, P Louridas, A Katsifodimos, D Jannach, M Fragkoulis, Proceedings of the 17th ACM Conference on Recommender Systems. the 17th ACM Conference on Recommender Systems2023</p>
<p>Backdoor attacks on pre-trained models by layerwise weight poisoning. L Li, D Song, X Li, J Zeng, R Ma, X Qiu, arXiv:2108.138882021arXiv preprint</p>
<p>Be careful about poisoned word embeddings: Exploring the vulnerability of the embedding layers in nlp models. W Yang, L Li, Z Zhang, X Ren, X Sun, B He, arXiv:2103.155432021arXiv preprint</p>
<p>Poisonprompt: Backdoor attack on promptbased large language models. H Yao, J Lou, Z Qin, arXiv:2310.124392023arXiv preprint</p>
<p>H Huang, Z Zhao, M Backes, Y Shen, Y Zhang, arXiv:2310.07676Composite backdoor attacks against large language models. 2023arXiv preprint</p>
<p>Poisoning language models during instruction tuning. A Wan, E Wallace, S Shen, D Klein, arXiv:2305.009442023arXiv preprint</p>
<p>Instructions as backdoors: Backdoor vulnerabilities of instruction tuning for large language models. J Xu, M D Ma, F Wang, C Xiao, M Chen, arXiv:2305.147102023arXiv preprint</p>
<p>Backdooring instruction-tuned large language models with virtual prompt injection. J Yan, V Yadav, S Li, L Chen, Z Tang, H Wang, V Srinivasan, X Ren, H Jin, NeurIPS 2023 Workshop on Backdoors in Deep Learning-The Good, the Bad, and the Ugly. 2023</p>
<p>Membership inference attacks against machine learning models. R Shokri, M Stronati, C Song, V Shmatikov, 2017 IEEE symposium on security and privacy (SP). IEEE2017</p>
<p>Damia: leveraging domain adaptation as a defense against membership inference attacks. H Huang, W Luo, G Zeng, J Weng, Y Zhang, A Yang, IEEE Transactions on Dependable and Secure Computing. 1952021</p>
<p>Quantifying privacy risks of masked language models using membership inference attacks. F Mireshghallah, K Goyal, A Uniyal, T Berg-Kirkpatrick, R Shokri, arXiv:2203.039292022arXiv preprint</p>
<p>Membership inference attacks against language models via neighbourhood comparison. J Mattern, F Mireshghallah, Z Jin, B Schölkopf, M Sachan, T Berg-Kirkpatrick, arXiv:2305.184622023arXiv preprint</p>
<p>Detecting pretraining data from large language models. W Shi, A Ajith, M Xia, Y Huang, D Liu, T Blevins, D Chen, L Zettlemoyer, arXiv:2310.167892023arXiv preprint</p>
<p>Do membership inference attacks work on large language models. M Duan, A Suri, N Mireshghallah, S Min, W Shi, L Zettlemoyer, Y Tsvetkov, Y Choi, D Evans, H Hajishirzi, arXiv:2402.078412024arXiv preprint</p>
<p>An empirical analysis of memorization in fine-tuned autoregressive language models. F Mireshghallah, A Uniyal, T Wang, D K Evans, T Berg-Kirkpatrick, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Membership inference attack susceptibility of clinical language models. A Jagannatha, B P S Rawat, H Yu, arXiv:2104.083052021arXiv preprint</p>
<p>Practical membership inference attacks against fine-tuned large language models via self-prompt calibration. W Fu, H Wang, C Gao, G Liu, Y Li, T Jiang, arXiv:2311.060622023arXiv preprint</p>
<p>Information leakage in embedding models. C Song, A Raghunathan, Proceedings of the 2020 ACM SIGSAC conference on computer and communications security. the 2020 ACM SIGSAC conference on computer and communications security2020</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, 30th USENIX Security Symposium (USENIX Security 21. 2021</p>
<p>Does bert pretrained on clinical notes reveal sensitive data. E Lehman, S Jain, K Pichotta, Y Goldberg, B C Wallace, arXiv:2104.077622021arXiv preprint</p>
<p>Text revealer: Private text reconstruction via model inversion attacks against transformers. R Zhang, S Hidano, F Koushanfar, arXiv:2209.105052022arXiv preprint</p>
<p>Privacy-preserving prompt tuning for large language model services. Y Li, Z Tan, Y Liu, arXiv:2305.062122023arXiv preprint</p>
<p>Privacy risks of general-purpose language models. X Pan, M Zhang, S Ji, M Yang, 2020 IEEE Symposium on Security and Privacy (SP). IEEE2020</p>
<p>Thieves on sesame street! model extraction of bert-based apis. K Krishna, G S Tomar, A P Parikh, N Papernot, M Iyyer, arXiv:1910.123662019arXiv preprint</p>
<p>Data-free model extraction. J.-B Truong, P Maini, R J Walls, N Papernot, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2021</p>
<p>Prompt stealing attacks against large language models. Z Sha, Y Zhang, arXiv:2402.129592024arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022</p>
<p>Training a helpful and harmless assistant with reinforcement learning from human feedback. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan, 2022</p>
<p>. Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kerr, J Mueller, J Ladish, J Landau, K Ndousse, K Lukosuite, L Lovitt, M Sellitto, N Elhage, N Schiefer, N Mercado, N Dassarma, R Lasenby, R Larson, S Ringer, S Johnston, S Kravec, S E Showk, S Fort, T Lanham, T Telleen-Lawton, T Conerly, T Henighan, T Hume, S R Bowman, Z Hatfield-Dodds, B Mann, D Amodei, N Joseph, S Mccandlish, T Brown, J Kaplan, 2022Constitutional ai: Harmlessness from ai feedback</p>
<p>Deduplicating training data mitigates privacy risks in language models. N Kandpal, E Wallace, C Raffel, 2022</p>
<p>. C Chen, X Feng, J Zhou, J Yin, X Zheng, 2023Federated large language model: A position paper</p>
<p>Federated foundation models: Privacy-preserving and collaborative learning for large models. S Yu, J P Muñoz, A Jannesari, 2023</p>
<p>Learning and evaluating a differentially private pre-trained language model. S Hoory, A Feder, A Tendler, S Erell, A Peled-Cohen, I Laish, H Nakhost, U Stemmer, A Benjamini, A Hassidim, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Dp-fp: Differentially private forward propagation for large models. J Du, H Mi, arXiv:2112.144302021arXiv preprint</p>
<p>Large language models can be strong differentially private learners. X Li, F Tramer, P Liang, T Hashimoto, arXiv:2110.056792021arXiv preprint</p>
<p>Fwdllm: Efficient fedllm using forward gradient. M Xu, D Cai, Y Wu, X Li, S Wang, 2024</p>
<p>Towards building the federated gpt: Federated instruction tuning. J Zhang, S Vahidian, M Kuo, C Li, R Zhang, T Yu, Y Zhou, G Wang, Y Chen, 2024</p>
<p>Fedbpt: Efficient federated black-box prompt tuning for large language models. J Sun, Z Xu, H Yin, D Yang, D Xu, Y Chen, H R Roth, 2023</p>
<p>Fate-llm: A industrial grade federated learning framework for large language models. T Fan, Y Kang, G Ma, W Chen, W Wei, L Fan, Q Yang, 2023</p>
<p>Ewtune: A framework for privately fine-tuning large language models with differential privacy. R Behnia, M R Ebrahimi, J Pacheco, B Padmanabhan, 2022 IEEE International Conference on Data Mining Workshops (ICDMW). IEEE2022</p>
<p>Just fine-tune twice: Selective differential privacy for large language models. W Shi, R Shea, S Chen, C Zhang, R Jia, Z Yu, arXiv:2204.076672022arXiv preprint</p>
<p>Adaptive differential privacy for language model training. X Wu, L Gong, D Xiong, Proceedings of the First Workshop on Federated Learning for Natural Language Processing. the First Workshop on Federated Learning for Natural Language ProcessingFL4NLP 2022. 2022</p>
<p>Differentially private decoding in large language models. J Majmudar, C Dupuy, C Peris, S Smaili, R Gupta, R Zemel, arXiv:2205.136212022arXiv preprint</p>
<p>Dp-forward: Fine-tuning and inference on language models with differential privacy in forward pass. M Du, X Yue, S S Chow, T Wang, C Huang, H Sun, Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. the 2023 ACM SIGSAC Conference on Computer and Communications Security2023</p>
<p>P Mai, R Yan, Z Huang, Y Yang, Y Pang, arXiv:2310.09130Split-and-denoise: Protect large language model inference with local differential privacy. 2023arXiv preprint</p>
<p>Rethinking machine unlearning for large language models. S Liu, Y Yao, J Jia, S Casper, N Baracaldo, P Hase, X Xu, Y Yao, H Li, K R Varshney, M Bansal, S Koyejo, Y Liu, 2024</p>
<p>Right to be forgotten in the era of large language models: Implications, challenges, and solutions. D Zhang, P Finckenberg-Broman, T Hoang, S Pan, Z Xing, M Staples, X Xu, 2023</p>
<p>Unlearn what you want to forget: Efficient unlearning for llms. J Chen, D Yang, 2023</p>
<p>Knowledge unlearning for mitigating privacy risks in language models. J Jang, D Yoon, S Yang, S Cha, M Lee, L Logeswaran, M Seo, 2022</p>
<p>Who's harry potter? approximate unlearning in llms. R Eldan, M Russinovich, 2023</p>
<p>Offsite-tuning: Transfer learning without full model. G Xiao, J Lin, S Han, 2023</p>
<p>The-x: Privacy-preserving transformer inference with homomorphic encryption. T Chen, H Bao, S Huang, L Dong, B Jiao, D Jiang, H Zhou, J Li, F Wei, arXiv:2206.002162022arXiv preprint</p>
<p>Iron: Private inference on transformers. M Hao, H Li, H Chen, P Xing, G Xu, T Zhang, Advances in Neural Information Processing Systems. 202235</p>
<p>Bumblebee: Secure two-party inference framework for large transformers. W.-J Lu, Z Huang, Z Gu, J Li, J Liu, K Ren, C Hong, T Wei, W Chen, Cryptology ePrint Archive. 2023</p>
<p>Converting transformers to polynomial form for secure inference over homomorphic encryption. I Zimerman, M Baruch, N Drucker, G Ezov, O Soceanu, L Wolf, arXiv:2311.086102023arXiv preprint</p>
<p>Llms can understand encrypted prompt: Towards privacy-computing friendly transformers. X Liu, Z Liu, arXiv:2305.183962023arXiv preprint</p>
<p>Characterization of mpc-based private inference for transformer-based models. Y Wang, G E Suh, W Xiong, B Lefaudeux, B Knott, M Annavaram, H.-H S Lee, 2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE2022</p>
<p>Ciphergpt: Secure two-party gpt inference. X Hou, J Liu, J Li, Y Li, W.-J Lu, C Hong, K Ren, Cryptology ePrint Archive. 2023</p>
<p>East: Efficient and accurate secure transformer framework for inference. Y Ding, H Guo, Y Guan, W Liu, J Huo, Z Guan, X Zhang, arXiv:2308.099232023arXiv preprint</p>
<p>Privformer: Privacy-preserving transformer with mpc. Y Akimoto, K Fukuchi, Y Akimoto, J Sakuma, 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&amp;P). IEEE2023</p>
<p>Y Dong, W.-J Lu, Y Zheng, H Wu, D Zhao, J Tan, Z Huang, C Hong, T Wei, W Cheng, arXiv:2307.12533Puma: Secure inference of llama-7b in five minutes. 2023arXiv preprint</p>
<p>Sigma: secure gpt inference with function secret sharing. K Gupta, N Jawalkar, A Mukherjee, N Chandran, D Gupta, A Panwar, R Sharma, Cryptology ePrint Archive. 2023</p>
<p>Textfusion: Privacy-preserving pre-trained model inference via token fusion. X Zhou, J Lu, T Gui, R Ma, Z Fei, Y Wang, Y Ding, Y Cheung, Q Zhang, X.-J Huang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Secure transformer inference. M Yuan, L Zhang, X.-Y Li, arXiv:2312.000252023arXiv preprint</p>
<p>On the security of homomorphic encryption on approximate numbers. B Li, D Micciancio, Advances in Cryptology-EUROCRYPT 2021: 40th Annual International Conference on the Theory and Applications of Cryptographic Techniques. Zagreb, CroatiaSpringerOctober 17-21, 2021. 2021Proceedings, Part I 40</p>
<p>A survey on homomorphic encryption schemes: Theory and implementation. A Acar, H Aksu, A S Uluagac, M Conti, ACM Computing Surveys (Csur). 5142018</p>
<p>Secure multi-party computation. O Goldreich, Manuscript. Preliminary version. 781101998</p>
<p>Fusion: Efficient and secure inference resilient to malicious servers. C Dong, J Weng, J Liu, Y Zhang, Y Tong, A Yang, Y Cheng, S Hu, 30th Annual Network and Distributed System Security Symposium, NDSS 2023. San Diego, California, USAThe Internet SocietyFebruary 27 -March 3, 2023. 2023</p>
<p>Function secret sharing. E Boyle, N Gilboa, Y Ishai, Annual international conference on the theory and applications of cryptographic techniques. Springer2015</p>
<p>Analyzing leakage of personally identifiable information in language models. N Lukas, A Salem, R Sim, S Tople, L Wutschitz, S Zanella-Béguelin, 2023</p>
<p>Simple and efficient identification of personally identifiable information on a public website. C Brown, C Morisset, 2022 IEEE International Conference on Big Data (Big Data). IEEE2022</p>
<p>Depn: Detecting and editing privacy neurons in pretrained language models. X Wu, J Li, M Xu, W Dong, S Wu, C Bian, D Xiong, arXiv:2310.201382023arXiv preprint</p>
<p>Vaccine: Using contextual integrity for data leakage detection. Y Shvartzshnaider, Z Pavlinovic, A Balashankar, T Wies, L Subramanian, H Nissenbaum, P Mittal, The World Wide Web Conference. 2019</p>
<p>Propile: Probing privacy leakage in large language models. S Kim, S Yun, H Lee, M Gubri, S Yoon, S J Oh, 2023</p>
<p>Llm self defense: By self examination, llms know they are being tricked. M Phute, A Helbling, M Hull, S Peng, S Szyller, C Cornelius, D H Chau, 2023</p>
<p>Jailbreaker in jail: Moving target defense for large language models. B Chen, A Paliwal, Q Yan, 2023</p>
<p>Can llms keep a secret? testing privacy implications of language models via contextual integrity theory. N Mireshghallah, H Kim, X Zhou, Y Tsvetkov, M Sap, R Shokri, Y Choi, 2023</p>
<p>Are large pre-trained language models leaking your personal information. J Huang, H Shao, K C , -C Chang, 2022</p>
<p>Llm censorship: A machine learning challenge or a computer security problem. D Glukhov, I Shumailov, Y Gal, N Papernot, V Papyan, 2023</p>
<p>Y Wang, Y Lin, X Zeng, G Zhang, arXiv:2311.14030Privatelora for efficient privacy preserving llm. 2023arXiv preprint</p>
<p>A verified confidential computing as a service framework for privacy preservation. H Chen, H H Chen, M Sun, K Li, Z Chen, X Wang, 32nd USENIX Security Symposium. USENIX Security. 202323</p>
<p>Enabling rack-scale confidential computing using heterogeneous trusted execution environment. J Zhu, R Hou, X Wang, W Wang, J Cao, B Zhao, Z Wang, Y Zhang, J Ying, L Zhang, 2020 IEEE Symposium on Security and Privacy (SP). IEEE2020</p>
<p>Extending on-chain trust to off-chain -trustworthy blockchain data collection using trusted execution environment (tee). C Liu, H Guo, M Xu, S Wang, D Yu, J Yu, X Cheng, IEEE Transactions on Computers. 71122022</p>
<p>Sok: Tee-assisted confidential smart contract. R Li, Q Wang, Q Wang, D Galindo, M Ryan, arXiv:2203.085482022arXiv preprint</p>
<p>On security of trustzone-m-based iot systems. L Luo, Y Zhang, C White, B Keating, B Pearson, X Shao, Z Ling, H Yu, C Zou, X Fu, IEEE Internet of Things Journal. 9122022</p>
<p>Peripheral-free secure pairing protocol by randomly switching power. J Weng, S Zhijian, Y Zhang, M Li, W Jiasi, Y Wu, L Weiqi, Mar. 1 2022265722</p>
<p>On manually reverse engineering communication protocols of linuxbased iot systems. K Liu, M Yang, Z Ling, H Yan, Y Zhang, X Fu, W Zhao, IEEE Internet of Things Journal. 882020</p>
<p>Sic 2: Securing microcontroller based iot devices with low-cost crypto coprocessors. B Pearson, C Zou, Y Zhang, Z Ling, X Fu, 2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS). IEEE2020</p>
<p>Creating the first confidential gpus: The team at nvidia brings confidentiality and integrity to user code and data for accelerated computing. G Dhanuskodi, S Guha, V Krishnan, A Manjunatha, M O'connor, R Nertney, P Rogers, Queue. 2142023</p>
<p>Secure community transformers: Private pooled data for llms. T South, G Zuskind, R Mahari, T Hardjono, </p>
<p>A fast, performant, secure distributed training framework for large language model. W Huang, Y Wang, A Cheng, A Zhou, C Yu, L Wang, arXiv:2401.097962024arXiv preprint</p>
<p>Cyclealign: Iterative distillation from black-box llm to white-box models for better human alignment. J Hong, Q Tu, C Chen, X Gao, J Zhang, R Yan, arXiv:2310.162712023arXiv preprint</p>
<p>Augmenting black-box llms with medical textbooks for clinical question answering. Y Wang, X Ma, W Chen, arXiv:2309.022332023arXiv preprint</p>
<p>Jailbreaking black box large language models in twenty queries. P Chao, A Robey, E Dobriban, H Hassani, G J Pappas, E Wong, arXiv:2310.084192023arXiv preprint</p>
<p>Seed-bench: Benchmarking multimodal llms with generative comprehension. B Li, R Wang, G Wang, Y Ge, Y Ge, Y Shan, arXiv:2307.161252023arXiv preprint</p>
<p>The impact of multimodal large language models on health care's future. B Meskó, Journal of Medical Internet Research. 25e528652023</p>
<p>Firewallm: A portable data protection and recovery framework for llm services. B Huang, S Yu, J Li, Y Chen, S Huang, S Zeng, S Wang, International Conference on Data Mining and Big Data. Springer2023</p>
<p>J Evertz, M Chlosta, L Schönherr, T Eisenhofer, arXiv:2402.06922Whispers in the machine: Confidentiality in llm-integrated systems. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
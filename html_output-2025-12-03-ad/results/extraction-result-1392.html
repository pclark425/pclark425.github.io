<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1392 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1392</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1392</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-dea0f1c5949f8d898b9b6ff68226a781558e413c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dea0f1c5949f8d898b9b6ff68226a781558e413c" target="_blank">MOPO: Model-based Offline Policy Optimization</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A new model-based offline RL algorithm is proposed that applies the variance of a Lipschitz-regularized model as a penalty to the reward function, and it is found that this algorithm outperforms both standard model- based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks.</p>
                <p><strong>Paper Abstract:</strong> Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy. While there has been significant progress in model-free offline RL, the most successful prior methods constrain the policy to the support of the data, precluding generalization to new states. In this paper, we observe that an existing model-based RL algorithm on its own already produces significant gains in the offline setting, as compared to model-free approaches, despite not being designed for this setting. However, although many standard model-based RL methods already estimate the uncertainty of their model, they do not by themselves provide a mechanism to avoid the issues associated with distributional shift in the offline setting. We therefore propose to modify existing model-based RL methods to address these issues by casting offline model-based RL into a penalized MDP framework. We theoretically show that, by using this penalized MDP, we are maximizing a lower bound of the return in the true MDP. Based on our theoretical results, we propose a new model-based offline RL algorithm that applies the variance of a Lipschitz-regularized model as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks that require generalizing from data collected for a different task.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1392.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1392.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOPO dynamics ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ensemble of probabilistic neural dynamics used in MOPO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble of probabilistic neural-network dynamics models (Gaussian outputs) trained on offline transitions; used in MOPO to provide next-state/reward predictions and an uncertainty estimator (max predicted std) that penalizes model-based rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOPO ensemble probabilistic dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An ensemble of independently-trained feedforward neural networks that output a Gaussian distribution over next state and reward: each model i predicts mean mu^i(s,a) and covariance/variance Sigma^i(s,a). The ensemble (N=7 trained, best 5 selected via hold-out error) is used to sample k-step rollouts and to estimate uncertainty via the maximum predicted standard deviation (Frobenius norm of Sigma) across ensemble members.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic neural dynamics ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline reinforcement learning for continuous control (MuJoCo tasks, specialized out-of-distribution tasks), and a simulated HIV treatment domain</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Predicted aleatoric variance (model output Sigma) and ensemble-based epistemic disagreement; hold-out prediction error (on 1000 transitions) used to select models; uncertainty estimator u(s,a)=max_i ||Sigma^i(s,a)||_F</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural-network ensemble; components (means and variances) are inspectable numerically but no interpretable latent structure is described; paper does not claim human-interpretable features.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Ensemble of 7 models (4-layer feedforward networks with 200 hidden units per layer; two-head output for mean and variance); training requires fitting all models on the offline dataset and evaluating hold-out error on 1000 transitions; rollout lengths up to 25 steps used for some tasks; exact GPU/time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Qualitative: model-based approach (MBPO/MOPO) provides greater sample-efficiency and better empirical performance in many offline settings versus model-free baselines (SAC, BEAR, BRAC), but computational cost is higher due to ensemble training and multi-step model rollouts; no quantitative wall-clock comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used within MOPO to train policies that achieve substantially improved returns on benchmarks: e.g., halfcheetah-jump 4016.6 ± 144, ant-angle 2530.9 ± 137 (MOPO results reported in paper); MOPO also outperforms baselines on many D4RL MuJoCo tasks (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The ensemble-based uncertainty estimate is used as a reward penalty to produce conservative policies that avoid overfitting model errors; paper shows this improves policy performance in offline settings and enables out-of-distribution generalization where model-free methods fail. Oracle (true) error-based penalty gives slightly better performance, indicating utility depends on quality of uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Design trades off generalization vs safety: using ensemble + max-variance penalty is conservative (reduces exploitation of erroneous model predictions) but can reduce exploratory excursions; increasing ensemble size/computation improves uncertainty estimation but raises compute; choice of penalty coefficient lambda trades return (escaping data support) against risk (model error).</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Probabilistic Gaussian outputs for next-state and reward; ensemble of N=7 independent models, choose best 5 by hold-out error; uncertainty estimator u(s,a)=max_i ||Sigma^i(s,a)||_F (max standard deviation) used as reward penalty; spectral normalization applied to network layers; rollout initialization from dataset states and random actions used during model rollouts in some OOD tasks; user-chosen lambda and rollout horizon h (hyperparameters per environment).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to vanilla MBPO (same base model but no uncertainty penalty), the MOPO ensemble with penalty yields more reliable offline performance and substantially better results in out-of-distribution tasks; compared to model-free methods (SAC, BEAR, BRAC), MOPO with ensemble uncertainty often attains higher returns and generalizes beyond behavioral data; compared to hard-threshold methods (MOReL) MOPO uses a soft penalty enabling brief risky actions then return to confident regions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends conservative choices: use ensembles and probabilistic outputs, choose penalty coefficient lambda larger if the heuristic u(s,a) underestimates true error and smaller if it overestimates; ensemble of 7 with best 5 selected was used in experiments; rollout lengths and lambda values are chosen per task (Table 6) — no single optimal configuration claimed, but guidance emphasizes tuning lambda and using conservative max-variance penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOPO: Model-based Offline Policy Optimization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1392.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1392.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBPO model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model used by MBPO (Model-based Policy Optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The probabilistic dynamics/reward model architecture used in MBPO: supervised-learned neural networks for next-state/reward prediction, used to generate multi-step model rollouts for policy optimization (Dyna-style).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When to trust your model: Model-based policy optimization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MBPO probabilistic dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural-network dynamics and reward models trained by maximum likelihood on environment transitions; used to generate k-step synthetic rollouts starting from dataset states and augment real data for policy updates. Typically modeled as probabilistic Gaussian outputs (mean and variance).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic neural dynamics (single-model or ensemble-capable)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Reinforcement learning continuous-control (MuJoCo) and other control tasks; evaluated in both online and offline settings in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Prediction likelihood / mean squared error on next-state/reward; in MBPO practice models are evaluated with prediction error on held-out transitions (paper uses hold-out error to select ensemble members), but MBPO itself does not prescribe a single fidelity metric in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural-network predictors; no interpretability methods described.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training cost similar to other neural dynamics models; MBPO uses k-step rollouts (rollout horizon h typically 1 or 5 in experiments), cost scales with rollout horizon and model complexity; exact training time/GPU usage not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Empirically MBPO (vanilla) performs better than model-free SAC on offline tasks in this paper (surprisingly so), indicating algorithmic/sample-efficiency advantage in the offline regime, though MBPO without uncertainty handling can exploit model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>MBPO baseline scores (examples from paper): halfcheetah-jump 2971.4 ± 1262, ant-angle 13.6 ± 66; on many D4RL tasks MBPO performed well but was improved by MOPO's uncertainty penalty in the offline setting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MBPO provides useful synthetic data for policy learning and can generalize beyond offline data support; however, without explicit uncertainty penalization it may exploit model inaccuracies in offline settings, causing poor performance on some OOD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MBPO's use of model rollouts increases effective data for policy learning (improves sample-efficiency) but risks compounding model error over multi-step rollouts; longer rollouts give more supervision but amplify model inaccuracies if not corrected.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>k-step model rollouts initialized from dataset states; probabilistic outputs for dynamics and reward; in practice rollout horizon and whether to use ensembles are tuned per environment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>MBPO (model-based) vs model-free (SAC): MBPO often attains higher offline performance; MBPO vs MOPO: adding uncertainty-penalty (MOPO) improves reliability in offline setting; MBPO without ensembles or penalties can fail dramatically (per ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>MBPO performance sensitive to rollout horizon and model quality; paper uses small horizons (1 or 5) in many tasks; no global optimum provided but experimentation over horizon and uncertainty-handling recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOPO: Model-based Offline Policy Optimization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1392.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1392.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOReL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOReL: Model-based Offline Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent model-based offline RL approach that constructs terminal/penalized states by applying a hard uncertainty threshold, in contrast to MOPO's soft reward penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Morel: Model-based offline reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MOReL dynamics with uncertainty-threshold termination</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Model-based approach that uses an uncertainty estimate to determine states considered too unreliable and turns them into terminal (or heavily penalized) states via a hard threshold on model uncertainty, thereby avoiding rollouts into highly uncertain regions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic dynamics with hard termination based on uncertainty</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Offline reinforcement learning (benchmarks similar to D4RL / continuous-control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Hard threshold on an uncertainty measure (not detailed in this paper); the method relies on an uncertainty quantifier to decide termination.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Not discussed in this paper; MOReL's approach is algorithmically explicit (threshold-based) but underlying model interpretability not described here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this text; presumably similar to other model-based methods but with added bookkeeping for thresholding/termination.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper argues soft penalty (MOPO) may allow occasional risky actions and return to confident regions whereas a hard threshold may disallow that; no quantitative efficiency comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Hard-threshold approach is more conservative and can prevent catastrophic exploitation of model errors, but may also prevent beneficial brief excursions beyond the dataset that could improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Hard termination (MOReL) trades safety for potential loss of useful exploration beyond data support; MOPO's soft penalty trades more flexibly between risk and return.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Hard threshold vs soft penalty; termination of rollouts when uncertainty exceeds threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared conceptually to MOPO: MOReL uses a binary terminating criterion on uncertainty, while MOPO uses continuous reward penalties for uncertainty allowing graded risk-taking and recovery; both aim to control model exploitation in offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed here; MOReL requires choice of threshold level which controls conservatism, analogous to MOPO's lambda but in discrete form.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOPO: Model-based Offline Policy Optimization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>When to trust your model: Model-based policy optimization <em>(Rating: 2)</em></li>
                <li>Morel: Model-based offline reinforcement learning <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Pilco: A model-based and data-efficient approach to policy search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1392",
    "paper_id": "paper-dea0f1c5949f8d898b9b6ff68226a781558e413c",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "MOPO dynamics ensemble",
            "name_full": "Ensemble of probabilistic neural dynamics used in MOPO",
            "brief_description": "An ensemble of probabilistic neural-network dynamics models (Gaussian outputs) trained on offline transitions; used in MOPO to provide next-state/reward predictions and an uncertainty estimator (max predicted std) that penalizes model-based rollouts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MOPO ensemble probabilistic dynamics",
            "model_description": "An ensemble of independently-trained feedforward neural networks that output a Gaussian distribution over next state and reward: each model i predicts mean mu^i(s,a) and covariance/variance Sigma^i(s,a). The ensemble (N=7 trained, best 5 selected via hold-out error) is used to sample k-step rollouts and to estimate uncertainty via the maximum predicted standard deviation (Frobenius norm of Sigma) across ensemble members.",
            "model_type": "probabilistic neural dynamics ensemble",
            "task_domain": "Offline reinforcement learning for continuous control (MuJoCo tasks, specialized out-of-distribution tasks), and a simulated HIV treatment domain",
            "fidelity_metric": "Predicted aleatoric variance (model output Sigma) and ensemble-based epistemic disagreement; hold-out prediction error (on 1000 transitions) used to select models; uncertainty estimator u(s,a)=max_i ||Sigma^i(s,a)||_F",
            "fidelity_performance": null,
            "interpretability_assessment": "Black-box neural-network ensemble; components (means and variances) are inspectable numerically but no interpretable latent structure is described; paper does not claim human-interpretable features.",
            "interpretability_method": null,
            "computational_cost": "Ensemble of 7 models (4-layer feedforward networks with 200 hidden units per layer; two-head output for mean and variance); training requires fitting all models on the offline dataset and evaluating hold-out error on 1000 transitions; rollout lengths up to 25 steps used for some tasks; exact GPU/time not reported.",
            "efficiency_comparison": "Qualitative: model-based approach (MBPO/MOPO) provides greater sample-efficiency and better empirical performance in many offline settings versus model-free baselines (SAC, BEAR, BRAC), but computational cost is higher due to ensemble training and multi-step model rollouts; no quantitative wall-clock comparison reported.",
            "task_performance": "Used within MOPO to train policies that achieve substantially improved returns on benchmarks: e.g., halfcheetah-jump 4016.6 ± 144, ant-angle 2530.9 ± 137 (MOPO results reported in paper); MOPO also outperforms baselines on many D4RL MuJoCo tasks (see Table 1).",
            "task_utility_analysis": "The ensemble-based uncertainty estimate is used as a reward penalty to produce conservative policies that avoid overfitting model errors; paper shows this improves policy performance in offline settings and enables out-of-distribution generalization where model-free methods fail. Oracle (true) error-based penalty gives slightly better performance, indicating utility depends on quality of uncertainty estimates.",
            "tradeoffs_observed": "Design trades off generalization vs safety: using ensemble + max-variance penalty is conservative (reduces exploitation of erroneous model predictions) but can reduce exploratory excursions; increasing ensemble size/computation improves uncertainty estimation but raises compute; choice of penalty coefficient lambda trades return (escaping data support) against risk (model error).",
            "design_choices": "Probabilistic Gaussian outputs for next-state and reward; ensemble of N=7 independent models, choose best 5 by hold-out error; uncertainty estimator u(s,a)=max_i ||Sigma^i(s,a)||_F (max standard deviation) used as reward penalty; spectral normalization applied to network layers; rollout initialization from dataset states and random actions used during model rollouts in some OOD tasks; user-chosen lambda and rollout horizon h (hyperparameters per environment).",
            "comparison_to_alternatives": "Compared to vanilla MBPO (same base model but no uncertainty penalty), the MOPO ensemble with penalty yields more reliable offline performance and substantially better results in out-of-distribution tasks; compared to model-free methods (SAC, BEAR, BRAC), MOPO with ensemble uncertainty often attains higher returns and generalizes beyond behavioral data; compared to hard-threshold methods (MOReL) MOPO uses a soft penalty enabling brief risky actions then return to confident regions.",
            "optimal_configuration": "Paper recommends conservative choices: use ensembles and probabilistic outputs, choose penalty coefficient lambda larger if the heuristic u(s,a) underestimates true error and smaller if it overestimates; ensemble of 7 with best 5 selected was used in experiments; rollout lengths and lambda values are chosen per task (Table 6) — no single optimal configuration claimed, but guidance emphasizes tuning lambda and using conservative max-variance penalty.",
            "uuid": "e1392.0",
            "source_info": {
                "paper_title": "MOPO: Model-based Offline Policy Optimization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "MBPO model",
            "name_full": "Model used by MBPO (Model-based Policy Optimization)",
            "brief_description": "The probabilistic dynamics/reward model architecture used in MBPO: supervised-learned neural networks for next-state/reward prediction, used to generate multi-step model rollouts for policy optimization (Dyna-style).",
            "citation_title": "When to trust your model: Model-based policy optimization",
            "mention_or_use": "use",
            "model_name": "MBPO probabilistic dynamics model",
            "model_description": "Neural-network dynamics and reward models trained by maximum likelihood on environment transitions; used to generate k-step synthetic rollouts starting from dataset states and augment real data for policy updates. Typically modeled as probabilistic Gaussian outputs (mean and variance).",
            "model_type": "probabilistic neural dynamics (single-model or ensemble-capable)",
            "task_domain": "Reinforcement learning continuous-control (MuJoCo) and other control tasks; evaluated in both online and offline settings in paper.",
            "fidelity_metric": "Prediction likelihood / mean squared error on next-state/reward; in MBPO practice models are evaluated with prediction error on held-out transitions (paper uses hold-out error to select ensemble members), but MBPO itself does not prescribe a single fidelity metric in this text.",
            "fidelity_performance": null,
            "interpretability_assessment": "Black-box neural-network predictors; no interpretability methods described.",
            "interpretability_method": null,
            "computational_cost": "Training cost similar to other neural dynamics models; MBPO uses k-step rollouts (rollout horizon h typically 1 or 5 in experiments), cost scales with rollout horizon and model complexity; exact training time/GPU usage not reported in this paper.",
            "efficiency_comparison": "Empirically MBPO (vanilla) performs better than model-free SAC on offline tasks in this paper (surprisingly so), indicating algorithmic/sample-efficiency advantage in the offline regime, though MBPO without uncertainty handling can exploit model errors.",
            "task_performance": "MBPO baseline scores (examples from paper): halfcheetah-jump 2971.4 ± 1262, ant-angle 13.6 ± 66; on many D4RL tasks MBPO performed well but was improved by MOPO's uncertainty penalty in the offline setting.",
            "task_utility_analysis": "MBPO provides useful synthetic data for policy learning and can generalize beyond offline data support; however, without explicit uncertainty penalization it may exploit model inaccuracies in offline settings, causing poor performance on some OOD tasks.",
            "tradeoffs_observed": "MBPO's use of model rollouts increases effective data for policy learning (improves sample-efficiency) but risks compounding model error over multi-step rollouts; longer rollouts give more supervision but amplify model inaccuracies if not corrected.",
            "design_choices": "k-step model rollouts initialized from dataset states; probabilistic outputs for dynamics and reward; in practice rollout horizon and whether to use ensembles are tuned per environment.",
            "comparison_to_alternatives": "MBPO (model-based) vs model-free (SAC): MBPO often attains higher offline performance; MBPO vs MOPO: adding uncertainty-penalty (MOPO) improves reliability in offline setting; MBPO without ensembles or penalties can fail dramatically (per ablations).",
            "optimal_configuration": "MBPO performance sensitive to rollout horizon and model quality; paper uses small horizons (1 or 5) in many tasks; no global optimum provided but experimentation over horizon and uncertainty-handling recommended.",
            "uuid": "e1392.1",
            "source_info": {
                "paper_title": "MOPO: Model-based Offline Policy Optimization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "MOReL",
            "name_full": "MOReL: Model-based Offline Reinforcement Learning",
            "brief_description": "A concurrent model-based offline RL approach that constructs terminal/penalized states by applying a hard uncertainty threshold, in contrast to MOPO's soft reward penalty.",
            "citation_title": "Morel: Model-based offline reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "MOReL dynamics with uncertainty-threshold termination",
            "model_description": "Model-based approach that uses an uncertainty estimate to determine states considered too unreliable and turns them into terminal (or heavily penalized) states via a hard threshold on model uncertainty, thereby avoiding rollouts into highly uncertain regions.",
            "model_type": "probabilistic dynamics with hard termination based on uncertainty",
            "task_domain": "Offline reinforcement learning (benchmarks similar to D4RL / continuous-control)",
            "fidelity_metric": "Hard threshold on an uncertainty measure (not detailed in this paper); the method relies on an uncertainty quantifier to decide termination.",
            "fidelity_performance": null,
            "interpretability_assessment": "Not discussed in this paper; MOReL's approach is algorithmically explicit (threshold-based) but underlying model interpretability not described here.",
            "interpretability_method": null,
            "computational_cost": "Not specified in this text; presumably similar to other model-based methods but with added bookkeeping for thresholding/termination.",
            "efficiency_comparison": "Paper argues soft penalty (MOPO) may allow occasional risky actions and return to confident regions whereas a hard threshold may disallow that; no quantitative efficiency comparison provided here.",
            "task_performance": null,
            "task_utility_analysis": "Hard-threshold approach is more conservative and can prevent catastrophic exploitation of model errors, but may also prevent beneficial brief excursions beyond the dataset that could improve performance.",
            "tradeoffs_observed": "Hard termination (MOReL) trades safety for potential loss of useful exploration beyond data support; MOPO's soft penalty trades more flexibly between risk and return.",
            "design_choices": "Hard threshold vs soft penalty; termination of rollouts when uncertainty exceeds threshold.",
            "comparison_to_alternatives": "Compared conceptually to MOPO: MOReL uses a binary terminating criterion on uncertainty, while MOPO uses continuous reward penalties for uncertainty allowing graded risk-taking and recovery; both aim to control model exploitation in offline RL.",
            "optimal_configuration": "Not discussed here; MOReL requires choice of threshold level which controls conservatism, analogous to MOPO's lambda but in discrete form.",
            "uuid": "e1392.2",
            "source_info": {
                "paper_title": "MOPO: Model-based Offline Policy Optimization",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "When to trust your model: Model-based policy optimization",
            "rating": 2
        },
        {
            "paper_title": "Morel: Model-based offline reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2
        },
        {
            "paper_title": "Pilco: A model-based and data-efficient approach to policy search",
            "rating": 1
        }
    ],
    "cost": 0.01679725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MOPO: Model-based Offline Policy Optimization</h1>
<p>Tianhe Yu ${ }^{\star 1}$, Garrett Thomas ${ }^{\star 1}$, Lantao Yu ${ }^{1}$, Stefano Ermon ${ }^{1}$, James Zou ${ }^{1}$, Sergey Levine ${ }^{2}$, Chelsea Finn $\dagger^{1}$, Tengyu Ma $\dagger^{1}$<br>Stanford University ${ }^{1}$, UC Berkeley ${ }^{2}$<br>{tianheyu, gwthomas}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distributional shift between the offline training data and those states visited by the learned policy. Despite significant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generalization to unseen states. In this paper, we first observe that an existing model-based RL algorithm already produces significant gains in the offline setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the offline setting's distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artificially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maximizes a lower bound of the policy's return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data. Our algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing offline RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task.</p>
<h2>1 Introduction</h2>
<p>Recent advances in machine learning using deep neural networks have shown significant successes in scaling to large realistic datasets, such as ImageNet [13] in computer vision, SQuAD [55] in NLP, and RoboNet [10] in robot learning. Reinforcement learning (RL) methods, in contrast, struggle to scale to many real-world applications, e.g., autonomous driving [74] and healthcare [22], because they rely on costly online trial-and-error. However, pre-recorded datasets in domains like these can be large and diverse. Hence, designing RL algorithms that can learn from those diverse, static datasets would both enable more practical RL training in the real world and lead to more effective generalization.</p>
<p>While off-policy RL algorithms [43, 27, 20] can in principle utilize previously collected datasets, they perform poorly without online data collection. These failures are generally caused by large extrapolation error when the Q-function is evaluated on out-of-distribution actions [19, 36], which can lead to unstable learning and divergence. Offline RL methods propose to mitigate bootstrapped error by constraining the learned policy to the behavior policy induced by the dataset [19, 36, 72, 30, $49,52,58]$. While these methods achieve reasonable performances in some settings, their learning is limited to behaviors within the data manifold. Specifically, these methods estimate error with respect to out-of-distribution actions, but only consider states that lie within the offline dataset and do not</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>consider those that are out-of-distribution. We argue that it is important for an offline RL algorithm to be equipped with the ability to leave the data support to learn a better policy for two reasons: (1) the provided batch dataset is usually sub-optimal in terms of both the states and actions covered by the dataset, and (2) the target task can be different from the tasks performed in the batch data for various reasons, e.g., because data is not available or hard to collect for the target task. Hence, the central question that this work is trying to answer is: can we develop an offline RL algorithm that generalizes beyond the state and action support of the offline data?</p>
<p>To approach this question, we first hypothesize that model-based RL methods [64, 12, 42, 38, 29, 44] make a natural choice for enabling generalization, for a number of reasons. First, model-based RL algorithms effectively receive more supervision, since the model is trained on every transition, even in sparse-reward settings. Second, they are trained with supervised learning, which provides more stable and less noisy gradients than bootstrapping. Lastly, uncertainty estimation techniques, such as bootstrap ensembles, are well developed for supervised learning methods [40, 35, 60] and are known to perform poorly for value-based RL methods [72]. All of these attributes have the potential to improve or control generalization. As a proof-of-concept experiment, we evaluate two state-of-the-art off-policy model-based and model-free algorithms, MBPO [29] and SAC [27], in Figure 1. Although neither method is designed for the batch setting, we find that the model-based method and its variant without ensembles show surprisingly large gains. This finding corroborates our hypothesis, suggesting that model-based methods are particularly well-suited for the batch setting, motivating their use in this paper.</p>
<p>Despite these promising preliminary results, we expect significant headroom for improvement. In particular, because offline model-based algorithms cannot improve the dynamics model using additional experience, we expect that such algorithms require careful use of the model in regions outside of the data support. Quantifying the risk imposed by imperfect dynamics and appropriately trading off that risk with the return is a key ingredient towards building a strong offline model-based RL algorithm. To do so, we modify MBPO to incorporate a reward penalty based on an estimate of the model error. Crucially, this estimate is model-dependent, and does not necessarily penalize all out-of-distribution states and actions equally, but rather prescribes penalties based on the estimated magnitude of model error. Further, this estimation is done both on states and actions, allowing generalization to both, in contrast to model-free approaches that only reason about uncertainty with respect to actions.</p>
<p>The primary contribution of this work is an offline model-based RL algorithm that optimizes a policy in an uncertainty-penalized MDP, where the reward function is penalized by an estimate of the model’s error. Under this new MDP, we theoretically show that we maximize a lower bound of the return in the true MDP, and find the optimal trade-off between the return and the risk. Based on our analysis, we develop a practical method that estimates model error using the predicted variance of a learned model, uses this uncertainty estimate as a reward penalty, and trains a policy using MBPO in this uncertainty-penalized MDP. We empirically compare this approach, model-based offline policy optimization (MOPO), to both MBPO and existing state-of-the-art model-free offline RL algorithms. Our results suggest that MOPO substantially outperforms these prior methods on the offline RL benchmark D4RL [18] as well as on offline RL problems where the agent must generalize to out-of-distribution states in order to succeed.</p>
<h2>2 Related Work</h2>
<p>Reinforcement learning algorithms are well-known for their ability to acquire behaviors through online trial-and-error in the environment [3, 65]. However, such online data collection can incur high sample complexity [46, 56, 57], limit the power of generalization to unseen random initialization [9, 76, 4], and pose risks in safety-critical settings [68]. These requirements often make real-world applications of RL less feasible. To overcome some of these challenges, we study the batch offline</p>
<p>RL setting [41]. While many off-policy RL algorithms [53, 11, 31, 48, 43, 27, 20, 24, 25] can in principle be applied to a batch offline setting, they perform poorly in practice [19].
Model-free Offline RL. Many model-free batch RL methods are designed with two main ingredients: (1) constraining the learned policy to be closer to the behavioral policy either explicitly [19, 36, $72,30,49]$ or implicitly [52, 58], and (2) applying uncertainty quantification techniques, such as ensembles, to stabilize Q-functions [1, 36, 72]. In contrast, our model-based method does not rely on constraining the policy to the behavioral distribution, allowing the policy to potentially benefit from taking actions outside of it. Furthermore, we utilize uncertainty quantification to quantify the risk of leaving the behavioral distribution and trade it off with the gains of exploring diverse states.
Model-based Online RL. Our approach builds upon the wealth of prior work on model-based online RL methods that model the dynamics by Gaussian processes [12], local linear models [42, 38], neural network function approximators [15, 21, 14], and neural video prediction models [16, 32]. Our work is orthogonal to the choice of model. While prior approaches have used these models to select actions using planning [67, 17, 54, 51, 59, 70], we choose to build upon Dyna-style approaches that optimize for a policy [64, 66, 73, 32, 26, 28, 44], specifically MBPO [29]. See [71] for an empirical evaluation of several model-based RL algorithms. Uncertainty quantification, a key ingredient to our approach, is critical to good performance in model-based RL both theoretically [63, 75, 44] and empirically [12, 7, 50, 39, 8], and in optimal control [62, 2, 34]. Unlike these works, we develop and leverage proper uncertainty estimates that particularly suit the offline setting.
Concurrent work by Kidambi et al. [33] also develops an offline model-based RL algorithm, MOReL. Unlike MOReL, which constructs terminating states based on a hard threshold on uncertainty, MOPO uses a soft reward penalty to incorporate uncertainty. In principle, a potential benefit of a soft penalty is that the policy is allowed to take a few risky actions and then return to the confident area near the behavioral distribution without being terminated. Moreover, while Kidambi et al. [33] compares to model-free approaches, we make the further observation that even a vanilla model-based RL method outperforms model-free ones in the offline setting, opening interesting questions for future investigation. Finally, we evaluate our approach on both standard benchmarks [18] and domains that require out-of-distribution generalization, achieving positive results in both.</p>
<h1>3 Preliminaries</h1>
<p>We consider the standard Markov decision process (MDP) $M=\left(\mathcal{S}, \mathcal{A}, T, r, \mu_{0}, \gamma\right)$, where $\mathcal{S}$ and $\mathcal{A}$ denote the state space and action space respectively, $T\left(s^{\prime} \mid s, a\right)$ the transition dynamics, $r(s, a)$ the reward function, $\mu_{0}$ the initial state distribution, and $\gamma \in(0,1)$ the discount factor. The goal in RL is to optimize a policy $\pi(a \mid s)$ that maximizes the expected discounted return $\eta_{M}(\pi):=$ $\underset{\pi, T, \mu_{0}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right)\right]$. The value function $V_{M}^{\pi}(s):=\underset{\pi, T}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right) \mid s_{0}=s\right]$ gives the expected discounted return under $\pi$ when starting from state $s$.
In the offline RL problem, the algorithm only has access to a static dataset $\mathcal{D}<em _env="{env" _text="\text">{\text {env }}=\left{\left(s, a, r, s^{\prime}\right)\right}$ collected by one or a mixture of behavior policies $\pi^{\mathrm{B}}$, and cannot interact further with the environment. We refer to the distribution from which $\mathcal{D}</em>$ was sampled as the behavioral distribution.
We also introduce the following notation for the derivation in Section 4. In the model-based approach we will have a dynamics model $\widehat{T}$ estimated from the transitions in $\mathcal{D}}<em 0="0">{\text {env }}$. This estimated dynamics defines a model MDP $\widehat{M}=\left(\mathcal{S}, \mathcal{A}, \widehat{T}, r, \mu</em>}, \gamma\right)$. Let $\mathbb{P<em _widehat_T="\widehat{T">{\widehat{T}, t}^{\pi}(s)$ denote the probability of being in state $s$ at time step $t$ if actions are sampled according to $\pi$ and transitions according to $\widehat{T}$. Let $\rho</em>}}^{\pi}(s, a)$ be the discounted occupancy measure of policy $\pi$ under dynamics $\widehat{T}: \rho_{\widehat{T}}^{\pi}(s, a):=\pi(a \mid s) \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P<em _widehat_T="\widehat{T">{\widehat{T}, t}^{\pi}(s)$. Note that $\rho</em>}}^{\pi}$, as defined here, is not a properly normalized probability distribution, as it integrates to $1 /(1-\gamma)$. We will denote (improper) expectations with respect to $\rho_{\widehat{T}}^{\pi}$ with $\mathbb{E}$, as in $\eta_{\widehat{M}}(\pi)=$ $\mathbb{E<em _widehat_T="\widehat{T">{\rho</em>[r(s, a)]$.
We now summarize model-based policy optimization (MBPO) [29], which we build on in this work. MBPO learns a model of the transition distribution $\widehat{T}}}^{\pi}<em _env="{env" _text="\text">{\theta}\left(s^{\prime} \mid s, a\right)$ parametrized by $\theta$, via supervised learning on the behavorial data $\mathcal{D}</em> \mid s, a\right)$ starting from state}}$. MBPO also learns a model of the reward function in the same manner. During training, MBPO performs $k$-step rollouts using $\widehat{T}_{\theta}\left(s^{\prime</p>
<p>$s \in \mathcal{D}<em _model="{model" _text="\text">{\text {env }}$, adds the generated data to a separate replay buffer $\mathcal{D}</em>}}$, and finally updates the policy $\pi(a \mid s)$ using data sampled from $\mathcal{D<em _model="{model" _text="\text">{\text {env }} \cup \mathcal{D}</em>$. When applied in an online setting, MBPO iteratively collects samples from the environment and uses them to further improve both the model and the policy. In our experiments in Table 1, Table 5.2 and Table 1, we observe that MBPO performs surprisingly well on the offline RL problem compared to model-free methods. In the next section, we derive MOPO, which builds upon MBPO to further improve performance.}</p>
<h1>4 MOPO: Model-Based Offline Policy Optimization</h1>
<p>Unlike model-free methods, our goal is to design an offline model-based reinforcement learning algorithm that can take actions that are not strictly within the support of the behavioral distribution. Using a model gives us the potential to do so. However, models will become increasingly inaccurate further from the behavioral distribution, and vanilla model-based policy optimization algorithms may exploit these regions where the model is inaccurate. This concern is especially important in the offline setting, where mistakes in the dynamics will not be corrected with additional data collection.
For the algorithm to perform reliably, it's crucial to balance the return and risk: 1. the potential gain in performance by escaping the behavioral distribution and finding a better policy, and 2. the risk of overfitting to the errors of the dynamics at regions far away from the behavioral distribution. To achieve the optimal balance, we first bound the return from below by the return of a constructed model MDP penalized by the uncertainty of the dynamics (Section 4.1). Then we maximize the conservative estimation of the return by an off-the-shelf reinforcement learning algorithm, which gives MOPO, a generic model-based off-policy algorithm (Section 4.2). We discuss important practical implementation details in Section 4.3.</p>
<h3>4.1 Quantifying the uncertainty: from the dynamics to the total return</h3>
<p>Our key idea is to build a lower bound for the expected return of a policy $\pi$ under the true dynamics and then maximize the lower bound over $\pi$. A natural estimator for the true return $\eta_{M}(\pi)$ is $\eta_{\widehat{M}}(\pi)$, the return under the estimated dynamics. The error of this estimator depends on, potentially in a complex fashion, the error of $\widehat{M}$, which may compound over time. In this subsection, we characterize how the error of $\widehat{M}$ influences the uncertainty of the total return. We begin by stating a lemma (adapted from [44]) that gives a precise relationship between the performance of a policy under dynamics $T$ and dynamics $\widehat{T}$. (All proofs are given in Appendix B.)
Lemma 4.1 (Telescoping lemma). Let $M$ and $\widehat{M}$ be two MDPs with the same reward function $r$, but different dynamics $T$ and $\widehat{T}$ respectively. Let $G_{\widehat{M}}^{\pi}(s, a):=\underset{s^{\prime} \sim \widehat{T}(s, a)}{\mathbb{E}}\left[V_{M}^{\pi}\left(s^{\prime}\right)\right]-\underset{s^{\prime} \sim \widehat{T}(s, a)}{\mathbb{E}}\left[V_{M}^{\pi}\left(s^{\prime}\right)\right]$. Then,</p>
<p>$$
\eta_{\widehat{M}}(\pi)-\eta_{M}(\pi)=\gamma_{\frac{\mathbb{E}}{\left(s, a\right) \sim \rho_{\hat{T}}^{\pi}}}\left[G_{\widehat{M}}^{\pi}(s, a)\right]
$$</p>
<p>As an immediate corollary, we have</p>
<p>$$
\eta_{M}(\pi)=\underset{(s, a) \sim \rho_{\hat{T}}^{\pi}}{\mathbb{E}}\left[r(s, a)-\gamma G_{\widehat{M}}^{\pi}(s, a)\right] \geq \underset{(s, a) \sim \rho_{\hat{T}}^{\pi}}{\mathbb{E}}\left[r(s, a)-\gamma\left|G_{\widehat{M}}^{\pi}(s, a)\right|\right]
$$</p>
<p>Here and throughout the paper, we view $T$ as the real dynamics and $\widehat{T}$ as the learned dynamics. We observe that the quantity $G_{\widehat{M}}^{\pi}(s, a)$ plays a key role linking the estimation error of the dynamics and the estimation error of the return. By definition, we have that $G_{\widehat{M}}^{\pi}(s, a)$ measures the difference between $M$ and $\widehat{M}$ under the test function $V^{\pi}$ — indeed, if $M=\widehat{M}$, then $G_{\widehat{M}}^{\pi}(s, a)=0$. By equation (1), it governs the differences between the performances of $\pi$ in the two MDPs. If we could estimate $G_{\widehat{M}}^{\pi}(s, a)$ or bound it from above, then we could use the RHS of (1) as an upper bound for the estimation error of $\eta_{M}(\pi)$. Moreover, equation (2) suggests that a policy that obtains high reward in the estimated MDP while also minimizing $G_{\widehat{M}}^{\pi}$ will obtain high reward in the real MDP.
However, computing $G_{\widehat{M}}^{\pi}$ remains elusive because it depends on the unknown function $V_{M}^{\pi}$. Leveraging properties of $V_{M}^{\pi}$, we will replace $G_{\widehat{M}}^{\pi}$ by an upper bound that depends solely on the error of the</p>
<p>dynamics $\widehat{T}$. We first note that if $\mathcal{F}$ is a set of functions mapping $\mathcal{S}$ to $\mathbb{R}$ that contains $V_{M}^{\pi}$, then,</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq \sup <em _mathcal_F="\mathcal{F">{f \in \mathcal{F}}\left|\underset{s^{\prime} \sim \widehat{T}(s, a)}{\mathbb{E}}\left[f\left(s^{\prime}\right)\right]-\underset{s^{\prime} \sim \widehat{T}(s, a)}{\mathbb{E}}\left[f\left(s^{\prime}\right)\right]\right|=: d</em>(s, a), T(s, a))
$$}}(\widehat{T</p>
<p>where $d_{\mathcal{F}}$ is the integral probability metric (IPM) [47] defined by $\mathcal{F}$. IPMs are quite general and contain several other distance measures as special cases [61]. Depending on what we are willing to assume about $V_{M}^{\pi}$, there are multiple options to bound $G_{M}^{\pi}$ by some notion of error of $\widehat{T}$, discussed in greater detail in Appendix A:
(i) If $\mathcal{F}=\left{f:|f|<em _mathcal_F="\mathcal{F">{\infty} \leq 1\right}$, then $d</em>\right|}}$ is the total variation distance. Thus, if we assume that the reward function is bounded such that $\forall(s, a),|r(s, a)| \leq r_{\max }$, we have $\left|V^{\pi<em t="0">{\infty} \leq \sum</em>$, and hence}^{\infty} \gamma^{t} r_{\max }=\frac{r_{\max }}{1-\gamma</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq \frac{r_{\max }}{1-\gamma} D_{\mathrm{TV}}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>(ii) If $\mathcal{F}$ is the set of 1-Lipschitz function w.r.t. to some distance metric, then $d_{\mathcal{F}}$ is the 1 -Wasserstein distance w.r.t. the same metric. Thus, if we assume that $V_{M}^{\pi}$ is $L_{v}$-Lipschitz with respect to a norm $|\cdot|$, it follows that</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq L_{v} W_{1}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>Note that when $\widehat{T}$ and $T$ are both deterministic, then $W_{1}(\widehat{T}(s, a), T(s, a))=|\widehat{T}(s, a)-T(s, a)|$ (here $T(s, a)$ denotes the deterministic output of the model $T$ ).
Approach (ii) has the advantage that it incorporates the geometry of the state space, but at the cost of an additional assumption which is generally impossible to verify in our setting. The assumption in (i), on the other hand, is extremely mild and typically holds in practice. Therefore we will prefer (i) unless we have some prior knowledge about the MDP. We summarize the assumptions and the inequalities in the options above as follows.
Assumption 4.2. Assume a scalar $c$ and a function class $\mathcal{F}$ such that $V_{M}^{\pi} \in c \mathcal{F}$ for all $\pi$.
As a direct corollary of Assumption 4.2 and equation (3), we have</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq c d_{\mathcal{F}}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>Concretely, option (i) above corresponds to $c=r_{\max } /(1-\gamma)$ and $\mathcal{F}=\left{f:|f|<em v="v">{\infty} \leq 1\right}$, and option (ii) corresponds to $c=L</em>={f: f$ is 1-Lipschitz $}$. We will analyze our framework under the assumption that we have access to an oracle uncertainty quantification module that provides an upper bound on the error of the model. In our implementation, we will estimate the error of the dynamics by heuristics (see sections 4.3 and D).
Assumption 4.3. Let $\mathcal{F}$ be the function class in Assumption 4.2. We say $u: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is an admissible error estimator for $\widehat{T}$ if $d_{\mathcal{F}}(\widehat{T}(s, a), T(s, a)) \leq u(s, a)$ for all $s \in \mathcal{S}, a \in \mathcal{A} .^{2}$}$ and $\mathcal{F</p>
<p>Given an admissible error estimator, we define the uncertainty-penalized reward $\tilde{r}(s, a):=r(s, a)-$ $\lambda u(s, a)$ where $\lambda:=\gamma c$, and the uncertainty-penalized MDP $\widetilde{M}=(\mathcal{S}, \mathcal{A}, \widehat{T}, \tilde{r}, \mu_{0}, \gamma)$. We observe that $\widetilde{M}$ is conservative in that the return under it bounds from below the true return:</p>
<p>$$
\begin{aligned}
\eta_{M}(\pi) &amp; \geq \underset{(s, a) \sim \rho_{T}^{\pi}}{\mathbb{E}}\left[r(s, a)-\gamma\left|G_{M}^{\pi}(s, a)\right|\right] \geq \underset{(s, a) \sim \rho_{T}^{\pi}}{\mathbb{E}}[r(s, a)-\lambda u(s, a)] \
&amp; \geq \underset{(s, a) \sim \rho_{T}^{\pi}}{\mathbb{E}}[\tilde{r}(s, a)]=\eta_{\widetilde{M}}(\pi)
\end{aligned}
$$</p>
<h1>4.2 Policy optimization on uncertainty-penalized MDPs</h1>
<p>Motivated by (7), we optimize the policy on the uncertainty-penalized MDP $\widetilde{M}$ in Algorithm 1.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Algorithm 1 Framework for Model-based Offline Policy Optimization (MOPO) with Reward Penalty
Require: Dynamics model $\widehat{T}$ with admissible error estimator $u(s, a)$; constant $\lambda$.
1: Define $\widehat{r}(s, a)=r(s, a)-\lambda u(s, a)$. Let $\widehat{M}$ be the MDP with dynamics $\widehat{T}$ and reward $\widehat{r}$.
2: Run any RL algorithm on $\widehat{M}$ until convergence to obtain $\hat{\pi}=\operatorname{argmax}<em _widehat_M="\widehat{M">{\pi} \eta</em>(\pi)$}</p>
<p>Theoretical Guarantees for MOPO. We will theoretical analyze the algorithm by establishing the optimality of the learned policy $\hat{\pi}$ among a family of policies. Let $\pi^{\star}$ be the optimal policy on $M$ and $\pi^{\mathrm{B}}$ be the policy that generates the batch data. Define $\epsilon_{u}(\pi)$ as</p>
<p>$$
\epsilon_{u}(\pi):=\underset{(s, a) \sim \rho_{\widehat{T}}^{\pi}}{\mathbb{E}}[u(s, a)]
$$</p>
<p>Note that $\epsilon_{u}$ depends on $\widehat{T}$, but we omit this dependence in the notation for simplicity. We observe that $\epsilon_{u}(\pi)$ characterizes how erroneous the model is along trajectories induced by $\pi$. For example, consider the extreme case when $\pi=\pi^{\mathrm{B}}$. Because $\widehat{T}$ is learned on the data generated from $\pi^{\mathrm{B}}$, we expect $\widehat{T}$ to be relatively accurate for those $(s, a) \sim \rho_{\widehat{T}}^{\pi^{\mathrm{B}}}$, and thus $u(s, a)$ tends to be small. Thus, we expect $\epsilon_{u}\left(\pi^{\mathrm{B}}\right)$ to be quite small. On the other end of the spectrum, when $\pi$ often visits states out of the batch data distribution in the real MDP, namely $\rho_{\widehat{T}}^{\pi}$ is different from $\rho_{\widehat{T}}^{\pi^{\mathrm{B}}}$, we expect that $\rho_{\widehat{T}}^{\pi}$ is even more different from the batch data and therefore the error estimates $u(s, a)$ for those $(s, a) \sim \rho_{\widehat{T}}^{\pi}$ tend to be large. As a consequence, we have that $\epsilon_{u}(\pi)$ will be large.
For $\delta \geq \delta_{\min }:=\min <em u="u">{\pi} \epsilon</em>$ be the best policy among those incurring model error at most $\delta$ :}(\pi)$, let $\pi^{\delta</p>
<p>$$
\pi^{\delta}:=\underset{\pi: \epsilon_{u}(\pi) \leq \delta}{\arg \max } \eta_{M}(\pi)
$$</p>
<p>The main theorem provides a performance guarantee on the policy $\hat{\pi}$ produced by MOPO.
Theorem 4.4. Under Assumption 4.2 and 4.3, the learned policy $\hat{\pi}$ in MOPO (Algorithm 1) satisfies</p>
<p>$$
\eta_{M}(\hat{\pi}) \geq \sup <em M="M">{\pi}\left{\eta</em>(\pi)\right}
$$}(\pi)-2 \lambda \epsilon_{u</p>
<p>In particular, for all $\delta \geq \delta_{\min }$,</p>
<p>$$
\eta_{M}(\hat{\pi}) \geq \eta_{M}\left(\pi^{\delta}\right)-2 \lambda \delta
$$</p>
<p>Interpretation: One consequence of (10) is that $\eta_{M}(\hat{\pi}) \geq \eta_{M}\left(\pi^{\mathrm{B}}\right)-2 \lambda \epsilon_{u}\left(\pi^{\mathrm{B}}\right)$. This suggests that $\hat{\pi}$ should perform at least as well as the behavior policy $\pi^{\mathrm{B}}$, because, as argued before, $\epsilon_{u}\left(\pi^{\mathrm{B}}\right)$ is expected to be small.
Equation (11) tells us that the learned policy $\hat{\pi}$ can be as good as any policy $\pi$ with $\epsilon_{u}(\pi) \leq \delta$, or in other words, any policy that visits states with sufficiently small uncertainty as measured by $u(s, a)$. A special case of note is when $\delta=\epsilon_{u}\left(\pi^{\star}\right)$, we have $\eta_{M}(\hat{\pi}) \geq \eta_{M}\left(\pi^{\star}\right)-2 \lambda \epsilon_{u}\left(\pi^{\star}\right)$, which suggests that the suboptimality gap between the learned policy $\hat{\pi}$ and the optimal policy $\pi^{\star}$ depends on the error $\epsilon_{u}\left(\pi^{\star}\right)$. The closer $\rho_{\widehat{T}}^{\pi^{\star}}$ is to the batch data, the more likely the uncertainty $u(s, a)$ will be smaller on those points $(s, a) \sim \rho_{\widehat{T}}^{\pi^{\star}}$. On the other hand, the smaller the uncertainty error of the dynamics is, the smaller $\epsilon_{u}\left(\pi^{\star}\right)$ is. In the extreme case when $u(s, a)=0$ (perfect dynamics and uncertainty quantification), we recover the optimal policy $\pi^{\star}$.
Second, by varying the choice of $\delta$ to maximize the RHS of Equation (11), we trade off the risk and the return. As $\delta$ increases, the return $\eta_{M}\left(\pi^{\delta}\right)$ increases also, since $\pi^{\delta}$ can be selected from a larger set of policies. However, the risk factor $2 \lambda \delta$ increases also. The optimal choice of $\delta$ is achieved when the risk balances the gain from exploring policies far from the behavioral distribution. The exact optimal choice of $\delta$ may depend on the particular problem. We note $\delta$ is only used in the analysis, and our algorithm automatically achieves the optimal balance because Equation (11) holds for any $\delta$.</p>
<h1>4.3 Practical implementation</h1>
<p>Now we describe a practical implementation of MOPO motivated by the analysis above. The method is summarized in Algorithm 2 in Appendix C, and largely follows MBPO with a few key exceptions.</p>
<p>Following MBPO, we model the dynamics using a neural network that outputs a Gaussian distribution over the next state and reward $^{3}: \widetilde{T}<em t_1="t+1">{\theta, \phi}\left(s</em>}, r \mid s_{t}, a_{t}\right)=\mathcal{N}\left(\mu_{\theta}\left(s_{t}, a_{t}\right), \Sigma_{\phi}\left(s_{t}, a_{t}\right)\right)$. We learn an ensemble of $N$ dynamics models $\left{\widetilde{T<em _theta="\theta">{\theta, \phi}^{i}=\mathcal{N}\left(\mu</em>$, with each model trained independently via maximum likelihood.}^{i}, \Sigma_{\phi}^{i}\right)\right}_{i=1}^{N</p>
<p>The most important distinction from MBPO is that we use uncertainty quantification following the analysis above. We aim to design the uncertainty estimator that captures both the epistemic and aleatoric uncertainty of the true dynamics. Bootstrap ensembles have been shown to give a consistent estimate of the population mean in theory [5] and empirically perform well in model-based RL [7]. Meanwhile, the learned variance of a Gaussian probabilistic model can theoretically recover the true aleatoric uncertainty when the model is well-specified. To leverage both, we design our error estimator $u(s, a)=\max <em _phi="\phi">{i=1}^{N}\left|\Sigma</em>(s, a)\right|}^{i<em N="N" _ldots_="\ldots," i="1,">{\mathrm{F}}$, the maximum standard deviation of the learned models in the ensemble. We use the maximum of the ensemble elements rather than the mean to be more conservative and robust. While this estimator lacks theoretical guarantees, we find that it is sufficiently accurate to achieve good performance in practice. ${ }^{4}$ Hence the practical uncertainty-penalized reward of MOPO is computed as $\tilde{r}(s, a)=\hat{r}(s, a)-\lambda \max </em>$.
We treat the penalty coefficient $\lambda$ as a user-chosen hyperparameter. Since we do not have a true admissible error estimator, the value of $\lambda$ prescribed by the theory may not be an optimal choice in practice; it should be larger if our heuristic $u(s, a)$ underestimates the true error and smaller if $u$ substantially overestimates the true error.}\left|\Sigma_{\phi}^{i}(s, a)\right|_{\mathrm{F}}$ where $\hat{r}$ is the mean of the predicted reward output by $\widehat{T</p>
<h1>5 Experiments</h1>
<p>In our experiments, we aim to study the follow questions: (1) How does MOPO perform on standard offline RL benchmarks in comparison to prior state-of-the-art approaches? (2) Can MOPO solve tasks that require generalization to out-of-distribution behaviors? (3) How does each component in MOPO affect performance?</p>
<p>Question (2) is particularly relevant for scenarios in which we have logged interactions with the environment but want to use those data to optimize a policy for a different reward function. To study (2) and challenge methods further, we construct two additional continuous control tasks that demand out-of-distribution generalization, as described in Section 5.2. To answer question (3), we conduct a complete ablation study to analyze the effect of each module in MOPO in Appendix D. For more details on the experimental set-up and hyperparameters, see Appendix G. For more details on the experimental set-up and hyperparameters, see Appendix G. The code is available online ${ }^{5}$.</p>
<p>We compare against several baselines, including the current state-of-the-art model-free offline RL algorithms. Bootstrapping error accumulation reduction (BEAR) aims to constrain the policy's actions to lie in the support of the behavioral distribution [36]. This is implemented as a constraint on the average MMD [23] between $\pi(\cdot \mid s)$ and a generative model that approximates $\pi^{\mathrm{R}}(\cdot \mid s)$. Behaviorregularized actor critic (BRAC) is a family of algorithms that operate by penalizing the value function by some measure of discrepancy ( KL divergence or MMD) between $\pi(\cdot \mid s)$ and $\pi^{\mathrm{R}}(\cdot \mid s)$ [72]. BRACv uses this penalty both when updating the critic and when updating the actor, while BRAC-p uses this penalty only when updating the actor and does not explicitly penalize the critic.</p>
<h3>5.1 Evaluation on the D4RL benchmark</h3>
<p>To answer question (1), we evaluate our method on a large subset of datasets in the D4RL benchmark [18] based on the MuJoCo simulator [69], including three environments (halfcheetah, hopper, and walker2d) and four dataset types (random, medium, mixed, medium-expert), yielding a total of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset type</th>
<th style="text-align: left;">Environment</th>
<th style="text-align: right;">BC</th>
<th style="text-align: right;">MOPO (ours)</th>
<th style="text-align: right;">MBPO</th>
<th style="text-align: right;">SAC</th>
<th style="text-align: right;">BEAR</th>
<th style="text-align: right;">BRAC-v</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: right;">2.1</td>
<td style="text-align: right;">$\mathbf{3 5 . 4} \pm 2.5$</td>
<td style="text-align: right;">$30.7 \pm 3.9$</td>
<td style="text-align: right;">30.5</td>
<td style="text-align: right;">25.5</td>
<td style="text-align: right;">28.1</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: right;">1.6</td>
<td style="text-align: right;">$11.7 \pm 0.4$</td>
<td style="text-align: right;">$4.5 \pm 6.0$</td>
<td style="text-align: right;">11.3</td>
<td style="text-align: right;">9.5</td>
<td style="text-align: right;">$\mathbf{1 2 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: right;">9.8</td>
<td style="text-align: right;">$\mathbf{1 3 . 6} \pm 2.6$</td>
<td style="text-align: right;">$8.6 \pm 8.1$</td>
<td style="text-align: right;">4.1</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: right;">36.1</td>
<td style="text-align: right;">$42.3 \pm 1.6$</td>
<td style="text-align: right;">$28.3 \pm 22.7$</td>
<td style="text-align: right;">-4.3</td>
<td style="text-align: right;">38.6</td>
<td style="text-align: right;">$\mathbf{4 5 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: right;">29.0</td>
<td style="text-align: right;">$28.0 \pm 12.4$</td>
<td style="text-align: right;">$4.9 \pm 3.3$</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">$\mathbf{4 7 . 6}$</td>
<td style="text-align: right;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: right;">6.6</td>
<td style="text-align: right;">$17.8 \pm 19.3$</td>
<td style="text-align: right;">$12.7 \pm 7.6$</td>
<td style="text-align: right;">0.9</td>
<td style="text-align: right;">33.2</td>
<td style="text-align: right;">$\mathbf{8 1 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: right;">38.4</td>
<td style="text-align: right;">$\mathbf{5 3 . 1} \pm 2.0$</td>
<td style="text-align: right;">$47.3 \pm 12.6$</td>
<td style="text-align: right;">-2.4</td>
<td style="text-align: right;">36.2</td>
<td style="text-align: right;">45.9</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: right;">11.8</td>
<td style="text-align: right;">$\mathbf{6 7 . 5} \pm 24.7$</td>
<td style="text-align: right;">$49.8 \pm 30.4$</td>
<td style="text-align: right;">1.9</td>
<td style="text-align: right;">10.8</td>
<td style="text-align: right;">0.9</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: right;">11.3</td>
<td style="text-align: right;">$\mathbf{3 9 . 0} \pm 9.6$</td>
<td style="text-align: right;">$22.2 \pm 12.7$</td>
<td style="text-align: right;">3.5</td>
<td style="text-align: right;">25.3</td>
<td style="text-align: right;">0.8</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: right;">35.8</td>
<td style="text-align: right;">$\mathbf{6 3 . 3} \pm 38.0$</td>
<td style="text-align: right;">$9.7 \pm 9.5$</td>
<td style="text-align: right;">1.8</td>
<td style="text-align: right;">51.7</td>
<td style="text-align: right;">45.3</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: right;">111.9</td>
<td style="text-align: right;">$23.7 \pm 6.0$</td>
<td style="text-align: right;">$\mathbf{5 6 . 0} \pm 34.5$</td>
<td style="text-align: right;">1.6</td>
<td style="text-align: right;">4.0</td>
<td style="text-align: right;">0.8</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: right;">6.4</td>
<td style="text-align: right;">$44.6 \pm 12.9$</td>
<td style="text-align: right;">$7.6 \pm 3.7$</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">26.0</td>
<td style="text-align: right;">$\mathbf{6 6 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Results for D4RL datasets. Each number is the normalized score proposed in [18] of the policy at the last iteration of training, averaged over 6 random seeds, $\pm$ standard deviation. The scores are undiscounted average returns normalized to roughly lie between 0 and 100, where a score of 0 corresponds to a random policy, and 100 corresponds to an expert. We include the performance of behavior cloning (BC) from the batch data for comparison. Numbers for model-free methods taken from [18], which does not report standard deviation. We omit BRAC-p in this table for space because BRAC-v obtains higher performance in 10 of these 12 tasks and is only slightly weaker on the other two. We bold the highest mean.</p>
<p>12 problem settings. We also perform empirical evaluations on non-MuJoCo environments in Appendix F. The datasets in this benchmark have been generated as follows: random: roll out a randomly initialized policy for 1 M steps. medium: partially train a policy using SAC, then roll it out for 1 M steps. mixed: train a policy using SAC until a certain (environment-specific) performance threshold is reached, and take the replay buffer as the batch. medium-expert: combine 1M samples of rollouts from a fully-trained policy with another 1 M samples of rollouts from a partially trained policy or a random policy.
Results are given in Table 1. MOPO is the strongest by a significant margin on all the mixed datasets and most of the medium-expert datasets, while also achieving strong performance on all of the random datasets. MOPO performs less well on the medium datasets. We hypothesize that the lack of action diversity in the medium datasets make it more difficult to learn a model that generalizes well. Fortunately, this setting is one in which model-free methods can perform well, suggesting that model-based and model-free approaches are able to perform well in complementary settings.</p>
<h1>5.2 Evaluation on tasks requiring out-of-distribution generalization</h1>
<p>To answer question (2), we construct two environments halfcheetah-jump and ant-angle where the agent must solve a task that is different from the purpose of the behavioral policy. The trajectories of the batch data in the these datasets are from policies trained for the original dynamics and reward functions HalfCheetah and Ant in OpenAI Gym [6] which incentivize the cheetach and ant to move forward as fast as possible. Note that for HalfCheetah, we set the maximum velocity to be 3. Concretely, we train SAC for 1 M steps and use the entire training replay buffer as the trajectories for the batch data. Then, we assign these trajectories with new rewards that incentivize the cheetach to jump and the ant to run towards the top right corner with a 30 degree angle. Thus, to achieve good performance for the new reward functions, the policy need to leave the observational distribution, as visualized in Figure 2. We include the exact forms of the new reward functions in Appendix G. In these environments, learning the correct behaviors requires leaving the support of the data distribution; optimizing solely within the data manifold will lead to sub-optimal policies.
In Table 2, we show that MOPO significantly outperforms the state-of-the-art model-free approaches. In particular, model-free offline RL cannot outperform the best trajectory in the batch dataset, whereas MOPO exceeds the batch max by a significant margin. This validates that MOPO is able to generalize to out-of-distribution behaviors while existing model-free methods are unable to solve those challenges. Note that vanilla MBPO performs much better than SAC in the two environments, consolidating our claim that vanilla model-based methods can attain better results than model-free methods in the offline setting, especially where generalization to out-of-distribution is needed. The visualization in Figure 2 suggests indeed the policy learned MOPO can effectively solve the tasks by reaching to states unseen in the batch data. Furthermore, we test the limit of the generalization abilities of MOPO in these environments and the results are included in Appendix E.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: We visualize the two out-of-distribution generalization environments halfcheetah-jump (bottom row) and ant-angle (top row). We show the training environments that generate the batch data on the left. On the right, we show the test environments where the agents perform behaviors that require the learned policies to leave the data support. In halfcheetah-jump, the agent is asked to run while jumping as high as possible given an training offline dataset of halfcheetah running. In ant-angle, the ant is rewarded for running forward in a 30 degree angle and the corresponding training offline dataset contains data of the ant running forward directly.</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>Batch Mean</th>
<th>Batch Max</th>
<th>MOPO (ours)</th>
<th>MBPO</th>
<th>SAC</th>
<th>BEAR</th>
<th>BRAC-p</th>
<th>BRAC-v</th>
</tr>
</thead>
<tbody>
<tr>
<td>halfcheetah-jump</td>
<td>-1022.6</td>
<td>1808.6</td>
<td>4016.6$\pm$144</td>
<td>2971.4$\pm$1262</td>
<td>-3588.2$\pm$1436</td>
<td>16.8$\pm$60</td>
<td>1069.9$\pm$232</td>
<td>871$\pm$41</td>
</tr>
<tr>
<td>ant-angle</td>
<td>866.7</td>
<td>2311.9</td>
<td>2530.9$\pm$137</td>
<td>13.6$\pm$66</td>
<td>-966.4$\pm$778</td>
<td>1658.2$\pm$16</td>
<td>1806.7$\pm$265</td>
<td>2333$\pm$139</td>
</tr>
</tbody>
</table>
<p>Table 2: Average returns halfcheetah-jump and ant-angle that require out-of-distribution policy. The MOPO results are averaged over 6 random seeds, $\pm$ standard deviation, while the results of other methods are averaged over 3 random seeds. We include the mean and max undiscounted return of the episodes in the batch data (under Batch Mean and Batch Max, respectively) for comparison. Note that Batch Mean and Max are significantly lower than on-policy SAC, suggesting that the behaviors stored in the buffers are far from optimal and the agent needs to go beyond the data support in order to achieve better performance. As shown in the results, MOPO outperforms all the baselines by a large margin, indicating that MOPO is effective in generalizing to out-of-distribution states where model-free offline RL methods struggle.</p>
<h1>6 Conclusion</h1>
<p>In this paper, we studied model-based offline RL algorithms. We started with the observation that, in the offline setting, existing model-based methods significantly outperform vanilla model-free methods, suggesting that model-based methods are more resilient to the overestimation and overfitting issues that plague off-policy model-free RL algorithms. This phenomenon implies that model-based RL has the ability to generalize to states outside of the data support and such generalization is conducive for offline RL. However, online and offline algorithms must act differently when handling out-of-distribution states. Model error on out-of-distribution states that often drives exploration and corrective feedback in the online setting [37] can be detrimental when interaction is not allowed. Using theoretical principles, we develop an algorithm, model-based offline policy optimization (MOPO), which maximizes the policy on a MDP that penalizes states with high model uncertainty. MOPO trades off the risk of making mistakes and the benefit of diverse exploration from escaping the behavioral distribution. In our experiments, MOPO outperforms state-of-the-art offline RL methods in both standard benchmarks [18] and out-of-distribution generalization environments.</p>
<p>Our work opens up a number of questions and directions for future work. First, an interesting avenue for future research to incorporate the policy regularization ideas of BEAR and BRAC into the reward penalty framework to improve the performance of MOPO on narrow data distributions (such as the "medium" datasets in D4RL). Second, it's an interesting theoretical question to understand why model-based methods appear to be much better suited to the batch setting than model-free methods. Multiple potential factors include a greater supervision from the states (instead of only the reward), more stable and less noisy supervised gradient updates, or ease of uncertainty estimation. Our work suggests that uncertainty estimation plays an important role, particularly in settings that demand generalization. However, uncertainty estimation does not explain the entire difference nor does it explain why model-free methods cannot also enjoy the benefits of uncertainty estimation. For those domains where learning a model may be very difficult due to complex dynamics, developing better model-free offline RL methods may be desirable or imperative. Hence, it is crucial to conduct future research on investigating how to bring model-free offline RL methods up to the level of the performance of model-based methods, which would require further understanding where the generalization benefits come from.</p>
<h1>Broader Impact</h1>
<p>MOPO achieves significant strides in offline reinforcement learning, a problem setting that is particularly scalable to real-world settings. Offline reinforcement learning has a number of potential application domains, including autonomous driving, healthcare, robotics, and is notably amenable to safety-critical settings where online data collection is costly. For example, in autonomous driving, online interaction with the environment runs the risk of crashing and hurting people; offline RL methods can significantly reduce that risk by learning from a pre-recorded driving dataset collected by a safe behavioral policy. Moreover, our work opens up the possibility of learning policies offline for new tasks for which we do not already have expert data.
However, there are still risks associated with applying learned policies to high-risk domains. We have shown the benefits of explicitly accounting for error, but without reliable out-of-distribution uncertainty estimation techniques, there is a possibility that the policy will behave unpredictably when given a scenario it has not encountered. There is also the challenge of reward design: although the reward function will typically be under the engineer's control, it can be difficult to specify a reward function that elicits the desired behavior and is aligned with human objectives. Additionally, parametric models are known to be susceptible to adversarial attacks, and bad actors can potentially exploit this vulnerability. Advances in uncertainty quantification, human-computer interaction, and robustness will improve our ability to apply learning-based methods in safety-critical domains.
Supposing we succeed at producing safe and reliable policies, there is still possibility of negative societal impact. An increased ability to automate decision-making processes may reduce companies' demand for employees in certain industries (e.g. manufacturing and logistics), thereby affecting job availability. However, historically, advances in technology have also created new jobs that did not previously exist (e.g. software engineering), and it is unclear if the net impact on jobs will be positive or negative.
Despite the aforementioned risks and challenges, we believe that offline RL is a promising setting with enormous potential for automating and improving sequential decision-making in highly impactful domains. Currently, much additional work is needed to make offline RL sufficiently robust to be applied in safety-critical settings. We encourage the research community to pursue further study in uncertainty estimation, particularly considering the complications that arise in sequential decision problems.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We thank Michael Janner for help with MBPO and Aviral Kumar for setting up BEAR and D4RL. TY is partially supported by Intel Corporation. CF is a CIFAR Fellow in the Learning in Machines and Brains program. TM and GT are also partially supported by Lam Research, Google Faculty Award, SDSI, and SAIL.</p>
<h2>References</h2>
<p>[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. Striving for simplicity in off-policy deep reinforcement learning. arXiv preprint arXiv:1907.04543, 2019.
[2] Andrzej Banaszuk, Vladimir A Fonoberov, Thomas A Frewen, Marin Kobilarov, George Mathew, Igor Mezic, Alessandro Pinto, Tuhin Sahai, Harshad Sane, Alberto Speranzon, et al. Scalable approach to uncertainty quantification and robust design of interconnected dynamical systems. Annual Reviews in Control, 35(1):77-98, 2011.
[3] Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834-846, 1983.
[4] Emmanuel Bengio, Joelle Pineau, and Doina Precup. Interference and generalization in temporal difference learning. arXiv preprint arXiv:2003.06350, 2020.
[5] Peter J Bickel and David A Freedman. Some asymptotic theory for the bootstrap. The annals of statistics, pages 1196-1217, 1981.</p>
<p>[6] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
[7] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems, pages 4754-4765, 2018.
[8] Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel. Model-based reinforcement learning via meta-policy optimization. arXiv preprint arXiv:1809.05214, 2018.
[9] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. arXiv preprint arXiv:1812.02341, 2018.
[10] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019.
[11] Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. arXiv preprint arXiv:1205.4839, 2012.
[12] Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472, 2011.
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.
[14] Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez, and Steffen Udluft. Learning and policy search in stochastic dynamical systems with bayesian neural networks. arXiv preprint arXiv:1605.07127, 2016.
[15] Andreas Draeger, Sebastian Engell, and Horst Ranke. Model predictive control using neural networks. IEEE Control Systems Magazine, 15(5):61-66, 1995.
[16] Frederik Ebert, Chelsea Finn, Sudeep Dasari, Annie Xie, Alex Lee, and Sergey Levine. Visual foresight: Model-based deep reinforcement learning for vision-based robotic control. arXiv preprint arXiv:1812.00568, 2018.
[17] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.
[18] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.
[19] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. arXiv preprint arXiv:1812.02900, 2018.
[20] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
[21] Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, ICML, volume 4, page 34, 2016.
[22] Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale Doshi-Velez, and Leo Anthony Celi. Guidelines for reinforcement learning in healthcare. Nat Med, 25(1):16-18, 2019.
[23] Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard Scholköpf, and Alexander J. Smola. A kernel approach to comparing distributions. In Proceedings of the 22nd National Conference on Artificial Intelligence - Volume 2, AAAI'07, page 1637-1641. AAAI Press, 2007. ISBN 9781577353232.
[24] Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E Turner, and Sergey Levine. Q-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247, 2016.</p>
<p>[25] Shixiang Shane Gu, Timothy Lillicrap, Richard E Turner, Zoubin Ghahramani, Bernhard Schölkopf, and Sergey Levine. Interpolated policy gradient: Merging on-policy and offpolicy gradient estimation for deep reinforcement learning. In Advances in neural information processing systems, pages 3846-3855, 2017.
[26] David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.
[27] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.
[28] G Zacharias Holland, Erin J Talvitie, and Michael Bowling. The effect of planning shape on dyna-style planning in high-dimensional state spaces. arXiv preprint arXiv:1806.01825, 2018.
[29] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, pages 12498-12509, 2019.
[30] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019.
[31] Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. arXiv preprint arXiv:1511.03722, 2015.
[32] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, and Henryk Michalewski. Model-based reinforcement learning for atari, 2019.
[33] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-based offline reinforcement learning. arXiv preprint arXiv:2005.05951, 2020.
[34] Kwang-Ki K Kim, Dongying Erin Shen, Zoltan K Nagy, and Richard D Braatz. Wiener's polynomial chaos for the analysis and control of nonlinear dynamical systems with probabilistic uncertainties [historical perspectives]. IEEE Control Systems Magazine, 33(5):58-67, 2013.
[35] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning using calibrated regression. arXiv preprint arXiv:1807.00263, 2018.
[36] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, pages 11761-11771, 2019.
[37] Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020.
[38] Vikash Kumar, Emanuel Todorov, and Sergey Levine. Optimal control with learned local models: Application to dexterous manipulation. In 2016 IEEE International Conference on Robotics and Automation (ICRA), pages 378-383. IEEE, 2016.
[39] Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
[40] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in neural information processing systems, pages 6402-6413, 2017.
[41] Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforcement learning, pages 45-73. Springer, 2012.
[42] Sergey Levine and Vladlen Koltun. Guided policy search. In International Conference on Machine Learning, pages 1-9, 2013.
[43] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[44] Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. arXiv preprint arXiv:1807.03858, 2018.</p>
<p>[45] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
[46] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937, 2016.
[47] Alfred Müller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429-443, 1997.
[48] Rémi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare. Safe and efficient off-policy reinforcement learning. In Advances in Neural Information Processing Systems, pages 1054-1062, 2016.
[49] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
[50] Anusha Nagabandi, Kurt Konoglie, Sergey Levine, and Vikash Kumar. Deep dynamics models for learning dexterous manipulation. arXiv preprint arXiv:1909.11652, 2019.
[51] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. In Advances in Neural Information Processing Systems, pages 6118-6128, 2017.
[52] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
[53] Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with function approximation. In ICML, pages 417-424, 2001.
[54] Sébastien Racanière, Théophane Weber, David Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez Rezende, Adria Puigdomenech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li, et al. Imagination-augmented agents for deep reinforcement learning. In Advances in neural information processing systems, pages 5690-5701, 2017.
[55] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.
[56] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889-1897, 2015.
[57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
[58] Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.
[59] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-toend learning and planning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3191-3199. JMLR. org, 2017.
[60] Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D Sculley, Joshua Dillon, Jie Ren, and Zachary Nado. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, pages 13969-13980, 2019.
[61] Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and Gert RG Lanckriet. On integral probability metrics, $\backslash$ phi-divergences and binary classification. arXiv preprint arXiv:0901.2698, 2009.
[62] Robert F Stengel. Optimal control and estimation. Courier Corporation, 1994.
[63] Alexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309-1331, 2008.
[64] Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart Bulletin, 2(4):160-163, 1991.
[65] Richard S Sutton and Andrew G Barto. Reinforcement learning, 1998.</p>
<p>[66] Richard S Sutton, Csaba Szepesvári, Alborz Geramifard, and Michael P Bowling. Dynastyle planning with linear function approximation and prioritized sweeping. arXiv preprint arXiv:1206.3285, 2012.
[67] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154-2162, 2016.
[68] Philip S Thomas. Safe reinforcement learning. PhD thesis, University of Massachusetts Libraries, 2015.
[69] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033. IEEE, 2012.
[70] Tingwu Wang and Jimmy Ba. Exploring model-based planning with policy networks. arXiv preprint arXiv:1906.08649, 2019.
[71] Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba. Benchmarking model-based reinforcement learning. arXiv preprint arXiv:1907.02057, 2019.
[72] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.
[73] Hengshuai Yao, Shalabh Bhatnagar, Dongcui Diao, Richard S Sutton, and Csaba Szepesvári. Multi-step dyna planning for policy evaluation and control. In Advances in neural information processing systems, pages 2187-2195, 2009.
[74] Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving video database with scalable annotation tooling. arXiv preprint arXiv:1805.04687, 2018.
[75] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds.
[76] Amy Zhang, Nicolas Ballas, and Joelle Pineau. A dissection of overfitting and generalization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018.</p>
<h1>Appendix</h1>
<h2>A Reminders about integral probability metrics</h2>
<p>Let $(\mathcal{X}, \Sigma)$ be a measurable space. The integral probability metric associated with a class $\mathcal{F}$ of (measurable) real-valued functions on $\mathcal{X}$ is defined as</p>
<p>$$
d_{\mathcal{F}}(P, Q)=\sup <em _mathcal_X="\mathcal{X">{f \in \mathcal{F}}\left|\int</em> Q\right|=\sup }} f \mathrm{~d} P-\int_{\mathcal{X}} f \mathrm{~d<em P="P" X="X" _sim="\sim">{f \in \mathcal{F}}\left|\sum</em>[f(Y)]\right|
$$}[f(X)]-\sum_{Y \sim Q</p>
<p>where $P$ and $Q$ are probability measures on $\mathcal{X}$. We note the following special cases:
(i) If $\mathcal{F}={f:|f|<em _mathcal_F="\mathcal{F">{\infty} \leq 1}$, then $d</em>$ is the total variation distance}</p>
<p>$$
d_{\mathcal{F}}(P, Q)=D_{\mathrm{TV}}(P, Q):=\sup _{A \in \Sigma}|P(A)-Q(A)|
$$</p>
<p>(ii) If $\mathcal{F}$ is the set of 1-Lipschitz function w.r.t. to some cost function (metric) $c$ on $\mathcal{X}$, then $d_{\mathcal{F}}$ is the 1 -Wasserstein distance w.r.t. the same metric:</p>
<p>$$
d_{\mathcal{F}}(P, Q)=W_{1}(P, Q):=\inf <em _mathcal_X="\mathcal{X">{\gamma \in \Gamma(P, Q)} \int</em> \gamma(x, y)
$$}^{2}} c(x, y) \mathrm{d</p>
<p>where $\Gamma(P, Q)$ denotes the set of all couplings of $P$ and $Q$, i.e. joint distributions on $\mathcal{X}^{2}$ which have marginals $P$ and $Q$.
(iii) If $\mathcal{F}={f:|f|<em _mathcal_F="\mathcal{F">{\mathcal{H}} \leq 1}$ where $\mathcal{H}$ is a reproducing kernel Hilbert space with kernel $k$, then $d</em>$ is the maximum mean discrepancy:}</p>
<p>$$
d_{\mathcal{F}}(P, Q)=\operatorname{MMD}(P, Q):=\sqrt{\mathbb{E}\left[k\left(X, X^{\prime}\right)\right]-2 \mathbb{E}[k(X, Y)]+\mathbb{E}\left[k\left(Y, Y^{\prime}\right)\right]}
$$</p>
<p>where $X, X^{\prime} \sim P$ and $Y, Y^{\prime} \sim Q$.
In the context of Section 4.1, we have (at least) the following instantiations of Assumption 4.2:
(i) Assume the reward is bounded by $r_{\max }$. Then (since $\left|V_{M}^{\pi}\right|<em _max="\max">{\infty} \leq \frac{r</em>$ )}}{1-\gamma</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq \frac{r_{\max }}{1-\gamma} D_{\mathrm{TV}}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>This corresponds to $c=\frac{r_{\max }}{1-\gamma}$ and $\mathcal{F}={f:|f|<em M="M">{\infty} \leq 1}$.
(ii) Assume $V</em>$-Lipschitz. Then}^{\pi}$ is $L_{v</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq L_{v} W_{1}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>This corresponds to $c=L_{v}$ and $\mathcal{F}={f: f$ is 1-Lipschitz $}$.
(iii) Assume $\left|V_{M}^{\pi}\right|_{\mathcal{H}} \leq \nu$. Then</p>
<p>$$
\left|G_{M}^{\pi}(s, a)\right| \leq \nu \operatorname{MMD}(\widehat{T}(s, a), T(s, a))
$$</p>
<p>This corresponds to $c=\nu$ and $\mathcal{F}={f:|f|_{\mathcal{H}} \leq 1}$.</p>
<h2>B Proofs</h2>
<p>We provide a proof for Lemma 4.1 for completeness. The proof is essentially the same as that for [44, Lemma 4.3].</p>
<p>Proof. Let $W_{j}$ be the expected return when executing $\pi$ on $\widehat{T}$ for the first $j$ steps, then switching to $T$ for the remainder. That is,</p>
<p>$$
W_{j}=\underset{\substack{t&lt;j: s_{t+1} \sim \widehat{T}\left(s_{t}, a_{t}\right) \ t \geq j: s_{t+1} \sim T\left(s_{t}, a_{t}\right)}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}, a_{t}\right)\right]
$$</p>
<p>Note that $W_{0}=\eta_{M}(\pi)$ and $W_{\infty}=\eta_{\widetilde{M}}(\pi)$, so</p>
<p>$$
\eta_{\widetilde{M}}(\pi)-\eta_{M}(\pi)=\sum_{j=0}^{\infty}\left(W_{j+1}-W_{j}\right)
$$</p>
<p>Write</p>
<p>$$
\begin{aligned}
W_{j} &amp; =R_{j}+\underset{s_{j}, a_{j} \sim \pi, \widehat{T}}{\mathbb{E}}\left[\underset{s_{j+1} \sim \widehat{T}\left(s_{t}, a_{t}\right)}{\mathbb{E}}\left[\gamma^{j+1} V_{M}^{\pi}\left(s_{j+1}\right)\right]\right] \
W_{j+1} &amp; =R_{j}+\underset{s_{j}, a_{j} \sim \pi, \widehat{T}}{\mathbb{E}}\left[\underset{s_{j+1} \sim \widehat{T}\left(s_{t}, a_{t}\right)}{\mathbb{E}}\left[\gamma^{j+1} V_{M}^{\pi}\left(s_{j+1}\right)\right]\right]
\end{aligned}
$$</p>
<p>where $R_{j}$ is the expected return of the first $j$ time steps, which are taken with respect to $\widehat{T}$. Then</p>
<p>$$
\begin{aligned}
W_{j+1}-W_{j} &amp; =\gamma^{j+1} \underset{s_{j}, a_{j} \sim \pi, \widehat{T}}{\mathbb{E}}\left[\underset{s^{\prime} \sim \widehat{T}\left(s_{j}, a_{j}\right)}{\mathbb{E}}\left[V_{M}^{\pi}\left(s^{\prime}\right)\right]-\underset{s^{\prime} \sim \widehat{T}\left(s_{j}, a_{j}\right)}{\mathbb{E}}\left[V_{M}^{\pi}\left(s^{\prime}\right)\right]\right] \
&amp; =\gamma^{j+1} \underset{s_{j}, a_{j} \sim \pi, \widehat{T}}{\mathbb{E}}\left[G_{\widetilde{M}}^{\pi}\left(s_{j}, a_{j}\right)\right]
\end{aligned}
$$</p>
<p>Thus</p>
<p>$$
\begin{aligned}
\eta_{\widetilde{M}}(\pi)-\eta_{M}(\pi) &amp; =\sum_{j=0}^{\infty}\left(W_{j+1}-W_{j}\right) \
&amp; =\sum_{j=0}^{\infty} \gamma^{j+1} \underset{s_{j}, a_{j} \sim \pi, \widehat{T}}{\mathbb{E}}\left[G_{\widetilde{M}}^{\pi}\left(s_{j}, a_{j}\right)\right] \
&amp; =\gamma_{\left(s, a^{\prime}\right) \sim \rho_{\widehat{T}}^{\pi}}\left[G_{\widetilde{M}}^{\pi}(s, a)\right]
\end{aligned}
$$</p>
<p>as claimed.
Now we prove Theorem 4.2.
Proof. We first note that a two-sided bound follows from Lemma 4.1:</p>
<p>$$
\left|\eta_{\widetilde{M}}(\pi)-\eta_{M}(\pi)\right| \leq \gamma_{\widetilde{M}}(\pi) \quad\left|G_{\widetilde{M}}^{\pi}(s, a)\right| \leq \lambda_{\widetilde{M}}(\pi) \quad \lambda \epsilon_{u}(\pi)
$$</p>
<p>Then we have, for any policy $\pi$,</p>
<p>$$
\begin{aligned}
\eta_{M}(\hat{\pi}) &amp; \geq \eta_{\widetilde{M}}(\hat{\pi}) \
&amp; \geq \eta_{\widetilde{M}}(\pi) \
&amp; =\eta_{\widetilde{M}}(\pi)-\lambda \epsilon_{u}(\pi) \
&amp; \left.\geq \eta_{M}(\pi)-2 \lambda \epsilon_{u}(\pi)\right)
\end{aligned}
$$</p>
<h1>C MOPO Practical Algorithm Outline</h1>
<p>We outline the practical MOPO algorithm in Algorithm 2.</p>
<h2>D Ablation Study</h2>
<p>To answer question (3), we conduct a thorough ablation study on MOPO. The main goal of the ablation study is to understand how the choice of reward penalty affects performance. We denote no ens. as a method without model ensembles, ens. pen. as a method that uses model ensemble disagreement as the reward penalty, no pen. as a method without reward penalty, and true pen. as</p>
<p>Algorithm 2 MOPO instantiation with regularized probabilistic dynamics and ensemble uncertainty
Require: reward penalty coefficient $\lambda$ rollout horizon $h$, rollout batch size $b$.
1: Train on batch data $\mathcal{D}<em i="1">{\text {env }}$ an ensemble of $N$ probabilistic dynamics $\left{\widehat{T}^{i}\left(s^{\prime}, r \mid s, a\right)=\right.$ $\left.\mathcal{N}\left(\mu^{i}(s, a), \Sigma^{i}(s, a)\right)\right}</em>$.
2: Initialize policy $\pi$ and empty replay buffer $\mathcal{D}}^{N<em 1="1">{\text {model }} \leftarrow \varnothing$.
3: for epoch $1,2, \ldots$ do $\triangleright$ This for-loop is essentially one outer iteration of MBPO
4: for $1,2, \ldots, b$ (in parallel) do
5: Sample state $s</em>}$ from $\mathcal{D<em j="j">{\text {env }}$ for the initialization of the rollout.
6: for $j=1,2, \ldots, h$ do
7: Sample an action $a</em>\right)$.
8: Randomly pick dynamics $\widehat{T}$ from $\left{\widehat{T}^{i}\right}} \sim \pi\left(s_{j<em j_1="j+1">{i=1}^{N}$ and sample $s</em>\right)$.
9: Compute $\hat{r}}, r_{j} \sim \widehat{T}\left(s_{j}, a_{j<em j="j">{j}=r</em>-\lambda \max <em j="j">{i=1}^{N}\left|\Sigma^{i}\left(s</em>\right)\right|}, a_{j<em j="j">{\mathrm{F}}$.
10: Add sample $\left(s</em>}, a_{j}, \hat{r<em j_1="j+1">{j}, s</em>}\right)$ to $\mathcal{D<em _env="{env" _text="\text">{\text {model }}$.
11: Drawing samples from $\mathcal{D}</em>}} \cup \mathcal{D<em 1="1">{\text {model }}$, use SAC to update $\pi$.
a method using the true model prediction error $|\widehat{T}(s, a)-T(s, a)|$ as the reward penalty. Note that we include true pen. to indicate the upper bound of our approach. Also, note that no ens. measures disagreement among the ensemble: precisely, if the models' mean predictions are denoted $\mu</em>$ and then take $\max }, \ldots, \mu_{N}$, we compute the average $\bar{\mu}=1 / N \sum_{i=1}^{N} \mu_{i<em i="i">{i}\left|\mu</em>\right|$ as the ensemble penalty.}-\bar{\mu</p>
<p>The results of our study are shown in Table 3. For different reward penalty types, reward penalties based on learned variance perform comparably to those based on ensemble disagreement in D4RL environments while outperforming those based on ensemble disagreement in out-of-distribution domains. Both reward penalties achieve significantly better performances than no reward penalty, indicating that it is imperative to consider model uncertainty in batch model-based RL. Methods that uses oracle uncertainty obtain slightly better performance than most of our methods. Note that MOPO even attains the best results on halfcheetah-jump. Such results suggest that our uncertainty quantification on states is empirically successful, since there is only a small gap. We believe future work on improving uncertainty estimation may be able to bridge this gap further. Note that we do not report the results of methods with oracle uncertainty on walker2d-mixed and ant-angle as we are not able to get the true model error from the simulator based on the pre-recorded dataset.</p>
<p>In general, we find that performance differences are much larger for halfcheetah-jump and ant-angle than the D4RL halfcheetah-mixed and walker2d-mixed datasets, likely because halfcheetah-jump and ant-angle requires greater generalization and hence places more demands on the accuracy of the model and uncertainty estimate.</p>
<p>Finally, we perform another ablation study on the choice of the reward penalty. We consider the $u^{\text {mean }}(s, a)=\frac{1}{N} \sum_{i=1}^{N}\left|\Sigma_{\phi}^{i}(s, a)\right|_{\mathrm{F}}$, the average standard deviation of the learned models in the ensemble, as the reward penalty instead of the max standard deviation as used in MOPO. We denote the variant of MOPO with the average learned standard deviation as MOPO, avg. var.. We compare MOPO to MOPO, avg. var. in the halfcheetah-jump domain. MOPO achieves $\mathbf{4 1 4 0 . 6} \pm 88$ average return while MOPO, avg. var. achieves $\mathbf{4 1 6 6 . 3} \pm 228.8$ where the results are averaged over 3 random seeds. The two methods did similarly, suggesting that using either mean variance or max variance would be a reasonable choice for penalizing uncertainty.</p>
<h1>E Empirical results on generalization capabilities</h1>
<p>We conduct experiments in ant-angle to show the limit of MOPO's generalization capabilties. As shown in Table 4, we show that MOPO generalizes to Ant running at a $45^{\circ}$ angle (achieving almost buffer max score), beyond the $30^{\circ}$ shown in the paper, while failing to generalize to a 60 and $90^{\circ}$ degree angle. This suggests that if the new task requires to explore states that are completely out of the data support, i.e. the buffer max and buffer mean both fairly bad, MOPO is unable to generalize.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: right;">halfcheetah-mixed</th>
<th style="text-align: right;">walker2d-mixed</th>
<th style="text-align: right;">halfcheetah-jump</th>
<th style="text-align: right;">ant-angle</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MOPO</td>
<td style="text-align: right;">$6405.8 \pm 35$</td>
<td style="text-align: right;">$1916.4 \pm 611$</td>
<td style="text-align: right;">$4016.6 \pm 144$</td>
<td style="text-align: right;">$2530.9 \pm 137$</td>
</tr>
<tr>
<td style="text-align: left;">MOPO, ens. pen.</td>
<td style="text-align: right;">$6448.7 \pm 115$</td>
<td style="text-align: right;">$1923.6 \pm 752$</td>
<td style="text-align: right;">$3577.3 \pm 461$</td>
<td style="text-align: right;">$2256.0 \pm 288$</td>
</tr>
<tr>
<td style="text-align: left;">MOPO, no pen.</td>
<td style="text-align: right;">$6409.1 \pm 429$</td>
<td style="text-align: right;">$1421.2 \pm 359$</td>
<td style="text-align: right;">$-980.8 \pm 5625$</td>
<td style="text-align: right;">$18.6 \pm 49$</td>
</tr>
<tr>
<td style="text-align: left;">MBPO</td>
<td style="text-align: right;">$5598.4 \pm 1285$</td>
<td style="text-align: right;">$1021.8 \pm 586$</td>
<td style="text-align: right;">$2971.4 \pm 1262$</td>
<td style="text-align: right;">$13.6 \pm 65$</td>
</tr>
<tr>
<td style="text-align: left;">MBPO, no ens.</td>
<td style="text-align: right;">$2247.2 \pm 581$</td>
<td style="text-align: right;">$500.3 \pm 34$</td>
<td style="text-align: right;">$-68.7 \pm 1936$</td>
<td style="text-align: right;">$-720.1 \pm 728$</td>
</tr>
<tr>
<td style="text-align: left;">MOPO, true pen.</td>
<td style="text-align: right;">$6984.0 \pm 148$</td>
<td style="text-align: right;">N/A</td>
<td style="text-align: right;">$3818.6 \pm 136$</td>
<td style="text-align: right;">N/A</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation study on two D4RL tasks halfcheetah-mixed and walker2d-mixed and two out-ofdistribution tasks halfcheetah-jump and ant-angle. We use average returns where the results of MOPO and its variants are averaged over 6 random seeds and MBPO results are averaged over 3 random seeds as in Table 2. We observe that different reward penalties can all lead to substantial improvement of the performance and reward penalty based on learned variance is a better choice than that based on ensemble disagreement in out-of-distribution cases. Methods that use oracle uncertainty as the reward penalty achieve marginally better performance than MOPO, implying that MOPO is effective at estimating the uncertainty.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Environment</th>
<th style="text-align: center;">Buffer Max</th>
<th style="text-align: center;">Buffer Mean</th>
<th style="text-align: center;">MOPO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ant-angle-45</td>
<td style="text-align: center;">3168.7</td>
<td style="text-align: center;">1105.5</td>
<td style="text-align: center;">$2571.3 \pm 598.1$</td>
</tr>
<tr>
<td style="text-align: left;">ant-angle-60</td>
<td style="text-align: center;">1953.7</td>
<td style="text-align: center;">846.7</td>
<td style="text-align: center;">$840.5 \pm 1103.7$</td>
</tr>
<tr>
<td style="text-align: left;">ant-angle-90</td>
<td style="text-align: center;">838.8</td>
<td style="text-align: center;">-901.6</td>
<td style="text-align: center;">$-503.2 \pm 803.4$</td>
</tr>
</tbody>
</table>
<p>Table 4: Limit of generalization on ant-angle.</p>
<h1>F Experiments on HIV domains</h1>
<p>Beyond continous control tasks in MuJoCo, we test MOPO on an HIV treatment simulator slightly modified from the one in the whynot package. The task simulates the sequential decision making in HIV treatment, which involves determining the amounts of two anti-HIV drugs to be administered to the patient in order to maximize the immune response and minimize the amount of virus. The agent observes both of those quantities as well as the (log) number of infected and uninfected T cells and macrophages.
We evaluated MOPO with the data generated from the first 200k steps of training an online SAC agent on this environment. We show results in Table 5, where MOPO outperforms BEAR and achieves almost the buffer max score.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Buffer Max</th>
<th style="text-align: center;">Buffer Mean</th>
<th style="text-align: center;">SAC (online)</th>
<th style="text-align: center;">BEAR</th>
<th style="text-align: center;">MOPO</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">15986.2</td>
<td style="text-align: center;">6747.2</td>
<td style="text-align: center;">$25716.3 \pm 254.3$</td>
<td style="text-align: center;">$11709.1 \pm 1292.1$</td>
<td style="text-align: center;">$\mathbf{1 3 4 8 4 . 6} \pm 3900.7$</td>
</tr>
</tbody>
</table>
<p>Table 5: HIV treatment results, averaged over 3 random seeds.</p>
<h2>G Experiment Details</h2>
<h2>G. 1 Details of out-of-distribution environments</h2>
<p>For halfcheetah-jump, the reward function that we use to train the behavioral policy is $r(s, a)=$ $\max \left{v_{x}, 3\right}-0.1 <em>|a|<em x="x">{2}^{2}$ where $v</em>, 3\right}-0.1 }$ denotes the velocity along the x-axis. After collecting the offline dataset, we relabel the reward function to $r(s, a)=\max \left{v_{x</em>|a|<em x="x">{2}^{2}+15 *(z-$ init z$)$ where $z$ denotes the z-position of the half-cheetah and init z denotes the initial z-position.
For ant-angle, the reward function that we use to train the behavioral policy is $r(s, a)=v</em>$ denote the velocity along the $x, y$-axis respectively.
For both out-of-distribution environments, instead of sampling actions from the learned policy during the model rollout (line 10 in Algorithm 2), we sample random actions from Unif $[-1,1]$, which achieves better performance empirically. One potential reason is that using random actions during model rollouts leads to better exploration of the OOD states.}-$ control cost. After collecting the offline dataset, we relabel the reward function to $r(s, a)=v_{x}$. $\cos \frac{\pi}{6}+v_{y} \cdot \sin \frac{\pi}{6}-$ control cost where $v_{x}, v_{y</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset type</th>
<th style="text-align: left;">Environment</th>
<th style="text-align: center;">MOPO $(h, \lambda)$</th>
<th style="text-align: center;">MBPO $h$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: center;">5,0.5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: center;">5,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">random</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: center;">1,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: center;">1,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: center;">5,5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">medium</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: center;">5,5</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: center;">5,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: center;">5,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">mixed</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: center;">1,1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">halfcheetah</td>
<td style="text-align: center;">5,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">hopper</td>
<td style="text-align: center;">5,1</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: left;">med-expert</td>
<td style="text-align: left;">walker2d</td>
<td style="text-align: center;">1,2</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameters used in the D4RL datasets.</p>
<h1>G. 2 Hyperparameters</h1>
<p>Here we list the hyperparameters used in the experiments.
For the D4RL datasets, the rollout length $h$ and penalty coefficient $\lambda$ are given in Table 6. We search over $(h, \lambda) \in{1,5}^{2}$ and report the best final performance, averaged over 3 seeds. The only exceptions are halfcheetah-random and walker2d-medium-expert, where other penalty coefficients were found to work better.</p>
<p>For the out-of-generalization tasks, we use rollout length 5 for halfcheetah-jump and 25 for ant-angle, and penalty coefficient 1 for halfcheetah-jump and 2 for ant-angle.</p>
<p>Across all domains, we train an ensemble of 7 models and pick the best 5 models based on their prediction error on a hold-out set of 1000 transitions in the offline dataset. Each of the model in the ensemble is parametrized as a 4-layer feedforward neural network with 200 hidden units and after the last hidden layer, the model outputs the mean and variance using a two-head architecture. Spectral normalization [45] is applied to all layers except the head that outputs the model variance.
For the SAC updates, we sample a batch of 256 transitions, $5 \%$ of them from $\mathcal{D}<em _model="{model" _text="\text">{\text {env }}$ and the rest of them from $\mathcal{D}</em>}}$. We also perform ablation studies on the percentage of the real data in a batch for MOPO. For simplicity, we use MBPO, which essentially MOPO without reward penalty, for this ablation study. We tried to train MBPO with data all sampled from $\mathcal{D<em _env="{env" _text="\text">{\text {model }}$ and no data from $\mathcal{D}</em>$ on all 12 settings in the D4RL benchmark. We find that the performances of both methods are not significantly distinct: no-real-data MBPO outperforms $5 \%$-real-data MBPO on 6 out of 12 tasks and lies within one SD of $5 \%$-real-data MBPO on 9 out of 12 tasks.}}$ and compare the performance to MBPO with $5 \%$ of data from $\mathcal{D}_{\text {env }</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ If the reward function is known, we do not have to estimate the reward. The theory in Sections 4.1 and 4.2 applies to the case where the reward function is known. To extend the theory to an unknown reward function, we can consider the reward as being concatenated onto the state, so that the admissible error estimator bounds the error on $\left(s^{\prime}, r\right)$, rather than just $s^{\prime}$.
${ }^{4}$ Designing prediction confidence intervals with strong theoretical guarantees is challenging and beyond the scope of this work, which focuses on using uncertainty quantification properly in offline RL.
${ }^{5}$ Code is released at https://github.com/tianhwyu927/mopo.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8163 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8163</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8163</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-261556862</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.02427v3.pdf" target="_blank">Cognitive Architectures for Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8163.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8163.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robot grounding agent that combines an LLM with a learned value function and a library of grounding skills to select and execute physical actions in a kitchen environment; uses the LLM to score skill usefulness and a learned value model to estimate grounded feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as I can, not as I say: Grounding language in robotic affordances.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based planner that ranks a fixed set of grounding skills for robotic control by combining LLM-scored usefulness with a learned affordance/value function; no long-term episodic or semantic memory is maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Kitchen robotic instruction following (kitchen grounding tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a user instruction (e.g., fetch a drink/snack), select and execute a sequence of robotic grounding skills (from a fixed skill library) in a physical kitchen environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied robotic grounding / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>procedural (code-based skills) and working memory (transient state)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Fixed procedural memory (predefined skill library) accessed by selecting skill identifiers; LLM used to rank usefulness while a learned value model estimates affordance/feasibility of skills.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Named grounding skills (code procedures), LLM context for current episode (working memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt-based selection where candidate skills are evaluated by LLM and scored by a learned value function (no semantic/episodic retrieval reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Described as using only procedural memory (skill library) and a learned value function; contrasted qualitatively with simpler single-step planners that do not combine affordance scoring, but no numeric with/without-memory ablation reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that combining LLM judgments of usefulness with a learned affordance/value model over a fixed skill library enables single-step planning for robotic grounding; procedural memory (skills) provides necessary grounding primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No semantic/episodic memory or learning actions in original formulation; limited to a fixed set of skills and requires a learned value model for grounding; updating procedural memory not emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8163.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8163.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent paradigm that interleaves chain-of-thought style reasoning with actions (tool/API calls or environment interactions) in a feedback loop, but typically lacks persistent long-term memory modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses LLM-generated internal reasoning traces interleaved with external grounding actions (tool calls, API queries) to solve tasks in digital environments; operates primarily with working memory (in-context chain-of-thought) and no long-term semantic/episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Digital environment tasks (e.g., API/Web, text games, QA requiring tools)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step tasks where the agent must combine internal reasoning with external tool use (web/API calls, navigation, text-game actions) to accomplish goals; reasoning guides which external actions to take and in what order.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive multi-step reasoning + tool use / web/API interaction</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory only (in-context reasoning), no persistent long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM context used as ephemeral working memory (chain-of-thought) across the decision cycle; no persistent retrieval or episodic/semantic store described.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Intermediate reasoning traces and immediate observations within prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation / in-context carrying of recent history; no long-term retrieval reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Presented as a baseline/simple agent that leverages reasoning+acting but lacks retrieval/learning; compared conceptually to agents that add memory (e.g., Voyager, Generative Agents) which extend capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interleaving reasoning and acting improves performance on tool-augmented problems compared to reasoning-only or action-only baselines, but absence of long-term memory limits adaptation and multi-episode learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No long-term writing/reading (episodic/semantic) means limited ability to accumulate knowledge across episodes; relies on prompt context length for working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8163.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8163.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-ended Minecraft agent that builds a procedural skill library (code-based skills) via LLM-guided proposal, execution, and learning, using dense retrieval to recall skills and expanding its procedural memory over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven agent in Minecraft that creates, refines, and stores code-based grounding procedures (skills) in procedural memory; uses reasoning to propose tasks and code, executes and refines code against environment feedback, and learns by adding successful skills to its library.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-ended Minecraft gameplay / skill acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Explore, craft, and achieve objectives in Minecraft by composing and writing code-like skills (procedures) which are stored in a skill library and reused for future tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied digital environment / open-ended exploration and skill learning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>procedural memory (code-based skill library); working memory; also retrieval of skills</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Dense retrieval over a procedural/skill library to recall existing skills; skills are code procedures stored as entries in procedural memory; learning action appends new skills to library after successful execution.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Code-based skills (textual code snippets with hierarchical structure), recent episode feedback in working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dense vector retrieval (embedding-based search) plus reasoning-based selection; uses retrieval to load candidate skills for composing solutions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Paper reports qualitative and experimental comparisons: Voyager outperforms baselines like ReAct and AutoGPT and ablations with procedural memory removed â€” showing better exploration, mastery of the Minecraft tech tree, and zero-shot generalization to unseen tasks when procedural memory and skill learning are enabled.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Procedural memory (an evolving skill library) combined with retrieval and LLM-guided code generation enables open-ended learning, improved exploration, and re-use of learned skills; writing new grounding procedures is crucial for long-horizon competence in complex digital worlds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Learning/updating procedural memory is powerful but risky and complex; the approach depends on reliable code generation and execution, and ablations indicate degradation when procedural memory is disabled; numeric metrics not provided in this survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8163.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8163.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents in a simulated sandbox that maintain episodic memory of past events, retrieve relevant past events using combined recency/importance/relevance scoring, reflect to form semantic knowledge, and use that content to plan and behave believably over long time horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated agents that store episodic memories as event traces, retrieve salient memories via recency/importance/embedding relevance, perform LLM-based reflections to distill semantic-level facts, and use retrieved content plus reasoning to plan daily activities and interact with others.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sandbox social simulation / interactive NPC behavior</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents must act believably in a simulated social world by remembering past events, reflecting to form higher-level beliefs/preferences, and using those to plan actions and dialogues over extended interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-agent social simulation / long-horizon interactive behavior</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory (event traces) and semantic memory (reflections derived from episodes); working memory for current plans</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Episodic memory stored as a list of events; retrieval uses combined signals: recency (rule-based), importance (reasoning-based scoring), and semantic relevance (embedding-based search); LLM reflection transforms recalled episodes into semantic knowledge written to semantic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Event descriptions (natural language episodic traces), distilled reflections/statements in semantic memory representing beliefs/preferences/knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Hybrid retrieval: recency heuristics, LLM-based importance scoring, and embedding-based semantic search to retrieve relevant episodes/reflections into working memory for reasoning and planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Generative Agents are contrasted qualitatively with memory-less agents (e.g., ReAct-style) to show richer, coherent long-term behavior when episodic and semantic memories plus reflection are used; no numeric ablation results are reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining episodic storage, retrieval with multiple scoring signals, and reflection into semantic memory enables agents to form and use long-term character and knowledge, yielding more coherent multi-day behavior and planning in social simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Memory management (what to store/retrieve) is nontrivial; retrieval strategies and importance scoring are design choices; evaluation is qualitative and metricized evaluation of memory benefits is limited in the surveyed work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8163.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8163.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree of Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberative reasoning framework that uses LLMs to propose and evaluate internal 'thoughts' organized in a search tree (BFS/DFS/MCTS-like) to solve complex multi-step reasoning tasks, operating without persistent long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of thoughts: Deliberate problem solving with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses LLMs to generate candidate internal reasoning states ('thoughts'), evaluates them with the LLM, and conducts explicit tree search to explore and backtrack across reasoning trajectories; relies on working memory (search tree) rather than long-term storage.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deliberative reasoning benchmarks (e.g., Game of 24, creative writing, crossword-like puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Problems requiring multi-step deliberative planning where intermediate reasoning states matter; agent must explore and select sequences of reasoning steps to reach a valid final solution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / planning / combinatorial search</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory/search tree only (no long-term episodic/semantic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Maintains an explicit search tree of generated thoughts (internal reasoning states) during planning; information is kept in working data structures across LLM calls but not appended to persistent long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Thought nodes (textual intermediate reasoning states) stored in an explicit tree structure</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Search over the tree structure (BFS/DFS/MCTS) guided by LLM-generated proposals and evaluations; no retrieval from external long-term stores</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared conceptually to single-pass chain-of-thought; ToT shows advantages for problems needing global exploration and local backtracking. Specific numeric comparisons exist in the original ToT work but are not reproduced as numbers in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit, deliberative propose-evaluate-select search using LLMs can overcome myopic autoregressive generation, enabling better performance on tasks requiring foresight and backtracking even without long-term persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Search is computationally expensive (many LLM calls); requires careful search budgeting and methods to evaluate candidate nodes reliably; does not by itself provide cross-episode learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8163.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8163.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent method that uses LLM-based reflection on failed episodes to generate semantic knowledge which is then stored and attached to future LLM contexts to improve performance on subsequent episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Applies LLM reflections to analyze failed trajectories, extracts actionable semantic assertions (e.g., 'there is no dishwasher in kitchen'), stores these in semantic memory, and conditions future reasoning on this memory; functions as a write-augmented memory/learning mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequential decision tasks / episodic tasks where environment facts affect future runs</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Environments where reflecting on failures and storing distilled semantic facts can prevent repeated mistakes and improve future episode performance (e.g., procedural tasks with environment-specific constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>episodic sequential decision making / learning from mistakes</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>semantic memory (derived from reflections) and episodic memory (trajectories used for reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-based reflection reads episodic traces (failed episodes), generates semantic inferences which are written to semantic memory and later attached to the LLM context as knowledge aiding subsequent decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Distilled natural-language semantic assertions (knowledge statements) and episodic trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Attach semantic reflections to prompts / include in-context when solving similar future episodes; retrieval is based on task relevance and used to augment prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Described qualitatively: storing reflections as semantic knowledge helps avoid repeated failures across episodes (example given: storing 'no dishwasher in kitchen'); numeric comparisons not reported in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Reflection + writing distilled semantic knowledge into memory enables language agents to leverage past failures to improve future behavior without parameter updates; demonstrates a practical, low-cost route for lifelong knowledge accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reliant on LLM reflection quality (risk of hallucinated or incorrect 'semantic' facts), storage management and retrieval relevance remain active design challenges; evaluation reported qualitatively in surveyed work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Architectures for Language Agents', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Voyager: An open-ended embodied agent with large language models. <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior. <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Do as I can, not as I say: Grounding language in robotic affordances. <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8163",
    "paper_id": "paper-261556862",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "SayCan",
            "name_full": "SayCan",
            "brief_description": "A robot grounding agent that combines an LLM with a learned value function and a library of grounding skills to select and execute physical actions in a kitchen environment; uses the LLM to score skill usefulness and a learned value model to estimate grounded feasibility.",
            "citation_title": "Do as I can, not as I say: Grounding language in robotic affordances.",
            "mention_or_use": "mention",
            "agent_name": "SayCan",
            "agent_description": "LLM-based planner that ranks a fixed set of grounding skills for robotic control by combining LLM-scored usefulness with a learned affordance/value function; no long-term episodic or semantic memory is maintained.",
            "model_name": null,
            "model_description": null,
            "task_name": "Kitchen robotic instruction following (kitchen grounding tasks)",
            "task_description": "Given a user instruction (e.g., fetch a drink/snack), select and execute a sequence of robotic grounding skills (from a fixed skill library) in a physical kitchen environment.",
            "task_type": "embodied robotic grounding / instruction following",
            "memory_used": true,
            "memory_type": "procedural (code-based skills) and working memory (transient state)",
            "memory_mechanism": "Fixed procedural memory (predefined skill library) accessed by selecting skill identifiers; LLM used to rank usefulness while a learned value model estimates affordance/feasibility of skills.",
            "memory_representation": "Named grounding skills (code procedures), LLM context for current episode (working memory)",
            "memory_retrieval_method": "Prompt-based selection where candidate skills are evaluated by LLM and scored by a learned value function (no semantic/episodic retrieval reported)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Described as using only procedural memory (skill library) and a learned value function; contrasted qualitatively with simpler single-step planners that do not combine affordance scoring, but no numeric with/without-memory ablation reported in this paper.",
            "key_findings": "Demonstrates that combining LLM judgments of usefulness with a learned affordance/value model over a fixed skill library enables single-step planning for robotic grounding; procedural memory (skills) provides necessary grounding primitives.",
            "limitations_or_challenges": "No semantic/episodic memory or learning actions in original formulation; limited to a fixed set of skills and requires a learned value model for grounding; updating procedural memory not emphasized.",
            "uuid": "e8163.0",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "A language-agent paradigm that interleaves chain-of-thought style reasoning with actions (tool/API calls or environment interactions) in a feedback loop, but typically lacks persistent long-term memory modules.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "Uses LLM-generated internal reasoning traces interleaved with external grounding actions (tool calls, API queries) to solve tasks in digital environments; operates primarily with working memory (in-context chain-of-thought) and no long-term semantic/episodic memory.",
            "model_name": null,
            "model_description": null,
            "task_name": "Digital environment tasks (e.g., API/Web, text games, QA requiring tools)",
            "task_description": "Multi-step tasks where the agent must combine internal reasoning with external tool use (web/API calls, navigation, text-game actions) to accomplish goals; reasoning guides which external actions to take and in what order.",
            "task_type": "interactive multi-step reasoning + tool use / web/API interaction",
            "memory_used": false,
            "memory_type": "working memory only (in-context reasoning), no persistent long-term memory",
            "memory_mechanism": "LLM context used as ephemeral working memory (chain-of-thought) across the decision cycle; no persistent retrieval or episodic/semantic store described.",
            "memory_representation": "Intermediate reasoning traces and immediate observations within prompt context",
            "memory_retrieval_method": "Prompt concatenation / in-context carrying of recent history; no long-term retrieval reported",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Presented as a baseline/simple agent that leverages reasoning+acting but lacks retrieval/learning; compared conceptually to agents that add memory (e.g., Voyager, Generative Agents) which extend capabilities.",
            "key_findings": "Interleaving reasoning and acting improves performance on tool-augmented problems compared to reasoning-only or action-only baselines, but absence of long-term memory limits adaptation and multi-episode learning.",
            "limitations_or_challenges": "No long-term writing/reading (episodic/semantic) means limited ability to accumulate knowledge across episodes; relies on prompt context length for working memory.",
            "uuid": "e8163.1",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager",
            "brief_description": "An open-ended Minecraft agent that builds a procedural skill library (code-based skills) via LLM-guided proposal, execution, and learning, using dense retrieval to recall skills and expanding its procedural memory over time.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models.",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "LLM-driven agent in Minecraft that creates, refines, and stores code-based grounding procedures (skills) in procedural memory; uses reasoning to propose tasks and code, executes and refines code against environment feedback, and learns by adding successful skills to its library.",
            "model_name": null,
            "model_description": null,
            "task_name": "Open-ended Minecraft gameplay / skill acquisition",
            "task_description": "Explore, craft, and achieve objectives in Minecraft by composing and writing code-like skills (procedures) which are stored in a skill library and reused for future tasks.",
            "task_type": "embodied digital environment / open-ended exploration and skill learning",
            "memory_used": true,
            "memory_type": "procedural memory (code-based skill library); working memory; also retrieval of skills",
            "memory_mechanism": "Dense retrieval over a procedural/skill library to recall existing skills; skills are code procedures stored as entries in procedural memory; learning action appends new skills to library after successful execution.",
            "memory_representation": "Code-based skills (textual code snippets with hierarchical structure), recent episode feedback in working memory",
            "memory_retrieval_method": "Dense vector retrieval (embedding-based search) plus reasoning-based selection; uses retrieval to load candidate skills for composing solutions",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Paper reports qualitative and experimental comparisons: Voyager outperforms baselines like ReAct and AutoGPT and ablations with procedural memory removed â€” showing better exploration, mastery of the Minecraft tech tree, and zero-shot generalization to unseen tasks when procedural memory and skill learning are enabled.",
            "key_findings": "Procedural memory (an evolving skill library) combined with retrieval and LLM-guided code generation enables open-ended learning, improved exploration, and re-use of learned skills; writing new grounding procedures is crucial for long-horizon competence in complex digital worlds.",
            "limitations_or_challenges": "Learning/updating procedural memory is powerful but risky and complex; the approach depends on reliable code generation and execution, and ablations indicate degradation when procedural memory is disabled; numeric metrics not provided in this survey summary.",
            "uuid": "e8163.2",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Generative Agents",
            "name_full": "Generative Agents",
            "brief_description": "Agents in a simulated sandbox that maintain episodic memory of past events, retrieve relevant past events using combined recency/importance/relevance scoring, reflect to form semantic knowledge, and use that content to plan and behave believably over long time horizons.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior.",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Simulated agents that store episodic memories as event traces, retrieve salient memories via recency/importance/embedding relevance, perform LLM-based reflections to distill semantic-level facts, and use retrieved content plus reasoning to plan daily activities and interact with others.",
            "model_name": null,
            "model_description": null,
            "task_name": "Sandbox social simulation / interactive NPC behavior",
            "task_description": "Agents must act believably in a simulated social world by remembering past events, reflecting to form higher-level beliefs/preferences, and using those to plan actions and dialogues over extended interactions.",
            "task_type": "multi-agent social simulation / long-horizon interactive behavior",
            "memory_used": true,
            "memory_type": "episodic memory (event traces) and semantic memory (reflections derived from episodes); working memory for current plans",
            "memory_mechanism": "Episodic memory stored as a list of events; retrieval uses combined signals: recency (rule-based), importance (reasoning-based scoring), and semantic relevance (embedding-based search); LLM reflection transforms recalled episodes into semantic knowledge written to semantic memory.",
            "memory_representation": "Event descriptions (natural language episodic traces), distilled reflections/statements in semantic memory representing beliefs/preferences/knowledge",
            "memory_retrieval_method": "Hybrid retrieval: recency heuristics, LLM-based importance scoring, and embedding-based semantic search to retrieve relevant episodes/reflections into working memory for reasoning and planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Generative Agents are contrasted qualitatively with memory-less agents (e.g., ReAct-style) to show richer, coherent long-term behavior when episodic and semantic memories plus reflection are used; no numeric ablation results are reported in this survey.",
            "key_findings": "Combining episodic storage, retrieval with multiple scoring signals, and reflection into semantic memory enables agents to form and use long-term character and knowledge, yielding more coherent multi-day behavior and planning in social simulations.",
            "limitations_or_challenges": "Memory management (what to store/retrieve) is nontrivial; retrieval strategies and importance scoring are design choices; evaluation is qualitative and metricized evaluation of memory benefits is limited in the surveyed work.",
            "uuid": "e8163.3",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Tree of Thoughts (ToT)",
            "name_full": "Tree of Thoughts",
            "brief_description": "A deliberative reasoning framework that uses LLMs to propose and evaluate internal 'thoughts' organized in a search tree (BFS/DFS/MCTS-like) to solve complex multi-step reasoning tasks, operating without persistent long-term memory.",
            "citation_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "mention_or_use": "mention",
            "agent_name": "Tree of Thoughts",
            "agent_description": "Uses LLMs to generate candidate internal reasoning states ('thoughts'), evaluates them with the LLM, and conducts explicit tree search to explore and backtrack across reasoning trajectories; relies on working memory (search tree) rather than long-term storage.",
            "model_name": null,
            "model_description": null,
            "task_name": "Deliberative reasoning benchmarks (e.g., Game of 24, creative writing, crossword-like puzzles)",
            "task_description": "Problems requiring multi-step deliberative planning where intermediate reasoning states matter; agent must explore and select sequences of reasoning steps to reach a valid final solution.",
            "task_type": "multi-step reasoning / planning / combinatorial search",
            "memory_used": false,
            "memory_type": "working memory/search tree only (no long-term episodic/semantic memory)",
            "memory_mechanism": "Maintains an explicit search tree of generated thoughts (internal reasoning states) during planning; information is kept in working data structures across LLM calls but not appended to persistent long-term memory.",
            "memory_representation": "Thought nodes (textual intermediate reasoning states) stored in an explicit tree structure",
            "memory_retrieval_method": "Search over the tree structure (BFS/DFS/MCTS) guided by LLM-generated proposals and evaluations; no retrieval from external long-term stores",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared conceptually to single-pass chain-of-thought; ToT shows advantages for problems needing global exploration and local backtracking. Specific numeric comparisons exist in the original ToT work but are not reproduced as numbers in this survey.",
            "key_findings": "Explicit, deliberative propose-evaluate-select search using LLMs can overcome myopic autoregressive generation, enabling better performance on tasks requiring foresight and backtracking even without long-term persistent memory.",
            "limitations_or_challenges": "Search is computationally expensive (many LLM calls); requires careful search budgeting and methods to evaluate candidate nodes reliably; does not by itself provide cross-episode learning.",
            "uuid": "e8163.4",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion",
            "brief_description": "A language-agent method that uses LLM-based reflection on failed episodes to generate semantic knowledge which is then stored and attached to future LLM contexts to improve performance on subsequent episodes.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Applies LLM reflections to analyze failed trajectories, extracts actionable semantic assertions (e.g., 'there is no dishwasher in kitchen'), stores these in semantic memory, and conditions future reasoning on this memory; functions as a write-augmented memory/learning mechanism.",
            "model_name": null,
            "model_description": null,
            "task_name": "Sequential decision tasks / episodic tasks where environment facts affect future runs",
            "task_description": "Environments where reflecting on failures and storing distilled semantic facts can prevent repeated mistakes and improve future episode performance (e.g., procedural tasks with environment-specific constraints).",
            "task_type": "episodic sequential decision making / learning from mistakes",
            "memory_used": true,
            "memory_type": "semantic memory (derived from reflections) and episodic memory (trajectories used for reflection)",
            "memory_mechanism": "LLM-based reflection reads episodic traces (failed episodes), generates semantic inferences which are written to semantic memory and later attached to the LLM context as knowledge aiding subsequent decision making.",
            "memory_representation": "Distilled natural-language semantic assertions (knowledge statements) and episodic trajectories",
            "memory_retrieval_method": "Attach semantic reflections to prompts / include in-context when solving similar future episodes; retrieval is based on task relevance and used to augment prompt context",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Described qualitatively: storing reflections as semantic knowledge helps avoid repeated failures across episodes (example given: storing 'no dishwasher in kitchen'); numeric comparisons not reported in this survey.",
            "key_findings": "Reflection + writing distilled semantic knowledge into memory enables language agents to leverage past failures to improve future behavior without parameter updates; demonstrates a practical, low-cost route for lifelong knowledge accumulation.",
            "limitations_or_challenges": "Reliant on LLM reflection quality (risk of hallucinated or incorrect 'semantic' facts), storage management and retrieval relevance remain active design challenges; evaluation reported qualitatively in surveyed work.",
            "uuid": "e8163.5",
            "source_info": {
                "paper_title": "Cognitive Architectures for Language Agents",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models.",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior.",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning.",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Do as I can, not as I say: Grounding language in robotic affordances.",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models.",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.017721,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cognitive Architectures for Language Agents
15 Mar 2024</p>
<p>Theodore R Sumers sumers@princeton.edu 
Princeton University</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Princeton University</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Princeton University</p>
<p>Thomas L Griffiths 
Princeton University</p>
<p>Cognitive Architectures for Language Agents
15 Mar 202429AB88AE65BAC6302AF1762A5E69B85CarXiv:2309.02427v3[cs.AI]https:openreview. netforum? id= 1i6ZCvflQJ
Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents.While these agents have achieved substantial empirical success, we lack a framework to organize existing agents and plan future developments.In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA).CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decisionmaking process to choose actions.We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents.Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.* Equal contribution, order decided by coin flip.Each person reserves the right to list their name first.A CoALA-based repo of recent work on language agents: https://github.com/ysymyth/awesome-language-agents.</p>
<p>Introduction</p>
<p>Language agents (Weng, 2023;Wang et al., 2023b;Xi et al., 2023;Yao and Narasimhan, 2023) are an emerging class of artifical intelligence (AI) systems that use large language models (LLMs; Vaswani et al., 2017;Brown et al., 2020;Devlin et al., 2019;OpenAI, 2023a) to interact with the world.They apply the latest advances in LLMs to the existing field of agent design (Russell and Norvig, 2013).Intriguingly, this synthesis offers benefits for both fields.On one hand, LLMs possess limited knowledge and reasoning capabilities.Language agents mitigate these issues by connecting LLMs to internal memory and environments, grounding them to existing knowledge or external observations.On the other hand, traditional agents often require handcrafted rules (Wilkins, 2014) or reinforcement learning (Sutton and Barto, 2018), making generalization to new environments challenging (Lake et al., 2016).Language agents leverage commonsense priors present in LLMs to adapt to novel tasks, reducing the dependence on human annotation or trial-and-error learning.</p>
<p>While the earliest agents used LLMs to directly select or generate actions (Figure 1B; Ahn et al., 2022;Huang et al., 2022b), more recent agents additionally use them to reason (Yao et al., 2022b), plan (Hao et al., 2023;Yao et al., 2023), and manage long-term memory (Park et al., 2023;Wang et al., 2023a) to improve decision-making.This latest generation of cognitive language agents use remarkably sophisticated internal processes (Figure 1C).Today, however, individual works use custom terminology to describe these processes (such as 'tool use', 'grounding', 'actions'), making it difficult to compare different agents, understand how they are evolving over time, or build new agents with clean and consistent abstractions.</p>
<p>In order to establish a conceptual framework organizing these efforts, we draw parallels with two ideas from the history of computing and artificial intelligence (AI): production systems and cognitive architectures.Production systems generate a set of outcomes by iteratively applying rules (Newell and Simon, 1972).They originated as string manipulation systems -an analog of the problem that LLMs solve -and were subsequently adopted by the AI community to define systems capable of complex, hierarchically structured Figure 1: Different uses of large language models (LLMs).A: In natural language processing (NLP), an LLM takes text as input and outputs text.B: Language agents (Ahn et al., 2022;Huang et al., 2022c) place the LLM in a direct feedback loop with the external environment by transforming observations into text and using the LLM to choose actions.C: Cognitive language agents (Yao et al., 2022b;Shinn et al., 2023;Wang et al., 2023a) additionally use the LLM to manage the agent's internal state via processes such as learning and reasoning.In this work, we propose a blueprint to structure such agents.</p>
<p>behaviors (Newell et al., 1989).To do so, they were incorporated into cognitive architectures that specified control flow for selecting, applying, and even generating new productions (Laird et al., 1987;Laird, 2022;Kotseruba and Tsotsos, 2020).We suggest a meaningful analogy between production systems and LLMs: just as productions indicate possible ways to modify strings, LLMs define a distribution over changes or additions to text.This further suggests that controls from cognitive architectures used with production systems might be equally applicable to transform LLMs into language agents.Thus, we propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to characterize and design general purpose language agents.CoALA organizes agents along three key dimensions: their information storage (divided into working and long-term memories); their action space (divided into internal and external actions); and their decision-making procedure (which is structured as an interactive loop with planning and execution).Through these three concepts (memory, action, and decision-making), we show CoALA can neatly express a large body of existing agents and identify underexplored directions to develop new ones.Notably, while several recent papers propose conceptual architectures for general intelligence (LeCun, 2022;McClelland et al., 2019) or empirically survey language models and agents (Mialon et al., 2023;Weng, 2023;Wang et al., 2023b), this paper combines elements of both: we propose a theoretical framework and use it to organize diverse empirical work.This grounds our theory to existing practices and allows us to identify both short-term and long-term directions for future work.</p>
<p>The plan for the rest of the paper is as follows.We first introduce production systems and cognitive architectures (Section 2) and show how these recent developments in LLMs and language agents recapitulate these historical ideas (Section 3).Motivated by these parallels, Section 4 introduces the CoALA framework and uses it to survey existing language agents.Section 5 provides a deeper case study of several prominent agents.Section 6 suggests actionable steps to construct future language agents, while Section 7 highlights open questions in the broader arc of cognitive science and AI.Finally, Section 8 concludes.Readers interested in applied agent design may prioritize Sections 4-6.</p>
<p>Background: From Strings to Symbolic AGI</p>
<p>We first introduce production systems and cognitive architectures, providing a historical perspective on cognitive science and artificial intelligence: beginning with theories of logic and computation (Post, 1943), and ending with attempts to build symbolic artificial general intelligence (Newell et al., 1989).We then briefly introduce language models and language agents.Section 3 will connect these ideas, drawing parallels between production systems and language models.</p>
<p>Production systems for string manipulation</p>
<p>In the first half of the twentieth century, a significant line of intellectual work led to the reduction of mathematics (Whitehead and Russell, 1997) and computation (Church, 1932;Turing et al., 1936) to symbolic manipulation.Production systems are one such formalism.Intuitively, production systems consist of a set of rules, each specifying a precondition and an action.When the precondition is met, the action can be taken.The idea originates in efforts to characterize the limits of computation.Post (1943) proposed thinking about arbitrary logical systems in these terms, where formulas are expressed as strings and the conclusions they license are identified by production rules (as one string "produces" another).This formulation was subsequently shown to be equivalent to a simpler string rewriting system.In such a system, we specify rules of the form
X Y Z â†’ X W Z
indicating that the string XY Z can be rewritten to the string XW Z. String rewriting plays a significant role in the theory of formal languages, in the form of Chomsky's phrase structure grammar (Chomsky, 1956).</p>
<p>Control flow: From strings to algorithms</p>
<p>By itself, a production system simply characterizes the set of strings that can be generated from a starting point.However, they can be used to specify algorithms if we impose control flow to determine which productions are executed.For example, Markov algorithms are production systems with a priority ordering (Markov, 1954).The following algorithm implements division-with-remainder by converting a number written as strokes | into the form Q * R, where Q is the quotient of division by 5 and R is the remainder:
* ||||| â†’ | * * â€¢ âˆ’ â†’ * â†’ *
where the priority order runs from top to bottom, productions are applied to the first substring matching their preconditions when moving from left to right (including the empty substring, in the last production), and â€¢ âˆ’ â†’ indicates the algorithm halts after executing the rule.The first rule effectively "subtracts" five if possible; the second handles the termination condition when no more subtraction is possible; and the third handles the empty substring input case.For example, given the input 11, this would yield the sequence of productions
* ||||||||||| â†’ | * |||||| â†’ || * | â€¢ âˆ’ â†’ || * |
which is interpreted as 2 remainder 1. Simple productions can result in complex behavior -Markov algorithms can be shown to be Turing complete.</p>
<p>Cognitive architectures: From algorithms to agents</p>
<p>Production systems were popularized in the AI community by Allen Newell, who was looking for a formalism to capture human problem solving (Newell, 1967;Newell and Simon, 1972).Productions were generalized beyond string rewriting to logical operations: preconditions that could be checked against the agent's goals and world state, and actions that should be taken if the preconditions were satisfied.In their landmark book Human Problem Solving (Newell and Simon, 1972), Allen Newell and Herbert Simon gave the example of a  Following this work, production systems were adopted by the AI community.The resulting agents contained large production systems connected to external sensors, actuators, and knowledge bases -requiring correspondingly sophisticated control flow.AI researchers defined "cognitive architectures" that mimicked human cognition -explicitly instantiating processes such as perception, memory, and planning (Adams et al., 2012) to achieve flexible, rational, real-time behaviors (Sun, 2004;Newell, 1980;1992;Anderson and Lebiere, 2003).This led to applications from psychological modeling to robotics, with hundreds of architectures and thousands of publications (see Kotseruba and Tsotsos (2020) for a recent survey).</p>
<p>A canonical example is the Soar architecture (Fig. 2A).Soar stores productions in long-term memory and executes them based on how well their preconditions match working memory (Fig. 2B).These productions specify actions that modify the contents of working and long-term memory.We next provide a brief overview of Soar and refer readers to Laird (2022;2019) for deeper introductions.</p>
<p>Memory.Building on psychological theories, Soar uses several types of memory to track the agent's state (Atkinson and Shiffrin, 1968).Working memory (Baddeley and Hitch, 1974) reflects the agent's current circumstances: it stores the agent's recent perceptual input, goals, and results from intermediate, internal reasoning.Long term memory is divided into three distinct types.Procedural memory stores the production system itself: the set of rules that can be applied to working memory to determine the agent's behavior.Semantic memory stores facts about the world (Lindes and Laird, 2016), while episodic memory stores sequences of the agent's past behaviors (Nuxoll and Laird, 2007).</p>
<p>Grounding.Soar can be instantiated in simulations (Tambe et al., 1995;Jones et al., 1999) or real-world robotic systems (Laird et al., 2012).In embodied contexts, a variety of sensors stream perceptual input into working memory, where it is available for decision-making.Soar agents can also be equipped with actuators, allowing for physical actions and interactive learning via language (Mohan et al., 2012;Mohan and Laird, 2014;Kirk and Laird, 2014).Decision making.Soar implements a decision loop that evaluates productions and applies the one that matches best (Fig. 2B).Productions are stored in long-term procedural memory.During each decision cycle, their preconditions are checked against the agent's working memory.In the proposal and evaluation phase, a set of productions is used to generate and rank a candidate set of possible actions.* The best action is then chosen.â€  Another set of productions is then used to implement the action -for example, modifying the contents of working memory or issuing a motor command.</p>
<p>Learning.Soar supports multiple modes of learning.First, new information can be stored directly in long-term memory: facts can be written to semantic memory, while experiences can be written to episodic memory (Derbinsky et al., 2012).This information can later be retrieved back into working memory when needed for decision-making.Second, behaviors can be modified.Reinforcement learning (Sutton and Barto, 2018) can be used to up-weight productions that have yielded good outcomes, allowing the agent to learn from experience (Nason and Laird, 2005).Most remarkably, Soar is also capable of writing new productions into its procedural memory (Laird et al., 1986) -effectively updating its source code.</p>
<p>Cognitive architectures were used broadly across psychology and computer science, with applications including robotics (Laird et al., 2012), military simulations (Jones et al., 1999;Tambe et al., 1995), and intelligent tutoring (Koedinger et al., 1997).Yet they have become less popular in the AI community over the last few decades.This decrease in popularity reflects two of the challenges involved in such systems: they are limited to domains that can be described by logical predicates and require many pre-specified rules to function.</p>
<p>Intriguingly, LLMs appear well-posed to meet these challenges.First, they operate over arbitrary text, making them more flexible than logic-based systems.Second, rather than requiring the user to specify productions, they learn a distribution over productions via pre-training on an internet corpus.Recognizing this, researchers have begun to use LLMs within cognitive architectures, leveraging their implicit world knowledge (Wray et al., 2021) to augment traditional symbolic approaches (Kirk et al., 2023;Romero et al., 2023).Here, we instead import principles from cognitive architecture to guide the design of LLM-based agents.</p>
<p>Language models and agents</p>
<p>Language modeling is a decades-old endeavor in the NLP and AI communities, aiming to develop systems that can generate text given some context (Jurafsky, 2000).Formally, language models learn a distribution P (w i |w &lt;i ), where each w is an individual token (word).This model can then generate text by sampling from the distribution, one token at a time.At its core, a language model is a probabilistic input-output system, since there are inherently several ways to continue a text (e.g., "I went to the" â†’ "market" | "beach" | ...).While earlier attempts at modeling language (e.g., n-grams) faced challenges in generalization and scaling, there has been a recent resurgence of the area due to the rise of Transformer-based (Vaswani et al., 2017) LLMs with a large number (billions) of parameters (e.g., GPT-4;OpenAI, 2023a) and smart tokenization schemes.Modern LLMs are trained on enormous amounts of data, which helps them accumulate knowledge from a large number of input-output combinations and successfully generate human-like text (Andreas, 2022).</p>
<p>Unexpectedly, training these models on internet-scale text also made them useful for many tasks beyond generating text, such as writing code (Li et al., 2022b;RoziÃ¨re et al., 2023;Li et al., 2023c), modeling proteins (Meier et al., 2021), and acting in interactive environments (Yao et al., 2022b;Nakano et al., 2021).The latter has led to the rise of "language agents" -systems that use LLMs as a core computation unit to reason, plan, and act -with applications in areas such as robotics (Ahn et al., 2022), manufacturing (Xia et al., 2023), web manipulation (Yao et al., 2022a;Deng et al., 2023), puzzle solving (Yao et al., 2023;Hao et al., 2023) and interactive code generation (Yang et al., 2023).The combination of language understanding Published in Transactions on Machine Learning Research (02/2024) and decision-making capabilities is an exciting and emerging direction that promises to bring these agents closer to human-like intelligence.</p>
<p>Connections between Language Models and Production Systems</p>
<p>Based on their common origins in processing strings, there is a natural analogy between production systems and language models.We develop this analogy, then show that prompting methods recapitulate the algorithms and agents based on production systems.The correspondence between production systems and language models motivates our use of cognitive architectures to build language agents, which we introduce in Section 4.</p>
<p>Language models as probabilistic production systems</p>
<p>In their original instantiation, production systems specified the set of strings that could be generated from a starting point, breaking this process down into a series of string rewriting operations.Language models also define a possible set of expansions or modifications of a string -the prompt provided to the model.â€¡ For example, we can formulate the problem of completing a piece of text as a production.If X is the prompt and Y the continuation, then we can write this as the production X â†’ X Y .Â§ We might want to allow multiple possible continuations, in which case we have X â†’ X Y i for some set of Y i .LLMs assign a probability to each of these completions.Viewed from this perspective, the LLM defines a probability distribution over which productions to select when presented with input X, yielding a distribution P (Y i |X) over possible completions (Dohan et al., 2022).LLMs can thus be viewed as probabilistic production systems that sample a possible completion each time they are called, e.g., X âˆ¼âˆ¼â–¸ X Y .This probabilistic form offers both advantages and disadvantages compared to traditional production systems.The primary disadvantage of LLMs is their inherent opaqueness: while production systems are defined by discrete and human-legible rules, LLMs consist of billions of uninterpretable parameters.This opaquenesscoupled with inherent randomness from their probabilistic formulation -makes it challenging to analyze or control their behaviors (Romero et al., 2023;Valmeekam et al., 2022).Nonetheless, their scale and pre-training provide massive advantages over traditional production systems.LLMs pre-trained on large-scale internet data learn a remarkably effective prior over string completions, allowing them to solve a wide range of tasks out of the box (Huang et al., 2022b).</p>
<p>Prompt engineering as control flow</p>
<p>The weights of an LLM define a prioritization over output strings (completions), conditioned by the input string (the prompt).The resulting distribution can be interpreted as a task-specific prioritization of productionsin other words, a simple control flow.Tasks such as question answering can be formulated directly as an input string (the question), yielding conditional distributions over completions (possible answers).</p>
<p>Early work on few-shot learning (Brown et al., 2020) and prompt engineering (Wei et al., 2022b;Kojima et al., 2022;Xu et al., 2023c) found that the LLM could be further biased towards high-quality productions by pre-processing the input string.These simple manipulations -typically concatenating additional text to the input -can themselves be seen as productions, meaning that these methods define a sequence of productions (Table 1).Later work extended these approaches to dynamic, context-sensitive prompts: for example, selecting few-shot examples that are maximally relevant to the input (Liu et al., 2021) or populating a template with external observations from video (Zeng et al., 2022) or databases (Lewis et al., 2020).For a survey of such prompting techniques, see Liu et al. (2023d).</p>
<p>Subsequent work used the LLM itself as a pre-processing step, eliciting targeted reasoning to foreground a particular aspect of the problem (Bai et al., 2022;Jin et al., 2022;Ganguli et al., 2023;Madaan et al., 2023;Saunders et al., 2022;Kim et al., 2023;Kirk et al., 2023) or generate intermediate reasoning steps (Tafjord Prompting Method Production Sequence  Figure 3: From language models to language agents.A: Basic structure of an LLM call.Prompt construction selects a template and populates it with variables from working memory.After calling the LLM, the string output is parsed into an action space and executed.An LLM call may result in one or more actions -for example, returning an answer, calling a function, or issuing motor commands.B: Prompt chaining techniques such as Self-Critique (Wang et al., 2022b) or Selection-Inference (Creswell et al., 2023) use a pre-defined sequence of LLM calls to generate an output.C: Language agents such as Inner Monologue (Huang et al., 2022c) and ReAct (Yao et al., 2022b) instead use an interactive feedback loop with the external environment.Vision-language models (VLMs) can be used to translate perceptual data into text for the LLM to process. et al., 2021;Creswell et al., 2023;Yao et al., 2023) before returning an answer.Chaining multiple calls to an LLM (Wu et al., 2022a;b;Dohan et al., 2022) allows for increasingly complicated algorithms (Fig. 3).
Zero-shot Q âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q A Few-shot Q âˆ’â†’ Q 1 A 1 Q 2 A 2 Q âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q 1 A 1 Q 2 A 2 Q A Retrieval Augmented Generation Q Wiki âˆ’ âˆ’âˆ’ â†’ Q O âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q O A Socratic Models Q âˆ¼âˆ¼âˆ¼âˆ¼â–¸ VLM Q O âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q O A Self-Critique Q âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q A âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q A C âˆ¼âˆ¼âˆ¼âˆ¼â–¸ LLM Q A C A</p>
<p>Towards cognitive language agents</p>
<p>Language agents move beyond pre-defined prompt chains and instead place the LLM in a feedback loop with the external environment (Fig. 1B).These approaches first transform multimodal input into text and pass it to the LLM.The LLM's output is then parsed and used to determine an external action (Fig. 3C).Early agents interfaced the LLM directly with the external environment, using it to produce high-level instructions based on the agent's state (Ahn et al., 2022;Huang et al., 2022c;Dasgupta et al., 2022).Later work developed more sophisticated language agents that use the LLM to perform intermediate reasoning before selecting an action (Yao et al., 2022b).The most recent agents incorporate sophisticated learning strategies such as reflecting on episodic memory to generate new semantic inferences (Shinn et al., 2023)   program code to generate procedural knowledge (Wang et al., 2023a), using their previous experience to adapt their future behaviors.</p>
<p>These cognitive language agents employ nontrivial LLM-based reasoning and learning (Fig. 1C).Just as cognitive architectures were used to structure production systems' interactions with agents' internal state and external environments, we suggest that they can help design LLM-based cognitive agents.In the remainder of the paper, we use this perspective to organize existing approaches and highlight promising extensions.</p>
<p>Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework</p>
<p>We present Cognitive Architectures for Language Agents (CoALA) as a framework to organize existing language agents and guide the development of new ones.CoALA positions the LLM as the core component of a larger cognitive architecture (Figure 4).Under CoALA, a language agent stores information in memory modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):</p>
<p>â€¢ External actions interact with external environments (e.g., control a robot, communicate with a human, navigate a website) through grounding (Section 4.2).</p>
<p>â€¢ Internal actions interact with internal memories.Depending on which memory gets accessed and whether the access is read or write, internal actions can be further decomposed into three kinds: retrieval (read from long-term memory; Section 4.3), reasoning (update the short-term working memory with LLM; Section 4.4), and learning (write to long-term memory; Section 4.5).</p>
<p>Language agents choose actions via decision-making, which follows a repeated cycle (Section 4.6, Figure 4B).</p>
<p>In each cycle, the agent can use reasoning and retrieval actions to plan.This planning subprocess selects a grounding or learning action, which is executed to affect the outside world or the agent's long-term memory.</p>
<p>CoALA's decision cycle is analogous to a program's "main" procedure (a method without return values, as</p>
<p>Grounding Retrieval</p>
<p>Learning Reasoning Planning External Internal Figure 5: Agents' action spaces can be divided into internal memory accesses and external interactions with the world.Reasoning and retrieval actions are used to support planning.opposed to functions) that runs in loops continuously, accepting new perceptual input and calling various action procedures in response.</p>
<p>CoALA (Figure 4) is inspired by the decades of research in cognitive architectures (Section 2.3), leveraging key concepts such as memory, grounding, learning, and decision-making.Yet the incorporation of an LLM leads to the addition of "reasoning" actions, which can flexibly produce new knowledge and heuristics for various purposes -replacing hand-written rules in traditional cognitive architectures.It also makes text the de facto internal representation, streamlining agents' memory modules.Finally, recent advances in vision-language models (VLMs; Alayrac et al., 2022) can simplify grounding by providing a straightforward translation of perceptual data into text (Zeng et al., 2022).</p>
<p>The rest of this section details key concepts in CoALA: memory, actions (grounding, reasoning, retrieval, and learning), and decision-making.For each concept, we use existing language agents (or relevant NLP/RL methods) as examples -or note gaps in the literature for future directions.</p>
<p>Memory</p>
<p>Language models are stateless: they do not persist information across calls.In contrast, language agents may store and maintain information internally for multi-step interaction with the world.Under the CoALA framework, language agents explicitly organize information (mainly textural, but other modalities also allowed) into multiple memory modules, each containing a different form of information.These include short-term working memory and several long-term memories: episodic, semantic, and procedural.</p>
<p>Working memory.Working memory maintains active and readily available information as symbolic variables for the current decision cycle (Section 4.6).This includes perceptual inputs, active knowledge (generated by reasoning or retrieved from long-term memory), and other core information carried over from the previous decision cycle (e.g., agent's active goals).Previous methods encourage the LLM to generate intermediate reasoning (Wei et al., 2022b;Nye et al., 2021), using the LLM's own context as a form of working memory.CoALA's notion of working memory is more general: it is a data structure that persists across LLM calls.On each LLM call, the LLM input is synthesized from a subset of working memory (e.g., a prompt template and relevant variables).The LLM output is then parsed back into other variables (e.g., an action name and arguments) which are stored back in working memory and used to execute the corresponding action (Figure 3A).Besides the LLM, the working memory also interacts with long-term memories and grounding interfaces.It thus serves as the central hub connecting different components of a language agent.Episodic memory.Episodic memory stores experience from earlier decision cycles.This can consist of training input-output pairs (Rubin et al., 2021), history event flows (Weston et al., 2014;Park et al., 2023), game trajectories from previous episodes (Yao et al., 2020;Tuyls et al., 2022), or other representations of the agent's experiences.During the planning stage of a decision cycle, these episodes may be retrieved into working memory to support reasoning.An agent can also write new experiences from working to episodic memory as a form of learning (Section 4.5).</p>
<p>Semantic memory.</p>
<p>Semantic memory stores an agent's knowledge about the world and itself.Traditional NLP or RL approaches that leverage retrieval for reasoning or decision-making initialize semantic memory from an external database for knowledge support.For example, retrieval-augmented methods in NLP (Lewis et al., 2020;Borgeaud et al., 2022;Chen et al., 2017) can be viewed as retrieving from a semantic memory of unstructured text (e.g., Wikipedia).In RL, "reading to learn" approaches (Branavan et al., 2012;Narasimhan et al., 2018;Hanjie et al., 2021;Zhong et al., 2021) leverage game manuals and facts as a semantic memory to affect the policy.While these examples essentially employ a fixed, read-only semantic memory, language agents may also write new knowledge obtained from LLM reasoning into semantic memory as a form of learning (Section 4.5) to incrementally build up world knowledge from experience.</p>
<p>Procedural memory.Language agents contain two forms of procedural memory: implicit knowledge stored in the LLM weights, and explicit knowledge written in the agent's code.The agent's code can be further divided into two types: procedures that implement actions (reasoning, retrieval, grounding, and learning procedures), and procedures that implement decision-making itself (Section 4.6).During a decision cycle, the LLM can be accessed via reasoning actions, and various code-based procedures can be retrieved and executed.Unlike episodic or semantic memory that may be initially empty or even absent, procedural memory must be initialized by the designer with proper code to bootstrap the agent.Finally, while learning new actions by writing to procedural memory is possible (Section 4.5), it is significantly riskier than writing to episodic or semantic memory, as it can easily introduce bugs or allow an agent to subvert its designers' intentions.</p>
<p>Grounding actions</p>
<p>Grounding procedures execute external actions and process environmental feedback into working memory as text.This effectively simplifies the agent's interaction with the outside world as a "text game" with textual observations and actions.We categorize three kinds of external environments: Physical environments.Physical embodiment is the oldest instantiation envisioned for AI agents (Nilsson, 1984).It involves processing perceptual inputs (visual, audio, tactile) into textual observations (e.g., via pre-trained captioning models), and affecting the physical environments via robotic planners that take language-based commands.Recent advances in LLMs have led to numerous robotic projects (Ahn et al., 2022;Liang et al., 2023a;Singh et al., 2023;Palo et al., 2023;Ren et al., 2023) that leverage LLMs as a "brain" for robots to generate actions or plans in the physical world.For perceptual input, vision-language models are typically used to convert images to text (Alayrac et al., 2022;Sumers et al., 2023) providing additional context for the LLM (Driess et al., 2023;Huang et al., 2023;Brohan et al., 2022;2023).</p>
<p>Dialogue with humans or other agents.Classic linguistic interactions allow the agent to accept instructions (Winograd, 1972;Tellex et al., 2011;Chen and Mooney, 2011;Bisk et al., 2016) or learn from people (Nguyen et al., 2021;Sumers et al., 2022;2021;Wang et al., 2016).Agents capable of generating language may ask for help (Ren et al., 2023;Nguyen et al., 2022b;2019;Nguyen and DaumÃ© III, 2019) or clarification (Biyik and Palan, 2019;Sadigh et al., 2017;Padmakumar et al., 2022;Thomason et al., 2020;Narayan-Chen et al., 2019) -or entertain or emotionally help people (Zhang et al., 2020;Zhou et al., 2018;Pataranutaporn et al., 2021;Hasan et al., 2023;Ma et al., 2023).Recent work also investigates interaction among multiple language agents for social simulation (Park et al., 2023;Jinxin et al., 2023;Gao et al., 2023), debate (Chan et al., 2023;Liang et al., 2023b;Du et al., 2023), improved safety (Irving et al., 2018), or collabrative task solving (Qian et al., 2023;Wu et al., 2023;Hong et al., 2023a;Dong et al., 2023).</p>
<p>Digital environments.This includes interacting with games (Hausknecht et al., 2020;CÃ´tÃ© et al., 2019;Shridhar et al., 2020;Wang et al., 2022a;Liu et al., 2023e), APIs (Schick et al., 2023;Yao et al., 2022b;Parisi et al., 2022;Tang et al., 2023b), and websites (Shi et al., 2017;Nakano et al., 2021;Yao et al., 2022a;Zhou et al., 2023b;Gur et al., 2023;Deng et al., 2023) as well as general code execution (Yang et al., 2023;Le et al., 2022;Ni et al., 2023).Such digital grounding is cheaper and faster than physical or human interaction.It is thus a convenient testbed for language agents and has been studied with increasing intensity in recent years.In particular, for NLP tasks that require augmentation of external knowledge or computation, stateless digital APIs (e.g., search, calculator, translator) are often packaged as "tools" (Parisi et al., 2022;Schick et al., 2023;Xu et al., 2023a;Tang et al., 2023b;Qin et al., 2023), which can be viewed as special "single-use" digital environments.</p>
<p>Retrieval actions</p>
<p>In CoALA, a retrieval procedure (Li et al., 2022a;Gu et al., 2018) reads information from long-term memories into working memory.Depending on the information and memory type, it could be implemented in various ways, e.g., rule-based, sparse, or dense retrieval.For example, Voyager (Wang et al., 2023a) loads code-based skills from a skill library via dense retrieval to interact with the Minecraft world -effectively retrieving grounding procedures from a procedural memory.Generative Agents (Park et al., 2023) retrieves relevant events from episodic memory via a combination of recency (rule-based), importance (reasoning-based), and relevance (embedding-based) scores.DocPrompting (Zhou et al., 2022a) proposes to leverage library documents to assist code generation, which can be seen as retrieving knowledge from semantic memory.While retrieval plays a key role in human decision-making (Zhou et al., 2023a;Zhao et al., 2022), adaptive and context-specific recall remains understudied in language agents.In Section 6, we suggest a principled integration of decision-making and retrieval as an important future direction.</p>
<p>Reasoning actions</p>
<p>Reasoning allows language agents to process the contents of working memory to generate new information.Unlike retrieval (which reads from long-term memory into working memory), reasoning reads from and writes to working memory.This allows the agent to summarize and distill insights about the most recent observation (Yao et al., 2022b;Peng et al., 2023), the most recent trajectory (Shinn et al., 2023), or information retrieved from long-term memory (Park et al., 2023).Reasoning can be used to support learning (by writing the results into long-term memory) or decision-making (by using the results as additional context for subsequent LLM calls).</p>
<p>Learning actions</p>
<p>Learning occurs by writing information to long-term memory, which includes a spectrum of diverse procedures.</p>
<p>Updating episodic memory with experience.It is common practice for RL agents to store episodic trajectories to update a parametric policy (Blundell et al., 2016;Pritzel et al., 2017) or establish a nonparametric policy (Ecoffet et al., 2019;Tuyls et al., 2022).For language agents, added experiences in episodic memory may be retrieved later as examples and bases for reasoning or decision-making (Weston et al., 2014;Rubin et al., 2021;Park et al., 2023).</p>
<p>Updating semantic memory with knowledge.Recent work (Shinn et al., 2023;Park et al., 2023) has applied LLMs to reason about raw experiences and store the resulting inferences in semantic memory.For example, Reflexion (Shinn et al., 2023) uses an LLM to reflect on failed episodes and stores the results (e.g., "there is no dishwasher in kitchen") as semantic knowledge to be attached to LLM context for solving later episodes.Finally, work in robotics (Chen et al., 2023a) uses vision-language models to build a semantic map of the environment, which can later be queried to execute instructions.</p>
<p>Updating LLM parameters (procedural memory).</p>
<p>The LLM weights represent implicit procedural knowledge.These can be adjusted to an agent's domain by fine-tuning during the agent's lifetime.Such finetuning can be accomplished via supervised (Liu et al., 2023c;Zhang et al., 2023b) or imitation learning (Hussein et al., 2017), reinforcement learning (RL) from environment feedback (Sutton and Barto, 2018), human feedback (RLHF; Christiano et al., 2017;Ouyang et al., 2022;Nakano et al., 2021), or AI feedback (Bai et al., 2022;Liu et al., 2023f).Classic LLM self-improvement methods (Huang et al., 2022a;Zelikman et al., 2022) use an external measure such as consistency Wang et al. (2022b) to select generations to fine-tune on.In reinforcement learning settings, this can be extended to use environmental feedback instead: for example, XTX (Tuyls et al., 2022) periodically fine-tunes a small language model on high-scoring trajectories stored in episodic memory, which serves as a robust "exploitation" policy to reach exploration frontiers in the face of stochasity.Fine-tuning the agent's LLM is a costly form of learning; thus, present studies specify learning schedules.However, as training becomes more efficient -or if agents utilize smaller subtask-specific LLMs -it may be possible to allow language agents to autonomously determine when and how to fine-tune their LLMs.</p>
<p>Updating agent code (procedural memory).</p>
<p>CoALA allows agents to update their source code, thus modifying the implementation of various procedures.These can be broken down as follows:</p>
<p>â€¢ Updating reasoning (e.g., prompt templates; Gao et al., 2020;Zhou et al., 2022b).For example, APE (Zhou et al., 2022b) infers prompt instructions from input-output examples, then uses these instructions as part of the LLM prompt to assist task solving.Such a prompt update can be seen as a form of learning to reason.</p>
<p>â€¢ Updating grounding (e.g., code-based skills; Liang et al., 2023a;Ellis et al., 2021;Wang et al., 2023a).For example, Voyager (Wang et al., 2023a) maintains a curriculum library.Notably, current methods are limited to creating new code skills to interact with external environments.</p>
<p>â€¢ Updating retrieval.To our knowledge, these learning options are not studied in recent language agents.Retrieval is usually considered a basic action designed with some fixed implementation (e.g., BM25 or dense retrieval), but research in query/document expansion (Nogueira et al., 2019;Wang et al., 2023c;Tang et al., 2023a) or retrieval distillion (Izacard et al., 2021) may be helpful for language agents to learn better retrieval procedures.</p>
<p>â€¢ Updating learning or decision-making.Finally, it is theoretically possible for CoALA agents to learn new procedures for learning or decision-making, thus providing significant adaptability.</p>
<p>In general, however, updates to these procedures are risky both for the agent's functionality and alignment.At present, we are not aware of any language agents that implement this form of learning; we discuss such possibilities more in Section 6.</p>
<p>While RL agents usually fix one way of learning (e.g., Q-learning, PPO, or A3C) and learn by updating model parameters, language agents can select from a diversity of learning procedures.This allows them to learn rapidly by storing task-relevant language (cheaper and quicker than parameter updates), and leverage multiple forms of learning to compound their self-improvement (e.g., Generative Agents discussed in Section 5).</p>
<p>Finally, while our discussion has mostly focused on adding to memory, modifying and deleting (a case of "unlearning") are understudied in recent language agents.We address these areas more in Section 6.</p>
<p>Decision making</p>
<p>With various actions (grounding, learning, reasoning, retrieval) in the action space, how should a language agent choose which action to apply?This is handled by the decision-making procedure, which is effectively the top-level or "main" agent program.CoALA structures this top-level program into decision cycles (Figure 4B) which yield an external grounding action (Section 4.2) or internal learning action (Section 4.5).In each cycle, program code defines a sequence of reasoning and retrieval actions to propose and evaluate alternatives (planning stage), then executes the selected action (execution stage) -then the cycle loops again.</p>
<p>Planning stage.During planning, reasoning and retrieval can be flexibly applied to propose, evaluate, and select actions, and these sub-stages could interleave or iterate to build up multi-step simulations (Tamari et al., 2020) before taking an external action (Yao et al., 2023;Hao et al., 2023).It also enables agents to iteratively improve candidate solutions -for example, by using the LLM to simulate them, identifying defects, and proposing modifications that address those defects (Kirk et al., 2023;Shinn et al., 2023).</p>
<p>â€¢ Proposal.The proposal sub-stage generates one or more action candidates.The usual approach is to use reasoning (and optionally retrieval) to sample one (Huang et al., 2022c) or more (Chen et al., 2021;Wang et al., 2022b) external grounding actions from the LLM.For simple domains with limited actions, the proposal stage might simply include all actions (e.g., SayCan in Section 5).More sophisticated agents use if-else or while-if code structures (Wang et al., 2023a;Park et al., 2023); while agents deployed in well-defined domains may utilize structured simulators (Haslum et al., 2019) to generate plausible rollouts (Liu et al., 2023a;Dagan et al., 2023).</p>
<p>â€¢ Evaluation.If multiple actions are proposed, the evaluation sub-stage assigns a value to each.This may use heuristic rules, LLM (perplexity) values (Ahn et al., 2022), learned values (Yao et al., Long-term External Internal Decision Memory Â¶ Grounding Actions Making SayCan (Ahn et al., 2022) physical -evaluate ReAct (Yao et al., 2022b) digital reason propose Voyager (Wang et al., 2023a) procedural digital reason/retrieve/learn propose Generative Agents (Park et al., 2023) episodic/semantic digital/agent reason/retrieve/learn propose Tree of Thoughts (Yao et al., 2023) digital â€– reason propose, evaluate, select</p>
<p>Table 2: Some recent language agents cast into the CoALA framework.2020), LLM reasoning (Yao et al., 2023;Hao et al., 2023), or some combination.Particularly, LLM reasoning can help evaluate actions by internally simulating their grounding feedback from the external world (Hao et al., 2023;Yang et al., 2023).</p>
<p>â€¢ Selection.Given a set of actions and their values, the selection step either selects one to execute or rejects them and loops back to the proposal step.Depending on the form of action values, selection may occur via argmax, softmax, or an alternative such as majority vote (Wang et al., 2022b).</p>
<p>Execution.The selected action is applied by executing the relevant procedures from the agent's source code.Depending on the agent implementation, this might be an external grounding action (e.g., an API call; Section 4.2) or an internal learning action (e.g., a write to episodic memory; Section 4.5).An observation can be made from the environment, providing feedback from the agent's action, and the cycle loops again.</p>
<p>Empirically, many early language agents simply use LLMs to propose an action (Schick et al., 2023), a sequence of actions (Huang et al., 2022b), or evaluate a fixed set of actions (Ahn et al., 2022) without intermediate reasoning or retrieval.Followup work (Yao et al., 2022b;Shinn et al., 2023;Xu et al., 2023b;Lin et al., 2023;Wang et al., 2023a;Park et al., 2023) has exploited intermediate reasoning and retrieval to analyze the situation, make and maintain action plans, refine the previous action given the environmental feedback, and leveraged a more complex procedure to propose a single action.Most recently, research has started to investigate more complex decision-making employing iterative proposal and evaluation to consider multiple actions.These procedures are modeled after classical planning algorithms: for example, Tree of Thoughts (Yao et al., 2023) and RAP (Hao et al., 2023) use LLMs to implement BFS/DFS and Monte Carlo Tree Search (MCTS; Browne et al., 2012) respectively.LLMs are used to generate proposals (i.e., to simulate rollouts conditioned on an action) and evaluate them (i.e., to value the outcome of the proposed action).</p>
<p>Case Studies</p>
<p>With variations and ablations of the memory modules, action space, and decision-making procedures, CoALA can express a wide spectrum of language agents.Table 2 lists some popular recent methods across diverse domains -from Minecraft to robotics, from pure reasoning to social simulacra.CoALA helps characterize their internal mechanisms and reveal their similarities and differences in a simple and structured way.</p>
<p>SayCan (Ahn et al., 2022) grounds a language model to robotic interactions in a kitchen to satisfy user commands (e.g., "I just worked out, can you bring me a drink and a snack to recover?").Its long-term memory is procedural only (an LLM and a learned value function).The action space is external only -a fixed set of 551 grounding skills (e.g., "find the apple", "go to the table"), with no internal actions of reasoning, retrieval, or learning.During decision-making, SayCan evaluates each action using a combination of LLM and learned values, which balance a skill's usefulness and groundedness.SayCan therefore employs the LLM (in conjunction with the learned value function) as a single-step planner.</p>
<p>ReAct (Yao et al., 2022b) is a language agent grounded to various digital environments (e.g., Wikipedia API, text game, website).Like SayCan, it lacks semantic or episodic memory and therefore has no retrieval or learning actions.Its action space consists of (internal) reasoning and (external) grounding.Its decision cycle is fixed to use a single reasoning action to analyze the situation and (re)make action plans, then generates a Â¶ All agents contain some procedural memory (agent code and LLM weights), so here we only list writable procedural memory.</p>
<p>â€– Special digital grounding with the only external action being submitting a final answer.</p>
<p>grounding action without evaluation or selection stages.ReAct can be considered the simplest language agent that leverages both internal and external actions, and is the initial work that demonstrates their synergizing effects: reasoning helps guide acting, while acting provides environmental feedback to support reasoning.</p>
<p>Voyager (Wang et al., 2023a) is a language agent grounded to the Minecraft API.Unlike SayCan, which grounds to perception via the learned value function, Voyager's grounding is text-only.It has a long-term procedural memory that stores a library of code-based grounding procedures a.k.a.skills (e.g., "combatZombie", "craftStoneSword").This library is hierarchical: complex skills can use simpler skills as sub-procedures (e.g., "combatZombie" may call "craftStoneSword" if no sword is in inventory).Most impressively, its action space has all four kinds of actions: grounding, reasoning, retrieval, and learning (by adding new grounding procedures).During a decision cycle, Voyager first reasons to propose a new task objective if it is missing in the working memory, then reasons to propose a code-based grounding procedure to solve the task.In the next decision cycle, Voyager reasons over the environmental feedback to determine task completion.If successful, Voyager selects a learning action adding the grounding procedure to procedural memory; otherwise, it uses reasoning to refine the code and re-executes it.The importance of long-term memory and procedural learning is empirically verified by comparing to baselines like ReAct and AutoGPT and ablations without the procedural memory.Voyager is shown to better explore areas, master the tech tree, and zero-shot generalize to unseen tasks.</p>
<p>Generative Agents (Park et al., 2023) are language agents grounded to a sandbox game affording interaction with the environment and other agents.Its action space also has all four kinds of actions: grounding, reasoning, retrieval, and learning.Each agent has a long-term episodic memory that stores events in a list.These agents use retrieval and reasoning to generate reflections on their episodic memory (e.g., "I like to ski now.") which are then written to long-term semantic memory.During decision-making, it retrieves relevant reflections from semantic memory, then reasons to make a high-level plan of the day.While executing the plan, the agent receives a stream of grounding observations; it can reason over these to maintain or adjust the plan.</p>
<p>Tree of Thoughts (ToT) (Yao et al., 2023) can be seen as a special kind of language agent with only one external action: submitting a final solution to a reasoning problem (game of 24, creative writing, crosswords puzzle).It has no long-term memory, and only reasoning in its internal action space, but differs from all previous agents in its deliberate decision-making.During planning, ToT iteratively proposes, evaluates, and selects "thoughts" (reasoning actions) based on LLM reasoning, and maintains them via a tree search algorithm to enable global exploration as well as local backtrack and foresight.</p>
<p>Actionable Insights</p>
<p>Compared to some recent empirical surveys around language agents (Mialon et al., 2023;Weng, 2023;Wang et al., 2023b), CoALA offers a theoretical framework grounded in the well-established research of cognitive architectures.This leads to a unique and complementary set of actionable insights.</p>
<p>Modular agents: thinking beyond monoliths.Perhaps our most important suggestion is that agents should be structured and modular.Practically, just as standardized software is used across robotics platforms (Quigley, 2009;Macenski et al., 2022), a framework for language agents would consolidate technical investment and improve compatibility.</p>
<p>â€¢ In academic research, standardized terms allow conceptual comparisons across works (Table 2), and open-source implementations would further facilitate modular plug-and-play and re-use.For example, the theoretical framework of Markov Decision Processes (Puterman, 2014) provides a standardized set of concepts and terminology (e.g., state, action, reward, transition) for reinforcement learning (Sutton and Barto, 2018).Correspondingly, empirical frameworks like OpenAI Gym (Brockman et al., 2016) provided standardized abstractions (e.g., obs, reward, done, info = env.step(action))that facilitate empirical RL work.Thus, it would be timely and impactful to also implement useful abstractions (e.g., Memory, Action, Agent classes) for language agents, and cast simpler agents into such an empirical CoALA framework as examples for building more complex agents.</p>
<p>â€¢ In industry applications, maintaining a single company-wide "language agent library" would reduce technical debt (Sculley et al., 2014;Lwakatare et al., 2020) by facilitating testing and component re-use across individual agent deployments.It could also standardize the customer experience: rather than interacting with a hodgepodge of language agents developed by individual teams, end users would experience a context-specific instantiation of the same base agent.</p>
<p>â€¢ LLMs vs. code in agent design.CoALA agents possess two forms of procedural memory: agent code (deterministic rules) and LLM parameters (a large, stochastic production system).Agent code is interpretable and extensible, but often brittle in face of stochasticity and limited to address situations the designer anticipates.In contrast, LLM parameters are hard to interpret, but offer significant zero-shot flexibility in new contexts (Huang et al., 2022b).CoALA thus suggests using code sparingly to implement generic algorithms that complement LLM limitations, e.g., implementing tree search to mitigate myopia induced by autoregressive generation (Yao et al., 2023;Hao et al., 2023).</p>
<p>Agent design: thinking beyond simple reasoning.CoALA defines agents over three distinct concepts: (i) internal memory, (ii) a set of possible internal and external actions, and (iii) a decision making procedure over those actions.Using CoALA to develop an application-specific agent consists of specifying implementations for each of these components in turn.We assume that the agent's environment and external action space are given, and show how CoALA can be used to determine an appropriate high-level architecture.For example, we can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items based on their queries and purchasing history.In this case, the external actions would consist of dialogue or returning search results to the user.</p>
<p>â€¢ Determine what memory modules are necessary.In our retail assistant example, it would be helpful for the agent to have semantic memory containing the set of items for sale, as well as episodic memory about each customer's previous purchases and interactions.It will need procedural memory defining functions to query these datastores, as well as working memory to track the dialogue state.</p>
<p>â€¢ Define the agent's internal action space.This consists primarily of defining read and write access to each of the agent's memory modules.In our example, the agent should have read and write access to episodic memory (so it can store new interactions with customers), but read-only access to semantic and procedural memory (since it should not update the inventory or its own code).</p>
<p>â€¢ Define the decision-making procedure.This step specifies how reasoning and retrieval actions are taken in order to choose an external or learning action.In general, this requires a tradeoff between performance and generalization: more complex procedures can better fit to a particular problem (e.g., Voyager (Wang et al., 2023a) for Minecraft) while simpler ones are more domain-agnostic and generalizable (e.g., ReAct (Yao et al., 2022b)).For our retail assistant, we may want to encourage retrieval of episodic memory of interactions with a user to provide a prior over their search intent, as well as an explicit evaluation step reasoning about whether a particular set of search results will satisfy that intent.We can simplify the decision procedure by deferring learning to the end of the interaction (Shinn et al., 2023;Park et al., 2023), summarizing the episode prior to storing it in episodic memory.</p>
<p>Structured reasoning: thinking beyond prompt engineering.Early work on prompt engineering manipulated the LLM's input and output via low-level string operations.CoALA suggests a more structured reasoning procedure to update working memory variables.</p>
<p>â€¢ Prompting frameworks like LangChain (LangChain, 2022) and LlamaIndex (LlamaIndex, 2023) can be used to define higher-level sequences of reasoning steps, reducing the burden of reasoning per LLM call and the low-level prompt crafting efforts.Structural output parsing solutions such as Guidance (Guidance, 2023) and OpenAI function calling (OpenAI, 2023b) can help update working memory variables.Defining and building good working memory modules will also be an important direction of future research.Such modules may be especially important for industry solutions where LLM reasoning needs to seamlessly integrate with large-scale code infrastructure.</p>
<p>â€¢ Reasoning usecases in agents can inform and reshape LLM training in terms of the types (e.g., reasoning for self-evaluation, reflection, action generation, etc.) and formats (e.g., CoT (Wei et al., 2022b), ReAct (Yao et al., 2022b), Reflexion (Shinn et al., 2023)) of training instances.By default, existing LLMs are trained and optimized for NLP tasks, but agent applications have explored new modes of LLM reasoning (e.g., self-evaluation) that have proven broadly useful.LLMs trained or finetuned towards these capabilities will more likely be the backbones of future agents.</p>
<p>Long-term memory: thinking beyond retrieval augmentation.While traditional retrieval-augmented language models (Guu et al., 2020;Lewis et al., 2020;Borgeaud et al., 2022) only read from human-written corpora, memory-augmented language agents can both read and write self-generated content autonomously.This opens up numerous possibilities for efficient lifelong learning.</p>
<p>â€¢ Combining existing human knowledge with new experience and skills can help agents bootstrap to learn efficiently.For example, a code-writing agent could be endowed with semantic programming knowledge in the form of manuals or textbooks.It could then generate its own episodic knowledge from experience; reflect on these experiences to generate new semantic knowledge; and gradually create procedural knowledge in the form of a code library storing useful methods.</p>
<p>â€¢ Integrating retrieval and reasoning can help to better ground planning.Recent computational psychological models implicate an integrated process of memory recall and decision-making (Zhou et al., 2023a;Zhao et al., 2022) -suggesting that adaptive mechanisms interleaving memory search and forward simulation will allow agents to make the most of their knowledge.</p>
<p>Learning: thinking beyond in-context learning or finetuning.CoALA's definition of "learning" encompasses these methods, but extends further to storing new experience or knowledge, or writing new agent code (Section 4.5).Important future directions include:</p>
<p>â€¢ Meta-learning by modifying agent code would allow agents to learn more effectively.For example, learning better retrieval procedures could enable agents to make better use of their experience.Recent expansion-based techniques (Nogueira et al., 2019;Wang et al., 2023c;Tang et al., 2023a) could allow agents to reason about when certain knowledge would be useful, and store this as metadata to facilitate later recall.These forms of meta-learning would enable agents to go beyond human-written code, yet are understudied due to their difficulty and risk.</p>
<p>â€¢ New forms of learning (and unlearning) could include fine-tuning smaller models for specific reasoning sub-tasks (Zelikman et al., 2022;Huang et al., 2022a;Ahn et al., 2022), deleting unneeded memory items for "unlearning" (Nguyen et al., 2022c), and studying the interaction effects between multiple forms of learning (Tuyls et al., 2022;Park et al., 2023;Xie et al., 2023;Khattab et al., 2022).</p>
<p>Action space: thinking beyond external tools or actions.Although "action space" is a standard term in reinforcement learning, it has been used sparingly with language agents.CoALA argues for defining a clear and task-suitable action space with both internal (reasoning, retrieval, learning) and external (grounding) actions, which will help systematize and inform the agent design.</p>
<p>â€¢ Size of the action space.More capable agents (e.g., Voyager, Generative Agents) have larger action spaces -which in turn means they face a more complex decision-making problem.As a result, these agents rely on more customized or hand-crafted decision procedures.The tradeoff of the action space vs. decision-making complexities is a basic problem to be considered before agent development, and taking the minimal action space necessary to solve a given task might be preferred.</p>
<p>â€¢ Safety of the action space.Some parts of the action space are inherently riskier."Learning" actions (especially procedural deletion and modification) could cause internal harm, while "grounding" actions (e.g., "rm" in bash terminal, harmful speech in human dialog, holding a knife in physical environments) could cause external harm.Today, safety measures are typically task-specific heuristics (e.g., remove "os" operations in Python (Chen et al., 2021), filter keywords in dialog (Chowdhery et al., 2022;Driess et al., 2023), limit robots to controlled environments (Ahn et al., 2022)).However, as agents are grounded to more complex environments with richer internal mechanisms, it may be necessary to specify and ablate the agent's action space for worst-case scenario prediction and prevention (Yao and Narasimhan, 2023).</p>
<p>Decision making: thinking beyond action generation.We believe one of the most exciting future directions for language agents is decision-making: as detailed in Section 4.6, most works are still confined to proposing (or directly generating) a single action.Present agents have just scratched the surface of more deliberate, propose-evaluate-select decision-making procedures.</p>
<p>â€¢ Mixing language-based reasoning and code-based planning may offer the best of both worlds.</p>
<p>Existing approaches either plan directly in natural language (Huang et al., 2022c;Ahn et al., 2022) or use LLMs to translate from natural language to structured world models (Wong et al., 2023;Liu et al., 2023a;Zhang et al., 2023a;Li et al., 2023a;Guan et al., 2023;Silver et al., 2022;2023).Future work could integrate these: just as Soar incorporates a simulator for physical reasoning (Laird, 2022), agents may write and execute simulation code on the fly to evaluate the consequences of plans.See Section 7 for more discussion.</p>
<p>â€¢ Extending deliberative reasoning to real-world settings.Initial works have implemented classical planning and tree search (Yao et al., 2023;Hao et al., 2023;Liu et al., 2023a;Dagan et al., 2023), using toy tasks such as game of 24 or block building.Extending these schemes to more complicated tasks with grounding (Qin et al., 2023) and long-term memory is an exciting direction.</p>
<p>â€¢ Metareasoning to improve efficiency.LLM calls are both slow and computationally intensive.</p>
<p>Using LLMs for decision-making entails a balance between their computational cost and the utility of the resulting improved plan.Most LLM reasoning methods fix a search budget by specifying a depth of reasoning (Yao et al., 2023), but humans appear to adaptively allocate computation (Russek et al., 2022;Lieder and Griffiths, 2020;Callaway et al., 2022;Gershman et al., 2015).Future work should develop mechanisms to estimate the utility of planning (Laidlaw et al., 2023) and modify the decision procedure accordingly, either via amortization (fine-tuning the LLM based on the results of previous actions, e.g.Nguyen, 2023;Hamrick et al., 2019), routing among several decision sub-procedures (e.g., ReAct (Yao et al., 2022b) investigated backing off to CoT (Wei et al., 2022b) and vice versa), or updates to the decision-making procedure.</p>
<p>â€¢ Calibration and alignment.More complex decision-making is currently bottlenecked by issues such as over-confidence and miscalibration (Jiang et al., 2021;Braverman et al., 2020;Chen et al., 2022), misalignment with human values or bias (Liang et al., 2021;Feng et al., 2023), hallucinations in self-evaluation (Shinn et al., 2023), and lack of human-in-the-loop mechanisms in face of uncertainties (Nguyen et al., 2022a;Ren et al., 2023).Solving these issues will significantly improve LLMs' utilities as agent backbones.</p>
<p>Discussion</p>
<p>In addition to the practical insights presented above, CoALA raises a number of open conceptual questions.</p>
<p>We briefly highlight the most interesting as important directions for future research and debate.</p>
<p>LLMs vs VLMs: should reasoning be language-only or multimodal?Most language agents use language-only models for decision-making (Yao et al., 2022b;Wang et al., 2023a;Yao et al., 2023), employing a separate captioning model to convert environment observations to text when necessary (Ahn et al., 2022;Zeng et al., 2022).However, the latest generation of language models are multimodal, allowing interleaved image and text input (OpenAI, 2023a;Alayrac et al., 2022;Team et al., 2023;Li et al., 2023b).Language agents built on such multimodal models natively reason over both image and text input (Bavishi et al., 2023;Elsen et al., 2023;Liu et al., 2023b;Hong et al., 2023b;Driess et al., 2023), allowing them to ingest perceptual data and directly produce actions.This bypasses the lossy image-to-text conversion; however, it also tightly couples the reasoning and planning process to the model's input modalities.</p>
<p>At a high level, the two approaches can be seen as different tokenization schemes to convert non-linguistic modalities into the core reasoning model's language domain.The modular approach uses a separate image-totext model to convert perceptual data into language (Ahn et al., 2022;Zeng et al., 2022), while the integrated approach projects images directly into the language model's representation space (Bavishi et al., 2023;Elsen et al., 2023;Liu et al., 2023b).Integrated, multimodal reasoning may allow for more human-like behaviors: a VLM-based agent could "see" a webpage, whereas a LLM-based agent would more likely be given raw HTML.However, coupling the agent's perception and reasoning systems makes the agent more domain-specific and difficult to update.In either case, the basic architectural principles described by CoALA -internal memories, a structured action space, and generalized decision-making -can be used to guide agent design.</p>
<p>Internal vs. external: what is the boundary between an agent and its environment?While humans or robots are clearly distinct from their embodied environment, digital language agents have less clear boundaries.For example, is a Wikipedia database an internal semantic memory or an external digital environment (Yao et al., 2022b)?If an agent iteratively executes and improves code before submitting an answer (Shinn et al., 2023;Yang et al., 2023), is the code execution internal or external?If a method consists of proposal and evaluation prompts (Yao et al., 2023), should it be considered a single agent or two collaborating simpler agents (proposer and evaluator)?</p>
<p>We suggest the boundary question can be answered in terms of controllability and coupling.For example, Wikipedia is not controllable: it is an external environment that may be unexpectedly modified by other users.However, an offline version that only the agent may write to is controllable, and thus can be considered an internal memory.Similarly, code execution on a internal virtual environment should be considered an internal reasoning action, whereas code execution on an external machine (which may possess security vulnerabilities) should be considered an external grounding action.Lastly, if aspects of the agent -such as proposal and evaluation prompts -are designed for and dependent on each other, then they are tightly coupled and best conceptualized as components in an individual agent.In contrast, if the steps are independently useful, a multi-agent perspective may be more appropriate.While these dilemmas are primarily conceptual, such understanding can support agent design and help the field align on shared terminology.Practioners may also just choose their preferred framing, as long as it is consistent and useful for their own work.</p>
<p>Physical vs. digital: what differences beget attention?</p>
<p>While animals only live once in the physical world, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials.This means digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task solving (e.g., a million web agents try different web paths), which may result in decision-making procedures different from current ones inspired by human cognition (Griffiths, 2020).</p>
<p>Learning vs. acting: how should agents continuously and autonomously learn?In the CoALA framework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately chooses to commit information to long-term memory.This is in contrast to most agents, which simply fix a learning schedule and only use decison making for external actions.Biological agents, however, do not have this luxury: they must balance learning against external actions in their lifetime, choosing when and what to learn (Mattar and Daw, 2018).More flexible language agents (Wang et al., 2023a;Park et al., 2023) would follow a similar design and treat learning on par with external actions.Learning could be proposed as a possible action during regular decision-making, allowing the agent to "defer" it until the appropriate time.</p>
<p>GPT-4 vs GPT-N: how would agent design change with more powerful LLMs?Agent design is a moving target as new LLM capabilities emerge with scale (Wei et al., 2022a).For example, earlier language models such as GPT-2 (Radford et al., 2019) would not support LLM agents -indeed, work at that time needed to combine GPT-2 with reinforcement learning for action generation (Yao et al., 2020); GPT-3 (Brown et al., 2020) unlocked flexible few-shot and zero-shot reasoning for NLP tasks; while only GPT-4 (OpenAI, 2023a) starts to afford more reliable self-evaluation (Saunders et al., 2022;Shinn et al., 2023;Yao et al., 2023) and self-refinement (Madaan et al., 2023;Chen et al., 2023b).Will future LLMs further reduce the need for coded rules and extra-learned models?Will this necessitate changes to the CoALA framework?As a thought experiment, imagine GPT-N could "simulate" memory, grounding, learning, and decision-making in context: list all the possible actions, simulate and evaluate each one, and maintain its entire long-term memory explicitly in a very long context.Or even more boldly: perhaps GPT-N+1 succeeds at generating the next action by simulating these implicitly in neurons, without any intermediate reasoning in context.While these extreme cases seem unlikely in the immediate future, incremental improvements may alter the importance of different CoALA components.For example, a longer context window could reduce the importance of long-term memory, while more powerful reasoning for internal evaluation and simulation could allow longer-horizon planning.In general, LLMs are not subject to biological limitations (Griffiths, 2020), and their emergent properties have been difficult to predict.Nonetheless, CoALA -and cognitive science more generally -may still help organize tasks where language agents succeed or fail, and suggest code-based procedures to complement a given LLM on a given task.Even in the most extreme case, where GPT implements all of CoALA's mechanisms in neurons, it may be helpful to leverage CoALA as a conceptual guide to discover and interpret those implicit circuits.Of course, as discussed in Section 6, agent usecases will also help discover, define and shape LLM capabilities.Similar to how chips and computer architectures have co-evolved, language model and agent design should also develop a reciprocal path forward.</p>
<p>Conclusion</p>
<p>We proposed Cognitive Architectures for Language Agents (CoALA), a conceptual framework to describe and build language agents.Our framework draws inspiration from the rich history of symbolic artificial intelligence and cognitive science, connecting decades-old insights to frontier research on large language models.We believe this approach provides a path towards developing more general and more human-like artificial intelligence.</p>
<p>Figure 2 :
2
Figure 2: Cognitive architectures augment a production system with sensory groundings, long-term memory, and a decision procedure for selecting actions.A: The Soar architecture, reproduced with permission from Laird (2022).B: Soar's decision procedure uses productions to select and implement actions.These actions may be internal (such as modifying the agent's memory) or external (such as a motor command).</p>
<p>Figure 4 :
4
Figure 4: Cognitive architectures for language agents(CoALA).A: CoALA defines a set of interacting modules and processes.The decision procedure executes the agent's source code.This source code consists of procedures to interact with the LLM (prompt templates and parsers), internal memories (retrieval and learning), and the external environment (grounding).B: Temporally, the agent's decision procedure executes a decision cycle in a loop with the external environment.During each cycle, the agent uses retrieval and reasoning to plan by proposing and evaluating candidate learning or grounding actions.The best action is then selected and executed.An observation may be made, and the cycle begins again.</p>
<p>Table 1 :
1
Conceptual diagram illustrating how prompting methods manipulate the input string before generating completions.Q = question, A = answer, O = observation, C = critique, and âˆ¼âˆ¼âˆ¼â–¸ denotes sampling from a stochastic production.These pre-processing manipulations -which can employ other models such as vision-language models (VLMs), or even the LLM itself -can be seen as productions.Prompting methods thus define a sequence of productions.
PromptQuestionEnvironmentHumanconstructionAnswerCritiqueRe nementAnswerVLMActLLMInputSelf-CritiqueInner MonologueLLM callsString parsingContextQuestionEnvironmentChain / AgentExecutionSelectionInferenceAnswerReasonActSelection-InferenceReAct</p>
<p>or modifying their
Procedural MemorySemantic MemoryEpisodic MemoryObservationPlanningLLMAgent CodeProposalPromptParseRetrievalLearningRetrievalLearningRetrievalLearningEvaluationReasoningDecision ProcedureActionsObservationsWorking MemorySelectionExecutionDialoguePhysicalDigital
â€¡  In this work, we focus on autoregressive LLMs which are typically used for language agents. However, bidirectional LLMs such as BERT(Devlin et al., 2019) can be seen in a similar light: they define a distribution over in-filling productions.Â§ Alternatively, we can treat the prompt as input and take the output of the LLM as the next state, represented by the production X â†’ Y -a more literal form of rewriting.
AcknowledgementsWe thank Harrison Chase, Baian Chen, Khanh Nguyen, Ofir Press, Noah Shinn, Jens Tuyls for proofreading and valuable feedback, and members from the Princeton NLP Group and Princeton Computational Cognitive Science Lab for helpful discussions.Finally, we thank our anonymous reviewers for insightful comments and suggestions.SY and KN acknowledge support from an Oracle Collaborative Research award and the National Science Foundation under Grant No. 2239363.Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.SY is also supported by the Harold W. Dodds Fellowship from Princeton.TS is supported by the National Defense Science and Engineering (NDSEG) Graduate Fellowship Program.
Mapping the landscape of human-level artificial general intelligence. S Adams, I Arel, J Bach, R Coop, R Furlan, B Goertzel, J S Hall, A Samsonovich, M Scheutz, M Schlesinger, AI magazine. 3312012</p>
<p>Do as I can, not as I say: Grounding language in robotic affordances. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.016912022arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>The Newell test for a theory of cognition. J R Anderson, C Lebiere, Behavioral and Brain Sciences. 2652003</p>
<p>Language models as agent models. J Andreas, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Human memory: A proposed system and its control processes. R C Atkinson, R M Shiffrin, Psychology of Learning and Motivation. Elsevier19682</p>
<p>Working memory. A D Baddeley, G Hitch, Psychology of Learning and Motivation. Elsevier19748</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional AI: Harmlessness from AI feedback. 2022arXiv preprint</p>
<p>Introducing our multimodal models. R Bavishi, E Elsen, C Hawthorne, M Nye, A Odena, A Somani, S TaÅŸÄ±rlar, 2023</p>
<p>Towards a dataset for human computer communication via grounded language acquisition. Y Bisk, D Marcu, W Wong, Workshops at the Thirtieth AAAI Conference on Artificial Intelligence. 2016</p>
<p>Asking easy questions: A user-friendly approach to active reward learning. E Biyik, M Palan, Proceedings of the 3rd Conference on Robot Learning. the 3rd Conference on Robot Learning2019</p>
<p>C Blundell, B Uria, A Pritzel, Y Li, A Ruderman, J Z Leibo, J Rae, D Wierstra, D Hassabis, arXiv:1606.04460Model-free episodic control. 2016arXiv preprint</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International Conference on Machine Learning. 2022</p>
<p>Learning to win by reading manuals in a Monte-Carlo framework. S Branavan, D Silver, R Barzilay, Journal of Artificial Intelligence Research. 432012</p>
<p>Calibration, entropy rates, and memory in language models. M Braverman, X Chen, S Kakade, K Narasimhan, C Zhang, Y Zhang, International Conference on Machine Learning. 2020</p>
<p>G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, Openai gym. 2016</p>
<p>RT-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>RT-2: Vision-language-action models transfer web knowledge to robotic control. A Brohan, N Brown, J Carbajal, Y Chebotar, X Chen, K Choromanski, T Ding, D Driess, A Dubey, C Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>A survey of Monte Carlo tree search methods. C B Browne, E Powley, D Whitehouse, S M Lucas, P I Cowling, P Rohlfshagen, S Tavener, D Perez, S Samothrakis, S Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Rational use of cognitive resources in human planning. F Callaway, B Van Opheusden, S Gul, P Das, P M Krueger, T L Griffiths, F Lieder, Nature Human Behaviour. 682022</p>
<p>C.-M Chan, W Chen, Y Su, J Yu, W Xue, S Zhang, J Fu, Z Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Openvocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023a</p>
<p>Learning to interpret natural language navigation instructions from observations. D Chen, R Mooney, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201125</p>
<p>Reading Wikipedia to answer open-domain questions. D Chen, A Fisch, J Weston, A Bordes, arXiv:1704.000512017arXiv preprint</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374arXiv:2304.05128X. Chen, M. Lin, N. SchÃ¤rli, and D. Zhou2021. 2023barXiv preprintTeaching large language models to self-debug</p>
<p>A close look into the calibration of pre-trained language models. Y Chen, L Yuan, G Cui, Z Liu, H Ji, arXiv:2211.001512022arXiv preprint</p>
<p>Three models for the description of language. N Chomsky, IRE Transactions on information theory. 231956</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730Advances in neural information processing systems</p>
<p>A set of postulates for the foundation of logic. A Church, Annals of mathematics. 1932</p>
<p>Textworld: A learning environment for text-based games. M.-A CÃ´tÃ©, A KÃ¡dÃ¡r, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, M Hausknecht, L El Asri, M Adada, Computer Games: 7th Workshop, CGW 2018. Springer2019</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. A Creswell, M Shanahan, I Higgins, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Dynamic Planning with a LLM. G Dagan, F Keller, A Lascarides, arXiv:2308.063912023arXiv preprint</p>
<p>Collaborating with language models for embodied reasoning. I Dasgupta, C Kaeser-Chen, K Marino, A Ahuja, S Babayan, F Hill, R Fergus, X Deng, Y Gu, B Zheng, S Chen, S Stevens, B Wang, H Sun, Y Su, arXiv:2306.06070Second Workshop on Language and Reinforcement Learning. 2022. 2023arXiv preprintMind2Web: Towards a generalist agent for the web</p>
<p>A multi-domain evaluation of scaling in a general episodic memory. N Derbinsky, J Li, J Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201226</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT (1). 2019</p>
<p>. D Dohan, W Xu, A Lewkowycz, J Austin, D Bieber, R G Lopes, Y Wu, H Michalewski, R A Saurous, arXiv:2207.10342J. Sohl-Dickstein. 2022arXiv preprint</p>
<p>Self-collaboration code generation via chatgpt. Y Dong, X Jiang, Z Jin, G Li, arXiv:2304.075902023arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Go-explore: a new approach for hardexploration problems. A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, arXiv:1901.109952019arXiv preprint</p>
<p>Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. K Ellis, C Wong, M Nye, M SablÃ©-Meyer, L Morales, L Hewitt, L Cary, A Solar-Lezama, J B Tenenbaum, Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation. the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation2021</p>
<p>Releasing Persimmon-8B. E Elsen, A Odena, M Nye, S TaÅŸÄ±rlar, T Dao, C Hawthorne, D Moparthi, A Somani, 2023</p>
<p>From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair nlp models. S Feng, C Y Park, Y Liu, Y Tsvetkov, arXiv:2305.082832023arXiv preprint</p>
<p>The capacity for moral self-correction in large language models. D Ganguli, A Askell, N Schiefer, T Liao, K LukoÅ¡iÅ«tÄ—, A Chen, A Goldie, A Mirhoseini, C Olsson, D Hernandez, arXiv:2302.074592023arXiv preprint</p>
<p>C Gao, X Lan, Z Lu, J Mao, J Piao, H Wang, D Jin, Y Li, arXiv:2307.14984S3: Social-network simulation system with large language model-empowered agents. 2023arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. T Gao, A Fisch, D Chen, arXiv:2012.157232020arXiv preprint</p>
<p>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. S J Gershman, E J Horvitz, J B Tenenbaum, Science. 34962452015</p>
<p>Understanding human intelligence through human limitations. T L Griffiths, Trends in Cognitive Sciences. 24112020</p>
<p>Search engine guided neural machine translation. J Gu, Y Wang, K Cho, V O Li, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, arXiv:2305.14909Guidance. Guidance. 2023. 2023arXiv preprint</p>
<p>A real-world webagent with planning, long context understanding. I Gur, H Furuta, A Huang, M Safdari, Y Matsuo, D Eck, A Faust, arXiv:2307.128562023arXiv preprintand program synthesis</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, International conference on machine learning. 2020</p>
<p>Combining q-learning and search with amortized value estimates. J B Hamrick, V Bapst, A Sanchez-Gonzalez, T Pfaff, T Weber, L Buesing, P W Battaglia, International Conference on Learning Representations. 2019</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. A W Hanjie, V Zhong, K Narasimhan, International Conference on Machine Learning (ICML). 2021</p>
<p>S Hao, Y Gu, H Ma, J J Hong, Z Wang, D Z Wang, Z Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>M Hasan, C Ozel, S Potter, E Hoque, arXiv:2308.03022Sapien: Affective virtual agents powered by large language models. 2023arXiv preprint</p>
<p>An introduction to the planning domain definition language. P Haslum, N Lipovetzky, D Magazzeni, C Muise, R Brachman, F Rossi, P Stone, 2019Springer13</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A CÃ´tÃ©, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>S Hong, X Zheng, J Chen, Y Cheng, C Zhang, Z Wang, S K S Yau, Z Lin, L Zhou, C Ran, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023aarXiv preprint</p>
<p>W Hong, W Wang, Q Lv, J Xu, W Yu, J Ji, Y Wang, Z Wang, Y Dong, M Ding, arXiv:2312.08914A visual language model for gui agents. 2023barXiv preprint</p>
<p>Large language models can self-improve. J Huang, S S Gu, L Hou, Y Wu, X Wang, H Yu, J Han, arXiv:2210.116102022aarXiv preprint</p>
<p>S Huang, Z Jiang, H Dong, Y Qiao, P Gao, H Li, arXiv:2305.11176Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model. 2023arXiv preprint</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, International Conference on Machine Learning. 2022b</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022carXiv preprint</p>
<p>Imitation learning: A survey of learning methods. A Hussein, M M Gaber, E Elyan, C Jayne, ACM Computing Surveys (CSUR). 5022017</p>
<p>G Irving, P Christiano, D Amodei, arXiv:1805.00899AI safety via debate. 2018arXiv preprint</p>
<p>G Izacard, M Caron, L Hosseini, S Riedel, P Bojanowski, A Joulin, E Grave, arXiv:2112.09118Unsupervised dense information retrieval with contrastive learning. 2021arXiv preprint</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Z Jiang, J Araki, H Ding, G Neubig, Transactions of the Association for Computational Linguistics. 92021</p>
<p>When to make exceptions: Exploring language models as accounts of human moral judgment. Z Jin, S Levine, F G Adauto, O Kamal, M Sap, M Sachan, R Mihalcea, J B Tenenbaum, B SchÃ¶lkopf, Advances in Neural Information Processing Systems. A H Oh, A Agarwal, D Belgrave, K Cho, 2022</p>
<p>S Jinxin, Z Jiabao, W Yilei, W Xingjiao, L Jiawen, H Liang, arXiv:2308.12503Cgmi: Configurable general multi-agent interaction framework. 2023arXiv preprint</p>
<p>Automated intelligent pilots for combat flight simulation. R M Jones, J E Laird, P E Nielsen, K J Coulter, P Kenny, F V Koss, AI magazine. 2011999</p>
<p>Speech &amp; language processing. D Jurafsky, 2000Pearson Education India</p>
<p>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. O Khattab, K Santhanam, X L Li, D Hall, P Liang, C Potts, M Zaharia, arXiv:2212.140242022arXiv preprint</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Interactive task learning for simple games. R Kirk, J E Laird, Advances in Cognitive Systems. 352014</p>
<p>Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis. J R Kirk, W Robert, P Lindes, J E Laird, arXiv:2306.067702023arXiv preprint</p>
<p>Intelligent tutoring goes to school in the big city. K R Koedinger, J R Anderson, W H Hadley, M A Mark, International Journal of Artificial Intelligence in Education. 811997</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 202235</p>
<p>40 years of cognitive architectures: core cognitive abilities and practical applications. I Kotseruba, J K Tsotsos, Artificial Intelligence Review. 5312020</p>
<p>Bridging rl theory and practice with the effective horizon. C Laidlaw, S Russell, A Dragan, arXiv:2304.098532023arXiv preprint</p>
<p>The Soar cognitive architecture. J E Laird, 2019MIT press</p>
<p>J E Laird, arXiv:2205.03854Introduction to Soar. 2022arXiv preprint</p>
<p>Chunking in Soar: The anatomy of a general learning mechanism. J E Laird, P S Rosenbloom, A Newell, Machine Learning. 11986</p>
<p>Soar: An architecture for general intelligence. J E Laird, A Newell, P S Rosenbloom, Artificial Intelligence. 3311987</p>
<p>Cognitive robotics using the Soar cognitive architecture. J E Laird, K R Kinkade, S Mohan, J Z Xu, CogRob @ AAAI. 2012</p>
<p>Building machines that learn and think like people. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, 2016</p>
<p>. Langchain, Langchain, 2022</p>
<p>Coderl: Mastering code generation through pretrained models and deep reinforcement learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Advances in Neural Information Processing Systems. 202235</p>
<p>A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Y Lecun, Open Review. 622022</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H KÃ¼ttler, M Lewis, W -T. Yih, T RocktÃ¤schel, Advances in Neural Information Processing Systems. 202033</p>
<p>Lampp: Language models as probabilistic priors for perception and action. B Z Li, W Chen, P Sharma, J Andreas, arXiv:2302.028012023aarXiv preprint</p>
<p>Multimodal foundation models: From specialists to general-purpose assistants. C Li, Z Gan, Z Yang, J Yang, L Li, L Wang, J Gao, arXiv:2309.100202023barXiv preprint</p>
<p>A survey on retrieval-augmented text generation. H Li, Y Su, D Cai, Y Wang, L Liu, arXiv:2202.011102022aarXiv preprint</p>
<p>. R Li, L B Allal, Y Zi, N Muennighoff, D Kocetkov, C Mou, M Marone, C Akiki, J Li, J Chim, Q Liu, E Zheltonozhskii, T Y Zhuo, T Wang, O Dehaene, M Davaadorj, J Lamy-Poirier, J Monteiro, O Shliazhko, N Gontier, N Meade, A Zebaze, M.-H Yee, L K Umapathi, J Zhu, B Lipkin, M Oblokulov, Z Wang, R Murthy, J Stillerman, S S Patel, D Abulkhanov, M Zocca, M Dey, Z Zhang, N Fahmy, U Bhattacharyya, W Yu, S Singh, S Luccioni, P Villegas, M Kunakov, F Zhdanov, M Romero, T Lee, N Timor, J Ding, C Schlesinger, H Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J. Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis, S. M. Hughes, T. Wolf, A. Guha, L. von Werra2023cand H. de Vries. Starcoder: may the source be with you! ArXiv, abs/2305.06161</p>
<p>Competition-level code generation with alphacode. Y Li, D H Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, Tom, J Eccles, F Keeling, A D Gimeno, T Lago, P Hubert, C Choy, M De, I Autume, X Babuschkin, P.-S Chen, J Huang, S Welbl, Gowal, Alexey, J Cherepanov, D J Molloy, E S Mankowitz, P Robson, N Kohli, De, K Freitas, O Kavukcuoglu, Vinyals, Science. 3782022b</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023a</p>
<p>Towards understanding and mitigating social biases in language models. P P Liang, C Wu, L.-P Morency, R Salakhutdinov, International Conference on Machine Learning. 2021</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. T Liang, Z He, W Jiao, X Wang, Y Wang, R Wang, Y Yang, Z Tu, S Shi, arXiv:2305.191182023barXiv preprint</p>
<p>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. F Lieder, T L Griffiths, Behavioral and Brain Sciences. 43e12020</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. B Y Lin, Y Fu, K Yang, P Ammanabrolu, F Brahman, S Huang, C Bhagavatula, Y Choi, X Ren, arXiv:2305.173902023arXiv preprint</p>
<p>Toward integrating cognitive linguistics and cognitive language processing. P Lindes, J E Laird, Proceedings of the 14th International Conference on Cognitive Modeling (ICCM). the 14th International Conference on Cognitive Modeling (ICCM)2016</p>
<p>B Liu, Y Jiang, X Zhang, Q Liu, S Zhang, J Biswas, P Stone, arXiv:2304.11477LLM+P: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, NeurIPS2023b</p>
<p>H Liu, C Sferrazza, P Abbeel, arXiv:2302.02676Languages are rewards: Hindsight finetuning using human feedback. 2023carXiv preprint</p>
<p>J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, arXiv:2101.06804What Makes Good In-Context Examples for GPT-3 ?. 2021arXiv preprint</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 0360-03005592023d</p>
<p>Mind's eye: Grounded language model reasoning through simulation. R Liu, J Wei, S S Gu, T.-Y Wu, S Vosoughi, C Cui, D Zhou, A M Dai, The Eleventh International Conference on Learning Representations. 2023e</p>
<p>Training socially aligned language models in simulated human society. R Liu, R Yang, C Jia, G Zhang, D Zhou, A M Dai, D Yang, S Vosoughi, arXiv:2305.169602023farXiv preprint</p>
<p>. Llamaindex, Llamaindex, 2023</p>
<p>Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions. Information and software technology. L E Lwakatare, A Raj, I Crnkovic, J Bosch, H H Olsson, 2020127106368</p>
<p>Understanding the benefits and challenges of using large language model-based conversational agents for mental well-being support. Z Ma, Y Mei, Z Su, arXiv:2307.158102023arXiv preprint</p>
<p>Robot operating system 2: Design, architecture, and uses in the wild. S Macenski, T Foote, B Gerkey, C Lalancette, W Woodall, Science Robotics. 76660742022</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, arXiv:2303.176512023arXiv preprint</p>
<p>The theory of algorithms. A A Markov, Trudy Matematicheskogo Instituta Imeni VA Steklova. 421954</p>
<p>Prioritized memory access explains planning and hippocampal replay. M G Mattar, N D Daw, Nature Neuroscience. 21112018</p>
<p>Extending machine language models toward human-level language understanding. J L Mcclelland, F Hill, M Rudolph, J Baldridge, H SchÃ¼tze, arXiv:1912.058772019arXiv preprint</p>
<p>Language models enable zero-shot prediction of the effects of mutations on protein function. J Meier, R Rao, R Verkuil, J Liu, T Sercu, A Rives, bioRxiv. 2021</p>
<p>Augmented language models: a survey. G Mialon, R DessÃ¬, M Lomeli, C Nalmpantis, R Pasunuru, R Raileanu, B RoziÃ¨re, T Schick, J Dwivedi-Yu, A Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Learning goal-oriented hierarchical tasks from situated interactive instruction. S Mohan, J Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201428</p>
<p>Acquiring grounded representations of words with situated interactive instruction. S Mohan, A H Mininger, J R Kirk, J E Laird, Advances in Cognitive Systems. 20122</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-Assisted Question-Answering with Human Feedback. 2021arXiv preprint</p>
<p>Deep transfer in reinforcement learning by language grounding. K Narasimhan, R Barzilay, T Jaakkola, Journal of Artificial Intelligence Research. 2018JAIR)</p>
<p>Collaborative dialogue in Minecraft. A Narayan-Chen, P Jayannavar, J Hockenmaier, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2019</p>
<p>Soar-RL: Integrating reinforcement learning with Soar. S Nason, J E Laird, Cognitive Systems Research. 612005</p>
<p>Language models are bounded pragmatic speakers. K X Nguyen, First Workshop on Theory of Mind in Communicating Agents. 2023</p>
<p>Interactive learning from activity description. K X Nguyen, D Misra, R Schapire, M DudÃ­k, P Shafto, International Conference on Machine Learning. 2021</p>
<p>A framework for learning to request rich and contextually useful information from humans. K X Nguyen, Y Bisk, H D Iii, International Conference on Machine Learning. 2022b</p>
<p>T T Nguyen, T T Huynh, P L Nguyen, A W , .-C Liew, H Yin, Q V H Nguyen, arXiv:2209.02299A survey of machine unlearning. 2022carXiv preprint</p>
<p>Lever: Learning to verify language-to-code generation with execution. A Ni, S Iyer, D Radev, V Stoyanov, W -T. Yih, S Wang, X V Lin, International Conference on Machine Learning. 2023</p>
<p>Shakey the robot. N J Nilsson, 1984Technical Note</p>
<p>Document expansion by query prediction. R Nogueira, W Yang, J Lin, K Cho, 2019</p>
<p>Extending cognitive architecture with episodic memory. A M Nuxoll, J E Laird, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2007</p>
<p>Show your work: Scratchpads for intermediate computation with language models. M Nye, A J Andreassen, G Gur-Ari, H Michalewski, J Austin, D Bieber, D Dohan, A Lewkowycz, M Bosma, D Luan, arXiv:2112.001142021arXiv preprint</p>
<p>. ArXiv, abs/2303.08774OpenAI. Gpt-4 technical report. 2023a</p>
<p>Function calling and other API updates. Openai, 2023b</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Teach: Task-driven embodied agents that chat. A Padmakumar, J Thomason, A Shrivastava, P Lange, A Narayan-Chen, S Gella, R Piramuthu, G Tur, D Hakkani-Tur, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Towards a unified agent with foundation models. N D Palo, A Byravan, L Hasenclever, M Wulfmeier, N Heess, M Riedmiller, Workshop on Reincarnating Reinforcement Learning at ICLR 2023. 2023</p>
<p>A Parisi, Y Zhao, N Fiedel, arXiv:2205.12255Talm: Tool augmented language models. 2022arXiv preprint</p>
<p>S Park, J C O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>AI-generated characters for supporting personalized learning and well-being. P Pataranutaporn, V Danry, J Leong, P Punpongsanon, D Novy, P Maes, M Sra, Nature Machine Intelligence. 3122021</p>
<p>Language guided state abstractions. A Peng, I Sucholutsky, B Li, T R Sumers, T L Griffiths, J Andreas, J A Shah, Workshop on Social Intelligence in Humans and Robots at RSS 2023. 2023</p>
<p>Formal reductions of the general combinatorial decision problem. E L Post, American Journal of Mathematics. 6521943</p>
<p>Neural episodic control. A Pritzel, B Uria, S Srinivasan, A P Badia, O Vinyals, D Hassabis, D Wierstra, C Blundell, International conference on machine learning. 2017</p>
<p>Markov decision processes: discrete stochastic dynamic programming. M L Puterman, 2014John Wiley &amp; Sons</p>
<p>C Qian, X Cong, C Yang, W Chen, Y Su, J Xu, Z Liu, M Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Ros: an open-source robot operating system. M Quigley, IEEE International Conference on Robotics and Automation. 2009</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI blog. 1892019</p>
<p>Robots that ask for help: Uncertainty alignment for large language model planners. A Z Ren, A Dixit, A Bodrova, S Singh, S Tu, N Brown, P Xu, L Takayama, F Xia, Z Xu, 7th Annual Conference on Robot Learning. 2023</p>
<p>Synergistic integration of large language models and cognitive architectures for robust ai: An exploratory analysis. O J Romero, J Zimmerman, A Steinfeld, A Tomasic, arXiv:2308.098302023arXiv preprint</p>
<p>Code llama: Open foundation models for code. B RoziÃ¨re, J Gehring, F Gloeckle, S Sootla, I Gat, X Tan, Y Adi, J Liu, T Remez, J Rapin, A Kozhevnikov, I Evtimov, J Bitton, M P Bhatt, C C Ferrer, A Grattafiori, W Xiong, A D'efossez, J Copet, F Azhar, H Touvron, L Martin, N Usunier, T Scialom, G Synnaeve, ArXiv, abs/2308.129502023</p>
<p>Learning to retrieve prompts for in-context learning. O Rubin, J Herzig, J Berant, arXiv:2112.086332021arXiv preprint</p>
<p>Time spent thinking in online chess reflects the value of computation. E Russek, D Acosta-Kane, B Van Opheusden, M G Mattar, T Griffiths, PsyArXiv. 2022</p>
<p>Artificial Intelligence: A Modern Approach. S Russell, P Norvig, 2013Pearson Education Limited London</p>
<p>Active preference-based learning of reward functions. D Sadigh, A D Dragan, S Sastry, S A Seshia, Robotics: Science and Systems XIII. N M Amato, S S Srinivasa, N Ayanian, S Kuindersma, 2017</p>
<p>Self-critiquing models for assisting human evaluators. W Saunders, C Yeh, J Wu, S Bills, L Ouyang, J Ward, J Leike, arXiv:2206.058022022arXiv preprint</p>
<p>T Schick, J Dwivedi-Yu, R DessÃ¬, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, Toolformer, arXiv:2302.04761Language models can teach themselves to use tools. 2023arXiv preprint</p>
<p>Machine Learning: The High Interest Credit Card of Technical Debt. D Sculley, G Holt, D Golovin, E Davydov, T Phillips, D Ebner, V Chaudhary, M Young, SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop). 2014</p>
<p>World of Bits: An Open-Domain platform for web-based agents. T Shi, A Karpathy, L Fan, J Hernandez, P Liang, International Conference on Machine Learning. 2017</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, arXiv:2303.113662023arXiv preprint</p>
<p>M Shridhar, X Yuan, M.-A CÃ´tÃ©, Y Bisk, A Trischler, M Hausknecht, arXiv:2010.03768Alfworld: Aligning text and embodied environments for interactive learning. 2020arXiv preprint</p>
<p>Pddl planning with pretrained large language models. T Silver, V Hariprasad, R S Shuttleworth, N Kumar, T Lozano-PÃ©rez, L P Kaelbling, NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022</p>
<p>T Silver, S Dan, K Srinivas, J B Tenenbaum, L P Kaelbling, M Katz, arXiv:2305.11014Generalized Planning in PDDL Domains with Pretrained Large Language Models. 2023arXiv preprint</p>
<p>Progprompt: Generating situated robot task plans using large language models. I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>How to talk so AI will learn: Instructions, descriptions, and autonomy. T Sumers, R Hawkins, M K Ho, T Griffiths, D Hadfield-Menell, Advances in Neural Information Processing Systems. 202235</p>
<p>Distilling internet-scale vision-language models into embodied agents. T Sumers, K Marino, A Ahuja, R Fergus, I Dasgupta, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Learning rewards from linguistic feedback. T R Sumers, M K Ho, R D Hawkins, K Narasimhan, T L Griffiths, Proceedings of the AAAI Conference on Artificial Intelligence. R S Sutton, A G Barto, the AAAI Conference on Artificial IntelligenceMIT press2021. 2004. 201835Reinforcement learning: An introduction</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. O Tafjord, B Dalvi, P Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Language (re)modelling: Towards embodied language understanding. R Tamari, C Shani, T Hope, M R L Petruck, O Abend, D Shahaf, 10.18653/v1/2020.acl-main.559Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>Intelligent agents for interactive simulation environments. M Tambe, W L Johnson, R M Jones, F Koss, J E Laird, P S Rosenbloom, K Schwamb, AI magazine. 1611995</p>
<p>Referral augmentation for zero-shot information retrieval. M Tang, S Yao, J Yang, K Narasimhan, 2023a</p>
<p>Q Tang, Z Deng, H Lin, X Han, Q Liang, L Sun, Toolalpaca, arXiv:2306.05301Generalized Tool Learning for Language Models with 3000 Simulated Cases. 2023barXiv preprint</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, arXiv:2312.11805family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Understanding natural language commands for robotic navigation and mobile manipulation. S Tellex, T Kollar, S Dickerson, M Walter, A Banerjee, S Teller, N Roy, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201125</p>
<p>Vision-and-dialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, Conference on Robot Learning. PMLR2020</p>
<p>On computable numbers, with an application to the entscheidungsproblem. A M Turing, J. of Math. 5851936</p>
<p>J Tuyls, S Yao, S Kakade, K Narasimhan, arXiv:2201.01251Multi-stage episodic control for strategic exploration in text games. 2022arXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). K Valmeekam, A Olmo, S Sreedharan, S Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Å Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023aarXiv preprint</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, A survey on large language model based autonomous agents. 2023b</p>
<p>Query2doc: Query expansion with large language models. L Wang, N Yang, F Wei, arXiv:2303.076782023carXiv preprint</p>
<p>R Wang, P Jansen, M.-A CÃ´tÃ©, P Ammanabrolu, arXiv:2203.07540Scienceworld: Is your agent smarter than a 5th grader?. 2022aarXiv preprint</p>
<p>Learning language games through interaction. S I Wang, P Liang, C D Manning, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics20161</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, arXiv:2203.111712022barXiv preprint</p>
<p>Emergent abilities of large language models. J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, E H Chi, T Hashimoto, O Vinyals, P Liang, J Dean, W Fedus, Transactions on Machine Learning Research. 2835-88562022aSurvey Certification</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022barXiv preprint</p>
<p>Llm-powered autonomous agents. lilianweng.github.io. L Weng, Jun 2023</p>
<p>. J Weston, S Chopra, A Bordes, arXiv:1410.39162014Memory networks. arXiv preprint</p>
<p>Practical planning: extending the classical AI planning paradigm. A N Whitehead, B Russell, D E Wilkins, Principia mathematica to* 56. Elsevier1997. 2014. 19722</p>
<p>From word models to world models: Translating from natural language to the probabilistic language of thought. L Wong, G Grand, A K Lew, N D Goodman, V K Mansinghka, J Andreas, J B Tenenbaum, arXiv:2306.126722023arXiv preprint</p>
<p>Language models as a knowledge source for cognitive agents. R E Wray, J R Kirk, J E Laird, arXiv:2109.082702021arXiv preprint</p>
<p>Q Wu, G Bansal, J Zhang, Y Wu, S Zhang, E Zhu, B Li, L Jiang, X Zhang, C Wang, arXiv:2308.08155Autogen: Enabling next-gen llm applications via multi-agent conversation framework. 2023arXiv preprint</p>
<p>Promptchainer: Chaining large language model prompts through visual programming. T Wu, E Jiang, A Donsbach, J Gray, A Molina, M Terry, C J Cai, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022a</p>
<p>AI chains: Transparent and controllable human-AI interaction by chaining large language model prompts. T Wu, M Terry, C J Cai, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022b</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Y Xia, M Shenoy, N Jazdi, M Weyrich, arXiv:2304.14721Towards autonomous system: flexible modular production system enhanced with large language model agents. 2023arXiv preprint</p>
<p>Y Xie, T Xie, M Lin, W Wei, C Li, B Kong, L Chen, C Zhuo, B Hu, Z Li, arXiv:2305.16334Olagpt: Empowering llms with human-like problem-solving abilities. 2023arXiv preprint</p>
<p>B Xu, X Liu, H Shen, Z Han, Y Li, M Yue, Z Peng, Y Liu, Z Yao, D Xu, arXiv:2308.04030Gentopia: A collaborative platform for tool-augmented llms. 2023aarXiv preprint</p>
<p>Rewoo: Decoupling reasoning from observations for efficient augmented language models. B Xu, Z Peng, B Lei, S Mukherjee, Y Liu, D Xu, arXiv:2305.183232023barXiv preprint</p>
<p>B Xu, A Yang, J Lin, Q Wang, C Zhou, Y Zhang, Z Mao, arXiv:2305.14688ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. 2023carXiv preprint</p>
<p>Intercode: Standardizing and benchmarking interactive coding with execution feedback. J Yang, A Prabhakar, K Narasimhan, S Yao, arXiv:2306.148982023arXiv preprint</p>
<p>Language agents in the digital world: Opportunities and risks. princetonnlp.github.io. S Yao, K Narasimhan, Jul 2023</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. S Yao, R Rao, M Hausknecht, K Narasimhan, arXiv:2010.029032020arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022barXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>STaR: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. A Zeng, M Attarian, B Ichter, K Choromanski, A Wong, S Welker, F Tombari, A Purohit, M Ryoo, V Sindhwani, arXiv:2204.005982022arXiv preprint</p>
<p>Grounded physical language understanding with probabilistic programs and simulated worlds. C Zhang, L Wong, G Grand, J Tenenbaum, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society2023a45</p>
<p>T Zhang, F Liu, J Wong, P Abbeel, J E Gonzalez, arXiv:2302.05206The wisdom of hindsight makes language models better instruction followers. 2023barXiv preprint</p>
<p>Dialogpt: Large-scale generative pre-training for conversational response generation. Y Zhang, S Sun, M Galley, Y.-C Chen, C Brockett, X Gao, J Gao, J Liu, W B Dolan, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations2020</p>
<p>Process and content in decisions from memory. W J Zhao, R Richie, S Bhatia, Psychological Review. 1291732022</p>
<p>SILG: The Multi-domain Symbolic Interactive Language Grounding Benchmark. V Zhong, A W Hanjie, S Wang, K Narasimhan, L Zettlemoyer, Advances in Neural Information Processing Systems. 202134</p>
<p>Episodic retrieval for model-based evaluation in sequential decision tasks. C Y Zhou, D Talmi, N Daw, M G Mattar, 2023a</p>
<p>Emotional chatting machine: Emotional conversation generation with internal and external memory. H Zhou, M Huang, T Zhang, X Zhu, B Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201832</p>
<p>Docprompting: Generating code by retrieving the docs. S Zhou, U Alon, F F Xu, Z Jiang, G Neubig, The Eleventh International Conference on Learning Representations. 2022a</p>
<p>S Zhou, F F Xu, H Zhu, X Zhou, R Lo, A Sridhar, X Cheng, Y Bisk, D Fried, U Alon, arXiv:2307.13854A Realistic Web Environment for Building Autonomous Agents. 2023barXiv preprint</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.019102022barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
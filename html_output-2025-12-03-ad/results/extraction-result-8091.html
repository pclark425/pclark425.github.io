<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8091 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8091</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8091</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-287fc092d552d4a77d914833c371c0e0b1119067</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/287fc092d552d4a77d914833c371c0e0b1119067" target="_blank">LLM-based relevance assessment still can't replace human relevance assessment</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Evaluating Information Access</p>
                <p><strong>Paper TL;DR:</strong> This paper critically examines the claim that LLM-based relevance assessments, such as those generated by the UMBRELA system, can fully replace traditional human relevance assessments in TREC-style evaluations, highlighting practical and theoretical limitations that undermine this conclusion.</p>
                <p><strong>Paper Abstract:</strong> The use of large language models (LLMs) for relevance assessment in information retrieval has gained significant attention, with recent studies suggesting that LLM-based judgments provide comparable evaluations to human judgments. Notably, based on TREC 2024 data, Upadhyay et al. make a bold claim that LLM-based relevance assessments, such as those generated by the UMBRELA system, can fully replace traditional human relevance assessments in TREC-style evaluations. This paper critically examines this claim, highlighting practical and theoretical limitations that undermine the validity of this conclusion. First, we question whether the evidence provided by Upadhyay et al. really supports their claim, particularly if a test collection is used asa benchmark for future improvements. Second, through a submission deliberately intended to do so, we demonstrate the ease with which automatic evaluation metrics can be subverted, showing that systems designed to exploit these evaluations can achieve artificially high scores. Theoretical challenges -- such as the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance -- must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8091.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8091.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Umbrella_vs_manual (Upadhyay et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Umbrella LLM-based relevance assessor compared to manual TREC relevance judgments (Upadhyay et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale comparison reported by Upadhyay et al. showing high overall rank correlation between Umbrella (an LLM-based assessor) and manual TREC relevance judgments on the TREC RAG 2024 retrieval task, with caveats about fine-grained agreement at the top of leaderboards.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval (run-level system evaluation, NDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2024 RAG task (MS MARCO Segment V2.1; 301 queries produced)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Umbrella (UMBRELA/Bing RELevance Assessor reproduction)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Open-source reproduction of Bing RELevance Assessor (Umbrella); LLM-based automatic relevance assessor (details in Upadhyay et al. / UMBRELA paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC manual assessors (human relevance judgments following TREC protocols)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.89</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced discrimination among top systems; possible circularity risk if reused as reranker; small number of manual judgments for robust inference; no reported error bars</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High overall correlation between Umbrella and manual judgments at run-level, but potential mismatches and notable outliers exist; close mimicry of human outcomes may reflect LLM acting as a strong ranker rather than a true human-centered evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalability and automation: fast, low-cost, and able to produce full-automatic run-level assessments; reported as able to capture run-level effectiveness in Upadhyay et al.'s analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Run-level comparison of NDCG@10 across submitted runs; Umbrella automatic assessments compared to fully manual TREC assessments (Upadhyay et al.). 301 runs/queries were produced in the task; Clarke notes limited manual-judged subset (discussed as 27 queries) which may affect robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8091.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8091.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_reassessment (Faggioli et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003) reassessment of TREC 2021 Deep Learning track</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Faggioli et al. applied GPT-3.5 to re-assess TREC 2021 runs and reported strong leaderboard correlation (Kendall's tau = 0.86) but cautioned against replacing human assessment due to bias and circularity risks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Perspectives on Large Language Models for Relevance Judgment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Perspectives on Large Language Models for Relevance Judgment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc retrieval evaluation (NDCG@10 leaderboard correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2021 Deep Learning track (reassessed runs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-3.5 (text-davinci-003) used to produce LLM-based relevance judgments for reassessment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC human assessors / crowdsourced human judgments (original TREC judgments used as reference)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.86</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>unknown/hidden biases in LLM judgments; circularity when LLMs evaluate outputs of other LLMs; LLMs are not human and may not reflect human utility</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Strong empirical correlation with manual judgments at leaderboard level, but authors warn about theoretical concerns (biases, circularity) and that LLM judgments cannot fully replace humans.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Demonstrates potential to produce scalable reassessments and strong empirical leaderboard correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3.5 used to fully reassess previously submitted runs to TREC 2021 Deep Learning track; correlation of run-level NDCG@10 compared to human judgments reported as Kendall's tau=0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8091.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8091.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clarke_reproduction_top60</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reproduction of Umbrella vs manual correlation on top 60 systems (Clarke 2025 analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Clarke reproduces and extends Upadhyay et al.'s analysis: excluding the bottom 15 systems (of 75) and recomputing yields a slightly lower Kendall's tau (0.84) over the remaining top 60 systems, indicating some inflation of correlation by poor-performing systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Ad-hoc passage retrieval run-level evaluation (NDCG variants)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2024 RAG task (MS MARCO Segment V2.1; TREC RAG 24 submission set)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Umbrella (UmbrelA / UMBRELA labels from the track)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Umbrella labels produced by the track organizers (LLM-based relevance assessor / reproduction of Bing assessor); used as evaluation labels and in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC manual assessors (human relevance judgments used as gold standard)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.84</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>overall Kendall's tau can be inflated by presence of many poor-performing systems; notable outliers where LLM and human scores diverge substantially</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Removing bottom-ranked systems reduces the reported correlation modestly; some individual runs show large LLM vs human score discrepancies (example: LLM score 0.81 vs manual 0.63 for an outlier run).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Confirms prior findings of substantial overall agreement, reinforcing Umbrella's potential usefulness for large-scale automatic assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Extended analysis on TREC RAG 24 data: removed bottom 15 of 75 systems and recomputed Kendall's tau across remaining top 60 systems; compared run-level scores under Umbrella vs manual judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8091.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8091.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clarke_top_systems_discrepancy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weakened agreement among top-performing systems (Clarke 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Clarke reports that while overall correlation is high, agreement between Umbrella and human judgments weakens substantially among the top-performing systems: Kendall's tau falls to 0.51 among top 20 systems and to 0.56 among top 15 systems, corresponding to many system-swaps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Distinguishing improvements among top retrieval systems (leaderboard ranking reliability)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2024 RAG task (top-performing submitted runs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Umbrella (LLM assessor)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Umbrella automatic relevance labels (as provided by track organizers); used to compute correlations restricted to top-ranked systems.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC manual assessors (used as basis for identifying top-performing systems)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.51</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>substantially reduced alignment at leaderboard frontier; 21–24% system-swaps among top systems; insufficient sensitivity to detect fine-grained improvements</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Automatic assessments fail to reliably identify the best systems at the top of the leaderboard; notable rank reversals where top automatic systems are far lower under manual evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>None asserted for fine-grained top-system discrimination; overall automation benefit remains but is insufficient for frontier evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Correlation analysis restricted to top 20 (and top 15) systems as defined by manual judgments; Kendall's tau and percentage of system swaps reported to show weakened agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8091.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8091.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Circularity_simulation_with_Umbrella</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation of circular evaluation when Umbrella is used both as re-ranker and as evaluator (Clarke 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Clarke simulates widespread adoption of Umbrella as a final-stage re-ranker for all submissions and shows severe degradation in agreement with human judgments: overall Kendall's tau drops to 0.63 and to as low as -0.40 among the top 5 systems, with substantial score inflation under Umbrella.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Impact of using same LLM assessor as final-stage re-ranker on leaderboard validity (NDCG comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2024 RAG task (all submitted runs reranked with Umbrella labels)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Umbrella (used both as reranker and as evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Umbrella relevance labels from the track reused as final-stage reranker labels; simulation applies those labels to re-rank every system and then compares evaluations under Umbrella vs human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>TREC manual assessors (human judgments used as gold standard for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.63</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>circularity-induced score inflation (multiple systems with NDCG>0.95 under Umbrella but 0.68–0.72 under manual labels); increased discordant system pairs (18% within top 60); dramatic tau reductions at top ranks (top20 tau=0.44; top15=0.49; top10=0.38; top5=-0.40); invalid evaluation when the evaluator is used as a system component.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When the evaluation metric is the same LLM used inside systems, system development is strongly incentivized to exploit that metric and leaderboard validity breaks down; apparent near-perfect scores under Umbrella do not match human-assessed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Using Umbrella as a re-ranker improves measured performance under human labels in this simulation (i.e., Umbrella re-ranking increases systems' NDCG under human judgments), which creates incentive to adopt it—this is precisely the mechanism that produces circularity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Applied Umbrella relevance labels as final-stage reranker for all submitted retrieval systems (using the Umbrella labels produced by the track); compared reranked systems' NDCG under Umbrella labels vs manual human labels; reported Kendall's tau and discordant pair percentages across top-K system subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8091.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8091.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>uwc1_adversarial_run</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WaterlooClarke adversarial run (uwc1) designed to subvert LLM-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deliberately crafted submission (uwc1) that pooled top documents from many preliminary runs and used LLM-based judgments (reported as GPT-40 in the paper) to generate final rankings, achieving high automatic-evaluation rank (5th) but low manual rank (28th), illustrating vulnerability to manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Demonstration of manipulation risk of LLM-based evaluation (run-level NDCG comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC 2024 RAG pooled candidate passages (top-20 from 15 preliminary runs)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-40 (as reported in Clarke's description of the uwc1 procedure)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Paper states the pool was judged by 'GPT-40' using the prompt described by Faggioli et al.; used to produce LLM-based pairwise and relevance judgments that guided final ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Manual TREC assessors (used for contrast to show divergence in final ranked positions)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>NDCG (run-level score comparisons; rank positions compared)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>vulnerability to strategic manipulation: can artifically inflate automatic-evaluation scores; single-run LLM vs human score divergence (example reported: LLM-based run score 0.81 vs manual 0.63 for an outlier)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Shows concrete example where a run optimized to match LLM assessor's preferences attains a high automatic rank but fares poorly under human judgments, evidencing brittleness and manipulability of LLM-only evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Enables automated pooling/labeling to create strong results quickly (which is an advantage in scale but becomes a vulnerability when exploited).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Created a pooled candidate set (top 20 from 15 preliminary runs); passages judged by GPT-40 with Faggioli prompt; top-graded passages judged pairwise (substituting LLM assessments for human judgments) and used as keys for final ranking; final run ranked 5th under Umbrella but 28th under manual assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "LLM-based relevance assessment still can't replace human relevance assessment", 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor <em>(Rating: 2)</em></li>
                <li>A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look <em>(Rating: 2)</em></li>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 2)</em></li>
                <li>Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation <em>(Rating: 2)</em></li>
                <li>LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant <em>(Rating: 2)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 1)</em></li>
                <li>LLM Evaluators Recognize and Favor Their Own Generations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8091",
    "paper_id": "paper-287fc092d552d4a77d914833c371c0e0b1119067",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "Umbrella_vs_manual (Upadhyay et al. 2024)",
            "name_full": "Umbrella LLM-based relevance assessor compared to manual TREC relevance judgments (Upadhyay et al., 2024)",
            "brief_description": "Large-scale comparison reported by Upadhyay et al. showing high overall rank correlation between Umbrella (an LLM-based assessor) and manual TREC relevance judgments on the TREC RAG 2024 retrieval task, with caveats about fine-grained agreement at the top of leaderboards.",
            "citation_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "mention_or_use": "mention",
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "evaluation_task": "Ad-hoc passage retrieval (run-level system evaluation, NDCG@10)",
            "dataset_name": "TREC 2024 RAG task (MS MARCO Segment V2.1; 301 queries produced)",
            "judge_model_name": "Umbrella (UMBRELA/Bing RELevance Assessor reproduction)",
            "judge_model_details": "Open-source reproduction of Bing RELevance Assessor (Umbrella); LLM-based automatic relevance assessor (details in Upadhyay et al. / UMBRELA paper)",
            "human_evaluator_type": "TREC manual assessors (human relevance judgments following TREC protocols)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.89,
            "reported_loss_aspects": "reduced discrimination among top systems; possible circularity risk if reused as reranker; small number of manual judgments for robust inference; no reported error bars",
            "qualitative_findings": "High overall correlation between Umbrella and manual judgments at run-level, but potential mismatches and notable outliers exist; close mimicry of human outcomes may reflect LLM acting as a strong ranker rather than a true human-centered evaluation metric.",
            "advantages_of_llm_judge": "Scalability and automation: fast, low-cost, and able to produce full-automatic run-level assessments; reported as able to capture run-level effectiveness in Upadhyay et al.'s analysis.",
            "experimental_setting": "Run-level comparison of NDCG@10 across submitted runs; Umbrella automatic assessments compared to fully manual TREC assessments (Upadhyay et al.). 301 runs/queries were produced in the task; Clarke notes limited manual-judged subset (discussed as 27 queries) which may affect robustness.",
            "uuid": "e8091.0",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-3.5_reassessment (Faggioli et al. 2023)",
            "name_full": "GPT-3.5 (text-davinci-003) reassessment of TREC 2021 Deep Learning track",
            "brief_description": "Faggioli et al. applied GPT-3.5 to re-assess TREC 2021 runs and reported strong leaderboard correlation (Kendall's tau = 0.86) but cautioned against replacing human assessment due to bias and circularity risks.",
            "citation_title": "Perspectives on Large Language Models for Relevance Judgment",
            "mention_or_use": "mention",
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "evaluation_task": "Ad-hoc retrieval evaluation (NDCG@10 leaderboard correlation)",
            "dataset_name": "TREC 2021 Deep Learning track (reassessed runs)",
            "judge_model_name": "GPT-3.5 (text-davinci-003)",
            "judge_model_details": "OpenAI GPT-3.5 (text-davinci-003) used to produce LLM-based relevance judgments for reassessment.",
            "human_evaluator_type": "TREC human assessors / crowdsourced human judgments (original TREC judgments used as reference)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.86,
            "reported_loss_aspects": "unknown/hidden biases in LLM judgments; circularity when LLMs evaluate outputs of other LLMs; LLMs are not human and may not reflect human utility",
            "qualitative_findings": "Strong empirical correlation with manual judgments at leaderboard level, but authors warn about theoretical concerns (biases, circularity) and that LLM judgments cannot fully replace humans.",
            "advantages_of_llm_judge": "Demonstrates potential to produce scalable reassessments and strong empirical leaderboard correlation.",
            "experimental_setting": "GPT-3.5 used to fully reassess previously submitted runs to TREC 2021 Deep Learning track; correlation of run-level NDCG@10 compared to human judgments reported as Kendall's tau=0.86.",
            "uuid": "e8091.1",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Clarke_reproduction_top60",
            "name_full": "Reproduction of Umbrella vs manual correlation on top 60 systems (Clarke 2025 analysis)",
            "brief_description": "Clarke reproduces and extends Upadhyay et al.'s analysis: excluding the bottom 15 systems (of 75) and recomputing yields a slightly lower Kendall's tau (0.84) over the remaining top 60 systems, indicating some inflation of correlation by poor-performing systems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment",
            "evaluation_task": "Ad-hoc passage retrieval run-level evaluation (NDCG variants)",
            "dataset_name": "TREC 2024 RAG task (MS MARCO Segment V2.1; TREC RAG 24 submission set)",
            "judge_model_name": "Umbrella (UmbrelA / UMBRELA labels from the track)",
            "judge_model_details": "Umbrella labels produced by the track organizers (LLM-based relevance assessor / reproduction of Bing assessor); used as evaluation labels and in simulation.",
            "human_evaluator_type": "TREC manual assessors (human relevance judgments used as gold standard)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.84,
            "reported_loss_aspects": "overall Kendall's tau can be inflated by presence of many poor-performing systems; notable outliers where LLM and human scores diverge substantially",
            "qualitative_findings": "Removing bottom-ranked systems reduces the reported correlation modestly; some individual runs show large LLM vs human score discrepancies (example: LLM score 0.81 vs manual 0.63 for an outlier run).",
            "advantages_of_llm_judge": "Confirms prior findings of substantial overall agreement, reinforcing Umbrella's potential usefulness for large-scale automatic assessment.",
            "experimental_setting": "Extended analysis on TREC RAG 24 data: removed bottom 15 of 75 systems and recomputed Kendall's tau across remaining top 60 systems; compared run-level scores under Umbrella vs manual judgments.",
            "uuid": "e8091.2",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Clarke_top_systems_discrepancy",
            "name_full": "Weakened agreement among top-performing systems (Clarke 2025)",
            "brief_description": "Clarke reports that while overall correlation is high, agreement between Umbrella and human judgments weakens substantially among the top-performing systems: Kendall's tau falls to 0.51 among top 20 systems and to 0.56 among top 15 systems, corresponding to many system-swaps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment",
            "evaluation_task": "Distinguishing improvements among top retrieval systems (leaderboard ranking reliability)",
            "dataset_name": "TREC 2024 RAG task (top-performing submitted runs)",
            "judge_model_name": "Umbrella (LLM assessor)",
            "judge_model_details": "Umbrella automatic relevance labels (as provided by track organizers); used to compute correlations restricted to top-ranked systems.",
            "human_evaluator_type": "TREC manual assessors (used as basis for identifying top-performing systems)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.51,
            "reported_loss_aspects": "substantially reduced alignment at leaderboard frontier; 21–24% system-swaps among top systems; insufficient sensitivity to detect fine-grained improvements",
            "qualitative_findings": "Automatic assessments fail to reliably identify the best systems at the top of the leaderboard; notable rank reversals where top automatic systems are far lower under manual evaluation.",
            "advantages_of_llm_judge": "None asserted for fine-grained top-system discrimination; overall automation benefit remains but is insufficient for frontier evaluation.",
            "experimental_setting": "Correlation analysis restricted to top 20 (and top 15) systems as defined by manual judgments; Kendall's tau and percentage of system swaps reported to show weakened agreement.",
            "uuid": "e8091.3",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Circularity_simulation_with_Umbrella",
            "name_full": "Simulation of circular evaluation when Umbrella is used both as re-ranker and as evaluator (Clarke 2025)",
            "brief_description": "Clarke simulates widespread adoption of Umbrella as a final-stage re-ranker for all submissions and shows severe degradation in agreement with human judgments: overall Kendall's tau drops to 0.63 and to as low as -0.40 among the top 5 systems, with substantial score inflation under Umbrella.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment",
            "evaluation_task": "Impact of using same LLM assessor as final-stage re-ranker on leaderboard validity (NDCG comparisons)",
            "dataset_name": "TREC 2024 RAG task (all submitted runs reranked with Umbrella labels)",
            "judge_model_name": "Umbrella (used both as reranker and as evaluator)",
            "judge_model_details": "Umbrella relevance labels from the track reused as final-stage reranker labels; simulation applies those labels to re-rank every system and then compares evaluations under Umbrella vs human labels.",
            "human_evaluator_type": "TREC manual assessors (human judgments used as gold standard for comparison)",
            "agreement_metric": "Kendall's tau",
            "agreement_score": 0.63,
            "reported_loss_aspects": "circularity-induced score inflation (multiple systems with NDCG&gt;0.95 under Umbrella but 0.68–0.72 under manual labels); increased discordant system pairs (18% within top 60); dramatic tau reductions at top ranks (top20 tau=0.44; top15=0.49; top10=0.38; top5=-0.40); invalid evaluation when the evaluator is used as a system component.",
            "qualitative_findings": "When the evaluation metric is the same LLM used inside systems, system development is strongly incentivized to exploit that metric and leaderboard validity breaks down; apparent near-perfect scores under Umbrella do not match human-assessed performance.",
            "advantages_of_llm_judge": "Using Umbrella as a re-ranker improves measured performance under human labels in this simulation (i.e., Umbrella re-ranking increases systems' NDCG under human judgments), which creates incentive to adopt it—this is precisely the mechanism that produces circularity.",
            "experimental_setting": "Applied Umbrella relevance labels as final-stage reranker for all submitted retrieval systems (using the Umbrella labels produced by the track); compared reranked systems' NDCG under Umbrella labels vs manual human labels; reported Kendall's tau and discordant pair percentages across top-K system subsets.",
            "uuid": "e8091.4",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "uwc1_adversarial_run",
            "name_full": "WaterlooClarke adversarial run (uwc1) designed to subvert LLM-based evaluation",
            "brief_description": "A deliberately crafted submission (uwc1) that pooled top documents from many preliminary runs and used LLM-based judgments (reported as GPT-40 in the paper) to generate final rankings, achieving high automatic-evaluation rank (5th) but low manual rank (28th), illustrating vulnerability to manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment",
            "evaluation_task": "Demonstration of manipulation risk of LLM-based evaluation (run-level NDCG comparisons)",
            "dataset_name": "TREC 2024 RAG pooled candidate passages (top-20 from 15 preliminary runs)",
            "judge_model_name": "GPT-40 (as reported in Clarke's description of the uwc1 procedure)",
            "judge_model_details": "Paper states the pool was judged by 'GPT-40' using the prompt described by Faggioli et al.; used to produce LLM-based pairwise and relevance judgments that guided final ranking.",
            "human_evaluator_type": "Manual TREC assessors (used for contrast to show divergence in final ranked positions)",
            "agreement_metric": "NDCG (run-level score comparisons; rank positions compared)",
            "agreement_score": null,
            "reported_loss_aspects": "vulnerability to strategic manipulation: can artifically inflate automatic-evaluation scores; single-run LLM vs human score divergence (example reported: LLM-based run score 0.81 vs manual 0.63 for an outlier)",
            "qualitative_findings": "Shows concrete example where a run optimized to match LLM assessor's preferences attains a high automatic rank but fares poorly under human judgments, evidencing brittleness and manipulability of LLM-only evaluation pipelines.",
            "advantages_of_llm_judge": "Enables automated pooling/labeling to create strong results quickly (which is an advantage in scale but becomes a vulnerability when exploited).",
            "experimental_setting": "Created a pooled candidate set (top 20 from 15 preliminary runs); passages judged by GPT-40 with Faggioli prompt; top-graded passages judged pairwise (substituting LLM assessments for human judgments) and used as keys for final ranking; final run ranked 5th under Umbrella but 28th under manual assessment.",
            "uuid": "e8091.5",
            "source_info": {
                "paper_title": "LLM-based relevance assessment still can't replace human relevance assessment",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "rating": 2
        },
        {
            "paper_title": "A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look",
            "rating": 2
        },
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 2
        },
        {
            "paper_title": "Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation",
            "rating": 2
        },
        {
            "paper_title": "LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant",
            "rating": 2
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 1
        },
        {
            "paper_title": "LLM Evaluators Recognize and Favor Their Own Generations",
            "rating": 1
        }
    ],
    "cost": 0.015988,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM-based Relevance Assessment Still Can't Replace Human Relevance Assessment</h1>
<p>Charles L. A. Clarke<br>University of Waterloo<br>Canada</p>
<h4>Abstract</h4>
<p>The use of large language models (LLMs) for relevance assessment in information retrieval has gained significant attention, with recent studies suggesting that LLM-based judgments provide comparable evaluations to human judgments. Notably, based on TREC 2024 data, Upadhyay et al. [11] make a bold claim that LLM-based relevance assessments, such as those generated by the Umbrella system, can fully replace traditional human relevance assessments in TREC-style evaluations. This paper critically examines this claim, highlighting practical and theoretical limitations that undermine the validity of this conclusion.</p>
<p>First, we question whether the evidence provided by Upadhyay et al. [11] genuinely supports their claim, particularly when the test collection is intended to serve as a benchmark for future research innovations. Second, we submit a system deliberately crafted to exploit automatic evaluation metrics, demonstrating that it can achieve artificially inflated scores without truly improving retrieval quality. Third, we simulate the consequences of circularity by analyzing Kendall's tau correlations under the hypothetical scenario in which all systems adopt Umbrella as a final-stage re-ranker, illustrating how reliance on LLM-based assessments can distort system rankings. Theoretical challenges - including the inherent narcissism of LLMs, the risk of overfitting to LLM-based metrics, and the potential degradation of future LLM performance - that must be addressed before LLM-based relevance assessments can be considered a viable replacement for human judgments.</p>
<h2>1 Introduction</h2>
<p>In early 2023, Faggioli et al. [6] applied GPT-3.5 (text-davinci-003) to fully reassess runs submitted to the TREC 2021 Deep Learning track. They reported Kendall's $\tau=0.86$ for human vs. LLM-based assessment on NDCG@10. A natural conclusion might be that LLMs could now replace humans for routine relevance assessment. Instead, Faggioli et al. [6] issue a warning. While recognizing the potential of LLMs to improve ranking and acknowledging their value as part of the relevance assessment process, they argue strongly against abandoning human assessment. They raise concerns about the potential for unknown biases that LLM-based assessments might introduce. They highlight the issue of circularity, where LLMs evaluate the outputs of other LLMs. Most importantly, their primary concern is that "LLMs are not people." Since information retrieval systems are designed to serve human needs, their evaluation must ultimately reflect human judgment and preferences.</p>
<p>Recently, Upadhyay et al. [11] analyze the retrieval task results from the TREC 2024 RAG track. This retrieval task (or "R task") mirrors a traditional TREC ad hoc retrieval task. Participating systems were tasked with executing 301 queries over the MS MARCO Segment V2.1 collection, producing a ranked set of 100 passages for</p>
<h2>Laura Dietz <br> University of New Hampshire USA</h2>
<p>each query (a "run"). They compare four procedures for assessing runs: 1) fully automatic assessments using the Umbrella LLM-based relevance assessment tool [12]; 2) fully manual assessments using established TREC evaluation protocols for human judgments; 3) a hybrid method where Umbrella filtered the set of passages to be judged; and 4) a hybrid method where humans refined UmbrelA's assessments. In this paper, we focus on the first two procedures: fully automatic and fully manual assessments. Based on their analysis, Upadhyay et al. [11] conclude:</p>
<p>Our results suggest that automatically generated UmbrelA judgments can replace fully manual judgments to accurately capture run-level effectiveness. Surprisingly, we find that LLM assistance does not appear to increase correlation with fully manual assessments, suggesting that costs associated with human-in-the-loop processes do not bring obvious tangible benefits... Our work validates the use of LLMs in academic TREC-style evaluations and provides the foundation for future studies.</p>
<p>We disagree. Not only does their reported correlation fail to provide stronger evidence than that of Faggioli et al. [6], but additional evidence from the track directly contradicts their conclusion. This evidence includes runs submitted by team WaterlooClarke, which were explicitly designed to subvert LLM-based relevance judgments by employing LLM-generated judgments as a final-stage ranker.</p>
<p>Faggioli et al. [6] already demonstrated a strong empirical correlation between manual judgments and LLM judgments, both in terms of inter-annotator agreement and leaderboard correlation, and many other work also observed this empirical correlation. However, after a detailed consideration of competing views, Faggioli et al. [6] concluded that there are too many theoretical concerns before human judgments can be replaced. These concerns, which remain critical to the discussion, have neither been addressed nor refuted in the work of Upadhyay et al. [11, 12].</p>
<p>Conceptually, there is no fundamental difference between an LLM-based relevance assessment and an LLM-based re-ranking method. Both predict an affinity score for a passage to be relevant for a given query. In contrast, human relevance judgments are privileged precisely because they are created by humans, and only humans can provide a gold standard for the evaluation of usefulness. While employing LLMs to train and implement rankers can lead to substantial performance gains, these improvements risk being illusory if they fail to reflect human judgments. The observation that LLM-based relevance judgments closely mimic the outcomes of human relevance judgments suggests that these LLM assessments may themselves represent a strong ranking method, rather than a valid evaluation metric.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Scatter plot extracted from Figure 3 of Upadhyay et al. [11] comparing manual assessment and automatic assessment. Each red dot represents the average performance of a run over all queries. The blue dots plot all run-query combination. The inset provides a closer view of the points in the green rectangle.</p>
<p>We acknowledge the value of work by Upadhyay et al. [12]. In particular, their use of a new collection with fresh queries, which guarantees that LLMs were not trained on this collection. Moreover, their work confirms that the correlation seen by Faggioli et al. [6] is not merely an artifact of training on the test collection. However, the warnings issued by Faggioli et al. [6] remain both valid and increasingly urgent, especially given the growing prevalence of LLM-based relevance assessments in information retrieval tasks.</p>
<h2>2 Reproduction of Umbrela Results</h2>
<p>Kendall's $\tau$ is a suitable metric for showing overall rank correlations on large-scale experiments. Upadhyay et al. [11] report a high overall Kendall's $\tau=0.89$ between manual relevance assessments and automatic Umbrela assessments. However, we note that some submitted systems perform substantially worse than others, making them easy to distinguish by an evaluation. The presence of such under-performing systems can artificially inflate Kendall's $\tau$ scores.</p>
<p>To investigate this effect, we extend the original analysis by excluding the bottom-ranked 15 systems (out of 75) and recomputing Kendall's $\tau$ over the remaining top 60 systems. As shown in Figure 2, this yields a slightly lower but still relatively high Kendall's $\tau$ of 0.84 . This corresponds to approximately $8 \%$ of system swaps, where the two evaluators disagree on which system performs better. Overall, these findings broadly confirm the results reported by Upadhyay et al. [11]. Nevertheless, we observe some notable outliers. For example, one system achieves a high LLM-based evaluation score ( 0.81 ) but a substantially lower manual evaluation score (0.63), as shown in Figure 2. We analyze such discrepancies further in Sections 4.1 and 4.2.</p>
<h2>3 Differences among Top-Performing Systems</h2>
<p>Demonstrating methodological advancements often involves reusing test collections with the goal of surpassing state-of-the-art systems. Identifying meaningful differences among top-performing
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Reproduction of Upadhyay et al. [11]: On top 60 original TREC RAG 24 systems and data, the Umbrela LLM evaluator correlates highly with manual assessors. Only few submitted retrieval systems included approaches from LLM evaluators. Each system represents one dot. Red dots mark systems provided by team WaterlooClarke, which are known to contain LLM evaluations for re-ranking.
runs is therefore critical for measuring significant progress. In contrast to the overall Kendall's $\tau$ of 0.89 between manual and automatic assessments, the correlation weakens substantially among the highest-scoring systems. While Kendall's $\tau$ among the top 60 systems ${ }^{1}$ remains relatively high at 0.85 (corresponding to $8 \%$ system swaps), it drops to $0.51(24 \%$ swaps) among the top 20 systems, and to $0.56(21 \%$ swaps) among the top 15 .</p>
<p>Thus, when using an LLM-based evaluator to demonstrate improvements over state-of-the-art systems, precise agreement with manual judgments is essential at the top of the leaderboard. A Kendall's $\tau \varrho 915$ of 0.56 suggests that automatic Umbrela assessments fail to demonstrate strong alignment with manual judgments at the top of the leaderboard. This misalignment undermines the reliability of automatic evaluations for tracking progress at the frontier of retrieval effectiveness.</p>
<p>Closer consideration of results from Upadhyay et al. [11] further highlights specific discrepancies in top-performing system rankings. Figure 1 reproduces a scatter plot extracted from Figure 3 of that paper, showing the performance of submitted runs (red dots) under manual assessment (y-axis) vs. automatic assessment (x-axis). ${ }^{2}$ Focusing on the top-performing systems (inset of Figure1), discrepancies become evident: for instance, the system ranked highest under automatic evaluation would only place fifth under manual evaluation. Conversely, the top system under manual evaluation ranks sixth under automatic evaluation. Particularly interesting is the case of the run circled in green. While it ranks fifth under automatic evaluation, it drops to 28th under manual evaluation.</p>
<p>These inconsistencies underscore a fundamental limitation of LLM-based assessments, such as those used by UmbreLA, in reliably</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Reranking with an LLM evaluator (Umbrella) improves performance under human relevance labels. This plot compares the original and reranked versions of all TREC RAG 24 systems based on manual assessment.
identifying the best-performing systems. As a result, caution is warranted when using these methods for evaluating and validating progress in retrieval tasks. While some differences could be the attributed to statistical noise - only 27 queries were manually judged, and no error bars are provided - the available evidence remains insufficient to justify the replacement of human judgments. This concern is particularly important when a collection is intended for re-use: claims of a novel system's superiority over existing methods must be supported by improvements that are both statistically meaningful and aligned with manual assessments. At present, such confidence is lacking.</p>
<h2>4 Subverting Automatic Evaluation</h2>
<p>When task relevance labels are generated entirely through a publicly known automatic process, such as Umbrella, the evaluation metric becomes vulnerable to manipulation. For instance, a participant could aggregate the outputs of many rankers, apply the UmbrelA system to this pooled set, and submit the resulting relevance labels as a new system for evaluation. Such a strategy could, in principle, achieve perfect scores across all metrics. Even if the specific LLM-based relevance assessment process includes undisclosed elements, such as the exact prompt or LLM used, participants could approximate the process enough to subvert the automatic evaluations.</p>
<h3>4.1 Empirical Demonstration of the Risk</h3>
<p>The run circled in green in Figure 1 exemplifies this vulnerability. Submitted by team WaterlooClarke as run uwc1, this submission was deliberately designed to subvert the automatic evaluation process. Specifically, the team pooled the top 20 documents from 15 preliminary runs, spanning neural and traditional rankers, with and without query expansion. This pool was then judged by GPT-40 with the prompt described by Faggioli et al. [6]. Top-graded passages were subsequently judged pairwise following the procedure of Clarke et al. [4] but substituting LLM-based assessments for crowdsourced human judgments.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Demonstration of the effects of circularity when using UmbrelA as both evluator and ranker using TREC RAG 24 data. Each submitted retrieval system is first re-ranked with UmbrelA, then evaluated under NDCG with relevance labels from human judges and the UmbrelA evaluator. We see that especially among top ranked systems, the evaluation strategy no longer agrees with human judges on which system is better. Axis ranges are adjusted to display the same top 60 (of 75) systems as in Figure 2.</p>
<p>The final ranking for uwc1 was determined using LLM-based preference judgments as the primary key, LLM-based relevance assessments as the secondary key, and the reciprocal rank fusion [5] of preliminary runs as a tertiary tie-breaker. As intended, this deliberate attempt to manipulate the evaluation process led uwc1 to rank significantly higher under automatic assessment (5th) than under manual assessment (28th).</p>
<h3>4.2 LLM-based Relevance Assessment for Re-Ranking</h3>
<p>As Soboroff [10] recognizes: "Retrieval and evaluation are the same problem. Asking a computer to decide if a document is relevant is no different than using a computer to retrieve documents and rank them in order of predicted degree of relevance." In this sense, LLMbased relevance assessment can be viewed as a specific form of LLMbased re-ranking. To employ an LLM-based relevance assessment tool as a LLM-based re-ranker, one starts with an initial ranking and prompts the LLM to assign a score - expressed as a relevance grade - to each of the top- 4 passages. These passages are then re-ranked according to their assigned relevance grades, preserving their original order among passages with equal grades.</p>
<p>This re-ranking interpretation is further illustrated by another run submitted by team WaterlooClarke, uwc2. For uwc2, the team re-ranked the track's baseline run using the prompt described by Arabzadeh and Clarke [2]. This re-ranking improves the baseline's performance from 9th to 4th place under manual assessments, and from 7th to 3rd place under automatic assessments.</p>
<h3>4.3 Simulation of Circularity</h3>
<p>To explore the potential effects of widespread adoption, we simulate what would have happened if TREC RAG 2024 systems had incorporated Umbrella as part of their pipeline. Following the methodology described in Section 4.2, we apply Umbrella as a final-stage reranker to the outputs of all submitted retrieval systems. For this experiment, we re-rank using the Umbrella relevance labels from the track itself. While the track organizers employed Umbrella for evaluation, the labels it produced could instead have been employed as a final-stage ranker. While we use the labels from the track organizers, any track participant could just as easily have employed Umbrella as a final-stage re-ranker themselves.</p>
<p>Comparing original and re-ranked systems on manual (i.e., human) judgments in Figure 3, we see that re-ranking with the Umbrella judgments consistently improves system performance. TREC and similar evaluation experiments generally allow participants to use any automatic re-ranking process for their submissions. Since Umbrella re-ranking is an entirely automatic process, it immediately loses its value as a measurement tool for those ranking experiments. If any automatic re-ranking process can be used, using the measurement tool itself provides an optimal re-ranking [10]. More generally, if system performance is measured solely by LLM-based tools, system developers are strongly incentivized to incorporate the same tools into their systems.</p>
<p>Once we adopt Umbrella as both a system component and an evaluation metric, it leads to an invalid circular evaluation. This effect is demonstrated in Figure 4, where the Umbrella-re-ranked runs from Figure 3 are evaluated with the same Umbrella relevance labels used for re-ranking and compared against evaluations based on human judgments. We observe a substantial increase in disagreement between the two evaluation methods, with discordant system pairs rising to $18 \%$ within the top 60 systems. As a result, Kendall's $\tau$ drops sharply to 0.63 . This degradation becomes even more pronounced at the top of the leaderboard: Kendall's $\tau$ further decreases to 0.44 among the top 20 systems, 0.49 among the top $15,0.38$ among the top 10 , and even turns negative ( $\tau=-0.40$ ) among the top 5 systems. Under Umbrella-based evaluation, twelve systems now obtain NDCG scores exceeding 0.95 , implying nearperfect ranking performance. Yet the same systems achieve manual NDCG scores only between 0.68 and 0.72 , illustrating substantial score inflation due to circularity.</p>
<p>For publication in peer-reviewed information retrieval research venues it is often necessary to demonstrate that a proposed system significantly outperforms all strong baselines. Under a circular evaluation, however, such findings would no longer be credible. These results highlight the risks of using LLM-based evaluation pipelines without safeguards against feedback loops, especially when test collections are intended for reuse. Taking manually created relevance labels as the gold standard, we conclude that evaluating systems using the Umbrella LLM assessor - when those systems internally apply Umbrella-based re-ranking - results in an invalid circular experimental evaluation.</p>
<h2>5 Automatic Judgments are not Gold Standards</h2>
<p>The prompts used to elicit relevance grades from LLM-based assessment tools resemble instructions typically given to human assessors. However, this resemblance is superficial and we should not be fooled by it. Such prompts merely represent one of many possible ways an LLM-based re-ranking method might assign scores, akin to an a LLM-based point-wise ranker. Despite being commonly referred to as relevance assessments, these scores are not equivalent to the judgments produced by humans.</p>
<p>LLM-based relevance assessments cannot serve as a gold standard because they lack the grounding of a human carrying out an information task necessary to evaluate the usefulness of retrieval systems. A true gold standard must originate from human assessments, as only humans can determine the relevance of information in a way that reflects real-world utility.</p>
<p>Faggioli et al. [6] raised concerns about the potential unknown biases inherent in LLM-based assessments. However, one clear and concerning bias is that LLM-based relevance assessments tend to favor LLM-based ranking systems. Recently, Balog et al. [3] report a detailed evaluation of how LLM-based rankers can influence LLMbased judges, providing the first empirical evidence that LLM judges exhibit "a clear and substantial bias in favor of LLM-based rankers." This bias has been observed in other contexts as well [8, 9, 13], where LLMs demonstrate a form of "narcissism," disproportionately favoring outputs generated by similar models. Furthermore, Alaofi et al. [1] show that LLMs can be deceived through well-crafted prompt attacks embedded in content, leading them to incorrectly judge irrelevant text as relevant. These vulnerabilities highlight not only the susceptibility of LLM-based assessments to manipulation but also their inability to objectively evaluate diverse ranking approaches. Such biases and flaws further undermine the reliability of LLM-based assessments as a substitute for human judgments in critical tasks.</p>
<h2>6 When Automatic Judgments become Useless</h2>
<p>While the uwc1 run demonstrates how a bad actor can strategically subvert an evaluation experiment, it is reasonable to assume that most participants are well-intentioned. These participants are not merely competing to win but are contributing to the creation of reusable test collections that support the development of innovative systems. However, even without malicious intent, the next generation of information systems will likely incorporate the latest advancements in LLMs, including prompting LLMs for relevance. As a result, some ranking methods will inherently embed elements that mirror LLM-based relevance judgments. In a future evaluation experiment, it is plausible that even a well-intentioned participant could inadvertently undermine the evaluation process.</p>
<p>Looking ahead, we anticipate that future retrieval systems will increasingly rely on automatically generated training data to optimize machine learning components. Here, Goodhart's law serves as a cautionary principle [7]: "When a measure becomes a target, it ceases to be a good measure." While the current observed correlation between manual and automatic assessment methods is strong, we predict that this correlation will degrade as developers incorporate LLM-based evaluation components into their systems in more refined ways than our simulation in Section 4.3. Over time, these</p>
<p>systems risk becoming disconnected from the human judgments they are intended to serve. If the entire end-to-end experimental pipeline - from query formulation to relevance labeling - is fully automated, the evaluation process devolves into an LLM assessing its own assessments. The circularity feared by Faggioli et al. [6] is no longer a hypothetical concern; it has already begun to manifest in practice.</p>
<h2>7 Conclusion</h2>
<p>This paper raises serious concerns about the claims made by Upadhyay et al. [11], which presents a preliminary analysis of data from the retrieval ("R") task of the TREC 2024 RAG Track. The author list of Upadhyay et al. [11] includes some of the most prominent experts in the area of information retrieval evaluation. Despite being preliminary, their conclusions strongly imply that LLM-based relevance assessment can replace human relevance assessment - a claim that does not withstand scrutiny. Given the authority of the authors and the strength of their implied conclusions, there is a risk that these findings may gain widespread acceptance within the research community without sufficient critical consideration. Nearly two years ago, Faggioli et al. [6] reached the opposite conclusion based on similar evidence. Their concerns have still not been addressed.</p>
<h2>Acknowledgments</h2>
<p>We have discussed our concerns with some of the authors of Upadhyay et al. [11], and we appreciate their attention and feedback. They also generously provided us with early access to their data to confirm factual statements in this paper.</p>
<p>This material is based in part upon work supported by the National Science Foundation under Grant No. 1846017. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
<h2>References</h2>
<p>[1] Marwah Alaofi, Paul Thomas, Falk Scholer, and Mark Sanderson. 2024. LLMs can be Pooled into Labelling a Document as Relevant: best café near me; this paper is perfectly relevant. In Proceedings of the 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 32-41.
[2] Negar Arabzadeh and Charles L. A. Clarke. 2024. A Comparison of Methods for Evaluating Generative IR. arXiv:2404.04044 [cs.IR] https://arxiv.org/abs/2404. 04044
[3] Krisztian Balog, Donald Metzler, and Zhen Qin. 2025. Rankers, Judges, and Assistants: Towards Understanding the Interplay of LLMs in Information Retrieval Evaluation. arXiv:2503.19092 [cs.IR] https://arxiv.org/abs/2503.19092
[4] Charles L. A. Clarke, Alexandra Vtyurina, and Mark D. Smucker. 2021. Assessing Top-4 Preferences. ACM Transactions on Information Systems, Article 33 (May' 2021), 21 pages.
[5] Gordon V. Cormack, Charles L A Clarke, and Stefan Buettcher. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. 758-759.
[6] Guglielmo Faggioli, Laura Durz, Charles L. A. Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. In SIGIR International Conference on Theory of Information Retrieval. 39-50.
[7] Charles Goodhart. 1975. Problems of Monetary Management: The UK experience in papers in monetary economics. Monetary Economics 1 (1975).
[8] Yiqi Liu, Nafise Sadat Moosavi, and Chenghua Lin. 2023. LLMs as narcissistic evaluators: When ego inflates evaluation scores. arXiv preprint arXiv:2311.09766 (2023).
[9] Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024. LLM Evaluators Recognize and Favor Their Own Generations. arXiv preprint arXiv:2404.13076 (2024).
[10] Ian Soboroff. 2025. Don't Use LLMs to Make Relevance Judgments. Information Retrieval Research 1, 1 (March 2025), 29-46.
[11] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Daniel Campos, Nick Craswell, Ian Soboroff, Hoa Trang Dang, and Jimmy Lin. 2024. A Large-Scale Study of Relevance Assessments with Large Language Models: An Initial Look. arXiv:2411.08275 [cs.IR] https://arxiv.org/abs/2411.08275
[12] Shivani Upadhyay, Ronak Pradeep, Nandan Thakur, Nick Craswell, and Jimmy Lin. 2024. UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor. arXiv:2406.06519 [cs.IR] https://arxiv.org/abs/2406.06519
[13] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large Language Models are not Fair Evaluators. arXiv preprint arXiv:2305.17926 (2023).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Throughout the paper, when we refer to "top-performing systems", we use the manual relevance judgments as a basis for this assessment.
${ }^{2}$ At the time of writing, Upadhyay et al. [11] do not provide a full public data release. They have generously provided limited access to their data for the purpose of confirming factual statements in this paper.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
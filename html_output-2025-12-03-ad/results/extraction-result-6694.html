<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6694 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6694</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6694</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e" target="_blank">Large Language Models Can Be Easily Distracted by Irrelevant Context</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.</p>
                <p><strong>Paper Abstract:</strong> Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6694.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6694.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>code-davinci-002 (Codex-family API model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI Codex-family decoder-only LLM used by the authors to evaluate arithmetic word-problem solving under distracting (irrelevant) context across multiple prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (Codex family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K); evaluations also reported on GSM8K, SVAMP and DROP</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems (grade-school math; multi-step reasoning, arithmetic)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (free-text questions)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school math problems requiring 2–7 reasoning steps; experiments use an "easy" 100-base subset and a 4,000-example GSM-IC sample</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot Chain‑of‑Thought (CoT), zero‑shot CoT (0‑CoT), Least‑to‑Most (LtM), Program prompts (Python), Self‑consistency (sampling+majority vote), instructed prompting (explicit 'ignore irrelevant information' sentence), variants with exemplars containing distractors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro accuracy, macro accuracy (consistency across distractor variants), normalized accuracy (w.r.t. base problem), recall of correct answer among sampled outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative results on GSM-IC-4K (from paper tables): Base (100-example) on clean GSM8K: CoT 95.0%, LtM 94.0%, Program 83.0%, 0‑CoT 44.0% (Table 2). On GSM-IC-4K (distractors added): CoT micro overall 72.4% (macro 6.0%); CoT+SC micro 88.1% (macro 30.0%); LtM micro overall 77.5% (macro 18.0%); LtM+SC micro 93.4% (macro 45.0%); Program micro overall 54.4% (+SC 74.6%); 0‑CoT micro 29.0% (+SC 64.3%). Recall of correct answer within 20 sampled outputs: 99.7% for CoT and LtM; 96.5% for 0‑CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper-level analyses (no low-level probe of activations) show: (1) breakdown by distractor factors: in-topic distractors, role-name lexical overlap, and (less so) in‑range numbers increase fragility; (2) exemplar design and adding an explicit instruction to ignore irrelevant info both teach the model to be more robust; (3) self-consistency (sampling multiple chains and majority voting) recovers correct answers in nearly all sampled outputs even when greedy decoding fails.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Common observed errors: (a) directly using numbers from irrelevant sentences in arithmetic (numerical hijacking); (b) inserting irrelevant information into the chain-of-thought and basing calculations on it; (c) producing inconsistent answers across different distractor variants (very low macro accuracy); (d) program prompts emitting code that includes operations based on irrelevant info (extra additions/subtractions); (e) systematic misinterpretation when role overlap makes pronoun/agent reference ambiguous or misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Comparison in paper: text-davinci-003 often attains higher normalized micro accuracy but sometimes worse macro accuracy than code-davinci-002; self-consistency consistently improves robustness across methods; using exemplars that include distractors or adding explicit instructions improves robustness without degrading clean-task accuracy; increasing number of exemplars (1→4) improved clean GSM8K performance but made prompts more susceptible to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6694.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (instruction-tuned GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction‑tuned OpenAI decoder-only model (GPT‑3.5 family) evaluated by the authors on the same distractor-augmented arithmetic benchmarks to compare robustness with code-davinci-002.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (GPT-3.5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K); also evaluated on GSM8K and SVAMP in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems (grade-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school math; 2–7 reasoning steps (paper focuses on an 'easy' subset and GSM-IC-4K)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>CoT, LtM, 0‑CoT, Program prompts, self-consistency, instructed prompting; evaluated with and without distractors in exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro accuracy, macro accuracy, normalized accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative results on GSM-IC-4K: CoT micro overall 68.4% (macro 9.0%); CoT+Instructed micro ~71.3% (macro 12.0%); LtM micro overall ~76.3% (macro 5.0%). Compared to code-davinci-002, text-davinci-003 in the paper shows better normalized micro accuracy in some settings but often worse macro accuracy (less consistent across distractor variants).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper-level analysis shows text-davinci-003 is sensitive to role-overlap distractors in some settings (macro accuracy can drop drastically), but instruction and exemplar-with-distractor strategies still provide gains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same qualitative failure modes as code-davinci-002: irrelevant numbers used in calculation, irrelevant sentences incorporated into reasoning, inconsistency across distractor variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>As reported in the paper, the instruction-tuned model (text-davinci-003) can be more instruction-following and sometimes achieves higher normalized micro accuracy, but that does not necessarily translate to higher macro (consistency) accuracy; self-consistency and exemplar design help both models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6694.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot prompting method that includes exemplar problems with step-by-step intermediate reasoning to elicit multi-step reasoning from LLMs; evaluated throughout the paper for sensitivity to irrelevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K), GSM8K, SVAMP, DROP</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language with step-by-step exemplar solutions</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>2–7 reasoning steps (grade-school math)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot CoT (1-exemplar and 4-exemplar variants), CoT combined with self-consistency and with explicit instruction to ignore irrelevant info</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro and macro accuracies; normalized accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On GSM-IC-4K (code-davinci-002) CoT micro overall 72.4% (macro 6.0%); CoT with instruction micro 77.8% (macro 15.0%); CoT + self-consistency micro 88.1% (macro 30.0%). Including distractors in exemplars raises micro to ~76.8% and improves macro (to ~14.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors show that CoT chains themselves sometimes mention irrelevant sentences and then perform calculations on them; exemplar choice (including distractors) and explicit instruction can teach the model to ignore distractors; more exemplars (4 vs 1) improved clean performance but often reduced robustness to distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Chain-of-thought answers often either (a) explicitly use irrelevant numbers in calculations, or (b) include irrelevant text in reasoning stream that leads to incorrect arithmetic operations; produces inconsistent outputs across distractor variants (low macro accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Self-consistency and instructed exemplars improve CoT robustness; increasing exemplar count can hurt robustness despite improving clean accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6694.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>0‑CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting variant that does not include exemplars and instead adds the instruction "Let's think step by step:" after the question to elicit stepwise reasoning in a zero-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero-shot reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language with zero-shot 'Let's think step by step' instruction</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot chain-of-thought (0‑CoT); evaluated with and without self-consistency and instruction prefix</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro/macro accuracy, recall across sampled outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>0‑CoT on GSM-IC-4K (code-davinci-002): micro overall 29.0% (macro ~1.0%); with self-consistency (20 samples) micro 64.3% and recall of correct solution within 20 samples 96.5%. Instruction prefix alone produced minor gains (30.5% micro).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Zero-shot CoT without sampling is especially brittle to distractors; sampling (self-consistency) recovers many correct answers, indicating correct reasoning appears in some sampled chains but not reliably in greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very low macro consistency (almost never produces the correct answer across all distractor variants under greedy decoding); vulnerable to using irrelevant numbers when present.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Self-consistency yields large improvements for 0‑CoT (e.g., +35.5 percentage points micro accuracy in reported experiments), indicating sampling-based aggregation is crucial for zero-shot chain behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6694.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LtM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that decomposes a problem into subproblems and solves them sequentially (using CoT for each subproblem); found in the paper to be the most robust prompting technique to irrelevant context in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K), GSM8K, SVAMP, DROP</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems and decomposition into subproblems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language with explicit decomposition prompts (subquestions)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step; works well for problems requiring many steps</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot Least-to-Most (LtM), combined with self-consistency and instructed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro accuracy, macro accuracy, normalized accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On GSM-IC-4K (code-davinci-002) LtM micro overall 77.5% (macro 18.0%); LtM + self-consistency micro 93.4% (macro 45.0%); using exemplars with distractors and instruction further improved micro and macro (e.g., exemplar-with-distractor LtM micro ~80.7%, macro 28.0%; with instruction micro ~82.8%). LtM shows relatively stable performance across problems with many reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>LtM's explicit decomposition makes the model less likely to be derailed by irrelevant context, presumably by focusing the model on subgoals; however, role overlap and in-topic distractors still reduce performance (especially on macro metric).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When distractors are lexically overlapping with problem entities or contain in-range numbers, LtM can still be tricked into using irrelevant numbers or forming incorrect subproblems; macro inconsistency remains non-negligible without SC or instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>LtM benefits substantially from self-consistency; exemplars with distractors and explicit instructions also boost robustness; unlike CoT, LtM's micro accuracy remains relatively stable as required reasoning steps increase.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6694.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program prompting / Program-as-rationale (Python execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt format that asks the model to produce a Python program computing the answer; the generated code is executed externally to obtain the numerical answer, used here to test whether explicit computation mitigates distractibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K), GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic word problems (converted to short Python programs)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>model outputs executable Python program; interpreter run to get numeric answer</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot program prompting (one exemplar), program prompting + self-consistency, exemplars with distractors, instructed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro/macro accuracy (after program execution), normalized accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On GSM-IC-4K (code-davinci-002) Program micro overall 54.4% (macro 5.0%); Program + SC micro 74.6% (macro 13.0%); with exemplars containing distractors Program micro ~62.2% (macro ~9.0%); instruction gave modest gains (e.g., Program+InST micro ~63.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Programs sometimes include operations incorporating irrelevant numbers (the model writes code that subtracts/adds distractor numbers), showing that representing computation explicitly does not eliminate distractibility; exemplars and instructions can reduce but not remove this tendency.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Generated code frequently contains extra lines that use irrelevant variables/numbers from distractor sentences, causing incorrect numeric outputs when executed; parser/variable-name confusions when role-overlap is present can also lead to wrong programs.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Self-consistency (sampling multiple program outputs) substantially improves program-based robustness; exemplar-with-distractor and instruction strategies help but program prompting remains less robust than LtM+SC in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6694.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (sampling + majority voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An output-aggregation technique that samples multiple chain-of-thought (or program) solutions and takes the majority final answer; used in the paper to mitigate distractibility.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>sampled chain-of-thought/program outputs aggregated by final-answer majority vote</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school multi-step</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>combine with CoT, LtM, 0‑CoT, Program; sampling temperature 0.7, 20 samples per problem in main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro/macro accuracy; recall of correct answer among samples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using 20 samples per problem (temperature 0.7) self-consistency boosts micro accuracy substantially: e.g., CoT: 72.4% → 88.1% (code-davinci-002); LtM: 77.5% → 93.4%; Program: 54.4% → 74.6%; for 0‑CoT micro 29.0% → 64.3%. Recall of correct answer within 20 samples: 99.7% for CoT and LtM, 96.5% for 0‑CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>The authors argue SC reveals that correct reasoning chains often exist in the model's sampling distribution even when greedy decoding fails; SC therefore recovers correct answers by aggregating over multiple sampled reasoning trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Even with SC, macro accuracy remains limited (best macro ~45%), meaning SC does not guarantee consistent correctness across different distractor variants; ties and majority-vote heuristics can still select wrong outputs if correct chains are insufficiently probable.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Large improvements with modest sampling (20 samples) across all prompting strategies tested; suggests sampling + aggregation is an effective mitigation for distractibility but not a complete solution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6694.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6694.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instructed prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instructed prompting with explicit 'ignore irrelevant' instruction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple prompting variant used by the authors that prepends an explicit instruction: 'Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.' — shown to improve robustness against irrelevant context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002; text-davinci-003 (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM-IC (derived from GSM8K); also tested on DROP, GSM8K, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems and reading-comprehension (DROP)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>few-shot or zero-shot prompts with added natural-language instruction to ignore irrelevant info</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school arithmetic; DROP reading-comprehension examples</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>explicit instruction prepended to exemplars and test prompt</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>micro/macro accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Adding the 'Feel free to ignore irrelevant information' instruction consistently improves performance across methods. Examples in paper: CoT (code-davinci-002) micro from 72.4% → 77.8% with instruction; LtM from 77.5% → 80.6%; similar gains for exemplars-with-distractors. Instruction also improves performance on DROP and does not degrade clean GSM8K/SVAMP performance.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>The instruction appears to guide models to disregard distractors without extra training; comparable robustness gains to showing exemplars that themselves contain distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Instruction reduces but does not eliminate distractibility — macro accuracy still far from clean-task accuracy for many base problems, and models can still use irrelevant numbers when distractors are highly overlapping or phrased to mislead.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Instruction-tuned models (e.g., text-davinci-003) are particularly receptive to explicit natural-language instructions, but instruction gains are observed across models; instructing plus exemplar-with-distractors is additive in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models Can Be Easily Distracted by Irrelevant Context', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>PaLM: Scaling language modeling with pathways <em>(Rating: 1)</em></li>
                <li>Large language models with controllable working memory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6694",
    "paper_id": "paper-3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "code-davinci-002",
            "name_full": "code-davinci-002 (Codex-family API model)",
            "brief_description": "An OpenAI Codex-family decoder-only LLM used by the authors to evaluate arithmetic word-problem solving under distracting (irrelevant) context across multiple prompting strategies.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002",
            "model_family": "decoder-only transformer (Codex family)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K); evaluations also reported on GSM8K, SVAMP and DROP",
            "task_type": "multi-step arithmetic word problems (grade-school math; multi-step reasoning, arithmetic)",
            "problem_format": "natural-language word problems (free-text questions)",
            "difficulty_level": "grade-school math problems requiring 2–7 reasoning steps; experiments use an \"easy\" 100-base subset and a 4,000-example GSM-IC sample",
            "prompting_method": "few-shot Chain‑of‑Thought (CoT), zero‑shot CoT (0‑CoT), Least‑to‑Most (LtM), Program prompts (Python), Self‑consistency (sampling+majority vote), instructed prompting (explicit 'ignore irrelevant information' sentence), variants with exemplars containing distractors",
            "performance_metric": "micro accuracy, macro accuracy (consistency across distractor variants), normalized accuracy (w.r.t. base problem), recall of correct answer among sampled outputs",
            "performance_value": "Representative results on GSM-IC-4K (from paper tables): Base (100-example) on clean GSM8K: CoT 95.0%, LtM 94.0%, Program 83.0%, 0‑CoT 44.0% (Table 2). On GSM-IC-4K (distractors added): CoT micro overall 72.4% (macro 6.0%); CoT+SC micro 88.1% (macro 30.0%); LtM micro overall 77.5% (macro 18.0%); LtM+SC micro 93.4% (macro 45.0%); Program micro overall 54.4% (+SC 74.6%); 0‑CoT micro 29.0% (+SC 64.3%). Recall of correct answer within 20 sampled outputs: 99.7% for CoT and LtM; 96.5% for 0‑CoT.",
            "internal_analysis": "Paper-level analyses (no low-level probe of activations) show: (1) breakdown by distractor factors: in-topic distractors, role-name lexical overlap, and (less so) in‑range numbers increase fragility; (2) exemplar design and adding an explicit instruction to ignore irrelevant info both teach the model to be more robust; (3) self-consistency (sampling multiple chains and majority voting) recovers correct answers in nearly all sampled outputs even when greedy decoding fails.",
            "failure_modes": "Common observed errors: (a) directly using numbers from irrelevant sentences in arithmetic (numerical hijacking); (b) inserting irrelevant information into the chain-of-thought and basing calculations on it; (c) producing inconsistent answers across different distractor variants (very low macro accuracy); (d) program prompts emitting code that includes operations based on irrelevant info (extra additions/subtractions); (e) systematic misinterpretation when role overlap makes pronoun/agent reference ambiguous or misleading.",
            "scaling_trend": "Comparison in paper: text-davinci-003 often attains higher normalized micro accuracy but sometimes worse macro accuracy than code-davinci-002; self-consistency consistently improves robustness across methods; using exemplars that include distractors or adding explicit instructions improves robustness without degrading clean-task accuracy; increasing number of exemplars (1→4) improved clean GSM8K performance but made prompts more susceptible to distractors.",
            "uuid": "e6694.0",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (instruction-tuned GPT-3.5 family)",
            "brief_description": "An instruction‑tuned OpenAI decoder-only model (GPT‑3.5 family) evaluated by the authors on the same distractor-augmented arithmetic benchmarks to compare robustness with code-davinci-002.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_family": "decoder-only transformer (GPT-3.5 family)",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K); also evaluated on GSM8K and SVAMP in paper",
            "task_type": "multi-step arithmetic word problems (grade-school math)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school math; 2–7 reasoning steps (paper focuses on an 'easy' subset and GSM-IC-4K)",
            "prompting_method": "CoT, LtM, 0‑CoT, Program prompts, self-consistency, instructed prompting; evaluated with and without distractors in exemplars",
            "performance_metric": "micro accuracy, macro accuracy, normalized accuracy",
            "performance_value": "Representative results on GSM-IC-4K: CoT micro overall 68.4% (macro 9.0%); CoT+Instructed micro ~71.3% (macro 12.0%); LtM micro overall ~76.3% (macro 5.0%). Compared to code-davinci-002, text-davinci-003 in the paper shows better normalized micro accuracy in some settings but often worse macro accuracy (less consistent across distractor variants).",
            "internal_analysis": "Paper-level analysis shows text-davinci-003 is sensitive to role-overlap distractors in some settings (macro accuracy can drop drastically), but instruction and exemplar-with-distractor strategies still provide gains.",
            "failure_modes": "Same qualitative failure modes as code-davinci-002: irrelevant numbers used in calculation, irrelevant sentences incorporated into reasoning, inconsistency across distractor variants.",
            "scaling_trend": "As reported in the paper, the instruction-tuned model (text-davinci-003) can be more instruction-following and sometimes achieves higher normalized micro accuracy, but that does not necessarily translate to higher macro (consistency) accuracy; self-consistency and exemplar design help both models.",
            "uuid": "e6694.1",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-thought prompting",
            "brief_description": "A few-shot prompting method that includes exemplar problems with step-by-step intermediate reasoning to elicit multi-step reasoning from LLMs; evaluated throughout the paper for sensitivity to irrelevant context.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K), GSM8K, SVAMP, DROP",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language with step-by-step exemplar solutions",
            "difficulty_level": "2–7 reasoning steps (grade-school math)",
            "prompting_method": "few-shot CoT (1-exemplar and 4-exemplar variants), CoT combined with self-consistency and with explicit instruction to ignore irrelevant info",
            "performance_metric": "micro and macro accuracies; normalized accuracy",
            "performance_value": "On GSM-IC-4K (code-davinci-002) CoT micro overall 72.4% (macro 6.0%); CoT with instruction micro 77.8% (macro 15.0%); CoT + self-consistency micro 88.1% (macro 30.0%). Including distractors in exemplars raises micro to ~76.8% and improves macro (to ~14.0%).",
            "internal_analysis": "Authors show that CoT chains themselves sometimes mention irrelevant sentences and then perform calculations on them; exemplar choice (including distractors) and explicit instruction can teach the model to ignore distractors; more exemplars (4 vs 1) improved clean performance but often reduced robustness to distractors.",
            "failure_modes": "Chain-of-thought answers often either (a) explicitly use irrelevant numbers in calculations, or (b) include irrelevant text in reasoning stream that leads to incorrect arithmetic operations; produces inconsistent outputs across distractor variants (low macro accuracy).",
            "scaling_trend": "Self-consistency and instructed exemplars improve CoT robustness; increasing exemplar count can hurt robustness despite improving clean accuracy.",
            "uuid": "e6694.2",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "0‑CoT",
            "name_full": "Zero-shot chain-of-thought prompting",
            "brief_description": "A prompting variant that does not include exemplars and instead adds the instruction \"Let's think step by step:\" after the question to elicit stepwise reasoning in a zero-shot manner.",
            "citation_title": "Large language models are zero-shot reasoners",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K)",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "natural-language with zero-shot 'Let's think step by step' instruction",
            "difficulty_level": "grade-school multi-step",
            "prompting_method": "zero-shot chain-of-thought (0‑CoT); evaluated with and without self-consistency and instruction prefix",
            "performance_metric": "micro/macro accuracy, recall across sampled outputs",
            "performance_value": "0‑CoT on GSM-IC-4K (code-davinci-002): micro overall 29.0% (macro ~1.0%); with self-consistency (20 samples) micro 64.3% and recall of correct solution within 20 samples 96.5%. Instruction prefix alone produced minor gains (30.5% micro).",
            "internal_analysis": "Zero-shot CoT without sampling is especially brittle to distractors; sampling (self-consistency) recovers many correct answers, indicating correct reasoning appears in some sampled chains but not reliably in greedy decoding.",
            "failure_modes": "Very low macro consistency (almost never produces the correct answer across all distractor variants under greedy decoding); vulnerable to using irrelevant numbers when present.",
            "scaling_trend": "Self-consistency yields large improvements for 0‑CoT (e.g., +35.5 percentage points micro accuracy in reported experiments), indicating sampling-based aggregation is crucial for zero-shot chain behavior.",
            "uuid": "e6694.3",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "LtM",
            "name_full": "Least-to-most prompting",
            "brief_description": "A prompting strategy that decomposes a problem into subproblems and solves them sequentially (using CoT for each subproblem); found in the paper to be the most robust prompting technique to irrelevant context in many settings.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K), GSM8K, SVAMP, DROP",
            "task_type": "multi-step arithmetic word problems and decomposition into subproblems",
            "problem_format": "natural-language with explicit decomposition prompts (subquestions)",
            "difficulty_level": "grade-school multi-step; works well for problems requiring many steps",
            "prompting_method": "few-shot Least-to-Most (LtM), combined with self-consistency and instructed prompting",
            "performance_metric": "micro accuracy, macro accuracy, normalized accuracy",
            "performance_value": "On GSM-IC-4K (code-davinci-002) LtM micro overall 77.5% (macro 18.0%); LtM + self-consistency micro 93.4% (macro 45.0%); using exemplars with distractors and instruction further improved micro and macro (e.g., exemplar-with-distractor LtM micro ~80.7%, macro 28.0%; with instruction micro ~82.8%). LtM shows relatively stable performance across problems with many reasoning steps.",
            "internal_analysis": "LtM's explicit decomposition makes the model less likely to be derailed by irrelevant context, presumably by focusing the model on subgoals; however, role overlap and in-topic distractors still reduce performance (especially on macro metric).",
            "failure_modes": "When distractors are lexically overlapping with problem entities or contain in-range numbers, LtM can still be tricked into using irrelevant numbers or forming incorrect subproblems; macro inconsistency remains non-negligible without SC or instruction.",
            "scaling_trend": "LtM benefits substantially from self-consistency; exemplars with distractors and explicit instructions also boost robustness; unlike CoT, LtM's micro accuracy remains relatively stable as required reasoning steps increase.",
            "uuid": "e6694.4",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Program",
            "name_full": "Program prompting / Program-as-rationale (Python execution)",
            "brief_description": "A prompt format that asks the model to produce a Python program computing the answer; the generated code is executed externally to obtain the numerical answer, used here to test whether explicit computation mitigates distractibility.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K), GSM8K",
            "task_type": "arithmetic word problems (converted to short Python programs)",
            "problem_format": "model outputs executable Python program; interpreter run to get numeric answer",
            "difficulty_level": "grade-school multi-step arithmetic",
            "prompting_method": "few-shot program prompting (one exemplar), program prompting + self-consistency, exemplars with distractors, instructed prompting",
            "performance_metric": "micro/macro accuracy (after program execution), normalized accuracy",
            "performance_value": "On GSM-IC-4K (code-davinci-002) Program micro overall 54.4% (macro 5.0%); Program + SC micro 74.6% (macro 13.0%); with exemplars containing distractors Program micro ~62.2% (macro ~9.0%); instruction gave modest gains (e.g., Program+InST micro ~63.2%).",
            "internal_analysis": "Programs sometimes include operations incorporating irrelevant numbers (the model writes code that subtracts/adds distractor numbers), showing that representing computation explicitly does not eliminate distractibility; exemplars and instructions can reduce but not remove this tendency.",
            "failure_modes": "Generated code frequently contains extra lines that use irrelevant variables/numbers from distractor sentences, causing incorrect numeric outputs when executed; parser/variable-name confusions when role-overlap is present can also lead to wrong programs.",
            "scaling_trend": "Self-consistency (sampling multiple program outputs) substantially improves program-based robustness; exemplar-with-distractor and instruction strategies help but program prompting remains less robust than LtM+SC in experiments.",
            "uuid": "e6694.5",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Self-consistency",
            "name_full": "Self-consistency (sampling + majority voting)",
            "brief_description": "An output-aggregation technique that samples multiple chain-of-thought (or program) solutions and takes the majority final answer; used in the paper to mitigate distractibility.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K)",
            "task_type": "multi-step arithmetic word problems",
            "problem_format": "sampled chain-of-thought/program outputs aggregated by final-answer majority vote",
            "difficulty_level": "grade-school multi-step",
            "prompting_method": "combine with CoT, LtM, 0‑CoT, Program; sampling temperature 0.7, 20 samples per problem in main experiments",
            "performance_metric": "micro/macro accuracy; recall of correct answer among samples",
            "performance_value": "Using 20 samples per problem (temperature 0.7) self-consistency boosts micro accuracy substantially: e.g., CoT: 72.4% → 88.1% (code-davinci-002); LtM: 77.5% → 93.4%; Program: 54.4% → 74.6%; for 0‑CoT micro 29.0% → 64.3%. Recall of correct answer within 20 samples: 99.7% for CoT and LtM, 96.5% for 0‑CoT.",
            "internal_analysis": "The authors argue SC reveals that correct reasoning chains often exist in the model's sampling distribution even when greedy decoding fails; SC therefore recovers correct answers by aggregating over multiple sampled reasoning trajectories.",
            "failure_modes": "Even with SC, macro accuracy remains limited (best macro ~45%), meaning SC does not guarantee consistent correctness across different distractor variants; ties and majority-vote heuristics can still select wrong outputs if correct chains are insufficiently probable.",
            "scaling_trend": "Large improvements with modest sampling (20 samples) across all prompting strategies tested; suggests sampling + aggregation is an effective mitigation for distractibility but not a complete solution.",
            "uuid": "e6694.6",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Instructed prompting",
            "name_full": "Instructed prompting with explicit 'ignore irrelevant' instruction",
            "brief_description": "A simple prompting variant used by the authors that prepends an explicit instruction: 'Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.' — shown to improve robustness against irrelevant context.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "code-davinci-002; text-davinci-003 (evaluated)",
            "model_family": "decoder-only transformer",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM-IC (derived from GSM8K); also tested on DROP, GSM8K, SVAMP",
            "task_type": "multi-step arithmetic word problems and reading-comprehension (DROP)",
            "problem_format": "few-shot or zero-shot prompts with added natural-language instruction to ignore irrelevant info",
            "difficulty_level": "grade-school arithmetic; DROP reading-comprehension examples",
            "prompting_method": "explicit instruction prepended to exemplars and test prompt",
            "performance_metric": "micro/macro accuracy",
            "performance_value": "Adding the 'Feel free to ignore irrelevant information' instruction consistently improves performance across methods. Examples in paper: CoT (code-davinci-002) micro from 72.4% → 77.8% with instruction; LtM from 77.5% → 80.6%; similar gains for exemplars-with-distractors. Instruction also improves performance on DROP and does not degrade clean GSM8K/SVAMP performance.",
            "internal_analysis": "The instruction appears to guide models to disregard distractors without extra training; comparable robustness gains to showing exemplars that themselves contain distractors.",
            "failure_modes": "Instruction reduces but does not eliminate distractibility — macro accuracy still far from clean-task accuracy for many base problems, and models can still use irrelevant numbers when distractors are highly overlapping or phrased to mislead.",
            "scaling_trend": "Instruction-tuned models (e.g., text-davinci-003) are particularly receptive to explicit natural-language instructions, but instruction gains are observed across models; instructing plus exemplar-with-distractors is additive in many cases.",
            "uuid": "e6694.7",
            "source_info": {
                "paper_title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "PaLM: Scaling language modeling with pathways",
            "rating": 1
        },
        {
            "paper_title": "Large language models with controllable working memory",
            "rating": 1
        }
    ],
    "cost": 0.023001,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Can Be Easily Distracted by Irrelevant Context</h1>
<p>Freda Shi ${ }^{12 <em>}$ Xinyun Chen ${ }^{1 </em>}$ Kanishka Misra ${ }^{13}$ Nathan Scales ${ }^{1}$ David Dohan ${ }^{1}$ Ed Chi ${ }^{1}$<br>Nathanael Schärli ${ }^{1}$ Denny Zhou ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information. ${ }^{1}$</p>
<h2>1. Introduction</h2>
<p>Prompting large language models performs decently well in a variety of domains (Brown et al., 2020; Chowdhery et al., 2022, inter alia). However, for most of theses evaluation benchmarks, all the information provided in the problem description is relevant to the problem solution, as the problems in exams. This is different from real-world situations, where problems usually come with several pieces of contextually</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Original Problem</h2>
<p>Jessica is six years older than Claire. In two years, Claire will be 20 years old. How old is Jessica now?</p>
<h2>Modified Problem</h2>
<p>Jessica is six years older than Claire. In two years, Claire will be 20 years old. Twenty years ago, the age of Claire's father is 3 times of Jessica's age. How old is Jessica now?
Standard Answer 24
Table 1. An example problem from GSM-IC. An irrelevant sentence (italic and underlined) that does not affect the standard answer is added immediately before the question.
related information, which may or may not be relevant to the problems that we want to solve. We have to identify what information is actually necessary during solving those problems. Studies in psychology have shown that irrelevant information may significantly decrease some children and even adults problem-solving accuracy (Hoyer et al., 1979; Pasolunghi et al., 1999; Marzocchi et al., 2002, inter alia).
In this work, we study the distractibility of large language models for various prompting techniques; i.e., how is large language model prompting affected by irrelevant context, and what strategies can be used to improve performance? To measure distractibility, we construct the GSM-IC dataset, a grade-school math problem dataset derived from GSM8K (Cobbe et al., 2021) and introduce two different metrics. In contrast to prior work that derives benchmark variations by substituting sentences of the base problems with variations (Patel et al., 2021; Kumar et al., 2021, inter alia), we keep the base problem description and add to it one irrelevant sentence, while making sure that it does not affect the solution of the problem (Table 1).</p>
<p>We use Codex (code-davinci-002) and GPT-3.5 ( text-davinci-003) in the GPT3 model family to evaluate state-of-the-art prompting techniques on GSM-IC, ${ }^{2}$ including chain-of-thought prompting (CoT; Wei et al., 2022), zero-shot chain-of-thought prompting (0-CO T; Kojima et al., 2022), least-to-most-prompting (LTM; Zhou et al., 2022), and prompting with programs (PROGRAM;</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Chowdhery et al., 2022). We find that their performance on GSM-IC greatly decreases compared to the original GSM8K (without irrelevant context). We then investigate several approaches to mitigate this weakness, including self-consistency (Wang et al., 2022c) and adding irrelevant information to the exemplars in the prompt. In addition to demonstrating how to handle irrelevant information via exemplars, we also investigate the usage of task-specific instructions (Wei et al., 2021; Sanh et al., 2021; Ouyang et al., 2022; Suzgun et al., 2022; Chung et al., 2022), where we prepend an instruction sentence "feel free to ignore irrelevant information in the problem description" to the exemplars. We summarize our key findings below:</p>
<ol>
<li>All investigated prompting techniques are sensitive to irrelevant information in the problem description. In particular, among the original problems that can be solved by baseline prompts with greedy decoding, no more than $18 \%$ of them can be consistently solved for all types of irrelevant information, showing that the large language model is easily distracted and produces inconsistent predictions when adding a small amount of irrelevant information to the problem description.</li>
<li>Self-consistency improves the performance of all prompting techniques on GSM-IC. In particular, the recall rate of the correct answer for GSM-IC is as high as $99.7 \%$ with 20 samples per problem, i.e., at least one of the 20 solutions result in the correct final answer, which means that using multiple samples allows the model to almost always retrieve the correct answer.</li>
<li>Adding irrelevant information to the exemplars shown in the prompt consistently boosts the performance, and the same holds for adding an instruction to ignore irrelevant context. This suggests that language models are-to some extent-able to learn to ignore irrelevant information by following examples or instructions.</li>
<li>We identify different factors of the irrelevant information that affect the model's sensitivity to irrelevant context. Our breakdown analysis shows that varying the numbers in the irrelevant information does not notably change the model performance, while the degree of lexical overlap with the original problem description matters.</li>
</ol>
<p>Filtering out irrelevant information is essential for handling real-world tasks. Our evaluation indicates that despite the strong performance on challenging reasoning problems, state-of-the-art language models still have fundamental weaknesses in context understanding and identifying the relevant information from the input. Our findings suggest that in order to gain a more holistic understanding of the reasoning capability of language models, future work should also consider the model sensitivity to irrelevant context, in addition to solving more challenging problems.</p>
<h2>2. Related Work</h2>
<p>Few-shot prompting. Few-shot prompting (Brown et al., 2020; Chowdhery et al., 2022, inter alia) has been significantly boosted with various techniques, including generating intermediate steps (Ling et al., 2017; Cobbe et al., 2021; Nye et al., 2021; Wei et al., 2022; Suzgun et al., 2022; Shi et al., 2022b, inter alia), problem decomposition (Zhou et al., 2022; Drozdov et al., 2022; Dohan et al., 2022; Khot et al., 2022; Press et al., 2022, inter alia), generating programs (Austin et al., 2021; Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022, inter alia), marginalizing intermediate steps that share the same result (Wang et al., 2022c; Shi et al., 2022a), and ensemble (Wang et al., 2022b; Drozdov et al., 2022). In addition, Kojima et al. (2022) demonstrate that appropriate hint in prompts also leads to decent performance, even without any exemplar. In this work, we examine these cutting-edge prompting techniques (Wei et al., 2022; Zhou et al., 2022; Kojima et al., 2022; Wang et al., 2022c) on our benchmark, and demonstrate that they are sensitive to irrelevant input context.</p>
<p>Natural language benchmarks with input perturbations. There has been a long line of work on adding input perturbations for natural language tasks, including model-agnostic input transformations (Liang et al., 2022; Ravichander et al., 2022, inter alia) and adversarial example generation against individual models (Jia \&amp; Liang, 2017; Shi et al., 2018; Morris et al., 2020; Wang et al., 2021). In particular, prior work has constructed arithmetic reasoning benchmarks through paraphrasing or rewriting sentences in the base problems from clean datasets (Patel et al., 2021; Kumar et al., 2021). Meanwhile, Liang et al. (2022) evaluate various large language models under several metrics, including accuracy, robustness, fairness, etc. Specifically, the input transformations in their robustness evaluation include semanticspreserving and semantics-altering perturbations, such as injecting typos and modifying sentences to change the groundtruth classification labels. In contrast the above work where the meaning of problem descriptions may be changed with perturbations, we keep all sentences in the original problem description, and introduce an irrelevant sentence that is ensured not to affect the standard answer.</p>
<p>Natural language benchmarks with irrelevant input context. Jia \&amp; Liang (2017) have shown that neural question answering systems are largely affected by adversarial distracting sentences, whereas follow up work (Khashabi et al., 2017; Ni et al., 2019) proposes learning strategies that mitigate the problem. Similar issues have been found for general-purpose pretrained language models, on the tasks of factual reasoning (Kassner \&amp; Schütze, 2020; Pandia \&amp; Ettinger, 2021; Misra et al., 2023; Li et al., 2022), code generation (Jones \&amp; Steinhardt, 2022), and syntactic generalization (Chaves \&amp; Richter, 2021). In particular, Li et al.</p>
<p>(2022) evaluated T5 (Raffel et al., 2020) and PaLM (Chowdhery et al., 2022) with few-shot prompts, and proposed knowledge-aware finetuning that finetunes the model on problems with counterfactual and irrelevant context, which strengthens the model robustness to noisy context. In our evaluation, we show that without training or finetuning, adding irrelevant context into demonstrations in the prompt also mitigates the distractibility of the underlying language model and significantly improves the model performance on our GSM-IC benchmark.</p>
<p>There exist some logical reasoning benchmarks that contain irrelevant content in task descriptions (Weston et al., 2015; Sinha et al., 2019; Clark et al., 2021; Han et al., 2022; Tafjord et al., 2020, inter alia). However, previous work largely focuses on designing models that require extra training, and prompting alone still hardly achieves the same level of performance as finetuned models for these tasks (Han et al., 2022; Creswell et al., 2022). In our work, we focus on arithmetic reasoning, where prompting techniques have achieved the state-of-the-art results, e.g., on GSM8K, while we show that adding a single irrelevant sentence into the problem description significantly degrades the performance.</p>
<p>Prompting with noisy ground truth. A line of work studies the model performance with incorrect prompting exemplars, i.e., the example problems are paired with wrong answers (Min et al., 2022; Kim et al., 2022). In addition, prior work has investigated the model sensitivity to other parts of the prompt, such as instruction tuning with misleading and irrelevant instructions (Webson \&amp; Pavlick, 2021) and wrong reasoning steps in the examples (Madaan \&amp; Yazdanbakhsh, 2022; Wang et al., 2022a). In particular, Madaan \&amp; Yazdanbakhsh (2022) conclude that the correctness of numbers and equations in chain-of-thought prompts does not play a key role in model performance, but using wrong entities and removing either equations or text explanation in the reasoning steps drastically hamper the performance. Different from this line of work, we always include correct answers to example problems in the prompt, and ensure that the irrelevant context added to the problem description does not change the ground truth answer. We show that the model performance significantly drops when presented with irrelevant context in problem descriptions, and different distributions of numbers and entities in the irrelevant context also lead to different levels of performance degradation.</p>
<h2>3. The GSM-IC Dataset</h2>
<p>In this section, we introduce the creation process of the GSM-IC dataset (§3.1) and the evaluation metrics (§3.2).</p>
<h3>3.1. Dataset Creation</h3>
<p>We randomly choose 1,000 problems from the GSM8K training set as a development set. To construct our base dataset,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">COT</th>
<th style="text-align: center;">LTM</th>
<th style="text-align: center;">Program</th>
<th style="text-align: center;">0-COT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">44.0</td>
</tr>
<tr>
<td style="text-align: center;">+ SC</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">76.0</td>
</tr>
</tbody>
</table>
<p>Table 2. Accuracy ( $\times 100$ ) on the base 100 -example dataset using code-davinci-002. See Table 3 for results with text-davinci-003.</p>
<h2>Original Problem</h2>
<p>Jeanne wants to ride the Ferris wheel, the roller coaster, and the bumper cars. The Ferris wheel costs 5 tickets, the roller coaster costs 4 tickets and the bumper cars cost 4 tickets. Jeanne has 5 tickets. [Irrelevant Sentence] How many more tickets should Jeanne buy?</p>
<div class="codehilite"><pre><span></span><code><span class="n">Options</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">Irrelevant</span><span class="w"> </span><span class="n">Sentence</span><span class="w"> </span><span class="n">Topic</span>
<span class="ow">In</span><span class="o">-</span><span class="n">Topic</span><span class="w"> </span><span class="o">[</span><span class="n">ROLE</span><span class="o">]</span><span class="w"> </span><span class="n">rides</span><span class="w"> </span><span class="o">[</span><span class="n">NUMBER</span><span class="o">]</span><span class="w"> </span><span class="n">kilometers</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bus</span><span class="w"> </span><span class="n">station</span><span class="w"> </span><span class="k">every</span><span class="w"> </span><span class="nf">day</span><span class="p">.</span>
<span class="k">Off</span><span class="o">-</span><span class="n">Topic</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">shoe</span><span class="w"> </span><span class="k">size</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="o">[</span><span class="n">ROLE</span><span class="o">]</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="o">[</span><span class="n">NUMBER</span><span class="o">]</span><span class="p">.</span>
</code></pre></div>

<p>Options for [ROLE]: Lexical Overlap with Original Characters?
Yes Jeanne's father, Jeanne's sister, Jeanne's neighbor...
No Ada, Jack, Mary, Tom...
$\qquad$
Options for [NUMBER]
In-Range
Out-of-Range
$5,6,7,8 \ldots$
$100,1000,5000 \ldots$</p>
<p>Figure 1. Illustration of the considered factors when creating the GSM-IC dataset. Best viewed in color.
we then choose 100 problems from this development set that can be correctly solved by at least one of the prompting techniques mentioned in this paper; ${ }^{3}$ that is, our base dataset is an "easy" subset of GSM8K (Table 2). Each base problem requires two to seven reasoning steps to solve. ${ }^{4}$ Among the 100 base problems, 60 of them can be solved with two reasoning steps. The full dataset statistics can be found in Appendix A.</p>
<p>We then generate the examples of our new dataset by adding to each base problem one sentence containing irrelevant information. We use a template-based method (Figure 1) to generate these sentences, which can be characterized by the following three factors:</p>
<ul>
<li>Topic of the inserted sentence. We write templates for both in-topic and off-topic sentences. In-topic sentences are closely related to the topic of the original problem, whereas off-topic sentences are about a different topic.</li>
<li>Role name overlap. Most sentence templates contain some role name blanks, which can be filled with names that may or may not overlap with the role names that occur in the problem. For blank fillers that have overlap with original role names, we: (1) randomly pick a role name A from the original problem description and (2) create the blank fillers with template such as A's father and A's sister.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<ul>
<li>Range of numbers. Since we focus on arithmetic reasoning, most sentence templates also contain a number blank. We can choose to fill in the number blank with a number of similar or different magnitude to those in the original problem description. Concretely, for a number $a$, if there exists a number $b$ in the original problem description or solution such that $\frac{1}{10} \leq \frac{a}{b} \leq 10$, we consider $a$ as an in-range number, and otherwise an out-of-range number. Since the standard answer to GSM8K problems are all positive integers, we only consider positive integers as the number blank fillers.</li>
</ul>
<p>We manually verify that (1) all the generated sentences are acceptable in English and that (2) adding them does not affect the standard solution of the base problem. Because the above factors are orthogonal, we generate for each base example a set of derived examples with different factor combinations. The full GSM-IC benchmark consists of 58,052 examples. More details about the dataset creation process can be found in Appendix A.</p>
<h3>3.2. Evaluation Metrics</h3>
<p>For a problem $p$, we denote its standard solution by $s(p)$, and the solution of method $\mathcal{M}$ by $\mathcal{M}(p)$. To evaluate the distractibility of $\mathcal{M}$, we consider the following two metrics:</p>
<ul>
<li>Micro accuracy $A c c_{\text {micro }}(\mathcal{M} ; \mathcal{P})$ is the average accuracy of method $\mathcal{M}$ over all the test problems $\mathcal{P}$.</li>
</ul>
<p>$$
A c c_{\text {micro }}(\mathcal{M} ; \mathcal{P})=\frac{\sum_{p \in \mathcal{P}} \mathbb{1}[\mathcal{M}(p)=s(p)]}{|\mathcal{P}|}
$$</p>
<p>This means that the micro accuracy weighs all the individual test problems equally.</p>
<ul>
<li>Macro accuracy $A c c_{\text {macro }}(\mathcal{M} ; \mathcal{B})$ is the average accuracy of method $\mathcal{M}$ over classes of test problems, where each class $\mathcal{P}(b)$ consists of the set of test examples derived from the base example $b \in \mathcal{B}$. We define $\mathcal{M}$ 's prediction for a class $\mathcal{P}(b)$ to be correct if and only if $\mathcal{M}$ 's prediction for all problems in this class are correct.</li>
</ul>
<p>$$
A c c_{\text {macro }}(\mathcal{M} ; \mathcal{B})=\frac{\sum_{b \in \mathcal{B}} \mathbb{1}\left[\bigwedge_{p \in \mathcal{P}(b)}[\mathcal{M}(p)=s(p)]\right]}{|\mathcal{B}|}
$$</p>
<p>This means that the macro accuracy is the fraction of base problems that can be consistently solved no matter what irrelevant sentence is being added.</p>
<ul>
<li>Normalized accuracy measures how a method is affected by the distractors, considering its accuracy on base problems. For a micro or macro accuracy $a_{\mathcal{M}}$ achieved by method $\mathcal{M}$, we calculate its corresponding normalized accuracy by</li>
</ul>
<p>$$
\operatorname{norm}\left(a_{\mathcal{M}} ; \mathcal{M}\right)=\frac{a_{\mathcal{M}}}{n_{\mathcal{M}}}
$$</p>
<p>where $n_{\mathcal{M}}$ denotes the base problem accuracy of method $\mathcal{M}$ (Table 2).</p>
<h2>4. Investigated Solutions</h2>
<p>In the following section, we review the investigated prompting techniques (§4.1), present the formats of our prompts (§4.2), and introduce instructed prompting (§4.3).</p>
<h3>4.1. Base Techniques</h3>
<p>Chain-of-thought prompting (CoT; Wei et al., 2022) is a prompting technique that guides the language models to solve a problem in a step-by-step manner. By presenting exemplars that solve the corresponding problems with intermediate reasoning steps in the prompts, COT significantly improves the reasoning performance over direct answer prediction without such intermediate reasoning steps.</p>
<p>Zero-shot chain-of-thought prompting (0-CO T; Kojima et al., 2022) is a variation of COT where the prompt does not contain any exemplar. Instead, the model is prompted directly with the problem of interest followed by the instruction "Let's think step by step:".</p>
<p>Least-to-most prompting (LTM; Zhou et al., 2022) teaches language models to (1) break down a problem into subproblems, and (2) solve those subproblems sequentially using CoT. The final answer is that to the last subproblem.</p>
<p>Program prompts (Program; Chowdhery et al., 2022) represent the arithmetic reasoning process as a program. Following prior work on solving GSM8K problems with code (Chowdhery et al., 2022; Gao et al., 2022; Chen et al., 2022), we include a Python program as the problem solution in the prompt, and execute the generated Python code using an external Python interpreter to obtain the final answer.</p>
<p>Self-consistency (SC; Wang et al., 2022c; Shi et al., 2022a) may further boost the reasoning performance by marginalizing over intermediate reasoning steps that share the same final result. In practice, SC can be implemented by (1) sampling several solutions from the large language model and (2) taking the majority vote. Note that SC is orthogonal to above techniques, and can be combined with any of them.</p>
<h3>4.2. Prompt Design</h3>
<p>We present some example prompts used in our experiments (Figure 2). For few-shot prompting techniques (i.e., COT, LTM and Program), the input prompt includes exemplar problems and their solutions before the problem of interest. In order to keep simplicity and avoid over-fitting in prompt engineering, we follow Zhou et al. (2022) on exemplar creation; that is, we only use one simple exemplar for our main experiments. This exemplar is either based on the [Original Problem] or the [Problem with Irrelevant Context], which</p>
<p>[Original Problem] =
Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together?
[Problem with Irrelevant Context] =
Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. $\underline{\text { Liz }}$ has 4 peaches. How many apples do they have together?
[CoT Solution] =
A: Anna has 2 more apples than Elsa, so Anna has $2+5=7$ apples. Elsa and Anna have $5+7=12$ apples together. The answer is 12 .
[LtM Solution] =
A: Let's break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together?</p>
<ol>
<li>Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples.</li>
<li>Elsa and Anna have $5+7=12$ apples together.
[PROGRAM Solution] =
A: Let's solve the problem by a Python program:
Elsa_apples $=5$
Anna_apples $=2+$ Elsa_apples
Elsa_Anna_apples = Elsa_apples + Anna_apples
print(Elsa_Anna_apples)
[Problem of Interest] =
Q: Jeanne wants to ride the Ferris wheel, the roller coaster, and the bumper cars. The Ferris wheel costs 5 tickets, the roller coaster costs 4 tickets and the bumper cars cost 4 tickets. Jeanne has 5 tickets. Jeanne's neighbor rides 8 kilometers to the bus station every day. How many more tickets should Jeanne buy?</li>
</ol>
<p>CoT Prompt
[Original Problem]
[CoT Solution]
Q: [Problem of Interest]
A:
0 -CoT Prompt (No Exemplar Problem)
Q: [Problem of Interest]
A: Let's think step by step:
LtM Prompt
[Original Problem]
[LtM Solution]
Q: [Problem of Interest]
A: Let's break down this problem:
PROGRAM Prompt
[Original Problem]
[ProGRAM Solution]
Q: [Problem of Interest]
A: Let's solve the problem by a Python program:
Instructed CoT Prompt
Solve grade school math problems. Feel free to ignore irrelevant information given in the questions.
[Original Problem]
[CoT Solution]
Q: [Problem of Interest]
A:</p>
<p>Figure 2. Prompt formats for the investigated techniques on the right, which are constructed from building blocks on the left (best viewed in color). The [Problem with Irrelevant Context] is obtained by adding an irrelevant sentence (italic and underlined) to the original problem description and it can be used as an alternative to the [Original Problem] in the prompts on the right. In these prompts, identifiers highlighted and wrapped by brackets (e.g., [Problem of Interest]) are replaced by the contents of the corresponding building blocks. The prompts for all settings can be found in Appendix C.
allows us to investigate the effect of irrelevant information in the prompt exemplar. For 0 -CoT, we adhere to Kojima et al. (2022) and directly present the problem of interest followed by "A: Let's think step by step:".</p>
<h3>4.3. Instructed Prompting</h3>
<p>In addition to presenting irrelevant information in the exemplars, we also investigate whether natural language instructions help language models ignore irrelevant context and become less distracted. Extending the line of work (Suzgun et al., 2022; Sanh et al., 2021; Ouyang et al., 2022) that includes a general task description before exemplars, we add the sentence "Solve grade school math problems. Feel free to ignore irrelevant information given in the questions." before our exemplars in the prompt (Figure 2), which explicitly instructs the language model to ignore irrelevant information in the problem description.</p>
<h2>5. Experiments</h2>
<p>Being mindful of the experiment costs, we uniformly sample 4,000 examples from the GSM-IC dataset (denoted by GSM-IC-4K) ${ }^{5}$ for evaluation and analysis purposes throughout this paper. Unless otherwise specified, we mainly use code-davinci-002 in our experiments, and we also evaluate text-davinci-003 which is a model trained with RLHF to better follow instructions (Ouyang et al., 2022). For experiments without self-consistency decoding, we use greedy decoding (i.e., temperature $\tau=0$ ); for self-consistency experiments that require multiple samples for a problem, we sample 20 responses with temperature $\tau=0.7$ following Wang et al. (2022c).</p>
<h3>5.1. Main Results on GSM-IC</h3>
<p>We compare the performance of different prompting techniques on GSM-IC-4K (Table 3), in terms of both micro</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Micro Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Macro Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2 Steps</td>
<td style="text-align: center;">$&gt;2$ Steps</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Norm</td>
<td style="text-align: center;">2 Steps</td>
<td style="text-align: center;">$&gt;2$ Steps</td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Norm</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/o Irrelevant Context, code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">6.3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + InST.</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">15.8</td>
</tr>
<tr>
<td style="text-align: center;">0 -CoT</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;">0 -CoT +INST.</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: center;">LtM + InST.</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">18.3</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">26.6</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">47.4</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr>
<td style="text-align: center;">Program + InST.</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">7.2</td>
</tr>
<tr>
<td style="text-align: center;">CoT + SC</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">31.3</td>
</tr>
<tr>
<td style="text-align: center;">0 -CoT + SC</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">64.3</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.3</td>
</tr>
<tr>
<td style="text-align: center;">LtM + SC</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">45.5</td>
</tr>
<tr>
<td style="text-align: center;">Program + SC</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/o Irrelevant Context, text-davinci-003</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: center;">CoT + InST.</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">70.3</td>
<td style="text-align: center;">71.3</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">12.5</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">15.0</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">LtM + InST.</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/ Irrelevant Context, code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;">CoT + InST.</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">17.9</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: center;">LtM + InST.</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">29.8</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">67.0</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">10.8</td>
</tr>
<tr>
<td style="text-align: center;">Program + InST.</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">54.8</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">14.5</td>
</tr>
</tbody>
</table>
<p>Table 3. Micro and macro accuracies ( $\times 100$ ) on the GSM-IC-4K dataset. SC denotes self-consistency. Norm is the overall accuracy normalized by the fraction of solved base problems (Table 2), which is a measure for robustness w.r.t. irrelevant information. For text-davinci-003, the base problem accuracy with CoT is 80.0 , and the base problem accuracy with LtM is 81.0. The best numbers in each column for each section (i.e., whether using code-davinci-002 or text-davinci-003, whether using exemplar with irrelevant context or not, and whether using self-consistency or not) are in boldface.
and macro accuracies, as well as their corresponding normalized accuracies. Overall, we observe significant performance drop for both models with all prompting techniques. The drop on macro accuracy is especially large, showing that fewer than $30 \%$ of the base problems are consistently solved after adding distractors. Comparing the results of two models, text-davinci-003 achieves better normalized micro accuracy than code-davinci-002, though its macro accuracy is mostly worse. In Figure 3, we present a GSM-IC-4K example where a single irrelevant sentence causes different types of errors in investigated prompting techniques. One common error type is wrongly using the number in the irrelevant sentence, as shown in the LtM prediction and other examples in Appendix B. Even if the model does not directly use the irrelevant number for numerical calculation, the presence of the irrelevant sentence in the reasoning steps alone can still cause a wrong prediction, as shown in the CoT prediction.</p>
<p>LtM is generally the most robust technique to irrelevant context. In terms of micro accuracy, LtM outperforms all other prompting methods across mod-
els. Using code-davinci-002, LtM achieves about double macro accuracy of CoT. Interestingly, with text-davinci-003, despite that LtM outperforms CoT on the micro accuracy, its macro accuracy is lower. Specifically, text-davinci-003 is highly susceptible to irrelevant context with role overlap; e.g., such irrelevant sentences decrease the macro accuracy to 0 on problems with more than 2 reasoning steps. See Table 4 for the breakdown performance on different types of irrelevant context.</p>
<p>Selecting exemplars with distractors mitigates the distractibility. For few-shot prompts, we find that using exemplars with distractors (i.e., including problems with irrelevant context) consistently outperforms using the original exemplars without distractors across prompting techniques. While prior work has shown that training and fine-tuning with different types of problems improves model robustness (Li et al., 2022), our results show that prompting with exemplars that demonstrate how to ignore irrelevant context also results in significant robustness improvement. In Table 5, we further show that using exemplars with distractors does not cause a performance drop on the original GSM8K dataset,</p>
<h2>Modified Problem</h2>
<p>Maria buys a large bar of French soap that lasts her for 2 months. She spends $\$ 8.00$ per bar of soap. Every 10 months, Maria's neighbor buys a new shampoo and moisturizer for Maria's neighbor. If Maria wants to stock up for the entire year, how much will she spend on soap?</p>
<h2>Correct CoT Output to the Original Problem</h2>
<p>Maria spends $\$ 8.00$ per bar of soap. She buys a bar of soap every 2 months. There are 12 months in a year. Maria will spend $\$ 8.00 \times 12 / 2=$ $\$ 48.00$ on soap for the year. The answer is $\$ 48.00$.</p>
<h2>Output to the Modified Problem</h2>
<h2>CoT</h2>
<p>Maria buys a large bar of French soap that lasts her for 2 months. She spends $\$ 8.00$ per bar of soap. Every 10 months, Maria's neighbor buys a new shampoo and moisturizer for Maria's neighbor. If Maria wants to stock up for the entire year, she will spend $\$ 8.00 \times 12=\$ 96.00$ on soap. The answer is $\$ 96.00$.</p>
<h2>LtM</h2>
<p>Let's break down this problem: 1. How much does Maria spend on soap every 10 months? 2. How much does Maria spend on soap every year? 1. Maria spends $\$ 8.00$ per bar of soap. She buys a new bar of soap every 10 months. So Maria spends $\$ 8.00 * 10=\$ 80.00$ every 10 months. 2. Maria spends $\$ 80.00$ every 10 months. So Maria spends $\$ 80.00 * 12=$ $\$ 960.00$ every year.
The answer is $\$ 960.00$.</p>
<h2>PROGRAM</h2>
<p>Soap_price $=8.00$
Soap_per_year $=2 * 12$
Soap_total $=$ Soap_price * Soap_per_year
Output: 192.0
print(Soap_total)</p>
<p>Figure 3. Example problem and corresponding outputs by different prompting techniques (best viewed in color). The CoT answer to the original problem is highlighted in green. The added irrelevant sentence is in italic and highlighted in red, which causes different errors (highlighted in yellow) for all prompting techniques. More examples of model predictions can be found in Appendix B.
indicating that such a prompt design can be beneficial in achieving better accuracy and robustness simultaneously.</p>
<p>Self-consistency significantly reduces the distractibility. Taking the majority vote from 20 samples, ${ }^{6}$ SC improves the overall micro accuracy by more than 11 percentage points. This means that in addition to improving model performance on clean arithmetic reasoning tasks (Wang et al., 2022c), SC also substantially reduces the distractibility of large language models to irrelevant context. The gain on micro accuracy is notably large on 0 -CoT ( 35.5 percentage points). Furthermore, the correct answer for $99.7 \%$ of the problems is in the 20 sampled answers for both CoT and LtM. Even for 0 -CoT, the recall of correct solutions within 20 samples is $96.5 \%$. Despite these improvements, the best macro accuracy among all prompting techniques is only $45 \%$, suggesting that for more than half of the base problems, SC fails to prevent the model from being distracted by different variants of irrelevant information. These results imply that a better algorithm may be developed to further reduce the distractibility based on a few sampled solutions.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 4. Micro accuracies on GSM-IC-4K with respect to the number of required reasoning steps.</p>
<h3>5.2. Break-Down Analysis</h3>
<h3>5.2.1. FACTORS OF THE IRRELEVANT CONTEXT</h3>
<p>We analyze the performance of CoT, LtM and Program with respect to the considered factors (§3.1) of the irrelevant sentences (Table 4). For both models, we find that (1) in-topic sentences with (2) role name overlap and (3) in-range numbers are generally more challenging, which is examplified by Figure 3. For LtM, the latter two factors do not have a large effect on the micro accuracy. The difference is more significant for the macro accuracy and, as an anomaly, using distractors with in-range numbers turns out to be less challenging than out-of-range numbers when using irrelevant context in the exemplar. Again, with code-davinci-002, LtM outperforms CoT and ProGRAM on all investigated sub-categories. On the other hand, using text-davinci-003, LtM outperforms CoT in terms of the micro accuracy, but the macro accuracy is much lower on all sub-categories.</p>
<h3>5.2.2. BREAK-DOWN ACCURACIES W.R.T. # STEPS</h3>
<p>We analyze the break-down accuracies for problems with respect to the reasoning steps (Figure 4). While we see a significant drop for COT and PROGRAM on problems that require four or more steps in the reasoning process, the performance of LtM is fairly consistent across difficulty. In addition to the advantage of LtM on clean problems for complicated reasoning (Zhou et al., 2022), our results show that LtM is also less sensitive to irrelevant context for complicated problems that require more steps to solve.</p>
<h3>5.3. Instructed Prompting Improves Robustness to Irrelevant Context</h3>
<p>We have shown that using exemplars with distractors improves robustness to irrelevant context. We also compare the performance of instructed prompting and that of the prompts without instructions in Table 3. Adding instructions to CoT, LtM, and Program consistently improves their performance. Surprisingly, instructed prompting with</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Micro Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Macro Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Topic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Role Overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num. Range</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Topic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Role Overlap</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Num. Range</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">In</td>
<td style="text-align: center;">Off</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">In</td>
<td style="text-align: center;">Out</td>
<td style="text-align: center;">In</td>
<td style="text-align: center;">Off</td>
<td style="text-align: center;">Yes</td>
<td style="text-align: center;">No</td>
<td style="text-align: center;">In</td>
<td style="text-align: center;">Out</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/o Irrelevant Context (code-davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">19.0</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">23.5</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">54.5</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">11.0</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/o Irrelevant Context (text-davinci-003)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">22.0</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">6.0</td>
</tr>
<tr>
<td style="text-align: center;">Prompting Exemplar w/ Irrelevant Context (code-davinci-002)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">18.4</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">35.0</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">14.4</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">17.0</td>
</tr>
</tbody>
</table>
<p>Table 4. Breakdown accuracies ( $\times 100$ ) w.r.t. the factors of the added irrelevant sentence. Lower accuracy indicates the model is more fragile to the corresponding type of irrelevant contexts. Note that the macro average accuracies are higher than the corresponding ones reported in Table 3, as we only include a subset of created problems (i.e., those corresponding to the appropriate factor) here to compute the metric. The best result in each column is in boldface.
original exemplars reaches comparable or even better performance than uninstructed prompting that uses exemplars with distractors for both CoT and LtM. Note that adding the instruction "Solve grade school math problems." alone does not significantly improve the performance, and it is the instruction "Feel free to ignore irrelevant information given in the questions." that makes the difference. Similar to the instruction "Let's think step by step." employed by 0-CoT, this shows that language models are-to some extent-able to follow natural language instructions in a way that dramatically changes their problem solving behavior, suggesting that such instructions may be useful for guiding the behavior of language models on more tasks.</p>
<p>On the original GSM8K development set (Cobbe et al., 2021; Zhou et al., 2022), we do not observe a drop in accuracy when using exemplars with irrelevant information, adding natural language instructions, or both (Table 5). The same holds for SVAMP (Patel et al., 2021), an arithmetic reasoning benchmark constructed by applying different types of variations to math problems from existing clean datasets, e.g., changing sentence structures, asking different questions with the same information, etc. This is impressive because the results on GSM-IC show that prompt exemplars with irrelevant information and instructed prompting both improve robustness. For the Program prompt, we find that using exemplars with distractors even increases performance on SVAMP.</p>
<h3>5.4. Complicated Prompts May Hurt the Robustness to Irrelevant Context</h3>
<p>We compare our 1-exemplar CoT prompt (Figure 2) to a 4-exemplar prompt (Appendix D of Zhou et al., 2022),</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Exemplar <br> w/ IRRCtx?</th>
<th style="text-align: center;">Accuracy <br> GSM8K Dev.</th>
<th style="text-align: center;">SVAMP Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">77.6</td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">+ InST.</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">78.7</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">76.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">76.6</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">76.2</td>
</tr>
<tr>
<td style="text-align: center;">+ InST.</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">76.3</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">80.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">77.3</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">59.2</td>
<td style="text-align: center;">77.9</td>
</tr>
<tr>
<td style="text-align: center;">+ InST.</td>
<td style="text-align: center;">$\boldsymbol{#}$</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">77.8</td>
</tr>
</tbody>
</table>
<p>Table 5. Accuracies ( $\times 100$ ) on the GSM8K development set and the SVAMP test set. IRRCtx denotes irrelevant contexts, and +INST. denotes instructed prompting. The baseline results (i.e., those using the simplest exemplars without irrelevant context and without instructions) are underlined.
which is reported as the best-performing CoT prompt on GSM8K, on GSM-IC (Table 6). Note that the 1-exemplar CoT prompt only includes a problem with a 2-step solution, while the 4-exemplar prompt includes problems that require more reasoning steps. While the 4-exemplar prompt leads to better performance on the original GSM8K development set, the 4-exemplar prompt is surprisingly more susceptible to the distraction provided by the irrelevant context. In particular, the 4-exemplar prompt is consistently worse than the 1-exemplar prompt on problems with more than 2 intermediate steps. Even for 2-step prompts, the accuracy</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">#Prompting <br> Exemplars</th>
<th style="text-align: center;">GSM8K <br> Dev.</th>
<th style="text-align: center;">GSM-IC-4K <br> 2 Steps</th>
<th style="text-align: center;">$&gt;2$ Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">70.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">66.3</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">69.4</td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: center;">+ InSt.</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">66.5</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">70.6</td>
</tr>
</tbody>
</table>
<p>Table 6. Micro accuracies $(\times 100)$ on the GSM8K development set and GSM-IC-4K. # Prompting exemplars denotes the number of exemplars used in the prompt. The best number in each column is in boldface.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">code-davinci-002</th>
<th style="text-align: center;">text-davinci-003</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">68.2</td>
</tr>
<tr>
<td style="text-align: left;">COT + InST.</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">69.9</td>
</tr>
<tr>
<td style="text-align: left;">LTM</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: left;">LTM + InST.</td>
<td style="text-align: center;">$\mathbf{7 4 . 4}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 7. Accuracies ( $\times 100$ ) on the football split of DROP (Dua et al., 2019) benchmark.
improvement from adding more exemplars is almost negligible when using instructions ( 79.0 vs 79.2 ). Overall, this finding indicates that adding more exemplars can make the prompt less robust as it leads to some overfitting.</p>
<h3>5.5. Extension to DROP</h3>
<p>In addition to GSM-IC, we extend our evaluation to the DROP dataset (Dua et al., 2019), where the task is to answer a question according to a long passage that naturally contains irrelevant context. We show an example about football games in Table 8.</p>
<p>We use the CoT and LTM prompts in (Zhou et al., 2022) as the baselines, and we evaluate the prompt variants with the instruction "Solve following questions. Feel free to ignore irrelevant information given in the questions." added before the exemplars. Note that by adding a problem reduction step in the exemplar solution, the least-to-most prompt implicitly leads the model to come up with relevant subproblems to solve the given problem. Again, we observe that the instruction consistently improves the performance of both COT and LTM prompting (Table 7).</p>
<h2>6. Conclusion and Discussion</h2>
<p>In this work, we introduce GSM-IC, a dataset that supports comprehensive study of the distractibility of large language models when performing arithmetic reasoning in presence of irrelevant contexts. We examine a variety of prompting techniques on GSM-IC, and demonstrate that they are all sensitive to the irrelevant information in the problems. Among the studied techniques, self-consistency (Wang et al., 2022c) leads to a substantial improvement in robustness to irrelevant context across the board, and presenting example</p>
<h2>Paragraph</h2>
<p>The Seahawks played the San Francisco 49ers. In the first quarter, the Hawks RB Julius Jones got a 27-yard TD run, along with DT Craig Terrill returning a fumble 9 yards for a touchdown. In the third quarter, the 49ers almost rallied as RB H. J. Torres made a 12-yard TD pass to Lucas Nelly, along with Mare kicking a 32-yard field goal. In the final quarter, Julius Jones got another 11-yard TD.</p>
<h2>Question</h2>
<p>How many yards do the shortest touchdown run and the longest touchdown pass combine for?</p>
<h2>LTM solution</h2>
<p>To answer the question "How many yards do the shortest touchdown run and the longest touchdown pass combine for?", we need to know: "How many yards was the shortest touchdown run?", "How many yards was the longest touchdown pass?".
Q: How many yards was the shortest touchdown run?
A: All the touchdown runs are: a 27-yard touchdown run, a 9-yard touchdown run, a 11-yard touchdown run. The smallest number between $27,9,11$ is 9 . The shortest touchdown run was 9 yards. The answer is 9 .
Q: How many yards was the longest touchdown pass?
A: All the touchdown passes are: a 12-yard touchdown pass. The longest touchdown pass was 12 yards. The answer is 12 .
Q: How many yards do the shortest touchdown run and the longest touchdown pass combine for?
A: We know that the shortest touchdown run was 9-yard. We also know that the longest touchdown pass was 12 yard. $9+12=21$. The answer is 21 .</p>
<p>Table 8. A DROP example about football games.
problems with irrelevant context in the prompt also consistently improves the performance. Similarly, we find that simply adding an instruction to ignore irrelevant information brings notable performance gains on our benchmark.</p>
<p>Despite the improvement achieved by these methods, the fundamental issue remains: a single piece of irrelevant information can distract the models and substantially degrade their performance, even on problems whose clean versions they correctly solve. We encourage researchers to also prioritize improving on this fundamental limitation when developing new training and prompting techniques. We leave further investigation on the distractibility for other tasks and different language models for future work.</p>
<h2>Acknowledgement</h2>
<p>We would like to thank Dale Schuurmans, Olivier Bousquet and Jack Nikodem for helpful discussion and feedback.</p>
<h2>References</h2>
<p>Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. NeurIPS, 2020.</p>
<p>Chaves, R. P. and Richter, S. N. Look at that! BERT can be easily distracted from paying attention to morphosyntax. In Proceedings of the Society for Computation in Linguistics 2021, pp. 28-38, Online, February 2021. Association for Computational Linguistics. URL https: //aclanthology.org/2021.scil-1.3.</p>
<p>Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.</p>
<p>Clark, P., Tafjord, O., and Richardson, K. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pp. 38823890, 2021.</p>
<p>Cobbe, K., Kosaraju, V., Bavarian, M., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. URL https://arxiv. org/pdf/2110.14168.</p>
<p>Creswell, A., Shanahan, M., and Higgins, I. Selectioninference: Exploiting large language models for interpretable logical reasoning. arXiv preprint arXiv:2205.09712, 2022.</p>
<p>Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., Murphy, K., and Sutton, C. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Drozdov, A., Schärli, N., Akyürek, E., Scales, N., Song, X., Chen, X., Bousquet, O., and Zhou, D. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022.</p>
<p>Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840, 2022.</p>
<p>Hoyer, W. J., Rebok, G. W., and Sved, S. M. Effects of varying irrelevant information on adult age differences in problem solving. Journal of gerontology, 34(4):553-560, 1979.</p>
<p>Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017.</p>
<p>Jones, E. and Steinhardt, J. Capturing failures of large language models via human cognitive biases. arXiv preprint arXiv:2202.12299, 2022.</p>
<p>Kassner, N. and Schütze, H. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7811-7818, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 698. URL https://aclanthology.org/2020. acl-main. 698.</p>
<p>Khashabi, D., Khot, T., Sabharwal, A., and Roth, D. Learning what is essential in questions. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pp. 80-89, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1010. URL https://aclanthology.org/K17-1010.</p>
<p>Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K., Clark, P., and Sabharwal, A. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022.</p>
<p>Kim, J., Kim, H. J., Cho, H., Jo, H., Lee, S.-W., Lee, S. g., Yoo, K. M., and Kim, T. Ground-truth labels matter: A deeper look into input-label demonstrations. arXiv preprint arXiv:2205.12685, 2022.</p>
<p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>Kumar, V., Maheshwary, R., and Pudi, V. Adversarial examples for evaluating math word problem solvers. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 2705-2712, 2021.</p>
<p>Li, D., Rawat, A. S., Zaheer, M., Wang, X., Lukasik, M., Veit, A., Yu, F., and Kumar, S. Large language models with controllable working memory. arXiv preprint arXiv:2211.05110, 2022.</p>
<p>Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.</p>
<p>Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015.</p>
<p>Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022.</p>
<p>Marzocchi, G. M., Lucangeli, D., De Meo, T., Fini, F., and Cornoldi, C. The disturbing effect of irrelevant information on arithmetic problem solving in inattentive children. Developmental neuropsychology, 21(1):73-92, 2002.</p>
<p>Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.</p>
<p>Misra, K., Rayz, J., and Ettinger, A. COMPS: Conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023.</p>
<p>Morris, J. X., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909, 2020.</p>
<p>Ni, J., Zhu, C., Chen, W., and McAuley, J. Learning to attend on essential terms: An enhanced retriever-reader model for open-domain question answering. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 335-344, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1030. URL https: //aclanthology.org/N19-1030.</p>
<p>Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. URL https://arxiv. org/abs/2203.02155.</p>
<p>Pandia, L. and Ettinger, A. Sorting through the noise: Testing robustness of information processing in pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 1583-1596, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 119. URL https://aclanthology.org/2021. emnlp-main. 119.</p>
<p>Pasolunghi, M. C., Cornoldi, C., and De Liberto, S. Working memory and intrusions of irrelevant information in a group of specific poor problem solvers. Memory \&amp; Cognition, 27:779-790, 1999.</p>
<p>Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? In NAACL-HLT, 2021. URL https://aclanthology. org/2021.naacl-main.168.pdf.</p>
<p>Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020. URL https://jmlr.org/papers/ v21/20-074.html.</p>
<p>Ravichander, A., Gardner, M., and Marasović, A. Condaqa: A contrastive reading comprehension dataset for reasoning about negation. arXiv preprint arXiv:2211.00295, 2022.</p>
<p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.</p>
<p>Shi, F., Fried, D., Ghazvininejad, M., Zettlemoyer, L., and Wang, S. I. Natural language to code translation with execution. In EMNLP, 2022a.</p>
<p>Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., and Wei, J. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022b. URL https://arxiv. org/pdf/2210.03057.</p>
<p>Shi, H., Mao, J., Xiao, T., Jiang, Y., and Sun, J. Learning visually-grounded semantics from contrastive adversarial samples. In Proceedings of the 27th International Conference on Computational Linguistics, pp. 3715-3727, 2018.</p>
<p>Sinha, K., Sodhani, S., Dong, J., Pineau, J., and Hamilton, W. L. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4506-4515, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL https://aclanthology.org/D19-1458.</p>
<p>Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>
<p>Tafjord, O., Mishra, B. D., and Clark, P. Proofwriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.</p>
<p>Wang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A. H., and Li, B. Adversarial glue: A multitask benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840, 2021.</p>
<p>Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022a.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b.</p>
<p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022c.</p>
<p>Webson, A. and Pavlick, E. Do prompt-based models really understand the meaning of their prompts? arXiv preprint arXiv:2109.01247, 2021.</p>
<p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022. URL https://openreview.net/ pdf?id=_VjQlMeSB_J.</p>
<p>Weston, J., Bordes, A., Chopra, S., Rush, A. M., Van Merriënboer, B., Joulin, A., and Mikolov, T. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/pdf/2205.10625.</p>
<h1>A. GSM-IC Details</h1>
<p>Each of the 100 base problem require two to seven steps to solve (Figure 5).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5. Base problem distribution of GSM-IC with respect to the number of reasoning steps in the ground truth problem solution.
Starting from the base problems, we follow the protocols below to create GSM-IC (§3.1).</p>
<h2>1. Irrelevant sentence template.</h2>
<p>(a) For in-topic sentences, we manually write templates within the topic that is close to the original problem description. We are particularly careful about the shareable stuff, for example, money is sometimes considered shareable between family members. In such cases, we make sure that the added do not change the amount of shareable stuff to ensure that the final standard answer is not affected.
(b) For off-topic sentences, we use general templates (Table 9) for all problems unless some of them can be considered as in-topic sentences for some problems-for example, the sentence "The height of ${$ role $}$ is ${$ number $}$ feet." is considered as an in-topic sentence for problems about heights of people.</p>
<p>The shoe size of [ROLE] is [NUMBER].
[ROLE] is [NUMBER] years old.
The height of [ROLE] is [NUMBER] feet.
[ROLE] bought [NUMBER] tomatoes from the grocery store.
[ROLE] has read [NUMBER] books in the past year.
Table 9. Off-topic sentence templates for GSM-IC.
(c) We make sure that all sentences derived by each template are grammatical English sentences.
(d) We write four in-topic and choose four off-topic distractor sentence templates for each problem.</p>
<h2>2. Blank fillers: role names.</h2>
<p>(a) We randomly choose a role name $X$, and use $X^{\prime}$ s father, $X^{\prime}$ s mother, $X^{\prime}$ s brother, $X^{\prime}$ s sister and $X^{\prime}$ s neighbor as the overlapped role names.
(b) We choose from the name set {Ada, David, Emma, Jack, John, Mary, Max, Tom} for nonoverlapped role names.
(c) We write five names that have overlap with the original character, and five names that do not have overlap for each problem.</p>
<h2>3. Blank fillers: numbers.</h2>
<p>(a) For in-range numbers, we randomly sample positive integers in the range of $\left[\frac{\ell}{10}, 10 r\right]$, where $\ell$ and $r$ denote the smallest and the largest number that appear in the problem description and standard solution, respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original Problem</th>
<th style="text-align: left;">Kim plants 80 cherry pits. 25\% of them sprout and Kim sells 6 of the saplings. <br> How many cherry saplings does she have left?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Added Sentence</td>
<td style="text-align: left;">Kim's mother plants 20 more potatoes.</td>
</tr>
<tr>
<td style="text-align: left;">Ambiguous Problem</td>
<td style="text-align: left;">Kim plants 80 cherry pits. 25\% of them sprout and Kim sells 6 of the saplings. <br> Kim's mother plants 20 more potatoes. How many cherry saplings does she have <br> left?</td>
</tr>
<tr>
<td style="text-align: left;">Fixed Problem</td>
<td style="text-align: left;">Kim plants 80 cherry pits. 25\% of them sprout and Kim sells 6 of the saplings. <br> Kim's mother plants 20 more potatoes. How many cherry saplings does Kim <br> have left?</td>
</tr>
</tbody>
</table>
<p>Table 10. An example that adding irrelevant contexts causes ambiguity: after adding the sentence, it is unclear whether she refers to Kim or Kim's mother. To ensure that the standard answer is unchanged, we modify the final question to make it clear and faithful to the original problem.
(b) For out-of-range numbers, we choose from the range of $[2,+\infty) \backslash\left[\frac{\ell}{10}, 10 r\right]$. For very few problems that $\ell$ is relatively large (i.e., $\ell&gt;10^{5}$ ) where we choose out-of-range numbers from the range of $\left[2, \frac{\ell}{10}\right]$; for other problems we choose out-of-range numbers $n=a \times 10^{b}$ from the range $[10 r, \infty)$, where $a$ and $b$ are both non-negative integers.
(c) We write four in-range numbers and four out-of-range numbers for each problem.
4. Finally, if adding the irrelevant sentence causes ambiguity (e.g., Table 10), we fix the question to ensure that the standard solution to the generated problem remain the same as the base problem.</p>
<h1>B. Sample Predictions on GSM-IC</h1>
<p>In addition to the example outputs shown in Figure 3, we include more example problems and the predictions by different techniques (Tables 11 and 12).</p>
<h2>C. Full prompts in experiments</h2>
<p>We list the prompts for all experiments in Tables 13 and 14.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Original GSM8K Problem <br> Jessica is six years older than Claire. In two years, Claire will be 20 years old. How old is Jessica now?</th>
<th style="text-align: center;">Modified Problem (w/ Irrelevant Sentence) <br> Jessica is six years older than Claire. In two years, Claire will be 20 years old. Twenty years ago, the age of Claire's father is 3 times of Jessica's age. How old is Jessica now?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">In two years, Claire will be 20 years old. So, Claire is 18 years old now. Jessica is six years older than Claire, so Jessica is 18 $+6=24$ years old now. The answer is 24 .</td>
<td style="text-align: center;">Jessica is six years older than Claire, so Jessica is 20 $6=14$ years old. Twenty years ago, the age of Claire's father is 3 times of Jessica's age, so 20 years ago, the age of Claire's father is $3 * 14=42$ years old. The answer is 14 .</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">Let's break down this problem: 1. How old is Claire now? 2. How old is Jessica now? <br> 1. In two years, Claire will be 20 years old. So Claire is 20 - 2 $=18$ years old now. <br> 2. Jessica is six years older than Claire. So Jessica is $18+6=$ 24 years old now.</td>
<td style="text-align: center;">A: Let's break down this problem: 1. How old is Claire now? 2. How old is Jessica now? <br> 1. In two years, Claire will be 20 years old. So Claire is 18 years old now. <br> 2. Twenty years ago, the age of Claire's father is 3 times of Jessica's age. So 20 years ago, Claire's father is 3 * Jessica's age. So Claire's father is 3 * Jessica's age now. So Claire's father is 3 * Jessica's age - 20 years old now. So Claire's father is 3 * Jessica's age - 20 + $18=3 <em>$ Jessica's age - 2 years old now. So Claire's father is 3 * Jessica's age - $2+6=3 </em>$ Jessica's age + 4 years old now. So Claire's father is 3 * Jessica's age + 4 - $18=3 <em>$ Jessica's age - 14 years old now. So Claire's father is 3 * Jessica's age - $14+20=3 </em>$ Jessica's age + 6 years old now. So 3 * Jessica's age $+6=3 <em>$ Jessica's age - $2+6=3 </em>$ Jessica's age + 4. So 3 * Jessica's age $+6=3 *$ Jessica's age + $4 . \operatorname{So} 6=4$. So Jessica's age $=$ 2. So Jessica is 2 years old now. <br> The answer is 2 .</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">Claire_age $=20-2$ <br> Jessica_age $=$ Claire_age +6 <br> print(Jessica_age)</td>
<td style="text-align: center;">Claire_age $=20-2$ <br> Jessica_age $=$ Claire_age - 6 <br> Claire_father_age $=3 *$ Jessica_age <br> print(Jessica_age)</td>
</tr>
</tbody>
</table>
<p>Table 11. Output example #1 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all techniques give correct answers to the original problem, they output different different results for the modified GSM-IC problem.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Original GSM8K Problem <br> Lucy has $\$ 65$ in the bank. She made a $\$ 15$ deposit and then followed by a $\$ 4$ withdrawal. What is Lucy's bank balance?</th>
<th style="text-align: center;">Modified Problem (w/ Irrelevant Sentence) <br> Lucy has $\$ 65$ in the bank. She made a $\$ 15$ deposit and then followed by a $\$ 4$ withdrawal. Lucy's mother's monthly rent is $\$ 10$. What is Lucy's bank balance?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">Lucy has $\$ 65$ in the bank. She made a $\$ 15$ deposit, so she has $\$ 65+\$ 15=\$ 80$. She then made a $\$ 4$ withdrawal, so she has $\$ 80-\$ 4=\$ 76$. The answer is $\$ 76$.</td>
<td style="text-align: center;">Lucy has $\$ 65$ in the bank. She made a $\$ 15$ deposit and then followed by a $\$ 4$ withdrawal. Lucy's mother's monthly rent is $\$ 10$. Lucy's bank balance is $\$ 65+\$ 15$ - \$4 - \$10 = \$66. The answer is $\$ 66$.</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">Let's break down this problem: 1. How much money did Lucy deposit? 2. How much money did Lucy withdraw? 3. How much money does Lucy have in the bank? <br> 1. Lucy deposited $\$ 15$. <br> 2. Lucy withdrew $\$ 4$. <br> 3. Lucy has $\$ 65+\$ 15-\$ 4=\$ 76$ in the bank. <br> The answer is $\$ 76$.</td>
<td style="text-align: center;">Let's break down this problem: 1. How much money did Lucy deposit? 2. How much money did Lucy withdraw? 3. How much money does Lucy have in the bank? <br> 1. Lucy deposited $\$ 15$. <br> 2. Lucy withdrew $\$ 4$. <br> 3. Lucy has $\$ 65+\$ 15-\$ 4-\$ 10=\$ 66$ in the bank. <br> The answer is $\$ 66$.</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">Lucy_balance $=65$ <br> Lucy_balance $=$ Lucy_balance +15 <br> Lucy_balance $=$ Lucy_balance -4 <br> print(Lucy_balance)</td>
<td style="text-align: center;">Lucy_bank $=65$ <br> Lucy_bank $=$ Lucy_bank +15 <br> Lucy_bank $=$ Lucy_bank - 4 <br> Lucy_bank $=$ Lucy_bank - 10 <br> print(Lucy_bank)</td>
</tr>
</tbody>
</table>
<p>Table 12. Output example #2 for different prompting techniques, on a GSM8K problem and a corresponding GSM-IC problem. While all techniques give correct answers to the original problem, they output the same incorrect result for the modified GSM-IC problem.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Example <br> w/ IrrCtx?</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they <br> have together? <br> A: Anna has 2 more apples than Elsa, so Anna has $2+5=7$ apples. Elsa and Anna <br> have $5+7=12$ apples together. The answer is 12 . <br> Q: [Problem of Interest] <br> A:</td>
</tr>
<tr>
<td style="text-align: center;">COT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How <br> many apples do they have together? <br> A: Anna has 2 more apples than Elsa, so Anna has $2+5=7$ apples. Elsa and Anna <br> have $5+7=12$ apples together. The answer is 12 . <br> Q: [Problem of Interest] <br> A:</td>
</tr>
<tr>
<td style="text-align: center;">LTM</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they <br> have together? <br> A: Let's break down this problem: 1. How many apples does Anna have? 2. How <br> many apples do Elsa and Anna have together? <br> 1. Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. <br> 2. Elsa and Anna have $5+7=12$ apples together. <br> Q: [Problem of Interest] <br> A: Let's break down this problem:</td>
</tr>
<tr>
<td style="text-align: center;">LTM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How <br> many apples do they have together? <br> A: Let's break down this problem: 1. How many apples does Anna have? 2. How <br> many apples do Elsa and Anna have together? <br> 1. Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. <br> 2. Elsa and Anna have $5+7=12$ apples together. <br> Q: [Problem of Interest] <br> A: Let's break down this problem:</td>
</tr>
<tr>
<td style="text-align: center;">0 -CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Q: [Problem of Interest] <br> A: Let's think step by step:</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they <br> have together? <br> A: Let's solve the problem by a Python program: <br> Elsa_apples $=5$ <br> Anna_apples $=2+$ Elsa_apples <br> Elsa_Anna_apples = Elsa_apples + Anna_apples <br> print(Elsa_Anna_apples) <br> Q: [Problem of Interest] <br> A: Let's solve the problem by a Python program:</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How <br> many apples do they have together? <br> A: Let's solve the problem by a Python program: <br> Elsa_apples $=5$ <br> Anna_apples $=2+$ Elsa_apples <br> Elsa_Anna_apples = Elsa_apples + Anna_apples <br> print(Elsa_Anna_apples) <br> Q: [Problem of Interest] <br> A: Let's solve the problem by a Python program:</td>
</tr>
</tbody>
</table>
<p>Table 13. Prompts used for all investigated techniques, without instruction. The placeholder [Problem of Interest] is substituted for each problem at the test time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Example <br> w/ IrrCtx?</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? A: Anna has 2 more apples than Elsa, so Anna has $2+5=7$ apples. Elsa and Anna have $5+7=12$ apples together. The answer is 12 . <br> Q: [Problem of Interest] <br> A:</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? <br> A: Anna has 2 more apples than Elsa, so Anna has $2+5=7$ apples. Elsa and Anna have $5+7=12$ apples together. The answer is 12 . <br> Q: [Problem of Interest] <br> A:</td>
</tr>
<tr>
<td style="text-align: center;">LTM</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? <br> A: Let's break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? <br> 1. Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. <br> 2. Elsa and Anna have $5+7=12$ apples together. <br> Q: [Problem of Interest] <br> A: Let's break down this problem:</td>
</tr>
<tr>
<td style="text-align: center;">LTM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? <br> A: Let's break down this problem: 1. How many apples does Anna have? 2. How many apples do Elsa and Anna have together? <br> 1. Anna has 2 more apples than Elsa. So Anna has $2+5=7$ apples. <br> 2. Elsa and Anna have $5+7=12$ apples together. <br> Q: [Problem of Interest] <br> A: Let's break down this problem:</td>
</tr>
<tr>
<td style="text-align: center;">0 -CoT</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: [Problem of Interest] <br> A: Let's think step by step:</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. How many apples do they have together? <br> A: Let's solve the problem by a Python program: <br> Elsa_apples $=5$ <br> Anna_apples $=2+$ Elsa_apples <br> Elsa_Anna_apples = Elsa_apples + Anna_apples <br> print(Elsa_Anna_apples) <br> Q: [Problem of Interest] <br> A: Let's solve the problem by a Python program:</td>
</tr>
<tr>
<td style="text-align: center;">Program</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Solve grade school math problems. Feel free to ignore irrelevant information given in the questions. Q: Elsa has 5 apples. Anna has 2 more apples than Elsa. Liz has 4 peaches. How many apples do they have together? <br> A: Let's solve the problem by a Python program: <br> Elsa_apples $=5$ <br> Anna_apples $=2+$ Elsa_apples <br> Elsa_Anna_apples = Elsa_apples + Anna_apples <br> print(Elsa_Anna_apples) <br> Q: [Problem of Interest] <br> A: Let's solve the problem by a Python program:</td>
</tr>
</tbody>
</table>
<p>Table 14. All prompts with instructions. The placeholder [Problem of Interest] is substituted for each problem at the test time.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ If there is a tie, we take a random top-tier result for evaluation, following Wang et al. (2022c) and Shi et al. (2022a).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ http://openai.com/api/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
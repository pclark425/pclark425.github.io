<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3125 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3125</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3125</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-52fa450740913a6cdcb4d9395b45e203f46cab79</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/52fa450740913a6cdcb4d9395b45e203f46cab79" target="_blank">Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work enables a BERT-based reading comprehension model to perform lightweight numerical reasoning by augmenting the model with a predefined set of executable ‘programs’ which encompass simple arithmetic as well as extraction.</p>
                <p><strong>Paper Abstract:</strong> Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable ‘programs’ which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a 33% absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3125.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3125.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT+Calculator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-based Reading Comprehension with Executable Programs (Giving BERT a Calculator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A BERT-based extractive QA model augmented with a small set of predefined executable operations (programs) for numerical reasoning; the model selects an operation and its arguments (pointers into the passage), then the operation is executed externally to produce the numeric answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-based reading comprehension model with operation-selection layer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained BERT encoder (Devlin et al., 2019) provides token-level vectors conditioned on question and passage; a lightweight neural layer constructs representations for candidate derivations (literals, spans, numeric operations, and shallow compositions) using MLPs and simple combination functions (includes use of the [CLS] vector for literals, start-token vectors for numbers/spans, and Hadamard products in binary-argument representations). Exact BERT size is not specified in the paper; model is trained by marginalizing over oracle derivations and uses pruning (top-k) during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Passage-based numeric reasoning (DROP): addition (Sum), subtraction (Diff), special unary ops (Diff100), multi-operand addition (Sum3), multiplication and division when added for math word problems (Illinois dataset), and composition of operations; also extraction of numeric arguments and multi-span answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Symbolic decomposition + learned argument selection: instead of training the neural encoder to carry out arithmetic, the model learns to (1) identify numeric arguments and text spans by producing vector representations and scores for candidate derivations, (2) select an operation from a fixed inventory (Sum, Diff, Mul, Div, Diff100, Sum3, Merge, Span, literals), and (3) execute the selected operation externally (offloading actual arithmetic to a deterministic calculator). Internally, numerical arguments are represented by BERT token vectors; binary-op representations computed by MLP_binary(h_i, h_j, h_i ◦ h_j) and scored with a linear layer.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Ablation experiments: incremental addition of operations (Diff100, Sum3, Merge) produced measurable improvements on DROP consistent with the prevalence of those operations in data, indicating operation selection is the mechanism driving numeric reasoning improvements. On math word problems, adding Mul and Div allowed the same architecture to learn multiplication/division with few examples (accuracy jump from 48.6% to 74.0% ±6.0). Entropy analysis of ambiguous oracle derivations shows the model rapidly concentrates probability mass (entropy drop from ~2.5 bits to <0.2 bits across training), supporting that the model learns to prefer correct derivations rather than performing arithmetic internally. Example corrections (compared to NAQANet) show the model picking Diff and correct arguments to avoid globally inconsistent tagging.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct evidence that BERT's internal activations implement algorithmic arithmetic — authors explicitly state the computation is offloaded (model picks program rather than manipulating numbers internally). Some limitations indicate the approach is not perfect: examples without any oracle derivation are skipped during training, pruning can drop correct arguments early (recall initially 80–90% after 1 epoch, plateauing at 95–98%), and multi-span answers are under-predicted due to class imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural/tool intervention: add an explicit set of executable operations (programs) and a neural module to predict operations and their arguments; add specific ops (Diff100, Sum3, Merge, Mul, Div); co-training/pretraining on CoQA; pruning (top-128 span/sum results) and ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Substantial performance improvement on DROP (absolute +33% over previous SOTA NAQANet). Adding Diff100 and Sum3 increased oracle and actual model performance consistent with their frequency in data; adding Mul and Div enabled learning multiplication/division on the Illinois dataset. Co-training with CoQA improved overall performance (best single-model results after +CoQA); ensembling added ~1% absolute. Pruning enabled tractable search with modest initial recall loss, recovered with training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DROP dev/test: compared to NAQANet baseline, the BERT+Calculator model achieves large gains. Key reported numbers: NAQANet overall dev EM ~44.2 and F1 ~47.8 (table values); Our best single model (+CoQA) on dev: EM 78.00, F1 81.56; ensemble: dev EM 78.95, F1 82.54; test (ensemble) EM 78.15, F1 81.78. Oracle derivation upper-bound reported ~93.01 on some measures. Illinois (math word problems, 5-fold CV): basic model 48.6 ±5.3% accuracy; +Mul and Div 74.0 ±6.0%; +transfer from DROP 83.2 ±6.0%. Pruning recall: 80–90% after 1 epoch, plateauing at 95–98%.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>If no derivation produces the gold answer the example is skipped in training; pruning can exclude correct derivations (initial recall <100%); under-prediction of multiple-span answers due to class imbalance; model forgets CoQA after fine-tuning on DROP unless co-trained (catastrophic forgetting); does not claim to learn algorithmic internal arithmetic — relies on external execution; spurious ambiguous derivations exist but are mostly resolved during training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>The approach explicitly uses symbolic execution (deterministic arithmetic execution) as a calculator, so arithmetic itself is handled symbolically rather than by neural approximation; when transferred to Illinois dataset the model (with DROP transfer) surpasses some specialized symbolic/rule-based systems (Liang et al., 2016) and outperforms a deep-RL approach (Wang et al., 2018), demonstrating that combining learned argument selection with symbolic execution can match or exceed specialized symbolic pipelines in these settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3125.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3125.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerically-aware QANet (NAQANet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of the QANet extractive QA architecture that incorporates numeric reasoning by predicting number tags and limited summation operations; used as the previous state-of-the-art baseline on DROP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NAQANet (numerically-aware QANet)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Built on QANet encoder, NAQANet augments extractive QA with special handling for numbers: it performs per-number tagging (3-way classification: plus, minus, zero) and predicts numbers (0–9) and summation operations in a largely flat tagging scheme. It does not generalize easily to new ops without architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Simple numeric operations expressible via per-number tagging and summation (addition/subtraction expressed via sign tags); limited multi-number composition.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Per-number flat sign classification and summation: each number in the passage is independently tagged with plus/minus/zero and then combined to form an answer (flat, independent tagging rather than compositional program selection).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Design and reported errors: NAQANet's independent per-number tagging is described and used as baseline; shown to work for some summation-style questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Global inconsistency issues arise because each number is tagged independently (example in paper: NAQANet predicted a single minus label and no plus labels leading to a negative count of people). It is also more difficult to generalize to new operations (e.g., Mul/Div) without large-scale architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural modification from QANet to add numeric tags and summation; no external calculator or compositional program selection in the NAQANet design.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Improved baseline numeric QA compared to plain QANet on DROP but insufficient for many DROP questions; the BERT+Calculator model substantially outperforms NAQANet when equipped with compositional ops and BERT encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported NAQANet overall dev performance in this paper: EM ≈ 44.24 and F1 ≈ 47.77 (Table 3, baseline), noticeably below the BERT+Calculator models.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Independent per-number tagging can produce globally inconsistent numeric answers (e.g., predicting a negative population), and the flat scheme does not scale to more diverse operations or compositional arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>NAQANet attempts to produce numeric answers within a purely neural extractive framework without explicit symbolic execution; contrasted with the BERT+Calculator approach which explicitly selects symbolic operations and executes them externally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension', 'publication_date_yy_mm': '2019-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs <em>(Rating: 2)</em></li>
                <li>Analysing mathematical reasoning abilities of neural models <em>(Rating: 2)</em></li>
                <li>Solving general arithmetic word problems <em>(Rating: 2)</em></li>
                <li>A tag-based statistical english math word problem solver with understanding, reasoning and explanation <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 1)</em></li>
                <li>MathDQN: Solving arithmetic word problems via deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3125",
    "paper_id": "paper-52fa450740913a6cdcb4d9395b45e203f46cab79",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "BERT+Calculator",
            "name_full": "BERT-based Reading Comprehension with Executable Programs (Giving BERT a Calculator)",
            "brief_description": "A BERT-based extractive QA model augmented with a small set of predefined executable operations (programs) for numerical reasoning; the model selects an operation and its arguments (pointers into the passage), then the operation is executed externally to produce the numeric answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-based reading comprehension model with operation-selection layer",
            "model_description": "Pretrained BERT encoder (Devlin et al., 2019) provides token-level vectors conditioned on question and passage; a lightweight neural layer constructs representations for candidate derivations (literals, spans, numeric operations, and shallow compositions) using MLPs and simple combination functions (includes use of the [CLS] vector for literals, start-token vectors for numbers/spans, and Hadamard products in binary-argument representations). Exact BERT size is not specified in the paper; model is trained by marginalizing over oracle derivations and uses pruning (top-k) during inference.",
            "arithmetic_task_type": "Passage-based numeric reasoning (DROP): addition (Sum), subtraction (Diff), special unary ops (Diff100), multi-operand addition (Sum3), multiplication and division when added for math word problems (Illinois dataset), and composition of operations; also extraction of numeric arguments and multi-span answers.",
            "reported_mechanism": "Symbolic decomposition + learned argument selection: instead of training the neural encoder to carry out arithmetic, the model learns to (1) identify numeric arguments and text spans by producing vector representations and scores for candidate derivations, (2) select an operation from a fixed inventory (Sum, Diff, Mul, Div, Diff100, Sum3, Merge, Span, literals), and (3) execute the selected operation externally (offloading actual arithmetic to a deterministic calculator). Internally, numerical arguments are represented by BERT token vectors; binary-op representations computed by MLP_binary(h_i, h_j, h_i ◦ h_j) and scored with a linear layer.",
            "evidence_for_mechanism": "Ablation experiments: incremental addition of operations (Diff100, Sum3, Merge) produced measurable improvements on DROP consistent with the prevalence of those operations in data, indicating operation selection is the mechanism driving numeric reasoning improvements. On math word problems, adding Mul and Div allowed the same architecture to learn multiplication/division with few examples (accuracy jump from 48.6% to 74.0% ±6.0). Entropy analysis of ambiguous oracle derivations shows the model rapidly concentrates probability mass (entropy drop from ~2.5 bits to &lt;0.2 bits across training), supporting that the model learns to prefer correct derivations rather than performing arithmetic internally. Example corrections (compared to NAQANet) show the model picking Diff and correct arguments to avoid globally inconsistent tagging.",
            "evidence_against_mechanism": "No direct evidence that BERT's internal activations implement algorithmic arithmetic — authors explicitly state the computation is offloaded (model picks program rather than manipulating numbers internally). Some limitations indicate the approach is not perfect: examples without any oracle derivation are skipped during training, pruning can drop correct arguments early (recall initially 80–90% after 1 epoch, plateauing at 95–98%), and multi-span answers are under-predicted due to class imbalance.",
            "intervention_type": "Architectural/tool intervention: add an explicit set of executable operations (programs) and a neural module to predict operations and their arguments; add specific ops (Diff100, Sum3, Merge, Mul, Div); co-training/pretraining on CoQA; pruning (top-128 span/sum results) and ensembling.",
            "effect_of_intervention": "Substantial performance improvement on DROP (absolute +33% over previous SOTA NAQANet). Adding Diff100 and Sum3 increased oracle and actual model performance consistent with their frequency in data; adding Mul and Div enabled learning multiplication/division on the Illinois dataset. Co-training with CoQA improved overall performance (best single-model results after +CoQA); ensembling added ~1% absolute. Pruning enabled tractable search with modest initial recall loss, recovered with training.",
            "performance_metrics": "DROP dev/test: compared to NAQANet baseline, the BERT+Calculator model achieves large gains. Key reported numbers: NAQANet overall dev EM ~44.2 and F1 ~47.8 (table values); Our best single model (+CoQA) on dev: EM 78.00, F1 81.56; ensemble: dev EM 78.95, F1 82.54; test (ensemble) EM 78.15, F1 81.78. Oracle derivation upper-bound reported ~93.01 on some measures. Illinois (math word problems, 5-fold CV): basic model 48.6 ±5.3% accuracy; +Mul and Div 74.0 ±6.0%; +transfer from DROP 83.2 ±6.0%. Pruning recall: 80–90% after 1 epoch, plateauing at 95–98%.",
            "notable_failure_modes": "If no derivation produces the gold answer the example is skipped in training; pruning can exclude correct derivations (initial recall &lt;100%); under-prediction of multiple-span answers due to class imbalance; model forgets CoQA after fine-tuning on DROP unless co-trained (catastrophic forgetting); does not claim to learn algorithmic internal arithmetic — relies on external execution; spurious ambiguous derivations exist but are mostly resolved during training.",
            "comparison_to_humans_or_symbolic": "The approach explicitly uses symbolic execution (deterministic arithmetic execution) as a calculator, so arithmetic itself is handled symbolically rather than by neural approximation; when transferred to Illinois dataset the model (with DROP transfer) surpasses some specialized symbolic/rule-based systems (Liang et al., 2016) and outperforms a deep-RL approach (Wang et al., 2018), demonstrating that combining learned argument selection with symbolic execution can match or exceed specialized symbolic pipelines in these settings.",
            "uuid": "e3125.0",
            "source_info": {
                "paper_title": "Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension",
                "publication_date_yy_mm": "2019-08"
            }
        },
        {
            "name_short": "NAQANet",
            "name_full": "Numerically-aware QANet (NAQANet)",
            "brief_description": "An extension of the QANet extractive QA architecture that incorporates numeric reasoning by predicting number tags and limited summation operations; used as the previous state-of-the-art baseline on DROP.",
            "citation_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "mention_or_use": "mention",
            "model_name": "NAQANet (numerically-aware QANet)",
            "model_description": "Built on QANet encoder, NAQANet augments extractive QA with special handling for numbers: it performs per-number tagging (3-way classification: plus, minus, zero) and predicts numbers (0–9) and summation operations in a largely flat tagging scheme. It does not generalize easily to new ops without architectural changes.",
            "arithmetic_task_type": "Simple numeric operations expressible via per-number tagging and summation (addition/subtraction expressed via sign tags); limited multi-number composition.",
            "reported_mechanism": "Per-number flat sign classification and summation: each number in the passage is independently tagged with plus/minus/zero and then combined to form an answer (flat, independent tagging rather than compositional program selection).",
            "evidence_for_mechanism": "Design and reported errors: NAQANet's independent per-number tagging is described and used as baseline; shown to work for some summation-style questions.",
            "evidence_against_mechanism": "Global inconsistency issues arise because each number is tagged independently (example in paper: NAQANet predicted a single minus label and no plus labels leading to a negative count of people). It is also more difficult to generalize to new operations (e.g., Mul/Div) without large-scale architectural changes.",
            "intervention_type": "Architectural modification from QANet to add numeric tags and summation; no external calculator or compositional program selection in the NAQANet design.",
            "effect_of_intervention": "Improved baseline numeric QA compared to plain QANet on DROP but insufficient for many DROP questions; the BERT+Calculator model substantially outperforms NAQANet when equipped with compositional ops and BERT encoder.",
            "performance_metrics": "Reported NAQANet overall dev performance in this paper: EM ≈ 44.24 and F1 ≈ 47.77 (Table 3, baseline), noticeably below the BERT+Calculator models.",
            "notable_failure_modes": "Independent per-number tagging can produce globally inconsistent numeric answers (e.g., predicting a negative population), and the flat scheme does not scale to more diverse operations or compositional arithmetic.",
            "comparison_to_humans_or_symbolic": "NAQANet attempts to produce numeric answers within a purely neural extractive framework without explicit symbolic execution; contrasted with the BERT+Calculator approach which explicitly selects symbolic operations and executes them externally.",
            "uuid": "e3125.1",
            "source_info": {
                "paper_title": "Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension",
                "publication_date_yy_mm": "2019-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "rating": 2
        },
        {
            "paper_title": "Analysing mathematical reasoning abilities of neural models",
            "rating": 2
        },
        {
            "paper_title": "Solving general arithmetic word problems",
            "rating": 2
        },
        {
            "paper_title": "A tag-based statistical english math word problem solver with understanding, reasoning and explanation",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 1
        },
        {
            "paper_title": "MathDQN: Solving arithmetic word problems via deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.0108545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension</h1>
<p>Daniel Andor, Luheng He, Kenton Lee, Emily Pitler<br>Google Research<br>{andor, luheng, kentonl, epitler}@google.com</p>
<h4>Abstract</h4>
<p>Reading comprehension models have been successfully applied to extractive text answers, but it is unclear how best to generalize these models to abstractive numerical answers. We enable a BERT-based reading comprehension model to perform lightweight numerical reasoning. We augment the model with a predefined set of executable 'programs' which encompass simple arithmetic as well as extraction. Rather than having to learn to manipulate numbers directly, the model can pick a program and execute it. On the recent Discrete Reasoning Over Passages (DROP) dataset, designed to challenge reading comprehension models, we show a $33 \%$ absolute improvement by adding shallow programs. The model can learn to predict new operations when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.</p>
<h2>1 Introduction</h2>
<p>End-to-end reading comprehension models have been increasingly successful at extractive question answering. For example, performance on the SQuAD 2.0 (Rajpurkar et al., 2018) benchmark has improved from 66.3 F1 to $89.5^{1}$ in a single year. However, the Discrete Reasoning Over Passages (DROP) (Dua et al., 2019) dataset demonstrates that as long as there is quantitative reasoning involved, there are plenty of relatively straightforward questions that current extractive QA systems find difficult to answer. Other recent work has shown that even state-of-the-art neural models struggle with numerical operations and quantitative reasoning when trained in an end-to-end manner (Saxton et al., 2019; Ravichander et al., 2019). In other words, even BERT (Devlin et al., 2019) is not very good at doing simple calculations.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>How many more Chinese nationals are there than European nationals?
The city of Bangkok has a population of 8,280,925 ...the census showed that it is home to 81,570 Japanese and 55,893 Chinese nationals, as well as 117,071 expatriates from other Asian countries, 48,341 from Europe, 23,418 from the Americas,...</p>
<p>NAQANet: -55893
Ours: Diff(55893, 48341) $=\mathbf{7 5 5 2}$
Table 1: Example from the DROP development set. The correct answer is not explicitly stated in the passage and instead must be computed. The NAQANet model ${ }^{2}$ (Dua et al., 2019) predicts a negative number of people, whereas our model predicts that an operation Diff should be taken and identifies the two arguments.</p>
<p>In this work, we extend an extractive QA system with numerical reasoning abilities. We do so by asking the neural network to synthesize small programs that can be executed. The model picks among simple programs of the form Operation(args, ...), where the possible operations include span extraction, answering yes or no, and arithmetic. For math operations, the arguments are pointers to numbers in the text and, in the case of composition, other operations. In this way, the burden of actually doing the computation is offloaded from the neural network to a calculator tool. The program additionally provides a thin layer of interpretability that mirrors some of the reasoning required for the answer. For example, in Table 1, the model predicts subtraction (Diff) over two numbers in the passage, and executes it to produce the final answer.</p>
<p>We start with a simple extractive question answering model based on BERT (Devlin et al., 2019), and show the following:</p>
<ol>
<li>Predicting unary and binary math operations
<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></li>
</ol>
<p>with arguments resulted in significant improvements on the DROP dataset.
2. Our model can smoothly handle more traditional reading comprehension inputs as well as math problems with new operations. Cotraining with the CoQA (Reddy et al., 2018) dataset improved performance on DROP. The DROP+CoQA trained model had never seen multiplication or division examples, but can learn to predict these two ops when appropriate in a math word problem setting (Roy and Roth, 2015) with very few training examples.</p>
<h2>2 Background and Related Work</h2>
<p>Discrete Reasoning over Paragraphs (DROP) (Dua et al., 2019) is a reading comprehension task that requires discrete reasoning. Inspired by semantic parsing tasks where models need to produce executable 'programs', it keeps the opendomain nature of reading comprehension tasks such as SQuAD 2.0 (Rajpurkar et al., 2018). As shown in Table 1, the system needs to perform fuzzy matching between "from Europe" and "European nationals" in order to identify the arguments.</p>
<p>Numerically-aware QANet (NAQANet) (Dua et al., 2019) is the current state-of-the-art ${ }^{3}$ system for DROP. It extends the QANet model (Yu et al., 2018) with predictions for numbers ( $0-9$ ) and summation operations. For the latter, it performs a 3 -way classification (plus, minus, and zero) on all the numbers in the passage.</p>
<p>While certain binary operations are expressible efficiently with flat sign prediction, it is difficult to generalize the architecture. Moreover, each number is tagged independently, which can cause global inconsistencies; for instance, in Table 1 it assigns a single minus label and no plus labels, leading to a prediction of negative people.</p>
<p>Mathematical Word Problems have been addressed with a wide variety of datasets and approaches; see Zhang et al. (2018) for an overview. One such dataset of arithmetic problems is the Illinois dataset (Roy and Roth, 2015). The problems are posed in simple natural language that has a specific, narrow domain, For example: "If there are 7 bottle caps in a box and Linda puts 7 more bottle caps inside, how many bottle caps are in</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the box?". Unlike DROP, the problems are typically $1-3$ sentences long and do not require reading complex passages. Instead, the main challenge is mathematical reasoning. According to Zhang et al. (2018), the current state of the art uses syntactic parses and deterministic rules to convert the input to logical forms (Liang et al., 2016).</p>
<h2>3 Model</h2>
<p>We extend a BERT-based extractive reading comprehension model with a lightweight extraction and composition layer. For details of the BERT architecture see Devlin et al. (2019). We only rely on the representation of individual tokens that are jointly conditioned on the given question $Q$ and passage $P$. Our model predicts an answer by selecting the top-scoring derivation (i.e. program) and executing it.</p>
<p>Derivations We define the space of possible derivations $\mathcal{D}$ as follows:</p>
<ul>
<li>Literals: ${$ Yes, No, Unknown, $0, \ldots 9}$.</li>
<li>Numerical operations: including various types of numerical compositions of numbers ${ }^{4}$, such as Sum or Diff.</li>
<li>Text spans: composition of tokens into text spans up to a pre-specified length.</li>
<li>Composition of compositions: we only consider two-step compositions, including merging text spans and nested summations.
The full set of operations are listed in Table 2. For example, Sum is a numerical operation that adds two numbers and produces a new number. While we could recursively search for compositions with deep derivations, here we are guided by what is required in the DROP data and simplify inference by heavily restricting multi-step composition. Specifically, spans can be composed into a pair of merged spans (Merge), and the sum of two numbers (Sum) can subsequently be summed with a third (Sum3). The results in Table 3 show the dev set oracle performance using these shallow derivations, by answer type.</li>
</ul>
<p>Representation and Scoring For each derivation $d \in \mathcal{D}$, we compute a vector representation $\mathbf{h}_{d}$ and a scalar score $\rho(d, P, Q)$ using the BERT output vectors. The scores $\rho$ are used for computing the probability $P(d \mid P, Q)$ as well as for pruning. For brevity, we will drop the dependence on $P$ and $Q$ in this section.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Derivations</th>
<th style="text-align: center;">Example Question</th>
<th style="text-align: center;">Answer Derivation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Literals</td>
<td style="text-align: center;">Yes, No, Unknown, 0, 1 ..., 9</td>
<td style="text-align: center;">How many field goals did Stover kick?</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Numerical</td>
<td style="text-align: center;">Diff100: $n_{0} \rightarrow 100-n_{1}$</td>
<td style="text-align: center;">How many percent of the national population does not live in Bangkok?</td>
<td style="text-align: center;">$100-12.6=87.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sum : $n_{0}, n_{1} \rightarrow n_{0}+n_{1}$ as well as: Diff, Mul, Div</td>
<td style="text-align: center;">How many from the census were in Ungheni and Cahul?</td>
<td style="text-align: center;">$\begin{aligned} &amp; 32,828+28,763= \ &amp; 61591 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">Text spans</td>
<td style="text-align: center;">Span: $i, j \rightarrow s$</td>
<td style="text-align: center;">Does Bangkok have more Japanese or Chinese nationals?</td>
<td style="text-align: center;">"Japanese"</td>
</tr>
<tr>
<td style="text-align: center;">Compositions</td>
<td style="text-align: center;">Merge : $s_{0}, s_{1} \rightarrow\left{s_{0}, s_{1}\right}$</td>
<td style="text-align: center;">What languages are spoken by more than $1 \%$, but fewer than $2 \%$ of Richmond's residents?</td>
<td style="text-align: center;">"Hmong-Mien languages", "Laotian" <br> Sum(64.56, 23.13)+</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Sum3 : $n_{0}, n_{1}, n_{2} \rightarrow\left(n_{0}+\right.$ $n_{1}$ ) $+n_{2}$</td>
<td style="text-align: center;">How many residents, in terms of percentage, speak either English, Spanish, or Tagalog?</td>
<td style="text-align: center;">$\operatorname{Sum}(64.56,23.13)+$ $2.11=89.8$</td>
</tr>
</tbody>
</table>
<p>Table 2: Operations supported by the model. $s, n$ refer to arguments of type span and number, respectively. $i, j$ are the start and end indices of span $s$. The omitted definitions of Diff, Mul, and Div are analogous to Sum.</p>
<p>Literals are scored as $\rho(d)=\mathbf{w}<em _lit="{lit" _text="\text">{\mathrm{d}}^{\mathrm{T}} \mathrm{MLP}</em>}}\left(\mathbf{h<em _mathrm_CLS="\mathrm{CLS">{\mathrm{CLS}}\right)$, where $\mathbf{h}</em>$ is the output vector at the [CLS] token of the BERT model (Devlin et al., 2019).}</p>
<p>Numeric operations use the vector representations $\mathbf{h}_{i}$ of the first token of each numeric argument. Binary operations are represented as</p>
<p>$$
\mathbf{h}<em _binary="{binary" _text="\text">{d}=\operatorname{MLP}</em>}}\left(\mathbf{h<em j="j">{i}, \mathbf{h}</em>}, \mathbf{h<em j="j">{i} \circ \mathbf{h}</em>\right)
$$</p>
<p>and scored as $\rho(d)=\mathbf{w}<em d="d">{\mathrm{op}}^{\mathrm{T}} \mathbf{h}</em>}$, where $\mathbf{h<em _mathrm_op="\mathrm{op">{d}$ represents the binary arguments and $o p$ is the operation type. $\circ$ is the Hadamard product. Unary operations such as Diff100 are scored as $\mathbf{w}</em>}}^{\mathrm{T}} \mathrm{MLP<em i="i">{\text {unary }}\left(\mathbf{h}</em>\right)$.</p>
<p>Text spans are scored as if they were another binary operation taking as arguments the start and end indices $i$ and $j$ of the span (Lee et al., 2017):</p>
<p>$$
\mathbf{h}<em _span="{span" _text="\text">{d}=\operatorname{MLP}</em>}}\left(\mathbf{h<em j="j">{i}, \mathbf{h}</em>\right)
$$</p>
<p>and scored as $\rho(d)=\mathbf{w}<em d="d">{\text {span }}^{\mathrm{T}} \mathbf{h}</em>$.
Compositions of compositions are scored with the vector representations of its children. For example, the ternary Sum3, comprising a Sum and a number, is scored with $\mathbf{w}<em _Sum3="{Sum3" _text="\text">{\text {Sum3 }}^{\mathrm{T}} \mathrm{MLP}</em>}}\left(\mathbf{h<em k="k">{d 0}, \mathbf{h}</em>}\right)$, where $\mathbf{h<em k="k">{d 0}$ corresponds to the representation from the first Sum, and $\mathbf{h}</em>}$ is the representation of the third number. The composition of two spans is scored as $\mathbf{w<em _Merge="{Merge" _text="\text">{\text {Merge }}^{\mathrm{T}} \mathrm{MLP}</em>}}\left(\mathbf{h<em 1="1" d="d">{d 0}, \mathbf{h}</em>}, \mathbf{h<em 1="1" d="d">{d 0} \circ \mathbf{h}</em>}\right)$, where $\mathbf{h<em 1="1" d="d">{d 0}$ and $\mathbf{h}</em>}$ are span representations from (2). The intuition for including $\mathbf{h<em 1="1" d="d">{d 0} \circ \mathbf{h}</em>$ is that it encodes span similarity, and spans with similar types are more likely to be merged.</p>
<p>This strategy differs from the NAQANet baseline in a few ways. One straightforward difference is that we use BERT as the base encoder rather than QANet. A more meaningful difference is that we model all derivations in the unified op scoring
framework described above, which allows generalizing to new operations, whereas NAQANet would require more large-scale changes to go beyond addition and subtraction. Generalizing the model to new ops is a case of extending the derivations and scoring functions. In Section 4, we will show the impact of incrementally adding Diff100, Sum3, and Merge.</p>
<h3>3.1 Training</h3>
<p>We used exhaustive pre-computed oracle derivations $\mathcal{D}^{<em>}$ following Dua et al. (2019). We marginalized out all derivations $d^{</em>}$ that lead to the answer ${ }^{5}$ and minimized:</p>
<p>$$
\begin{aligned}
\mathcal{J}\left(P, Q, \mathcal{D}^{<em>}\right) &amp; =-\log \sum_{d^{</em>} \in \mathcal{D}^{<em>}} P(d^{</em>}|P, Q) \
P(d \mid P, Q) &amp; =\frac{\exp \rho(d, P, Q)}{\sum_{d^{\prime}} \exp \rho\left(d^{\prime}, P, Q\right)}
\end{aligned}
$$</p>
<p>If no derivation lead to the gold answer ( $\mathcal{D}^{*}$ is empty), we skipped the example.</p>
<p>Pruning During inference, the Merge and Sum3 operations are composed from the results of Span and Sum operations, respectively. The space of possible results of Merge is quadratic in the number $|\mathcal{S}|$ of possible spans. With $|\mathcal{S}| \sim 10^{4}$, the complete set of Merge instances becomes overwhelming. Similarly, with $|\mathcal{N}| \sim 100$ numbers in each passage, there are millions of possible Sum3 derivations. To do training and inference efficiently, we kept only the top 128 Span and Sum results when computing Merge and Sum3. ${ }^{6}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Oracle <br> Dev EM</th>
<th style="text-align: center;">Overall Dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Overall Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Date (1.6\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Number (62\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Span (32\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Spans (4.4\%)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: center;">NAQANet</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">46.75</td>
<td style="text-align: center;">50.39</td>
<td style="text-align: center;">44.24</td>
<td style="text-align: center;">47.77</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">27.3</td>
</tr>
<tr>
<td style="text-align: center;">Our basic ${ }^{7}$</td>
<td style="text-align: center;">80.03</td>
<td style="text-align: center;">66.50</td>
<td style="text-align: center;">69.91</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">35.7</td>
</tr>
<tr>
<td style="text-align: center;">+Diff100</td>
<td style="text-align: center;">88.75</td>
<td style="text-align: center;">75.52</td>
<td style="text-align: center;">78.82</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">35.8</td>
</tr>
<tr>
<td style="text-align: center;">+Sum3</td>
<td style="text-align: center;">90.16</td>
<td style="text-align: center;">76.70</td>
<td style="text-align: center;">80.06</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">36.0</td>
</tr>
<tr>
<td style="text-align: center;">+Merge</td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;">76.95</td>
<td style="text-align: center;">80.48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">45.0</td>
</tr>
<tr>
<td style="text-align: center;">+CoQA</td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;">78.00</td>
<td style="text-align: center;">81.56</td>
<td style="text-align: center;">76.93</td>
<td style="text-align: center;">80.47</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">46.8</td>
</tr>
<tr>
<td style="text-align: center;">+Ensemble</td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;">78.95</td>
<td style="text-align: center;">82.54</td>
<td style="text-align: center;">78.15</td>
<td style="text-align: center;">81.78</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracies on the DROP dev and test set in terms of exact match (EM) and token-level F1. The righthand columns show the performance breakdown with different answer types on the development set. The largest improvements come from Date, Number, and Spans (answers with multiple spans). Oracle rows and columns indicate the performance that could be achieved by perfect selections of derivations. The ensemble used 6 models.</p>
<p>Spurious ambiguities Of the answers for which we could find at least one oracle derivation, $36 \%$ had two or more alternatives. During training, the model became effective at resolving many of these ambiguities. We monitored the entropy of $P\left(d^{*} \mid P, Q\right)$ for the ambiguous examples as training progressed. At the start, the entropy was 2.5 bits, which matches the average ambiguous oracle length of $\sim 6$ alternatives. By the end of 4 epochs, the average entropy had dropped to $&lt;0.2$ bits, comparable to a typical certainty of $95-99 \%$ that one of the derivations is the correct one.</p>
<h2>4 Experiments</h2>
<p>Our main experiments pertain to DROP (Dua et al., 2019), using DROP and, optionally, CoQA (Reddy et al., 2018) data for training. Preprocessing and hyperparameter details are given in the supplementary material. In addition to full DROP results, we performed ablation experiments for the incremental addition of the Diff100, Sum3, and Merge operations, and finally the CoQA training data. We ran on the CoQA dev set, to show that the model co-trained on CoQA can still perform traditional reading comprehension. To investigate our model's ability to do symbolic reasoning at the other extreme, we performed fewshot learning experiments on the Illinois dataset of math problems (Roy and Roth, 2015).</p>
<h3>4.1 DROP Results</h3>
<p>As shown in Table 3, our model achieves over $50 \%$ relative improvement (over $33 \%$ absolute) over the previous state-of-the-art NAQANet system. The ablations indicate that the improvements due to the addition of extra ops (Diff100, Sum3,</p>
<p>Merge) are roughly consistent with their proportion in the data. Specifically, the Diff100 and Sum3 derivations increase the oracle performance by $8.7 \%$ and $1.4 \%$ respectively, corresponding to model improvements of roughly $9 \%$ and $1.1 \%$, respectively. Answers requiring two spans occur about $2.8 \%$ of the time, which is a $60.4 \%$ proportion of the Spans answer type. Merge only improves the Spans answer type by $9 \%$, which we think is due to the significant 11:1 class imbalance between competing single and multiple spans. As a result, multiple spans are under-predicted, leaving considerable headroom there.</p>
<p>Pre-training on CoQA then fine-tuning on DROP lead to our best results on DROP, reported in Table 3. After fine-tuning on DROP, the model forgot how to do CoQA, with an overall F1 score of 52.2 on the CoQA dev set. If one prefers a model competent in both types of input, then the forgetting can be prevented by fine-tuning on both CoQA and DROP datasets simultaneously. This resulted in dev set F1 scores of 82.2 on CoQA and 81.1 on DROP. The CoQA performance is decent and compares well with the pre-trained model performance of 82.5 . The $0.5 \%$ drop in DROP performance is likely attributable to the difference between pre-training versus fine-tuning on CoQA.</p>
<p>We ensembled 6 models ( 3 seeds $\times 2$ learning rates) for an additional $1 \%$ improvement.</p>
<h3>4.2 Results on Math Word Problems</h3>
<p>We trained our model on the Illinois math word problems dataset (Roy and Roth, 2015), which contains answers requiring multiplication and</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Roy et al. (2015)</th>
<th style="text-align: left;">73.9</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Liang et al. (2016)</td>
<td style="text-align: left;">$\mathbf{8 0 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Wang et al. (2018)</td>
<td style="text-align: left;">73.3</td>
</tr>
<tr>
<td style="text-align: left;">Our basic: IL data</td>
<td style="text-align: left;">$48.6 \pm 5.3$</td>
</tr>
<tr>
<td style="text-align: left;">+ Mul and Div</td>
<td style="text-align: left;">$74.0 \pm 6.0$</td>
</tr>
<tr>
<td style="text-align: left;">+ DROP data</td>
<td style="text-align: left;">$\mathbf{8 3 . 2} \pm \mathbf{6 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy on the Illinois (IL) dataset ${ }^{8}$ of 562 single-step word problems, using the five crossvalidation folds of Roy and Roth (2015). Standard deviations were computed from the five folds. Roughly half the questions require the use of Sum and Diff, and half require Mul and Div.
division-operations not present in DROP-as well as addition and subtraction, in roughly equal proportion. Given the small $(N=562)$ dataset size, training and evaluation is done with five-fold cross-validation on a standardized set of splits. As shown in Table 4, when we added Mul and Div to our basic DROP operations, the model was able to learn to use them. Transferring from the DROP dataset further improved performance beyond that of Liang et al. (2016), a model specific to math word problems that uses rules over dependency trees. Compared to other more general systems, our model outperforms the deep reinforcement learning based approach of Wang et al. (2018).</p>
<h2>5 Conclusions and Future Work</h2>
<p>We proposed using BERT for reading comprehension combined with lightweight neural modules for computation in order to smoothly handle both traditional factoid question answering and questions requiring symbolic reasoning in a single unified model. On the DROP dataset, which includes a mix of reading comprehension and numerical reasoning, our model achieves a $33 \%$ absolute improvement over the previous best. The same model can also do standard reading comprehension on CoQA, and focused numerical reasoning on math word problems. We plan to generalize this model to more complex and compositional answers, with better searching and pruning strategies of the derivations.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Chris Alberti and Livio Baldini Soares for tremendously helpful discussions, and we are grateful to all members of the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Google Research Language team.</p>
<h2>References</h2>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL.</p>
<p>Kenton Lee, Tom Kwiatkowski, Ankur P. Parikh, and Dipanjan Das. 2017. Learning recurrent span representations for extractive question answering. CoRR, abs/1611.01436.</p>
<p>Chao-Chun Liang, Kuang-Yi Hsu, Chien-Tsung Huang, Chung-Min Li, Shen-Yu Miao, and Keh-Yih Su. 2016. A tag-based statistical english math word problem solver with understanding, reasoning and explanation. In IJCAI.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789.</p>
<p>Abhilasha Ravichander, Aakanksha Naik, Carolyn Penstein Rosé, and Eduard H. Hovy. 2019. Equate: A benchmark evaluation framework for quantitative reasoning in natural language inference. CoRR, abs/1901.03735.</p>
<p>Siva Reddy, Danqi Chen, and Christopher D. Manning. 2018. CoQA: A conversational question answering challenge. CoRR, abs/1808.07042.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In EMNLP.</p>
<p>Subhro Roy, Tim Vieira, and Dan Roth. 2015. Reasoning about quantities in natural language. Transactions of the Association for Computational Linguistics, 3:1-13.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. CoRR, abs/1904.01557.</p>
<p>Lei Wang, Dongxiang Zhang, Lianli Gao, Jingkuan Song, Long Guo, and Heng Tao Shen. 2018. MathDQN: Solving arithmetic word problems via deep reinforcement learning. In AAAI.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. Proceedings of ICLR.</p>
<p>Dongxiang Zhang, Lei Wang, Nuo Xu, Bing Tian Dai, and Heng Tao Shen. 2018. The gap of semantic parsing: A survey on automatic math word problem solvers. IEEE transactions on pattern analysis and machine intelligence.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://cogcomp.org/page/resource_view/98&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Numbers are heuristically extracted from the text.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
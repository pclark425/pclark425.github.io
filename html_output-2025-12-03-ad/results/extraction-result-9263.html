<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9263 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9263</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9263</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-276885379</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.05565v1.pdf" target="_blank">Evaluating open-source Large Language Models for automated fact-checking</a></p>
                <p><strong>Paper Abstract:</strong> The increasing prevalence of online misinformation has heightened the demand for automated fact-checking solutions. Large Language Models (LLMs) have emerged as potential tools for assisting in this task, but their effectiveness remains uncertain. This study evaluates the fact-checking capabilities of various open-source LLMs, focusing on their ability to assess claims with different levels of contextual information. We conduct three key experiments: (1) evaluating whether LLMs can identify the semantic relationship between a claim and a fact-checking article, (2) assessing models' accuracy in verifying claims when given a related fact-checking article, and (3) testing LLMs' fact-checking abilities when leveraging data from external knowledge sources such as Google and Wikipedia. Our results indicate that LLMs perform well in identifying claim-article connections and verifying fact-checked stories but struggle with confirming factual news, where they are outperformed by traditional fine-tuned models such as RoBERTa. Additionally, the introduction of external knowledge does not significantly enhance LLMs' performance, calling for more tailored approaches. Our findings highlight both the potential and limitations of LLMs in automated fact-checking, emphasizing the need for further refinements before they can reliably replace human fact-checkers.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9263.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9263.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ZS+FullArticle (Llama3 8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-Shot prompt with full-article context on Llama3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that for Llama3 8B, providing the full fact-checking article with a relatively simple/zero-shot prompt (i.e., no complex prompt modules) yielded the highest F1 across prompt configurations for Task 1 (article-claim connection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 1: Understanding the article-statement connection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decide whether a given claim and a provided fact-checking article are related (article is fact-checking the claim) or unrelated.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot style: Role + Task + JSON + Final modules; full article text provided as the contextual input; no Few-Shot examples or Chain-of-Thought. Prompts used the structured JSON output requirement and asked for an explanation and a score (0-100).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to enriched prompts (detailed class definitions), few-shot and Chain-of-Thought variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors report that Llama3 8B performed best when given the full article and a straightforward prompt rather than more complex prompting strategies—suggesting that for this smaller 8B model, providing richer raw context (full article) is more helpful than elaborate prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Task1 used balanced pairs (related vs random unrelated articles). Prompts built from 24 prompt configurations; Zero-shot variant included Role, Task, JSON, Final modules. Models called via HuggingFace API with temperature=0.1, max new tokens=256. Article lengths varied widely (up to ~20k words); full-article inputs were truncated/limited to fit context window.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9263.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EnrichedPrompt (Llama3 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enriched-class-definition prompts on Llama3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing enriched prompts (explicit, detailed criteria for True/False categories) improved Task 1 performance for larger Llama3 70B, which achieved the strongest results across prompt configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 1: Understanding the article-statement connection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Judge whether a claim and article are contextually related.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Enriched prompt module: Role + Task + Enriched criteria (detailed label definitions) + JSON + Final; tested both zero-shot and few-shot variants but with explicit enriched criteria added.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Zero-Shot (simple prompts), Few-Shot, Chain-of-Thought and other prompt modules.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1 ≈ 0.9 (reported as approximately 0.9 depending on prompt configuration) for overall Task 1 performance under best prompt configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperformed Llama3 8B and Mixtral 8x7B on the same task and prompt variants; also outperformed fine-tuned RoBERTa baseline on Task 1 in F1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger-capacity Llama3 70B benefits from more explicit, enriched prompt instructions that clarify class boundaries, improving ability to map article content to the claim and produce structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>24 prompt configurations total; enriched prompts included explicit examples of what constitutes False (fake quotes, conspiracies) vs True. Fault rate very low for Llama3 70B (median faults 0.003) in Task 1. Models executed with temperature=0.1 and 256 new tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9263.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral Avg Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7B average performance across prompt formats (Task 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mixtral 8x7B showed the weakest and most variable performance across prompt formats on Task 1, with an average F1 far below the larger Llama3 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 1: Understanding the article-statement connection</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary related/unrelated classification of claim-article pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Evaluated across 24 prompt configurations including Zero-Shot, Few-Shot (one example per class), Chain-of-Thought, Enriched criteria, Self-Reflection, Summary; no single format dominated for Mixtral.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across all prompt formats; also compared to RoBERTa baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>F1 average ≈ 0.65 (reported average across prompt variations for Mixtral 8x7B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Lower than Llama3 70B (≈0.9) and sometimes below random threshold (reported occasional drops <0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller/ensemble 8x7B Mixtral model appears more sensitive to prompt choice and less able to benefit from enriched prompting; overall underperforms larger Llama3 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>24 prompt settings; faulty response rate high (median faults 0.393 in Task 1). Few-shot used one example per class, encapsulated in square brackets. Chain-of-Thought prompts appended 'Let's think step-by-step' in CoT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9263.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Complexity -> Faults</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt complexity on faulty/invalid responses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that more complex prompt pipelines (many modules, strict JSON formatting, longer context) increased the rate of faulty outputs (invalid/malformed responses), particularly for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 1 (and Task 2) - Prompt robustness / output validity</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure of model adherence to expected JSON output format and provision of a valid score/explanation under different prompt complexities.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Complex prompts combining Role, Task, JSON, Final plus optional modules (Enriched, Self-Reflection, Summary, Chain-of-Thought); strict JSON-required outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to simpler prompts (e.g., Role+Task+JSON only) and across models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported median faulty-response rates (Task 1): Llama3 8B median faults = 0.241; Llama3 70B median faults = 0.003; Mixtral 8x7B median faults = 0.393.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Smaller models (Llama3 8B, Mixtral) produced far more faulty outputs under the same prompt configurations than Llama3 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Fault rate difference: Llama3 8B vs Llama3 70B median ≈ +0.238 (i.e., ~24.1% vs 0.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller models struggle to consistently follow strict output formatting and multi-step/long prompts, so increasing prompt complexity raises invalid-output frequency for them; larger model shows greater robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors observed faults increased when token limits applied for some models; overall ~1-2% null outputs for JSON prompts, but significantly larger median faults for smaller models. Prompts required explanation then a 0–100 score; any malformed JSON counted as a fault.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9263.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT / Few-Shot (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought and Few-Shot prompting across models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study evaluated Few-Shot (one example per class) and Chain-of-Thought (append 'Let's think step-by-step') prompting but found no single prompting strategy that consistently dominated across all models and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (Llama3 8B, Llama3 70B, Mixtral 8x7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks 1 & 2 (and evaluated across Task 3 prompt design in zero-shot ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot: provide one example per class; CoT: encourage stepwise reasoning; used in attempts to improve claim/article understanding and verdict extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-Shot: one illustrative example per class included (examples encapsulated in square brackets); CoT: examples + 'Let's think step-by-step' appended to encourage reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to Zero-Shot and Enriched criteria prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors observe that prompt effectiveness is model-dependent; CoT and Few-Shot sometimes help but do not uniformly improve performance across all evaluated LLMs. Thus, prompt choice must be tailored to model capacity and task.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Few-shot used one example per class (to avoid one-shot brittleness); examples randomly selected per iteration; Chain-of-Thought used the canonical 'Let's think step-by-step' cue. Total of 24 prompt modular configurations tested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9263.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Article-based Verdicts (Llama3 70B, Task2)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using provided fact-checking article to judge claim veracity on Llama3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When given the associated fact-checking article, larger Llama3 70B can extract the article's verdict and achieve high F1 on the negative class, while performing worse on the rarer positive class.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 2: Providing an accurate verdict based on the fact-checking article</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given an article that explicitly contains the verdict on the claim, produce the claim evaluation (mapped to True/False) and a supporting explanation/score.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts included Role + Task + JSON + Final; article text provided as context (full article or summary variants tested); authors also tested enriched prompts, CoT and few-shot variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to fine-tuned RoBERTa baseline and to other prompt formats (e.g., enriched vs simple).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Negative class F1 ≈ 0.9 (Llama3 70B) on Task 2; RoBERTa baseline reported negative-class F1 ≈ 0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Llama3 70B came close to but did not fully match RoBERTa on the negative class; on the positive (scarcer) class Llama3 70B scored >0.6 while RoBERTa remained stronger.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger LLMs can accurately extract and reason over information explicitly contained in an article (particularly for false/fake cases), but struggle more with the minority positive class; prompt design again showed model-dependent effects.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Task 2 used 1,000 samples; prompts tested all 24 configurations. Faulty response rates increased for some prompt types (Llama3 70B reached nearly 30% faults for some prompts). RoBERTa was fine-tuned on article-claim pairs with max sequence length 512 (articles truncated).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9263.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9263.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct+ExternalFormats (Task3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct framework with different external-information presentation formats (snippets, full article, LLM-generated summary) on Task 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For zero-shot fact-checking with external retrieval (ReAct agent using Wikipedia or Google), the paper finds that presenting retrieved information as concise structured summaries yields better model performance than raw full pages or isolated snippets, but overall LLMs still underperform a fine-tuned RoBERTa baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Meta-Llama-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Task 3: Fact-checking the claim with external knowledge (ReAct)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The LLM acts as an agent that may query external sources (Wikipedia API, Google search limited to time before claim) once (one ReAct iteration) to retrieve evidence, then returns a verdict and score.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>ReAct agent with one reasoning iteration allowed; external tool returns either: (a) snippets (top-3 snippets), (b) full scraped article text (truncated to context window), or (c) LLM-generated summaries of retrieved pages (three summaries). The agent used structured ReAct JSON prompt with Action/Final answer modes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared snippets vs full article vs LLM-generated summaries; compared use of Google vs Wikipedia vs no external context; compared to RoBERTa fine-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Models' classification accuracy improved when provided with structured summaries versus raw full pages or snippets, but overall LLMs were still outperformed by the fine-tuned RoBERTa baseline on Task 3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>External summaries > full article > snippets for LLM performance; however, none of these configurations allowed LLMs to surpass RoBERTa on overall Task 3 performance (RoBERTa remained superior).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that concise, organized contextual information reduces noise and better fits the model's inference process; however, LLMs' outdated parametric knowledge and potential reliance on stylistic cues limit overall gains from external evidence. They also note mixed signals in conclusions (intro of external knowledge did not significantly enhance overall LLM performance).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>ReAct limited to one iteration; Wikipedia tool returned top-3 matches (title, link, snippet); Google retrieval via Selenium scanned first 20 results then top-3 filtered within a time window (up to one week before claim date). Summaries were generated by additional LLM calls (more expensive). Models executed with temperature=0.1 and 256 new tokens. Dataset: whole dataset for Task 3; temporal analysis split before-2024 vs 2024 claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating open-source Large Language Models for automated fact-checking', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method <em>(Rating: 2)</em></li>
                <li>Towards mitigating LLM hallucination via self reflection <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9263",
    "paper_id": "paper-276885379",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "ZS+FullArticle (Llama3 8B)",
            "name_full": "Zero-Shot prompt with full-article context on Llama3 8B",
            "brief_description": "The paper reports that for Llama3 8B, providing the full fact-checking article with a relatively simple/zero-shot prompt (i.e., no complex prompt modules) yielded the highest F1 across prompt configurations for Task 1 (article-claim connection).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "Task 1: Understanding the article-statement connection",
            "task_description": "Decide whether a given claim and a provided fact-checking article are related (article is fact-checking the claim) or unrelated.",
            "presentation_format": "Zero-shot style: Role + Task + JSON + Final modules; full article text provided as the contextual input; no Few-Shot examples or Chain-of-Thought. Prompts used the structured JSON output requirement and asked for an explanation and a score (0-100).",
            "comparison_format": "Compared to enriched prompts (detailed class definitions), few-shot and Chain-of-Thought variants.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors report that Llama3 8B performed best when given the full article and a straightforward prompt rather than more complex prompting strategies—suggesting that for this smaller 8B model, providing richer raw context (full article) is more helpful than elaborate prompt engineering.",
            "null_or_negative_result": false,
            "experimental_details": "Task1 used balanced pairs (related vs random unrelated articles). Prompts built from 24 prompt configurations; Zero-shot variant included Role, Task, JSON, Final modules. Models called via HuggingFace API with temperature=0.1, max new tokens=256. Article lengths varied widely (up to ~20k words); full-article inputs were truncated/limited to fit context window.",
            "uuid": "e9263.0",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "EnrichedPrompt (Llama3 70B)",
            "name_full": "Enriched-class-definition prompts on Llama3 70B",
            "brief_description": "Providing enriched prompts (explicit, detailed criteria for True/False categories) improved Task 1 performance for larger Llama3 70B, which achieved the strongest results across prompt configurations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-70B-Instruct",
            "model_size": "70B",
            "task_name": "Task 1: Understanding the article-statement connection",
            "task_description": "Judge whether a claim and article are contextually related.",
            "presentation_format": "Enriched prompt module: Role + Task + Enriched criteria (detailed label definitions) + JSON + Final; tested both zero-shot and few-shot variants but with explicit enriched criteria added.",
            "comparison_format": "Compared to Zero-Shot (simple prompts), Few-Shot, Chain-of-Thought and other prompt modules.",
            "performance": "F1 ≈ 0.9 (reported as approximately 0.9 depending on prompt configuration) for overall Task 1 performance under best prompt configurations.",
            "performance_comparison": "Outperformed Llama3 8B and Mixtral 8x7B on the same task and prompt variants; also outperformed fine-tuned RoBERTa baseline on Task 1 in F1.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Larger-capacity Llama3 70B benefits from more explicit, enriched prompt instructions that clarify class boundaries, improving ability to map article content to the claim and produce structured outputs.",
            "null_or_negative_result": false,
            "experimental_details": "24 prompt configurations total; enriched prompts included explicit examples of what constitutes False (fake quotes, conspiracies) vs True. Fault rate very low for Llama3 70B (median faults 0.003) in Task 1. Models executed with temperature=0.1 and 256 new tokens.",
            "uuid": "e9263.1",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Mixtral Avg Prompting",
            "name_full": "Mixtral-8x7B average performance across prompt formats (Task 1)",
            "brief_description": "Mixtral 8x7B showed the weakest and most variable performance across prompt formats on Task 1, with an average F1 far below the larger Llama3 70B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-Instruct",
            "model_size": "8x7B",
            "task_name": "Task 1: Understanding the article-statement connection",
            "task_description": "Binary related/unrelated classification of claim-article pairs.",
            "presentation_format": "Evaluated across 24 prompt configurations including Zero-Shot, Few-Shot (one example per class), Chain-of-Thought, Enriched criteria, Self-Reflection, Summary; no single format dominated for Mixtral.",
            "comparison_format": "Compared across all prompt formats; also compared to RoBERTa baseline.",
            "performance": "F1 average ≈ 0.65 (reported average across prompt variations for Mixtral 8x7B).",
            "performance_comparison": "Lower than Llama3 70B (≈0.9) and sometimes below random threshold (reported occasional drops &lt;0.5).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Smaller/ensemble 8x7B Mixtral model appears more sensitive to prompt choice and less able to benefit from enriched prompting; overall underperforms larger Llama3 70B.",
            "null_or_negative_result": false,
            "experimental_details": "24 prompt settings; faulty response rate high (median faults 0.393 in Task 1). Few-shot used one example per class, encapsulated in square brackets. Chain-of-Thought prompts appended 'Let's think step-by-step' in CoT variants.",
            "uuid": "e9263.2",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Prompt Complexity -&gt; Faults",
            "name_full": "Effect of prompt complexity on faulty/invalid responses",
            "brief_description": "The paper reports that more complex prompt pipelines (many modules, strict JSON formatting, longer context) increased the rate of faulty outputs (invalid/malformed responses), particularly for smaller models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-8B-Instruct",
            "model_size": "8B",
            "task_name": "Task 1 (and Task 2) - Prompt robustness / output validity",
            "task_description": "Measure of model adherence to expected JSON output format and provision of a valid score/explanation under different prompt complexities.",
            "presentation_format": "Complex prompts combining Role, Task, JSON, Final plus optional modules (Enriched, Self-Reflection, Summary, Chain-of-Thought); strict JSON-required outputs.",
            "comparison_format": "Compared to simpler prompts (e.g., Role+Task+JSON only) and across models.",
            "performance": "Reported median faulty-response rates (Task 1): Llama3 8B median faults = 0.241; Llama3 70B median faults = 0.003; Mixtral 8x7B median faults = 0.393.",
            "performance_comparison": "Smaller models (Llama3 8B, Mixtral) produced far more faulty outputs under the same prompt configurations than Llama3 70B.",
            "format_effect_size": "Fault rate difference: Llama3 8B vs Llama3 70B median ≈ +0.238 (i.e., ~24.1% vs 0.3%).",
            "explanation_or_hypothesis": "Smaller models struggle to consistently follow strict output formatting and multi-step/long prompts, so increasing prompt complexity raises invalid-output frequency for them; larger model shows greater robustness.",
            "null_or_negative_result": false,
            "experimental_details": "Authors observed faults increased when token limits applied for some models; overall ~1-2% null outputs for JSON prompts, but significantly larger median faults for smaller models. Prompts required explanation then a 0–100 score; any malformed JSON counted as a fault.",
            "uuid": "e9263.3",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "CoT / Few-Shot (general)",
            "name_full": "Chain-of-Thought and Few-Shot prompting across models",
            "brief_description": "The study evaluated Few-Shot (one example per class) and Chain-of-Thought (append 'Let's think step-by-step') prompting but found no single prompting strategy that consistently dominated across all models and tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (Llama3 8B, Llama3 70B, Mixtral 8x7B)",
            "model_size": null,
            "task_name": "Tasks 1 & 2 (and evaluated across Task 3 prompt design in zero-shot ReAct)",
            "task_description": "Few-shot: provide one example per class; CoT: encourage stepwise reasoning; used in attempts to improve claim/article understanding and verdict extraction.",
            "presentation_format": "Few-Shot: one illustrative example per class included (examples encapsulated in square brackets); CoT: examples + 'Let's think step-by-step' appended to encourage reasoning.",
            "comparison_format": "Compared to Zero-Shot and Enriched criteria prompts.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors observe that prompt effectiveness is model-dependent; CoT and Few-Shot sometimes help but do not uniformly improve performance across all evaluated LLMs. Thus, prompt choice must be tailored to model capacity and task.",
            "null_or_negative_result": true,
            "experimental_details": "Few-shot used one example per class (to avoid one-shot brittleness); examples randomly selected per iteration; Chain-of-Thought used the canonical 'Let's think step-by-step' cue. Total of 24 prompt modular configurations tested.",
            "uuid": "e9263.4",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Article-based Verdicts (Llama3 70B, Task2)",
            "name_full": "Using provided fact-checking article to judge claim veracity on Llama3 70B",
            "brief_description": "When given the associated fact-checking article, larger Llama3 70B can extract the article's verdict and achieve high F1 on the negative class, while performing worse on the rarer positive class.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-70B-Instruct",
            "model_size": "70B",
            "task_name": "Task 2: Providing an accurate verdict based on the fact-checking article",
            "task_description": "Given an article that explicitly contains the verdict on the claim, produce the claim evaluation (mapped to True/False) and a supporting explanation/score.",
            "presentation_format": "Prompts included Role + Task + JSON + Final; article text provided as context (full article or summary variants tested); authors also tested enriched prompts, CoT and few-shot variants.",
            "comparison_format": "Compared to fine-tuned RoBERTa baseline and to other prompt formats (e.g., enriched vs simple).",
            "performance": "Negative class F1 ≈ 0.9 (Llama3 70B) on Task 2; RoBERTa baseline reported negative-class F1 ≈ 0.95.",
            "performance_comparison": "Llama3 70B came close to but did not fully match RoBERTa on the negative class; on the positive (scarcer) class Llama3 70B scored &gt;0.6 while RoBERTa remained stronger.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Larger LLMs can accurately extract and reason over information explicitly contained in an article (particularly for false/fake cases), but struggle more with the minority positive class; prompt design again showed model-dependent effects.",
            "null_or_negative_result": false,
            "experimental_details": "Task 2 used 1,000 samples; prompts tested all 24 configurations. Faulty response rates increased for some prompt types (Llama3 70B reached nearly 30% faults for some prompts). RoBERTa was fine-tuned on article-claim pairs with max sequence length 512 (articles truncated).",
            "uuid": "e9263.5",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ReAct+ExternalFormats (Task3)",
            "name_full": "ReAct framework with different external-information presentation formats (snippets, full article, LLM-generated summary) on Task 3",
            "brief_description": "For zero-shot fact-checking with external retrieval (ReAct agent using Wikipedia or Google), the paper finds that presenting retrieved information as concise structured summaries yields better model performance than raw full pages or isolated snippets, but overall LLMs still underperform a fine-tuned RoBERTa baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Meta-Llama-3-70B-Instruct",
            "model_size": "70B",
            "task_name": "Task 3: Fact-checking the claim with external knowledge (ReAct)",
            "task_description": "The LLM acts as an agent that may query external sources (Wikipedia API, Google search limited to time before claim) once (one ReAct iteration) to retrieve evidence, then returns a verdict and score.",
            "presentation_format": "ReAct agent with one reasoning iteration allowed; external tool returns either: (a) snippets (top-3 snippets), (b) full scraped article text (truncated to context window), or (c) LLM-generated summaries of retrieved pages (three summaries). The agent used structured ReAct JSON prompt with Action/Final answer modes.",
            "comparison_format": "Compared snippets vs full article vs LLM-generated summaries; compared use of Google vs Wikipedia vs no external context; compared to RoBERTa fine-tuned baseline.",
            "performance": "Qualitative: Models' classification accuracy improved when provided with structured summaries versus raw full pages or snippets, but overall LLMs were still outperformed by the fine-tuned RoBERTa baseline on Task 3.",
            "performance_comparison": "External summaries &gt; full article &gt; snippets for LLM performance; however, none of these configurations allowed LLMs to surpass RoBERTa on overall Task 3 performance (RoBERTa remained superior).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize that concise, organized contextual information reduces noise and better fits the model's inference process; however, LLMs' outdated parametric knowledge and potential reliance on stylistic cues limit overall gains from external evidence. They also note mixed signals in conclusions (intro of external knowledge did not significantly enhance overall LLM performance).",
            "null_or_negative_result": true,
            "experimental_details": "ReAct limited to one iteration; Wikipedia tool returned top-3 matches (title, link, snippet); Google retrieval via Selenium scanned first 20 results then top-3 filtered within a time window (up to one week before claim date). Summaries were generated by additional LLM calls (more expensive). Models executed with temperature=0.1 and 256 new tokens. Dataset: whole dataset for Task 3; temporal analysis split before-2024 vs 2024 claims.",
            "uuid": "e9263.6",
            "source_info": {
                "paper_title": "Evaluating open-source Large Language Models for automated fact-checking",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method",
            "rating": 2,
            "sanitized_title": "towards_llmbased_fact_verification_on_news_claims_with_a_hierarchical_stepbystep_prompting_method"
        },
        {
            "paper_title": "Towards mitigating LLM hallucination via self reflection",
            "rating": 2,
            "sanitized_title": "towards_mitigating_llm_hallucination_via_self_reflection"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        }
    ],
    "cost": 0.016907,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating open-source Large Language Models for automated fact-checking
7 Mar 2025</p>
<p>Nicolò Fontana nicolo.fontana@polimi.it 
Politecnico di Milano
Italy</p>
<p>Francesco Corso francesco.corso@polimi.it 
Politecnico di Milano
Italy</p>
<p>CENTAI
Italy</p>
<p>Enrico Zuccolotto 
Politecnico di Milano
Italy</p>
<p>Francesco Pierri francesco.pierri@polimi.it 
Politecnico di Milano
Italy</p>
<p>Evaluating open-source Large Language Models for automated fact-checking
7 Mar 2025E90F534100AAF1D8305BD1CB2B9995EBarXiv:2503.05565v1[cs.CY]fact-checkinglarge language modelsprompting analysis
The increasing prevalence of online misinformation has heightened the demand for automated fact-checking solutions.Large Language Models (LLMs) have emerged as potential tools for assisting in this task, but their effectiveness remains uncertain.This study evaluates the fact-checking capabilities of various open-source LLMs, focusing on their ability to assess claims with different levels of contextual information.We conduct three key experiments: (1) evaluating whether LLMs can identify the semantic relationship between a claim and a fact-checking article, (2) assessing models' accuracy in verifying claims when given a related fact-checking article, and (3) testing LLMs' fact-checking abilities when leveraging data from external knowledge sources such as Google and Wikipedia.Our results indicate that LLMs perform well in identifying claimarticle connections and verifying fact-checked stories but struggle with confirming factual news, where they are outperformed by traditional fine-tuned models such as RoBERTa.Additionally, the introduction of external knowledge does not significantly enhance LLMs' performance, calling for more tailored approaches.Our findings highlight both the potential and limitations of LLMs in automated fact-checking, emphasizing the need for further refinements before they can reliably replace human fact-checkers.</p>
<p>I. INTRODUCTION</p>
<p>In today's digital era, the vast availability of online information has facilitated the rapid spread of both misinformation and disinformation.Online social platforms, in particular, enable false narratives to gain traction, due to their highconnectivity nature [1].This challenge is amplified when influential individuals propagate misleading content, with research suggesting that such misinformation can significantly impact critical events, including election outcomes [2], [3].</p>
<p>The burden of fact-checking and debunking false claims has been traditionally left to journalists.However, the rise of the Internet, combined with growing distrust in traditional media [4], has led to the emergence of independent factchecking organizations [5].These groups focus on verifying rumors, misconceptions, and fake news spread online.One of the most notable fact-checking initiatives, Politifact 1 , received the Pulitzer Prize for national reporting in 2009 2 .Politifact introduced a rating system to evaluate claims, and many organizations have since adopted similar systems.However, these subjective and often ambiguous ratings complicate comparisons across fact-checks [6].</p>
<p>Fact-checking remains a labor-intensive process, requiring teams to spend days or even weeks verifying claims [7].Given the overwhelming flow of information online 3 , traditional methods cannot keep pace, highlighting the need for more efficient approaches.</p>
<p>One promising avenue is the automation of fact-checking through artificial intelligence (AI) technologies [8].Researchers have investigated AI-driven models, such as Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs), to aid in these efforts.While these models have made significant progress, their effectiveness remains limited [9], [10].More recently, Large Language Models (LLMs) based on transformer architectures have demonstrated significant potential [11], [12].These models excel at generating natural language responses, answering complex questions, and producing high-quality content [13], [14], making them an accessible and versatile tool for a broad audience.Their advanced reasoning capabilities further position them as suitable candidates for fact-checking [15].</p>
<p>However, a major limitation of LLMs lies in the outdated nature of their training data [16], which hampers their ability to address recent misinformation effectively.To address this challenge, researchers are exploring innovative approaches to integrate real-time, external knowledge into these models.Such advancements aim to enhance their ability to provide accurate and timely responses to rapidly evolving misinformation [17].</p>
<p>Building on previous research, our study explores the use of diverse open LLMs to investigate whether smaller models can achieve a more favorable balance between costeffectiveness and accuracy compared to larger models.Costeffective methods are particularly critical for watchdog groups, non-profits, and smaller organizations that often operate with limited budgets and resources.These entities play a crucial role in combating misinformation but frequently lack the financial capacity to deploy and maintain large-scale AI systems.By identifying efficient yet accurate alternatives, our research aims to empower such organizations with accessible tools, enabling them to enhance their fact-checking capabilities without incurring prohibitive costs.Additionally, we employed as a baseline for comparison a Small Language Model (SLM) by fine-tuning RoBERTa.This reference allows us to understand the relative importance of the LLMs' performance.</p>
<p>We articulate our contributions into three key research questions:</p>
<p>1) RQ1: Can LLMs identify the connection between a claim and an article?This research question investigates whether LLMs can accurately determine if a given claim and its paired article are contextually related, addressing the same topic.The focus is on evaluating the models' capability to assess the relevance and alignment between the claim and the content of the article.2) RQ2: Can LLMs judge a claim based on the related fact-checking article?This question explores whether LLMs can effectively analyze a related fact-checking article and provide a trustworthy evaluation of the claim based on the information contained within the document.3) RQ3: Are LLMs able to retrieve contextual information and fact-check claims?This question assesses the models' ability to verify the truthfulness of a claim when they are provided with a related article, as opposed to relying solely on their pre-trained internal knowledge.</p>
<p>The objective is to determine whether the inclusion of new, external information enhances their accuracy in fact-checking tasks.</p>
<p>To address the first two questions (RQ1, RQ2), we conduct experiments using 24 different prompts to guide the models' performance, emphasizing the role of effective prompting.For the third question (RQ3), we use neutral and straightforward prompts to evaluate various sources of external informationsuch as Google or Wikipedia (or their absence)-and the representation format (snippet, summary, or full article) that yields the best results.In this third scenario, the model acts autonomously, conducting internet searches to gather information and refine its verdicts.For Google searches, we limit the results to a time frame before the claim to avoid bias and better simulate real-world fact-checking.</p>
<p>Our analysis utilizes the Fact-Checking Insights dataset 4 , which is a comprehensive resource containing structured data from tens of thousands of claims made by political figures and social media posts, scrutinized and rated by independent fact-checking organizations such as AFP 5 , Politifact 6 , and Snopes 7 .</p>
<p>II. RELATED WORK</p>
<p>The study of fake news generation and dissemination has gained increasing attention, particularly with the rise of online platforms that facilitate rapid information diffusion [18].Research in this domain has primarily focused on two key aspects: the identification of fake news and the detection of its spreaders [15].</p>
<p>Over time, human fact-checking has been increasingly supported by machine learning methods.Early approaches leveraged classical machine learning techniques for keywordbased text analysis using traditional neural networks [19] and applied network analysis to assess the trustworthiness of sources propagating misinformation [20].More recently, the emergence of Large Language Models has significantly advanced fake news detection, enabling more sophisticated analyses.These include evaluating the performance of fake news detectors against synthetically generated misinformation rather than human-created content [21] and conducting sentiment-based assessments through emotional analysis of news texts [22].</p>
<p>Some approaches aim to enhance human fact-checking by integrating feedback from LLMs.For instance, LLMs have been used to improve document retrieval, aiding in the verification of statements [23], or to decompose claims hierarchically into sub-statements for systematic verification [24].Conversely, other methodologies seek to achieve fully autonomous fact-checking, relying on LLMs to assess the veracity of claims using a zero-shot approach, without requiring additional training or human intervention.</p>
<p>A major challenge in automating fact-checking with LLMs lies in their tendency to produce biased answers [25], [26] and to hallucinate facts [27] and their reliance on static training data, which may often be outdated [16].To address this limitation, researchers have explored methods to integrate external knowledge into LLMs using frameworks such as ReAct, which combines reasoning and action within LLMs [28].This has led to several innovative fact-checking approaches.For example, FActScore assigns factuality scores to responses based on information retrieved from Wikipedia [29], while FACTOOL employs various tools for evidence collection and reasoning to analyze claims and assign factuality labels based on supporting evidence [30].Additionally, Toolformer enables LLMs to autonomously integrate external tools, enhancing performance across various tasks while preserving core language modeling capabilities [31].Despite these advancements, research indicates that while LLMs generally perform well, they struggle more with verifying factual statements than identifying false ones [17].</p>
<p>To further improve accuracy, researchers have explored collaborative approaches where multiple models interact and reason together to reach a verdict.FactCheck-GPT, for instance, addresses factual inaccuracies by enabling multiple LLMs to debate and converge on a consensus through iterative discussions, supplemented by external searches such as Google [32]- [34].</p>
<p>Recently, the focus has expanded beyond mere verification to include misinformation correction.Systems like MUSE represent a significant advancement in this area, demonstrating the evolving capability of LLMs to both detect and correct misinformation in real-time environments [35].Similarly, Verify-and-Edit enhances LLM reasoning by incorporating Chain-of-Thought (CoT) reasoning and external knowledge sources such as DrQA, Wikipedia, and Google Search to refine responses and correct factual inaccuracies [36].The Chain-of-Verification technique further improves factual accuracy by leveraging parametric knowledge to revise LLM-generated responses [37].Another approach, SELF-CHECKER, evaluates factuality by integrating real-time web searches (e.g., Bing) to assign factuality labels, reinforcing the role of external knowledge in automated fact-checking [38].</p>
<p>Contrary to prior research suggesting LLM superiority, some studies indicate that task-specific Small Language Models (SLMs), such as fine-tuned BERT, may outperform LLMs in certain fact-checking tasks.One study proposes a hybrid approach where LLMs generate rationales while SLMs handle classification, leveraging the strengths of both model types [39].This finding aligns with our analysis, which shows that LLMs fail to achieve significant performance improvements in zero-shot fact-checking on isolated statements, even when provided with temporally contextualized information.Similarly, [40] demonstrates that LLMs can be more effective as supplementary tools to traditional detection methods.Specifically, the study analyzes feature propagation in networks of entities and concepts extracted from articles using LLM assistance.</p>
<p>Another relevant research direction investigates the stylistic metrics underlying written content.One study explores whether LLMs exhibit human-like planning and creativity in news article generation [41].Additionally, building on the Undeutsch psychological theory that memories of real events differ from those of imagined ones, [42] suggests that fake and real news may have distinct stylistic patterns.This raises the possibility that LLM-based fact-checking methods may rely more on stylistic cues than actual content analysis, particularly in low-context scenarios-an issue reflected in our final task.</p>
<p>Unlike other studies, we restrict the retrieval date settings in Google to obtain realistic estimates of how the LLM would perform on new data.We also use a new dataset called Fact-Checking Insights, which compiles information from various sources to reduce bias.Instead of feeding articles to the LLM, we provide contextual data, such as the author and the date the claim was made.Additionally, we conduct a temporal analysis of claim accuracy to determine whether the training data's cutoff date affects performance.Furthermore, we focus on more economical, open-source models, limiting our analysis to those with at most 70 billion parameters to better understand their strengths and weaknesses.</p>
<p>III. EXPERIMENTAL DESIGN</p>
<p>We focus exclusively on English-language claims, as most models are primarily trained in English and are expected to perform best in this language [43].Our analysis utilizes the Fact-Check Insights dataset, a comprehensive resource available to researchers, journalists, and other stakeholders engaged in countering political misinformation and falsehoods online.This dataset comprises structured data from tens of thousands of claims made by political figures and social media posts, scrutinized and rated by independent fact-checking organizations such as AFP, Politifact, and Snopes.</p>
<p>A. Data Preprocessing</p>
<p>Starting from a collection of over 200K observations downloaded from Fact-Check Insights in June 2024, we removed duplicate entries, rows with incomplete or incorrect attributes, and data that were not in the English language.Next, we scraped the original fact-checking article associated with each claim in the dataset, removing those that could not be collected successfully.After this process, we obtained a final dataset of 60.000 claims along with their corresponding fact-checking articles.</p>
<p>Following the literature [17], we grouped the claims under two macro-categories: True and False.The former includes entries deemed accurate or of higher quality, while the latter contains content lacking a factual basis, such as fake quotes, conspiracy theories, misleading edited media, or ironic and exaggerated criticism.In addition, we labeled all entries labeled as a mixture of true and false content as False.For example, labels like Geppetto mark, trustworthy, mostly true, and correct attribution were categorized as True, while labels such as false, mostly false, Quattro Pinocchio, and unproven or legend were classified as False.The resulting classes are highly unbalanced, with 90% of the claims labeled as False.This is reasonable since fact-checking activities usually focus on false claims [44].</p>
<p>Next, we sampled 50 claims (25 True and 25 False) for each year from 2013 to 2023.We instead sampled 500 claims published in 2024, 30 of which only were True as this was the maximum number available in that year at the time of collection.We include observations from 2024 to better test the capabilities of LLMs at judging claims that are not available in their training data, i.e., because they were published after the training cutoff date of the model.</p>
<p>B. Approach</p>
<p>To evaluate LLMs as effective tools for fact-checking, we designed three distinct tasks aimed at thoroughly assessing their capabilities in this field and addressing the key questions outlined in Section .</p>
<p>1) Understanding the article-statement connection: This task evaluates how accurately an LLM can answer when provided with a pair article-claim that are either related (i.e., the article is fact-checking the claim) or not (i.e., a random fact-checking article is picked).2) Providing an accurate verdict based on a factchecking article: In this task, the model is required to answer what is the verdict on a given claim based on the associated fact-checking article.3) Fact-Checking the claim: In this task, the LLM evaluates the truthfulness of a given statement, with or without additional contextual information, using a neutral prompt (i.e. a prompt that does not steer the model towards a certain decision).</p>
<p>Each task evaluates distinct aspects of the models' performance in fact-checking scenarios.We anticipate that the first two tasks will be relatively easier for the models, as they are required to answer queries based on manually provided contextual information.These tasks test the models' capabilities using a range of prompts, as described below.The third task instead simulates a real-world scenario where the model must retrieve relevant information to verify a given claim.To achieve this, we incorporate the ReAct framework [28], enabling models to access the internet for gathering information useful in fact-checking.</p>
<p>In all cases, we provide the models with two basic metadata information related to the claim: the publication date and, when available (∼70% of the cases), the claim's author.Including the date allows the model to contextualize the claim, as the validity of a statement may change over time with the emergence of new information.Knowing the claim's author can offer insights into its credibility, as claims from unreliable sources are more likely to be false [45].Also, fact-checking agencies such as AFP and PolitiFact highlight the importance of both the claimant's identity and the claim's timing in assessing its veracity.</p>
<p>C. Prompt Engineering</p>
<p>In this section, we provide a comprehensive overview of the 24 prompts evaluated in our experiments, which we categorize into three approaches: Zero-Shot, Few-Shot, and Chain-of-Thought.Zero-Shot prompts require the model to generate responses without any prior examples, relying solely on its pre-trained knowledge [46].Few-Shot [47] prompts provide a small number of examples to guide the model toward the desired output style and reasoning process.Chain-of-Thought [48] prompts explicitly break down the reasoning process into intermediate steps, encouraging the model to generate more structured and explainable responses.We construct these prompts by combining different modules, as detailed next.</p>
<p>1) Basic prompt modules: Here we detail a few basic components that are included in all prompts, in the order they appear as shown in Figure 1 and provided in the Appendix.</p>
<p>The Role prompt module primes the LLM to act as a factchecker, aligning its reasoning with fact-checking principles.This framing encourages a more critical approach to evaluating claims and, as noted by [49], can improve performance.</p>
<p>The Task prompt module requires the LLM to generate an explanation prior to providing an answer.This methodology is supported by literature indicating that additional tokens enhance the reasoning process of models [50].After generating the explanation, the model assigns a score (0-100) rather than a definitive verdict.This scoring mechanism allows for flexible adjustment of the model's skepticism, as the acceptance threshold for responses can be modified.We also decided to implement a scoring mechanism instead of relying on labels because, during the setup of our experiments, we empirically found that instructing the LLM to return a factual label often led to inconsistent outputs.This approach is particularly relevant in our context, where understanding and verifying the reasoning behind a claim is crucial for ensuring the accuracy and trustworthiness of the verification process.</p>
<p>We employ JSON prompt module that provides a structured format for extracting expected responses, facilitating consistent and reliable evaluation across the various tasks.Adopting a similar approach to other work [50], [51], we developed a prompt that enables the LLMs to produce structured JSON outputs.Although these methods may occasionally result in null outputs (approximately 1% of the results) they are highly advantageous, as they guarantee the presence of a result at the specified position.</p>
<p>The Final prompt module serves as a brief reminder to the model regarding how it should structure its responses, including the required format and the necessity of always providing a score.This prompt is significant given that the articles being analyzed can be lengthy.Smaller models, in particular, struggle to adhere to the specified format when processing long texts.By including this reminder at the end, we significantly improve the model's precision in generating responses.Hence, this structured approach ensures that the model remains focused and compliant with the expected output format.</p>
<p>2) Prompt modules for Tasks 1 and 2: We consider three different approaches for the first two tasks:</p>
<p>• Zero Shot (ZS): This prompt module requires the model to generate responses based solely on its preexisting knowledge and understanding.It does not require examples and it relies on indirect information for prediction.However, it heavily depends on the quality of semantic information and may struggle with highly diverse categories [52].</p>
<p>• Few Shot (FS): Unlike the ZS approach, the FS prompt module provides the model with several examples before responding.In this process, we provide one example per class to avoid the limitations of one-shot prompting, which relies on only one example.However, there is a risk of overfitting the small dataset, and the model's performance can be susceptible to the quality and diversity of the examples provided.To minimize these problems, the instances are randomly selected in each iteration.Also, to avoid confusion, especially for smaller models, we encapsulated each sample within square brackets to emphasize that these represent distinct entities, leading to more precise and clearly defined responses from the model.The example articles were limited to remain inside the context windows.This method assesses the model's ability to generalize from a limited set of examples [52].</p>
<p>• Chain of Thoughts (CoT): Recent research has shown that adding a chain-of-thought paradigm can significantly boost the performance of language models across various tasks [53], [54].In this approach, we include the phrase "Let's think step-by-step" after the examples, recognizing that language models can act as zero-shot reasoners [55].This technique aims to enhance reasoning skills and achieve better results.</p>
<p>Furthermore, we added the following approach to obtain more sophisticated prompts:</p>
<p>• Enriched criteria: This approach enriches the prompt by providing a more explicit rationale behind labels.For the False category, we specify that it includes misinformation types such as fake quotes and conspiracy theories.Conversely, the True category encompasses content that demonstrates factual accuracy or contains higherquality information.As suggested by previous studies, this enhancement is expected to lead to higher accuracy in ambiguous cases.In our context, it should assist the model in accurately labeling mixed verdicts, such as mixture [35].</p>
<p>• Self-Reflection: Once we receive the model's answer, we feed the LLM with the previous prompt and its generated response and ask it to reflect on its answer.This allows the model to reason and think about its answer before providing a new response.By encouraging this reflective process, we aim to reduce instances of hallucination, where the model might generate inaccurate or fabricated information.This approach fosters critical thinking and should enhance the reliability of the model's outputs [56].</p>
<p>• Summary: We also investigated whether providing a summary focused on the claim rather than an entire article could enhance the effectiveness of our approach.This strategy distills relevant information, optimizing the context window's use.This prompt depends hardly on the large language model's ability to generate a coherent and accurate summary.</p>
<p>The last two methods are more costly because they involve calling the LLM twice and require processing longer texts compared to the first method.In contrast, the first method involves only a minimal increase in tokens, making it far more cost-efficient.</p>
<p>3) ReAct for Task 3: Our fact-checking methodology incorporates the ReAct framework [28] to enhance precision and efficiency in information retrieval.This method can be broken into two pieces: Reasoning (Re) and Act (Action).First, the model uses its internal knowledge to reflect and think about the user's input, and then it identifies the steps required to solve the problem.After Reasoning, the model starts to Act following the steps identified before; in our case, it acts using an external tool to retrieve information from a source.This framework allows the model to perform this series of Reasoning an Action multiple times, but we limited our experiment to only one iteration.The model could also conclude that it does not need to search online to produce an optimal result and, after just one reasoning step, return the answer; otherwise, it would perform one search before answering.In this framework, the LLM operates using a structured conversational ReAct JSON prompt, ensuring that each response adheres to a predefined format.The JSON responses can take one of two forms: "Final answer," which includes the final score and explanation, or "Action," which enables the model to perform an online search.When performing a search, the agent autonomously generates a query and calls the search tool, providing the query as metadata.After the tool retrieves relevant information, the LLM is called again to produce a final answer, incorporating the newly obtained knowledge.</p>
<p>In particular, we utilize two information sources: Wikipedia and Google.For Wikipedia, we leverage its API by developing a tool that, given a query generated by the LLM, returns the top 3 matches.Each match includes the title, link, and a snippet.These results are used to retrieve the snippets, while for complete content retrieval, we scrape the corresponding URLs, limiting the data to a certain length to ensure it fits within the context window.Finally, to generate summaries, we feed the scraped pages back into the model and request a summary for each page, resulting in three separate model calls for summarization.</p>
<p>Meanwhile, for Google, we simulate an actual user experience using the Selenium automation tool.We apply a stricter criterion, filtering results based on a time range of up to one week before the claim's assertion date.Using Selenium, we systematically scan the first 20 search results and extract the top three relevant links.As with Wikipedia, we use the results to gather snippets.For full content, we scrape the URLs, and in the case of summaries, we prompt the model to generate summaries from the scraped pages.</p>
<p>Following this process, we conduct experiments across three different settings:</p>
<p>1) Snippets: In this setting, we provide only short snippets of information.This approach allows for a quick, surface-level analysis and is highly cost-effective.2) Full Article: Here, we present the complete articles retrieved from the search results.This setting enables a more in-depth content analysis but may risk overloading the model's context window.3) Summary: In this approach, we supply LLM-generated summaries based on the articles collected in the previous setting.These summaries should distill the essential information, facilitating efficient analysis and comparison while avoiding the context window limitation.However, this method is more expensive as it requires invoking the LLM twice.As we can see in this task, the number of operations, the complexity, the strict formatting requirements, and the use of external tools make it more challenging for the model to follow the format consistently.As a result, we observed that approximately 2% of the outputs were invalid.</p>
<p>All the prompts used in this study are detailed in the Appendix.</p>
<p>D. Experimental settings</p>
<p>In our study, we employed four models: two small and two large ones.Specifically, we utilized the Mistral family models, including Mistral-7B-Instruct-v0.3 and Mixtral-8x7B-Instruct-v0.1, alongside LLaMA models, which included Meta-Llama-3-8B-Instruct and Meta-Llama-3-70B-Instruct.</p>
<p>For interaction with these models, we resort to the Hugging Face API 8 .We established fixed parameters to ensure reproducibility across all tasks and tested models.We set the temperature to a low value of 0.1 to enhance the consistency of our experiments, while the number of new tokens was fixed at 256.</p>
<p>To perform the experiments for Task 1 and Task 2, we structured a framework with placeholders, as presented in subsection III-C.We experimented with all possible combinations of different prompt techniques for each dataset entry.</p>
<p>For the Summary enhancement, we provided a summary instead of feeding the full article to the model.As mentioned before, we asked the model to generate a separate response, following the steps outlined earlier.These procedures were repeated for each model across the first two tasks.</p>
<p>1) Task 1: Understanding the article-statement connection: Our experiments involved feeding our dataset into the models.Each entry was resource-intensive due to its 24 settings.To ensure a balanced dataset, we divided the entries into two groups: one with the correct claim and the other with a random, unrelated claim.This allowed us to create a dataset with an equal number of explained and unexplained entries. 8https://huggingface.co/blog/inference-pro#supported-models 2) Task 2: Providing an accurate verdict based on the article's knowledge: Similarly to the previous task, this experiment involved a dataset of 1,000 samples.We provided the LLM with the article verifying the claim and asked for their verdict, which was explicitly contained within the article.Our primary objectives were to verify the accuracy of our label mapping and to assess whether the LLMs could effectively extract critical information from the text.This task was instrumental in refining our label mapping.</p>
<p>3) Task 3: Fact-Checking the claim: In this task, we utilized the complete dataset.Unlike the previous tasks, we provided the model with claims along with their corresponding metadata and allowed it to explore freely in 3 settings: Wikipedia, Google, or no context.</p>
<p>As described in previous sections, we employed a ReAct agent that required the LLM to perform queries and interact with external resources, specifically the Wikipedia API and the Selenium tool.</p>
<p>We developed an agent using the Langchain9 library to implement this functionality, creating two specific tools: one for Wikipedia and another for Google.</p>
<p>By cleverly manipulating the URL, we filtered out results published.This approach aimed to avoid articles that clearly fact-check the claim.</p>
<p>We deployed our models as autonomous agents by integrating the capabilities of LangChain with its HuggingFace integration.To ensure objectivity in the responses, we employed a zero-shot prompt while assigning the model the role of a factchecker.We evaluated three scenarios by providing the model with different input formats for each source of information: the snippet, the full article, or a summary generated by the LLM.</p>
<p>E. Extraction of results</p>
<p>While extracting results from the model, we encountered situations where the model returned multiple answers for a single query.In such cases, we selected the last complete answer, which was often the most comprehensive.To ensure the extracted data complied with the JSON format, we addressed formatting issues, specifically converting single quotes (') to double quotes (").This adjustment was made using a regular expression.</p>
<p>Once the formatting was corrected, we utilized the JSON structure to extract relevant scores and explanations.This data was then compiled into a resulting dataset and subsequently fed into another program for score computation.</p>
<p>We recorded None entries in the dataset when the extraction failed or yielded no valid response.</p>
<p>We decided to fix the threshold for a positive label with a score of 50.Labels with a value of 50 or lower were categorized as False, while those greater than 50 were categorized as True.Any values outside the range of 0 to 100 were classified as None.</p>
<p>F. Baseline</p>
<p>To compare our models, we fine-tuned a RoBERTa model to perform binary classification on article-claim pairs (Task 1 and Task 2) or claim-only embeddings (Task 3) using our dataset.This dataset consists of a large number of claims, each labeled as True or False, and originally paired with fact-checking articles.However, given that RoBERTa can only process sequences up to 512 tokens, we decided to reduce the articles' length, which can exceed 20,000 characters.</p>
<p>For preprocessing, we tokenized the claims using the RoBERTa tokenizer with a maximum sequence length of 512 tokens.We trained the model on this dataset using a crossentropy loss function and an AdamW optimizer, fine-tuning for three epochs.The dataset was split into training and validation sets, and model performance was evaluated using accuracy and classification metrics.</p>
<p>Our results indicate that fine-tuning RoBERTa shows great performance in distinguishing between true and false claims.</p>
<p>These results are included as baselines in the main graphs.</p>
<p>G. Evaluation metrics</p>
<p>To evaluate our model's performance comprehensively, we employ five distinct metrics: Precision, Recall, F1 score, and ROC AUC Score.These metrics collectively offer a detailed understanding of the model's efficacy.In addition, we compute recall, precision, and f1 score separately for each class, thereby distinguishing the model's handling of true positives/negatives from false positives/negatives.This differentiation is crucial for identifying any potential difficulties the model may encounter when processing different cases.</p>
<p>In fact-checking, the disaggregation of evaluation metrics cases serves a particularly critical function.This distinction is important because the implications of misclassification can vary considerably depending on whether a true statement is incorrectly labeled as false or a false statement is incorrectly labeled as true.It also helps identify potential biases within the model, such as a predisposition towards skepticism or credulity.Furthermore, this strategy aligns closely with the practical priorities of fact-checking, where the relative importance of avoiding false positives versus false negatives can vary depending on the context.</p>
<p>IV. RESULTS</p>
<p>We present results for Llama3 8B, Llama3 70B, and Mixtral 8x7B, while the results for Mistral 7B have been relegated to the Appendix due to the exceptionally high number of faulty responses generated by this model.Interestingly, this issue was further exacerbated when a token limit was applied to the answer prompt, while, this same modification reduced the occurrence of faulty responses in the other models.</p>
<p>Task 1: Understanding the article-statement connection</p>
<p>In Fig. 2, we show that all models outperform the fine-tuned RoBERTa, which exhibits particularly low F1 scores, especially for the negative class.The only exceptions are certain prompts for both Mixtral 8x7B and Llama3 8B.Among the three models, Llama3 70B consistently delivers the strongest performance, reaching an F1 score of approximately 0.9, depending on the prompt configuration.In contrast, Llama3 8B and Mixtral 8x7B produce lower scores, with Mixtral 8x7B emerging as the weakest model, averaging 0.65 (Fig. 3) and occasionally dropping below the random threshold of 0.5.</p>
<p>As shown in Fig. 3, and consistent with findings in the literature [57], no single prompt format stands out as the best across all models.For Llama3 8B, providing the full article without relying on more complex prompting strategies yields the highest F1 scores across all prompt configurations.Meanwhile, Llama3 70B and Mixtral 8x7B achieve their best results when enriched prompts-those with more detailed class definitions-are incorporated.</p>
<p>Figure 4 highlights an approximately linear relationship between the F1 scores for the positive and negative classes, suggesting that the models maintain a balanced performance on this task.However, there is a slight tendency for less effective models, such as Llama3 8B and Mixtral 8x7B, to perform better on the negative class, possibly due to their smaller number of parameters.</p>
<p>Finally, Llama3 70B not only outperforms the other models in terms of F1 score but also generates the fewest faulty responses (i.e.instances where the model fails to provide a score or adhere to the expected output format), as illustrated in Fig. 5.In particular, it exhibits the lowest median percentage of faults: 0.003 compared to Llama3 8B's 0.241 and Mixtral 8x7B's 0.393.The average F1 score for each model is also reported with 0.95 confidence intervals.</p>
<p>)SRVLWLYHFODVV</p>
<p>)QHJDWLYHFODVV /ODPD% /ODPD% 0L[WUDO[% Fig. 4. Task 1: Comparison between the F1 scores obtained by each model for each prompt variation with respect to both classes.The bisector is reported as a reference.</p>
<p>Task 2: Providing an accurate verdict based on a fact-checking article</p>
<p>In this task, the performance gap between the positive and negative classes is more pronounced, affecting not only the three evaluated models but also the fine-tuned RoBERTa.</p>
<p>This discrepancy is primarily due to the class imbalance Focusing on the results for the negative class, all three models perform relatively well compared to the baseline set by the fine-tuned RoBERTa, although none fully match its performance.Among them, Llama3 70B comes closest, achieving an F1 score of approximately 0.9, compared to RoBERTa's 0.95.</p>
<p>The performance gap widens even further for the positive class, which contains fewer samples.This is evident both in comparison to RoBERTa's baseline and relative to the other models.Llama3 70B remains the strongest performer, consistently achieving scores above 0.6 across all configurations and coming closest to RoBERTa's threshold.</p>
<p>As observed in Task 1, no single prompting strategy proves to be the most effective across all models.Additionally, the rate of faulty responses increases for both Llama3 8B and Llama3 70B, with the latter reaching nearly 30% for some prompts.Meanwhile, Mixtral 8x7B maintains a consistently high fault rate, similar to its performance in Task 1.  Task 3: Fact-Checking the claim Figure 9 shows that, under the current experimental settings, LLMs achieve high F1 scores for the negative class.However, they are consistently outperformed by the RoBERTa baseline.This suggests that while advanced models can sometimes perform well in classifying claims as True or False, simpler approaches may consistently yield better results.</p>
<p>/ODPD%</p>
<p>A notable trend in the results is the widening gap between the F1 scores of the positive and negative classes.Specifically, the highest F1 score achieved for the positive class is lower than the lowest F1 score observed for the negative class.This discrepancy highlights a significant challenge in the classification task.As was also the case in Task 2, this phenomenon can be attributed to dataset imbalance, which skews the model's ability to correctly predict both classes with equal proficiency.</p>
<p>Further analysis reveals an intriguing temporal pattern when breaking down the performance by claim publication date.As depicted in Figure 10, models exhibit substantially better performance on the positive class for claims that were published before 2024.Conversely, Figure 11 shows an opposite trend for the negative class: claims originating from 2024 tend to yield the highest performance.This suggests that temporal factors, possibly related to shifts in linguistic patterns, dataset composition, or the evolving nature of factual claims, may be influencing the model's ability to distinguish between true and false claims.</p>
<p>Unlike the previous tasks, one striking difference is the relatively low rate of faulty responses across all three models, as illustrated in Figure 12.This suggests that, at least within the scope of this particular task, the models are more stable and less prone to generating erroneous classifications compared to their performance in earlier experiments.</p>
<p>Another interesting observation emerges when analyzing the impact of external content sources on model performance.Specifically, as shown in Figure 13, when models are supplemented with information extracted from Google and Wikipedia, their classification accuracy improves.However, a crucial factor in this improvement appears to be the format in which the external information is presented.The results indicate that models perform significantly better when provided with a structured summary of the search results, rather than being fed entire web pages or isolated snippets.This suggests that concise, well-organized contextual information is more beneficial for improving model accuracy than raw, unstructured text.Google contextual information, and with Wikipedia contextual information) the best prompt's F1 score is reported.The average F1 score for each model is also reported with 0.95 confidence intervals.</p>
<p>V. CONCLUSION</p>
<p>Our study builds upon and expands existing research on the role of LLMs in fact-checking, offering new insights into their capabilities and limitations.</p>
<p>To systematically evaluate LLMs in this context, we de-signed our study around three key tasks.The first task assessed the models' ability to recognize the semantic relationship between a claim and an article.The second task measured their performance in verifying a claim's truthfulness when provided with a related fact-checking article.Finally, the third task explored the zero-shot fact-checking abilities of LLMs under varying levels of external knowledge support.Our findings from the first task indicate that LLMs effectively identify the semantic connection between claims and articles, outperforming fine-tuned Small Language Models in this regard.This suggests a strong potential for LLMs in assisting fact-checking efforts.Additionally, we observed that prompt design plays a crucial role in model performance, highlighting the need for careful prompt engineering in factchecking applications.</p>
<p>In the second task, we found that larger LLMs can accurately determine the veracity of claims when provided with a fact-checking article, particularly when evaluating fake news.However, in line with previous work [17], their performance declines significantly when verifying true news, where simpler fine-tuned Small Language Models substantially outperform them.</p>
<p>Similarly, in the third task, all three LLMs underperformed compared to a fine-tuned RoBERTa model when asked to assess claim veracity-regardless of whether additional external knowledge (sourced from Google or Wikipedia) was incorporated.These results align with prior findings [39], which highlight the superior performance of fine-tuned Small Language Models over LLMs in fake news detection.This reinforces the idea that, while LLMs can be valuable tools in fact-checking, they are not yet reliable enough to fully automate the process.</p>
<p>Contrary to expectations from previous studies, introducing external knowledge did not enhance LLM performance.A possible explanation for this, as suggested by [42], is that fake and factual news often exhibit distinct writing styles, which may significantly hinder LLMs' ability to differentiate between them.</p>
<p>Overall, our study highlights both the promise and the challenges of using LLMs for fact-checking, emphasizing the need for further advancements before they can serve as a standalone solution.</p>
<p>Evaluating open-source Large Language Models for automated fact-checking -Supplementary Materials Nicolò Fontana 1 , Francesco Corso 1,2 , Enrico Zuccolotto 1 , Francesco Pierri 1  1 Politecnico di Milano, Italy 2 CENTAI, Italy {nicolo.fontana,francesco.corso,francesco.pierri}@polimi.itAbstract-The increasing prevalence of online misinformation has heightened the demand for automated fact-checking solutions.Large Language Models (LLMs) have emerged as potential tools for assisting in this task, but their effectiveness remains uncertain.This study evaluates the fact-checking capabilities of various open-source LLMs, focusing on their ability to assess claims with different levels of contextual information.We conduct three key experiments: (1) evaluating whether LLMs can identify the semantic relationship between a claim and a fact-checking article, (2) assessing models' accuracy in verifying claims when given a related fact-checking article, and (3) testing LLMs' fact-checking abilities when leveraging data from external knowledge sources such as Google and Wikipedia.Our results indicate that LLMs perform well in identifying claimarticle connections and verifying fact-checked stories but struggle with confirming factual news, where they are outperformed by traditional fine-tuned models such as RoBERTa.Additionally, the introduction of external knowledge does not significantly enhance LLMs' performance, calling for more tailored approaches.Our findings highlight both the potential and limitations of LLMs in automated fact-checking, emphasizing the need for further refinements before they can reliably replace human fact-checkers.</p>
<p>Index Terms-fact-checking, large language models, prompting analysis</p>
<p>SUPPLEMENTARY MATERIALS</p>
<p>Dataset</p>
<p>Our analysis utilizes the Fact-Check Insights dataset, a comprehensive resource invaluable to researchers, journalists, technologists, and other stakeholders engaged in countering political misinformation and falsehoods online.This dataset comprises structured data from tens of thousands of claims made by political figures and social media posts, meticulously scrutinized and rated by independent fact-checker organizations such as AFP, Politifact, and Snopes.</p>
<p>A. Data Selection</p>
<p>Given the multipurpose and extensive nature of the dataset, we undertook a preprocessing phase to align it with our specific research requirements.We selected only a subset of pertinent columns, as seen in Table I.</p>
<p>We decided to retain the fact-checking article"s publication date to maintain a temporal reference point.This approach enables temporal analysis and enhances our ability to assess the importance of external context in claims made after the training data cut-off date.</p>
<p>B. Data Cleaning</p>
<p>We applied a rigorous preprocessing methodology to ensure data accuracy and reliability.The first step involved ensuring the reliability of the language through a cross-validation method.We utilized the langdetect library to identify the language of the text in claimReviewed.This detection was then cross-referenced with the language extracted from the respective website domains in the url column to ensure consistency in language identification.Subsequently, we refined the dataset through a series of operations to ensure its accuracy and reliability.First, we eliminated entries that lacked critical data, such as the claimReviewed or datePublished fields or cases where the datePublished was set in the future.The absence of a claim makes fact-checking impossible, and without a valid date, the entry becomes unreliable.Additionally, entries that did not include an alternateName, or verdict, were filtered out, as a verdict is essential for our analysis.</p>
<p>Next, we excluded entries with unknown language.If language detection was unsuccessful, the entry was removed to maintain data consistency and reliability across the dataset.</p>
<p>Finally, we addressed the issue of duplicate entries.To preserve the integrity of the data, we carefully identified and removed duplicates.This step was crucial in ensuring our analysis was not skewed by repetitive information.</p>
<p>This careful selection left us with a dataset of around 200,000 claims in 40 languages spanning under 30 years, from 1996 to 2024.As shown in Figure 3, there has been a higher concentration of claims in recent years, with a significant increase starting in 2019.The surge in 2020 includes many claims related to COVID-19 and a substantial number focused on political figures.As evidenced by the graphs 4, English is the predominant language of fact-checkers, followed by Arabic, Spanish, Portuguese, and Italian, reflecting the top languages spoken in the Western world.</p>
<p>After selecting the English language, we extracted the text through a scraping procedure, utilizing newspaper3k library on the url column.This process created an additional field containing the text verifying the claimReviewed.These articles have varying lengths from 1000 to 20000 words (Figure 2).Subsequently, we removed entries with unsuccessful scrapes.This leaves us with 60,000 claims, each linked to its corresponding golden document.</p>
<p>C. Labeling</p>
<p>Following the methodology outlined by Quelle [1], we divided our dataset into two categories: true and false.The true category includes entries deemed accurate or of higher quality, while the false category encompasses content lacking a factual basis, such as fake quotes, conspiracy theories, misleading edited media, or ironic and exaggerated criticism.</p>
<p>To classify our entries, we manually mapped the first 500 common verdicts into one of these two categories (Here is a sample Figure 5).Although this process was timeconsuming and meticulous, it resulted in a robust and rigorous  classification of our data.For example, labels like Geppetto mark, trustworthy, mostly true, and correct attribution were categorized as true, while labels such as false, mostly false, Quattro Pinocchio, and unproven or legend were classified as false.Additionally, terms like mixture were categorized as false, given that a statement containing both true and false elements is considered overall inaccurate.</p>
<p>D. Balancing</p>
<p>Due to a substantial class imbalance, 90% of claims were categorized as false.Since fact-checkers typically verify false claims, steps were taken to address this disparity by equalizing the number of true and false samples.This adjustment resulted in a much smaller but more balanced dataset.</p>
<p>As depicted in Figure 3 our dataset spans various years, with a notable concentration of claims from 2013 to 2024.This temporal distribution enhances the statistical reliability of the dataset across different periods.Given the limited API usage and the large number of experiments to be conducted, we decided to select a subset of the balanced dataset.Specifically, we chose 50 entries for each year from 2013 onward, consisting of 25 true claims and 25 false ones, as shown in Figure 1.Stratified sampling based on verdicts was employed to ensure optimal representation of both categories.</p>
<p>Instead, for the year 2024, we included 500 samples.In this case, the dataset could not be perfectly balanced due to
Field Description claimReviewed
The statement or claim under examination.datePublished</p>
<p>The date of the article"s publication.url</p>
<p>The source URL from which the data was gathered.reviewRating.alternateNameThe verdict assigned to the claim.</p>
<p>author.name</p>
<p>The name of the fact-checking organization conducting the analysis.language</p>
<p>The language in which the claim was evaluated.</p>
<p>reviewRating.author.name</p>
<p>The author of the claim being reviewed.the limited number of true claims, with only 30 available.This is particularly significant because most of the LLMs used in this research were trained before 2024, except for Mistral 7B, which was fine-tuned in June 2024.This portion of the dataset estimates how well these models can perform as factcheckers in real-time.This process culminated in creating a comprehensive English dataset used in all the experiments for each task.</p>
<p>E. Mistral 7B Results</p>
<p>In our analysis, we initially included Mistral-7B-Instruct-v0.3 as a representative of smaller models within the Mistral family.However, we observed a high percentage of faulty responses in the first two preparatory tasks, leading us to exclude its predictions from the main analysis.For completeness, we provide the plots including the results obtained by Mistral 7B for all three tasks.</p>
<p>Fig. 1 .
1
Fig. 1.Example of prompt structure with optional configurations.The black text represents the main prompt, shared across all tasks and configurations.Red placeholders indicate where the actual article and statement are inserted for each task.Blue components correspond to optional prompt modules, which are integrated into the main prompt (at the indicated position) based on the specific combination being tested.For instance, a prompt incorporating both Enrich and Chain-of-Thought will consist of the black main prompt, with the blue Enrich component placed between the Role and Task sub-prompts, and the blue Chain-of-Thought component appended at the end.</p>
<p>•F1 Score = 2 ×
2
Recall (Sensitivity/True Positive Rate): Measures the proportion of actual positives that were correctly identified.Recall = TP TP + FN • Precision (Positive Predictive Value): Measures the proportion of predicted positives that are truly positive.Precision = TP TP + FP • Accuracy: The overall proportion of correct predictions, both true positives and true negatives.Accuracy = TP + TN Total Population • F1 Score: The harmonic mean of precision and recall.It balances the two, which is especially useful when there is an uneven class distribution like in our case.Precision × Recall Precision + Recall • ROC AUC Score (Area Under the Curve): A numerical value representing the area under the ROC curve.It summarizes the model's ability to distinguish between classes.A score of 1.0 indicates perfect classification, while 0.5 indicates random guessing.</p>
<p>Fig. 2 .Fig. 3 .
23
Fig. 2. Task 1: Models' F1 scores computed for both classes.Fine-tuned RoBERTa is used as a reference baseline.</p>
<p>Fig. 5 .
5
Fig. 5. Task 1: Percentage of faults for each model.The median faults percentage for each model is: 0.241 (Llama3 8B), 0.003 (Llama3 70B), 0.393 (Mixtral 8x7B)</p>
<p>Fig. 6 .Fig. 7 .Fig. 8 .
678
Fig.6.Task 2: Models' F1 scores computed for both classes.Fine-tuned RoBERTa is used as a reference baseline.</p>
<p>Fig. 10 .Fig. 11 .Fig. 12 .Fig. 13 .
10111213
Fig. 10.Task 3: Models' F1 scores computed for the positive class distinguishing between claims dated before 2024 (thus possibly included in the models' training dataset) and from 2024.Fine-tuned RoBERTa is used as a reference baseline.</p>
<p>Fig. 1 .
1
Fig. 1.For each year, the number of True and False claims in the used dataset.</p>
<p>Fig. 2 .
2
Fig. 2. Distribution of articles length.</p>
<p>Fig. 3 .
3
Fig. 3. Distribution of fact-checked claims over the years.</p>
<p>Fig. 4 .
4
Fig. 4. Number of fact-checked claims by the claim language.</p>
<p>Fig. 5 .
5
Fig. 5. Verdict distribution.</p>
<p>Fig. 6 .Fig. 7 .Fig. 8 .Fig. 9 .Fig. 10 .Fig. 11 .Fig. 12 .
6789101112
Fig.6.Task 1: Models' F1 scores computed for both classes.Fine-tuned RoBERTa is used as a reference baseline.</p>
<p>Fig. 13 .Fig. 14 .
1314
Fig.13.Task 3: Models' F1 scores computed for both classes.Fine-tuned RoBERTa is used as a reference baseline.</p>
<p>F</p>
<p>Fig. 18 .
18
Fig. 18.Task 3: Precision and Recall curve for the positive class for Llama3 8B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 19 .
19
Fig. 19.Task 3: Precision and Recall curve for the positive class for Llama3 70B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 20 .
20
Fig. 20.Task 3: Precision and Recall curve for the positive class for Mixtral 8x7B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 21 .
21
Fig. 21.Task 3: Precision and Recall curve for the negative class for Llama3 8B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 22 .Fig. 23 .
2223
Fig. 22. Task 3: Precision and Recall curve for the negative class for Llama3 70B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 24 .
24
Fig. 24.Task 3: ROC curve for the positive class for Llama3 8B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 25 .Fig. 26 .
2526
Fig. 25.Task 3: ROC curve for the positive class for Llama3 70B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 27 .
27
Fig. 27.Task 3: ROC curve for the negative class for Llama3 8B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>Fig. 28 .Fig. 29 .
2829
Fig. 28.Task 3: ROC curve for the negative class for Llama3 70B.The best prompt of each category is highlighted.Fine-tuned RoBERTa and a random classifier are used as a baseline for comparison.</p>
<p>TABLE I DESCRIPTION
I
OF FACT-CHECKING DATABASE FIELDS.</p>
<p>https://www.politifact.com/
https://www.pulitzer.org/prize-winners-by-year/2009
https://explodingtopics.com/blog/data-generated-per-day
https://www.factcheckinsights.org/download
https://factcheck.afp.com
https://www.politifact.com/
https://www.snopes.com/
https://www.langchain.com
ACKNOWLEDGMENTSThe authors are thankful to Sofia Mongardi for her support in the design of this manuscript.The work in this paper was originally submitted as a Master Thesis titled: "Evaluating the Effectiveness of Open Large Language Models in Factchecking Claims" written by Enrico Zuccolotto and supervised by Prof. Francesco Pierri.This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program.
The spreading of misinformation online. M Del Vicario, A Bessi, F Zollo, F Petroni, A Scala, G Caldarelli, H E Stanley, W Quattrociocchi, 10.1073/pnas.1517441113Proceedings of the National Academy of Sciences. 1133Jan. 2016</p>
<p>Social media and fake news in the 2016 election. H Allcott, M Gentzkow, 10.1257/jep.31.2.211Journal of Economic Perspectives. 312May 2017</p>
<p>Systematic discrepancies in the delivery of political ads on facebook and instagram. D Bär, F Pierri, G De Francisci, S Morales, Feuerriegel, 10.1093/pnasnexus/pgae247PNAS Nexus. 37e247Jul. 2024a</p>
<p>The biggest challenge facing journalism: A lack of trust. K Fink, 10.1177/1464884918807069Journalism. 2012019</p>
<p>The fact-checking explosion: In a bitter political landscape marked by rampant allegations of questionable credibility, more and more news outlets are launching truth-squad operations. C Spivak, American Journalism Review. 3242010</p>
<p>Checking how fact-checkers check. C Lim, Research &amp; Politics. 532018</p>
<p>Show me the work: Fact-checkers' requirements for explainable automated factchecking. G Warren, I Shklovski, I Augenstein, arXiv:2502.09083Feb. 2025</p>
<p>A survey on automated fact-checking. Z Guo, M Schlichtkrull, A Vlachos, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Deep learning for fake news detection: A comprehensive survey. L Hu, S Wei, Z Zhao, B Wu, AI Open. 32022</p>
<p>Fake news detection via knowledge-driven multimodal graph convolutional networks. Y Wang, S Qian, J Hu, Q Fang, C Xu, 10.1145/3372278.3390713Proceedings of the 2020 International Conference on Multimedia Retrieval, ser. ICMR '20. the 2020 International Conference on Multimedia Retrieval, ser. ICMR '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>A survey of transformers. T Lin, Y Wang, X Liu, X Qiu, AI Open. 32022</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. M U Hadi, R Qureshi, A Shah, M Irfan, A Zafar, M B Shaikh, N Akhtar, J Wu, S Mirjalili, 2023Authorea Preprints</p>
<p>Nicer than humans: How do large language models behave in the prisoner's dilemma?. N Fontana, F Pierri, L M Aiello, arXiv:2406.13605Sep. 2024</p>
<p>A survey on the use of large language models (llms) in fake news. E Papageorgiou, C Chronis, I Varlamis, Y Himeur, Future Internet. 1688298Aug. 2024</p>
<p>Limitations of large language models. E Sanu, T K Amudaa, P Bhat, G Dinesh, A U Kumar Chate, R K , 2024 8th International Conference on Computational System and Information Technology for Sustainable Solutions (CSITSS). Nov. 2024</p>
<p>The perils and promises of fact-checking with large language models. D Quelle, A Bovet, 10.3389/frai.2024.1341697Frontiers in Artificial Intelligence. 7Feb. 2024</p>
<p>False news on social media: A data-driven survey. F Pierri, S Ceri, SIGMOD Rec. 482Dec. 2019</p>
<p>. 10.1145/3377330.3377334</p>
<p>Fake news detection: A hybrid cnn-rnn based deep learning approach. J A Nasir, O S Khan, I Varlamis, International Journal of Information Management Data Insights. 11100007Apr. 2021b</p>
<p>Cipf: Identifying fake profiles on social media using a cnn-based communal influence propagation framework. A Mewada, R K Dewang, 10.1007/s11042-023-16685-zMultimedia Tools and Applications. 8310Mar. 2024a</p>
<p>Adapting fake news detection to the era of large language models. J Su, C Cardie, P Nakov, Findings of the Association for Computational Linguistics: NAACL 2024. K Duh, H Gomez, S Bethard, Mexico City, MexicoAssociation for Computational LinguisticsJun. 2024</p>
<p>Mining dual emotion for fake news detection. X Zhang, J Cao, X Li, Q Sheng, L Zhong, K Shu, 10.1145/3442381.3450004Proceedings of the Web Conference 2021, ser. WWW '21. the Web Conference 2021, ser. WWW '21New York, NY, USAAssociation for Computing MachineryJun. 2021</p>
<p>Reinforcement retrieval leveraging finegrained feedback for fact checking news claims with black-box llm. X Zhang, W Gao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024. N Calzolari, M.-Y Kan, V Hoste, A Lenci, S Sakti, N Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method. Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter. Long Papers. Nusa Dua, the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific ChapterBaliAssociation for Computational LinguisticsNov. 2023a1</p>
<p>Toxic bias: Perspective api misreads german as more toxic. G Nogara, F Pierri, S Cresci, L Luceri, P Törnberg, S Giordano, arXiv:2312.12651Jul. 2024c</p>
<p>Comparing diversity, negativity, and stereotypes in chinese-language ai technologies: an investigation of baidu, ernie and qwen. G Liu, C A Bono, F Pierri, arXiv:2408.15696Feb. 2025b</p>
<p>Factuality challenges in the era of large language models and opportunities for fact-checking. I Augenstein, T Baldwin, M Cha, T Chakraborty, G L Ciampaglia, D Corney, R Diresta, E Ferrara, S Hale, A Halevy, E Hovy, H Ji, F Menczer, R Miguez, P Nakov, D Scheufele, S Sharma, G Zagni, Nature Machine Intelligence. 68Aug. 2024</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). Jan. 2023</p>
<p>Factscore: Finegrained atomic evaluation of factual precision in long form text generation. S Min, K Krishna, X Lyu, M Lewis, W -T. Yih, P Koh, M Iyyer, L Zettlemoyer, H Hajishirzi, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. J Bouamor, K Pino, Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Factool: Factuality detection in generative ai -a tool augmented framework for multi-task and multi-domain scenarios. I.-C Chern, S Chern, S Chen, W Yuan, K Feng, C Zhou, J He, G Neubig, P Liu, arXiv:2307.13528jul 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessi, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Advances in Neural Information Processing Systems. Dec. 202336</p>
<p>Factcheck-bench: Finegrained evaluation benchmark for automatic fact-checkers. Y Wang, R Gangi, Z M Reddy, A Mujahid, A Arora, J Rubashevskii, O Geng, L Mohammed Afzal, N Pan, A Borenstein, I Pillai, I Augenstein, P Gurevych, Nakov, Findings of the Association for Computational Linguistics: EMNLP 2024. Y Al-Onaizan, M Bansal, Y.-N Chen, Miami, Florida, USAAssociation for Computational LinguisticsNov. 2024</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, Proceedings of the 41st International Conference on Machine Learning, ser. ICML'24. the 41st International Conference on Machine Learning, ser. ICML'24Vienna, AustriaJMLR.orgJul. 2024235</p>
<p>Can llms produce faithful explanations for fact-checking? towards faithful explainable fact-checking via multi-agent debate. K Kim, S Lee, K.-H Huang, H P Chan, M Li, H Ji, 2024</p>
<p>Correcting misinformation on social media with a large language model. X Zhou, A Sharma, A X Zhang, T Althoff, 2024</p>
<p>Verify-and-edit: A knowledge-enhanced chain-of-thought framework. R Zhao, X Li, S Joty, C Qin, L Bing, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJul. 20231</p>
<p>Chain-of-verification reduces hallucination in large language models. S Dhuliawala, M Komeili, J Xu, R Raileanu, X Li, A Celikyilmaz, J Weston, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>Self-checker: Plug-and-play modules for fact-checking with large language models. M Li, B Peng, M Galley, J Gao, Z Zhang, Findings of the Association for Computational Linguistics: NAACL 2024. K Duh, H Gomez, S Bethard, Mexico City, MexicoAssociation for Computational LinguisticsJun. 2024</p>
<p>Bad actor, good advisor: Exploring the role of large language models in fake news detection. B Hu, Q Sheng, J Cao, Y Shi, Y Li, D Wang, P Qi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020. Mar. 202438</p>
<p>On fake news detection with llm enhanced semantics mining. X Ma, Y Zhang, K Ding, J Yang, J Wu, H Fan, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing</p>
<p>. Chen , Nov. 2024Association for Computational LinguisticsMiami, Florida, USA</p>
<p>Do llms plan like human writers? comparing journalist coverage of press releases with llms. A Spangher, N Peng, S Gehrmann, M Dredze, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNov. 2024a</p>
<p>Fake news in sheep's clothing: Robust fake news detection against llm-empowered style attacks. J Wu, J Guo, B Hooi, 10.1145/3637528.3671977Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ser. KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ser. KDD '24New York, NY, USAAssociation for Computing MachineryAug. 2024b</p>
<p>ChatGPT beyond English: Towards a comprehensive evaluation of large language models in multilingual learning. V D Lai, N Ngo, A Pouran Ben, H Veyseh, F Man, T Dernoncourt, T H Bui, Nguyen, Findings of the Association for Computational Linguistics: EMNLP 2023. J Bouamor, K Pino, Bali, SingaporeAssociation for Computational LinguisticsDec. 202313</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, 10.1145/3411763.3451760Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, ser. CHI EA '21. New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Reading is believing: The truth effect and source credibility. L A Henkel, M E Mattson, Consciousness and cognition. 2042011</p>
<p>A survey of zero-shot learning: Settings, methods, and applications. W Wang, V W Zheng, H Yu, C Miao, 10.1145/3293318ACM Trans. Intell. Syst. Technol. 102Jan. 2019</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Better zero-shot reasoning with role-play prompting. A Kong, S Zhao, H Chen, Q Li, Y Qin, R Sun, X Zhou, E Wang, X Dong, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJun. 20241</p>
<p>Towards reliable misinformation mitigation: Generalization, uncertainty, and gpt-4. K Pelrine, A Imouza, C Thibault, M Reksoprodjo, C Gupta, J Christoph, J.-F Godbout, R Rabbany, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. J Bouamor, K Pino, Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Structuredrag: Json response formatting with large language models. C Shorten, C Pierse, T B Smith, E Cardenas, A Sharma, J Trengrove, B Van Luijt, 2024</p>
<p>The prompt report: A systematic survey of prompting techniques. S Schulhoff, M Ilie, N Balepur, K Kahadze, A Liu, C Si, Y Li, A Gupta, H Han, S Schulhoff, P S Dulepet, S Vidyadhara, D Ki, S Agrawal, C Pham, G Kroiz, F Li, H Tao, A Srivastava, H D Costa, S Gupta, M L Rogers, I Goncearenco, G Sarli, I Galynker, D Peskoff, M Carpuat, J White, S Anadkat, A Hoyle, P Resnik, 2024</p>
<p>Least-tomost prompting enables complex reasoning in large language models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, E Chi, 2023</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q Le, E Chi, D Zhou, J Wei, Findings of the Association for Computational Linguistics: ACL 2023. J Rogers, N Boyd-Graber, Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJul. 2023</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Towards mitigating LLM hallucination via self reflection. Z Ji, T Yu, Y Xu, N Lee, E Ishii, P Fung, Findings of the Association for Computational Linguistics: EMNLP 2023. J Bouamor, K Pino, Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Conspiracy theories and where to find them on tiktok. F Corso, F Pierri, G D F Morales, arXiv:2407.125452024arXiv preprint</p>
<p>The perils and promises of fact-checking with large language models. D Quelle, A Bovet, 10.3389/frai.2024.1341697Frontiers in Artificial Intelligence. 7Feb. 2024</p>            </div>
        </div>

    </div>
</body>
</html>
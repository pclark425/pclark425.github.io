<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1768 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1768</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1768</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-d5a16234851a507be828abc331f8bfedaf6536e8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d5a16234851a507be828abc331f8bfedaf6536e8" target="_blank">Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work builds a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and trains robust navigation policies using Liquid neural networks, and obtains a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks.</p>
                <p><strong>Paper Abstract:</strong> Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target manoeuvres curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1768.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1768.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Quadrotor-Liquid-GS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous quadrotor navigation policy trained with Gaussian Splatting rendering and Liquid (CfC) neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end visual navigation policy mapping onboard RGB camera images to body-frame velocity and yaw-rate commands; trained via behavior cloning in a simulator that couples PyBullet dynamics with photorealistic Gaussian Splatting rendering and deployed zero-shot on a DJI M300 RTK quadrotor for indoor and outdoor navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>DJI M300 RTK (evaluation); simulated Crazyflie-like quadrotor (training)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Physical evaluation used a DJI Matrice 300 RTK quadrotor with gimbal-stabilized Zenmuse Z30 camera, DJI Manifold 2 companion computer and NVIDIA Jetson TX2 for onboard inference; the simulated agent used a Crazyflie-parameterized quadrotor dynamics model in PyBullet for dataset generation and expert controllers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotics — aerial navigation / autonomous quadrotor navigation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Custom PyBullet dynamics + 3D Gaussian Splatting (GS) renderer (referred to as GS simulator); also plain PyBullet non-photorealistic simulator for ablations</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A coupled simulator: PyBullet provides high-frequency (240 Hz) rigid-body quadrotor dynamics and PID expert-controller trajectories; those poses are mapped into a photorealistic 3D Gaussian Splatting radiance-field renderer trained from SfM/phone images (COLMAP outputs) to produce RGB camera views. Task objects (red/blue spheres) were added via Blender → GS models and blended into the scene.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Photorealistic rendering (high visual fidelity) combined with high-rate rigid-body dynamics (240 Hz) — i.e., high-fidelity visuals + realistic dynamics; but static-scene rendering (no dynamic objects) and approximated platform mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Photorealistic lighting and appearance via Gaussian Splatting; camera viewpoint and gimbal-stabilized view rendering; rigid-body flight dynamics at 240 Hz via PyBullet; object placement, scale and geometry; camera intrinsics/resolution alignment; per-trajectory time evolution (used for temporal/irregular sampling augmentation).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No explicit modeling of moving/dynamic scene elements (static scenes only); limited or no explicit sensor noise modeling (no explicit camera noise model reported); actuator/flight-controller internals treated as black-box (manufacturer flight controller on real platform differs from simulated dynamics); physics/parameter randomization of mass/drag/actuator properties not used; some GS visual artifacts/fogginess present; no wind model in simulation; no per-pixel photometric calibration or detailed camera sensor noise/temporal delay modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Indoor: the same indoor flight room captured for GS model, goals implemented as cardboard disks/tripod-mounted targets, 4–5 m starting distances, tests from five start positions; Hardware: DJI M300 RTK with Zenmuse Z30 gimbal camera, Manifold 2 companion computer and Jetson TX2, runtime ≈2.6 Hz. Outdoor: urban campus lawn with sunlight, shadows, reflections and varied backgrounds for zero-shot generalization and a multi-checkpoint ‘hike’ demonstration (5 checkpoints).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-only approach-and-turn navigation to colored checkpoints (approach to hover position, then yaw turn left/right depending on target color); composed sequential checkpoint navigation (multi-step hiking task).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning — behavior cloning from expert PID controller trajectories generated in simulation (PyBullet + GS); datasets include recovery trajectories and irregular time-sampled augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Primary metrics: per-target success rate (first-target reach-and-correct-turn success, reported per-color and total) and mean number of checkpoints completed (mean hike length) in multi-step tests; failure cases defined by overshoot, wrong-turn, target loss, or excessive stall time.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Multiple reported simulation-stage results: Sim-to-Sim (non-photorealistic PyBullet) Liquid mean hike length = 56.1 checkpoints (Liquid) and 83.4 (Liquid 3Hz) in 100-step hiking test; GS-to-GS (photorealistic) Liquid first-target success: Red 94%, Blue 98%, Total 96%, mean checkpoints 1.72 (N=100).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>GS-to-Real indoor: Liquid (time-scale augmented) first-target success dropped from 96% (GS) to ≈75% on real indoor platform (text reports 'goes from 96 to 75%'); GS-to-Real outdoor (zero-shot): Liquid first-target success: Red 40%, Blue 60%, Total 50% (N=40); LSTM baseline: essentially failed in outdoor tests (0% success).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Limited/randomization: task-relevant object positioning randomized (initial target angles/distances, lateral/vertical edges sampling), RGB image augmentations (brightness, contrast, saturation), and irregular time-sampling augmentation for Liquid models; no scene-level randomization of textures/geometry or physics parameter randomization was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Platform dynamics mismatch (simulated Crazyflie-like dynamics vs. DJI M300 real dynamics and proprietary flight controller), unmodeled environmental factors (wind outdoors), lighting and reflections differences, camera/gimbal and sensor differences, rendering artifacts in GS, timing and inference frequency mismatch (training datasets sampled at various rates; runtime ≈2.6 Hz), lack of dynamic scene modeling, and missing explicit sensor/actuator noise models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Key enabling factors reported: (1) photorealistic visual data from Gaussian Splatting aligned with dynamics (GS + PyBullet alignment and manual scaling), (2) careful dataset design with recovery trajectories exposing off-distribution starts, (3) Liquid (CfC) continuous-time neural networks with irregular time-sampled augmentation and time-delta inputs (improves OOD generalization and frequency robustness), (4) RGB augmentations and object-position randomization (task-relevant randomization), (5) closed-loop inference protocol matching simulated trajectories to rendered views, and (6) lightweight on-board inference feasible on TX2 enabling in-field deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Authors identify that photorealistic rendering is crucial for vision-only sim-to-real: networks trained on non-photorealistic PyBullet data failed to generalize to GS and to real; dynamics-visual alignment (matching scale and gravity-aligned frames) is required; large distribution shifts still require domain randomization — no strict numerical fidelity thresholds were provided.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparison: models trained on low-visual-fidelity PyBullet struggled to transfer to GS (Sim-to-GS: Liquid total success ≈3%); photorealistic GS-trained Liquid models performed well in GS (96% first-target) and showed substantial but acceptable drops in real indoor transfer (≈96% → 75%); zero-shot outdoor transfer succeeded at 50% total for Liquid. LSTM baseline performed well in some GS metrics but collapsed in real-world transfer (large drop to 12.5% or 0% outdoors). Irregular/time-scale augmentation and Liquid architecture improved robustness to frequency and distribution shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High-quality photorealistic visual simulation (Gaussian Splatting) combined with accurate alignment to dynamics and dataset design (recovery trajectories + time-delta augmentation) enables zero-shot sim-to-real transfer of vision-only end-to-end quadrotor navigation; Liquid (CfC) continuous-time networks with irregular sampling are substantially more robust to sim-to-real and frequency shifts than LSTM baselines; models trained on non-photorealistic visuals fail to transfer, indicating photorealism (or equivalent domain bridging) is an important fidelity requirement; nevertheless, remaining sim-to-real gaps (platform dynamics mismatch, lighting, wind, sensor differences) reduce real-world performance and large distribution shifts still motivate domain randomization or adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robust flight navigation out of distribution with liquid neural networks <em>(Rating: 2)</em></li>
                <li>3D Gaussian Splatting for RealTime Radiance Field Rendering <em>(Rating: 2)</em></li>
                <li>Pegasus: Physically enhanced gaussian splatting simulation system for 6dof object pose dataset generation <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Vision-only robot navigation in a neural radiance world <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1768",
    "paper_id": "paper-d5a16234851a507be828abc331f8bfedaf6536e8",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Quadrotor-Liquid-GS",
            "name_full": "Autonomous quadrotor navigation policy trained with Gaussian Splatting rendering and Liquid (CfC) neural networks",
            "brief_description": "An end-to-end visual navigation policy mapping onboard RGB camera images to body-frame velocity and yaw-rate commands; trained via behavior cloning in a simulator that couples PyBullet dynamics with photorealistic Gaussian Splatting rendering and deployed zero-shot on a DJI M300 RTK quadrotor for indoor and outdoor navigation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "DJI M300 RTK (evaluation); simulated Crazyflie-like quadrotor (training)",
            "agent_system_description": "Physical evaluation used a DJI Matrice 300 RTK quadrotor with gimbal-stabilized Zenmuse Z30 camera, DJI Manifold 2 companion computer and NVIDIA Jetson TX2 for onboard inference; the simulated agent used a Crazyflie-parameterized quadrotor dynamics model in PyBullet for dataset generation and expert controllers.",
            "domain": "robotics — aerial navigation / autonomous quadrotor navigation",
            "virtual_environment_name": "Custom PyBullet dynamics + 3D Gaussian Splatting (GS) renderer (referred to as GS simulator); also plain PyBullet non-photorealistic simulator for ablations",
            "virtual_environment_description": "A coupled simulator: PyBullet provides high-frequency (240 Hz) rigid-body quadrotor dynamics and PID expert-controller trajectories; those poses are mapped into a photorealistic 3D Gaussian Splatting radiance-field renderer trained from SfM/phone images (COLMAP outputs) to produce RGB camera views. Task objects (red/blue spheres) were added via Blender → GS models and blended into the scene.",
            "simulation_fidelity_level": "Photorealistic rendering (high visual fidelity) combined with high-rate rigid-body dynamics (240 Hz) — i.e., high-fidelity visuals + realistic dynamics; but static-scene rendering (no dynamic objects) and approximated platform mismatch.",
            "fidelity_aspects_modeled": "Photorealistic lighting and appearance via Gaussian Splatting; camera viewpoint and gimbal-stabilized view rendering; rigid-body flight dynamics at 240 Hz via PyBullet; object placement, scale and geometry; camera intrinsics/resolution alignment; per-trajectory time evolution (used for temporal/irregular sampling augmentation).",
            "fidelity_aspects_simplified": "No explicit modeling of moving/dynamic scene elements (static scenes only); limited or no explicit sensor noise modeling (no explicit camera noise model reported); actuator/flight-controller internals treated as black-box (manufacturer flight controller on real platform differs from simulated dynamics); physics/parameter randomization of mass/drag/actuator properties not used; some GS visual artifacts/fogginess present; no wind model in simulation; no per-pixel photometric calibration or detailed camera sensor noise/temporal delay modeling.",
            "real_environment_description": "Indoor: the same indoor flight room captured for GS model, goals implemented as cardboard disks/tripod-mounted targets, 4–5 m starting distances, tests from five start positions; Hardware: DJI M300 RTK with Zenmuse Z30 gimbal camera, Manifold 2 companion computer and Jetson TX2, runtime ≈2.6 Hz. Outdoor: urban campus lawn with sunlight, shadows, reflections and varied backgrounds for zero-shot generalization and a multi-checkpoint ‘hike’ demonstration (5 checkpoints).",
            "task_or_skill_transferred": "Vision-only approach-and-turn navigation to colored checkpoints (approach to hover position, then yaw turn left/right depending on target color); composed sequential checkpoint navigation (multi-step hiking task).",
            "training_method": "Imitation learning — behavior cloning from expert PID controller trajectories generated in simulation (PyBullet + GS); datasets include recovery trajectories and irregular time-sampled augmentation.",
            "transfer_success_metric": "Primary metrics: per-target success rate (first-target reach-and-correct-turn success, reported per-color and total) and mean number of checkpoints completed (mean hike length) in multi-step tests; failure cases defined by overshoot, wrong-turn, target loss, or excessive stall time.",
            "transfer_performance_sim": "Multiple reported simulation-stage results: Sim-to-Sim (non-photorealistic PyBullet) Liquid mean hike length = 56.1 checkpoints (Liquid) and 83.4 (Liquid 3Hz) in 100-step hiking test; GS-to-GS (photorealistic) Liquid first-target success: Red 94%, Blue 98%, Total 96%, mean checkpoints 1.72 (N=100).",
            "transfer_performance_real": "GS-to-Real indoor: Liquid (time-scale augmented) first-target success dropped from 96% (GS) to ≈75% on real indoor platform (text reports 'goes from 96 to 75%'); GS-to-Real outdoor (zero-shot): Liquid first-target success: Red 40%, Blue 60%, Total 50% (N=40); LSTM baseline: essentially failed in outdoor tests (0% success).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Limited/randomization: task-relevant object positioning randomized (initial target angles/distances, lateral/vertical edges sampling), RGB image augmentations (brightness, contrast, saturation), and irregular time-sampling augmentation for Liquid models; no scene-level randomization of textures/geometry or physics parameter randomization was performed.",
            "sim_to_real_gap_factors": "Platform dynamics mismatch (simulated Crazyflie-like dynamics vs. DJI M300 real dynamics and proprietary flight controller), unmodeled environmental factors (wind outdoors), lighting and reflections differences, camera/gimbal and sensor differences, rendering artifacts in GS, timing and inference frequency mismatch (training datasets sampled at various rates; runtime ≈2.6 Hz), lack of dynamic scene modeling, and missing explicit sensor/actuator noise models.",
            "transfer_enabling_conditions": "Key enabling factors reported: (1) photorealistic visual data from Gaussian Splatting aligned with dynamics (GS + PyBullet alignment and manual scaling), (2) careful dataset design with recovery trajectories exposing off-distribution starts, (3) Liquid (CfC) continuous-time neural networks with irregular time-sampled augmentation and time-delta inputs (improves OOD generalization and frequency robustness), (4) RGB augmentations and object-position randomization (task-relevant randomization), (5) closed-loop inference protocol matching simulated trajectories to rendered views, and (6) lightweight on-board inference feasible on TX2 enabling in-field deployment.",
            "fidelity_requirements_identified": "Authors identify that photorealistic rendering is crucial for vision-only sim-to-real: networks trained on non-photorealistic PyBullet data failed to generalize to GS and to real; dynamics-visual alignment (matching scale and gravity-aligned frames) is required; large distribution shifts still require domain randomization — no strict numerical fidelity thresholds were provided.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparison: models trained on low-visual-fidelity PyBullet struggled to transfer to GS (Sim-to-GS: Liquid total success ≈3%); photorealistic GS-trained Liquid models performed well in GS (96% first-target) and showed substantial but acceptable drops in real indoor transfer (≈96% → 75%); zero-shot outdoor transfer succeeded at 50% total for Liquid. LSTM baseline performed well in some GS metrics but collapsed in real-world transfer (large drop to 12.5% or 0% outdoors). Irregular/time-scale augmentation and Liquid architecture improved robustness to frequency and distribution shifts.",
            "key_findings": "High-quality photorealistic visual simulation (Gaussian Splatting) combined with accurate alignment to dynamics and dataset design (recovery trajectories + time-delta augmentation) enables zero-shot sim-to-real transfer of vision-only end-to-end quadrotor navigation; Liquid (CfC) continuous-time networks with irregular sampling are substantially more robust to sim-to-real and frequency shifts than LSTM baselines; models trained on non-photorealistic visuals fail to transfer, indicating photorealism (or equivalent domain bridging) is an important fidelity requirement; nevertheless, remaining sim-to-real gaps (platform dynamics mismatch, lighting, wind, sensor differences) reduce real-world performance and large distribution shifts still motivate domain randomization or adaptation.",
            "uuid": "e1768.0",
            "source_info": {
                "paper_title": "Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robust flight navigation out of distribution with liquid neural networks",
            "rating": 2
        },
        {
            "paper_title": "3D Gaussian Splatting for RealTime Radiance Field Rendering",
            "rating": 2
        },
        {
            "paper_title": "Pegasus: Physically enhanced gaussian splatting simulation system for 6dof object pose dataset generation",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2
        },
        {
            "paper_title": "Vision-only robot navigation in a neural radiance world",
            "rating": 2
        }
    ],
    "cost": 0.012046249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Gaussian Splatting to Real World Flight Navigation Transfer with Liquid Networks</h1>
<p>Alex Quach*<br>aquach@mit.edu</p>
<h2>Makram Chahine* <br> chahine@mit.edu</h2>
<p>Alexander Amini
amini@mit.edu</p>
<p>Ramin Hasani
rhasani@csail.mit.edu</p>
<p>Daniela Rus
rus@csail.mit.edu</p>
<ul>
<li>equal contribution</li>
</ul>
<p>CSAIL, MIT
Project Website</p>
<p>Abstract: Simulators are powerful tools for autonomous robot learning as they offer scalable data generation, flexible design, and optimization of trajectories. However, transferring behavior learned from simulation data into the real world proves to be difficult, usually mitigated with compute-heavy domain randomization methods or further model fine-tuning. We present a method to improve generalization and robustness to distribution shifts in sim-to-real visual quadrotor navigation tasks. To this end, we first build a simulator by integrating Gaussian Splatting with quadrotor flight dynamics, and then, train robust navigation policies using Liquid neural networks. In this way, we obtain a full-stack imitation learning protocol that combines advances in 3D Gaussian splatting radiance field rendering, crafty programming of expert demonstration training data, and the task understanding capabilities of Liquid networks. Through a series of quantitative flight tests, we demonstrate the robust transfer of navigation skills learned in a single simulation scene directly to the real world. We further show the ability to maintain performance beyond the training environment under drastic distribution and physical environment changes. Our learned Liquid policies, trained on single target manoeuvres curated from a photorealistic simulated indoor flight only, generalize to multi-step hikes onboard a real hardware platform outdoors.</p>
<p>Keywords: End-to-end learning, Gaussian Splatting, Sim-to-real transfer
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We create a photorealistic policy learning environment by combining Gaussian Splatting with quadrotor flight dynamics to generate high-fidelity training datasets. A model, $\pi$, with parameters $\theta$, is trained using input images $x_{t}$ to match its predicted output $\mu_{t}$ with ground truth labels $y_{t}=\left[v_{t}^{x}, v_{t}^{y}, v_{t}^{z}, \hat{\psi}<em t="t">{t}\right]$, where $v</em>$ is the yaw rate. Liquid neural networks learn the task through behavior cloning and achieve sim-to-real transfer.}$ denotes desired triaxial velocities and $\hat{\psi}_{t</p>
<h1>1 Introduction</h1>
<p>Recent breakthroughs in simulation now allow for the creation of more lifelike and varied datasets. Techniques like neural scene reconstruction [1, 2, 3] enable detailed, photorealistic 3D models that mirror real scenes, providing advanced training grounds for robotics. Generative AI also expands training data possibilities, with entirely new, realistic environments and scenarios created from text descriptions or similar inputs [4].</p>
<p>Building on these advances, we design a methodology to improve the generalization and robustness of autonomous quadrotor navigation from simulation to real-world environments. Our approach includes three schemes: 1) integrating a 3D Gaussian Splatting radiance field with a quadrotor flight dynamics engine to create a realistic simulator for generating high-quality training datasets for drone navigation; 2) designing an imitation learning pipeline to train liquid time-constant (LTC) networks [5, 6], which have out-of-distribution generalization capabilities [7], on the generated data; and 3) deploying the trained networks on a real drone to demonstrate robust generalization.</p>
<p>Quantitative flight tests show successful transfer of learned navigation skills from simulation to real-world settings. Policies trained exclusively on photorealistic simulated data of indoor flights targeting single manoeuvres generalize to complex, multi-step tasks in outdoor environments on real hardware. To the best of our knowledge, this is the first published report of an autonomous quadrotor policy trained entirely in simulation using imitation learning, without scene randomization, that is capable of being deployed into the real world, and generalizing to out-of-distribution environments. Our contribution relies on five key pillars:</p>
<ol>
<li>Dynamics Augmented Gaussian Splatting Simulator: A simulator that combines the strengths of Gaussian Splatting and flight dynamics, providing both photorealistic rendering alongside high-fidelity dynamics.</li>
<li>Implicit Closed-Loop Augmentation: Expert trajectory design that inherently includes edge cases and recovery scenarios, eliminating the need for additional closed-loop augmentation and model fine-tuning.</li>
<li>Liquid NNs Augmentation with Irregularly Time-Sampled Data: Robustifying performance of Liquid NNs via irregularly time-sampled data training, and enhancing their adaptability to diverse temporal dynamics.</li>
<li>Simulation and Real-World Validation: Extensive testing to validate the effectiveness of our method across different simulation environments and in real-world flight scenarios.</li>
<li>Zero-Shot Real-World Transfer: Showcase of the pipeline's capability for zero-shot transfer, enabling a quadrotor to successfully navigate outdoor environments in a real-world hiking demonstration.</li>
</ol>
<h2>2 Related Works</h2>
<p>Imitation Learning. Imitation learning, and more specifically behavior cloning, involves an agent learning a task by mimicking expert demonstrations, which is especially advantageous when defining a reward function is difficult. This approach captures implicit navigation knowledge in complex environments and is more sample-efficient than Reinforcement Learning (RL) [8], though it can suffer from policy stationarity and compounding errors [9]. Robot navigation methods built on foundation models [10, 11] and diffusion policies [12], exhibit very potent capabilities but violate compute constraints for real-time operability on edge devices.</p>
<p>The generalization problem of few-shot, one-shot, and zero-shot imitation learning agents has received extensive attention in the literature, with methods including augmentation strategies [13, 14], human interventions [15, 16], goal-conditioning [17, 18, 19, 20], reward conditioning [21, 22, 23, 24], task-embedding [25], and meta-learning [26].</p>
<p>We opt for this specific setting since expert drone trajectories are easy to design in simulation and thus labeled data can be produced flexibly at speed for specific flight tasks.</p>
<p>Liquid Neural Networks. Liquid Neural Networks (LNNs) are a family of continuous-time (CT) neural networks [27] that can be trained via gradient descent in modern automatic differentiation frameworks. Their building blocks are Liquid Time Constant networks (LTCs) built off of leaky-integrator neural models [28] with the steady-state dynamics of a conductance-based nonlinear synapse model [29]. The model is a differentiable dynamical system with an input-dependent varying (hence liquid) time characteristic. Outputs are obtained by numerical differential equation solvers in the ordinary differential equations formulation and by continuous functions when approximately solved as Closed-form Continuous-time (CfC) models [6]. Amongst LNNs' desirable characteristics are their stable and bounded behavior, superior expressivity [5], improved performance on time series prediction [30] and great promise in both end-to-end imitation and reinforcement learning, to map high-dimensional visual input stream of pixels to robust control decisions [7, 31, 32, 33].</p>
<p>Note: LNNs refer to the general category of models that are presented either by LTCs or CfCs. In this work, we use CfCs which have exhibited superior generalization performance when trained via behavior cloning for out-of-distribution flight tasks [7].</p>
<p>Sim-to-Real Transfer. Photorealistic simulators are popular for training quadrotor visual policies [34, 35, 36, 37]. Yet, photorealism is not sufficient to ensure the transfer of skills learned in simulation to the real-world, due to differences in physical properties, sensor noise, or actuator dynamics. Many sim-to-real techniques have been developed to mitigate the transfer challenge [38, 39, 40, 41]. Generalization to unseen environments is mostly achieved via domain randomization [40], which incentivizes networks to learn representations that are scene and domain-independent by randomizing input features that are superfluous to the task. Indeed, amongst existing techniques [42, 43] are position randomization, depth image noise randomization, texture randomization, size randomization, and so on.</p>
<p>In this work, we use a well-designed randomization in the positioning of task-relevant objects in the scene and perform RGB image augmentation on the training data (brightness, contrast, and saturation) [13, 44, 45, 46]. However, we do not resort to any randomization in the scenes simulated. In fact, we attempt to provide a simple environment data generation and training protocol and to subsequently test the fundamental task representation learning capabilities of networks.</p>
<p>Gaussian Splatting. 3D Gaussian splatting (3D GS, or GS for brevity) [2] is a technique for explicit radiance field rendering. It relies on the fitting of millions of 3D Gaussians to the training dataset images, a significantly different approach from the neural radiance field (NeRF) methodologies [1]. Offering real-time rendering without visual quality compromises, GS enables many new avenues in fields such as virtual reality and augmented reality, real-time cinematic rendering [47, 48, 49], but also autonomous driving [50]. The explicit GS construction comes with significant advantages, as it enables real-time rendering capabilities in addition to vast levels of control, compositionality and editability for complex scenario handling [51, 52].</p>
<p>Connections between Unmanned Aerial Vehicles (UAVs) and radiance field rendering models are well established, primarily focusing on optimizing UAV deployment for training these models. Notable methods include real-time online NeRF training from UAV camera streams [53], efficient reconstruction of unbounded large-scale scenes [54], and autonomous positioning for real-time 3D reconstruction [55], demonstrating success in various scenes and structures [56]. Additionally, GS has been utilized in Simultaneous Localization and Mapping (SLAM) for active perception modules [57] and real-time SLAM algorithms [58, 59, 60]. The simulation of quadrotor flight in neural radiance field environments is explored in [61], where planning and estimation algorithms are developed for deployment within the NeRF setting. Closest to our simulator is the PEGASUS system [3], which combines a physics engine with GS rendering and enhances objects with physical properties for manipulation data generation.</p>
<h1>3 Methods</h1>
<p>The end-to-end visual navigation model $\pi$ takes sequences of RGB images captured from drone onboard camera as input. For training, each frame $x_{i}$ is labeled with the instantaneous quadrotor velocities and yaw angular rate in the quadrotor body frame. More precisely, its label, $y_{i}$, is a 4dimensional vector containing the x- (pitch axis, forward/backward), y- (roll axis, left/right), and z(throttle axis, up/down) axis velocities in body-frame, in addition to the yaw rate of the quadrotor. The behavior cloning process, depicted in Fig. 1, trains the network, $\pi$, parametrized by $\theta$ to output at time $t$ a control command, $\mu_{t}$, given an input image, $x_{t}$, that minimizes a Mean Squared Error (MSE) loss function relative to the ground truth labels.</p>
<h3>3.1 High-fidelity Vision and Dynamics Simulation</h3>
<p>GS model of the indoor flight room. The environment selected for rendering is an indoor fight room. Images are collected using an iPhone 12 Pro camera via the Polycam app, resulting in a dataset of 430 images at $1024 \times 768$ pixels. The images are processed with COLMAP to create depth maps, and the GS model is trained on these outputs at half resolution, rendering images with satisfactory photorealism (Fig. 2) despite some visual artifacts.</p>
<p>Adding models of task-related objects. We use two objects-red and blue spheres-defined in the geometry object file format ( obj ). Using the open-source 3D creation suite Blender, we render images of the spheres from all angles. We then apply the same training procedure as before to train a Gaussian Splatting model for each object. Finally, we manually tune the size and intensity of the trained GS models to ensure smooth blending into the indoor environment, as shown in Fig. 2.</p>
<p>Dynamics for GS trajectories. To generate images reflecting the dynamics of a simulated quadrotor, we align the visual GS world with the dynamics solver by finding the gravity-aligned vertical vector. Using the PyBullet physics engine [62], we tune the scaling to ensure consistent object positioning between the environments. The physics simulation runs at 240 Hz , while inference occurs at a slower rate. The GS camera moves based on relative displacement in PyBullet, rendering updated views from the quadrotor's positions for trajectory data or neural network inference (see Fig. 3). Implementation details are in Appendix A.</p>
<h3>3.2 Task Description</h3>
<p>The task is to navigate through multiple checkpoints using visual cues. The agent must reach the current target and turn according to its color: left for red and right for blue, as shown in Fig. 2. This setup mimics following trail blazes on a hike, guiding the hiker both in direction and toward the next cue. Consequently, each expert demonstration trajectory consists of two phases: approach and turn. We generate an equal number of right and left trajectories to avoid bias in the models.</p>
<p>Approach. Naive Imitation Learning for autonomous navigation often fails when encountering trajectories outside the training distribution. To address this, recovery trajectories-showing how to return to desired paths from suboptimal positions-are added to the training data. This method is usually infeasible with real-world demonstrations but can be synthesized. Here, we ensure all trajectories contain recovery information by positioning the quadrotor at trajectory starts so the target partially appears at the image border and by randomizing the initial target distance. This exposes the network to various approach angles and distances, driven by PID controllers that adjust the quadrotor's velocities and yaw rate to center the target in the frame. The significance of this strategy is discussed in Appendix C.4.</p>
<p>Turn. The turn procedure is straightforward and requires no special treatment, as it begins once the drone hovers at a fixed distance from the target. We send a constant yaw rate command, with an amplitude of about $12^{\circ}$ per second-nearly double the maximum rate during the approach phase when the sphere is at the frame's edge. This clearly distinguishes the switch in behavior at test time, reducing the risk of confusing a segment of the approach maneuver with a decision to turn.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Gaussian Splatting dataset sample trajectories. The trajectories are designed in two phases, first an approach where the drone gets close to the target and centers it in the middle of the image, followed by a turn procedure in the direction corresponding to the color of the target.</p>
<h3>3.3 Irregularly Sampled Data Augmentation</h3>
<p>It is often assumed that the inference frequency of a neural network at test time is that of the training data. This holds when data collection and model deployment are both executed on the same platform and data processing and inference times are negligible with respect to the sample period. In fact, popular recurrent models cannot handle data that is irregularly sampled or sampled at a different frequency, as they treat time steps between inputs as a constant during training.</p>
<p>Neural ODEs [63], however, can be evaluated at arbitrary time intervals, as their hidden states x(t) evolve according to the time-continuous differential equation, $\frac{d\mathbf{x}(t)}{dt}=f(\mathbf{x}(t), \mathbf{I}(t), t, \theta)$.</p>
<p>More specifically, the closed-form CfC representation of the hidden states is given by [6]:</p>
<p>$$
\mathbf{x}(t)=\sigma\left(-f\left(\mathbf{x}, \mathbf{I} ; \theta_{f}\right) t\right) \odot g\left(\mathbf{x}, \mathbf{I} ; \theta_{g}\right)+\left[1-\sigma\left(-[f\left(\mathbf{x}, \mathbf{I} ; \theta_{f}\right)] t\right)\right] \odot h\left(\mathbf{x}, \mathbf{I} ; \theta_{h}\right)
$$</p>
<p>Here, $\mathbf{x}(t)$ is the hidden state, $\odot$ is the Hadamard product, $\sigma$ is the sigmoid function, $f, g$, and $h$ are three neural network heads with a shared backbone, parameterized by $\theta_{f}, \theta_{g}$, and $\theta_{h}$, respectively. $\mathbf{I}(t)$ is an external input, and $t$ is time sampled by input time-stamps, as illustrated in Fig. 10 in Appendix D.</p>
<p>As time appears explicitly in equation (1), the formulation eliminates the need for complex numerical solvers (speedup between one and five orders of magnitude). We utilize this flexible data handling to implement an irregularly sampled data augmentation scheme that randomizes the time steps between training samples, helping networks generalize across frequency domains.</p>
<h1>4 Experiments</h1>
<h3>4.1 Hardware Platform</h3>
<p>All real-world evaluations of trained policies are executed on a DJI M300 RTK quadcopter. The M300 interfaces with the DJI Manifold 2 companion computer, enabling programmatic control of the drone. The onboard computer runs an NVIDIA Jetson TX2, which has GPU capability for onboard inference in real-time (more information about the hardware setup in Appendix A).</p>
<h3>4.2 Models Deployed</h3>
<p>All models share the same general architecture of a Convolutional Neural Network (CNN) backbone that extracts features from the input images ( $256 \times 144$ pixels) and a Recurrent Neural Network (RNN) head for sequential decision-making, trained with the MSE loss. Details about the training procedure and model architectures are provided in Appendix D.</p>
<p>Liquid variants. We train three CfC models to test the impact of training data frequency and time scale augmentation on performance. All three models share the same skeleton of 34 liquid neurons,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Diagram of the in-simulation inference protocol with physics engine dynamics and GS rendering. At time $t$, the GS image $x_{t}$ is rendered from the simulated position and attitude, and processed through the CNN of the policy network $\pi\left(x_{t}\right)$. Obtained features, along with the elapsed time $\delta t=t-t_{-}$, are input into the Liquid network, which outputs the velocity control command.
and are trained on the same collection of trajectories. However, the frequencies at which the data points are logged along the trajectories are modulated to generate three different datasets.</p>
<p>The first model, called Liquid, is designed for time-scale augmentation. The frequency of recorded images and labels varies throughout the trajectories, with each data point taken at a random time difference corresponding to frequencies between 1 and 10 Hz (see Appendix B). The dataset includes input images, labels, and the time delta since the last input, as discussed in section 3.3. Liquid 3 Hz and Liquid 9 Hz are trained on datasets with constant sampling rates of 3 Hz and 9 Hz , respectively.</p>
<p>LSTM Baseline Model. Long Short-term Memory (LSTM) [64] is a discrete-time neural network that consists of a memory cell that is controlled by learnable gates that access, store, clear, and retrieve the information contained in the memory. Amongst a plethora of RNN architectures tested on similar visual flight navigation tests, the LSTM architecture seems to offer the best performance amid the non-liquid neural networks [7]. Thus, we select it as our baseline and train on the 3 Hz dataset to maintain proximity to the test hardware running frequency (LSTM cannot handle irregularly sampled data). The LSTM architecture we devise comprises just over 300k parameters. It is worth noting that its size is about 25 times that of its liquid counterparts that we deploy in this work.</p>
<h1>4.3 Results</h1>
<p>Evaluation protocol. Performance is associated to manoeuvre success rates. For a checkpoint to be valid, we require the quadrotor to come to a hover at a distance corresponding to the stopping position seen in training ( 0.5 m in simulation, between 1 and 1.5 m for the real platform) and then turn in the correct direction at a rate close to that used of the labels (above $10^{\circ}$ per second). Failure cases are defined in Appendix C.</p>
<p>Sim-to-Sim. The initial setup consists of training with the non-photorealistic input images from the PyBullet simulator. This enables longer and more elaborate hiking tests, exploiting the flexibility and unlimited space available. We evaluate the models at an inference frequency of 3 Hz on a 100-step hiking task, with randomly initialized targets ensuring the next checkpoint is centered in the drone's field of view after correct turns. The bar plot showing the lengths of the hikes completed by each agent for 10 different initial checkpoint placements is provided in Fig 4.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Results of the infinite simulated hike experiment for 10 different initializations of the goal positions in the PyBullet environment.</p>
<p>Liquid and Liquid 3 Hz models consistently achieve long hikes (mean lengths of 56.1 and 83.4 , respectively), successfully performing both the 1-target task and chronological composition. The Liquid 9 Hz agent averages only 2.3 checkpoints, sometimes failing the first target manoeuvre, highlighting challenges in closed-loop inference at different frequencies from training. Elsewhere, the model really struggled to perform the task on blue targets. The LSTM model can perform the 1target task consistently but fails to generalize to multiple steps as it generates more erratic commands ( 1.4 hike length on average).</p>
<p>GS-to-GS. The models trained on the GS datasets are evaluated in the GS-rendered simulator at an inference frequency of 3 Hz , following the process shown in Fig. 3. Space constraints in the rendered indoor flight room limit the number of targets that can reasonably fit into the scene to two. Agents have a limited time window to navigate the two checkpoints, with success rates for the first step based on color and the average steps completed shown in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">$1^{\text {st }}$ target rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2 target</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Red</td>
<td style="text-align: center;">Blue</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: left;">Liquid</td>
<td style="text-align: center;">$\mathbf{0 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{1 . 7 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 3Hz</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$\mathbf{0 . 9 6}$</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">1.35</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 9Hz</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">1.04</td>
</tr>
</tbody>
</table>
<p>Table 1: GS-to-GS evaluation results of models on 2-target runs in the GS indoor flight room scene ( $N=100$ with color decided by a coin flip, 53 runs start with a red target and 47 with blue).</p>
<p>The first observation is the Liquid 9 Hz agent's incapacity to compete with the other Liquid models, highlighting how training data frequency impacts performance at different rates. Amongst the other models, we notice that both LSTM and Liquid 3Hz exhibit a bias in performance in favor of one of the colors, respectively red and blue (both over $96 \%$ success rate), doing much worse on the other. With only $43 \%$ on blue, LSTM achieves an average of 1.04 out of 2 hike steps. Similarly, but to a lesser extent, Liquid 3Hz struggles with red checkpoints ( $64 \%$ success rate), reaching an average length of 1.35 checkpoints. The Liquid agent seems to have equally well-mastered left and right manoeuvres ( 94 and $98 \%$ success respectively) and completes longer hikes, 1.72 steps on average. It also exhibits the most efficient and smooth manoeuvring (details in Appendix C).</p>
<p>GS-to-Real. The sim-to-real transfer evaluation, in the indoor flight room used for training the GS rendering, is deployed on the Matrice 300 quadrotor platform. We use cardboard disks as the goals. In turn, we position the targets on a tripod in the center of the room at a height of about 2 meters. We select five starting positions around the target, placing the quadrotor 4 to 5 meters away to give each agent four attempts at each color and expose them to multiple room angles. The translation velocities inferred by the networks are scaled in such a way as to balance enough time for human intervention and repeated test efficiency. The per color and total success rates are given in Table 2.</p>
<p>The challenge of direct sim-to-real Transfer is epitomized by the large drop in performance suffered by the LSTM agent (from 0.72 in GS to 0.125 ). Liquid neural networks, on the other hand, suffer minimally in comparison. Indeed, both the Liquid 3 Hz and 9 Hz agents succeed just over half of the time. Again, it is the time-scale augmented Liquid agent that performs best. It goes from 96 to $75 \%$ success rate, maintaining balanced performance across colors.</p>
<p>GS-to-Real + Generalization. We deploy the same test protocol on an outdoor lawn in an urban campus. In addition to the completely unseen scene, the various starting positions now expose the agents to sunlight from various angles, making for a challenging generalization scenario. We compare the best Liquid model and the LSTM model for reference in this setting, and the results are provided in Table 3.</p>
<p>Unsurprisingly, the LSTM agent is incapable of completing the task even once in 40 attempts. Our Liquid agent, however, demonstrates zero-shot task completion capability. Though relatively struggling with red targets ( $40 \%$ ) in the outdoors setting it does better on blue checkpoints ( $60 \%$ ), exhibiting the ability to perform the simulation-learned task more than half of the time completely out-of-distribution outdoor real-world setting. Challenging background lighting conditions, as well as tree shades and reflections off buildings impact the appearance and robust perception of targets.</p>
<p>Encouraged by these results, we design a multistep outdoor setup in an alternative campus location to showcase performance on the comprehensive hiking task. Our Liquid agent demonstrates state-of-the-art transfer and generalization by successfully navigating through a series of 5 consecutive checkpoints. Videos of training data runs, experiments and real world demos are provided on the project website (link).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Success rate</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Red</td>
<td style="text-align: right;">Blue</td>
<td style="text-align: right;">Total</td>
</tr>
<tr>
<td style="text-align: left;">Liquid</td>
<td style="text-align: right;">$\mathbf{0 . 4}$</td>
<td style="text-align: right;">$\mathbf{0 . 6}$</td>
<td style="text-align: right;">$\mathbf{0 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
<p>Table 3: GS-to-Real zero-shot evaluation results of models on single target runs on the outdoor campus lawn $(N=40)$.</p>
<h1>5 Limitations</h1>
<p>Dynamic scenes: Gaussian Splatting, like other neural rendering techniques, excels in static scenes but struggles with dynamic environments, such as moving objects and changing conditions, limiting its effectiveness in generating dynamic data for real-world scenarios. While affine transformations can adjust Gaussian attributes for moving objects, this process increases rendering time and is constrained to discrete components. In contrast, Liquid neural networks demonstrate robustness to dynamic backgrounds during testing [7], allowing them to focus on tasks despite moving distractions. However, efficiently learning from dynamic data in applications like urban air mobility and wildlife tracking remains outside the scope of this work.
Trajectory augmentation: As tasks become more complex and require longer planning horizons, designing efficient trajectory augmentation schemes becomes increasingly challenging. Current methods may not cover the full range of scenarios, especially edge cases and recovery situations, which are crucial for robust performance. This general limitation of imitation learning holds for datasets collected in the real world, in simulation, and in rendered environments.
Distribution shifts: Large distribution shifts between training and real-world environments can severely hinder performance; thus, the need for domain randomization to bridge substantial gaps remains essential for robustness across real-world environments not encountered during training.</p>
<p>Addressing these limitations is crucial for improving the robustness and applicability of visual end-to-end autonomous navigation systems trained in simulation for diverse real-world scenarios.</p>
<h2>6 Conclusion</h2>
<p>3D Gaussian Splatting has emerged as a potential solution for generating realistic visual training data flexibly and at scale for autonomous systems, surpassing the limitations of real-world data collection. However, bridging the gap between these simulated training environments and real-world applications remains a significant challenge in robot learning, typically addressed through domain randomization. In this work, we propose an end-to-end protocol for training autonomous agents for deployment into the real world, encompassing data-driven simulation methods, and reliance on Liquid neural networks, with newly explored augmentation techniques for improved transfer and generalization. Comprehensive experiments empirically confirm the ability of learned agents obtained via our pipeline to directly be deployed on an autonomous quadrotor platform to successfully perform a hiking task in environments they have never seen before. We believe our approach represents a leap in the employment of Gaussian Splatting and Liquid neural networks for training robust visual end-to-end autonomous navigation policies for direct, real-world deployment.</p>
<h1>Acknowledgments</h1>
<p>Support for this research has been provided in part by the Boeing company and ONR's Science of Autonomy program. We are grateful for it.</p>
<h2>References</h2>
<p>[1] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99-106, dec 2021. ISSN 0001-0782. doi:10.1145/3503250. URL https://doi.org/10.1145/ 3503250 .
[2] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3D Gaussian Splatting for RealTime Radiance Field Rendering. ACM Transactions on Graphics, 42(4):1-14, July 2023. doi:10.1145/3592433. URL https://inria.hal.science/hal-04088161.
[3] L. Meyer, F. Erich, Y. Yoshiyasu, M. Stamminger, N. Ando, and Y. Domae. Pegasus: Physically enhanced gaussian splatting simulation system for 6dof object pose dataset generation, 2024.
[4] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators.
[5] R. Hasani, M. Lechner, A. Amini, D. Rus, and R. Grosu. Liquid time-constant networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(9):7657-7666, 2021.
[6] R. Hasani, M. Lechner, A. Amini, L. Liebenwein, A. Ray, M. Tschaikowski, G. Teschl, and D. Rus. Closed-form continuous-time neural models. arXiv preprint arXiv:2106.13898, 2021.
[7] M. Chahine, R. Hasani, P. Kao, A. Ray, R. Shubert, M. Lechner, A. Amini, and D. Rus. Robust flight navigation out of distribution with liquid neural networks. Science Robotics, 8 (77):eadc8892, 2023. doi:10.1126/scirobotics.adc8892. URL https://www.science.org/ doi/abs/10.1126/scirobotics.adc8892.
[8] S. Ross and D. Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 661-668. JMLR Workshop and Conference Proceedings, 2010.
[9] S. Schaal. Is imitation learning the route to humanoid robots? Trends in cognitive sciences, 3 (6):233-242, 1999.
[10] D. Shah, A. Sridhar, N. Dashora, K. Stachowicz, K. Black, N. Hirose, and S. Levine. Vint: A foundation model for visual navigation, 2023.
[11] A. Maalouf, N. Jadhav, K. M. Jatavallabhula, M. Chahine, D. M. Vogt, R. J. Wood, A. Torralba, and D. Rus. Follow anything: Open-set detection, tracking, and following in real-time. IEEE Robotics and Automation Letters, 9(4):3283-3290, 2024. doi:10.1109/LRA.2024.3366013.
[12] A. Sridhar, D. Shah, C. Glossop, and S. Levine. Nomad: Goal masked diffusion policies for navigation and exploration, 2023.
[13] S. Ross, G. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.</p>
<p>[14] T. Zhang, Z. McCarthy, O. Jow, D. Lee, X. Chen, K. Goldberg, and P. Abbeel. Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 5628-5635. IEEE, 2018.
[15] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 991-1002. PMLR, 2022.
[16] P. Goyal, R. J. Mooney, and S. Niekum. Zero-shot task adaptation using natural language. arXiv preprint arXiv:2106.02972, 2021.
[17] F. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy. End-to-end driving via conditional imitation learning. In 2018 IEEE international conference on robotics and automation (ICRA), pages 4693-4700. IEEE, 2018.
[18] D. Ghosh, A. Gupta, A. Reddy, J. Fu, C. M. Devin, B. Eysenbach, and S. Levine. Learning to reach goals via iterated supervised learning. In International Conference on Learning Representations, 2020.
[19] C. Lynch, M. Khansari, T. Xiao, V. Kumar, J. Tompson, S. Levine, and P. Sermanet. Learning latent plans from play. In Conference on robot learning, pages 1113-1132. PMLR, 2020.
[20] S. Emmons, B. Eysenbach, I. Kostrikov, and S. Levine. Rvs: What is essential for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.
[21] R. K. Srivastava, P. Shyam, F. Mutz, W. Jaśkowski, and J. Schmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877, 2019.
[22] G. Neumann and J. Peters. Fitted q-iteration by advantage weighted regression. Advances in neural information processing systems, 21, 2008.
[23] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 2021.
[24] S. Haldar, J. Pari, A. Rai, and L. Pinto. Teach a robot to fish: Versatile imitation from one minute of demonstrations. arXiv preprint arXiv:2303.01497, 2023.
[25] S. James, M. Bloesch, and A. J. Davison. Task-embedded control networks for few-shot imitation learning. In Conference on robot learning, pages 783-795. PMLR, 2018.
[26] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via meta-learning. In Conference on robot learning, pages 357-368. PMLR, 2017.
[27] R. T. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 6572-6583, 2018.
[28] L. Lapique. Recherches quantitatives sur l'excitation electrique des nerfs traitee comme une polarization. Journal of Physiology and Pathololgy, 9:620-635, 1907.
[29] C. Koch and I. Segev. Methods in neuronal modeling: from ions to networks. MIT press, 1998.
[30] R. Hasani, M. Lechner, T.-H. Wang, M. Chahine, A. Amini, and D. Rus. Liquid structural state-space models, 2022.
[31] M. Lechner, R. Hasani, A. Amini, T. A. Henzinger, D. Rus, and R. Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10):642-652, 2020.
[32] C. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus. Causal navigation by continuoustime neural networks. Advances in Neural Information Processing Systems, 34, 2021.</p>
<p>[33] L. Yin, M. Chahine, T.-H. Wang, T. Seyde, C. Liu, M. Lechner, R. Hasani, and D. Rus. Towards cooperative flight control using visual-attention. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6334-6341, 2023. doi:10.1109/IROS55552. 2023.10342229.
[34] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, 2017. URL https: //arxiv.org/abs/1705.05065.
[35] A. Juliani, V.-P. Berges, E. Vckay, Y. Gao, H. Henry, M. Mattar, and D. Lange. Unity: A general platform for intelligent agents. ArXiv, abs/1809.02627, 2018. URL https://api. semanticscholar.org/CorpusID:52185833.
[36] Y. Song, S. Naji, E. Kaufmann, A. Loquercio, and D. Scaramuzza. Flightmare: A flexible quadrotor simulator. In Conference on Robot Learning, 2020.
[37] I. Zamora, N. G. Lopez, V. M. Vilches, and A. H. Cordero. Extending the openai gym for robotics: a toolkit for reinforcement learning using ros and gazebo, 2017.
[38] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In Proceedings of Robotics: Science and Systems, Pittsburgh, Pennsylvania, June 2018. doi:10.15607/RSS.2018.XIV.010.
[39] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In D. Precup and Y. W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1126-1135. PMLR, 06-11 Aug 2017. URL https://proceedings.mlr.press/ v70/finn17a.html.
[40] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30, 2017. doi: 10.1109/IROS.2017.8202133.
[41] O. M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020. doi:10.1177/0278364919887447. URL https://doi.org/10.1177/0278364919887447.
[42] A. Loquercio, E. Kaufmann, R. Ranftl, M. Müller, V. Koltun, and D. Scaramuzza. Learning high-speed flight in the wild. Science Robotics, 6(59):eabg5810, 2021. doi:10.1126/scirobotics.abg5810. URL https://www.science.org/doi/abs/10.1126/ scirobotics.abg5810.
[43] C. Xiao, P. Lu, and Q. He. Flying through a narrow gap using end-to-end deep reinforcement learning augmented with curriculum learning and sim2real. IEEE Transactions on Neural Networks and Learning Systems, 34(5):2701-2708, 2023. doi:10.1109/TNNLS.2021.3107742.
[44] J. Chen, B. Yuan, and M. Tomizuka. Deep imitation learning for autonomous driving in generic urban scenarios with enhanced safety. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 2884-2890. IEEE, 2019.
[45] X. Li, P. Ye, J. Jin, F. Zhu, and F.-Y. Wang. Data augmented deep behavioral cloning for urban traffic control operations under a parallel learning framework. IEEE Transactions on Intelligent Transportation Systems, 2021.</p>
<p>[46] A. Amini, T.-H. Wang, I. Gilitschenski, W. Schwarting, Z. Liu, S. Han, S. Karaman, and D. Rus. Vista 2.0: An open, data-driven simulator for multimodal sensing and policy learning for autonomous vehicles. arXiv preprint arXiv:2111.12083, 2021.
[47] D. Kalkofen, E. Mendez, and D. Schmalstieg. Comprehensible visualization for augmented reality. IEEE Transactions on Visualization and Computer Graphics, 15(2):193-204, 2009. doi:10.1109/TVCG.2008.96.
[48] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke, and A. Lefohn. Towards foveated rendering for gaze-tracked virtual reality. ACM Trans. Graph., 35(6), dec 2016. ISSN 0730-0301. doi:10.1145/2980179.2980246. URL https://doi.org/10.1145/ 2980179.2980246.
[49] R. Albert, A. Patney, D. Luebke, and J. Kim. Latency requirements for foveated rendering in virtual reality. ACM Trans. Appl. Percept., 14(4), sep 2017. ISSN 1544-3558. doi:10.1145/ 3127589. URL https://doi.org/10.1145/3127589.
[50] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes, 2023.
[51] R. Chabra, J. E. Lenssen, E. Ilg, T. Schmidt, J. Straub, S. Lovegrove, and R. Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d reconstruction. In A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, editors, Computer Vision - ECCV 2020, pages 608625, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58526-6.
[52] Z. Wang, J. Philion, S. Fidler, and J. Kautz. Learning indoor inverse rendering with 3d spatially-varying lighting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12538-12547, October 2021.
[53] J. Yu, J. E. Low, K. Nagami, and M. Schwager. Nerfbridge: Bringing real-time, online neural radiance field training to robotics, 2023.
[54] Z. Jia, B. Wang, and C. Chen. Drone-nerf: Efficient nerf based 3d scene reconstruction for large-scale drone survey. Image and Vision Computing, 143:104920, 2024. ISSN 0262-8856. doi:https://doi.org/10.1016/j.imavis.2024.104920. URL https://www.sciencedirect. com/science/article/pii/S0262885624000234.
[55] D. Patel, P. Pham, and A. Bera. Dronerf: Real-time multi-agent drone pose optimization for computing neural radiance fields, 2023.
[56] Y. Ge, B. Guo, P. Zha, S. Jiang, Z. Jiang, and D. Li. 3d reconstruction of ancient buildings using uav images and neural radiation field with depth supervision. Remote Sensing, 16(3), 2024. ISSN 2072-4292. doi:10.3390/rs16030473. URL https://www.mdpi.com/2072-4292/16/ $3 / 473$.
[57] S. He, C. D. Hsu, D. Ong, Y. S. Shao, and P. Chaudhari. Active perception using neural radiance fields, 2023.
[58] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li. Gs-slam: Dense visual slam with 3d gaussian splatting, 2024.
[59] H. Matsuki, R. Murai, P. H. J. Kelly, and A. J. Davison. Gaussian splatting slam, 2023.
[60] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald. Gaussian-slam: Photo-realistic dense slam with gaussian splatting, 2023.
[61] M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson, J. Bohg, and M. Schwager. Vision-only robot navigation in a neural radiance world. IEEE Robotics and Automation Letters, 7(2):4606-4613, 2022. doi:10.1109/LRA.2022.3150497.</p>
<p>[62] J. Panerati, H. Zheng, S. Zhou, J. Xu, A. Prorok, and A. P. Schoellig. Learning to fly-a gym environment with pybullet physics for reinforcement learning of multi-agent quadcopter control. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.
[63] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary differential equations, 2019.
[64] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997.
[65] J. L. Schönberger and J.-M. Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.
[66] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl. Algorithms for hyper-parameter optimization. Advances in neural information processing systems, 24, 2011.
[67] J. Bergstra, D. Yamins, and D. Cox. Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. In International conference on machine learning, pages 115-123. PMLR, 2013.
[68] T. Akiba, S. Sano, T. Yanase, T. Ohta, and M. Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019.</p>
<h1>Appendices</h1>
<h2>A Simulator and hardware setup</h2>
<h2>A. 1 Indoor room GS model</h2>
<p>Images of the room are collected using a consumer smartphone (iPhone 12 Pro) camera through the free access application Polycam. The application automates the capture of frames, recording images when the novelty in the frame is sufficiently large. We thus obtain a dataset of 430 images of the room at a resolution of $1024 \times 768$ pixels. The images are processed with the generalpurpose Structure-from-Motion (SfM) pipeline COLMAP [65] to create depth maps. The GS model is trained on the COLMAP outputs at half resolution ( $512 \times 384$ ), densification interval of 500, and 30,000 iterations. Training takes around 15 minutes on a single GPU (NVIDIA GeForce RTX 3080 Ti). The obtained model renders images of satisfactory photorealism as seen in Fig. 2, however, with the presence of many objects in the room and limited training, there remains some fogginess and some visual artifacts in the images. The purpose of this work is to achieve robust generalization, rather than to improve radiance field rendering. Thus we did not pursue further improvements of the GS model.</p>
<h2>A. 2 Grounding GS dynamics</h2>
<p>We ensure that the points of view we use for the generation of images are on trajectories true to the dynamics of an accurately simulated quadrotor model. To do so, we first use the GS renderer user interface to approximately find the gravity-aligned vertical vector. This is necessary to align reference frames between the visual GS world and the dynamics solver. PyBullet physics is used in our simulator which we adapt from the codebase of the authors in [62]. The drone dynamics we use are based on Bitcraze's Crazyflie 2.x nano-quadrotor.</p>
<p>Scaling is tuned manually such that distance units in PyBullet and units in the corresponding GS environment match. Furthermore, we ensure that the positioning of objects matches: for a given position and attitude, the drone front image extracted from the PyBullet environment and that obtained from the GS rendering contains the object of the same size and in the same position (as depicted in Fig. 3). The simulator thus behaves as a direct photorealistic enhancement of images seen from trajectories in the PyBullet physics simulator. Hence, each trajectory can be visualized in both environments and datasets can be generated for either.</p>
<h2>A. 3 Simulation protocol</h2>
<p>The physics simulation runs at 240 Hz , and we allow low-level flight control to occur at the same frequency, although inference runs at a much slower rate. Indeed, for inference or data collection, we simulate the evolution of the system with constant velocity command for a number of steps corresponding to the desired period. Then, we compute the relative displacement in the PyBullet environment and convert it to the GS space's dimensions. Next, we move the GS camera accordingly and render the view from the updated quadrotor position and attitude. Finally, the captured image is added to the trajectory data or fed into the neural network for inference before repeating with the current velocity command (process visualized in Fig. 3).</p>
<h2>A. 4 Hardware details</h2>
<p>The DJI Onboard SDK (Software Development Kit) and its associated ROS wrapper provide an interface for feeding the drone's low-level flight controller with desired high-level translation velocities and yaw rate commands. The flight controller itself is a black box system provided by the manufacturer which controls the four-rotor speeds to track the velocities specified by the TX2 computer. It is worth mentioning that the dynamics of the platform we use do not match those of the</p>
<p>nano-quadrotors simulated. A Zenmuse Z30 camera and gimbal are mounted on the underside of the drone, pointed forward. The gimbal compensates for the drone's current orientation, such that the roll or pitch of the drone does not result in the roll or pitch of the camera image. The gimbal follows the drone's current yaw such that the camera is always pointed forward. Input images gathered by the gimbal-stabilized camera are available to the companion computer via the SDK. We reach a runtime frequency of around 2.6 Hz with our setup, a budget shared between image capture, communication with the onboard computer, pre-processing and model inference, and sending of the output command.</p>
<h1>B Dataset design</h1>
<h2>B. 1 Trajectory design supplemental details</h2>
<p>Designing our dataset, we limit the trajectories to the case of reaching and turning at a single target, as opposed to generating runs hopping through multiple checkpoints. This choice is deliberate as it gives us more control over the initialization augmentation procedure, leading to better training data efficiency while ensuring the multi-step capability of the networks through sequential composition.</p>
<p>In the simulator, we always assume that the target is positioned at the origin of the $(x, y)$ horizontal plane, and at a fixed altitude. Each new trajectory is generated using PID controllers that stabilize the yaw, altitude, and distance gaps to a centered position facing the target. We uniformly sample an angular position anywhere around the origin, a starting distance from the target in a prefixed range. We then randomize the initial attitude and altitude of the drone as to randomize the placement of the target in the first frame. Three quarters of trajectories are such that the target is at a lateral edge (vertical position uniformly sampled). In the remaining runs, the target is at a vertical edge, with uniformly distributed yaw. This procedure is summarized in Algorithm 1 and a visualization of the positions of the spheres in the first frame of a 100 sampled trajectories is provided in Fig. 5.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 : Initial position and attitude sampling
Require: Maximum yaw angle \(\psi_{M}\), Distance range \(\left[d_{m}, d_{M}\right]\), Altitude range \(\left[z_{m}, z_{M}\right]\)
    # Initialize roll and pitch to zero:
    \(\left(\theta_{0}, \phi_{0}\right)=(0,0)\)
    # Sample initial angle and distance:
    \(\theta \sim \mathcal{U}(0,2 \pi), d \sim \mathcal{U}\left(d_{m}, d_{M}\right)\)
    # Position the drone in the horizontal plane:
    \(\left(x_{0}, y_{0}\right)=(d \cos (\theta), d \sin (\theta))\)
    # Sample chance parameter:
    \(c \sim \mathcal{U}(0,1)\)
    # Select yaw and altitude accordingly:
    if \(c&lt;0.75\) then
        \(z_{0} \sim \mathcal{U}\left(z_{m}, z_{M}\right)\)
        \(\psi_{0} \sim \mathcal{B}\left(0.5,\left\{-\psi_{M},+\psi_{M}\right\}\right)\)
    else
        \(z_{0} \sim \mathcal{B}\left(0.5,\left\{z_{m}, z_{M}\right\}\right)\)
        \(\psi_{0} \sim \mathcal{U}\left(-\psi_{M},+\psi_{M}\right)\)
    end if
    return \(s_{0}=\left[x_{0}, y_{0}, x_{0}, \theta_{0}, \phi_{0}, \psi_{0}\right]\)
</code></pre></div>

<p>We also provide sample trajectories for 600 approach phases generated according to the above described protocol with the ground-truth informed PID expert controller in Figure 6 and Figure 7. The trajectories show a dense coverage of the entire physical space available between the drone and the target in terms of both lateral and vertical displacements, thus including recovery information from a large set of situations by design.</p>
<h2>B. $2 \delta t$ probability mass function</h2>
<p>The time deltas are sampled to be divisible at the resolution of our simulation frequency $(240 \mathrm{~Hz})$. Time delta in seconds, $\delta t$, is sampled by the following distribution:</p>
<p>$$
\delta t \sim \delta t_{\min }\left[\alpha \mathcal{N}\left(\mu_{1}, \sigma_{1}\right)+\beta \mathcal{N}\left(\mu_{2}, \sigma_{2}\right)\right]
$$</p>
<p>with $\delta t_{\text {min }}$ being the physics simulation unit time step, $\mathcal{N}(\mu, \sigma)$ a Gaussian distribution of mean $\mu$ and standard deviation $\sigma$, and $\alpha, \beta, \mu_{1}, \sigma_{1}, \mu_{2}$, and $\sigma_{2}$ constants tuned to ensure adequate training run lengths (we require a minimum of 64 frames for a single trajectory), while still representing the full range of values between 1 and 10 Hz . The probability mass function can be seen in Figure 8.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Distribution of 100 initial altitudes and yaw offsets for trajectory generation.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Lateral plane projection of training dataset trajectories. Trajectories are curved as both yaw and forward movements are actuated simultaneously ( $\mathrm{n}=600$ ).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Vertical plane projection of training dataset trajectories. Trajectories depict the quadrotor rising or diving to align with the target $(\mathrm{n}=600)$.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Probability mass function of the $\delta t$ sampled for the generation of irregularly sampled datasets with the tuned parameters $\alpha=3, \beta=1, \mu_{1}=21, \sigma_{1}=30, \mu_{2}=196, \sigma_{2}=100$, and $t_{\min }=\frac{t}{240}$.</p>
<h1>C Additional Results</h1>
<h2>C. 1 Failure case identification</h2>
<p>Situations where the drone shoots past the targets (no slowing down), turns in the wrong direction (turn triggered at the rates of interest but with the opposite sign), stays still for prolonged periods (sub centimeter per second forward velocity during the approach phase for over 20 seconds in the real world tests, or if taking more than twice the average time of a manoeuvre in simulation), or loses the target before completing the approach (target out of frame during the approach and/or approaching another visual cue), all constitute failure cases. In our experiments, we notice that the most common mode of failure is the loss of the target out of the field of view during the approach phase (Sim, GS and Real) as well as turns in the wrong direction after a successful approach (GS and Real). Cases where the agents get stuck indefinitely occurred a handful of times across testing platforms.</p>
<h2>C. 2 Sim-to-GS</h2>
<p>The successful transfer of task completion capability from the GS trained liquid models to the real world poses the question of the level of photo-realism required to sustain such a transfer. Thus, we test whether models trained with non-rendered simulation data can generalize to the GS based simulation evaluation. We see this Sim-to-GS as an intermediate step between Sim-to-Sim and Sim-to-Real, which would most likely be informative about the prospects of achieving the latter. The results presented in Table 4 shed light on the difficulty agents have in generalizing meaningful behavior to environments with drastic visual characteristic changes. In fact, the Liquid 3 Hz and 9 Hz models suffer from a massive color-blind biases towards right turns and LSTM towards the left. The Liquid agent fails to initialize turns which leads to an extremely low performance. Thus we verify our claim that photo-realism is crucial for transferring vision-only end-to-end tasks learned in simulation.</p>
<p>Table 4: Sim-to-GS evaluation results of models on 2-target runs in the GS indoor flight room scene ( $N=100$ with color decided by a coin flip. 53 runs start with a red target and 47 with blue).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">1st-target success rate</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">2-target hike length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Red</td>
<td style="text-align: left;">Blue</td>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: left;">Liquid</td>
<td style="text-align: left;">0.04</td>
<td style="text-align: left;">0.02</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: center;">0.015</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 3Hz</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.04</td>
<td style="text-align: left;">0.09</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 9Hz</td>
<td style="text-align: left;">0.43</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">0.00</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.06</td>
<td style="text-align: center;">0.03</td>
</tr>
</tbody>
</table>
<h2>C. 3 Half duration GS-to-GS</h2>
<p>Elsewhere, we rerun the GS-to-GS experiment after halving the time budget allocated to the agents to complete the 2 -step hike. This allows us to evaluate the speed and efficiency closed-loop navigation exhibited by the different networks. The results, provided in Table 5, show the superiority of the Liquid agent which performs at the same standards in half the time (from 1.72 to 1.66 on 2target score), while the LSTM performance is halved on the 2 -target metric ( 1.04 to 0.51 ) and drops significantly for the first target as well ( 0.72 to 0.51 ). Both regularly time sampled Liquid models fail to complete the second passage in the reduced time (apart from 2 full runs completed by Liquid 3 Hz ).</p>
<h2>C. 4 Importance of recovery trajectories</h2>
<p>We compare 5 trained instances of the Liquid model with the data generation scheme described in the paper (Full Window), 5 instances on a dataset generated with the same procedure but with the window (along which initial positions are sampled) halved in height and width (Half Window), and 5 instances on a dataset generated by randomly sampling the original yaw and height as to have the</p>
<p>Table 5: Half time budget GS-to-GS evaluation results of models on 2-target runs in the GS indoor flight room scene ( $N=100$ with color decided by a coin flip. 53 runs start with a red target and 47 with blue).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">1st-target success rate</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">2-target hike length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Red</td>
<td style="text-align: center;">Blue</td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: left;">Liquid</td>
<td style="text-align: center;">$\mathbf{0 . 9 6}$</td>
<td style="text-align: center;">$\mathbf{1 . 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{1 . 6 6}$</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 3 Hz</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.79</td>
</tr>
<tr>
<td style="text-align: left;">Liquid 9 Hz</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.51</td>
</tr>
</tbody>
</table>
<p>target uniformly dispersed in the initial frame (Uniform Sampling). These are done in the PyBullet simulation environment and the best checkpoint based on validation error is saved for each.</p>
<p>All 15 models are first tested on the Sim-to-Sim experiment as reported in the manuscript's Section 4.3. The average hike lengths for each training iteration of each model (across the same 10 initializations used in 4.3) are provided in Table 6.</p>
<p>Table 6: Average hike length for different recovery strategies</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Instance</th>
<th style="text-align: center;">Full Window</th>
<th style="text-align: center;">Half Window</th>
<th style="text-align: center;">Uniform Sampling</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">10.3</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">58.2</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">88.9</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">49.1</td>
</tr>
</tbody>
</table>
<p>It is clear that sampling initial positions that never reach the sides and corners of the initial image is not a good strategy (even here, where testing does not involve extreme recoveries). Two training instances are around the 10 average hike length with the Half Window recovery dataset, with the others considerably worse. Our Full Window recovery strategy generates the best-performing models overall, although other training instances lead to poor performance. Uniform Sampling produces slightly less high-scoring models as well, with an improved average due to a smaller drop-off for lesser runs.</p>
<p>We then set up a recovery-specific test, where we force the initial placement of the target in one of the four corners of the frame, and run 100 single-target reach and turn tests with a fixed time window for each training iteration as above (discarding the disqualified Half Window strategy). The success rates are given in Table 7.</p>
<p>Table 7: Recovery importance target-in-corner test results comparing Full Window and Uniform Sampling. Values are success rates $(N=100)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Instance</th>
<th style="text-align: center;">Full Window Recovery</th>
<th style="text-align: center;">Uniform Sampling</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.83</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.00</td>
<td style="text-align: center;">0.64</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>We notice that while uniform sampling can only lead to a single model performing above the $80 \%$ mark, our full window approach only leads to a single checkpoint that does not reach that figure with all others well above and two out of five with perfect performance. Overall, this leads to an</p>
<p>$18 \%$ improvement in performance on average and highlights the importance of maximizing recovery trajectories in the training dataset to ensure robustness.</p>
<h1>C. 5 Effect of irregularly sampled data augmentation on LSTM</h1>
<p>To assert the architectural superiority of liquid neural networks we modify the LSTM architecture to include the irregularly sampled time difference to the last frame as an additional input. We call this variant $\mathrm{LSTM}+\delta t$. We train 5 instances of the $\mathrm{LSTM}+\delta t$ (with the LSTM hyperparameters) to compare to 5 retrained Liquid models on the PyBullet simulator data (same checkpoints as above). The models are all tested on the Sim-to-Sim 100 step hike. The average hike length obtained among the 10 initializations used in Section 4.3, as well as their average across training instances, are provided in Table 8.</p>
<p>Table 8: Average hike length for Liquid vs. LSTM $+\delta t$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training Instance</th>
<th style="text-align: center;">Liquid</th>
<th style="text-align: center;">LSTM $+\delta \mathbf{t}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">9.9</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4.5</td>
<td style="text-align: center;">9.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">3.3</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">6.2</td>
</tr>
<tr>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">6.0</td>
</tr>
</tbody>
</table>
<p>The best $\mathrm{LSTM}+\delta t$ model obtained can barely reach 10 targets per run, which is under a fifth of the score reported in Section 4.3 for Liquid (56.1) and close to ten times less than the bestobtained model after retraining (94.4). It does, however, perform better than the LSTM model reported in the paper (average of 1.4 hike length). This shows that augmenting models with timestep knowledge is indeed beneficial, but much larger performance discrepancies are to be attributed to model architecture.</p>
<h2>C. 6 Real-world trajectory visualisation</h2>
<p>Figure 9 provides a top-down view of the drone's trajectory in the horizontal plane during the demonstration provided in our supplementary material (video with 5 checkpoints in the completely out-ofdistribution environment). The blue and red dots represent the approximate reconstructed locations of the targets along the hike. We use a colorbar to visualize the variations in forward velocity $v_{x}$. The trajectory starts in the bottom left corner and the drone makes its way up to the top right. We notice that as programmed in the training data, the forward velocity is relatively high during approach and then reduces to a near stop before turning in front of the targets. Although robust in navigating in a similar fashion to the proportional gain controller from training (increasingly aligned with target, no systematic overshoot), the smoothness and duration of transitions are somewhat variable due to control and time delay errors, exterior conditions (wind), and the high level of distribution shift that the model has to deal with.</p>            </div>
        </div>

    </div>
</body>
</html>
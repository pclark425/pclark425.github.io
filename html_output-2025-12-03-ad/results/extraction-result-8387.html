<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8387 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8387</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8387</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-a28ee1bea680c745636d7e09218fffda5d544ffe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a28ee1bea680c745636d7e09218fffda5d544ffe" target="_blank">Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work provides the first demonstration of neural networks recognizing the generalized Dyck languages, which express the core of what it means to be a language with hierarchical structure.</p>
                <p><strong>Paper Abstract:</strong> We introduce three memory-augmented Recurrent Neural Networks (MARNNs) and explore their capabilities on a series of simple language modeling tasks whose solutions require stack-based mechanisms. We provide the first demonstration of neural networks recognizing the generalized Dyck languages, which express the core of what it means to be a language with hierarchical structure. Our memory-augmented architectures are easy to train in an end-to-end fashion and can learn the Dyck languages over as many as six parenthesis-pairs, in addition to two deterministic palindrome languages and the string-reversal transduction task, by emulating pushdown automata. Our experiments highlight the increased modeling capacity of memory-augmented models over simple RNNs, while inflecting our understanding of the limitations of these models.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8387.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8387.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM counting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long Short-Term Memory counting mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentions and uses of LSTM models developing internal counting-like representations that enable recognition of counting languages (e.g., a^n b^n, a^n b^n c^n and Dyck D1), evidenced by analysis of hidden-state values and successful generalization in prior work; used here as a baseline and discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM (Long Short-Term Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard single-layer LSTM recurrent network used in experiments as a baseline (in this paper typically with 8 or 12 hidden units); architecture per Hochreiter & Schmidhuber (1997). No large-scale pretraining or transformer-style training — trained from scratch on synthetic formal-language corpora using Adam for a few epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Dynamic counting tasks and counter-like languages: a^n b^n, a^n b^n c^n, well-balanced parenthesis (Dyck D1), and other counting-type synthetic languages.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Internal hidden-state activations that implement counters (continuous-valued variables in hidden units or gating/state vectors) that increment/decrement in response to symbols; representations interpreted as numeric counters or digit-like activations rather than explicit digit-by-digit arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Analysis/visualization of hidden state values (inspection of activations over time); evaluation of generalization to longer sequences (out-of-distribution lengths); comparison to other RNN variants (vanilla RNN, GRU) as empirical probe.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior work (cited) shows LSTMs can learn/count languages such as a^n b^n and a^n b^n c^n and generalize far outside training lengths. In this paper LSTM baseline performance: on D2 training mean ≈ 52.38% (Table 2) but test generalization very poor (test mean ≈ 1.39%); on D3 training mean ≈ 32.58% and test mean ≈ 0.04% (Table 3). LSTMs performed much better on simple counting (D1 / earlier literature) than on languages requiring full stack behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Fails to generalize to stack-based tasks requiring nested matching beyond one-counter capability (Dyck D2+); poor OOD length generalization on nested languages; susceptibility to local minima in training on more complex hierarchical tasks; limited by being a counter-machine rather than a stack machine for Dyck>1.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Cited prior analyses (Gers & Schmidhuber 2001; Weiss et al. 2018; Suzgun et al. 2019b) showing interpretable hidden-state trajectories that behave like counters; in this paper LSTM baselines' hidden-state diagnostics are discussed as motivation and their empirical failure on Dyck>1 supports the claim that LSTMs implement counters but not full stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Although LSTMs can implement counting/counters (e.g., D1), they fail on Dyck D2 and higher in this paper's experiments (very low test accuracy), demonstrating counting mechanism does not suffice for hierarchical stack requirements. Other architectures (memory-augmented models) outperform LSTMs on these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8387.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8387.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GRU counting limitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gated Recurrent Unit inability to implement counting mechanisms</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper references prior work showing GRUs cannot perform the same type of dynamic counting that LSTMs can, and uses this as part of the motivation for exploring memory-augmented architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>On the practical computational power of finite precision rnns for language recognition</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GRU (Gated Recurrent Unit)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard GRU recurrent architecture (Cho et al., 2014) discussed in related work; not the focus of experiments in this paper (cited as showing inferior counting ability).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Counting / dynamic counting tasks (e.g., a^n b^n and other counter languages).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Argument in cited work: GRU gating/architecture does not permit the same stable, saturating counter-like representations LSTMs can develop; therefore GRUs fail to realize reliable incremental counters in finite precision.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Prior literature analyses and theoretical/empirical comparisons (cited). This paper does not perform new probing on GRUs but cites these results as background.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No new GRU experiments in this paper; cited prior results indicate GRU failure on counter tasks while LSTMs succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Unable to form sustained numeric counter-like activations needed for dynamic counting; fails to generalize on counting languages according to cited studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Referenced theoretical and empirical analyses (Weiss et al., cited) concluding GRUs lack mechanisms to implement the required counters.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not evaluated experimentally here; included to contextualize why memory augmentation is necessary for richer hierarchical languages.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8387.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8387.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARNN pushdown emulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-Augmented Recurrent Neural Networks emulating pushdown automata (Stack-RNN, Stack-LSTM, Baby-NTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's introduced memory-augmented RNNs (Stack-RNN, Stack-LSTM, Baby-NTM) implement differentiable stack/memory mechanisms (push/pop, rotates) and empirically learn to emulate pushdown automata to solve stack-requiring tasks (Dyck D2,D3,D6, palindrome, reversal); internal memory entries show stack-like behavior and special markers indicating stack bottom.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Stack-RNN, Stack-LSTM, Baby-NTM (memory-augmented RNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three architectures introduced/used in this paper: (1) Stack-RNN — RNN with differentiable unbounded stack where top element s^{(0)} influences hidden state and push/pop are soft (softmax or Gumbel-softmax controls); (2) Stack-LSTM — same with LSTM controller; (3) Baby-NTM — fixed-size external memory with deterministic read/write semantics and five operations (ROTATE-RIGHT, ROTATE-LEFT, NO-OP, POP-LEFT, POP-RIGHT) combined by soft weights. Experiments typically used single-layer controllers with 8 (or 12) hidden units and one-dimensional memory entries (higher-dim experiments also performed), trained with Adam for 3 epochs on synthetic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Stack-based counting / hierarchical matching tasks: generalized Dyck languages (D2, D3, D6), deterministic homomorphic palindrome (w # φ(w^R)), simple palindrome (w # w^R), and string-reversal transduction (w #^{|w|} -> #^{|w|} w^R). These tasks require stack-like push/pop behavior rather than simple numeric addition.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Differentiable stack/memory entries act as stored symbols (one-dimensional values) with weighted linear interpolation to implement push/pop in superposition; controllers learn to emit weights (via softmax/Gumbel-softmax with temperature) that correspond to push/pop actions; memory contents include distinct marker values used to represent stack-bottom or empty slots; the hidden-state acts as finite-state control while memory implements stack storage (so the overall mechanism is a learned PDA emulation).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Probing by visualization of memory-operation strengths and memory-entry values across time (Figures 3 and 4), varying softmax temperature and employing Gumbel-softmax sampling, varying memory dimensionality (1D to 5D), and measuring generalization to longer test sequences (training lengths 2–50, test 52–100). Evaluation is via sequence-prediction MSE and thresholded k-hot outputs; interventions include changing number of memory dimensions and softmax temperature/gumbel sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Substantial empirical results in paper: On D2 (memory dim=1, 8 hidden units) many MARNNs achieved near-perfect generalization: Stack-RNN+Softmax training mean 100% and test mean 99.99% (Table 2); Stack-RNN+Softmax-Temp similar. Vanilla RNN test mean 0%; Vanilla LSTM test mean 1.39%. On D3 (same small memory) some MARNNs achieved >99% in subset of runs but overall variability (Table 3); increasing memory dimensionality improved performance. On D6 with larger memory (12 hidden units, 5-dim memory) MARNNs achieved high performance: Stack-RNN+Softmax test mean 99.85%, Baby-NTM+Softmax test mean 99.87% (Table 4). Palindrome/homomorphic palindrome: many MARNNs achieved high test accuracy (Table 5); simple palindrome with 1D memory had failures but succeeded when memory dim increased to 5. String reversal: many MARNNs obtained perfect test accuracy in a substantial fraction of runs (Table 6; e.g., Baby-NTM+Gumbel-Softmax training mean ≈ 90.01%, test med 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Training instability and variability across random restarts (some runs fail, see Min/Max ranges in tables); insufficient memory dimensionality causes failures on more complex tasks (D6, simple palindrome) until memory dimension increased; confusion when input halves reuse same alphabet (simple palindrome) causing interference and trapping in local minima; Gumbel-softmax sometimes produced worse/unstable results compared to plain softmax; softmax-temperature did not consistently improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct visualization of learned memory operations and memory entries (Figures 3 and 4) showing push/pop-like sequences, rotation/pop operations corresponding to open/close parentheses, and presence of distinctive marker values that denote the bottom/empty stack; ablation-style interventions (varying memory dimensionality) showing improved performance when memory capacity increased; performance gap between vanilla RNN/LSTM and MARNNs on Dyck>1 tasks consistent with pushdown emulation claim; quantitative generalization to much longer lengths (train up to 50, test 52–100) where MARNNs succeed and vanilla models fail.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all MARNN configurations succeed uniformly — e.g., some Gumbel-softmax runs fail; Joulin & Mikolov's Stack-RNN (prior variant) performed nearly as well in some settings but not always better; full training stability remains a challenge. Softmax-temperature variations did not reliably improve results. The results show that memory augmentation plus sufficient memory capacity is necessary but not always sufficient (sensitivity to architecture, controller size, random seed).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8387.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8387.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Special-marker memory representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distinct memory-value markers used to represent stack bottom/empty slots</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed phenomenon in Baby-NTM and other MARNNs: the networks learn particular numeric values in memory entries that act as markers to denote the bottom of the used stack region or an empty cell, facilitating correct push/pop predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baby-NTM / Stack-RNN / Stack-LSTM (as observed)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Memory-augmented controllers with one-dimensional or multi-dimensional external memory entries; experiments used visualization-friendly 1D entries in many runs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used within stack-based language recognition tasks (Dyck languages, palindromes) to support stack emulation rather than solving numeric arithmetic per se.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Memory entries converge to certain distinct continuous values (e.g., ~0.45 or ~0.21/0.22 reported in examples) that consistently indicate 'empty' or 'base' positions in the memory stack; these markers allow the controller to know when to start predicting only open-parenthesis symbols or when stack is empty.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Visualization of memory-entry values over time on sample sequences (Figures 3 and 4); inspecting learned numerical values and correlation with push/pop events.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative evidence: models that learned such markers correspond to successful runs that generalize (e.g., Baby-NTM examples in Figures corresponding to high test accuracy); no explicit quantitative metric just for marker learning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>When such markers are not learned or memory capacity is insufficient, models fail to generalize (e.g., D6 with 1-D memory). Marker reliance indicates fragility: if marker values shift due to training variability the controller's decision logic may fail.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Visualization of recurring unique memory values in successful trained models; correlation between marker presence and correct push/pop behavior in memory traces (paper explicitly points out marker values in their visualizations).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Markers are emergent and not guaranteed across runs; some runs do not learn clean markers and result in poor performance; reliance on numeric marker values is sensitive to memory dimensionality and training stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages <em>(Rating: 2)</em></li>
                <li>On the practical computational power of finite precision rnns for language recognition <em>(Rating: 2)</em></li>
                <li>LSTM networks can perform dynamic counting <em>(Rating: 2)</em></li>
                <li>Inferring Algorithmic Patterns with Stack-augmented Recurrent Nets <em>(Rating: 2)</em></li>
                <li>Learning to Transduce with Unbounded Memory <em>(Rating: 2)</em></li>
                <li>Context-Free Transductions with Neural Stacks <em>(Rating: 2)</em></li>
                <li>Learning Operations on a Stack with Neural Turing Machines <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8387",
    "paper_id": "paper-a28ee1bea680c745636d7e09218fffda5d544ffe",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "LSTM counting",
            "name_full": "Long Short-Term Memory counting mechanisms",
            "brief_description": "Mentions and uses of LSTM models developing internal counting-like representations that enable recognition of counting languages (e.g., a^n b^n, a^n b^n c^n and Dyck D1), evidenced by analysis of hidden-state values and successful generalization in prior work; used here as a baseline and discussed in related work.",
            "citation_title": "LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages",
            "mention_or_use": "use",
            "model_name": "LSTM (Long Short-Term Memory)",
            "model_description": "Standard single-layer LSTM recurrent network used in experiments as a baseline (in this paper typically with 8 or 12 hidden units); architecture per Hochreiter & Schmidhuber (1997). No large-scale pretraining or transformer-style training — trained from scratch on synthetic formal-language corpora using Adam for a few epochs.",
            "arithmetic_task_type": "Dynamic counting tasks and counter-like languages: a^n b^n, a^n b^n c^n, well-balanced parenthesis (Dyck D1), and other counting-type synthetic languages.",
            "mechanism_or_representation": "Internal hidden-state activations that implement counters (continuous-valued variables in hidden units or gating/state vectors) that increment/decrement in response to symbols; representations interpreted as numeric counters or digit-like activations rather than explicit digit-by-digit arithmetic.",
            "probing_or_intervention_method": "Analysis/visualization of hidden state values (inspection of activations over time); evaluation of generalization to longer sequences (out-of-distribution lengths); comparison to other RNN variants (vanilla RNN, GRU) as empirical probe.",
            "performance_metrics": "Prior work (cited) shows LSTMs can learn/count languages such as a^n b^n and a^n b^n c^n and generalize far outside training lengths. In this paper LSTM baseline performance: on D2 training mean ≈ 52.38% (Table 2) but test generalization very poor (test mean ≈ 1.39%); on D3 training mean ≈ 32.58% and test mean ≈ 0.04% (Table 3). LSTMs performed much better on simple counting (D1 / earlier literature) than on languages requiring full stack behavior.",
            "error_types_or_failure_modes": "Fails to generalize to stack-based tasks requiring nested matching beyond one-counter capability (Dyck D2+); poor OOD length generalization on nested languages; susceptibility to local minima in training on more complex hierarchical tasks; limited by being a counter-machine rather than a stack machine for Dyck&gt;1.",
            "evidence_for_mechanism": "Cited prior analyses (Gers & Schmidhuber 2001; Weiss et al. 2018; Suzgun et al. 2019b) showing interpretable hidden-state trajectories that behave like counters; in this paper LSTM baselines' hidden-state diagnostics are discussed as motivation and their empirical failure on Dyck&gt;1 supports the claim that LSTMs implement counters but not full stacks.",
            "counterexamples_or_challenges": "Although LSTMs can implement counting/counters (e.g., D1), they fail on Dyck D2 and higher in this paper's experiments (very low test accuracy), demonstrating counting mechanism does not suffice for hierarchical stack requirements. Other architectures (memory-augmented models) outperform LSTMs on these tasks.",
            "uuid": "e8387.0",
            "source_info": {
                "paper_title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "GRU counting limitation",
            "name_full": "Gated Recurrent Unit inability to implement counting mechanisms",
            "brief_description": "Paper references prior work showing GRUs cannot perform the same type of dynamic counting that LSTMs can, and uses this as part of the motivation for exploring memory-augmented architectures.",
            "citation_title": "On the practical computational power of finite precision rnns for language recognition",
            "mention_or_use": "mention",
            "model_name": "GRU (Gated Recurrent Unit)",
            "model_description": "Standard GRU recurrent architecture (Cho et al., 2014) discussed in related work; not the focus of experiments in this paper (cited as showing inferior counting ability).",
            "arithmetic_task_type": "Counting / dynamic counting tasks (e.g., a^n b^n and other counter languages).",
            "mechanism_or_representation": "Argument in cited work: GRU gating/architecture does not permit the same stable, saturating counter-like representations LSTMs can develop; therefore GRUs fail to realize reliable incremental counters in finite precision.",
            "probing_or_intervention_method": "Prior literature analyses and theoretical/empirical comparisons (cited). This paper does not perform new probing on GRUs but cites these results as background.",
            "performance_metrics": "No new GRU experiments in this paper; cited prior results indicate GRU failure on counter tasks while LSTMs succeed.",
            "error_types_or_failure_modes": "Unable to form sustained numeric counter-like activations needed for dynamic counting; fails to generalize on counting languages according to cited studies.",
            "evidence_for_mechanism": "Referenced theoretical and empirical analyses (Weiss et al., cited) concluding GRUs lack mechanisms to implement the required counters.",
            "counterexamples_or_challenges": "Not evaluated experimentally here; included to contextualize why memory augmentation is necessary for richer hierarchical languages.",
            "uuid": "e8387.1",
            "source_info": {
                "paper_title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "MARNN pushdown emulation",
            "name_full": "Memory-Augmented Recurrent Neural Networks emulating pushdown automata (Stack-RNN, Stack-LSTM, Baby-NTM)",
            "brief_description": "This paper's introduced memory-augmented RNNs (Stack-RNN, Stack-LSTM, Baby-NTM) implement differentiable stack/memory mechanisms (push/pop, rotates) and empirically learn to emulate pushdown automata to solve stack-requiring tasks (Dyck D2,D3,D6, palindrome, reversal); internal memory entries show stack-like behavior and special markers indicating stack bottom.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Stack-RNN, Stack-LSTM, Baby-NTM (memory-augmented RNNs)",
            "model_description": "Three architectures introduced/used in this paper: (1) Stack-RNN — RNN with differentiable unbounded stack where top element s^{(0)} influences hidden state and push/pop are soft (softmax or Gumbel-softmax controls); (2) Stack-LSTM — same with LSTM controller; (3) Baby-NTM — fixed-size external memory with deterministic read/write semantics and five operations (ROTATE-RIGHT, ROTATE-LEFT, NO-OP, POP-LEFT, POP-RIGHT) combined by soft weights. Experiments typically used single-layer controllers with 8 (or 12) hidden units and one-dimensional memory entries (higher-dim experiments also performed), trained with Adam for 3 epochs on synthetic datasets.",
            "arithmetic_task_type": "Stack-based counting / hierarchical matching tasks: generalized Dyck languages (D2, D3, D6), deterministic homomorphic palindrome (w # φ(w^R)), simple palindrome (w # w^R), and string-reversal transduction (w #^{|w|} -&gt; #^{|w|} w^R). These tasks require stack-like push/pop behavior rather than simple numeric addition.",
            "mechanism_or_representation": "Differentiable stack/memory entries act as stored symbols (one-dimensional values) with weighted linear interpolation to implement push/pop in superposition; controllers learn to emit weights (via softmax/Gumbel-softmax with temperature) that correspond to push/pop actions; memory contents include distinct marker values used to represent stack-bottom or empty slots; the hidden-state acts as finite-state control while memory implements stack storage (so the overall mechanism is a learned PDA emulation).",
            "probing_or_intervention_method": "Probing by visualization of memory-operation strengths and memory-entry values across time (Figures 3 and 4), varying softmax temperature and employing Gumbel-softmax sampling, varying memory dimensionality (1D to 5D), and measuring generalization to longer test sequences (training lengths 2–50, test 52–100). Evaluation is via sequence-prediction MSE and thresholded k-hot outputs; interventions include changing number of memory dimensions and softmax temperature/gumbel sampling.",
            "performance_metrics": "Substantial empirical results in paper: On D2 (memory dim=1, 8 hidden units) many MARNNs achieved near-perfect generalization: Stack-RNN+Softmax training mean 100% and test mean 99.99% (Table 2); Stack-RNN+Softmax-Temp similar. Vanilla RNN test mean 0%; Vanilla LSTM test mean 1.39%. On D3 (same small memory) some MARNNs achieved &gt;99% in subset of runs but overall variability (Table 3); increasing memory dimensionality improved performance. On D6 with larger memory (12 hidden units, 5-dim memory) MARNNs achieved high performance: Stack-RNN+Softmax test mean 99.85%, Baby-NTM+Softmax test mean 99.87% (Table 4). Palindrome/homomorphic palindrome: many MARNNs achieved high test accuracy (Table 5); simple palindrome with 1D memory had failures but succeeded when memory dim increased to 5. String reversal: many MARNNs obtained perfect test accuracy in a substantial fraction of runs (Table 6; e.g., Baby-NTM+Gumbel-Softmax training mean ≈ 90.01%, test med 100%).",
            "error_types_or_failure_modes": "Training instability and variability across random restarts (some runs fail, see Min/Max ranges in tables); insufficient memory dimensionality causes failures on more complex tasks (D6, simple palindrome) until memory dimension increased; confusion when input halves reuse same alphabet (simple palindrome) causing interference and trapping in local minima; Gumbel-softmax sometimes produced worse/unstable results compared to plain softmax; softmax-temperature did not consistently improve robustness.",
            "evidence_for_mechanism": "Direct visualization of learned memory operations and memory entries (Figures 3 and 4) showing push/pop-like sequences, rotation/pop operations corresponding to open/close parentheses, and presence of distinctive marker values that denote the bottom/empty stack; ablation-style interventions (varying memory dimensionality) showing improved performance when memory capacity increased; performance gap between vanilla RNN/LSTM and MARNNs on Dyck&gt;1 tasks consistent with pushdown emulation claim; quantitative generalization to much longer lengths (train up to 50, test 52–100) where MARNNs succeed and vanilla models fail.",
            "counterexamples_or_challenges": "Not all MARNN configurations succeed uniformly — e.g., some Gumbel-softmax runs fail; Joulin & Mikolov's Stack-RNN (prior variant) performed nearly as well in some settings but not always better; full training stability remains a challenge. Softmax-temperature variations did not reliably improve results. The results show that memory augmentation plus sufficient memory capacity is necessary but not always sufficient (sensitivity to architecture, controller size, random seed).",
            "uuid": "e8387.2",
            "source_info": {
                "paper_title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages",
                "publication_date_yy_mm": "2019-11"
            }
        },
        {
            "name_short": "Special-marker memory representation",
            "name_full": "Distinct memory-value markers used to represent stack bottom/empty slots",
            "brief_description": "Observed phenomenon in Baby-NTM and other MARNNs: the networks learn particular numeric values in memory entries that act as markers to denote the bottom of the used stack region or an empty cell, facilitating correct push/pop predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Baby-NTM / Stack-RNN / Stack-LSTM (as observed)",
            "model_description": "Memory-augmented controllers with one-dimensional or multi-dimensional external memory entries; experiments used visualization-friendly 1D entries in many runs.",
            "arithmetic_task_type": "Used within stack-based language recognition tasks (Dyck languages, palindromes) to support stack emulation rather than solving numeric arithmetic per se.",
            "mechanism_or_representation": "Memory entries converge to certain distinct continuous values (e.g., ~0.45 or ~0.21/0.22 reported in examples) that consistently indicate 'empty' or 'base' positions in the memory stack; these markers allow the controller to know when to start predicting only open-parenthesis symbols or when stack is empty.",
            "probing_or_intervention_method": "Visualization of memory-entry values over time on sample sequences (Figures 3 and 4); inspecting learned numerical values and correlation with push/pop events.",
            "performance_metrics": "Qualitative evidence: models that learned such markers correspond to successful runs that generalize (e.g., Baby-NTM examples in Figures corresponding to high test accuracy); no explicit quantitative metric just for marker learning reported.",
            "error_types_or_failure_modes": "When such markers are not learned or memory capacity is insufficient, models fail to generalize (e.g., D6 with 1-D memory). Marker reliance indicates fragility: if marker values shift due to training variability the controller's decision logic may fail.",
            "evidence_for_mechanism": "Visualization of recurring unique memory values in successful trained models; correlation between marker presence and correct push/pop behavior in memory traces (paper explicitly points out marker values in their visualizations).",
            "counterexamples_or_challenges": "Markers are emergent and not guaranteed across runs; some runs do not learn clean markers and result in poor performance; reliance on numeric marker values is sensitive to memory dimensionality and training stability.",
            "uuid": "e8387.3",
            "source_info": {
                "paper_title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages",
                "publication_date_yy_mm": "2019-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages",
            "rating": 2
        },
        {
            "paper_title": "On the practical computational power of finite precision rnns for language recognition",
            "rating": 2
        },
        {
            "paper_title": "LSTM networks can perform dynamic counting",
            "rating": 2
        },
        {
            "paper_title": "Inferring Algorithmic Patterns with Stack-augmented Recurrent Nets",
            "rating": 2
        },
        {
            "paper_title": "Learning to Transduce with Unbounded Memory",
            "rating": 2
        },
        {
            "paper_title": "Context-Free Transductions with Neural Stacks",
            "rating": 2
        },
        {
            "paper_title": "Learning Operations on a Stack with Neural Turing Machines",
            "rating": 1
        }
    ],
    "cost": 0.015872499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages</h1>
<p>Mirac Suzgun ${ }^{1}$ Sebastian Gehrmann ${ }^{1}$ Yonatan Belinkov ${ }^{12}$ Stuart M. Shieber ${ }^{1}$<br>${ }^{1}$ Harvard John A. Paulson School of Engineering and Applied Sciences<br>${ }^{2}$ MIT Computer Science and Artificial Intelligence Laboratory<br>Cambridge, MA, USA<br>{msuzgun@college, {gehrmann, belinkov, shieber}@seas}.harvard.edu</p>
<h4>Abstract</h4>
<p>We introduce three memory-augmented Recurrent Neural Networks (MARNNs) and explore their capabilities on a series of simple language modeling tasks whose solutions require stack-based mechanisms. We provide the first demonstration of neural networks recognizing the generalized Dyck languages, which express the core of what it means to be a language with hierarchical structure. Our memory-augmented architectures are easy to train in an end-to-end fashion and can learn the Dyck languages over as many as six parenthesis-pairs, in addition to two deterministic palindrome languages and the string-reversal transduction task, by emulating pushdown automata. Our experiments highlight the increased modeling capacity of memory-augmented models over simple RNNs, while inflecting our understanding of the limitations of these models.</p>
<h2>1 Introduction</h2>
<p>Recurrent Neural Networks (RNNs) have proven to be an effective and powerful model choice for capturing long-distance dependencies and complex representations in sequential tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), and speech recognition (Graves et al., 2013). In theory, RNNs with rational state weights and infinite numeric precision are known to be computationally universal models (Siegelmann and Sontag, 1994, 1995). Yet, in practice, the computational power of RNNs with finite numeric precision is still unknown. Hence, the classes of languages that can be learned, empirically or theoretically, by RNNs with finite numeric precision are still to be discovered.</p>
<p>A natural question arises, then, as to what extent RNN models can learn languages that exem-
plify important formal properties found in natural languages, such properties as long-distance dependencies, counting, hierarchy, and repetition.</p>
<p>Along these lines, Gers and Schmidhuber (2001); Weiss et al. (2018); Suzgun et al. (2019a) have demonstrated that Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber (1997)), a popular variant of RNNs, can develop counting mechanisms to recognize simple strictly contextfree and context-sensitive languages, such as $a^{n} b^{n}$ and $a^{n} b^{n} c^{n}$, as evidenced by analysis of the hidden state values. ${ }^{1}$ By contrast, Weiss et al. have shown that Gated Recurrent Units (GRUs; Cho et al. (2014)), another popular variant of RNNs, cannot perform this type of counting and provided an explanation for some of the difference in performance between LSTMs and GRUs.</p>
<p>Merrill (2019) studied the theoretical expressiveness of various real-time neural networks with finite precision under asymptotic conditions, showing that RNNs and GRUs can capture regular languages whereas LSTMs can further recognize a subset of real-time counter languages. And empirically, Suzgun et al. (2019b) demonstrated that LSTM networks can learn to perform dynamic counting, as exemplified by the wellbalanced parenthesis (Dyck) language $\mathcal{D}<em 1="1">{1}$ as well as the shuffles of multiple $\mathcal{D}</em>$ languages.</p>
<p>Counting in real-time is an important property, differentiating some of the language classes in the Chomsky hierarchy, and echoes of it appear in natural languages as well, for instance, in the requirement that the number of arguments of a set of verbs match their subcategorization requirements. But counting does not exhaust the kinds of structural properties that may be apposite for natural language. Chomsky (1957) emphasizes the hierarchical structure found in natural languages, for instance, in the nested matching of "both ... and"</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and "either ...or". Indeed, this kind of nestedmatching phenomenon forms the essence of the strictly context-free languages (CFLs). Here simple (real-time) counters are not sufficient; a stack is required. The formal-language-theory reflex of this phenomenon is found most sparely in the Dyck languages $\mathcal{D}<em _1="&gt;1">{n}$ of well-nested strings over $n$ pairs of brackets, where $n&gt;1$. (We refer to these as the $\mathcal{D}</em>$ languages.)</p>
<p>The centrality of this nested stack structure in characterizing the class of context-free languages can be seen in various ways. (i) The automata-theoretic analog of context-free grammars, the pushdown automaton, is defined by its use of a stack (Chomsky, 1962). (ii) Chomsky and Schützenberger (1963) famously showed that all context-free languages are homomorphic images of regular-intersected $\mathcal{D}<em _1="&gt;1">{n}$ languages. ${ }^{2}$ (iii) The hardest CFL of Greibach (1973) and the hardest deterministic CFL of Sudborough (1976) are built using Dyck-language-style matching. For these reasons, we think of the $\mathcal{D}</em>$ languages and other languages requiring a stack, if we want these neural architectures to be able to manifest hierarchical structures.}$ languages as expressing the core of what it means to be a contextfree language with hierarchical structure, even if it is not itself a universal CFL. This property of the Dyck languages accounts for the heavy focus on the them in prior work (Deleu and Dureau, 2016; Bernardy, 2018; Sennhauser and Berwick, 2018; Skachkova et al., 2018; Hao et al., 2018; Zaremba et al., 2016; Suzgun et al., 2019a; Yu et al., 2019; Hahn, 2019) as well as in this work. It would thus be notable for finite precision neural networks to learn languages, like the $\mathcal{D}_{&gt;1</p>
<p>In this paper, we introduce three enhanced RNN models that consist of recurrent layers and external memory structures, namely stack-augmented RNNs (Stack-RNNs), stack-augmented LSTMs (Stack-LSTMs), and Baby Neural Turing Machines (Baby-NTMs), and show that they can effectively learn to recognize some $\mathcal{D}<em 2="2">{&gt;1}$ languages from limited data by emulating deterministic pushdown automata. Previous studies used simple RNN models (Bernardy, 2018; Sennhauser and Berwick, 2018; Suzgun et al., 2019b; Yu et al., 2019) and memory-augmented architectures (Hao et al., 2018) to attempt to learn $\mathcal{D}</em>$ under different</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>training platforms; however, none of them were able to obtain good performance on this task. We thus present the first demonstration that a memoryaugmented neural network (MARNN) can learn $\mathcal{D}<em 2="2">{&gt;1}$ languages. Moreover, we evaluate the learning capabilities of our architectures on six tasks whose solutions require the employment of stackbased approaches, namely learning the $\mathcal{D}</em>}, \mathcal{D<em 6="6">{3}$ and $\mathcal{D}</em>$}$ languages, recognizing the deterministic palindrome language ( $w # w^{R}$ ) and the deterministic homomorphic palindrome language ( $w # \varphi\left(w^{R}\right)$ ), and performing the string-reversal transduction task ( $w #^{|w|} \Rightarrow #^{|w|} w^{R}$ ). Our results reflect the better modeling capacity of our MARNNs over the standard RNN and LSTM models in capturing hierarchical representations, in addition to providing an insightful glimpse of the promise of these models for real-world natural-language processing tasks. ${ }^{3</p>
<h2>2 Related Work</h2>
<h3>2.1 Learning Formal Languages Using Neural Networks</h3>
<p>Using neural network architectures to recognize formal languages has been a central computational task in gaining an understanding of their expressive ability for application to natural-languageprocessing tasks. Elman (1991) marked the beginning of such methodological investigations and devised an artificial language learning platform where Simple Recurrent Networks (SRNs) (Elman, 1990), were trained to learn the hierarchical and recursive relationships between clauses. An analysis of the hidden state dynamics revealed that the models learned internal representations that encoded information about the grammatical structure and dependencies of the synthetic language. Later, Das et al. (1992) introduced the first RNN model with an external stack, the Recurrent Neural Network Pushdown Automaton (NNPDA), to learn simple deterministic context-free grammars.</p>
<p>Following Elman's work, many studies used SRNs (Steijvers, 1996; Tonkes and Wiles, 1997; Hölldobler et al., 1997; Rodriguez and Wiles, 1998; Bodén et al., 1999; Bodén and Wiles, 2000; Rodriguez, 2001) and stack-based RNNs (Das et al., 1993; Zeng et al., 1994) to recognize simple context-free and context-sensitive counter languages, including $a^{n} b^{n}, a^{n} b^{n} c b^{m} a^{m}$,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>$a^{n} b^{m} B^{m} A^{n}, a^{n+m} b^{n} c^{m}, a^{n} b^{n} c^{n},\left(b a^{n}\right)^{m}$, and $\mathcal{D}_{1}$. Nonetheless, none of the SRN-based models were able to generalize far beyond their training set. Some of these studies also focused on understanding and visualizing the internal representations learned by the hidden units of the networks, as well as the computational capabilities and limitations of the models.</p>
<p>In contrast, Gers and Schmidhuber (2001), Schmidhuber et al. (2002), and Gers et al. (2002), showed that their (LSTM) networks could not only competently learn two strictly context-free languages, $a^{n} b^{n}$ and $a^{n} b^{m} B^{m} A^{n}$, and one strictly context-sensitive language $a^{n} b^{n} c^{n}$, but also generalize far beyond the training datasets.</p>
<h3>2.2 Memory-Augmented Neural Networks</h3>
<p>Recently, memory-augmented architectures have been considered for language modeling tasks: Joulin and Mikolov (2015) proposed a differentiable stack structure controlled by an RNN to infer algorithmic patterns that require some combination of counting and memorization. Though their model could learn $a^{n} b^{n}, a^{n} b^{n} c^{n}, a^{n} b^{n} c^{n} d^{n}$, $a^{n} b^{2 n}, a^{n} b^{m} c^{n+m}$, it did not exceed the performance of a standard LSTM on a language modeling task. Inspired by the early architecture design of NNPDA, Grefenstette et al. (2015) introduced LSTM models equipped with unbounded differentiable memory structures, such as stacks, queues, and double-linked lists, and explored their computational power on synthetic transduction tasks. In their experiments, their Neural-Stack and Neural-Queue architectures outperformed the standard LSTM architectures. Neither of these studies on stack-augmented neural architectures inspected the internal representations learned by the recurrent hidden layers or investigated the performance of their models on the Dyck language.</p>
<p>Graves et al. (2014) introduced the Neural Turing Machine (NTM), which consists of a neural network (which can be either feed-forward or recurrent) together with a differentiable external memory, and demonstrated its successful performance on a series of simple algorithmic tasks, such as copying, repeated copying, and sorting. At each time step, an NTM can interact with the external memory via its differentiable attention mechanisms and determine its output using the information from the current hidden state together with the filtered context from the external memory. It is evident from the design differences that the
degree of freedom of NTMs is much greater than that of stack-augmented recurrent networks. However, this freedom comes at a price: The different ways in which we can attend to the memory to read and write at each time step make the training of the neural models challenging. Since the publication of the original NTM paper, there have been a number of studies addressing instability issues of the NTM architecture, or more broadly memoryaugmented recurrent network models, and proposing new architecture designs. We refer to Zaremba and Sutskever (2015); Kurach et al. (2015); Yang (2016); Graves et al. (2016); Gulcehre et al. (2018) for such proposals.</p>
<h3>2.3 Investigations of the Dyck Languages</h3>
<p>Deleu and Dureau (2016) used NTMs to capture long-distance dependencies in $\mathcal{D}<em 1="1">{1}$. Their examination showed that NTMs indeed learn to emulate stack representations and generalize to longer sequences. However, a model need not be equipped with a stack to recognize this simplest Dyck language in a standard learning environment; counting is sufficient for an automaton to capture $\mathcal{D}</em>$ (Suzgun et al., 2019b; Yu et al., 2019).</p>
<p>In assessing the ability of recurrent neural networks to process deep and long-distance dependencies, Skachkova et al. (2018) and Sennhauser and Berwick (2018) conducted experiments on the Dyck languages to see whether LSTMs could learn nested structures. The former sought to predict the single correct closing parenthesis, given a Dyck word without its final closing symbol. Although LSTMs performed almost perfectly in this completion task, one cannot draw any definitive conclusion about whether these models really learn the Dyck languages, since even counter automata can achieve perfect accuracy on this task. ${ }^{4}$</p>
<p>Similarly, Bernardy (2018) used three different recurrent networks, namely LSTM, GRU, and RUSS, and combinations thereof, to predict the next possible parenthesis at each time step, assuming that it is a closing parenthesis. His RUSS model is a purpose-designed model containing recurrent units with stack-like states and appears to generalize well to deeper and longer sequences.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>However, as the author mentions, the specificity of the RUSS architecture disqualifies it as a practical model choice for real-world language modeling tasks.</p>
<p>Hao et al. (2018) studied the interpretability of Neural Stack models (Grefenstette et al., 2015) in a number of simple language modeling tasks, including parenthesis prediction, string reversal, and XOR evaluation. Though their Neural Stacks exhibited intuitive stack behaviors on their contextfree transduction tasks and performed almost as well as the standard LSTM models, the authors noted that their stack-augmented models were more difficult to train than the traditional LSTMs.</p>
<p>More recently, Suzgun et al. (2019b) corroborated the theoretical findings of Weiss et al. (2018) by showing that RNN, GRU, and LSTM models could perform dynamic counting by recognizing $\mathcal{D}<em 1="1">{1}$ as well as shuffles of multiple $\mathcal{D}</em>$.}$ languages by emulating simple $k$-counter machines, while being incapable of recognizing $\mathcal{D}_{2</p>
<p>Adopting the experimental framework of Sennhauser and Berwick (2018) and the data generation procedure of Skachkova et al. (2018), Yu et al. (2019) conducted experiments on $\mathcal{D}<em 2="2">{2}$ under different training schemes and objectives using relatively large bi-directional LSTM models. Their recurrent networks failed to generalize well beyond the scope of their training data to learn $\mathcal{D}</em>$}$ under the closing-parenthesis completion and sequence-to-sequence settings. ${ }^{5</p>
<p>Finally, Hahn (2019) used $\mathcal{D}<em 2="2">{2}$ to explore the theoretical limitations of self-attention architectures (Vaswani et al., 2017). He demonstrated that selfattention models, even when equipped with infinite precision, cannot capture $\mathcal{D}</em>$, unless the number of layers or attention heads increases with the length of the input sequence.</p>
<p>In summary, recognizing the Dyck languages has been an important probing task for understanding the ability of neural networks to capture hierarchical information. Thus far, none of the recurrent neural networks have been shown to capture $\mathcal{D}<em _1="&gt;1">{&gt;1}$. This present work, therefore, provides the first demonstration of RNN-based models learning $\mathcal{D}</em>}$, in particular, $\mathcal{D<em 3="3">{2}, \mathcal{D}</em>$, in addition to other difficult context-free languages.}$, and $\mathcal{D}_{6</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>3 Models</h2>
<p>In this section, we describe the mathematical formulations of our memory-augmented RNNs.The inspiration for our stack-augmented neural architectures came from the pushdown automaton, an abstract machine capable of recognizing contextfree languages. Similar stack-based neural networks, however, have also been proposed by others (Pollack, 1991; Das et al., 1992; Joulin and Mikolov, 2015; Grefenstette et al., 2015). Our models differ from them in their theoretical simplicity and empirical success. Our Baby-NTM, on the other hand, can be considered as a simplification of the original NTM architecture (Graves et al., 2014): As opposed to using soft-attention mechanisms to read and write to the external memory, we make deterministic decisions and always read content from and write to the first entry of the memory, thereby making the learning process easier while retaining universal expressivity.</p>
<p>Notation We will assume the following notation:</p>
<ul>
<li>$x=x_{1}, \ldots, x_{T}$ : The input sequence of onehot vectors, with the $i$-th token $x_{i}$.</li>
<li>$y_{i}$ : The output associated with $x_{i}$.</li>
<li>$\mathbf{W}$ : The learnable weights of the model.</li>
<li>b: The learnable bias terms of the model.</li>
<li>$\mathbf{h}_{i}$ : The $i$-th hidden state representation.</li>
<li>$D$ : The dim. of the input and output samples.</li>
<li>$H$ : The dim. of the hidden state of the model.</li>
<li>$M$ : The dim. of the external stack/memory.</li>
</ul>
<h3>3.1 Stack-RNN</h3>
<p>Before we begin describing our Stack-RNN, recall the formulation of a standard RNN:</p>
<p>$$
\begin{aligned}
\mathbf{h}<em h="h" i="i">{t} &amp; =\tanh \left(\mathbf{W}</em>} x_{t}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>} \mathbf{h<em h="h">{(t-1)}+\mathbf{b}</em>\right) \
y_{t} &amp; =f\left(\mathbf{W}<em t="t">{y} \mathbf{h}</em>\right)
\end{aligned}
$$</p>
<p>where $x_{t} \in \mathbb{R}^{D}$ is the input, $\mathbf{h}<em t="t">{t} \in \mathbb{R}^{H}$ the hidden state, $y</em>$ the linear output layer, and $f$ a transformation.} \in \mathbb{R}^{D}$ the output at time $t, \mathbf{W}_{y} \in \mathbb{R}^{D \times H</p>
<p>While designing our Stack-RNN, we come across two important questions: (i) Where and how should we place the stack in the neural network, and (ii) how should we design the stack so that we can backpropagate errors through the stack at the time of training? Regarding (i), we place the stack in such a way that it interacts with the hidden layers at each time step. The benefit of this</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An abstract representation of our Stack-RNN architecture.</p>
<p>approach is that errors made in future stages of the model affect and backpropagate through not only the hidden states but also the stack states. Regarding (ii), we construct a <em>differentiable</em> stack structure. Figure 1 provides a visualization of the Stack-RNN. Its formulation is:</p>
<p>$$
\begin{split}
\tilde{\mathbf{h}}<em _t-1_="(t-1)">{(t-1)} &amp;= \mathbf{h}</em>} + \mathbf{W<em _t-1_="(t-1)">{sh}s</em> \
\mathbf{h}}^{(0)<em ih="ih">t &amp;= \tanh(\mathbf{W}</em>}x_t + \mathbf{b<em hh="hh">{ih} + \mathbf{W}</em>}\tilde{\mathbf{h}<em hh="hh">{(t-1)} + \mathbf{b}</em>) \
y_t &amp;= \sigma(\mathbf{W}<em a="a">{y}\mathbf{h}_t) \
a_t &amp;= \text{softmax}(\mathbf{W}</em>}\mathbf{h<em _t-1_="(t-1)">t) \
n_t &amp;= \sigma(\mathbf{W}_n\mathbf{h}_t) \
s_t^{(0)} &amp;= a_t^{(0)}n_t + a_t^{(1)}s</em> \
s_t^{(i)} &amp;= a_t^{(0)}s_{(t-1)}^{(i-1)} + a_t^{(1)}s_{(t-1)}^{(i+1)}
\end{split}
$$}^{(1)</p>
<p>where $s_t = s_t^{(0)}s_t^{(1)} \dots s_t^{(k)}$ is the stack configuration at time step $t$, with $s_t^{(0)}$ the topmost stack element; $\mathbf{W}<em y="y">{sh} \in \mathbb{R}^{H \times M}$, $\mathbf{W}</em>$ are all learnable linear weights of the model.} \in \mathbb{R}^{D \times H}$, $\mathbf{W}_a \in \mathbb{R}^{2 \times H}$, and $\mathbf{W}_n \in \mathbb{R}^{M \times H</p>
<p>At each time step, we combine the topmost stack element $s_{(t-1)}^{(0)}$ with the previous hidden state $\mathbf{h}<em _t-1_="(t-1)">{(t-1)}$ via a linear mapping to produce an intermediate hidden state $\tilde{\mathbf{h}}</em>^2$ is a probability distribution over the two operations. Finally, we update the stack elements in such a way that the elements become the weighted linear interpolation of both possible stack operations. We can, therefore, consider the elements in the stack as variables in }$. We then use $\tilde{\mathbf{h}}_{(t-1)}$, together with the input, to generate the current hidden state $\mathbf{h}_t$, from which both the output at that time step $y_t$ and the weights of the PUSH ($a_t^{(0)}$) and POP ($a_t^{(1)}$) operations by the stack controller are determined simultaneously. Here $a_t \in \mathbb{R<em>superposition</em> states.</p>
<p>We highlight the following differences between our Stack-RNN and the Stack-RNN by Joulin and Mikolov (2015), as further explicated in the appendix. First, their model does not contain the term $\tilde{\mathbf{h}}_{(t-1)}$ and it updates $\mathbf{h}_t$ as follows:</p>
<p>$$
\mathbf{h}<em ih="ih">t = \sigma(\mathbf{W}</em>}x_t + \mathbf{W<em _t-1_="(t-1)">{hh}\mathbf{h}</em>} + \mathbf{W<em _t-1_="(t-1)">{sh}s</em>)
$$}^{(0:k)</p>
<p>where $\mathbf{W}<em _t-1_="(t-1)">{sh} \in \mathbb{R}^{H \times k}$ and $s</em>}^{(0:k)}$ the $k$-topmost elements of the stack at time $t - 1$. But a simple analysis of our Stack-RNN formulation divulges that $s_{(t-1)}^{(0)}$ depends on both $\mathbf{W<em sh="sh">{hh}$ and $\mathbf{W}</em>}$ in our formulation, whereas it only depends on $\mathbf{W<em _t-1_="(t-1)">{sh}$ in Joulin and Mikolov's formulation. Furthermore, their architecture takes the sigmoid of the linear combination of $x_t$, $\mathbf{h}</em>_t$.}$, and $s_{(t-1)}^{(0:k)}$, in addition to excluding the bias terms, to update $\mathbf{h</p>
<h3>3.2 Stack-LSTM</h3>
<p>The Stack-LSTM is similar to the Stack-RNN but contains additional components of the standard LSTM architecture by Hochreiter and Schmidhuber (1997). In this model, we update the hidden state of the model according to the standard LSTM equations, that is $\mathbf{h}<em _t-1_="(t-1)">t = \text{LSTM}(x_t, \tilde{\mathbf{h}}</em>)$.</p>
<h3>3.3 Baby-NTM</h3>
<p>The Baby-NTM is both an extension of the Stack-RNN and a simplification of the original NTM. While the Stack-RNN contains an unbounded stack mechanism, it can perform only two basic operations on the stack, namely the PUSH and POP actions. In the Baby-NTM architecture, we fix the size of the external memory but provide more freedom to the model: While the interaction between the controller and the memory in the design of the Baby-NTM is mostly similar</p>
<p>to that of the Stack-RNN, we allow five operations on the memory at each time step to update its contents: ROTATE-RIGHT, ROTATE-LEFT, NO-OP, POP-LEFT, and POP-RIGHT. Suppose that the current memory configuration $\mathbf{M}$ is $[a, b, c, d, e]$, where $\mathbf{M}^{(i)} \in \mathbb{R}$. Then the operations produce the following configurations at the next time step:</p>
<p>$$
\begin{aligned}
\text { ROTATE-RIGHT } &amp; :[e, a, b, c, d] \
\text { ROTATE-LEFT } &amp; :[b, c, d, e, a] \
\text { NO-OP } &amp; :[a, b, c, d, e] \
\text { POP-RIGHT } &amp; :[0, a, b, c, d] \
\text { POP-LEFT } &amp; :[b, c, d, e, 0]
\end{aligned}
$$</p>
<p>If we think of the memory as a set $\mathbf{M}$ sitting on an $n$-dimensional Euclidean space $\mathbb{R}^{n}$, we can then think of these operations as $n \times n$ matrices. From an algebraic point of view, we can realize these actions on the memory as left-monoid actions on a set, since matrix multiplication is associative and the matrix corresponding to the operation NO-OP serves the role of an identity element in our computations. Below is the formulation of the BabyNTM architecture:</p>
<p>$$
\begin{aligned}
\tilde{\mathbf{h}}<em _t-1_="(t-1)">{(t-1)} &amp; =\mathbf{h}</em>}+\mathbf{W<em _t-1_="(t-1)">{m} \mathbf{M}</em> \
\mathbf{h}}^{(0)<em h="h" i="i">{t} &amp; =\tanh \left(\mathbf{W}</em>} x_{t}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>} \tilde{\mathbf{h}<em h="h">{(t-1)}+\mathbf{b}</em>\right) \
y_{t} &amp; =\sigma\left(\mathbf{W}<em t="t">{y} \mathbf{h}</em>\right) \
a_{t} &amp; =\operatorname{softmax}\left(\mathbf{W}<em t="t">{a} \mathbf{h}</em>\right) \
n_{t} &amp; =\sigma\left(\mathbf{W}<em t="t">{n} \mathbf{h}</em>\right) \
\mathbf{M}<em i="1">{t} &amp; =\sum</em>}^{N} a_{t}^{(i)} \cdot\left[\mathbf{O P}^{(i)}\right] \mathbf{M<em t="t">{t-1} \
\mathbf{M}</em>}^{(0)} &amp; =\mathbf{M<em t="t">{t}^{(0)}+n</em>
\end{aligned}
$$</p>
<p>where $\mathbf{M}<em t="t">{t}$ denotes the memory configuration at time step $t, n</em>$}$ the value of the element to be inserted to the first entry of the memory at time step $t, \mathbf{O P}^{(i)}$ the matrix corresponding to the $i$-th action on the memory and $a_{t}^{(i)}$ the weight of that action at time $t$, and all $\mathbf{W}$ 's learnable matrices of the model. ${ }^{6</p>
<h3>3.4 Softmax Functions</h3>
<p>The softmax function in the calculation of $a_{t}$ in all these models enables us to map the values of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the vector $\mathbf{W}<em t="t">{a} \mathbf{h}</em>$ to a categorical probability distribution. We investigate the effect of more deterministic decisions about the stack/memory operations on the robustness of our model. A natural approach is to employ a softmax function with varying temperature $\tau$ :</p>
<p>$$
\text { softmax-temp }\left(x_{i}, \tau\right)=\frac{\exp \left(x_{i} / \tau\right)}{\sum_{j=1}^{N} \exp \left(x_{j} / \tau\right)}
$$</p>
<p>The softmax-temp function behaves exactly like the standard softmax function when the temperature value $\tau$ equals 1 . As the temperature increases, softmax-temp produces more uniform categorical class probabilities, whereas as the temperature decreases, the function outputs more discrete probabilities, like a one-hot encoding.</p>
<p>Furthermore, Jang et al. (2016) proposed an efficient and differentiable approximation to sampling from a discrete categorical distribution using a reparameterization trick:</p>
<p>$$
\begin{aligned}
&amp; \text { Gumbel-softmax-temp }\left(x_{i},\left{g_{1}, \ldots, g_{N}\right}, \tau\right) \
&amp; \quad=\frac{\exp \left(\left(\log x_{i}+g_{i}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\left(\log x_{j}+g_{j}\right) / \tau\right)}
\end{aligned}
$$</p>
<p>where $g_{i} \stackrel{\text { i.i.d. }}{=} \operatorname{Gumbel}(0,1)$. As an alternative to the softmax function with varying temperature, one might want to use the Gumbel-softmax sampling method. In cases where we have more than two operations on the stack/memory, it might be tempting to prefer the Gumbel-softmax sampling approach for the calculation of $a_{t}$ values. We experiment with these alternatives below.</p>
<h2>4 Experimental Setup</h2>
<p>To evaluate the performance of the MARNNs, we conducted experiments on six computational tasks whose solutions require the formation of stack structures. In all the experiments, we used both standard and memory-augmented RNNs to explore the differences in their performances, in addition to the Stack-RNN model by Joulin and Mikolov (2015), and repeated each experiment 10 times. Furthermore, we aimed to investigate softmax functions with varying temperature in our MARNNs and thus employed 12 models with different configurations-two vanilla recurrent models, three MARNNs with three different softmax functions, and one Stack-RNN by Joulin and Mikolov (2015)-for the six tasks.</p>
<table>
<thead>
<tr>
<th>Sample</th>
<th>( [ ] )</th>
<th>$a b c # z y x$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input</td>
<td>( [ ] )</td>
<td>$a$</td>
</tr>
<tr>
<td>Output</td>
<td>(/[/) (/[/] (/[/) (/[</td>
<td>$a / b / c / #$</td>
</tr>
</tbody>
</table>
<p>Table 1: Example input-output pairs for $\mathcal{D}_{2}$ (left) and the deterministic homomorphic palindrome language (right) under the sequence prediction paradigm.</p>
<h3>4.1 The Sequence Prediction Task</h3>
<p>Following Gers and Schmidhuber (2001), we trained the networks as follows: At each time step, we presented one input symbol to the network and then asked the model to predict the set of next possible symbols in the language, based on the current symbol, the prior hidden states, and the stack. We used a one-hot representation to encode the inputs and a $k$-hot representation to encode the outputs. Table 1 provides example input-output pairs for two of the experiments.</p>
<p>In all the experiments, the objective was to minimize the mean-squared error of the sequence predictions. We used an output threshold criterion of 0.5 for the sigmoid layer $\left(y_{t}=\sigma(\cdot)\right)$ to indicate which symbols were predicted by the model. Finally, we turned this sequence prediction task into a sequence classification task by accepting a sequence if the model predicted all of its output values correctly and rejecting it otherwise.</p>
<h3>4.2 Training Details</h3>
<p>In contrast to the models in Joulin and Mikolov (2015); Grefenstette et al. (2015); Hao et al. (2018); Yu et al. (2019), our architectures are economical: Unless otherwise stated, the models are all single-layer networks with 8 hidden units. In all the experiments, the entries of the memory were set to be one-dimensional, while the size of the memory in the Baby-NTMs was fixed to 104 (since the length of the longest sequence in all the tasks was 100). We used the Adam optimizer (Kingma and Ba, 2014) and trained our models for three epochs.</p>
<h2>5 Learning the Dyck Languages</h2>
<p>As described in the introduction, the Dyck languages $\mathcal{D}<em 1="1">{&gt;1}$ provide an ideal test-bed for exploring the ability of recurrent neural networks to capture the core properties of the context-free languages, their hierarchical modeling ability. None of the previous studies were able to learn the Dyck languages, with the exception of $\mathcal{D}</em>$ (which can be captured using a simple one-counter machine). The main motivation of this paper was to introduce
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Length and maximum depth distributions of training/test sets for an example $\mathcal{D}<em 2="2">{2}$ experiment.
new neural architectures that could recognize $\mathcal{D}</em>$ and other difficult context-free languages.</p>
<h3>5.1 The $\mathcal{D}_{2}$ Language</h3>
<p>We trained the Stack-RNN, Stack-LSTM, and Baby-NTM architectures with slightly different memory-controller configurations, in addition to standard RNNs, to learn $\mathcal{D}_{2}$.</p>
<p>A probabilistic context-free grammar for $\mathcal{D}_{2}$ can be written as follows:</p>
<p>$$
S \rightarrow \begin{cases}(S) &amp; \text { with probability } \frac{p}{2} \ [S] &amp; \text { with probability } \frac{p}{2} \ S S &amp; \text { with probability } q \ \varepsilon &amp; \text { with probability } 1-(p+q)\end{cases}
$$</p>
<p>where $0&lt;p, q&lt;1$ and $p+q&lt;1$.
Setting $p=\frac{1}{2}$ and $q=\frac{1}{4}$, we generated 5000 distinct Dyck words, whose lengths were bounded to $[2,50]$, for the training sets. Similarly, we generated 5000 distinct words whose lengths were bounded to $[52,100]$ for the test sets. Hence, there was no overlap between the training and test sets. Test set performance requires generalization well past the training set lengths. As it can be seen in the length and maximum depth distributions of the training and test sets for one of the $\mathcal{D}_{2}$ experiments in Figure 2, the test samples contained longer dependencies than the training sample.</p>
<p>Table 2 lists the performances of the vanilla and memory-augmented recurrent models on the training and test sets for the Dyck language. Our empirical results highlight the dramatic performance difference between the memory-augmented recurrent networks and vanilla recurrent networks: We</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Training Set</th>
<th></th>
<th></th>
<th></th>
<th>Test Set</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
</tr>
<tr>
<td>Vanilla RNN</td>
<td>3.32</td>
<td>12.78</td>
<td>6.41</td>
<td>7.11</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Vanilla LSTM</td>
<td>36.16</td>
<td>62.80</td>
<td>53.24</td>
<td>52.38</td>
<td>0.28</td>
<td>4.10</td>
<td>1.02</td>
<td>1.39</td>
</tr>
<tr>
<td>Stack-RNN by J&amp;M (2015)</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.50</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
</tr>
<tr>
<td>Stack-RNN+Softmax</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>99.96</td>
<td>100</td>
<td>100</td>
<td>99.99</td>
</tr>
<tr>
<td>Stack-RNN+Softmax-Temp</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>99.92</td>
<td>100</td>
<td>100</td>
<td>99.99</td>
</tr>
<tr>
<td>Stack-RNN+Gumbel-Softmax</td>
<td>3.44</td>
<td>100</td>
<td>99.98</td>
<td>90.32</td>
<td>0</td>
<td>100</td>
<td>99.96</td>
<td>89.96</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax</td>
<td>62.52</td>
<td>100</td>
<td>100</td>
<td>95.69</td>
<td>2.78</td>
<td>100</td>
<td>98.25</td>
<td>87.51</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax-Temp</td>
<td>46.70</td>
<td>100</td>
<td>100</td>
<td>94.67</td>
<td>0.80</td>
<td>100</td>
<td>99.73</td>
<td>89.84</td>
</tr>
<tr>
<td>Stack-LSTM+Gumbel-Softmax</td>
<td>50.26</td>
<td>100</td>
<td>99.94</td>
<td>94.97</td>
<td>0.70</td>
<td>99.94</td>
<td>99.33</td>
<td>88.68</td>
</tr>
<tr>
<td>Baby-NTM+Softmax</td>
<td>2.56</td>
<td>100</td>
<td>100</td>
<td>75.80</td>
<td>0</td>
<td>100</td>
<td>99.91</td>
<td>68.73</td>
</tr>
<tr>
<td>Baby-NTM+Softmax-Temp</td>
<td>1.16</td>
<td>100</td>
<td>99.88</td>
<td>72.43</td>
<td>0</td>
<td>100</td>
<td>96.97</td>
<td>68.23</td>
</tr>
<tr>
<td>Baby-NTM+Gumbel-Softmax</td>
<td>5.66</td>
<td>100</td>
<td>99.88</td>
<td>89.39</td>
<td>0</td>
<td>99.90</td>
<td>99.54</td>
<td>86.85</td>
</tr>
</tbody>
</table>
<p>Table 2: The performances of the vanilla and memory-augmented recurrent models on $\mathcal{D}_{2}$. Min/Max/Median/Mean results were obtained from 10 different runs of each model with the same random seed across each run. We note that both Stack-RNN+Softmax and Stack-RNN+Softmax-Temp achieved full accuracy on the test sets in 8 out of 10 times.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualizations of the strength of the memory operations (left) and the values of the memory entries (right) of a Baby-NTM+Softmax model trained to learn $\mathcal{D}<em 2="2">{2}$. We highlight that the Baby-NTM appears to have learned to emulate a simple but effective differentiable pushdown automaton to recognize $\mathcal{D}</em>$.</p>
<p>Note that almost all our stack/memory-augmented architectures achieved full accuracy on the test set, which contained longer and deeper sequences than the training set, while the vanilla RNNs and LSTMs failed to generalize with below 5% accuracy. We further observe that the Stack-RNN proposed by Joulin and Mikolov (2015) performed nearly as well as our models, though ours performed better than theirs on average.</p>
<p>When evaluated based on their empirical median and mean percent-wise performances, the Stack-RNNs appear to be slightly more successful than the Stack-LSTMs and the Baby-NTMs. Both the Stack-RNN+Softmax and Stack-RNN+Softmax-Temp obtained perfect accuracy on the test sets 8 out of 10 times, whereas the best Stack-LSTM variant, Stack-LSTM+Softmax-Temp, achieved perfect accuracy only 3 out of 10 times. Nevertheless, we acknowledge that most of our stack-augmented models were able to successfully generalize well beyond the training data. [7]</p>
<p>Figure 3 provides a visualization of the strengths of the memory operations and the change in the values of the entries of the memory component of one of our memory-augmented models (a Baby-NTM+Softmax with 8 hidden units) at each time step when the model was presented a sample.</p>
<p>[7] We additionally note that, contrary to our initial expectation, using a softmax activation function with varying temperature did not improve the performance of our memory-augmented neural models in general. However, the networks might actually benefit from temperature-based softmax functions in the presence of more categorical choices, because currently the models have only a very limited number of memory operations.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Training Set</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Test Set</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
<td></td>
</tr>
<tr>
<td>Vanilla RNN</td>
<td>0.82</td>
<td>14.88</td>
<td>11.19</td>
<td>9.52</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
</tr>
<tr>
<td>Vanilla LSTM</td>
<td>24.16</td>
<td>39.76</td>
<td>31.55</td>
<td>32.58</td>
<td>0</td>
<td>0.16</td>
<td>0.02</td>
<td>0.04</td>
<td></td>
</tr>
<tr>
<td>Stack-RNN by J&amp;M (2015)</td>
<td>9.02</td>
<td>100</td>
<td>98.17</td>
<td>79.32</td>
<td>0</td>
<td>100</td>
<td>91.29</td>
<td>66.72</td>
<td></td>
</tr>
<tr>
<td>Stack-RNN+Softmax</td>
<td>7.80</td>
<td>100</td>
<td>100</td>
<td>81.75</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>80.00</td>
<td></td>
</tr>
<tr>
<td>Stack-RNN+Softmax-Temp</td>
<td>37.64</td>
<td>99.98</td>
<td>95.74</td>
<td>81.95</td>
<td>0.06</td>
<td>98.18</td>
<td>67.32</td>
<td>52.49</td>
<td></td>
</tr>
<tr>
<td>Stack-RNN+Gumbel-Softmax</td>
<td>1.78</td>
<td>100</td>
<td>44.55</td>
<td>50.71</td>
<td>0</td>
<td>99.98</td>
<td>21.94</td>
<td>43.65</td>
<td></td>
</tr>
<tr>
<td>Stack-LSTM+Softmax</td>
<td>33.98</td>
<td>100</td>
<td>92.25</td>
<td>77.97</td>
<td>0.04</td>
<td>99.94</td>
<td>61.54</td>
<td>55.49</td>
<td></td>
</tr>
<tr>
<td>Stack-LSTM+Softmax-Temp</td>
<td>37.64</td>
<td>99.98</td>
<td>95.74</td>
<td>81.95</td>
<td>0.06</td>
<td>98.18</td>
<td>67.32</td>
<td>52.49</td>
<td></td>
</tr>
<tr>
<td>Stack-LSTM+Gumbel-Softmax</td>
<td>25.74</td>
<td>99.98</td>
<td>78.21</td>
<td>72.01</td>
<td>0</td>
<td>99.2</td>
<td>27.08</td>
<td>42.17</td>
<td></td>
</tr>
<tr>
<td>Baby-NTM+Softmax</td>
<td>4.60</td>
<td>100</td>
<td>84.29</td>
<td>60.63</td>
<td>0</td>
<td>100</td>
<td>23.44</td>
<td>44.51</td>
<td></td>
</tr>
<tr>
<td>Baby-NTM+Softmax-Temp</td>
<td>6.40</td>
<td>100</td>
<td>16.44</td>
<td>39.97</td>
<td>0</td>
<td>100</td>
<td>0.51</td>
<td>27.46</td>
<td></td>
</tr>
<tr>
<td>Baby-NTM+Gumbel-Softmax</td>
<td>0.76</td>
<td>100</td>
<td>11.70</td>
<td>43.42</td>
<td>0</td>
<td>99.9</td>
<td>0</td>
<td>38.76</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 3: The performances of the vanilla and memory-augmented recurrent models on $\mathcal{D}_{3}$. In 32 out of 100 trials, the MARNNs with 8 hidden units and one-dimensional memory achieved over $99 \%$ accuracy on the test sets. However, increasing the dimensional of the memory for our MARNNs further improved our results.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of the values of the memory entries of a Baby-NTM+Softmax model trained to learn $\mathcal{D}<em 2="2">{3}$.
ple in $\mathcal{D}</em>$}$. The Baby-NTM appears to be using its ROTATE-RIGHT and POP-RIGHT operations for the open parentheses '(' and '|', respectively, and POP-LEFT operation for both of the closing parentheses ')' and '|', thereby emulating a simple PDA-like mechanism. A careful inspection of the memory entries of the Baby-NTM indicates that the model utilizes a special marker with a distinct value ( $\sim 0.45$ in our example) to distinguish an empty stack configuration from a processed stack configuration. On the other hand, the memory alone does not dictate the output values: The hidden states of the model govern the overall behavior and embody a finite-state control, as shown in the formulation of the Baby-NTM. ${ }^{8</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>5.2 The $\mathcal{D}<em 6="6">{3}$ and $\mathcal{D}</em>$ Languages</h3>
<p>We further conducted experiments on the $\mathcal{D}<em 6="6">{3}$ and $\mathcal{D}</em>$, due to its complexity.}$ languages to evaluate the ability of our memory-augmented architectures to encode more complex hierarchical representations. The training and test corpora were generated in the same style as the previous task; however, we included 15,000 samples in the training set for $\mathcal{D}_{6</p>
<p>As shown in Table 3, the Stack-RNN+Softmax model had the best performance among all the neural networks on the $\mathcal{D}_{3}$ learning task, obtaining</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">Training Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Set</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">Min</td>
<td style="text-align: center;">Max</td>
<td style="text-align: center;">Med</td>
<td style="text-align: center;">Mean</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla RNN</td>
<td style="text-align: center;">21.19</td>
<td style="text-align: center;">25.71</td>
<td style="text-align: center;">23.53</td>
<td style="text-align: center;">23.39</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla LSTM</td>
<td style="text-align: center;">32.47</td>
<td style="text-align: center;">41.62</td>
<td style="text-align: center;">37.35</td>
<td style="text-align: center;">37.05</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">Stack-RNN by J\&amp;M (2015)</td>
<td style="text-align: center;">99.47</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">97.60</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.70</td>
</tr>
<tr>
<td style="text-align: center;">Stack-RNN+Softmax</td>
<td style="text-align: center;">99.92</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.32</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.85</td>
</tr>
<tr>
<td style="text-align: center;">Stack-RNN+Softmax-Temp</td>
<td style="text-align: center;">36.83</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.54</td>
<td style="text-align: center;">80.09</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">78.44</td>
<td style="text-align: center;">60.88</td>
</tr>
<tr>
<td style="text-align: center;">Stack-RNN+Gumbel-Softmax</td>
<td style="text-align: center;">20.90</td>
<td style="text-align: center;">99.98</td>
<td style="text-align: center;">99.93</td>
<td style="text-align: center;">91.62</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">99.92</td>
<td style="text-align: center;">99.50</td>
<td style="text-align: center;">87.69</td>
</tr>
<tr>
<td style="text-align: center;">Stack-LSTM+Softmax</td>
<td style="text-align: center;">98.48</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.79</td>
<td style="text-align: center;">91.12</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.18</td>
<td style="text-align: center;">98.23</td>
</tr>
<tr>
<td style="text-align: center;">Stack-LSTM+Softmax-Temp</td>
<td style="text-align: center;">36.83</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.54</td>
<td style="text-align: center;">80.09</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">78.44</td>
<td style="text-align: center;">60.88</td>
</tr>
<tr>
<td style="text-align: center;">Stack-LSTM+Gumbel-Softmax</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">68.62</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">99.90</td>
<td style="text-align: center;">24.61</td>
<td style="text-align: center;">44.47</td>
</tr>
<tr>
<td style="text-align: center;">Baby-NTM+Softmax</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.00</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">99.87</td>
</tr>
<tr>
<td style="text-align: center;">Baby-NTM+Softmax-Temp</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.15</td>
<td style="text-align: center;">8.56</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.91</td>
<td style="text-align: center;">88.40</td>
</tr>
<tr>
<td style="text-align: center;">Baby-NTM+Gumbel-Softmax</td>
<td style="text-align: center;">22.56</td>
<td style="text-align: center;">99.98</td>
<td style="text-align: center;">99.86</td>
<td style="text-align: center;">75.46</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">99.86</td>
<td style="text-align: center;">99.23</td>
<td style="text-align: center;">63.49</td>
</tr>
</tbody>
</table>
<p>Table 4: The performances of the vanilla and memory-augmented recurrent models on the $\mathcal{D}_{6}$. We note that the MARNNs in this example contain 12 hidden units and 5 -dimensional external stack/memory. In 70 out of 100 trials, the MARNNs performed over $99 \%$ accuracy. Overall, the Baby-NTM+Softmax had the best performance.
perfect accuracy in eight out of ten trials. Following the Stack-RNNs, the Stack-LSTMs and BabyNTMs, on average, achieved around $50 \%$ and $37 \%$ accuracy on the test set, respectively. On the other hand, the Stack-RNN by Joulin and Mikolov (2015) could generalize better than most of our models in terms of its median and mean scores, albeit still not better than our Stack-RNN.</p>
<p>Figure 4 illustrates the behavior of one of our Baby-NTMs as the model is presented a long sequence in $\mathcal{D}<em 3="3">{3}$. It is remarkable to witness how the memory-augmented model makes use of its external memory to learn a sequence of actions to recognize a sample in $\mathcal{D}</em>$. Similar to the behavior of the previous model in Figure 3, the RNN controller of the Baby-NTM model in this instance appears to be using the differentiable memory as a stack-like structure and inserting distinct values to the memory at different time steps. Furthermore, we note the presence of special markers ( 0.22 in the first half and 0.21 in the second halfboth colored blue) in the memory: These idiosyncratic memory elements marking the bottom of the used portion of the stack enable the model to know when to predict only the set of open parentheses.</p>
<p>In contrast, the overall performance of the MARNNs for $\mathcal{D}<em 2="2">{6}$ was much lower than for $\mathcal{D}</em>$. For instance, none of our models could obtain full accuracy on the training or test sets; the maximum score our models could achieve was $60.38 \%$. We wondered whether increasing the dimension of the memory would remedy the prob-
lem. Table 4 summarizes our new results with the same architectures containing 12 hidden units and 5 -dimensional augmented stack/memory. We saw a significant increase in the performance of our models: In 60 out of 90 trials, our enhanced MARNNs achieved almost perfect ( $\geq 99 \%$ ) accuracy on the test set.}$ and $\mathcal{D}_{3</p>
<h2>6 Learning Palindrome Languages</h2>
<p>Our previous results established that the MARNNs can learn Dyck languages, which represent the core of the CFL class. We note that the Dyck languages incorporate a notion of palindrome: The intersection of $\mathcal{D}<em i="i">{n}$ with $p^{<em>} \bar{p}^{</em>}$ leads to a definition of a specific type of a homomorphic palindrome language $w \varphi\left(w^{R}\right)$, where * is the Kleene star, $\varphi$ a homomorphism given by $p</em>$ the reversal of $w$. Therefore, we would expect our models to be able to learn various deterministic versions of palindrome languages.} \mapsto \bar{p}_{i}$, and $w^{R</p>
<h3>6.1 Homomorphic Palindrome Language</h3>
<p>Our first target of exploration is the deterministic homomorphic palindrome language, the language of words $w # \varphi\left(w^{R}\right)$ where $w \in{a, b, c}^{*}, #$ is a symbol serving to mark the center of the palindrome, and $\varphi$ maps $a$ to $x, b$ to $y$, and $c$ to $z$. We use the notion of recognition from the previous section, predicting at each symbol the set of all possible following symbols. Viewed as a transduction, this amounts to the following task:</p>
<p>$$
w # \varphi\left(w^{R}\right) \Rightarrow(a / b / c / #)^{|w|} \varphi\left(w^{R}\right) \dashv
$$</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Training Set</th>
<th></th>
<th></th>
<th></th>
<th>Test Set</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
</tr>
<tr>
<td>Vanilla RNN</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Vanilla LSTM</td>
<td>0</td>
<td>5.22</td>
<td>2.23</td>
<td>2.39</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Stack-RNN by J\&amp;M (2015)</td>
<td>0</td>
<td>100</td>
<td>46.04</td>
<td>49.13</td>
<td>0</td>
<td>100</td>
<td>50.42</td>
<td>50.00</td>
</tr>
<tr>
<td>Stack-RNN+Softmax</td>
<td>0</td>
<td>100</td>
<td>99.99</td>
<td>60.00</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>60.00</td>
</tr>
<tr>
<td>Stack-RNN+Softmax-Temp</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
</tr>
<tr>
<td>Stack-RNN+Gumbel-Softmax</td>
<td>0</td>
<td>100</td>
<td>17.10</td>
<td>43.42</td>
<td>0</td>
<td>100</td>
<td>16.98</td>
<td>43.39</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>61.36</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>60.00</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax-Temp</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.07</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
</tr>
<tr>
<td>Stack-LSTM+Gumbel-Softmax</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>80.20</td>
<td>0</td>
<td>100</td>
<td>99.98</td>
<td>79.99</td>
</tr>
<tr>
<td>Baby-NTM+Softmax</td>
<td>0</td>
<td>100</td>
<td>67.16</td>
<td>53.43</td>
<td>0</td>
<td>100</td>
<td>66.55</td>
<td>53.31</td>
</tr>
<tr>
<td>Baby-NTM+Softmax-Temp</td>
<td>0</td>
<td>100</td>
<td>99.99</td>
<td>60.00</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>60.00</td>
</tr>
<tr>
<td>Baby-NTM+Gumbel-Softmax</td>
<td>0</td>
<td>100</td>
<td>60.43</td>
<td>52.09</td>
<td>0</td>
<td>100</td>
<td>61.30</td>
<td>52.26</td>
</tr>
</tbody>
</table>
<p>Table 5: The performances of the vanilla and memory-augmented recurrent models on the deterministic homomorphic palindrome language. Most of our MARNNs achieved almost full accuracy on the test sets.</p>
<p>The training set for this task contained 5000 unique samples of length varying from 2 to 50 , and the test set contained 5000 unique samples of length varying from 52 to 100 . We remark that there was no overlap between the training and test sets, just as in the case of the Dyck language tasks.</p>
<p>Table 5 lists the performances of the vanilla and memory-augmented models on the deterministic homomorphic palindrome language. We highlight the success of our models once again: While our MARNNs often performed with perfect accuracy on the training and test sets, the standard recurrent models could not predict even one sample in the test set correctly. Overall, most of the variants of the Stack-RNN/LSTM and Baby-NTM models seem to have learned how to emulate pushdown automata: They learned to push certain values into their memory or stack whenever they read a character from the ${a, b, c}$ alphabet and then started popping them one by one after they would see #, and at the last step, the model predicted the end of the sequence token $\dashv$. The models did not perform equally well though: When evaluated on their mean and median percent-wise performances, for instance, the Stack-LSTMs were found to generalize better than the Stack-RNNs and the Baby-NTMs in this task. Further, it is hard to make a conclusive statement about whether employing a softmax function with varying temperature in our MARNNs had any benefit. Nevertheless, the Stack-LSTMs+Gumbel-Softmax performed slightly better than the other models in terms of their mean percentages on the test sets.</p>
<h3>6.2 Simple Palindrome Language</h3>
<p>Taking the homomorphism $\varphi$ to be the identity map in the previous language, it is reasonable to expect the models to learn the $w # w^{R}$ palindrome language. We evaluated recognition of this language once again as a possible-next-symbol prediction task, which can be viewed as the following sequence transduction task:</p>
<p>$$
w # w^{R} \Rightarrow(a / b / c / #)^{|w|} w^{R} \dashv
$$</p>
<p>Surprisingly, all of our MARNN models had difficulty learning this language. Only in three of 90 trials were our MARNNs able to learn the language; other times, the models typically obtained $0 \%$ accuracy during testing. When we increased the dimensionality of the memory to five, however, our MARNNs immediately learned the task with almost full accuracy again. Given that the only difference between the previous task and this task is the second half of the strings, we conjectured that our models were getting confused in the second half: Because of vocabulary overlap in the two halves, the models might be using information from the second half when predicting the set of possible symbols in the second half, thereby getting distracted and finding themselves stuck at bad local minima. To verify our hypothesis, we thus performed one more task, string reversal, which we describe in the following section.</p>
<h2>7 Learning the String-Reversal Task</h2>
<p>In the previous section, we witnessed a strange phenomenon: Our MARNN models with 8 hidden</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Training Set</th>
<th></th>
<th></th>
<th></th>
<th>Test Set</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
<td>Min</td>
<td>Max</td>
<td>Med</td>
<td>Mean</td>
</tr>
<tr>
<td>Vanilla RNN</td>
<td>0.06</td>
<td>0.90</td>
<td>0.46</td>
<td>0.46</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Vanilla LSTM</td>
<td>0.68</td>
<td>5.08</td>
<td>3.62</td>
<td>3.50</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Stack-RNN by J\&amp;M (2015)</td>
<td>0.16</td>
<td>100</td>
<td>50.29</td>
<td>50.19</td>
<td>0</td>
<td>100</td>
<td>50.00</td>
<td>50.00</td>
</tr>
<tr>
<td>Stack-RNN+Softmax</td>
<td>0.38</td>
<td>100</td>
<td>100</td>
<td>77.39</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>76.65</td>
</tr>
<tr>
<td>Stack-RNN+Softmax-Temp</td>
<td>0.14</td>
<td>100</td>
<td>100</td>
<td>80.05</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>80.00</td>
</tr>
<tr>
<td>Stack-RNN+Gumbel-Softmax</td>
<td>0.18</td>
<td>100</td>
<td>99.98</td>
<td>77.71</td>
<td>0</td>
<td>100</td>
<td>99.98</td>
<td>77.83</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax</td>
<td>2.02</td>
<td>100</td>
<td>100</td>
<td>80.60</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>79.99</td>
</tr>
<tr>
<td>Stack-LSTM+Softmax-Temp</td>
<td>0.06</td>
<td>100</td>
<td>100</td>
<td>70.49</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
</tr>
<tr>
<td>Stack-LSTM+Gumbel-Softmax</td>
<td>2.18</td>
<td>100</td>
<td>100</td>
<td>80.48</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>80.00</td>
</tr>
<tr>
<td>Baby-NTM+Softmax</td>
<td>0.08</td>
<td>100</td>
<td>100</td>
<td>86.65</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>86.67</td>
</tr>
<tr>
<td>Baby-NTM+Softmax-Temp</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.07</td>
<td>0</td>
<td>100</td>
<td>100</td>
<td>70.00</td>
</tr>
<tr>
<td>Baby-NTM+Gumbel-Softmax</td>
<td>0.20</td>
<td>100</td>
<td>100</td>
<td>90.01</td>
<td>0</td>
<td>100</td>
<td>99.97</td>
<td>89.96</td>
</tr>
</tbody>
</table>
<p>Table 6: The performances of the vanilla and memory-augmented recurrent models on the string reversal task under the transduction setting. In 32 out of 90 trials, our MARNNs obtained perfect accuracy.
units and one-dimensional external memory could learn the deterministic homomorphic palindrome language, but not the simple palindrome language. Since the only difference between the two tasks is the existence of a non-trivial isomorphism $\varphi$ (and the vocabulary overlap in the two halves), we wanted to perform the string reversal task under a sequence transduction setting in which the reversal appears only in the output:</p>
<p>$$
w #^{|w|} \mapsto #^{|w|} w^{R}
$$</p>
<p>The training and test sets were similar to the previous cases: 5000 samples each, with lengths bounded by $[2,50]$ and $[52,100]$, respectively.</p>
<p>Table 6 illustrates that most of our MARNNs achieved perfect accuracy on the test sets in this version of the string reversal task. The results corroborated our conjecture by showing that when the second half of the input samples contained symbols from an alphabet other than the one used in the first half (in this case, the # symbol), the memory-augmented models do not get confused and act in the desired way (pushing elements into the stack in the first half and popping them one by one in the second half after seeing the marker #). When we visualized the hidden states and the memory entries of the models for this task, we observed that our MARNNs learned to emulate simple pushdown-automata.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we introduced three memoryaugmented neural architectures and provided the
first demonstration of neural networks learning to recognize the generalized Dyck languages, which represent the "core" of the context-free language class. We further evaluated the learning capabilities of our models on recognizing the deterministic homomorphic palindrome language and simple palindrome language under the sequence prediction framework and performing the string-reversal task under a sequence transduction setting. In all the experiments, our MARNNs outperformed the vanilla RNN and LSTM models and often attained perfect accuracy on both the training and test sets.</p>
<p>Since we limited the dimensionality of the external memory in our memory-augmented architectures to one, we were also able to visualize the changes in the external memory of the BabyNTMs trained to learn the $\mathcal{D}<em 3="3">{2}$ and $\mathcal{D}</em>$ language, and further address the difficulty of training stack-augmented recurrent networks. Although we agree that it is challenging to train MARNNs due to various optimization issues, one can still train these models with as few as eight or twelve hidden units to learn the Dyck languages, and our empirical findings support this claim.}$ languages. Our simple analysis revealed that our MARNNs learned to emulate pushdown-automata to recognize these Dyck languages. Hao et al. (2018) mention that their Neural-Stack models could not perfectly employ stack-based strategies to learn an appropriate representation to recognize the $\mathcal{D}_{2</p>
<h2>9 Acknowledgment</h2>
<p>The authors appreciate the helpful comments of Michael Hahn, Yoav Goldberg, Drew Pendergrass, Dan Stefan Eniceicu, and Filippos Sytilidis. M.S. gratefully acknowledges the support of the Harvard College Research Program (HCRP) and the Harvard Center for Research on Computation and Society Research Fellowship for Undergraduate Students. S.G. was supported by a Siebel Fellowship. Y.B. was supported by the Harvard Mind, Brain, and Behavior Initiative. The computations in this paper were run on the Odyssey cluster supported by the FAS Division of Science, Research Computing Group at Harvard University.</p>
<h2>References</h2>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. arXiv preprint arXiv:1409.0473.</p>
<p>Jean-Philippe Bernardy. 2018. Can Recurrent Neural Networks Learn Nested Recursion? LiLT (Linguistic Issues in Language Technology), 16(1).</p>
<p>Mikael Bodén and Janet Wiles. 2000. ContextFree and Context-Sensitive Dynamics in Recurrent Neural Networks. Connection Science, 12(3-4):197-210.</p>
<p>Mikael Bodén, Janet Wiles, Bradley Tonkes, and Alan Blair. 1999. Learning to Predict a Context-Free Language: Analysis of Dynamics in Recurrent Hidden Units.</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</p>
<p>Noam Chomsky. 1957. Syntactic Structures. Mouton, The Hague.</p>
<p>Noam Chomsky. 1962. Context-Free Grammars and Pushdown Storage. MIT Res. Lab. Electron. Quart. Prog. Report., 65:187-194.</p>
<p>Noam Chomsky and Marcel P Schützenberger. 1963. The Algebraic Theory of Context-Free</p>
<p>Languages. In Studies in Logic and the Foundations of Mathematics, volume 35, pages 118161. Elsevier.</p>
<p>Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992. Learning Context-free Grammars: Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory. In Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, page 14.</p>
<p>Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1993. Using Prior Knowledge in a NNPDA to Learn Context-Free Languages. In Advances in neural information processing systems, pages $65-72$.</p>
<p>Tristan Deleu and Joseph Dureau. 2016. Learning Operations on a Stack with Neural Turing Machines. arXiv preprint arXiv:1612.00827.</p>
<p>Jeffrey L Elman. 1990. Finding Structure in Time. Cognitive science, 14(2):179-211.</p>
<p>Jeffrey L Elman. 1991. Distributed Representations, Simple Recurrent Networks, and Grammatical Structure. Machine learning, 7(23):195-225.</p>
<p>Felix A Gers, Juan Antonio Pérez-Ortiz, Douglas Eck, and Jürgen Schmidhuber. 2002. Learning Context Sensitive Languages with LSTM Trained with Kalman Filters. In International Conference on Artificial Neural Networks, pages 655-660. Springer.</p>
<p>Felix A Gers and E Schmidhuber. 2001. LSTM Recurrent Networks Learn Simple Context-Free and Context-Sensitive Languages. IEEE Transactions on Neural Networks, 12(6):1333-1340.</p>
<p>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech Recognition with Deep Recurrent Neural Networks. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 66456649. IEEE.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural Turing Machines. arXiv preprint arXiv:1410.5401.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka</p>
<p>Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. 2016. Hybrid Computing Using a Neural Network with Dynamic External Memory. Nature, 538(7626):471.</p>
<p>Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to Transduce with Unbounded Memory. In Advances in Neural Information Processing Systems, pages 1828-1836.</p>
<p>Sheila A Greibach. 1973. The hardest contextfree language. SIAM Journal on Computing, 2(4):304-310.</p>
<p>Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. 2018. Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes. Neural computation, 30(4):857-884.</p>
<p>Michael Hahn. 2019. Theoretical limitations of self-attention in neural sequence models. arXiv preprint arXiv:1906.06755.</p>
<p>Yiding Hao, William Merrill, Dana Angluin, Robert Frank, Noah Amsel, Andrew Benz, and Simon Mendelsohn. 2018. Context-Free Transductions with Neural Stacks. arXiv preprint arXiv:1809.02836.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural computation, 9(8):1735-1780.</p>
<p>Steffen Hölldobler, Yvonne Kalinke, and Helko Lehmann. 1997. Designing a Counter: Another Case Study of Dynamics and Activation Landscapes in Recurrent Networks. In Annual Conference on Artificial Intelligence, pages 313324. Springer.</p>
<p>Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical Reparameterization with GumbelSoftmax. arXiv preprint arXiv:1611.01144.</p>
<p>Armand Joulin and Tomas Mikolov. 2015. Inferring Algorithmic Patterns with Stackaugmented Recurrent Nets. In Advances in neural information processing systems, pages 190198.</p>
<p>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700-1709.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Karol Kurach, Marcin Andrychowicz, and Ilya Sutskever. 2015. Neural Random-Access Machines. arXiv preprint arXiv:1511.06392.</p>
<p>Frédéric Magniez, Claire Mathieu, and Ashwin Nayak. 2014. Recognizing well-parenthesized expressions in the streaming model. SIAM Journal on Computing, 43(6):1880-1905.</p>
<p>William Merrill. 2019. Sequential neural networks as automata. arXiv preprint arXiv:1906.01615.</p>
<p>Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent Neural Network Based Language Model. In Eleventh annual conference of the international speech communication association.</p>
<p>Jordan B Pollack. 1991. The Induction of Dynamical Recognizers. In Connectionist Approaches to Language Learning, pages 123148. Springer.</p>
<p>Paul Rodriguez. 2001. Simple Recurrent Networks Learn Context-Free and ContextSensitive Languages by Counting. Neural computation, 13(9):2093-2118.</p>
<p>Paul Rodriguez and Janet Wiles. 1998. Recurrent Neural Networks can Learn to Implement Symbol-Sensitive Counting. In Advances in Neural Information Processing Systems, pages 87-93.</p>
<p>Jürgen Schmidhuber, F Gers, and Douglas Eck. 2002. Learning Nonregular Languages: A Comparison of Simple Recurrent Networks and LSTM. Neural Computation, 14(9):20392041.</p>
<p>Luzi Sennhauser and Robert Berwick. 2018. Evaluating the Ability of LSTMs to Learn ContextFree Grammars. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing</p>
<p>and Interpreting Neural Networks for NLP, pages 115-124.</p>
<p>Hava T Siegelmann and Eduardo D Sontag. 1994. Analog Computation via Neural Networks. Theoretical Computer Science, 131(2):331360.</p>
<p>Hava T Siegelmann and Eduardo D Sontag. 1995. On the Computational Power of Neural Nets. Journal of computer and system sciences, 50(1):132-150.</p>
<p>Natalia Skachkova, Thomas Trost, and Dietrich Klakow. 2018. Closing Brackets with Recurrent Neural Networks. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 232-239.</p>
<p>Mark Steijvers. 1996. A Recurrent Network that Performs a Context-Sensitive Prediction Task.</p>
<p>Ivan Hal Sudborough. 1976. On deterministic context-free languages, multihead automata, and the power of an auxiliary pushdown store. In Proceedings of the eighth annual ACM symposium on Theory of computing, pages 141148. ACM.</p>
<p>Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. 2012. LSTM Neural Networks for Language Modeling. In INTERSPEECH.</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to Sequence Learning with Neural Networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Mirac Suzgun, Yonatan Belinkov, and Stuart M Shieber. 2019a. On Evaluating the Generalization of LSTM Models in Formal Languages. Proceedings of the Society for Computation in Linguistics (SCiL), pages 277-286.</p>
<p>Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart Shieber. 2019b. LSTM networks can perform dynamic counting. In Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges, pages 44-54, Florence. Association for Computational Linguistics.</p>
<p>Bradley Tonkes and Janet Wiles. 1997. Learning a Context-Free Task with a Recurrent Neural Network: An Analysis of Stability. In In</p>
<p>Proceedings of the Fourth Biennial Conference of the Australasian Cognitive Science Society. Citeseer.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 59986008.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision rnns for language recognition. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 740-745.</p>
<p>Greg Yang. 2016. Lie Access Neural Turing Machine. arXiv preprint arXiv:1602.08671.</p>
<p>Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. 2019. Learning the Dyck language with attentionbased Seq2Seq models. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 138-146, Florence, Italy. Association for Computational Linguistics.</p>
<p>Wojciech Zaremba, Tomas Mikolov, Armand Joulin, and Rob Fergus. 2016. Learning Simple Algorithms from Examples. In International Conference on Machine Learning, pages 421429.</p>
<p>Wojciech Zaremba and Ilya Sutskever. 2015. Reinforcement Learning Neural Turing MachinesRevised. arXiv preprint arXiv:1505.00521.</p>
<p>Zheng Zeng, Rodney M Goodman, and Padhraic Smyth. 1994. Discrete Recurrent Neural Networks for Grammatical Inference. IEEE Transactions on Neural Networks, 5(2):320-330.</p>
<h1>A Comparison of Stack-RNN architectures</h1>
<p>Recall the formulation of our Stack-RNN architecture in Section 3.1. We update $\mathbf{h}_{t}$, the hidden state at time $t$, as follows:</p>
<p>$$
\mathbf{h}<em h="h" i="i">{t}=\tanh \left(\mathbf{W}</em>} x_{t}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>} \tilde{\mathbf{h}<em h="h">{(t-1)}+\mathbf{b}</em>\right)
$$</p>
<p>where $\tilde{\mathbf{h}}_{(t-1)}$ is defined to be:</p>
<p>$$
\tilde{\mathbf{h}}<em _t-1_="(t-1)">{(t-1)}=\mathbf{h}</em>}+\mathbf{W<em _t-1_="(t-1)">{s h} s</em>
$$}^{(0)</p>
<p>Rewriting the equation for $\mathbf{h}_{t}$, we realize that our formulation of Stack-RNN is almost equivalent to the Stack-RNN model by Joulin and Mikolov (2015):</p>
<p>$$
\begin{aligned}
\mathbf{h}<em h="h" i="i">{t} &amp; =\tanh \left(\mathbf{W}</em>} x_{t}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>} \tilde{\mathbf{h}<em h="h">{(t-1)}+\mathbf{b}</em>\right) \
&amp; =\tanh \left(\mathbf{W}<em t="t">{i h} x</em>}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>}\left(\mathbf{h<em h="h" s="s">{(t-1)}+\mathbf{W}</em>} s_{(t-1)}^{(0)}\right)+\mathbf{b<em h="h" i="i">{h h}\right) \
&amp; =\tanh \left(\mathbf{W}</em>} x_{t}+\mathbf{b<em h="h">{i h}+\mathbf{W}</em>} \mathbf{h<em h="h">{(t-1)}+\underbrace{\mathbf{W}</em>} \mathbf{W<em _="(*)">{s h}}</em>\right)
\end{aligned}
$$} s_{(t-1)}^{(0)}+\mathbf{b}_{h h</p>
<p>In our Stack-RNN architecture, $s_{(t-1)}^{(0)}$ depends on $(*)$, namely $\mathbf{W}<em h="h" s="s">{h h} \mathbf{W}</em>}$, whereas in Joulin and Mikolov's Stack-RNN model, it only depends on $\mathbf{W<em h="h" i="i">{s h}$. Furthermore, we make use of $\tanh (\cdot)$, instead of $\sigma(\cdot)$, to achieve non-linearity and include bias terms, namely $\mathbf{b}</em>}$ and $\mathbf{b<em t="t">{h h}$, in our definition of $\mathbf{h}</em>$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ The visualizations for the other memory-augmented models were qualitatively similar, though some networks learned more complex representations. We further empha-&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>size that the dimensions of the external stack and memory entries in our MARNNs were set up to be one-dimensional for visualization purposes, but we additionally experimented with higher dimensional memory structures and observed that such additions often increased the overall performances of the models, especially the performance of the Baby-NTMs.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
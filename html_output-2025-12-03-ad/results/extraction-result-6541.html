<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6541 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6541</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6541</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-270285630</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.03816v3.pdf" target="_blank">ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search</a></p>
                <p><strong>Paper Abstract:</strong> Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called ReST-MCTS*, based on integrating process reward guidance with tree search MCTS* for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS* circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS* is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS* achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST$^\text{EM}$ and Self-Rewarding LM. We release all code at https://github.com/THUDM/ReST-MCTS.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6541.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6541.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST-MCTS*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReST-MCTS* (Process Reward Guided MCTS for LLM self-training)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training framework that uses a modified Monte Carlo Tree Search (MCTS*) guided by an automatically inferred per-step process reward (PRM) to generate high-quality reasoning traces and per-step values to iteratively train both policy and reward models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLM backbones (iterative experiments reported across GLM4, GPT-3.5-turbo, Mistral-7B:MetaMATH, LLaMA-3-8B-Instruct, SciGLM-6B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MCTS* (value / Process-Reward guided Monte Carlo Tree Search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH (math reasoning benchmark; token-budgeted tree-search comparisons in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>college-level mathematical reasoning problems (multi-step math problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>48.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ReST-MCTS* uses inferred per-step process rewards to guide tree search and selects high-quality traces for self-training. The paper reports that after iterative self-training the MCTS*-based verifier+search achieves 48.5% on MATH, significantly above Self-Consistency's 42.5%; authors attribute improvements to (1) higher-quality, per-step verification (PRM) that filters false-positive traces, and (2) the tree-search exploration that finds diverse correct reasoning paths that single-sequence CoT often misses.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6541.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency over Chain-of-Thought (multiple CoT samples, majority-vote answer selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-style decoding strategy that samples multiple chain-of-thought (CoT) traces and selects the final answer that appears most frequently.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLM backbones (evaluated in token-budgeted comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency CoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (sampling + majority vote)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple sampled traces aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step mathematical problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>42.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ReST-MCTS*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-6.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Self-Consistency samples many CoT traces and aggregates answers; authors observe that while SC is strong, it can require more token budget than MCTS* and can be outperformed when better per-step verification (PRM) and structured search produce higher-quality traces.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6541.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST-MCTS* (SciEval / GLM4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReST-MCTS* evaluated with GLM4 on SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the ReST-MCTS* tree-search + PRM pipeline evaluated on SciEval; paper reports an overall accuracy for GLM4 using ReST-MCTS*.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>MCTS* (PRM-guided tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SciEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-part scientific reasoning / exam-style scientific problems</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>79.87</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT, ToT and other baselines (same experimental suite)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports ReST-MCTS* attains 79.87% (GLM4) on SciEval and likewise outperforms baselines for GPT-3.5-turbo (62.31%), concluding that PRM-guided tree search improves accuracy on science benchmarks though on some simple single-step subsets CoT/Self-Consistency may already be competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6541.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Process Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Process Reward Model (per-step value predictor used to evaluate partial solutions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model trained to predict a quality/value score for partial solutions (per intermediate reasoning step) used to guide tree search and to filter traces for self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>initially finetuned ChatGLM3-6B and Mistral-7B variants (value-model backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B / 7B (ChatGLM3-6B, Mistral-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Process Reward Model (PRM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>per-step value prediction / verifier</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single-style (per-step verification applied across traces) </td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>value-model evaluation set (science+math partial-solution set; test split ~14k samples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>predict continuous quality value v_k for partial solutions (within tolerance 0.1) used to guide MCTS*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>value prediction accuracy (|predicted - target| < 0.1)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>69.3</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>MATH-SHEPHERD and outcome reward models (ORM) in paper comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Initial PRM trained without human per-step labels via inferred values from verified search trees achieves 69.3% test accuracy (tolerance 0.1), and authors report that PRM-based guidance yields better trace quality than alternative automated PRM generation (e.g., MATH-SHEPHERD) and ORMs for filtering false-positive full traces.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6541.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Best-of-N (BoN) / ORM+BoN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Best-of-N selection (optionally using an Outcome Reward Model to score full traces)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sampling multiple complete reasoning traces and selecting the trace with highest outcome-reward model score (ORM+BoN) or simply the top final-answer choice (BoN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs used as generator/verifier in baselines</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Best-of-N (with ORM scoring in some baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble / selection-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed (many sampled complete traces but individual traces are single-sequence style)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, SciBench (token-budgeted comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step reasoning with selection of best-scored full trace</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and token-efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ReST-MCTS* (token-efficiency and final accuracy comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports that ORM+BoN and plain BoN baselines can be competitive but are less token-efficient than MCTS*; also that selecting on final-outcome rewards (ORM) can keep false-positive traces because correct final answer can arise from incorrect intermediate steps â€” motivating PRM per-step verification.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6541.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT; planning-based tree search over coherent units of thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning-style reasoning method that builds a tree over intermediate 'thought' states and explores multiple paths (often implemented with DFS/BFS style search).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with GLM4, GPT-3.5-turbo, LLaMA2-13B-Chat in paper comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search (planning)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>SciBench (college-level scientific reasoning) and other science/math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-path deliberative problem solving (scientific reasoning questions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ReST-MCTS*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT is a tree-structured baseline; paper notes ToT sometimes performs well and in some subjects can even surpass ReST-MCTS*, highlighting that when the value model guidance is less effective or when the base policy is weak, ToT or simpler strategies may be competitive. ToT explores multiple reasoning styles (paths) explicitly whereas CoT is a single sequential style.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6541.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReST EM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReST EM (reinforced self-training EM-style baseline referenced in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior self-training approach that uses outcome reward (final-answer correctness) to select generated traces for iterative policy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with LLaMA-3-8B-Instruct, Mistral-7B:MetaMATH, SciGLM-6B in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (8B, 7B, 6B)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Outcome-reward based self-training (ReST EM)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>self-training (generate & filter by final outcome)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single-style selection (positive-only final-solution selection)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH, GPQA, Diamond, CEval-Hard, SciBench variants (as used in paper Table 2)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>math / scientific reasoning with iterative self-training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) (iteration-dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ReST-MCTS* (self-training paradigm comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Paper reports that ReST-MCTS* consistently outperforms ReST EM across multiple iterations and backbones, arguing that using per-step PRM labels inferred by tree search produces higher-quality training traces than outcome-only selection which allows false-positive intermediate reasoning to persist.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6541.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6541.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Rewarding (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Rewarding Language Models (LLM-generated outcome rewards / judging without human labels)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training variant where the LLM itself judges final answers or generates reward signals (outcome reward), used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>evaluated with LLaMA-3-8B-Instruct, Mistral-7B:MetaMATH, SciGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Rewarding (LLM-as-judge outcome reward)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>self-training / LLM-judged outcome reward</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single-style (outcome judged full traces)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH and science benchmarks used in self-training experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>math/scientific reasoning iterative self-training</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) across iterations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ReST-MCTS*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>The paper finds ReST-MCTS* outperforms Self-Rewarding in successive self-training iterations, attributing gains to per-step value signals (PRM) inferred via tree search that better filter out false-positive traces than LLM-generated outcome-only judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain-of-Thought Reasoning in Language Models <em>(Rating: 1)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 2)</em></li>
                <li>MATH-SHEPHERD: A label-free step-by-step verifier for LLMs in mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Self-Rewarding Language Models <em>(Rating: 1)</em></li>
                <li>ReST EM (Reinforced Self-Training EM-style approaches) <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6541",
    "paper_id": "paper-270285630",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "ReST-MCTS*",
            "name_full": "ReST-MCTS* (Process Reward Guided MCTS for LLM self-training)",
            "brief_description": "A self-training framework that uses a modified Monte Carlo Tree Search (MCTS*) guided by an automatically inferred per-step process reward (PRM) to generate high-quality reasoning traces and per-step values to iteratively train both policy and reward models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLM backbones (iterative experiments reported across GLM4, GPT-3.5-turbo, Mistral-7B:MetaMATH, LLaMA-3-8B-Instruct, SciGLM-6B)",
            "model_size": null,
            "reasoning_method_name": "MCTS* (value / Process-Reward guided Monte Carlo Tree Search)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "MATH (math reasoning benchmark; token-budgeted tree-search comparisons in paper)",
            "task_description": "college-level mathematical reasoning problems (multi-step math problem solving)",
            "performance_metric": "accuracy (%)",
            "performance_value": 48.5,
            "comparison_target_method": "Self-Consistency (SC)",
            "performance_difference": 6.0,
            "statistical_significance": true,
            "analysis_notes": "ReST-MCTS* uses inferred per-step process rewards to guide tree search and selects high-quality traces for self-training. The paper reports that after iterative self-training the MCTS*-based verifier+search achieves 48.5% on MATH, significantly above Self-Consistency's 42.5%; authors attribute improvements to (1) higher-quality, per-step verification (PRM) that filters false-positive traces, and (2) the tree-search exploration that finds diverse correct reasoning paths that single-sequence CoT often misses.",
            "ablation_study_present": null,
            "uuid": "e6541.0",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Consistency (SC)",
            "name_full": "Self-Consistency over Chain-of-Thought (multiple CoT samples, majority-vote answer selection)",
            "brief_description": "An ensemble-style decoding strategy that samples multiple chain-of-thought (CoT) traces and selects the final answer that appears most frequently.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLM backbones (evaluated in token-budgeted comparisons)",
            "model_size": null,
            "reasoning_method_name": "Self-Consistency CoT",
            "reasoning_method_type": "ensemble (sampling + majority vote)",
            "reasoning_style_diversity": "diverse (multiple sampled traces aggregated)",
            "benchmark_name": "MATH",
            "task_description": "multi-step mathematical problem solving",
            "performance_metric": "accuracy (%)",
            "performance_value": 42.5,
            "comparison_target_method": "ReST-MCTS*",
            "performance_difference": -6.0,
            "statistical_significance": true,
            "analysis_notes": "Self-Consistency samples many CoT traces and aggregates answers; authors observe that while SC is strong, it can require more token budget than MCTS* and can be outperformed when better per-step verification (PRM) and structured search produce higher-quality traces.",
            "ablation_study_present": null,
            "uuid": "e6541.1",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReST-MCTS* (SciEval / GLM4)",
            "name_full": "ReST-MCTS* evaluated with GLM4 on SciEval",
            "brief_description": "Application of the ReST-MCTS* tree-search + PRM pipeline evaluated on SciEval; paper reports an overall accuracy for GLM4 using ReST-MCTS*.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM4",
            "model_size": null,
            "reasoning_method_name": "MCTS* (PRM-guided tree search)",
            "reasoning_method_type": "tree-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "SciEval",
            "task_description": "multi-part scientific reasoning / exam-style scientific problems",
            "performance_metric": "accuracy (%)",
            "performance_value": 79.87,
            "comparison_target_method": "CoT, ToT and other baselines (same experimental suite)",
            "performance_difference": null,
            "statistical_significance": true,
            "analysis_notes": "Paper reports ReST-MCTS* attains 79.87% (GLM4) on SciEval and likewise outperforms baselines for GPT-3.5-turbo (62.31%), concluding that PRM-guided tree search improves accuracy on science benchmarks though on some simple single-step subsets CoT/Self-Consistency may already be competitive.",
            "ablation_study_present": null,
            "uuid": "e6541.2",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Process Reward Model (PRM)",
            "name_full": "Process Reward Model (per-step value predictor used to evaluate partial solutions)",
            "brief_description": "A model trained to predict a quality/value score for partial solutions (per intermediate reasoning step) used to guide tree search and to filter traces for self-training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "initially finetuned ChatGLM3-6B and Mistral-7B variants (value-model backbone)",
            "model_size": "6B / 7B (ChatGLM3-6B, Mistral-7B)",
            "reasoning_method_name": "Process Reward Model (PRM)",
            "reasoning_method_type": "per-step value prediction / verifier",
            "reasoning_style_diversity": "single-style (per-step verification applied across traces) ",
            "benchmark_name": "value-model evaluation set (science+math partial-solution set; test split ~14k samples)",
            "task_description": "predict continuous quality value v_k for partial solutions (within tolerance 0.1) used to guide MCTS*",
            "performance_metric": "value prediction accuracy (|predicted - target| &lt; 0.1)",
            "performance_value": 69.3,
            "comparison_target_method": "MATH-SHEPHERD and outcome reward models (ORM) in paper comparisons",
            "performance_difference": null,
            "statistical_significance": true,
            "analysis_notes": "Initial PRM trained without human per-step labels via inferred values from verified search trees achieves 69.3% test accuracy (tolerance 0.1), and authors report that PRM-based guidance yields better trace quality than alternative automated PRM generation (e.g., MATH-SHEPHERD) and ORMs for filtering false-positive full traces.",
            "ablation_study_present": null,
            "uuid": "e6541.3",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Best-of-N (BoN) / ORM+BoN",
            "name_full": "Best-of-N selection (optionally using an Outcome Reward Model to score full traces)",
            "brief_description": "Sampling multiple complete reasoning traces and selecting the trace with highest outcome-reward model score (ORM+BoN) or simply the top final-answer choice (BoN).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various LLMs used as generator/verifier in baselines",
            "model_size": null,
            "reasoning_method_name": "Best-of-N (with ORM scoring in some baselines)",
            "reasoning_method_type": "ensemble / selection-based",
            "reasoning_style_diversity": "mixed (many sampled complete traces but individual traces are single-sequence style)",
            "benchmark_name": "MATH, SciBench (token-budgeted comparisons)",
            "task_description": "multi-step reasoning with selection of best-scored full trace",
            "performance_metric": "accuracy (%) and token-efficiency",
            "performance_value": null,
            "comparison_target_method": "ReST-MCTS* (token-efficiency and final accuracy comparisons)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper reports that ORM+BoN and plain BoN baselines can be competitive but are less token-efficient than MCTS*; also that selecting on final-outcome rewards (ORM) can keep false-positive traces because correct final answer can arise from incorrect intermediate steps â€” motivating PRM per-step verification.",
            "ablation_study_present": null,
            "uuid": "e6541.4",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Tree-of-Thoughts (ToT)",
            "name_full": "Tree-of-Thoughts (ToT; planning-based tree search over coherent units of thought)",
            "brief_description": "A planning-style reasoning method that builds a tree over intermediate 'thought' states and explores multiple paths (often implemented with DFS/BFS style search).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "evaluated with GLM4, GPT-3.5-turbo, LLaMA2-13B-Chat in paper comparisons",
            "model_size": "various",
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search (planning)",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "SciBench (college-level scientific reasoning) and other science/math benchmarks",
            "task_description": "multi-path deliberative problem solving (scientific reasoning questions)",
            "performance_metric": "accuracy (%)",
            "performance_value": null,
            "comparison_target_method": "ReST-MCTS*",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "ToT is a tree-structured baseline; paper notes ToT sometimes performs well and in some subjects can even surpass ReST-MCTS*, highlighting that when the value model guidance is less effective or when the base policy is weak, ToT or simpler strategies may be competitive. ToT explores multiple reasoning styles (paths) explicitly whereas CoT is a single sequential style.",
            "ablation_study_present": null,
            "uuid": "e6541.5",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReST EM",
            "name_full": "ReST EM (reinforced self-training EM-style baseline referenced in paper)",
            "brief_description": "A prior self-training approach that uses outcome reward (final-answer correctness) to select generated traces for iterative policy fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "evaluated with LLaMA-3-8B-Instruct, Mistral-7B:MetaMATH, SciGLM-6B in the paper",
            "model_size": "various (8B, 7B, 6B)",
            "reasoning_method_name": "Outcome-reward based self-training (ReST EM)",
            "reasoning_method_type": "self-training (generate & filter by final outcome)",
            "reasoning_style_diversity": "single-style selection (positive-only final-solution selection)",
            "benchmark_name": "MATH, GPQA, Diamond, CEval-Hard, SciBench variants (as used in paper Table 2)",
            "task_description": "math / scientific reasoning with iterative self-training",
            "performance_metric": "accuracy (%) (iteration-dependent)",
            "performance_value": null,
            "comparison_target_method": "ReST-MCTS* (self-training paradigm comparison)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Paper reports that ReST-MCTS* consistently outperforms ReST EM across multiple iterations and backbones, arguing that using per-step PRM labels inferred by tree search produces higher-quality training traces than outcome-only selection which allows false-positive intermediate reasoning to persist.",
            "ablation_study_present": null,
            "uuid": "e6541.6",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Rewarding (LLM-as-judge)",
            "name_full": "Self-Rewarding Language Models (LLM-generated outcome rewards / judging without human labels)",
            "brief_description": "A self-training variant where the LLM itself judges final answers or generates reward signals (outcome reward), used as a baseline in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "evaluated with LLaMA-3-8B-Instruct, Mistral-7B:MetaMATH, SciGLM-6B",
            "model_size": "various",
            "reasoning_method_name": "Self-Rewarding (LLM-as-judge outcome reward)",
            "reasoning_method_type": "self-training / LLM-judged outcome reward",
            "reasoning_style_diversity": "single-style (outcome judged full traces)",
            "benchmark_name": "MATH and science benchmarks used in self-training experiments",
            "task_description": "math/scientific reasoning iterative self-training",
            "performance_metric": "accuracy (%) across iterations",
            "performance_value": null,
            "comparison_target_method": "ReST-MCTS*",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "The paper finds ReST-MCTS* outperforms Self-Rewarding in successive self-training iterations, attributing gains to per-step value signals (PRM) inferred via tree search that better filter out false-positive traces than LLM-generated outcome-only judgments.",
            "ablation_study_present": null,
            "uuid": "e6541.7",
            "source_info": {
                "paper_title": "ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-Consistency Improves Chain-of-Thought Reasoning in Language Models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chainofthought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "MATH-SHEPHERD: A label-free step-by-step verifier for LLMs in mathematical reasoning",
            "rating": 2,
            "sanitized_title": "mathshepherd_a_labelfree_stepbystep_verifier_for_llms_in_mathematical_reasoning"
        },
        {
            "paper_title": "Self-Rewarding Language Models",
            "rating": 1,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "ReST EM (Reinforced Self-Training EM-style approaches)",
            "rating": 2,
            "sanitized_title": "rest_em_reinforced_selftraining_emstyle_approaches"
        }
    ],
    "cost": 0.02287175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ReST-MCTS * : LLM Self-Training via Process Reward Guided Tree Search
18 Nov 2024</p>
<p>Dan Zhang 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Sining Zhoubian 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Ziniu Hu 
California Institute of Technology</p>
<p>Yisong Yue 
California Institute of Technology</p>
<p>Yuxiao Dong 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>Jie Tang 
The Knowledge Engineering Group (KEG)
Tsinghua University</p>
<p>ReST-MCTS * : LLM Self-Training via Process Reward Guided Tree Search
18 Nov 2024051E3FF667AB2E7DA66CF1DCFE2C74FBarXiv:2406.03816v3[cs.CL]
Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data.This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning).In this paper, we develop a reinforced self-training approach, called ReST-MCTS * , based on integrating process reward guidance with tree search MCTS * for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models.ReST-MCTS * circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS * is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer.These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training.We first show that the treesearch policy in ReST-MCTS * achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget.We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReST EM and Self-Rewarding LM.We release all code at https://github.com/THUDM/ReST-MCTS.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are mostly trained on human-generated data.But as we approach the point where most available high-quality human-produced text on the web has been crawled and used for LLM training [1], the research focus has shifted towards using LLM-generated content to conduct self-training [2; 3; 4; 5; 6; 7].Similar to most Reinforcement Learning (RL) problems, LLM self-training requires a reward signal.Most existing reinforced self-improvement approaches (e.g., STaR [4], RFT [5], ReST EM [6], V-STaR [7]) assume to have access to a ground-truth reward model (labels from supervised dataset, or a pre-trained reward model).These approaches use an LLM to generate multiple samples for each question, and assume the one that leads to high reward (correct solution) is the high-quality sample, and later train on these samples (hence self-training).Such procedures can be effective in improving LLM performance, in some cases solving reasoning tasks that the base LLM cannot otherwise solve [8; 9; 10].However, a key limitation of the above procedure is that even if a reasoning trace results in a correct solution, it does not necessarily imply that the entire trace is accurate.LLMs often generate Table 1: Key differences between existing self-improvement methods and our approach.Train refers to whether to train a reward model.</p>
<p>Method</p>
<p>Reasoning Policy Reward Guidance</p>
<p>Method Value Label Train</p>
<p>STaR [4] CoT+Reflexion Final outcome reward annotated by ground-truth answer ReST EM [6] / RFT [5] / RPO [11] CoT</p>
<p>Verify Step-by-Step [1] Best-of-N Per-step process reward annotated by human MATH-SHEPHERD [12] / pDPO [13] Best-of-N Per-step process reward inferred from random rollout TS-LLM [14] MCTS Per-step process reward inferred from TD-Î» [15] V-STaR [7] CoT Final outcome reward generated by multi-iteration LLMs Self-Rewarding [16] CoT Final outcome reward generated and judged by LLMs</p>
<p>ReST-MCTS * (Ours) MCTS * Per-step process reward inferred from tree search (MCTS * ) Multi-Iter wrong or useless intermediate reasoning steps, while still finding the correct solution by chance [17].</p>
<p>Consequently, a self-training dataset can often contain many false positives -intermediate reasoning traces or plans are incorrect, but the final output is correct -which limits the final performance of LLM fine-tuning for complex reasoning tasks [18; 19].One way to tackle this issue is to use a value function or reward model to verify reasoning traces for correctness (which then serves as a learning signal for self-training) [1; 12].However, training a reliable reward model to verify every step in a reasoning trace generally depends on dense human-generated annotations (per reasoning step) [1], which does not scale well.Our research aims to address this gap by developing a novel approach that automates the acquisition of reliable reasoning traces while effectively utilizing reward signals for verification purposes.Our key research question is: How can we automatically acquire high-quality reasoning traces and effectively process reward signals for verification and LLM self-training?</p>
<p>In this paper, we propose ReST-MCTS * , a framework for training LLMs using model-based RL training.Our proposed approach utilizes a modified Monte Carlo Tree Search (MCTS) algorithm as the reasoning policy, denoted MCTS * , guided by a trained per-step process reward (value) model.A key aspect of our method is being able to automatically generate per-step labels for training per-step reward models, by performing a sufficient number of rollouts.This labeling process effectively filters out the subset of samples with the highest quality, without requiring additional human intervention.Table 1 summarizes the key distinctions between our approach and previous approaches.We validate experimentally that ReST-MCTS * outperforms prior work in discovering good reasoning traces, such as Self-Consistency (SC) and Best-of-N (BoN) under the same search budget on the SciBench [20] and MATH [21] benchmarks, which consequently leads to improved self-training.</p>
<p>To summarize, our contributions are:</p>
<p>â€¢ We propose ReST-MCTS * , a self-training approach that generates process rewards searched by MCTS * .A key step is to automatically annotate the process reward of each intermediate node via sufficient times of rollouts, using MCTS * .We validate multiple reasoning benchmarks and find that ReST-MCTS * outperforms existing self-training approaches (e.g., ReST EM and Self-Rewarding) as shown in Table 2 and reasoning policies (e.g., CoT and ToT) as shown in Table 4.</p>
<p>â€¢ The reward generator in ReST-MCTS * leads to a higher-quality process reward model compared to previous process reward generation techniques, e.g., MATH-SHEPHERD, as shown in Table 3.</p>
<p>â€¢ Given the same search budget, the search algorithm (MCTS * ) in ReST-MCTS * achieves higher accuracy than Self-Consistency and Best-of-N, as shown in Figure 2.</p>
<p>Background on Reasoning &amp; Self-Training</p>
<p>We follow the standard setup in LLM-based reasoning.We start with a policy, denoted by Ï€, that is instantiated using a base LLM.Given an input problem Q, in the simplest case, Ï€ can generate an output sequence, or trace, of reasoning steps (s 1 , s 2 , â€¢ â€¢ â€¢ , s K ) âˆ¼ Ï€(â€¢|Q) by autoregressively predicting the next token.For simplicity, we assume a reasoning step comprises a single sentence (which itself comprises multiple tokens).We also assume the last output s K is the final step.LLMs can also be prompted or conditioned to bias the generation along certain traces.For a prompt c, we can write the policy as Ï€(â€¢|Q, c).This idea was most famously used in chain-of-thought (CoT) [22].</p>
<p>Self-Consistency (SC).Self-Consistency [23] samples multiple reasoning traces from Ï€ and chooses the final answer that appears most frequently.</p>
<p>Tree-Search &amp; Value Function.Another idea is to use tree-structured reasoning traces [24; 14], that branch from intermediate reasoning steps.One key issue in using a so-called tree-search reasoning algorithm is the need to have a value function to guide the otherwise combinatorially large search process [14].Two common value functions include Outcome Reward Models (ORMs) [25], which are trained only on the correctness of the final answer, and Process Reward Models (PRMs) [1], which are trained on the correctness of each reasoning step.We assume r s k is the PRM's output sigmoid score at k-th step.Our ReST-MCTS * approach uses tree-search to automatically learn a good PRM.</p>
<p>Best-of-N.As an alternative to Self-Consistency, one can also use a learned value function (PRM or ORM) to select the reasoning trace with the highest value [1].</p>
<p>Self-Training.At a high level, there are two steps to self-training [6; 12].The first step is generation, where we sample multiple reasoning traces using Ï€ (in our case, tree-structured traces).The second step is improvement, where a learning signal is constructed on the reasoning traces, which is then used to fine-tune Ï€.The process can repeat for multiple iterations.</p>
<p>Limitation of Prior Works.The main challenge in doing reliable self-training is the construction of a useful learning signal.Ideally, one would want a dense learning signal on the correctness of every intermediate reasoning step, which is given by a PRM.Otherwise, with sparse learning signals, one suffers from a credit assignment similar to that in reinforcement learning.Historically, the main challenge with learning a PRM is the lack of supervised annotations per reasoning step.This is the principal challenge that our ReST-MCTS * approach seeks to overcome.We describe detailed preliminaries in Appendix A.</p>
<p>3 The ReST-MCTS * Method</p>
<p>Our approach, ReST-MCTS * , is outlined in Figure 1 and developed using four main components.</p>
<p>â€¢ MCTS * which performs a tree search with sufficient rollout time under the guidance of the PRM.</p>
<p>â€¢ Process Reward Model (PRM) which evaluates any partial solution's quality and guides MCTS * .</p>
<p>â€¢ Policy Model which generates multiple intermediate reasoning steps for each question.</p>
<p>â€¢ LLM Self-Training, which uses MCTS * to collect reasoning traces, trains policy model on positive samples, and trains process reward model on all generated traces.</p>
<p>Search-based Reasoning Policy for LLM</p>
<p>Value v k for a Partial Solution.The value (process) reward v k of the partial solution p k = [s 1 , s 2 , â€¢ â€¢ â€¢ , s k ] should satisfy the following basic qualities:</p>
<p>â€¢ Limited range: v k is constrained within a specific range.This restriction ensures that the values of v k are bounded and do not exceed a certain limit.â€¢ Reflecting probability of correctness: v k reflects the probability that a partial solution is a complete and correct answer.Higher values of v k indicate better quality or a higher likelihood of being closer to a correct answer.</p>
<p>â€¢ Reflecting correctness and contribution of solution steps: v k incorporates both the correctness and contribution of each solution step.When starting from a partial solution, a correct next step should result in a higher v k compared to false ones.Additionally, a step that makes more correct deductions toward the final answer should lead to a higher v k value.This property ensures that v k captures the incremental progress made towards the correct solution and rewards steps that contribute to the overall correctness of the solution.</p>
<p>Reasoning Distance m k for a Partial Solution.To estimate the progress of a solution step, we define the reasoning distance m k of p k as the minimum reasoning steps a policy model requires to reach the correct answer, starting from p k .Reasoning distance reflects the progress made as well as the difficulty for a policy to figure out a correct answer based on current steps, thus it can be further
{(ð‘„ ! , ð‘¦ ! )| !"# $ } (ð‘„ ! , ðœ ! " ) (ð‘„ # , ðœ # " ) â€¢â€¢â€¢ PRM ð‘  ! ð‘  # ð‘  $,!ð‘ ð‘  ! âˆ¼ ðœ‹(â€¢ |ð‘„ ! ) ð‘  # âˆ¼ ðœ‹(â€¢ |ð‘„ ! , ð‘  ! ) ð´ ! = ð‘¦ ! ð´ # â‰  ð‘¦ ! MCTS * ð‘  $ âˆ¼ ðœ‹(â€¢ |ð‘„ ! , ð‘  ! , ð‘  # )</p>
<p>Policy Model</p>
<p>-,
621 DS 0 â‡¡ ! â‡¡S 0 DG ! D Gi(Aj =a â‡¤ ) â‡¡S i 1 ! â‡¡S i ,(13)
where i = 1 for RFT and i 1 for STaR, ReST EM , V-STaR, and Self-Rewarding.</p>
<p>622</p>
<p>Improvement.The practical way to accomplish reasoning tasks on DS 0 is supervised fine-tuning</p>
<p>623</p>
<p>(SFT) that trains a policy model by minimizing the negative log-likelihood loss on the training dataset:
624 LSFT(â‡¡) = E (Q,s)2DS 0 â‡¥ T X t=1 logâ‡¡(st|s&lt;t, Q) â‡¤ . (14)
Recent offline preference learning methods replace LLM verifiers (before being trained on LLM 625 generator and binary classification) with DPO [36].The training DPO objective for a verifier â‡¡V is 626 described as follows:
627 LDPO(â‡¡V ; â‡¡S 0 ) = E (Q,s + ,s )â‡ DVER [log (r(Q, s + ) r(Q, s ))],(15)
where is the logistic function.631
ws k = 1 vk 1 mk + 1 (1 2rs k ), k = 1, 2, â€¢ â€¢ â€¢ (16)
And we know that rs k 2 [0, 1].Now, let's examine the maximum possible value of the term</p>
<p>632</p>
<p>(1 2rs k ).Since rs k 2 [0, 1], the maximum value of (1 2rs k ) occurs when rs k = 0.In this case,</p>
<p>633</p>
<p>(1 2rs k ) = 1.Therefore, we can conclude that 1 ï£¿ (1 2rs k ) ï£¿ 1.</p>
<p>634</p>
<p>Next, let's consider the denominator, (mk +1).Since mk = K k, and K &gt;= k, we have mk &gt;= 0 635 and mk + 1 &gt;= 1.Therefore, we can conclude that (mk + 1) &gt;= 1.</p>
<p>636</p>
<p>Combining these results, we can rewrite the weighted reward as: problem Q using the above-mentioned reasoning strategies:
637 ws k = 1 vk 1 mk + 1 (1 2rs k ) ï£¿ |1 vk 1| â€¢ |1 2rs k | &lt;= |1 vk 1|(17570 DS 0 = {(Q j 1 , s j 1 )| N j=1 , â€¢ â€¢ â€¢ , (Q j M , s j M )| N j=1 }.(5)
As shown in Table 1, STaR [58], RFT [56], ReST EM [39], V-STaR [20], and Self-Rewarding [55] 571 adopt CoT prompting.</p>
<p>Step-by-step [29] and MATH-SHEPHERD [47] leverage the best-of-N 572 selection as a reasoning evaluation strategy.TS-LLM [14] utilizes MCTS as a reasoning policy to
LRL = EQ2D S0 [E s2â‡¡(s|Q) [r(Q, s)]].(6)
Recent works [29; 47], through PRMs and ORMs, model the objective of reasoning as a search to 581 find the highest cumulative reward trajectory s to a problem Q and infer the final answer A.</p>
<p>582</p>
<p>â€¢ ORM (D â‡¥ S !R) is trained with a cross-entropy loss:
583 LORM = Aslog rs + (1 As) log(1 rs), (7)
where As is the golden answer (As = 1 if s is correct else As = 0) and rs is the ORM's output 584 sigmoid score.STaR [58], RFT [56], and ReST EM [39] consider the outcome reward as value 585 label As compared with ground truth answer.In V-STaR [20], its outcome reward is generated by 586 multi-iteration LLMs, and reward r is defined via verifier â‡¡V and LLM generator â‡¡S 0 as follows:
587 r(Q, s) = log â‡¡V (s|Q) â‡¡S 0 (s|Q) . (8)
is the hyper-parameter that controls the proximity of the reference policy â‡¡S 0 .Different from V-588 STaR, the outcome rewards r in self-rewarding [55] is generated through LLM-as-a-Judge prompting 589 using dataset DS 0 and policy model â‡¡S 0 .In both V-STaR and self-rewarding, collected DVER is built
590
for training verifiers as follows:
591 DVER = {(Qj, s + j,1 , s j,1 ), â€¢ â€¢ â€¢ , (Qj, s + j,d , s j,d )} N j=1 ,(9)
d is the number of preference pairs.However, previous works [29; 47] suggested PRMs demonstrate 592 better supervision than ORMs among false positive solutions and provide more reliable feedback.</p>
<p>593</p>
<p>â€¢ PRM (D â‡¥ S !R + ) is trained with:
594 LPRM = K X i=1 As k log rs k + (1 As k ) log(1 rs k ),(10)
where As k is the golden answer (As k = 1 if sk is correct else As k = 0) of sk and rs k is the PRM's
A HE sk = â‡¢ 1, 9Aj 2 A â‡¤ , Aj = a â‡¤ 0, Otherwise ,(11)</p>
<p>602</p>
<p>and
A SE sk = âŒƒ N j=1 I(Aj = a â‡¤ ) N . (12)
16 used to evaluate the quality of p k .However, we point out that m k can not be directly calculated.It is more like a hidden variable that can be estimated by performing simulations or trace sampling starting from p k and finding the actual minimum steps used to discover the correct answer.
ð· 1 ! ð· 1 !</p>
<p>Weighted Reward w s k for a Single</p>
<p>Step.Based on the desired qualities for evaluating partial solutions, we introduce the concept of a weighted reward to reflect the quality of the current step s k , denoted as w s k .Based on the common PRM reward r s k , w s k further incorporates the reasoning distance m k as a weight factor, reflecting the incremental progress s k makes.</p>
<p>Representations for Quality Value and Weighted Reward.To determine the quality value v k of a partial solution at step k, we incorporate the previous quality value and the weighted reward of the current step.By considering the previous quality value, we account for the cumulative progress and correctness achieved up to the preceding step.Therefore, the v k can be iteratively updated as:
v k = 0, k = 0 max(v kâˆ’1 + w s k , 0), else(1)
The weighted reward w s k of the current step provides a measure of the quality and contribution of that specific step towards the overall solution.Based on m k (where m k = K âˆ’k and K is the total number of reasoning steps of a solution s), previous quality value v kâˆ’1 , and r s k in MATH-SHEPHERD [12], we can update the definition of the weighted reward w s k iteratively as follows:
w s k = 1 âˆ’ v kâˆ’1 m k + 1 (1 âˆ’ 2r s k ), k = 1, 2, â€¢ â€¢ â€¢(2)
As k increases, m k decreases, indicating that fewer reasoning steps are needed to reach the correct answer.This leads to a higher weight placed on the weighted reward of the current step.We can also derive that w s k and v k satisfy the expected boundedness shown in the theorem below.Theorem 1 (Boundedness of w s k and v k ).If r s k is a sigmoid score ranged between [0, 1], then w s k and v k defined as above satisfy following boundedness: ).An overall pseudo-code for MCTS * is presented in Algorithm 2.
w s k â‰¤ 1 âˆ’ v kâˆ’1 , v k âˆˆ [0, 1].</p>
<p>Self-Training Pipeline</p>
<p>As shown in Figure 1, based on the proposed tree search algorithm MCTS * , we perform selfimprovement on the reasoning policy and process reward model.After initialization of the policy Ï€ and process reward model V Î¸ , we iteratively employ them and utilize the search tree T q generated in the process to generate high-quality solutions for specific science or math questions and conduct a self-improvement process, called ReST-MCTS * .Our work draws inspiration from the MuZero [20] framework and applies it to the training of LLMs which we term "MuZero-style learning of LLMs".</p>
<p>Instruction Generation.In this stage, initialization starts from an original dataset D 0 for the training process reward model V Î¸ .</p>
<p>â€¢ Collect process reward for process reward model.The extraction of new value data is relatively more complex, we derive the target quality value of partial solutions of every tree node near a correct reasoning path on the pruned search tree T â€² q .We first calculate m k for every tree node C that is on at least one correct reasoning trace (including the root) according to its minimum reasoning steps required to get to a correct answer in T â€² q .Then, we use the hard estimation in Eq. (11) in [12] to calculate r s k , i.e. r s k = 1 âˆ’ r HE s k , which means a reasoning step is considered correct if it can reach a correct answer in T â€² q .Using m k and r s k , we are able to derive the value of the partial solution of every node on or near one correct reasoning trace.For each node C (with partial solution
p C = [s 1 , s 2 , â€¢ â€¢ â€¢ , s kâˆ’1 ]
) on at least one correct trace and a relevant forward step s k , we can derive the value v k using Eq. ( 1) and weighted reward w k using Eq. ( 2), with m k set to the same as m kâˆ’1 if r HE s k = 0 in Eq. (11).A concrete and detailed example of this inferring process is shown in Figure 3.We update all these rewards and values starting from the root and collect all (Q, p, v) pairs to form D Vi in i-th iteration, which is used for training a process reward model in the next iteration.</p>
<p>â€¢ Collect reasoning traces for policy model.As shown in Figure 4, the search process produces a search tree T q , consisting of multiple reasoning traces.We first prune all the unfinished branches (branches that do not reach a final answer).Then we verify other traces' final answers acquired in the tree search according to their correctness through simple string matching or LLM judging and select the correct solutions.These verified reasoning traces, as D Gi(Aj =a * )| N j=1 (where N is the number of sampling solutions, A j is the j-th solution, and a * is the final correct answer) in i-th iteration, are then used for extracting new training data for policy self-improvement.This process is followed by Eq. ( 13) (i â‰¥ 1) to execute the policy self-training.</p>
<p>Mutual Self-training for Process Reward Model and Policy Model.Compared to previous work like ReST EM  [6], which only concerns self-training for the policy and demonstrates that the policy can improve by iteratively generating new traces and learning from the high-reward ones generated by itself, our work simultaneously improves the process reward model and policy model self-training.With the process reward model's training set D V0 initialized and new problem set D G given, we can start the iterative self-training process upon V Î¸ and Ï€.We use Ï€ to perform MCTS * and generate solutions for D G , with implement details illustrated in Section 3.1.In the i-th  for j = 1 to N do
(i = 1, 2, â€¢ â€¢ â€¢ ) iteration, we train V Î¸ with D Viâˆ’1 to obtain V i andÏ€ S0 â† SFT(Ï€, D S0 ) // fine-tune generator 2: D V0 â† generate_value_data(D 0 , w, v) // initialize train set for value model 3: V 0 â† train_value_model(V Î¸ , D V0 ) // initialize value model 4: for i = 1 to T do 5: D Gi â† generate_policy_data(Ï€ Siâˆ’1 , V iâˆ’1 guided MCTS * , D G , N ) //V i â† train_value_model(V iâˆ’1 , D Vi ) // self-training value model 12: end for Output: Ï€ S T , V T</p>
<p>Experiments</p>
<p>We validate ReST-MCTS * from three perspectives:</p>
<p>â€¢ Self-Training approaches which use generated samples and evaluated for multiple iterations, such as ReST EM and Self-Rewarding, on in-distribution and out-of-distribution benchmarks under three LLM backbones, as shown in Table 2. ReST-MCTS * outperforms existing approaches in each iteration and continuously self-improves with the data it generates.</p>
<p>â€¢ Process reward models which are compared with the state-of-the-art techniques, such as MATH-SHEPHERD (MS) and SC + MS on GSM8K and MATH500, as shown in Table 3. Results indicate that the ReST-MCTS * learns a good PRM and our reward model implements higher accuracy.</p>
<p>â€¢ Tree-Search policy which are compared on college-level scientific reasoning benchmark under three LLMs, such as CoT and ToT, as shown in Table 4.We also evaluated under the same search budget on MATH and SciBench, such as SC and Best-of-N, as shown in Figure 2. Results show the ReST-MCTS * significantly outperforms other baselines despite insufficient budget.</p>
<p>Initialization of Value Model</p>
<p>To obtain accurate feedback from the environment, we build the value model's initial train set D V0 from a set of selected science or math questions D 0 using process reward (value) inference, with no human labeling process required.Then, we finetune the ChatGLM3-6B [27; 28] and Mistral-7B [29] model on this dataset, respectively, obtaining initial value models that, as variants of PRM, guide the LLM tree search for higher-quality solutions upon both math and science questions.</p>
<p>Fine-grained dataset for science and math.Aiming to gather value train data for science, we integrate questions of a lean science dataset D sci within SciInstruct [10] into D 0 .This dataset consists of 11,554 questions, where each question is paired with a correct step-by-step solution.</p>
<p>For each question
q (i) (i = 1, 2, â€¢ â€¢ â€¢ , N ) and corresponding solution s (i) = s (i) 1,2,â€¢â€¢â€¢ ,Ki in D sci , we extract all partial solutions to form samples d (i) k = <a href="k = 1, 2, â€¢ â€¢ â€¢ , K i">q (i) , s (i) 1,2,â€¢â€¢â€¢ ,k (p (i) k )</a>.
To make the value model distinguish false steps, we also employ a LLM policy (ChatGLM2) that is basically incompetent for reasoning tasks of this difficulty to generate single steps s
(i) â€² k+1 given q (i) and p (i) k , obtaining new partial solutions p (i) â€² k+1 = [s (i) 1,2,â€¢â€¢â€¢ ,k , s (i) â€² k+1 ] and new samples d (i) k,j = <a href="j = 1, 2, 3">q (i) , s (i) 1,2,â€¢â€¢â€¢ ,k , s (i) â€²
k+1,j </a>.For simplicity, the generated steps are regarded as incorrect.In total, we collect 473.4k samples for training the initial value model.Afterward, we derive target quality values for all samples d   In order to thoroughly examine the influence of ReST-MCTS * self-training on varied backbones, we execute 2 iterations of self-training and compare two representative self-training approaches, ReST EM , which compares outcome reward with ground-truth answer, and Self-Rewarding, which judges outcome reward by LLMs, upon 3 different base models, namely LLaMA-3-8B-Instruct [30], Mistral-7B: MetaMATH [29; 31] and SciGLM-6B [10].Primary results are shown in Table 2.</p>
<p>Concerning the dataset for sample generation, since we are primarily interested in the continuous improvement ability of ReST-MCTS * in a specific domain, we mainly include math questions in the dataset.For simplicity, we use the same dataset D G in each iteration.It involves questions selected from a train set of well-known benchmarks including MATH, GSM8K, and TheoremQA [32].With the policy and value model trained simultaneously on samples generated from D G , we observe that our self-training paradigm enables continuous enhancement of the capabilities of both models on in-distribution and out-of-distribution benchmarks, regardless of which backbone is used.</p>
<p>â€¢ Iterative performance improvement on policy model.Previous LLM self-training approaches mostly rely on the generating responses of LLM and assume each question with the correct solution is a high-quality sample while the intermediate reasoning steps are wrong or useless in many cases.Therefore, we compare the ReST-MCTS * with recent self-training paradigms by generating new samples under different reward (value) supervision strategies.For ReST EM and Self-Rewarding, the default sampling strategy is generating CoT data, with generated data refined according to ground truth or reward provided by the policy, respectively.In comparison, ReST-MCTS * generates data samples via MCTS * , with data refined referring to quality value and ground truth.The results in Table 2 show that all three backbones can be continuously self-improved by data generated by itself, using ReST-MCTS * as a paradigm.ReST-MCTS * significantly outperforms previous self-training methods ReST EM and Self-Rewarding basically in each iteration.This means the ReST-MCTS * can screen out self-generated data of higher quality for better self-improvement.</p>
<p>â€¢ Iterative performance improvement on reward model.We also compare how our iterative trained policy and value model can improve the overall search results under the same token usage on the test set of MATH [33].See implementation details in Appendix E.3.We show results in Figure 2 (a), where ReST-MCTS * (Iter #1) greatly outperforms most baselines but does not completely surpass Self-Consistency.In comparison, after more iterations of self-training, verification based on the enhanced value model basically outperforms Self-Consistency on every point, achieving the highest accuracy of 48.5% that significantly exceeds the 42.5% of Self-Consistency.This indicates the effectiveness of our self-training pipeline.Our main hypothesis in this paper is that a better search policy getting higher-quality traces can improve self-training.In this section, we mainly focus on whether our process reward guided MCTS * can gain improvement to get better samples over different reasoning tasks.We first evaluate the effectiveness of the value model itself standalone in Table 3 and then evaluate the performance of different reasoning policies in Table 4.</p>
<p>Performance Comparison of Various Verification Models.As [1] suggested, different value models or reward models vary in accuracy and fineness.We perform tests on the questions of the GSM8K and MATH500 using multiple reward models and verification methods.It is worth noting that we include the same experiment settings of MATH-SHEPHERD (MS) [12] as a comparison since it also adopts an automatic train data generation method for reward models.For SC+ReST-MCTS  [34] in Tabel 4 and SciEval [35] in Table 8.All benchmark setups are illustrated in Appendix E.2.For the backbone of models, large-scale models GLM4 and GPT-3.5-turbo(both API), as well as a small-scale model LLaMA2-13B-Chat are included.As shown in Table 4, with the experiment repeated for 2 times, we report the average accuracy scores (%) of 3 methods on 10 subjects.Concerning overall accuracy, the ReST-MCTS * outperforms other baselines for all 3 models, with GLM4 improved over 4.0% and GPT-3.5-turboover 3.1%.On specific subjects such as chemmc, quan, and stat, the ReST-MCTS * achieves significant improvement over 5.0%, indicating its great potential in discovering accurate solutions.Besides, we notice that our ToT baseline also performs well on many subjects, sometimes even surpassing ReST-MCTS * .This reflects that our value model can provide appropriate guidance for tree-search-based methods.We also discovered that for LLaMA2-13B-Chat, the improvement is not very prominent.This reveals that small-scale policies may face difficulties when adopting complex tree search approaches since their capability for step-wise inference is relatively low.According to this formula, InstructGPT [51] has achieved remarkable success in optimizing LLMs to align human preferences by utilizing RL from Human Feedback (RLHF) [52].RLAIF then uses AI feedback to extend RL from human feedback [53].Our work aims to propose an LLM self-training method via process rewards guided tree search.</p>
<p>Large Language Model Reasoning</p>
<p>LLM reasoning algorithms include prompt-based chain-of-thought (CoT) [22], planning-based represented by tree-of-thought (ToT) [24].Scientific reasoning has several categories to mine the potential of existing large language models, resulting from different performances for problemsolving.Previous studies have attempted to outperform the direct generation.For example, in this paper [54], an approach for generating solutions in a step-by-step manner is proposed, another model or function is used to select the top-ranked answers, and hallucination is avoided by limiting the output to a narrower set.[55] presents a maieutic prompting inference method, which can generate abductive explanations of various hypotheses explained by recursion, eliminate contradicting candidates, and achieve logically consistent reasoning.Chain-of-thoughts (CoT) [22] imitates the thought process like humans to provide step-by-step solutions given a question.Self-Consistency CoT [23] improves the reliability and Self-Consistency of answers by sampling multiple interpretations from LM and then selecting the final answer that appears most frequently.Tree-of-Thoughts (ToT) [24] further generalizes the CoT methodology by considering multiple different reasoning paths in the tree and exploring coherent units of thought to execute thoughtful decision-making.In our work, we benchmark hard science reasoning tasks against [22; 24; 34; 35].</p>
<p>Conclusion</p>
<p>In this paper, we propose ReST-MCTS * , self-training both policy and process reward model by high-quality samples generated by reward guided tree search.Inferred rewards from the previous iteration are able to refine the process reward model and self-train the policy model with high-quality traces.Experimental results show that the ReST-MCTS * outperforms other self-training paradigms and achieves higher accuracy than previous reasoning baselines under the same search budget.</p>
<p>Limitation: We discussed limitation in detail at Section H in Appendix.In summary, we need to show the ReST-MCTS * can generalize to other reasoning tasks outside of math (like coding, agent, etc); and tasks without ground-truth (dialogue, SWE-Bench [56], etc).We also need to scale up the proposed value model and further improve the data filtering techniques.One potential idea is to incorporate online RL algorithms that can help perform better self-training for value models and policy models.</p>
<p>Appendix Table of Contents</p>
<p>A Preliminaries</p>
<p>In this section, we briefly describe LLM reasoning, reward verification, and LLM self-training.The definitions for notations are in Table 5 and model comparison in Figure 6.</p>
<p>A.1 LLM Reasoning</p>
<p>The use of reasoning approaches can significantly improve LLM problem-solving abilities [10].Given a policy model, Ï€ (an autoregressive pre-trained language model) and an input problem Q, Ï€ can autoregressive generate an output sequence s = (s 1 , s 2 , â€¢ â€¢ â€¢ , s K ) by predicting the next token.</p>
<p>The conditional probability distribution of generating the complete output sequence is: Any problem can be reasoned by zero-shot prompting, few-shot prompting [57], chain-of-thought (CoT) [22], self-consistency CoT [23] or best-of-N (BoN) selection [1], tree-of-thought (ToT) [24], Monte Carlo tree search (MCTS) [14], graph-of-thought (GoT) [58], amongst other approaches.Generally, recent studies represented by CoT [22] aim to improve the overall performance as follows:
Ï€(s|Q) = K k=1 Ï€(s t |s &lt;t , Q).(3)P Ï€ (A = a * | Q) = E (s0,s1,â€¢â€¢â€¢ ,s K )âˆ¼PÏ€(s|Q) P (A = a * | s 0 , s 1 , â€¢ â€¢ â€¢ , s K , Q) .(4)
We often call each trajectory (s 1 , s 2 , â€¢ â€¢ â€¢ , s K ) a reasoning trace.P (A = a * | s 0 , s 1 . . ., s K , Q) is the probability to get correct answer a * given a problem Q and a reasoning trace s.Given a original training dataset
D = {Q 1 , Q 2 , â€¢ â€¢ â€¢ , Q M }
, a new dataset can be produced by sampling Ï€ N times per problem Q using the above-mentioned reasoning strategies:
D S0 = {(Q j 1 , s j 1 )| N j=1 , â€¢ â€¢ â€¢ , (Q j M , s j M )| N j=1 }.(5)
As shown in Table 1, STaR [4], RFT [5], ReST EM [6], V-STaR [7], and Self-Rewarding [16] adopt CoT prompting.</p>
<p>Step-by-step [1] and MATH-SHEPHERD [12] leverage the best-of-N selection as a reasoning evaluation strategy.TS-LLM [14] utilizes MCTS as a reasoning policy to fully generate traces.Our work similarly seeks a correct reasoning path to maximize the expected cumulative P .</p>
<p>A.2 Reward Verification</p>
<p>In the context of LLMs, we assume that the reasoning trajectory is derived from the policy model Ï€ sampling.The common first step of self-training methods is fine-tuning the base model Ï€ on the original dataset D S0 and obtaining a new generator Ï€ S0 .Besides, a reward r is considered to evaluate the value of the history trace.The objective of reinforcement learning (RL) with reward r(Q, s) is:
L RL = E QâˆˆD S 0 [E sâˆˆÏ€(s|Q) [r(Q, s)]].(6)
Recent works [1; 12], through PRMs and ORMs, model the objective of reasoning as a search to find the highest cumulative reward trajectory s to a problem Q and infer the final answer A.</p>
<p>â€¢ ORM (D Ã— S â†’ R) is trained with a cross-entropy loss:
L ORM = A s log r s + (1 âˆ’ A s ) log(1 âˆ’ r s ),(7)
where A s is the golden answer (A s = 1 if s is correct else A s = 0) and r s is the ORM's output sigmoid score.STaR [4], RFT [5], and ReST EM [6] consider the outcome reward as value label A s compared with ground truth answer.In V-STaR [7], its outcome reward is generated by multi-iteration LLMs, and reward r is defined via verifier Ï€ V and LLM generator Ï€ S0 as follows:
r(Q, s) = Î² log Ï€ V (s|Q) Ï€ S0 (s|Q) .(8)
Î² is the hyper-parameter that controls the proximity of the reference policy Ï€ S0 .Different from V-STaR, the outcome rewards r in self-rewarding [16] is generated through LLM-as-a-Judge prompting using dataset D S0 and policy model Ï€ S0 .In both V-STaR and self-rewarding, collected D VER is built for training verifiers as follows:
D VER = {(Q j , s + j,1 , s âˆ’ j,1 ), â€¢ â€¢ â€¢ , (Q j , s + j,d , s âˆ’ j,d )} N j=1 ,(9)
d is the number of preference pairs.However, previous works [1; 12] suggested PRMs demonstrate better supervision than ORMs among false positive solutions and provide more reliable feedback.</p>
<p>â€¢ PRM (D Ã— S â†’ R + ) is trained with:
L PRM = K i=1 A s k log r s k + (1 âˆ’ A s k ) log(1 âˆ’ r s k ),(10)
where A s k is the golden answer (A
s k = 1 if s k is correct else A s k = 0) of s k andA HE s k = 1, âˆƒA j âˆˆ A * , A j = a * 0, Otherwise ,(11)
and
A SE s k = Î£ N j=1 I(A j = a * ) N .(12)
A * = {A j } N j=1 and N is the number of solutions.To more precisely find a reasoning trajectory with the highest expected reward, RAP [59] utilizes MCTS to estimate the expected future reward via state-action function (Q : S Ã— A âˆ’â†’ R) in traditional RL for each node.However, the decoding process of incorporating MCTS into LLMs is costly, because estimating and updating a state-action function requires a recursive visit, and reward values need to be calculated by LLMs.The concurrent work pDPO [13], though effective, does not take internal accuracy generated by LLMs into account and ignores the number of steps to be generated.In this paper, we integrate process reward guidance with tree search to explore efficient solution space and synthesize high-quality trajectories.</p>
<p>A.3 LLM Self-Training</p>
<p>Generation.Given a new training dataset D G , self-training methods use generator Ï€ S0 to generate reasoning steps s and final answer A per problem Q.In each iteration i (i â‰¥ 1), STaR, RFT, and ReST EM check the generated solutions D Gi with the binary correctness label z and keeps the correct solutions
(A j = a * )| N j=1 as D Gi(Aj =a * )| N j=1 .
Based on the continuous iteration on positive samples, V-STaR and Self-Rewarding keep the correct and incorrect generated solutions per problem Q and train preference data pairs on constructed verifier data D VER with all data D Gi , so the Ï€ V can learn the error patterns produced by a generator in each iteration i.Then, the generator Ï€ Siâˆ’1 , here is Ï€ S0 , is fine-tuned on new generated dataset D Gi(Aj =a * )| N j=1 and again is updated as generator Ï€ Si .This process is continuously running in subsequent iterations.Their iterative process and reward value are as follows:
D S0 Ï€ âˆ’ â†’ Ï€ S0 D G âˆ’ âˆ’ â†’ D Gi(Aj =a * ) Ï€ S iâˆ’1 âˆ’ âˆ’âˆ’âˆ’ â†’ Ï€ Si ,(13)
where i = 1 for RFT and i â‰¥ 1 for STaR, ReST EM , V-STaR, and Self-Rewarding.</p>
<p>Improvement.The practical way to accomplish reasoning tasks on D S0 is supervised fine-tuning (SFT) that trains a policy model by minimizing the negative log-likelihood loss on the training dataset:
L SFT (Ï€) = âˆ’E (Q,s)âˆˆD S 0 T t=1 logÏ€(s t |s &lt;t , Q) .(14)
Recent offline preference learning methods replace LLM verifiers (before being trained on LLM generator and binary classification) with DPO [50].The training DPO objective for a verifier Ï€ V is described as follows:
L DPO (Ï€ V ; Ï€ S0 ) = âˆ’E (Q,s + ,s âˆ’ )âˆ¼DVER [log Ïƒ(r(Q, s + ) âˆ’ r(Q, s âˆ’ ))],(15)
where Ïƒ is the logistic function.</p>
<p>B Deduction Demonstration B.1 Definition of Weighted Value and Quality Value</p>
<p>Weighted Value.Recall the definition of the weighted reward:
w s k = 1 âˆ’ v kâˆ’1 m k + 1 (1 âˆ’ 2r s k ), k = 1, 2, â€¢ â€¢ â€¢(16)
And we know that r s k âˆˆ [0, 1].Now, let's examine the maximum possible value of the term (1 âˆ’ 2r s k ).Since r s k âˆˆ [0, 1], the maximum value of (1 âˆ’ 2r s k ) occurs when r s k = 0.In this case,
(1 âˆ’ 2r s k ) = 1. Therefore, we can conclude that âˆ’1 â‰¤ (1 âˆ’ 2r s k ) â‰¤ 1.
Next, let's consider the denominator, (m k + 1).Since m k = K âˆ’ k, and K â‰¥ k, we have m k â‰¥ 0 and m k + 1 â‰¥ 1.Therefore, we can conclude that (m k + 1) â‰¥ 1.</p>
<p>Combining these results, we can rewrite the weighted reward as follows:
w s k = 1 âˆ’ v kâˆ’1 m k + 1 (1 âˆ’ 2r s k ) â‰¤ |1 âˆ’ v kâˆ’1 | â€¢ |1 âˆ’ 2r s k | â‰¤ |1 âˆ’ v kâˆ’1 |(17)
Hence, we deduce that w s k â‰¤ |1 âˆ’ v kâˆ’1 |, which indicates that the weighted reward is bounded by the absolute value of the difference between 1 and the previous quality value.</p>
<p>Quality Value.Recall that the quality value v k is determined by incorporating the previous quality value v kâˆ’1 and the weighted reward w s k of the current step.
v k = max(v kâˆ’1 + w s k , 0)(18)
Now, we can inductively reach the conclusion that v k âˆˆ [0, 1], starting from the fact that v 0 = 0.
Assuming that v kâˆ’1 âˆˆ [0, 1], then we can derive v k âˆˆ [0, 1] using the bound of w s k . v k = max(v kâˆ’1 + w s k , 0) â‰¤ v kâˆ’1 + |1 âˆ’ v kâˆ’1 | = 1(19)
Therefore, based on the properties of the weighted reward and the definition of the quality value, we can deduce that v k is indeed confined within the range [0, 1], k = 0, 1, 2, â€¢ â€¢ â€¢ .</p>
<p>Fine-grained Dataset for Math.We adopt an alternative method to generate value train data for math.For this method, we only demand a correct final answer a * for each question q, which is simpler to satisfy.Specifically, we integrate the MATH [33] train set into D 0 .For each question q (i) and answer a (i) * , we use Mistral-7B: MetaMATH [31] as a policy to generate solution traces in a simple breadth-first-search (BFS) manner, obtaining a search tree T (i) q similar to the one of the self-training process.Subsequently, we verify the obtained answers of all leaf nodes of T (i) q according to a (i) * .The verified search trees are then used to derive data samples with target values for D V0 .</p>
<p>â€¢ Construction of value model training set.Previous approaches like [1] that employ PRMs usually require human annotation to initialize a train set, which is quite costly.In comparison, our value model's initial training set can be constructed at a lower expense.</p>
<p>For math data, we deploy the same approach mentioned in section 3.2 to infer process rewards and quality values of partial solutions within the verified search tree T (i) q .While for science data, this value-inferring process is slightly different.We still derive the target value of p (i) k based on the definition in Eq. (1) and Eq. ( 2).Under the assumption that original solutions are reliable and concise, we can simply regard s (i) as the globally optimal reasoning path for q (i) .Therefore, we derive that:
r (i) s k = 0, m (i) k = K i âˆ’ k, w (i) k = 1 K i and v (i) k = k K i . (20)
Derivation.Please refer to the detailed derivation for Eq. ( 20) in Appendix B.2.</p>
<p>In contrast, for generated false samples, we set r
(i) â€² s k+1 = 1, m (i) â€² k+1 = K i âˆ’ k (since still K i âˆ’ k correct reasoning steps required to reach final answer). Considering that v (i) k = k
Ki , we have:
w (i) â€² k+1 = âˆ’ K i âˆ’ k K i âˆ’ k + 1 1 K i ,(21) v(i) â€² k+1 = max(0, k âˆ’ 1 K i + 1 K i â€¢ (K i âˆ’ k + 1)
).</p>
<p>Derivation.Please refer to the detailed derivation for Eq. ( 21) and Eq. ( 22) in Appendix B.2. Collecting all samples and their corresponding derived quality values, we acquire the initial training set D V0 for value model V Î¸ , as described in Appendix E.1.</p>
<p>B.2 Detailed Deduction for Weighted Value and Quality Value</p>
<p>Here, we deduce the weighted reward using Eq. ( 2) and quality value using Eq. ( 1):</p>
<p>when k = 0:
v 0 = 0(23)
when k = 1:
w 1 = 1 âˆ’ 0 K âˆ’ 1 + 1 (1 âˆ’ 2 Ã— 0) = 1 K (24) v 1 = max(0 + 1 K , 0) = 1 K(25)
when k = 2:
w 2 = 1 âˆ’ 1 K K âˆ’ 2 + 1 (1 âˆ’ 2 Ã— 0) = K âˆ’ 1 K(K âˆ’ 1) = 1 K(26)v 2 = max( 1 K + 1 K , 0) = 2 K(27)
therefore,
w k = 1 K , w kâˆ’1 = 1 K , w k+1 = 1 K(28)v k = k K , v kâˆ’1 = k âˆ’ 1 K , v k+1 = k + 1 K(29)m k = K âˆ’ k, m kâˆ’1 = K âˆ’ k + 1, m k+1 = K âˆ’ k âˆ’ 1.(30)
Then, we deduce the Eq. ( 21) and Eq. ( 22):
w k+1 = 1 âˆ’ v k m k+1 + 1 (1 âˆ’ 2r s k+1 ) = 1 âˆ’ k K (K âˆ’ k) + 1 (1 âˆ’ 2 Ã— 1) = âˆ’ K âˆ’ k K(K âˆ’ k + 1) = âˆ’ 1 K K âˆ’ k K âˆ’ k + 1(31)v k+1 = max(v k + w k+1 , 0) = max( k K âˆ’ 1 K K âˆ’ k K âˆ’ k + 1 , 0) = max( k(K âˆ’ k + 1) âˆ’ (K âˆ’ k) K(K âˆ’ k + 1) , 0) = max( k(K âˆ’ k + 1) âˆ’ (K âˆ’ k + 1) + 1 K(K âˆ’ k + 1) , 0) = max( (K âˆ’ k + 1)(k âˆ’ 1) + 1 K(K âˆ’ k + 1) , 0) = max( (K âˆ’ k + 1)(k âˆ’ 1) K(K âˆ’ k + 1) + 1 K(K âˆ’ k + 1) , 0) = max( k âˆ’ 1 K + 1 K(K âˆ’ k + 1) , 0)(32)</p>
<p>C Algorithm Detail and Process Example</p>
<p>C.1 Algorithm Details of MCTS * Node Selection.Similar to [14], we propose to start each selection process from the initial root, since this allows backtracking.Within each iteration, the node selection stage is first executed, where a leaf node C select is hierarchically selected starting from the initial root.To incorporate the quality value of nodes, we use UCB as the criterion to select a child rather than the UCT [26], which is as follows:
U CB(C) = v C + Ïµ ln n parent n C ,(33)
where n parent is the number of visits of the parent node of C, Ïµ is a exploration constant.For each intermediate node, we select its child with maximum UCB.This criterion considers both quality value and visit count, thus it encourages the exploration of high-quality nodes while leaving some opportunity for underexplored nodes.</p>
<p>Thought Expansion.Secondly, the value of the selected node C select is compared with a threshold l (in our experiments, l it is set to 0.9).If the v C select &gt;= l, the node's recorded partial solution
p C select = [s 1 , s 2 , â€¢ â€¢ â€¢ , s k ]
is deemed acceptable as a final solution (since v C get close to 1 only when C is close to correct final answer), which is then directly returned as output, terminating the algorithm.This is different from the method adopted by [60] since no reward model estimation is required.Otherwise, the expansion stage is initiated, where new solution steps s k+1,i (i = 1, 2
C i = ([s 1 , s 2 , â€¢ â€¢ â€¢ , s k , s k+1,i ], 0, v Ci ) are added to T q with v Ci assigned by the value model, v Ci â† V Î¸ (p Ci |q)
. Note that we also incorporate a self-critic mechanism into this expansion process, which will be illustrated later.</p>
<p>Greedy MC Rollout.[60] and [14] use a simplified three-stage iteration that doesn't include a simulation process on leaf nodes.In contrast, we believe that a simulation process still brings about useful information for value estimation, despite the rise in generation and time cost.In this stage, we propose to simulate a few steps upon the new node C i with maximum predicted value.Reasoning steps starting from this node will be sampled step-by-step and evaluated, while only the most valuable path is further explored, until a step limit m is reached.The highest quality value acquired in the sampling process v max is recorded and used to update v Ci with a weight parameter Î± following:
v Ci â† Î±v Ci + (1 âˆ’ Î±)v max .(34)
Besides, the visit count n Ci is also updated by n Ci â† n Ci + 1.</p>
<p>Value Backpropagation.Finally, we conduct value backup starting from C select .The value of every parent node of C select is updated using a weighted average method.For every node C on the trace from root to C select , we update its n C and v C as follows:
n C â† n C + 1(35)
Algorithm 2: The proposed value guided search algorithm MCTS * .Input: question q, inference_model Ï€ S0 , value_model V Î¸ , max_iterations T , threshold l, branch b, rollout_steps m, roll_branch d, weight_parameter Î±. 1: T q â†Initialize_tree(q) 2: Ï€ S0 , V Î¸ â†Initialize_models(Ï€ S0 , V Î¸ ) 3: for i in range(T ) do C â†root(T q ) 5:</p>
<p>-----Node Selection-----
6:
while C is not leaf node do 7: -----Self-Critic----- -----Thought Expansion----- end for 20:
C â† argmax C â€² âˆˆchildren(C) (v C â€² + Ïµ ln n C n C â€² ) // Select
-----Greedy MC Rollout-----
21: C â€² â† argmax C â€² âˆˆchildren(C) (v C â€² ) 22: p = p C â€² 23: v max = 0 24:
for k in range(m) do 25:</p>
<p>p, v max â† Get_next_step_with_best_value(p|Ï€ S0 , V Î¸ , d, q) // Sample new children and record the best-observed value</p>
<p>26:</p>
<p>end for 27: -----Value Backpropagation-----
v C â€² â† Î±v C â€² + (1 âˆ’ Î±)v max 28: n C â€² â† n C â€² + 1 //</p>
<p>31:</p>
<p>Back_propagate(C) // Update value of parent nodes using weighted average 32: end for 33: C=Get_best_node(T q ) // Fetch the node with the highest value in the search tree 34: Return p C Output:
p C and v C â† Î£ i n Ci â€¢ v Ci Î£ i n Ci (36)
where Step 3: Calculate the probability.
C i (i = 1, 2, â€¢ â€¢ â€¢ , b)
Based on the preceding steps, we already know that the number of outcomes satisfying the problem condition is 3, and the total number of outcomes of the two dice is 36.Therefore, the probability is equal to 3 divided by 36, which is 1/12.Therefore, the probability that the sum of the two dice will be 4 is 1/12. = 1  = 0.50</p>
<p>Step 3: Calculate the probability.The probability that the sum of the two dice will be 4 is 4/36=1/9. = âˆ’0.25  = 0.22  = âˆ’0.17</p>
<p>Step 2: Suppose the sum of the two dice is 4. We can list all the results that satisfy the condition: (1,3), (2,2), (3,1), (4,0).So the number of results that satisfy the problem condition is 4. Thus, the probability is 4/30=2/15.</p>
<p>Step 1: Each die has six sides, so there are six possibilities for the outcome of each die.Therefore, the total number of outcomes of the two dice is 6*5=30.</p>
<p>ð‘š = 3 ð‘£ = 0</p>
<p>Q: Suppose that we roll two fair 6-sided dice.What is the probability that the two numbers rolled sum to 4?</p>
<p>Step 1: First, we need to determine the possible outcomes of the two dice.Each die has six sides, so there are six possibilities for the outcome of each die.Therefore, the total number of outcomes of the two dice is 6*6=36.</p>
<p>= âˆ’0.25</p>
<p>Step 1: Calculate the probability.The probability that the sum of the two dice will be 4 is 6/36=1/6.</p>
<p>Step 2: Determine the number of results that satisfy the problem condition.According to the problem, the sum of the two dice is 4. We can list all the results that satisfy the condition: (1,3), (2,2), (3,1).So the number of results that satisfy the problem condition is 3.
ð‘¤ = 0.22 ð‘š = 2 ð‘£ = 0.56
Step 2: Next, we can list all the results that satisfy the condition: (1,3), (2,2), (3,1).So the number of results that satisfy the problem condition is 3.
ð‘¤ = 0.33 ð‘š = 1 ð‘£ = 0.78
Step 3: The calculated probability can be obtained by dividing the number of outcomes satisfying the condition by the number of total outcomes.</p>
<p>ð‘š = 0 ð‘£ = 1</p>
<p>Step 4: Therefore, the probability that the sum of the two dice will be 4 is 3/36=1/12.The search tree T q has already been pruned during this stage, with traces verified.Starting with the inference of m, we gradually update all weighted rewards w for actions (steps) and quality value v for states (partial solutions).Taking the false Step 3 as an example, since it makes a mistake in calculation, it still requires the same number of steps as its parent to reach the correct answer (one step to correct the calculation mistake), i.e. m = 1.As no trace starting from this node reaches the correct answer, we have r s = 1.Thus, we derive w = 1âˆ’0.671+1 (1 âˆ’ 2 â€¢ 1) = âˆ’0.17,and therefore v = max(0.67+ (âˆ’0.17), 0) = 0.50.</p>
<p>skipped if an EoI signal is received.Otherwise, the advice will be utilized in the following expansion stage as part of the inference prompt, so Ï€ S0 generates new steps based on both o and p.An overall pseudo-code for ReST-MCTS * is presented in Algorithm 2.</p>
<p>C.2 Data Generation Process and Specific Example for Reward Inference</p>
<p>The data generation process of our self-training approach consists of mainly four stages, namely search, prune, verify, and reward inference, which is demonstrated in Figure 4.For reward inference, a detailed example is shown in Figure 3.</p>
<p>D Model Comparison</p>
<p>ReST-MCTS * vs. AlphaLLM.As an approach that aims to enhance LLM inference, AlphaLLM [61] utilizes a tailored MCTS algorithm and critic models to provide precise feedback.Even though AlphaLLM also adopts MCTS and critic models for self-improvement, their approach is different from ours in various crucial aspects, as elaborated below.</p>
<p>(1) Design of MCTS algorithm.For the level of search, AlphaLLM's Î·MCTS considers options as action, with termination signals delivered by a termination function Î².In contrast, we use reasoning steps as action, which is achieved through tailored prompt design.Concerning critic models, we use a single value model to provide evaluation for intermediate nodes.The model is trained to predict specially designed quality values that reflect completeness and correctness of partial solutions, rather than estimating the conventional definition of value function in RL.In addition, we also incorporate self-critic mechanisms into the tree search algorithm to provide insights for the policy (Appendix C.1), which AlphaLLM does not adopt.</p>
<p>(2) Definition of reward/value.Our definition of weighted reward and quality value is novel, leading to significant differences between our method and AlphaLLM across various processes such as critic model training, data synthesizing, and data filtration.Since our design of quality value involves information on process reward and reasoning distance, our value model trained on this target can naturally provide sufficient feedback during the search, with no need for implementing other critic models mentioned by AlphaLLM.</p>
<p>(3) Self-Training algorithm.Although AlphaLLM also includes iterative self-training, the implementation method varies greatly.Most importantly, their critical model is static throughout the iterations, which means they focus more on the improvement of policy.In comparison, we also consider the impacts of self-training on the critic value model.As demonstrated in Algorithm 1, we calculate process rewards and quality values according to the final search tree of questions within each iteration, which are then used as new training data for the value model.</p>
<p>E Experimental Details E.1 Training and Evaluation of Initial Value Model</p>
<p>Initialization of Value Model.We split D V0 and use the train set to finetune ChatGLM3-6B and Mistral-7B to predict the value of partial solutions.We simply add a linear layer to the model to directly transform probabilities to a scalar value.Moreover, we use the AdamW optimizer [62] and MSE loss in Eq. ( 37) to optimize, eventually obtaining an initial value model V Î¸ that can evaluate the correctness and completeness of step-by-step solutions.Note that the learning rate is set to 1e-6 in this process.The MSE training loss is shown below:
L MSE = E (q,p,v)âˆ¼D V 0 |V Î¸ (p|q) âˆ’ v| 2 . (37)
Evaluation of Value Model.We use the test set containing 14k data samples to evaluate the value model with an absolute tolerance of 0.1:
Accuracy = 1 t Î£ t i=1 I(|clip(V Î¸ (p i |q i ), 0, 1) âˆ’ v * i | &lt; 0.1)(38)
where t is the number of test data samples, q i is the question of sample i, p i is the partial solution of sample i and v * i is the target value of sample i.Our initial value model achieves an accuracy of 69.3%, which means it is reliable in most situations.We also conducted a study to measure the value model's performance on science benchmark SciBench [34] compared to outcome-supervised reward models and self-critic methods in Table 7.</p>
<p>E.2 Benchmark Setup</p>
<p>To compare the performance of different search methods, we construct a standardized benchmark test that can be generally used on labeled science or math datasets like MATH, SciBench, and SciEval.Aside from the ReST-MCTS * , we incorporate two other baselines: chain-of-thought (CoT) and tree-of-thought (ToT).For each method, specialized prompts P are designed to execute the search process.Besides, an inference model Ï€ and value model V are deployed to provide deduction and feedback.Concerning the CoT baseline, we use Self-Consistency to calculate accuracy.For the ToT baseline, we use a simple greedy depth-first-search (DFS) algorithm with node values assigned by the value model.The algorithm stops exploitation when a max depth of 10 is reached and ends when a node value exceeds the threshold 0.9.For ReST-MCTS * , self-critic is used and the ending threshold is also set to 0.9.The rollout step limit m is set to 2, Î± is set to 0.5, and the number of iterations T is set to 50 by default.Moreover, both tree search algorithms use b = 3 by default, where b is the number of samples generated in the expansion process as mentioned in the former sections.After the search process, the policy is prompted to extract the final answer based on the obtained solution, which is then compared with the ground truth to determine correctness.The results of these methods on benchmarks are illustrated in Section 4.3.</p>
<p>E.3 Baselines of Search Verification</p>
<p>The basic settings of relevant verification baselines are illustrated as follows:</p>
<p>â€¢ ORM + Best-of-N (BoN) For simplicity, we employ the ORM used by [10], which is trained on SciInstruct.For each question, we sample N solutions and select the solution with the highest ORM score as output.N is used to control token usage.</p>
<p>â€¢ ReST-MCTS * Implementation of ReST-MCTS * , using the value model V Î¸ as PRM to guide MCTS * .The variable controlling token usage is the iteration number T and branch parameter b.</p>
<p>â€¢ Self-Consistency N solutions are generated for each question using a simple CoT prompt.Their final answers are then extracted and classified, with the most frequently occurring answer selected as the final output.N is used to control token usage.</p>
<p>â€¢ PRM + Best-of-N (BoN) With value model V Î¸ used as PRM, we perform DFS-based tree search.Every selected solution s is evaluated by a PRM score r PRM = Î  K i=1 v i .The one with the highest PRM score among all N solutions is regarded as the final output.Under this setting, b is set to 3, while N is used to control token usage.</p>
<p>As shown in Figure 2, we have compared the number of tokens consumed by each algorithm to achieve certain accuracy on MATH and SciBench.Results reveal that to reach a certain expected accuracy, MCTS * basically requires fewer tokens than other algorithms like SC and ORM + BoN.This means that based on the same expected standards, MCTS * outperforms other algorithms while maintaining a reasonable computation cost.As for the actual running time, we have recorded  the average running time of different algorithms (under our basic experiment settings) on a single question, as shown in Table 6.We see that MCTS * spends more time on exploration and simulation than other simple algorithms.However, since our method adopts a different design of value, it does not require massive Monte Carlo estimations.This reduces the running time of our algorithm and limits the time consumption to a reasonable range.Notice that MCTS * can achieve high accuracy that other algorithms can never attain even at unlimited cost, we believe this extra time is fairly acceptable.</p>
<p>E.4 Value Model of ReST-MCTS * on SciBench</p>
<p>We employ the reward model obtained by [10] (which is used as a classifier for SciGLM) as the ORM and our fine-grained value model as PRM to provide the outcome reward and step-wise value respectively.We also include the Self-Rewarding method, where the policy model itself is instructed to provide step-wise value.For all methods, the number of samples for each step is set to 3. Using this setting, we record the model accuracy of GLM4 and GPT-3.5-turbo on the selected questions, which are as shown in Table 7. Results indicate that compared to ORM and Self-Rewarding, PRMbased methods exhibit higher accuracy.This confirms the effectiveness of our value model.In addition, Figure 5 concerns the total consumption of the token budget, including all prompt tokens and completion tokens.However, we still have to note that the total token usage (especially prompt tokens) of the ReST-MCTS * increases rapidly as hyper-parameters b and T rise.</p>
<p>E.5 ReST-MCTS * on SciEval</p>
<p>SciEval.Similar to SciBench, we perform benchmark tests on SciEval.Results are shown in Table 8.For both GLM4 and GPT-3.5-turbo,ReST-MCTS * again outperforms other baselines in overall accuracy, with an accuracy of 79.87% and 62.31% respectively.However, we notice that though tree-search-based methods demonstrate an advantage on average, they fail to improve the performance of the CoT baseline on some parts of SciEval.We examine the data distribution and discover that these parts are basically all single-choice questions.As they are less difficult compared to other types of questions, the Self-Consistency CoT approach may already be competent.Besides, these questions often require few reasoning steps, which may be the main reason why tree search methods do not perform as well as expected.</p>
<p>F Prompt and Instruction Examples</p>
<p>We present some instruction examples used in ReST-MCTS * and self-training process in this section, including:</p>
<p>â€¢ Inference instruction This instruction is used in tree search for the policy to generate new steps based on previous self-critic information.</p>
<p>â€¢ Self-critic instruction Used for generating the EoI signal or advice for further search.</p>
<p>â€¢ LLM verify instruction This instruction is employed in the data generation process of self-training when an answer needs verification by LLM (GPT-4 for our case).MCTS [63] is a search algorithm for optimal decision-making in large and complex combinatorial spaces.This algorithm represents search spaces as search trees and works on the principle of the best-first search based on the evaluations of stochastic simulations.This technique has been widely employed in multiple gaming scenarios and achieved tremendous success, such as AlphaGo and AlphaZero [64] for computer Go Game.The basic MCTS algorithm involves iteratively search process with four steps for building a search tree:</p>
<p>(1) Selection.The agent, starting from an empty tree's root node, traverses the search tree's visited nodes and selects the next node according to the given selection strategy until the scalable node or leaf node is reached.</p>
<p>(2) Expansion.If this algorithm arrives at an expandable node, it expands the search tree by selecting an unvisited child node.</p>
<p>(3) Simulation.After finishing the expansion, if the current node is in a non-terminal state, the algorithm will conduct one or multiple independent simulations from the current node until it reaches the terminal state.In this process, the actions are chosen at random.</p>
<p>(4) Backpropagation.The node statistics on the path from the current node to the root are updated based on the search results.Note that the scores assessed are based on the termination state achieved.</p>
<p>To trade off the less tested paths with the best strategy identified so far, MCTS maintains a proper balance between exploration and exploitation by maximizing the Upper Confidence Bounds for Trees (UCT) when a child node k is selected as follows, UCT = X k + 2C p 2 ln n n k : where the first term, X k , is the average reward form arm k and this term encourages the exploitation of higher-reward choices.It is generally understood that X k to be within [0, 1].In the second exploration term, C p &gt; 0 is a constant to satisfy the Hoeffding inequality with rewards in the range of [0, 1] [26].n is the number of times the current node has been visited and n k is the number of times child k has been visited.Generally, n k = 0 produces a UCT value of âˆž, so that all children of a node have a non-zero probability and are considered.</p>
<p>G.2 LLM Reasoning with Monte Carlo Tree Search</p>
<p>LLMs have been invented, used in the past for autoregressive text generation, and are now very great at reasoning.Reasoning algorithms include prompt-based chain-of-thought (CoT) [22], planningbased represented by tree-of-thought (ToT) [24], which successfully achieved the LLMs' reasoning performance improvement.ToT combines the power of tree search (e.g., depth/breadth-first search) as an algorithm and LLMs' power as a heuristic to tradeoff evaluation and generation.Reasoning via Planning (RAP) [59] with Monto Carlo Tree Search (MCTS) performs reasoning exploration and obtains reward reasoning paths.</p>
<p>Recent studies [60] present that Monte Carlo Tree Search (MCTS) agents benefit from task-specific extension and expansion of the research tree.Specifically, the MCTS agents provide appropriate selection strategies for the state of the visit to guide the upcoming search based on the evaluation results (e.g., rewards and number of times the node has been visited) produced by the rollout and backpropagation process.Its mechanism coordinates exploration and thought exploitation within search space, which is superior to traditional depth-first search (DFS) or breadth-first search (BFS) algorithms based on the Tree of Thought (ToT).Building on MCTS, some studies have also explored the ability of the reasoning agent to provide search guidance.In catalyst design, [65] proposed Monte Carlo Thought Search, using LLM for complex scientific reasoning queries.[59] presents Reasoning via Planning (RAP), which adopts MCTS as a planning algorithm and repurposes the LLM as both a world model and a reasoning agent.Others like [66] experiment using the value function, a byproduct of the Proximal Policy Optimization (PPO) process, to guide the token-level decoding based on MCTS.In general, these approaches improve LLM's reasoning ability, whereas their performance on some challenging science tasks remains unsatisfying.In addition, this series of methods differs from our contribution, where we propose a value model approach as reward functions for optimizing the reasoning path and improving model output.</p>
<p>H Limitations</p>
<p>In this section, we discuss some limitations of the ReST-MCTS * .</p>
<p>Generalization to other tasks, especially those without labels.Similar to many existing selftraining works, ReST-MCTS * also relies on ground-truth oracle labels in a supervised dataset to filter the responses in the first place; in the future, we need to show ReST-MCTS * can generalize to other reasoning tasks outside of math (like coding, agent, conversation, etc); in addition, for those very complicated tasks that require multistep planning and reasoning (like implementing the whole software like SWE-Agent), which does not have ground-truth answers, we need to propose a better way to collect reward feedback (from few human labeling and symbolic execution or solver), and train a generalizable reward model that can work and help for a wider range of tasks.</p>
<p>Scale and diversity of proposed value model.Although we trained a value model based on Mistral-7B: MetaMATH that performs better than the most advanced value model MATH-SHEPHERD, a larger scale value model backbone is still needed for better PRM training.In addition, the initial training set of the training proposal PRM was generated by SciGLM, a model that focuses on mathematical and scientific reasoning tasks but still lacks generality.While the current PRM achieves the best results on multiple mathematical and scientific reasoning tasks, such as MATH and SciBench, it's worth exploring more diverse training sets to expand into various fields in the future, such as code generation and agent planning.</p>
<p>Self-training data filtering techniques.As we mentioned in Section 1, the quality of reasoning trajectory affects the effectiveness of self-training, and generating a high-quality training set plays an important role.Therefore, we train the iterative process reward model to guide the tree search direction to obtain high-quality trajectories.On the other hand, since well-trained value models can help filter out the top-k generated trajectories with the highest process values, we also expect that a stronger and larger LLM model as the backbone of the value model might help to gain more.</p>
<p>I Broader Impact</p>
<p>ReST-MCTS * aims to introduce a general self-training approach that uses MCTS * to automatically label and generate process rewards, which will help generate high-quality datasets and improve the reasoning capabilities of LLMs.Fine-tuning a variety of LLMs on synthesized high-quality datasets can directly improve the performance of value models and generators and help to avoid the cost of manually generating process rewards during the training process reward model.The disadvantage is that a single reward model cannot be scaled to multiple domains, and we can solve this problem by training various reward models together on various reasoning domains.We believe that on the whole, the advantages outweigh the disadvantages.</p>
<p>J Reproducibility</p>
<p>We have made significant efforts to ensure the reproducibility of our all experimental results.The training code, tree search algorithm, and the evaluation details for the ReST-MCTS * are public in our repository.</p>
<p>Training.Detailed training information about the value model, self-training backbones, and experimental settings can be found in Section 4.1.</p>
<p>Tree Search Algorithm.Regarding the enhanced tree search algorithm MCTS * , please refer to the Algorithm 1 and public code.</p>
<p>Evaluation.We organized all evaluations, including the iterative self-training and value model, a variety of value models, performance comparison under the same search budget, and different reasoning policies.All details can be found in Section 4.2 for self-improvement evaluation and Section 4.3 for value models and reasoning policies comparison.</p>
<p>Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We have uploaded all codes of our research at https://github.com/THUDM/ReST-MCTS.Guidelines:</p>
<p>â€¢ The answer NA means that paper does not include experiments requiring code.â€¢ Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</p>
<p>Experimental Setting/Details</p>
<p>Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have provided the experimental setting and details of our research in Section E.1 and Section E.2.</p>
<p>Guidelines:</p>
<p>â€¢ The answer NA means that the paper does not include experiments.</p>
<p>â€¢ The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.â€¢ The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have provided the experimental statistical significance of our research in Figure 2 in Section 4.3.</p>
<p>Guidelines:</p>
<p>â€¢ The answer NA means that the paper does not include experiments.</p>
<p>â€¢ The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.</p>
<p>â€¢ The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).â€¢ The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) â€¢ The assumptions made should be given (e.g., Normally distributed errors).â€¢ It should be clear whether the error bar is the standard deviation or the standard error of the mean.Guidelines:</p>
<p>â€¢ The answer NA means that the paper does not include experiments.</p>
<p>â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.â€¢ The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.â€¢ The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p>
<p>Code Of Ethics</p>
<p>Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?</p>
<p>Answer: [Yes]</p>
<p>Justification: We have reviewed the NeurIPS Code of Ethics and make sure to preserve anonymity.Our research is based on open-source models and datasets and we cite all original papers that produced the code package or dataset.There is nothing that violates the NeurIPS Code of Ethics in our work.</p>
<p>Guidelines:</p>
<p>â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.</p>
<p>â€¢ If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).</p>
<p>Broader Impacts</p>
<p>Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?</p>
<p>Answer: [Yes] Justification: We have provided the broader impacts of our research in Section I.</p>
<p>â€¢ The answer NA means that there is no societal impact of the work performed.</p>
<p>â€¢ If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.â€¢ Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.â€¢ The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.â€¢ The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).</p>
<p>Safeguards</p>
<p>Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?</p>
<p>Answer: [Yes]</p>
<p>Justification: Our research is based on open-source models and datasets and we cite all original papers that produced the code package or dataset.There is no risk of misuse in our work.</p>
<p>Guidelines:</p>
<p>â€¢ The answer NA means that the paper poses no such risks.</p>
<p>â€¢ Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.â€¢ Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.â€¢ We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes]</p>
<p>Justification: Our research is based on open-source models and datasets and we cite all original papers that produced the code package or dataset.And we select the proper license when we submit this work.</p>
<p>Guidelines:</p>
<p>â€¢ The answer NA means that the paper does not use existing assets.</p>
<p>â€¢ The authors should cite the original paper that produced the code package or dataset.</p>
<p>â€¢ The authors should state which version of the asset is used and, if possible, include a URL.â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.â€¢ If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.â€¢ For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.â€¢ If this information is not available online, the authors are encouraged to reach out to the asset's creators.13.New Assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [Yes] Justification: Our research is based on open-source models and datasets and we cite all original papers that produced the code package or dataset.In addition, we submit all code in the anonymized zip file.Guidelines:</p>
<p>â€¢ The answer NA means that the paper does not release new assets.</p>
<p>â€¢ Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. â€¢ The paper should discuss whether and how consent was obtained from people whose asset is used.â€¢ We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.â€¢ For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.</p>
<p># ,  % ,â€¢â€¢â€¢, )</p>
<p>628 C Deduction Demonstration 629 C. 1
6291
Definition of Weighted Value and Quality Value 630 Weighted Value.Recall the definition of the weighted reward:</p>
<p>) 17 the
17
probability to get correct answer a â‡¤ given a problem Q and a reasoning trace s.Given a original 568 training dataset D = {Q1, Q2, â€¢ â€¢ â€¢ , QM }, a new dataset can be produced by sampling â‡¡ N times per 569</p>
<p>595</p>
<p>output sigmoid score.Specifically,[29] regards PRM training as a three-class classification with 596 costly human annotations.Similarly, MATH-SHEPHERD [47] collects random rollout trajectories 597 via BoN reasoning policy and synthesizes process rewards to construct the PRM training dataset 598 autonomously.MATH-SHEPHERD defines the automated quality rs k , the potential to deduce the 599 correct answer, for each reasoning step sk through hard estimation (HE) and soft estimation (SE), 600 which are, 601</p>
<p>Figure 1 :
1
Figure 1: The left part presents the process of inferring process rewards and how we conduct process reward guide tree-search.The right part denotes the self-training of both the process reward model and the policy model.</p>
<p>7 :DÏ€D
7
Gi(Aj =a * ) â† label_correctness(D Gi ) // match and select correct solutions Si â† SFT(Ï€ Siâˆ’1 , D Gi(Aj =a * )| N j=1 ) // self-training policy model 10: Vi â† extract_value_data(D Gi ) // collect process reward and extract value data 11:</p>
<p>(i) k,j and d (i) k and use them to construct D V0 , which is illustrated in Appendix B.1.We adopt an alternative method to generate value train data for math, as shown in Appendix B.1.Comparison of value model on SciBench.</p>
<p>Figure 2 :
2
Figure 2: Accuracy of different searches on MATH and SciBench with varied sampling budget.</p>
<p>are the children of C.This actually updates the value of C according to its children's value expectation.Determine Termination via Self-Critic.Although the value model provides accurate evaluation for partial solutions, it cannot consistently signal logical termination, especially when the inference model reaches a false conclusion.Consequently, even when a false final answer is generated, further exploration beneath this node may still be conducted, leading to reduced search efficiency.Therefore, we propose to use self-critic to provide extra timely signals of logical termination that avoid unwise exploration as well as insight into deeper search.Specifically, we prompt the inference model to generate an End of Inference (EoI) signal or offer advice o on following exploration steps based on existing partial solutions p before each expansion stage.The expansion and MC rollout stage will be</p>
<p>Figure 3 :
3
Figure3: Detailed inferred process of a concrete example.The search tree T q has already been pruned during this stage, with traces verified.Starting with the inference of m, we gradually update all weighted rewards w for actions (steps) and quality value v for states (partial solutions).Taking the false Step 3 as an example, since it makes a mistake in calculation, it still requires the same number of steps as its parent to reach the correct answer (one step to correct the calculation mistake), i.e. m = 1.As no trace starting from this node reaches the correct answer, we have r s = 1.Thus, we derive w = 1âˆ’0.671+1 (1 âˆ’ 2 â€¢ 1) = âˆ’0.17,and therefore v = max(0.67+ (âˆ’0.17), 0) = 0.50.</p>
<p>Figure 4 :
4
Figure 4: Detailed process of new sample data generation for the self-training framework.</p>
<p>Figure 5 :
5
Figure 5: Accuracy of different methods on SciBench with varied total token usage per question.Both the completion token and prompt token are included.</p>
<p>Figure 6 : 2 + b 2 .LLM Output: 1 G
6221
Figure 6: Comparison between existing self-training methods with our proposed ReST-MCTS * .</p>
<p>[14] reasoning route starting from p k requires more steps to get to the correct answer, then the single-step weighted reward w s k is lower.Observation 2. w s k decreases as the PRM's predicted sigmoid score r s k rises.Thus, w s k has a positive correlation with the PRM's prediction of a step's correctness.Observation 3. v k â†’ 1 â‡â‡’ r s k â†’ 0, m k = 0, i.e. v k converges to upper bound 1 only when s k reaches the correct answer.Based on the features of v k and w s k , we can directly predict the quality value of partial solutions and guide search once we have a precise PRM and accurate prediction of m k .In our approach, instead of separately training models to predict r s k and m k , we simply train a process reward model V Î¸ to predict v k , serving as a variant of common PRM.With reward incorporated in the calculation of v k , there is no need to separately train a reward model, saving considerable effort for answer selection.Process Reward Model Guided Tree Search MCTS * .Tree search methods like[24]and[26]require a value function and outcome reward model r Ï• to prune branches, evaluate final solutions and backup value.However, using ORM to evaluate final solutions and backpropagate means every search trace must be completely generated, which is costly and inefficient.Recent work[14]suggests using a learned LLM value function in MCTS so the backup process can happen in the intermediate step, without the need for complete generations.Their work greatly improves search efficiency but still relies on an ORM to select the final answer.Drawing inspiration from these works, we further propose a new variant of MCTS, namely MCTS * , which uses quality value v k as a value target for a trained LLM-based process reward model and guidance for MCTS as well.Given the above properties, we can directly use the process reward model V Î¸ to evaluate the quality of any partial solution, select, and backpropagate in intermediate nodes.Aside from the use of quality value, we also incorporate a special Monte Carlo rollout method and self-critic mechanism to enhance efficiency and precision, which are explained detailedly in Appendix C.1.We express MCTS * as an algorithm that comprises four main stages in each iteration, namely node selection, thought expansion, greedy MC rollout, and value backpropagation.Similar to common MCTS settings, the algorithm runs on a search tree T q for each single science reasoning question q.Every tree node C represents a series of thoughts or steps, where a partial solution p C , number of visits n C , and corresponding quality value v C are recorded.For simplicity, we denote each node as a tuple C = (p C , n C , v C
Derivation. Please refer to the detailed derivation in Appendix B.1.Therefore, we can conclude that w s k and v k has following properties that match our expectations:Observation 1.</p>
<p>train policy model Ï€ Siâˆ’1 on D Gi to generate new generator Ï€ Si .At the same time, D Gi drives the update of V i to V i+1 .We present iterative self-training that the process reward model and policy model complement each other in Algorithm 1. Mutual self-training ReST-MCTS * for value model and policy model.Input: base LLM Ï€, original dataset for policy model D S0 , original dataset for value model D 0 , new problem set D
Algorithm 1:
G , number of solutions N , j-th solution A j , correct solution a * , value model V Î¸ , weighted value function w, quality value function v, number of iterations T .1:</p>
<p>Table 2 :
2
Primary results by training both policy and value model for multiple iterations.For each backbone, different self-training approaches are conducted separately.This means each approach has its own generated train data and corresponding reward (value) model.Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.
ModelSelf-Training MethodsMATH GPQA Diamond CEval-Hard Ave.0th iteration (zero-shot) 0th iteration (few-shot)20.76 30.0027.27 31.3126.32 25.6624.78 28.99(Below are fine-tuned from model of previous iteration with self-generated traces)LLaMA-3-8B-Instructw/ ReST EM (1st iteration) w/ Self-Rewarding (1st iteration) w/ ReST-MCTS  *  (1st iteration)30.84 30.34 31.4226.77 26.26 24.2421.05 25.66 26.9717.22 27.42 27.55w/ ReST EM (2nd iteration) w/ Self-Rewarding (2nd iteration) w/ ReST-MCTS  *  (2nd iteration)33.52 33.89 34.2825.25 26.26 27.7821.71 23.03 25.0026.83 27.73 29.020th iteration (zero-shot) 0th iteration (few-shot)29.34 28.2827.78 29.299.87 9.2122.33 22.26(Below are fine-tuned from model of previous iteration with self-generated traces)Mistral-7B: MetaMATHw/ ReST EM (1st iteration) w/ Self-Rewarding (1st iteration) w/ ReST-MCTS  *  (1st iteration)23.84 25.70 31.0626.26 27.78 26.2620.39 19.74 17.1123.50 24.40 24.81w/ ReST EM (2nd iteration) w/ Self-Rewarding (2nd iteration) w/ ReST-MCTS  *  (2nd iteration)23.86 23.90 24.4026.26 26.77 28.7922.37 25.00 26.3224.16 25.22 26.500th iteration25.1823.7451.9733.63SciGLM-6Bw/ ReST EM (1st iteration) w/ Self-Rewarding (1st iteration) w/ ReST-MCTS  *  (1st iteration)22.72 22.50 24.8624.75 26.26 25.2551.32 47.37 51.3232.93 32.04 33.81w/ ReST EM (2nd iteration) w/ Self-Rewarding (2nd iteration) w/ ReST-MCTS  *  (2nd iteration)25.86 23.86 23.9025.25 28.79 31.8248.68 48.03 51.9733.27 33.56 35.90
(Below are fine-tuned from model of previous iteration with self-generated traces)</p>
<p>Table 3 :
3
Accuracy of different verifiers on GSM8K test set and MATH500.SC: Self-Consistency, MS: MATH-SHEPHERD.Verification is based on 256 outputs.
ModelsDatasetSCORM SC+ORM MS SC + MS SC + ReST-MCTS  *  (Value)Mistral-7B: MetaMATHGSM8K MATH500 35.1 83.986.2 36.486.6 38.087.1 37.386.3 38.387.5 39.0
4.2 Evaluating Self-Improvement of ReST-MCTS *</p>
<p>Table 4 :
4
Overall performance comparison with representative models on SciBench.
ModelsSubject MethodChemistry atkins chemmc quan matter fund class thermo PhysicsdiffMath statcalcAll Ave.GLM4CoT ToT ReST-MCTS  *  13.08 11.21 11.2123.07 23.07 28.208.82 8.82 14.704.08 12.24 22.22 6.38 19.44 2.12 8.16 22.22 4.257.46 5.97 7.4610.00 12.00 28.57 12.68 12.00 25.33 30.95 15.82 12.00 26.66 30.95 16.77GPT-3.5-turboCoT ToT ReST-MCTS  <em>5.60 8.41 5.607.69 12.82 12.825.88 11.76 11.766.12 6.12 6.126.94 11.11 0.00 2.12 6.94 8.512.98 0.00 2.984.00 16.00 11.90 6.92 10.00 18.66 9.52 8.44 10.00 24.00 11.90 10.06LLaMA2-13B-ChatCoT ToT ReST-MCTS  </em>2.80 0.93 0.932.56 5.12 5.122.94 2.94 2.942.04 4.08 2.042.77 2.77 4.162.12 0.00 2.120.00 1.49 0.002.00 0.00 4.002.66 4.00 5.532.38 2.38 2.382.23 2.37 2.904.3 Evaluating Reward Guidance and Reasoning Policy of ReST-MCTS  *</p>
<p>When LLMs are trained with the RL algorithm, the generation of LLMs can be naturally expressed as the Markov Decision Process (MDP) and optimized for specific objectives.
5 Related Work5.1 Large Language Model TrainingLarge Language Models (LLMs) [36; 37; 38] have emerged as a notable success in various natural language tasks. Recent studies focus on improving the reasoning capabilities of LLMs, including collecting high-quality or larger domain-specific data [39; 40; 41; 42; 10; 43], designing elaborate prompting [22; 44; 45; 46], or training supervised learning [10; 31; 32; 47] or reinforcement learning (RL) [48; 49; 50; 16].</p>
<p>LLM Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.2 Reward Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A.3 LLM Self-Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .Definition of Weighted Value and Quality Value . . . . . . . . . . . . . . . . .B.2 Detailed Deduction for Weighted Value and Quality Value . . . . . . . . . . . .C Algorithm Detail and Process Example C.1 Algorithm Details of MCTS * . . . . . . . . . . . . . . . . . . . . . . . . . . .C.2 Data Generation Process and Specific Example for Reward Inference . . . . . .
A PreliminariesA.1 B Deduction DemonstrationB.1 D Model ComparisonE Experimental DetailsH LimitationsI Broader ImpactJ Reproducibility
E.1 Training and Evaluation of Initial Value Model . . . . . . . . . . . . . . . . .E.2 Benchmark Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .E.3 Baselines of Search Verification . . . . . . . . . . . . . . . . . . . . . . . . .E.4 Value Model of ReST-MCTS * on SciBench . . . . . . . . . . . . . . . . . . .E.5 ReST-MCTS * on SciEval . . . . . . . . . . . . . . . . . . . . . . . . . . . .F Prompt and Instruction Examples G Further Preliminaries of MCTS and LLM Reasoning with MCTS G.1 Monte Carlo Tree Search (MCTS) . . . . . . . . . . . . . . . . . . . . . . . .G.2 LLM Reasoning with Monte Carlo Tree Search . . . . . . . . . . . . . . . . .</p>
<p>Table 5 :
5
Notation Table. of a single step s k , used to define weighted reward w sk weighted reward of a single step s k , inferred in self-training process after trace generation v k quality value of partial steps p k , used to guide search; inferred in self-training process
Character Q A a  *  s p s k K A j N d r sk common PRM reward m k number of reasoning steps of a solution Meaning given question/problem decoded answer final correct answer solution partial solution k-th step of solution s j-th solution number of solutions number of preference pairs reasoning distance of partial steps p k Ï€ B base language model D S0 original training dataset V Î¸ process reward model r Ï• outcome reward model</p>
<p>[12]k is the PRM's output sigmoid score.Specifically,[1]regards PRM training as a three-class classification with costly human annotations.Similarly, MATH-SHEPHERD[12]collects random rollout trajectories via BoN reasoning policy and synthesizes process rewards to construct the PRM training dataset autonomously.MATH-SHEPHERD defines the automated quality r s k , the potential to deduce the correct answer, for each reasoning step s k through hard estimation (HE) and soft estimation (SE), which are,</p>
<p>child node based on UCB
8: 9: 10: 11: 12:end while if v C â‰¥ l then Return p C // Output solution end if</p>
<p>Update value and visit count of the rollout node
29: 30:end if</p>
<p>Table 6 :
6
The average running time of different algorithms (under our basic experiment settings) on a single question.
MethodCoT + SC ORM + BoN PRM + BoN MCTS  *Running time (s) MATH41 37.043 33.573 34.5108 41.5</p>
<p>Table 7 :
7
Accuracy of different reward/value models on the questions selected from SciBench, they all use MCTS * as search policy.
DatasetModelsORM PRM Self-Rewarding ReST-MCTS  *  (Value)SciBenchGLM4 GPT-3.5-turbo20.2 12.822.0 17.420.2 13.722.9 20.2ReST-MCTS*0.22PRM+Best-of-N ORM+Best-of-NSelf-Consistency0.20Accuracy0.16 0.180.140.12025,000 Total Tokens (Average Per Question) 50,000 75,000100,000</p>
<p>Table 8 :
8
Overall performance comparison with representative models on SciEval.
ModelsMethodPart I Part II Part III Part IV Ave.GLM4CoT ToT ReST-MCTS  *  69.16 88.05 52.50 85.83 60.83 87.7780.83 84.16 83.3378.11 79.77 78.9474.32 78.13 79.87GPT-3.5-TurboCoT ToT ReST-MCTS  *  29.72 78.88 28.88 78.61 32.50 76.1170.00 68.05 69.7271.46 67.59 70.9162.24 61.06 62.31</p>
<p>â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.â€¢ While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).â€¢ The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.â€¢ The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. â€¢ The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.â€¢ At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).</p>
<p>â€¢ It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.â€¢For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).â€¢ If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. Experiments Compute ResourcesQuestion: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes]
Justification: We have provided the experiments compute resources of our research in Table2in Section 4.</p>
<p>â€¢ At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.14.Crowdsourcing and Research with Human Subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: Our research does not involve crowdsourcing nor research with human subjects.Guidelines: â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.â€¢ Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA] Justification: Our research does not involve crowdsourcing nor research with human subjects.Guidelines: â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research.If you obtained IRB approval, you should clearly state this in the paper.</p>
<p>AcknowledgmentsDan and Sining would like to thank Zhipu AI for sponsoring the computation resources used in this work.Yisong is supported in part by NSF #1918655.Yuxiao and Jie are supported in part by the NSFC 62276148, NSFC for Distinguished Young Scholar 62425601, a research fund from Zhipu, New Cornerstone Science Foundation through the XPLORER PRIZE and Tsinghua University (Department of Computer Science and Technology) -Siemens Ltd., China Joint Research Center for Industrial Intelligence and Internet of Things (JCIIOT).Corresponding authors: Yisong Yue, Yuxiao Dong, and Jie Tang.NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer: [Yes] Justification: We have provided the main claims of our research in the abstract and introduction.Guidelines:â€¢ The answer NA means that the abstract and introduction do not include the claims made in the paper.â€¢ The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.â€¢ The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer: [Yes] Justification: We have provided the limitations of our research in Section H. Guidelines:â€¢ The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.â€¢ The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.â€¢ The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.â€¢ The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.â€¢ If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.â€¢ While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.Theory Assumptions and ProofsQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]Justification: We have provided the theory assumptions and proofs of our research in Section B.1 and Section B.2.Guidelines:â€¢ The answer NA means that the paper does not include theoretical results.â€¢ All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.â€¢ The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.â€¢ Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.Experimental Result ReproducibilityQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?Answer:[Yes]Justification: We have provided the experimental result reproducibility of our research in Section J.Guidelines:â€¢ The answer NA means that the paper does not include experiments.â€¢ If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.â€¢ While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.Open access to data and code
Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, NoemÃ­ Mercado, ; Samuel, R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam Mccandlish, Tom Brown, Jared Kaplan, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume,2022Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott JohnstonConstitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, CoRR, abs/2401.013352024</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, Chang Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, arXiv:2312.065852023arXiv preprint</p>
<p>V-star: Training verifiers for self-taught reasoners. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, Rishabh Agarwal, arXiv:2402.064572024arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024arXiv preprint</p>
<p>. Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, Jason Weston, arXiv:2404.197332024arXiv preprintIterative reasoning preference optimization</p>
<p>Peiyi Wang, Lei Li, Zhihong Shao, Damai Xu, Yifei Dai, Deli Li, Chen, Zhifang Wu, Sui, arXiv:2312.08935Math-shepherd: A label-free step-by-step verifier for llms in mathematical reasoning. 2023arXiv preprint</p>
<p>Learning planning-based reasoning by trajectories collection and process reward synthesizing. Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F Chen, Shafiq Joty, arXiv:2402.006582024arXiv preprint</p>
<p>Alphazerolike tree-search can guide large language model decoding and training. Xidong Feng, Ziyu Wan, Muning Wen, Ying Wen, Weinan Zhang, Jun Wang, arXiv:2309.171792023arXiv preprint</p>
<p>Learning to predict by the methods of temporal differences. Richard S Sutton, Machine learning. 31988</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, arXiv:2401.10020Self-rewarding language models. 2024arXiv preprint</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Less: Selecting influential data for targeted instruction tuning. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen, arXiv:2402.043332024arXiv preprint</p>
<p>Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Advances in Neural Information Processing Systems. 202436</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 58878392020</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Improved monte-carlo search. Levente Kocsis, Csaba SzepesvÃ¡ri, Jan Willemson, Univ. Tartu, Estonia, Tech. Rep. 12006</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.024142022arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>A I Meta, Meta llama 3. 2024</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, Metamath, arXiv:2309.12284Bootstrap your own mathematical questions for large language models. 2023arXiv preprint</p>
<p>Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, arXiv:2309.05653Mammoth: Building math generalist models through hybrid instruction tuning. 2023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.106352023arXiv preprint</p>
<p>Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, arXiv:2308.13149Scieval: A multi-level large language model evaluation benchmark for scientific research. 2023arXiv preprint</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Glm Team, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, arXiv:2406.12793A family of large language models from glm-130b to glm-4 all tools. 2024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, TimothÃ©e Lachaux, Baptiste Lacroix, Naman RoziÃ¨re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Textbooks are all you need. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar, Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo De Rosa, Olli Saarikivi, arXiv:2306.116442023arXiv preprint</p>
<p>Autore: Document-level relation extraction with large language models. Xue Lilong, Zhang Dan, Dong Yuxiao, Tang Jie, arXiv:2403.148882024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang, arXiv:2311.04155Black-box prompt optimization: Aligning large language models without model training. 2023arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang, arXiv:2310.128232023arXiv preprint</p>
<p>Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang, arXiv:2304.06767Raft: Reward ranked finetuning for generative foundation model alignment. 2023arXiv preprint</p>
<p>Reinforced self-training (rest) for language modeling. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730Advances in neural information processing systems</p>
<p>Rlaif: Scaling reinforcement learning from human feedback with ai feedback. Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, Abhinav Rastogi, arXiv:2309.002672023arXiv preprint</p>
<p>Antonia Creswell, Murray Shanahan, arXiv:2208.14271Faithful reasoning using large language models. 2022arXiv preprint</p>
<p>Maieutic prompting: Logically consistent reasoning with recursive explanations. Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, arXiv:2205.118222022arXiv preprint</p>
<p>Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, Karthik Narasimhan, arXiv:2310.06770Swe-bench: Can language models resolve real-world github issues?. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023arXiv preprint</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 55076762017</p>
<p>Toward self-improvement of llms via imagination, searching, and criticizing. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu, arXiv:2404.122532024arXiv preprint</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Hongming Zhang, Tianyang Yu, Alphazero, Deep Reinforcement Learning: Fundamentals, Research and Applications. 2020</p>
<p>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. Carl Henry W Sprueill, Edwards, Udishnu Mariefel V Olarte, Sanyal, Ji Heng, Sutanay Choudhury, arXiv:2310.144202023arXiv preprint</p>
<p>Don't throw away your value model! making ppo even better via value-guided monte-carlo tree search decoding. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz, 20232309arXiv e-prints</p>            </div>
        </div>

    </div>
</body>
</html>
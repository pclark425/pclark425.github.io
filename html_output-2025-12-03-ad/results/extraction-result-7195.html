<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7195 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7195</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7195</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-7eda139d737eea10fc1d95364327a41ec0cee4a4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7eda139d737eea10fc1d95364327a41ec0cee4a4" target="_blank">CoLAKE: Contextualized Language and Knowledge Embedding</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The Contextualized Language and Knowledge Embedding (CoLAKE) is proposed, which jointly learns contextualized representation for both language and knowledge with the extended MLM objective, and achieves surprisingly high performance on a synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language andknowledge representation.</p>
                <p><strong>Paper Abstract:</strong> With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7195.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7195.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WK graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>word-knowledge graph (WK graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified heterogeneous graph representation that mosaics a fully-connected word graph (the tokenized sentence) with extracted knowledge sub-graphs (entities and relation nodes) centered on linked entities; used as the input unit for pre-training a masked-Transformer (CoLAKE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>word-knowledge graph (WK graph)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each input sentence is tokenized and treated as a fully-connected word graph; recognized mentions are replaced by anchor entity nodes; for each anchor the model extracts a KG sub-graph (triplets where the anchor is head), turning entities and relations into graph nodes; the word graph and all extracted sub-graphs are concatenated into a single heterogeneous graph whose node sequence (words, entity nodes, relation nodes) is given position indices (soft-position indexing) and node-type embeddings and fed to a Transformer encoder with an adjacency-based attention mask that prevents attention between disconnected nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>heterogeneous graph serialized into a token-based sequential node list and processed with adjacency-masked self-attention (lossy due to neighbor sampling and head-only triplet selection)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Concatenate the fully-connected token sequence and the extracted KG subgraphs centered on anchor entities; for each anchor randomly select up to 15 neighboring relations and entities (only triples where anchor is head); relations are inserted as nodes; assign soft-position indices that allow repeats and keep triplet tokens continuous; use an adjacency mask matrix to restrict self-attention to graph edges (1-hop per layer).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Pre-training on English Wikipedia aligned to Wikidata5M (Wikidata5M); 26M WK graph training samples; 3,085,345 entity embeddings and 822 relation embeddings used.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pre-training for masked language modeling on WK graphs (extended MLM); evaluated on downstream knowledge-driven tasks (entity typing, relation extraction), knowledge probing (LAMA), GLUE, and word-knowledge graph completion.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoLAKE (Transformer encoder initialized from RoBERTa_BASE with modifications)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder initialized from RoBERTa_BASE; embedding layer extended with separate lookup tables for words (BPE), entities, and relations and node-type & soft-position embeddings; encoder uses masked multi-head self-attention with an adjacency mask matrix to reflect the WK graph structure; trained with an extended MLM objective masking word/entity/relation nodes; entity embeddings stored/updated in CPU with mixed CPU-GPU training; negative sampling used for entity prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Entity typing (Open Entity) micro P/R/F; Relation extraction (FewRel) macro P/R/F; LAMA / LAMA-UHN P@1; GLUE per-task scores and avg; Word-knowledge graph completion: MR, MRR, HITS@1/3/10.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Open Entity (micro): P=77.0, R=75.7, F=76.4; FewRel (macro): P=90.6, R=90.6, F=90.5; LAMA-Google-RE P@1=9.5, LAMA-T-REx P@1=28.8 (see Table 3 for more LAMA variants); GLUE average dev=86.3 (CoLAKE) vs RoBERTa 86.4; Word-KG completion (transductive): MR=2.03, MRR=82.48, H@1=72.14, H@3=92.19, H@10=98.58; Word-KG completion (inductive): MR=31.01, MRR=28.10, H@1=15.69, H@3=30.28, H@10=58.05.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides a direct way to jointly train language and knowledge representations via a single extended MLM objective; improves performance on knowledge-required tasks and factual probing compared to RoBERTa_BASE and several joint baselines; yields very strong results on a synthetic word-knowledge graph completion task (both transductive and inductive), demonstrating better structure-aware and inductive generalization than traditional KE models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Design choices introduce limitations: only up to 15 neighbors per anchor are injected (caps context), only triples where anchor is head are considered (directional loss of information), neighbor sub-graphs are sampled (stochastic / lossy coverage), training needs mixed CPU-GPU memory and negative sampling due to very large entity vocabulary, masked-anchor prediction can be trivial unless neighbors are discarded sometimes (authors discard neighbors 50% of time during pre-training), and CoLAKE slightly underperforms RoBERTa on general GLUE NLU average (small degradation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to semi-contextualized joint models (e.g., ERNIE, KnowBERT, KEPLER) CoLAKE jointly contextualizes entity and relation representations during pre-training (not fixed or separately pre-trained entity embeddings), and uses WK graphs rather than single pre-trained entity vectors; CoLAKE outperforms these counterparts on most knowledge-driven tasks and on the WK graph completion task, while showing comparable (slightly lower) performance on standard GLUE NLU tasks. Versus pure KE models (TransE, DistMult, ComplEx, RotatE), CoLAKE significantly outperforms them on the word-knowledge graph completion task (owing to combined structural and text semantics).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7195.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7195.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mention+Entity concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>concatenated mention and entity token representation (as in Pörner et al., 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple textual encoding strategy that concatenates the textual mention tokens with the corresponding entity token (anchor) in the input sequence (e.g., 'Jean Mara##is Jean_Marais'), used to encourage alignment between mention surface forms and symbolic entity embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>mention+entity inline concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For annotated mentions, the input sequence includes both the original textual mention (tokenized via BPE) and the linked symbolic entity token placed adjacent or concatenated (paper example: 'Jean Mara ##is Jean_Marais'), so the Transformer sees both surface form and entity identifier as tokens in the same input.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based sequential augmentation (textual inline augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Inline concatenation: keep the textual mention tokens and insert/append the corresponding anchor entity token(s) next to them in the sentence token sequence; use standard token, type, and position embeddings for both kinds of tokens (CoLAKE uses this approach for fine-tuning experiments following Pörner et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used during fine-tuning on Open Entity and FewRel (and generally for entity-annotated inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Entity typing and relation extraction fine-tuning; also used to align words and entity embeddings during pre-training instances that include anchors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CoLAKE (and previously E-BERT and similar works have used related schemes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied as an input preprocessing / tokenization convention rather than a distinct model; CoLAKE uses RoBERTa_BASE-initialized Transformer with the concatenated tokens present in the input graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>No isolated metric reported in the paper specifically for this preprocessing choice; downstream tasks use micro/macro P/R/F (Open Entity, FewRel) which used this scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Encourages alignment between mention surface forms and entity embeddings and is used to help map words and entities into a common representation space; adopted in both pre-training alignment objectives (anchor masking) and fine-tuning inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not report ablation isolating this preprocessing's quantitative effect; as a textual augmentation it increases token length and may increase computational cost; does not by itself capture multi-hop graph structure (relies on injected sub-graphs for structural info).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to replacing mentions by only symbolic entity tokens, concatenation preserves the original surface form and thus better encourages alignment between language and KG representations; it is a simpler alternative to methods that embed entities separately or generate entity embeddings from descriptions (e.g., KEPLER).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoLAKE: Contextualized Language and Knowledge Embedding', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>K-BERT: enabling language representation with knowledge graph <em>(Rating: 2)</em></li>
                <li>KEPLER: A unified model for knowledge embedding and pre-trained language representation <em>(Rating: 2)</em></li>
                <li>BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA <em>(Rating: 2)</em></li>
                <li>Commonsense knowledge mining from pretrained models <em>(Rating: 2)</em></li>
                <li>Inducing relational knowledge from BERT <em>(Rating: 1)</em></li>
                <li>ERNIE: enhanced language representation with informative entities <em>(Rating: 1)</em></li>
                <li>Knowledge enhanced contextual word representations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7195",
    "paper_id": "paper-7eda139d737eea10fc1d95364327a41ec0cee4a4",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "WK graph",
            "name_full": "word-knowledge graph (WK graph)",
            "brief_description": "A unified heterogeneous graph representation that mosaics a fully-connected word graph (the tokenized sentence) with extracted knowledge sub-graphs (entities and relation nodes) centered on linked entities; used as the input unit for pre-training a masked-Transformer (CoLAKE).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "word-knowledge graph (WK graph)",
            "representation_description": "Each input sentence is tokenized and treated as a fully-connected word graph; recognized mentions are replaced by anchor entity nodes; for each anchor the model extracts a KG sub-graph (triplets where the anchor is head), turning entities and relations into graph nodes; the word graph and all extracted sub-graphs are concatenated into a single heterogeneous graph whose node sequence (words, entity nodes, relation nodes) is given position indices (soft-position indexing) and node-type embeddings and fed to a Transformer encoder with an adjacency-based attention mask that prevents attention between disconnected nodes.",
            "representation_type": "heterogeneous graph serialized into a token-based sequential node list and processed with adjacency-masked self-attention (lossy due to neighbor sampling and head-only triplet selection)",
            "encoding_method": "Concatenate the fully-connected token sequence and the extracted KG subgraphs centered on anchor entities; for each anchor randomly select up to 15 neighboring relations and entities (only triples where anchor is head); relations are inserted as nodes; assign soft-position indices that allow repeats and keep triplet tokens continuous; use an adjacency mask matrix to restrict self-attention to graph edges (1-hop per layer).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Pre-training on English Wikipedia aligned to Wikidata5M (Wikidata5M); 26M WK graph training samples; 3,085,345 entity embeddings and 822 relation embeddings used.",
            "task_name": "Pre-training for masked language modeling on WK graphs (extended MLM); evaluated on downstream knowledge-driven tasks (entity typing, relation extraction), knowledge probing (LAMA), GLUE, and word-knowledge graph completion.",
            "model_name": "CoLAKE (Transformer encoder initialized from RoBERTa_BASE with modifications)",
            "model_description": "Transformer encoder initialized from RoBERTa_BASE; embedding layer extended with separate lookup tables for words (BPE), entities, and relations and node-type & soft-position embeddings; encoder uses masked multi-head self-attention with an adjacency mask matrix to reflect the WK graph structure; trained with an extended MLM objective masking word/entity/relation nodes; entity embeddings stored/updated in CPU with mixed CPU-GPU training; negative sampling used for entity prediction.",
            "performance_metric": "Entity typing (Open Entity) micro P/R/F; Relation extraction (FewRel) macro P/R/F; LAMA / LAMA-UHN P@1; GLUE per-task scores and avg; Word-knowledge graph completion: MR, MRR, HITS@1/3/10.",
            "performance_value": "Open Entity (micro): P=77.0, R=75.7, F=76.4; FewRel (macro): P=90.6, R=90.6, F=90.5; LAMA-Google-RE P@1=9.5, LAMA-T-REx P@1=28.8 (see Table 3 for more LAMA variants); GLUE average dev=86.3 (CoLAKE) vs RoBERTa 86.4; Word-KG completion (transductive): MR=2.03, MRR=82.48, H@1=72.14, H@3=92.19, H@10=98.58; Word-KG completion (inductive): MR=31.01, MRR=28.10, H@1=15.69, H@3=30.28, H@10=58.05.",
            "impact_on_training": "Provides a direct way to jointly train language and knowledge representations via a single extended MLM objective; improves performance on knowledge-required tasks and factual probing compared to RoBERTa_BASE and several joint baselines; yields very strong results on a synthetic word-knowledge graph completion task (both transductive and inductive), demonstrating better structure-aware and inductive generalization than traditional KE models.",
            "limitations": "Design choices introduce limitations: only up to 15 neighbors per anchor are injected (caps context), only triples where anchor is head are considered (directional loss of information), neighbor sub-graphs are sampled (stochastic / lossy coverage), training needs mixed CPU-GPU memory and negative sampling due to very large entity vocabulary, masked-anchor prediction can be trivial unless neighbors are discarded sometimes (authors discard neighbors 50% of time during pre-training), and CoLAKE slightly underperforms RoBERTa on general GLUE NLU average (small degradation).",
            "comparison_with_other": "Compared to semi-contextualized joint models (e.g., ERNIE, KnowBERT, KEPLER) CoLAKE jointly contextualizes entity and relation representations during pre-training (not fixed or separately pre-trained entity embeddings), and uses WK graphs rather than single pre-trained entity vectors; CoLAKE outperforms these counterparts on most knowledge-driven tasks and on the WK graph completion task, while showing comparable (slightly lower) performance on standard GLUE NLU tasks. Versus pure KE models (TransE, DistMult, ComplEx, RotatE), CoLAKE significantly outperforms them on the word-knowledge graph completion task (owing to combined structural and text semantics).",
            "uuid": "e7195.0",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Mention+Entity concatenation",
            "name_full": "concatenated mention and entity token representation (as in Pörner et al., 2019)",
            "brief_description": "A simple textual encoding strategy that concatenates the textual mention tokens with the corresponding entity token (anchor) in the input sequence (e.g., 'Jean Mara##is Jean_Marais'), used to encourage alignment between mention surface forms and symbolic entity embeddings.",
            "citation_title": "BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA",
            "mention_or_use": "use",
            "representation_name": "mention+entity inline concatenation",
            "representation_description": "For annotated mentions, the input sequence includes both the original textual mention (tokenized via BPE) and the linked symbolic entity token placed adjacent or concatenated (paper example: 'Jean Mara ##is Jean_Marais'), so the Transformer sees both surface form and entity identifier as tokens in the same input.",
            "representation_type": "token-based sequential augmentation (textual inline augmentation)",
            "encoding_method": "Inline concatenation: keep the textual mention tokens and insert/append the corresponding anchor entity token(s) next to them in the sentence token sequence; use standard token, type, and position embeddings for both kinds of tokens (CoLAKE uses this approach for fine-tuning experiments following Pörner et al.).",
            "canonicalization": false,
            "average_token_length": null,
            "dataset_name": "Used during fine-tuning on Open Entity and FewRel (and generally for entity-annotated inputs)",
            "task_name": "Entity typing and relation extraction fine-tuning; also used to align words and entity embeddings during pre-training instances that include anchors.",
            "model_name": "CoLAKE (and previously E-BERT and similar works have used related schemes)",
            "model_description": "Applied as an input preprocessing / tokenization convention rather than a distinct model; CoLAKE uses RoBERTa_BASE-initialized Transformer with the concatenated tokens present in the input graphs.",
            "performance_metric": "No isolated metric reported in the paper specifically for this preprocessing choice; downstream tasks use micro/macro P/R/F (Open Entity, FewRel) which used this scheme.",
            "performance_value": null,
            "impact_on_training": "Encourages alignment between mention surface forms and entity embeddings and is used to help map words and entities into a common representation space; adopted in both pre-training alignment objectives (anchor masking) and fine-tuning inputs.",
            "limitations": "Paper does not report ablation isolating this preprocessing's quantitative effect; as a textual augmentation it increases token length and may increase computational cost; does not by itself capture multi-hop graph structure (relies on injected sub-graphs for structural info).",
            "comparison_with_other": "Compared to replacing mentions by only symbolic entity tokens, concatenation preserves the original surface form and thus better encourages alignment between language and KG representations; it is a simpler alternative to methods that embed entities separately or generate entity embeddings from descriptions (e.g., KEPLER).",
            "uuid": "e7195.1",
            "source_info": {
                "paper_title": "CoLAKE: Contextualized Language and Knowledge Embedding",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "K-BERT: enabling language representation with knowledge graph",
            "rating": 2
        },
        {
            "paper_title": "KEPLER: A unified model for knowledge embedding and pre-trained language representation",
            "rating": 2
        },
        {
            "paper_title": "BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA",
            "rating": 2
        },
        {
            "paper_title": "Commonsense knowledge mining from pretrained models",
            "rating": 2
        },
        {
            "paper_title": "Inducing relational knowledge from BERT",
            "rating": 1
        },
        {
            "paper_title": "ERNIE: enhanced language representation with informative entities",
            "rating": 1
        },
        {
            "paper_title": "Knowledge enhanced contextual word representations",
            "rating": 1
        }
    ],
    "cost": 0.01265325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CoLAKE: Contextualized Language and Knowledge Embedding</h1>
<p>Tianxiang Sun ${ }^{1, *}$, Yunfan Shao ${ }^{1}$, Xipeng Qiu ${ }^{1, \dagger}$ Qipeng Guo ${ }^{1}$, Yaru Hu ${ }^{1}$, Xuanjing Huang ${ }^{1}$, Zheng Zhang ${ }^{2}$<br>${ }^{1}$ Shanghai Key Laboratory of Intelligent Information Processing, Fudan University<br>${ }^{1}$ School of Computer Science, Fudan University<br>${ }^{2}$ Amazon Shanghai AI Lab<br>{txsun19,yfshao19,xpqiu,qpguo16,xjhuang}@fudan.edu.cn<br>yrhu112358@outlook.com zhaz@amazon.com</p>
<h4>Abstract</h4>
<p>With the emerging branch of incorporating factual knowledge into pre-trained language models such as BERT, most existing models consider shallow, static, and separately pre-trained entity embeddings, which limits the performance gains of these models. Few works explore the potential of deep contextualized knowledge representation when injecting knowledge. In this paper, we propose the Contextualized Language and Knowledge Embedding (CoLAKE), which jointly learns contextualized representation for both language and knowledge with the extended MLM objective. Instead of injecting only entity embeddings, CoLAKE extracts the knowledge context of an entity from large-scale knowledge bases. To handle the heterogeneity of knowledge context and language context, we integrate them in a unified data structure, word-knowledge graph (WK graph). CoLAKE is pre-trained on large-scale WK graphs with the modified Transformer encoder. We conduct experiments on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks. Experimental results show that CoLAKE outperforms previous counterparts on most of the tasks. Besides, CoLAKE achieves surprisingly high performance on our synthetic task called word-knowledge graph completion, which shows the superiority of simultaneously contextualizing language and knowledge representation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Deep contextualized language models pre-trained on large-scale unlabeled corpora have achieved significant improvement on a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). However, they are shown to have difficulty capturing factual knowledge (Logan et al., 2019).</p>
<p>Recently, there is a growing interest in combining pre-trained language models (PLMs) with structured knowledge. A popular approach is to inject pre-trained entity embeddings into PLMs to better capture factual knowledge, such as ERNIE (Zhang et al., 2019) and KnowBERT (Peters et al., 2019). The shortcomings of these models can be summarized as follows: (1) The entity embeddings are separately pre-trained with some knowledge embedding (KE) models (e.g., TransE (Bordes et al., 2013)), and fixed during training PLMs. Thus they are not real joint models to learn the knowledge embedding and language embedding simultaneously. (2) The previous models only take entity embeddings to enhance PLMs, which are hard to fully capture the rich contextual information of an entity in the knowledge graph (KG). Thus their performance gains are limited by the quality of pre-trained entity embeddings. (3) The pre-trained entity embeddings are static and need to be re-trained when the KG is slightly changed.</p>
<p>In this paper, we propose the Contextualized Language And Knowledge Embedding (CoLAKE), which jointly learns language representation and knowledge representation in a common representation space. Different from the previous models, CoLAKE dynamically represents an entity according to its knowledge context and language context. For each entity, CoLAKE considers a sub-graph surrounding</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (a) When injecting knowledge, CoLAKE considers the <em>knowledge context</em> about the entity while previous semi-contextualized joint models only consider a single entity embedding. By contextualizing the entity, CoLAKE is able to directly access (Harry_Potter, <em>enemy of</em>, Lord_Voldemort) to help understand sentence (1) and access (Harry_Potter, <em>mother</em>, Lily_Poter) to help understand sentence (2). (b) The word-knowledge graph (WK graph) is a unified structure to represent both language context and knowledge context, which is composed of two parts: the fully-connected word graph and knowledge sub-graphs extracted from large KGs.</p>
<p>it as its knowledge context that contains the facts (triplets) about the entity. In this way, CoLAKE can dynamically access different facts as background knowledge to help understand the current text. As shown in Figure 1(a), to understand different sentences, CoLAKE can utilize different facts about the linked entity Harry_Potter. The <em>knowledge context</em> of Harry_Potter is a sub-graph containing the triplets about it. According to whether or not to utilize entities' knowledge context, our proposed CoLAKE can be distinguished from previous models, which we call <em>semi-contextualized joint models</em> since they only contextualize language representation.</p>
<p>To deal with the heterogeneous structure of language and KG, we build a graph to integrate them into a unified data structure, called <em>word-knowledge graph (WK graph)</em>. Most recent successful PLMs use Transformer architecture (Vaswani et al., 2017), which treats input sequences as fully-connected word graphs. WK graph is knowledge-augmented word graph. Using entities mentioned in the sentence, we extract sub-graphs centered on those mentioned entities from KGs. Then we mosaic such sub-graphs and the word graph in a unified heterogeneous graph, i.e. WK graph. An instance of the WK graph can be found in Figure 1(b). The constructed WK graph is fed into CoLAKE along with its adjacency matrix to control the information flow to reflect the graph structure. CoLAKE is based on the Transformer encoder, with the embedding layer and the encoder layers slightly modified to adapt to input in the form of WK graph. Besides, we extend the masked language model (MLM) objective (Devlin et al., 2019) to the whole input graph. That is, apply the same masking strategy to word, entity, and relation nodes and training the model to predict the masked nodes based on the rest of the graph.</p>
<p>We evaluate CoLAKE on several knowledge-required tasks and GLUE (Wang et al., 2019a). Experimental results demonstrate that CoLAKE outperforms previous semi-contextualized counterparts on most of the tasks. To explore potential applications of CoLAKE, we design a synthetic task called word-knowledge graph completion. Our evaluation on this task shows that CoLAKE outperforms several KE models by a large margin, in transductive setting and inductive setting.</p>
<p>In summary, CoLAKE can be characterized in three-fold: (1) CoLAKE learns contextualized language representation and contextualized knowledge representation simultaneously with the extended MLM objective. (2) CoLAKE adopts the WK graph to integrate the heterogeneous input for language and knowledge. (3) CoLAKE is essentially a pre-trained graph neural network (GNN), thereby being structure-aware and easy to extend.</p>
<h1>2 Related Work</h1>
<p>Language Representation Learning. The past decade has witnessed the great success of pre-trained language representation. Initially, word representation pre-trained using multi-task objectives (Collobert and Weston, 2008) or co-occurrence statistics (Mikolov et al., 2013; Pennington et al., 2014) are static and non-contextual. Recently, contextualized word representation pre-trained on large-scale unlabeled corpora with deep neural networks has dominated across a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Qiu et al., 2020).</p>
<p>Knowledge Representation Learning. Knowledge Representation Learning (KRL) is also termed as Knowledge Embedding (KE), which is to map entities and relations into low-dimensional continuous vectors. Most existing methods use triplets as training samples to learn static, non-contextual embeddings for entities and relations (Bordes et al., 2013; Yang et al., 2015; Lin et al., 2015). Recent advances focusing on contextualized representation, which use subgraphs or paths as training samples, have achieved new state-of-the-art results on KG tasks (Wang et al., 2019b; Wang et al., 2020a).</p>
<p>Joint Language and Knowledge Models. Due to the mutual information existing in language and KGs, joint models often benefit both sides. Besides, tasks such as entity linking also require entity embeddings that are compatible with word embeddings. Combining the success of Mikolov et al. (2013) and Bordes et al. (2013), Wang et al. (2014) jointly learn embeddings for language and KG. Targeting mention-entity matching in entity linking, Yamada et al. (2016), Ganea and Hofmann (2017) also proposed joint methods to map entities and words into the same vector space. Inspired by the recent success of contextualized language representation, much effort has been devoted to injecting entity embeddings into PLMs (Zhang et al., 2019; Peters et al., 2019). Despite their success, the knowledge gains are limited by the expressivity of their used pre-trained entity embeddings, which is static and inflexible. In contrast, KEPLER (Wang et al., 2019c) aims to benefit both sides so jointly learn language model and knowledge embedding. However, KEPLER does not directly learn embeddings for each entity but learns to generate entity embeddings with PLMs from entity descriptions. Besides, none of these work exploits the potential of contextualized knowledge representation, which makes them different from our proposed CoLAKE. A brief comparison can be found in Table 1. CoLAKE is conceptually similar to K-BERT (Liu et al., 2020) and BERT-MK (He et al., 2019). CoLAKE differs from K-BERT in that, instead of injecting triplets during fine-tuning, CoLAKE jointly learns embeddings for entities and relations during pretraining LMs. Besides, CoLAKE places language and knowledge representation learning into a unified pre-training task, masked language model, which makes it more concise than BERT-MK. In addition, CoLAKE is a general-purpose joint model while BERT-MK mainly focuses on medical domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Joint Models</th>
<th style="text-align: center;">Language <br> Objective</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Knowledge <br> Contextualized?</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Non-contextual</td>
<td style="text-align: left;">Wang et al. (2014)</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">TransE</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Yamada et al. (2016)</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">Skip-Gram</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">TransE*</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Semi-contextualized</td>
<td style="text-align: left;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">KEPLER (Wang et al., 2019c)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">TransE</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: left;">Contextualized</td>
<td style="text-align: left;">CoLAKE (Ours)</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">MLM</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of several joint models based on whether the representation is contextualized. *The entity embeddings are fixed during pre-training ERNIE. KnowBERT does not have restrictions on the entity embeddings. For ERNIE and KnowBERT, we omit the next sentence prediction (NSP) objective.</p>
<h2>3 CoLAKE</h2>
<p>CoLAKE jointly learns contextualized representation for language and knowledge by pre-training on structured, unlabeled word-knowledge graphs (WK graphs). We first introduce how to construct such WK graphs, then we describe the model architecture and the implementation details.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of WK graph construction. The WK graph is an undirected heterogeneous graph. The numbers marked on graph nodes indicate the position index introduced in Section 3.2.</p>
<h1>3.1 Graph Construction</h1>
<p>Typically, language embedding models take sequences as input while knowledge embedding (KE) models take triplets or knowledge sub-graphs as input. Recent successful PLMs take Transformer (Vaswani et al., 2017) as their backbone architecture, which actually processes sequences as fully-connected word graphs. Thus, graph is a common data structure to represent language and knowledge. In this section, we show how to integrate word graph and knowledge sub-graphs into the unified WK graph.</p>
<p>We first tokenize a sentence into a sequence of tokens and fully connect them as a word graph. Then we recognize the mentions in the sentence and use an entity linker to find the corresponding entities in a certain KG. The mention nodes are then replaced by their linked entities, which are called anchor nodes. By this replacement, the model is encouraged to map the injected entities and mention words near one another in the vector space. Centered on the anchor nodes $\left{e_{i}\right}<em i="i">{i}$, we can extract their knowledge contexts $\left{\left{e</em>\right}}, r_{i j}, e_{i j<em i="i">{j}\right}</em>$ to form sub-graphs, in which relations are also transformed into graph nodes. The extracted sub-graphs and the word graph are then concatenated with anchor nodes to obtain the WK graph. Figure 2 shows the process of constructing a WK graph. In practice, for each anchor node we randomly select up to 15 neighboring relations and entities to construct a sub-graph to be injected into the WK graph. We only consider triplets in which anchor node is head (subject) instead of tail (object). In the WK graph, entities are unique but relations are allowed to repeat.</p>
<h3>3.2 Model Architecture</h3>
<p>The constructed WK graphs are then fed into the Transformer (Vaswani et al., 2017) encoder. We modify the embedding and encoder layers of vanilla Transformer to adapt to input in the form of WK graph.</p>
<p>Embedding Layer. The input embedding is the sum of token embedding, type embedding, and position embedding. For token embedding, we maintain three lookup tables for words, entities, and relations respectively. For word embedding, we follow RoBERTa (Liu et al., 2019) which uses Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to transform sequence into subwords units to handle the large vocabulary. In contrast, we directly learn embeddings for each unique entity and relation as common knowledge embedding methods do. The token embeddings are obtained by concatenating word, entity, and relation embeddings, which are of the same dimensionality. There are different types of nodes so the WK graph is heterogeneous. To handle this, we simply use type embedding to indicate the node types, i.e. word, entity, and relation. For position embedding, we need to assign each injected entity and relation a position index. Inspired by Liu et al. (2020), we adopt the soft-position index which allows repeated position indices and keeps tokens in the same triplet continuous. Figure 2 shows an intuitive example of how to assign position index to graph nodes.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Overall architecture of CoLAKE. In this case, three triplets, (Harry_Potter, mother, Lily_Potter), (Harry_Potter, spouse, Ginny_Weasley), and (Harry_Potter, enemy of, Lord_Voldemort) are injected into the raw sequence. The model is asked to predict the masked word wand, the masked entity Lily_Potter, and the masked relation enemy of.</p>
<p>Masked Transformer Encoder. We use masked multi-head self-attention to control the information flow to reflect the structure of WK graph. Given the representation of graph nodes $\mathbf{X} \in \mathbb{R}^{n \times d}$, where $n$ is the number of nodes and $d$ is the dimension for each node, the representation after masked self-attention is obtained by</p>
<p>$$
\begin{aligned}
\mathbf{Q}, \mathbf{K}, \mathbf{V} &amp; =\mathbf{X} \mathbf{W}^{Q}, \mathbf{X} \mathbf{W}^{K}, \mathbf{X} \mathbf{W}^{V} \
\mathbf{A} &amp; =\frac{\mathbf{Q K}^{\top}}{\sqrt{d_{k}}} \
\operatorname{Attn}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) &amp; =\operatorname{Softmax}(\mathbf{A}+\mathbf{M}) \mathbf{V}
\end{aligned}
$$</p>
<p>where $\mathbf{W}^{Q}, \mathbf{W}^{K}, \mathbf{W}^{V} \in \mathbb{R}^{d \times d_{k}}$ are learnable parameters. $\mathbf{M} \in \mathbb{R}^{n \times n}$ is the mask matrix given by</p>
<p>$$
\mathbf{M}<em i="i">{i j}= \begin{cases}0 &amp; \text { if } x</em>
$$} \text { and } x_{j} \text { are connected } \ -\inf &amp; \text { if } x_{i} \text { and } x_{j} \text { are disconnected }\end{cases</p>
<p>With the masked Transformer encoder, each node can only gather information from its 1-hop neighbor at each layer. Masked Transformer encoder works similar to GAT (Velickovic et al., 2018).</p>
<h1>3.3 Pre-Training Objective</h1>
<p>The Masked Language Model (MLM) objective is to randomly mask some of tokens from the input and train the model to predict the original vocabulary id of the masked tokens based on their contexts. In this section, we extend the MLM from word sequences to WK graphs.</p>
<p>In particular, we mask $15 \%$ of graph nodes at random. When a node is masked, we replace it with (1) the [MASK] token $80 \%$ of time, (2) a randomly sampled node with the same type as the original node $10 \%$ of time, (3) the unchanged node $10 \%$ of time. As different types of nodes are masked, we encourage CoLAKE to learn different aspects of capabilities:</p>
<ul>
<li>
<p>Masking word nodes. When words are masked, the objective is similar to traditional MLM. The difference is, CoLAKE can predict masked words based on not only the context words but also the entities and relations in the WK graph. Masking words helps CoLAKE learn linguistic knowledge.</p>
</li>
<li>
<p>Masking entity nodes. If the masked entity is an anchor node, the objective, which is to predict the anchor node based on its context, helps to align the representation spaces of language and knowledge. Take the instance in Figure 1(b), the embedding of entity Harry_Potter will be learned to be similar to its textual form, Harry Potter. If the masked entity is not an anchor node, the MLM objective is similar to that used in semantic matching-based KE methods such as ConvE (Dettmers et al., 2018) and CoKE (Wang et al., 2019b), which enables CoLAKE to learn a large number of entity embeddings. Masking entity nodes helps CoLAKE (a) map words and entities into a common representation space, and (b) learn contextualized representation for entities.</p>
</li>
<li>Masking relation nodes. If the masked relation is between two unique anchor nodes, the objective is similar to distantly supervised relation extraction (Craven and Kumlien, 1999), which requires the model to classify the relationship between two entities mentioned in the text. Otherwise, the objective is to predict the relationship between its two neighboring entities, which is similar to traditional KE methods. Masking relation nodes helps CoLAKE (a) learn to do relation extraction, and (b) learn contextualized representation for relations.</li>
</ul>
<p>However, the pre-training task of predicting masked anchor nodes could be trivial because the model is easy to accomplish this task only based on the knowledge context instead of the language context, which is more varied than knowledge context. To mitigate this, we discard neighbors of anchor nodes in $50 \%$ of time during pre-training.</p>
<h1>3.4 Model Training</h1>
<p>CoLAKE is trained with cross-entropy loss. We use three classification heads to predict three types of nodes. In practice, however, the large number of entities brings challenges to training and predicting.</p>
<p>Mixed CPU-GPU Training. Due to the large number of entities in KG, training the whole model in GPU is intractable. To handle this, we asynchronously update entity embeddings in CPU memory while keeping the rest of our model updated in GPU. In particular, we store and update entity embeddings in CPU memory which is shared among multiple trainer processes. During pre-training, the trainer processes read the entity embeddings from the shared CPU memory and write the gradients back to CPU. Our implementation is based on the distributed key-value store (KVStore) from Zheng et al. (2020).</p>
<p>Negative Sampling. Applying the Softmax function to the huge number of entities is very timeconsuming. CoLAKE uses negative sampling to conduct prediction for each entity over one positive entity and $k(k \ll n)$ negative entities instead of all $n$ entities in KG. Following Mikolov et al. (2013), we sample negative entities from the $3 / 4$ powered entity frequency distribution.</p>
<h2>4 Experiments</h2>
<p>In this section, we present the details of pre-training and fine-tuning CoLAKE, and its experimental results on knowledge-driven tasks, knowledge probing tasks, and language understanding tasks.</p>
<h3>4.1 Pre-Training Data and Implementation Details</h3>
<p>CoLAKE uses English Wikipedia (2020/03/01) ${ }^{2}$ as pre-training data and uses WikiExtractor ${ }^{3}$ to process the downloaded Wikipedia dump. We use Wikipedia anchors to align text to Wikidata5M (Wang et al., 2019c), which is a newly proposed large-scale KG containing 21M fact triplets. We construct WK graphs as training samples and filter out graph samples without entity nodes and relation nodes. Finally, CoLAKE pre-trained the Transformer encoder along with 3,085,345 entity embeddings and 822 relation embeddings on 26 M training samples.</p>
<p>The Transformer encoder of CoLAKE is initialized with RoBERTa ${ }<em _BASE="{BASE" _text="\text">{\text {BASE }}$ (Liu et al., 2019). We use the implementation from HuggingFace's Transformer (Wolf et al., 2019). The entity embeddings and relation embeddings are initialized with the average of the RoBERTa ${ }</em>$ BPE embeddings of entity}</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>and relation aliases provided by Wang et al. (2019c). AdamW with $\beta_{1}=0.9, \beta_{2}=0.98$ is used in pre-training. We train CoLAKE with the batch size of 2048 and the learning rate of 1e-4 for 1 epoch. For each anchor node, we sample $k=200$ negative entities. CoLAKE is trained on 832 G NVIDIA V100 GPUs for 38 hours.</p>
<h1>4.2 Knowledge-Driven Tasks</h1>
<p>We first fine-tune and evaluate CoLAKE on knowledge-driven tasks. To annotate entities in the sentence, we use TAGME (Ferragina and Scaiella, 2010) to link mentions to entities in KGs. Instead of replacing the textual mention with its symbolic entity, we follow Pörner et al. (2019) and concatenate the two forms of tokens, e.g. Jean Mara ##is Jean_Marais. Concretely, we conduct experiments on two knowledge-driven tasks: entity typing and relation extraction.</p>
<p>Entity Typing. The entity typing task is to classify the semantic type of a given entity mention based on its surface form and context. We add two special tokens, [ENT] and [/ENT], before and after the entity mentions to be classified and use the final representation of the [CLS] token as the feature to conduct classification ${ }^{4}$. We evaluate CoLAKE on Open Entity (Choi et al., 2018). To compare with ERNIE, KnowBERT, and KEPLER, we adopt the same experiment setting which considers nine general types. To be consistent with previous work, we adopt micro precision, recall, and F1 score as evaluation metrics. The experimental results are shown in Table 2.</p>
<p>Relation Extraction. The relation extraction task is to classify the relationship between two entities mentioned in a given sentence. During fine-tuning, we add four special tokens, [HD], [/HD], [TL] and [/TL] to identify the head entity and the tail entity. Also, we use the final representation of the [CLS] token as the feature to be fed into the classifier. We evaluate CoLAKE on FewRel (Han et al., 2018) that is rearranged by Zhang et al. (2019). Since FewRel is built with Wikidata, we discard triplets in the FewRel test set from pre-training data to avoid information leakage. Following previous work, we report macro precision, recall and F1 score on FewRel. The experimental results can be found in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Open Entity</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">FewRel</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F</td>
</tr>
<tr>
<td style="text-align: left;">BERT (Devlin et al., 2019)</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">84.9</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa (Liu et al., 2019)</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">85.3</td>
</tr>
<tr>
<td style="text-align: left;">ERNIE (Zhang et al., 2019)</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: left;">KnowBERT (Peters et al., 2019)</td>
<td style="text-align: center;">$\mathbf{7 8 . 6}$</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER (Wang et al., 2019c)</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">E-BERT (Pörner et al., 2019)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">88.5</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE (Ours)</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">$\mathbf{7 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 4}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on Open Entity and FewRel.</p>
<h3>4.3 Knowledge Probing</h3>
<p>LAMA (LAnguage Model Analysis) probe (Petroni et al., 2019) aims to measure factual knowledge stored in language models via cloze-style statement like: Dante was born in [MASK]. Subsequently, a more "factual" subset of LAMA, LAMA-UHN (Pörner et al., 2019), is constructed by filtering out easy-to-answer samples. We evaluate CoLAKE on these two probes and report the mean precision at one (P@1) macro-averaged over relations.</p>
<p>For fair comparision, we use the intersection of the vocabularies for all considered models and construct a common vocabulary of $\sim 18 \mathrm{~K}$ case-sensitive tokens. In this experiment, considered models include ELMo (Peters et al., 2018), ELMo5.5B (Peters et al., 2018), BERT ${ }<em _BASE="{BASE" _text="\text">{\text {BASE }}$ (Devlin et al., 2019), RoBERTa ${ }</em>$ (Liu et al., 2019), and K-Adapter (Wang et al., 2020b).}</p>
<p>The results of LAMA and LAMA-UHN are shown in Table 3. It is worth noticing that BERT outperforms RoBERTa by a large margin. Wang et al. (2020b) reported the same phenomenon in their</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Corpus</th>
<th style="text-align: center;">Pre-trained Models</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">ELMo</td>
<td style="text-align: center;">ELMo5.5B</td>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">RoBERTa</td>
<td style="text-align: center;">CoLAKE</td>
<td style="text-align: center;">K-Adapter*</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-Google-RE</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">7.0</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-Google-RE</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">2.7</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">3.7</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">LAMA-UHN-T-REx</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">23.3</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">20.4</td>
<td style="text-align: center;">23.0</td>
</tr>
</tbody>
</table>
<p>Table 3: P@1 on LAMA and LAMA-UHN. *K-Adapter is based on RoBERTa ${ }<em _BASE="{BASE" _text="\text">{\text {LARGE }}$ while other Transformer-based LMs are of BASE size. Besides, K-Adapter uses a subset of T-REx as its training data, which may contribute to its superiority over CoLAKE on LAMA-T-REx and LAMA-UHN-T-REx.
paper. We conjecture that the main reason behind this is the larger and byte-level BPE vocabulary used by RoBERTa. Though, CoLAKE outperforms its baseline, RoBERTa ${ }</em>$, by a significant margin. Besides, CoLAKE even improves over the LARGE size of model, K-Adapter, by $2.5 \%$ and $1.2 \%$ on LAMA-Google-RE and LAMA-UHN-Google-RE respectively.}</p>
<h1>4.4 Language Understanding Tasks</h1>
<p>We also evaluate CoLAKE on the General Language Understanding Evaluation (GLUE) (Wang et al., 2019a), which provides a collection of diverse NLU tasks. Since these tasks require little factual knowledge, we attempt to explore whether CoLAKE degenerates the performance on these NLU tasks.</p>
<p>The experimental results on GLUE dev set are shown in Table 4. CoLAKE is slightly degraded from RoBERTa but improves over KEPLER by $1.4 \%$ on average. We conclude that CoLAKE is able to simultaneously model text and knowledge via the heterogeneous WK graph. In summary, our experiments demonstrate that CoLAKE significantly improves the performance on knowledge-required tasks and at the same time achieves comparable results on language understanding tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MNLI (m/mm)</th>
<th style="text-align: center;">QQP</th>
<th style="text-align: center;">QNLI</th>
<th style="text-align: center;">SST-2</th>
<th style="text-align: center;">CoLA</th>
<th style="text-align: center;">STS-B</th>
<th style="text-align: center;">MRPC</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">AVG.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RoBERTa</td>
<td style="text-align: center;">$87.5 / 87.3$</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: left;">KEPLER</td>
<td style="text-align: center;">$87.2 / 86.5$</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">84.9</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">$87.4 / 87.2$</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">92.4</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">86.3</td>
</tr>
</tbody>
</table>
<p>Table 4: GLUE results on dev set. Both of KEPLER and CoLAKE are initialized with RoBERTa ${ }_{\text {BASE }}$.</p>
<h3>4.5 Word-Knowledge Graph Completion</h3>
<p>Note that CoLAKE is essentially a GNN pre-trained on large-scale WK graphs, which makes it structureaware and easy to generalize to unseen entities.</p>
<p>To probe CoLAKE's capability of modeling both structural and semantic features, we design a task named word-knowledge graph completion. In particular, we use the FewRel test set to construct two experimental settings: transductive setting and inductive setting. In both settings, each sample provides a triplet $(h, r, t)$ and a sentence that expresses the triplet. The considered models are required to predict the relation $r$. For each sample in transductive setting, the two entities, $h$ and $t$, and their relation $r$ are seen in training phase. But the triplet $(h, r, t)$ has not appeared in the training data. We collect 10 K samples from the FewRel test set to construct the transductive setting. For each sample in inductive setting, at least one entity is unseen during training. This setting requires the model to be inductive so that it can generalize to unseen entities. We collect 1 K samples from the FewRel test set to construct the inductive setting. We directly evaluate CoLAKE in the two settings without further training on the FewRel training set. The forms of word-knowledge graph are depicted in Figure 4. For inductive setting, we encourage CoLAKE to infer the unseen entity by aggregating messages from its neighbors.</p>
<p>We take several well-known models for link prediction as our baselines ${ }^{5}$. For transductive setting, we compare CoLAKE with four widely-used models, i.e. TransE (Bordes et al., 2013), DistMult (Yang et</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the input word-knowledge graph for transductive setting and inductive setting. In transductive setting, Harry_Potter and Lord_Voldemort are seen during training. In inductive setting, Harry_Potter is unknown but its neighboring entities are seen in training data.
al., 2015), ComplEx (Trouillon et al., 2016), and RotatE (Sun et al., 2019). We use DGL-KE (Zheng et al., 2020) to train the four baseline models on Wikidata $5 \mathrm{M}^{6}$. For inductive setting, we take DKRL (Xie et al., 2016) as our baseline. As shown in Table 5, CoLAKE outperforms other models by a large margin thanks to its capability of simultaneously utilizing structural knowledge and rich text semantics while traditional KE models can only handle structural knowledge. Besides, the inductive ability of CoLAKE is more realistic. Unlike DKRL and KEPLER, which generate entity embeddings from descriptions, CoLAKE generates entity embeddings based on their neighbors.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">MR $\downarrow$</th>
<th style="text-align: center;">MRR</th>
<th style="text-align: center;">HITS@1</th>
<th style="text-align: center;">HITS@3</th>
<th style="text-align: center;">HITS@10</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Transductive setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">TransE (Bordes et al., 2013)</td>
<td style="text-align: center;">15.97</td>
<td style="text-align: center;">67.30</td>
<td style="text-align: center;">60.28</td>
<td style="text-align: center;">70.96</td>
<td style="text-align: center;">79.75</td>
</tr>
<tr>
<td style="text-align: left;">DistMult (Yang et al., 2015)</td>
<td style="text-align: center;">27.09</td>
<td style="text-align: center;">60.56</td>
<td style="text-align: center;">48.66</td>
<td style="text-align: center;">69.69</td>
<td style="text-align: center;">79.61</td>
</tr>
<tr>
<td style="text-align: left;">ComplEx (Trouillon et al., 2016)</td>
<td style="text-align: center;">26.73</td>
<td style="text-align: center;">61.09</td>
<td style="text-align: center;">49.80</td>
<td style="text-align: center;">70.64</td>
<td style="text-align: center;">79.78</td>
</tr>
<tr>
<td style="text-align: left;">RotatE (Sun et al., 2019)</td>
<td style="text-align: center;">30.36</td>
<td style="text-align: center;">70.90</td>
<td style="text-align: center;">64.74</td>
<td style="text-align: center;">74.89</td>
<td style="text-align: center;">81.05</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">2.03</td>
<td style="text-align: center;">82.48</td>
<td style="text-align: center;">72.14</td>
<td style="text-align: center;">92.19</td>
<td style="text-align: center;">98.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Inductive setting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">DKRL (Xie et al., 2016)</td>
<td style="text-align: center;">168.21</td>
<td style="text-align: center;">8.18</td>
<td style="text-align: center;">5.03</td>
<td style="text-align: center;">7.28</td>
<td style="text-align: center;">14.13</td>
</tr>
<tr>
<td style="text-align: left;">CoLAKE</td>
<td style="text-align: center;">31.01</td>
<td style="text-align: center;">28.10</td>
<td style="text-align: center;">15.69</td>
<td style="text-align: center;">30.28</td>
<td style="text-align: center;">58.05</td>
</tr>
</tbody>
</table>
<p>Table 5: The experimental results on word-knowledge graph completion task.</p>
<h1>5 Conclusion</h1>
<p>In this paper, we propose CoLAKE to jointly learn contextualized representation for language and knowledge. We integrate the language context and knowledge context in a unified data structure, wordknowledge graph. The experimental results show the effectiveness of CoLAKE on knowledge-required tasks. Besides, to explore the potential application of the WK graph, we design a task named WK graph completion, which shows that CoLAKE is essentially a powerful GNN that is structure-aware and inductive. The surprisingly high performance on WK graph completion inspires the potential applications of WK graph, for example, (a) CoLAKE may help to denoise distantly annotated samples of relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009), (b) CoLAKE can be used to measure the quality of graph-to-text templates (Davison et al., 2019; Bouraoui et al., 2020) due to its capability of preserving the original graph structure. We leave these applications as future work.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the National Key Research and Development Program of China (No. 2020AAA0106700) and National Natural Science Foundation of China (No. 62022027 and 61976056).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In NIPS.</p>
<p>Zied Bouraoui, José Camacho-Collados, and Steven Schockaert. 2020. Inducing relational knowledge from BERT. In AAAI, pages 7456-7463.</p>
<p>Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. 2018. Ultra-fine entity typing. In ACL, pages 87-96.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In ICML, pages 160-167.</p>
<p>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, pages 77-86.</p>
<p>Joe Davison, Joshua Feldman, and Alexander M. Rush. 2019. Commonsense knowledge mining from pretrained models. In EMNLP-IJCNLP, pages 1173-1178.</p>
<p>Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In AAAI, pages 1811-1818.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171-4186.</p>
<p>Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short text fragments (by wikipedia entities). In CIKM, pages 1625-1628.</p>
<p>Octavian-Eugen Ganea and Thomas Hofmann. 2017. Deep joint entity disambiguation with local neural attention. In EMNLP, pages 2619-2629.</p>
<p>Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In EMNLP, pages $4803-4809$.</p>
<p>Bin He, Di Zhou, Jinghui Xiao, Xin Jiang, Qun Liu, Nicholas Jing Yuan, and Tong Xu. 2019. Integrating graph contextualized knowledge into pre-trained language models. CoRR, abs/1912.00147.</p>
<p>Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In AAAI, pages 2181-2187.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: enabling language representation with knowledge graph.</p>
<p>Robert L. Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack’s wife hillary: Using knowledge graphs for fact-aware language modeling. In ACL, pages 5962-5971.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111-3119.</p>
<p>Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In $A C L$, pages 1003-1011.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP, pages 1532-1543.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In NAACL-HLT, pages 2227-2237.</p>
<p>Matthew E. Peters, Mark Neumann, Robert L. Logan IV, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In EMNLP-IJCNLP, pages 43-54.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In EMNLP-IJCNLP, pages 2463-2473.</p>
<p>Nina Pörner, Ulli Waltinger, and Hinrich Schütze. 2019. BERT is not a knowledge base (yet): Factual knowledge vs. name-based reasoning in unsupervised QA. CoRR, abs/1911.03681.</p>
<p>Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained models for natural language processing: A survey. CoRR, abs/2003.08271.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In $A C L$.</p>
<p>Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. 2019. Rotate: Knowledge graph embedding by relational rotation in complex space. In $I C L R$.</p>
<p>Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link prediction. In ICML.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998-6008.</p>
<p>Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In $I C L R$.</p>
<p>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In EMNLP, pages 1591-1601.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In $I C L R$.</p>
<p>Quan Wang, Pingping Huang, Haifeng Wang, Songtai Dai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu, and Hua Wu. 2019b. Coke: Contextualized knowledge graph embedding. CoRR, abs/1911.02168.</p>
<p>Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2019c. KEPLER: A unified model for knowledge embedding and pre-trained language representation. CoRR, abs/1911.06136.</p>
<p>Hongwei Wang, Hongyu Ren, and Jure Leskovec. 2020a. Entity context and relational paths for knowledge graph completion. CoRR, abs/2002.06757.</p>
<p>Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2020b. K-adapter: Infusing knowledge into pre-trained models with adapters. CoRR, abs/2002.01808.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-theart natural language processing. ArXiv, abs/1910.03771.</p>
<p>Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation learning of knowledge graphs with entity descriptions. In $A A A I$.</p>
<p>Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2016. Joint learning of the embedding of words and entities for named entity disambiguation. In CoNLL, pages 250-259.</p>
<p>Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In $I C L R$.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, pages 5754-5764.</p>
<p>Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: enhanced language representation with informative entities. In $A C L$, pages 1441-1451.</p>
<p>Da Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang, and George Karypis. 2020. DGL-KE: training knowledge graph embeddings at scale. CoRR, abs/2004.08532.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ The triplets in FewRel test set are removed from Wikidata5M to avoid information leakage.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>